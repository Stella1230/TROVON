static void node_shift(struct node *n, int shift)\r\n{\r\nuint32_t nr_entries = le32_to_cpu(n->header.nr_entries);\r\nuint32_t value_size = le32_to_cpu(n->header.value_size);\r\nif (shift < 0) {\r\nshift = -shift;\r\nBUG_ON(shift > nr_entries);\r\nBUG_ON((void *) key_ptr(n, shift) >= value_ptr(n, shift, value_size));\r\nmemmove(key_ptr(n, 0),\r\nkey_ptr(n, shift),\r\n(nr_entries - shift) * sizeof(__le64));\r\nmemmove(value_ptr(n, 0, value_size),\r\nvalue_ptr(n, shift, value_size),\r\n(nr_entries - shift) * value_size);\r\n} else {\r\nBUG_ON(nr_entries + shift > le32_to_cpu(n->header.max_entries));\r\nmemmove(key_ptr(n, shift),\r\nkey_ptr(n, 0),\r\nnr_entries * sizeof(__le64));\r\nmemmove(value_ptr(n, shift, value_size),\r\nvalue_ptr(n, 0, value_size),\r\nnr_entries * value_size);\r\n}\r\n}\r\nstatic void node_copy(struct node *left, struct node *right, int shift)\r\n{\r\nuint32_t nr_left = le32_to_cpu(left->header.nr_entries);\r\nuint32_t value_size = le32_to_cpu(left->header.value_size);\r\nBUG_ON(value_size != le32_to_cpu(right->header.value_size));\r\nif (shift < 0) {\r\nshift = -shift;\r\nBUG_ON(nr_left + shift > le32_to_cpu(left->header.max_entries));\r\nmemcpy(key_ptr(left, nr_left),\r\nkey_ptr(right, 0),\r\nshift * sizeof(__le64));\r\nmemcpy(value_ptr(left, nr_left, value_size),\r\nvalue_ptr(right, 0, value_size),\r\nshift * value_size);\r\n} else {\r\nBUG_ON(shift > le32_to_cpu(right->header.max_entries));\r\nmemcpy(key_ptr(right, 0),\r\nkey_ptr(left, nr_left - shift),\r\nshift * sizeof(__le64));\r\nmemcpy(value_ptr(right, 0, value_size),\r\nvalue_ptr(left, nr_left - shift, value_size),\r\nshift * value_size);\r\n}\r\n}\r\nstatic void delete_at(struct node *n, unsigned index)\r\n{\r\nunsigned nr_entries = le32_to_cpu(n->header.nr_entries);\r\nunsigned nr_to_copy = nr_entries - (index + 1);\r\nuint32_t value_size = le32_to_cpu(n->header.value_size);\r\nBUG_ON(index >= nr_entries);\r\nif (nr_to_copy) {\r\nmemmove(key_ptr(n, index),\r\nkey_ptr(n, index + 1),\r\nnr_to_copy * sizeof(__le64));\r\nmemmove(value_ptr(n, index, value_size),\r\nvalue_ptr(n, index + 1, value_size),\r\nnr_to_copy * value_size);\r\n}\r\nn->header.nr_entries = cpu_to_le32(nr_entries - 1);\r\n}\r\nstatic unsigned del_threshold(struct node *n)\r\n{\r\nreturn le32_to_cpu(n->header.max_entries) / 3;\r\n}\r\nstatic unsigned merge_threshold(struct node *n)\r\n{\r\nreturn 2 * (le32_to_cpu(n->header.max_entries) / 3) + 1;\r\n}\r\nstatic int init_child(struct dm_btree_info *info, struct node *parent,\r\nunsigned index, struct child *result)\r\n{\r\nint r, inc;\r\ndm_block_t root;\r\nresult->index = index;\r\nroot = value64(parent, index);\r\nr = dm_tm_shadow_block(info->tm, root, &btree_node_validator,\r\n&result->block, &inc);\r\nif (r)\r\nreturn r;\r\nresult->n = dm_block_data(result->block);\r\nif (inc)\r\ninc_children(info->tm, result->n, &le64_type);\r\n*((__le64 *) value_ptr(parent, index, sizeof(__le64))) =\r\ncpu_to_le64(dm_block_location(result->block));\r\nreturn 0;\r\n}\r\nstatic int exit_child(struct dm_btree_info *info, struct child *c)\r\n{\r\nreturn dm_tm_unlock(info->tm, c->block);\r\n}\r\nstatic void shift(struct node *left, struct node *right, int count)\r\n{\r\nif (!count)\r\nreturn;\r\nif (count > 0) {\r\nnode_shift(right, count);\r\nnode_copy(left, right, count);\r\n} else {\r\nnode_copy(left, right, count);\r\nnode_shift(right, count);\r\n}\r\nleft->header.nr_entries =\r\ncpu_to_le32(le32_to_cpu(left->header.nr_entries) - count);\r\nBUG_ON(le32_to_cpu(left->header.nr_entries) > le32_to_cpu(left->header.max_entries));\r\nright->header.nr_entries =\r\ncpu_to_le32(le32_to_cpu(right->header.nr_entries) + count);\r\nBUG_ON(le32_to_cpu(right->header.nr_entries) > le32_to_cpu(right->header.max_entries));\r\n}\r\nstatic void __rebalance2(struct dm_btree_info *info, struct node *parent,\r\nstruct child *l, struct child *r)\r\n{\r\nstruct node *left = l->n;\r\nstruct node *right = r->n;\r\nuint32_t nr_left = le32_to_cpu(left->header.nr_entries);\r\nuint32_t nr_right = le32_to_cpu(right->header.nr_entries);\r\nif (nr_left + nr_right <= merge_threshold(left)) {\r\nnode_copy(left, right, -nr_right);\r\nleft->header.nr_entries = cpu_to_le32(nr_left + nr_right);\r\ndelete_at(parent, r->index);\r\ndm_tm_dec(info->tm, dm_block_location(r->block));\r\n} else {\r\nunsigned target_left = (nr_left + nr_right) / 2;\r\nunsigned shift_ = nr_left - target_left;\r\nBUG_ON(le32_to_cpu(left->header.max_entries) <= nr_left - shift_);\r\nBUG_ON(le32_to_cpu(right->header.max_entries) <= nr_right + shift_);\r\nshift(left, right, nr_left - target_left);\r\n*key_ptr(parent, r->index) = right->keys[0];\r\n}\r\n}\r\nstatic int rebalance2(struct shadow_spine *s, struct dm_btree_info *info,\r\nunsigned left_index)\r\n{\r\nint r;\r\nstruct node *parent;\r\nstruct child left, right;\r\nparent = dm_block_data(shadow_current(s));\r\nr = init_child(info, parent, left_index, &left);\r\nif (r)\r\nreturn r;\r\nr = init_child(info, parent, left_index + 1, &right);\r\nif (r) {\r\nexit_child(info, &left);\r\nreturn r;\r\n}\r\n__rebalance2(info, parent, &left, &right);\r\nr = exit_child(info, &left);\r\nif (r) {\r\nexit_child(info, &right);\r\nreturn r;\r\n}\r\nreturn exit_child(info, &right);\r\n}\r\nstatic void __rebalance3(struct dm_btree_info *info, struct node *parent,\r\nstruct child *l, struct child *c, struct child *r)\r\n{\r\nstruct node *left = l->n;\r\nstruct node *center = c->n;\r\nstruct node *right = r->n;\r\nuint32_t nr_left = le32_to_cpu(left->header.nr_entries);\r\nuint32_t nr_center = le32_to_cpu(center->header.nr_entries);\r\nuint32_t nr_right = le32_to_cpu(right->header.nr_entries);\r\nuint32_t max_entries = le32_to_cpu(left->header.max_entries);\r\nunsigned target;\r\nBUG_ON(left->header.max_entries != center->header.max_entries);\r\nBUG_ON(center->header.max_entries != right->header.max_entries);\r\nif (((nr_left + nr_center + nr_right) / 2) < merge_threshold(center)) {\r\nunsigned shift = min(max_entries - nr_left, nr_center);\r\nBUG_ON(nr_left + shift > max_entries);\r\nnode_copy(left, center, -shift);\r\nleft->header.nr_entries = cpu_to_le32(nr_left + shift);\r\nif (shift != nr_center) {\r\nshift = nr_center - shift;\r\nBUG_ON((nr_right + shift) >= max_entries);\r\nnode_shift(right, shift);\r\nnode_copy(center, right, shift);\r\nright->header.nr_entries = cpu_to_le32(nr_right + shift);\r\n}\r\n*key_ptr(parent, r->index) = right->keys[0];\r\ndelete_at(parent, c->index);\r\nr->index--;\r\ndm_tm_dec(info->tm, dm_block_location(c->block));\r\n__rebalance2(info, parent, l, r);\r\nreturn;\r\n}\r\ntarget = (nr_left + nr_center + nr_right) / 3;\r\nBUG_ON(target > max_entries);\r\nshift(left, center, nr_left - target);\r\nshift(center, right, target - nr_right);\r\n*key_ptr(parent, c->index) = center->keys[0];\r\n*key_ptr(parent, r->index) = right->keys[0];\r\n}\r\nstatic int rebalance3(struct shadow_spine *s, struct dm_btree_info *info,\r\nunsigned left_index)\r\n{\r\nint r;\r\nstruct node *parent = dm_block_data(shadow_current(s));\r\nstruct child left, center, right;\r\nr = init_child(info, parent, left_index, &left);\r\nif (r)\r\nreturn r;\r\nr = init_child(info, parent, left_index + 1, &center);\r\nif (r) {\r\nexit_child(info, &left);\r\nreturn r;\r\n}\r\nr = init_child(info, parent, left_index + 2, &right);\r\nif (r) {\r\nexit_child(info, &left);\r\nexit_child(info, &center);\r\nreturn r;\r\n}\r\n__rebalance3(info, parent, &left, &center, &right);\r\nr = exit_child(info, &left);\r\nif (r) {\r\nexit_child(info, &center);\r\nexit_child(info, &right);\r\nreturn r;\r\n}\r\nr = exit_child(info, &center);\r\nif (r) {\r\nexit_child(info, &right);\r\nreturn r;\r\n}\r\nr = exit_child(info, &right);\r\nif (r)\r\nreturn r;\r\nreturn 0;\r\n}\r\nstatic int get_nr_entries(struct dm_transaction_manager *tm,\r\ndm_block_t b, uint32_t *result)\r\n{\r\nint r;\r\nstruct dm_block *block;\r\nstruct node *n;\r\nr = dm_tm_read_lock(tm, b, &btree_node_validator, &block);\r\nif (r)\r\nreturn r;\r\nn = dm_block_data(block);\r\n*result = le32_to_cpu(n->header.nr_entries);\r\nreturn dm_tm_unlock(tm, block);\r\n}\r\nstatic int rebalance_children(struct shadow_spine *s,\r\nstruct dm_btree_info *info, uint64_t key)\r\n{\r\nint i, r, has_left_sibling, has_right_sibling;\r\nuint32_t child_entries;\r\nstruct node *n;\r\nn = dm_block_data(shadow_current(s));\r\nif (le32_to_cpu(n->header.nr_entries) == 1) {\r\nstruct dm_block *child;\r\ndm_block_t b = value64(n, 0);\r\nr = dm_tm_read_lock(info->tm, b, &btree_node_validator, &child);\r\nif (r)\r\nreturn r;\r\nmemcpy(n, dm_block_data(child),\r\ndm_bm_block_size(dm_tm_get_bm(info->tm)));\r\nr = dm_tm_unlock(info->tm, child);\r\nif (r)\r\nreturn r;\r\ndm_tm_dec(info->tm, dm_block_location(child));\r\nreturn 0;\r\n}\r\ni = lower_bound(n, key);\r\nif (i < 0)\r\nreturn -ENODATA;\r\nr = get_nr_entries(info->tm, value64(n, i), &child_entries);\r\nif (r)\r\nreturn r;\r\nif (child_entries > del_threshold(n))\r\nreturn 0;\r\nhas_left_sibling = i > 0;\r\nhas_right_sibling = i < (le32_to_cpu(n->header.nr_entries) - 1);\r\nif (!has_left_sibling)\r\nr = rebalance2(s, info, i);\r\nelse if (!has_right_sibling)\r\nr = rebalance2(s, info, i - 1);\r\nelse\r\nr = rebalance3(s, info, i - 1);\r\nreturn r;\r\n}\r\nstatic int do_leaf(struct node *n, uint64_t key, unsigned *index)\r\n{\r\nint i = lower_bound(n, key);\r\nif ((i < 0) ||\r\n(i >= le32_to_cpu(n->header.nr_entries)) ||\r\n(le64_to_cpu(n->keys[i]) != key))\r\nreturn -ENODATA;\r\n*index = i;\r\nreturn 0;\r\n}\r\nstatic int remove_raw(struct shadow_spine *s, struct dm_btree_info *info,\r\nstruct dm_btree_value_type *vt, dm_block_t root,\r\nuint64_t key, unsigned *index)\r\n{\r\nint i = *index, r;\r\nstruct node *n;\r\nfor (;;) {\r\nr = shadow_step(s, root, vt);\r\nif (r < 0)\r\nbreak;\r\nif (shadow_has_parent(s)) {\r\n__le64 location = cpu_to_le64(dm_block_location(shadow_current(s)));\r\nmemcpy(value_ptr(dm_block_data(shadow_parent(s)), i, sizeof(__le64)),\r\n&location, sizeof(__le64));\r\n}\r\nn = dm_block_data(shadow_current(s));\r\nif (le32_to_cpu(n->header.flags) & LEAF_NODE)\r\nreturn do_leaf(n, key, index);\r\nr = rebalance_children(s, info, key);\r\nif (r)\r\nbreak;\r\nn = dm_block_data(shadow_current(s));\r\nif (le32_to_cpu(n->header.flags) & LEAF_NODE)\r\nreturn do_leaf(n, key, index);\r\ni = lower_bound(n, key);\r\nroot = value64(n, i);\r\n}\r\nreturn r;\r\n}\r\nint dm_btree_remove(struct dm_btree_info *info, dm_block_t root,\r\nuint64_t *keys, dm_block_t *new_root)\r\n{\r\nunsigned level, last_level = info->levels - 1;\r\nint index = 0, r = 0;\r\nstruct shadow_spine spine;\r\nstruct node *n;\r\ninit_shadow_spine(&spine, info);\r\nfor (level = 0; level < info->levels; level++) {\r\nr = remove_raw(&spine, info,\r\n(level == last_level ?\r\n&info->value_type : &le64_type),\r\nroot, keys[level], (unsigned *)&index);\r\nif (r < 0)\r\nbreak;\r\nn = dm_block_data(shadow_current(&spine));\r\nif (level != last_level) {\r\nroot = value64(n, index);\r\ncontinue;\r\n}\r\nBUG_ON(index < 0 || index >= le32_to_cpu(n->header.nr_entries));\r\nif (info->value_type.dec)\r\ninfo->value_type.dec(info->value_type.context,\r\nvalue_ptr(n, index, info->value_type.size));\r\ndelete_at(n, index);\r\n}\r\n*new_root = shadow_root(&spine);\r\nexit_shadow_spine(&spine);\r\nreturn r;\r\n}
