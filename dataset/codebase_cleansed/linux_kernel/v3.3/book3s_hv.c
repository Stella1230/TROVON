void kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nlocal_paca->kvm_hstate.kvm_vcpu = vcpu;\r\nlocal_paca->kvm_hstate.kvm_vcore = vcpu->arch.vcore;\r\n}\r\nvoid kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nvoid kvmppc_set_msr(struct kvm_vcpu *vcpu, u64 msr)\r\n{\r\nvcpu->arch.shregs.msr = msr;\r\nkvmppc_end_cede(vcpu);\r\n}\r\nvoid kvmppc_set_pvr(struct kvm_vcpu *vcpu, u32 pvr)\r\n{\r\nvcpu->arch.pvr = pvr;\r\n}\r\nvoid kvmppc_dump_regs(struct kvm_vcpu *vcpu)\r\n{\r\nint r;\r\npr_err("vcpu %p (%d):\n", vcpu, vcpu->vcpu_id);\r\npr_err("pc = %.16lx msr = %.16llx trap = %x\n",\r\nvcpu->arch.pc, vcpu->arch.shregs.msr, vcpu->arch.trap);\r\nfor (r = 0; r < 16; ++r)\r\npr_err("r%2d = %.16lx r%d = %.16lx\n",\r\nr, kvmppc_get_gpr(vcpu, r),\r\nr+16, kvmppc_get_gpr(vcpu, r+16));\r\npr_err("ctr = %.16lx lr = %.16lx\n",\r\nvcpu->arch.ctr, vcpu->arch.lr);\r\npr_err("srr0 = %.16llx srr1 = %.16llx\n",\r\nvcpu->arch.shregs.srr0, vcpu->arch.shregs.srr1);\r\npr_err("sprg0 = %.16llx sprg1 = %.16llx\n",\r\nvcpu->arch.shregs.sprg0, vcpu->arch.shregs.sprg1);\r\npr_err("sprg2 = %.16llx sprg3 = %.16llx\n",\r\nvcpu->arch.shregs.sprg2, vcpu->arch.shregs.sprg3);\r\npr_err("cr = %.8x xer = %.16lx dsisr = %.8x\n",\r\nvcpu->arch.cr, vcpu->arch.xer, vcpu->arch.shregs.dsisr);\r\npr_err("dar = %.16llx\n", vcpu->arch.shregs.dar);\r\npr_err("fault dar = %.16lx dsisr = %.8x\n",\r\nvcpu->arch.fault_dar, vcpu->arch.fault_dsisr);\r\npr_err("SLB (%d entries):\n", vcpu->arch.slb_max);\r\nfor (r = 0; r < vcpu->arch.slb_max; ++r)\r\npr_err(" ESID = %.16llx VSID = %.16llx\n",\r\nvcpu->arch.slb[r].orige, vcpu->arch.slb[r].origv);\r\npr_err("lpcr = %.16lx sdr1 = %.16lx last_inst = %.8x\n",\r\nvcpu->kvm->arch.lpcr, vcpu->kvm->arch.sdr1,\r\nvcpu->arch.last_inst);\r\n}\r\nstruct kvm_vcpu *kvmppc_find_vcpu(struct kvm *kvm, int id)\r\n{\r\nint r;\r\nstruct kvm_vcpu *v, *ret = NULL;\r\nmutex_lock(&kvm->lock);\r\nkvm_for_each_vcpu(r, v, kvm) {\r\nif (v->vcpu_id == id) {\r\nret = v;\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&kvm->lock);\r\nreturn ret;\r\n}\r\nstatic void init_vpa(struct kvm_vcpu *vcpu, struct lppaca *vpa)\r\n{\r\nvpa->shared_proc = 1;\r\nvpa->yield_count = 1;\r\n}\r\nstatic unsigned long do_h_register_vpa(struct kvm_vcpu *vcpu,\r\nunsigned long flags,\r\nunsigned long vcpuid, unsigned long vpa)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nunsigned long pg_index, ra, len;\r\nunsigned long pg_offset;\r\nvoid *va;\r\nstruct kvm_vcpu *tvcpu;\r\ntvcpu = kvmppc_find_vcpu(kvm, vcpuid);\r\nif (!tvcpu)\r\nreturn H_PARAMETER;\r\nflags >>= 63 - 18;\r\nflags &= 7;\r\nif (flags == 0 || flags == 4)\r\nreturn H_PARAMETER;\r\nif (flags < 4) {\r\nif (vpa & 0x7f)\r\nreturn H_PARAMETER;\r\npg_index = vpa >> kvm->arch.ram_porder;\r\npg_offset = vpa & (kvm->arch.ram_psize - 1);\r\nif (pg_index >= kvm->arch.ram_npages)\r\nreturn H_PARAMETER;\r\nif (kvm->arch.ram_pginfo[pg_index].pfn == 0)\r\nreturn H_PARAMETER;\r\nra = kvm->arch.ram_pginfo[pg_index].pfn << PAGE_SHIFT;\r\nra |= pg_offset;\r\nva = __va(ra);\r\nif (flags <= 1)\r\nlen = *(unsigned short *)(va + 4);\r\nelse\r\nlen = *(unsigned int *)(va + 4);\r\nif (pg_offset + len > kvm->arch.ram_psize)\r\nreturn H_PARAMETER;\r\nswitch (flags) {\r\ncase 1:\r\nif (len < 640)\r\nreturn H_PARAMETER;\r\ntvcpu->arch.vpa = va;\r\ninit_vpa(vcpu, va);\r\nbreak;\r\ncase 2:\r\nif (len < 48)\r\nreturn H_PARAMETER;\r\nif (!tvcpu->arch.vpa)\r\nreturn H_RESOURCE;\r\nlen -= len % 48;\r\ntvcpu->arch.dtl = va;\r\ntvcpu->arch.dtl_end = va + len;\r\nbreak;\r\ncase 3:\r\nif (len < 8)\r\nreturn H_PARAMETER;\r\nif (!tvcpu->arch.vpa)\r\nreturn H_RESOURCE;\r\ntvcpu->arch.slb_shadow = va;\r\nlen = (len - 16) / 16;\r\ntvcpu->arch.slb_shadow = va;\r\nbreak;\r\n}\r\n} else {\r\nswitch (flags) {\r\ncase 5:\r\nif (tvcpu->arch.slb_shadow || tvcpu->arch.dtl)\r\nreturn H_RESOURCE;\r\ntvcpu->arch.vpa = NULL;\r\nbreak;\r\ncase 6:\r\ntvcpu->arch.dtl = NULL;\r\nbreak;\r\ncase 7:\r\ntvcpu->arch.slb_shadow = NULL;\r\nbreak;\r\n}\r\n}\r\nreturn H_SUCCESS;\r\n}\r\nint kvmppc_pseries_do_hcall(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long req = kvmppc_get_gpr(vcpu, 3);\r\nunsigned long target, ret = H_SUCCESS;\r\nstruct kvm_vcpu *tvcpu;\r\nswitch (req) {\r\ncase H_CEDE:\r\nbreak;\r\ncase H_PROD:\r\ntarget = kvmppc_get_gpr(vcpu, 4);\r\ntvcpu = kvmppc_find_vcpu(vcpu->kvm, target);\r\nif (!tvcpu) {\r\nret = H_PARAMETER;\r\nbreak;\r\n}\r\ntvcpu->arch.prodded = 1;\r\nsmp_mb();\r\nif (vcpu->arch.ceded) {\r\nif (waitqueue_active(&vcpu->wq)) {\r\nwake_up_interruptible(&vcpu->wq);\r\nvcpu->stat.halt_wakeup++;\r\n}\r\n}\r\nbreak;\r\ncase H_CONFER:\r\nbreak;\r\ncase H_REGISTER_VPA:\r\nret = do_h_register_vpa(vcpu, kvmppc_get_gpr(vcpu, 4),\r\nkvmppc_get_gpr(vcpu, 5),\r\nkvmppc_get_gpr(vcpu, 6));\r\nbreak;\r\ndefault:\r\nreturn RESUME_HOST;\r\n}\r\nkvmppc_set_gpr(vcpu, 3, ret);\r\nvcpu->arch.hcall_needed = 0;\r\nreturn RESUME_GUEST;\r\n}\r\nstatic int kvmppc_handle_exit(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nstruct task_struct *tsk)\r\n{\r\nint r = RESUME_HOST;\r\nvcpu->stat.sum_exits++;\r\nrun->exit_reason = KVM_EXIT_UNKNOWN;\r\nrun->ready_for_interrupt_injection = 1;\r\nswitch (vcpu->arch.trap) {\r\ncase BOOK3S_INTERRUPT_HV_DECREMENTER:\r\nvcpu->stat.dec_exits++;\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_EXTERNAL:\r\nvcpu->stat.ext_intr_exits++;\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_PERFMON:\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_PROGRAM:\r\n{\r\nulong flags;\r\nflags = vcpu->arch.shregs.msr & 0x1f0000ull;\r\nkvmppc_core_queue_program(vcpu, flags);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_SYSCALL:\r\n{\r\nint i;\r\nif (vcpu->arch.shregs.msr & MSR_PR) {\r\nkvmppc_book3s_queue_irqprio(vcpu, BOOK3S_INTERRUPT_SYSCALL);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\nrun->papr_hcall.nr = kvmppc_get_gpr(vcpu, 3);\r\nfor (i = 0; i < 9; ++i)\r\nrun->papr_hcall.args[i] = kvmppc_get_gpr(vcpu, 4 + i);\r\nrun->exit_reason = KVM_EXIT_PAPR_HCALL;\r\nvcpu->arch.hcall_needed = 1;\r\nr = RESUME_HOST;\r\nbreak;\r\n}\r\ncase BOOK3S_INTERRUPT_H_DATA_STORAGE:\r\nvcpu->arch.shregs.dsisr = vcpu->arch.fault_dsisr;\r\nvcpu->arch.shregs.dar = vcpu->arch.fault_dar;\r\nkvmppc_inject_interrupt(vcpu, BOOK3S_INTERRUPT_DATA_STORAGE, 0);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_H_INST_STORAGE:\r\nkvmppc_inject_interrupt(vcpu, BOOK3S_INTERRUPT_INST_STORAGE,\r\n0x08000000);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOK3S_INTERRUPT_H_EMUL_ASSIST:\r\nkvmppc_core_queue_program(vcpu, 0x80000);\r\nr = RESUME_GUEST;\r\nbreak;\r\ndefault:\r\nkvmppc_dump_regs(vcpu);\r\nprintk(KERN_EMERG "trap=0x%x | pc=0x%lx | msr=0x%llx\n",\r\nvcpu->arch.trap, kvmppc_get_pc(vcpu),\r\nvcpu->arch.shregs.msr);\r\nr = RESUME_HOST;\r\nBUG();\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nint i;\r\nsregs->pvr = vcpu->arch.pvr;\r\nmemset(sregs, 0, sizeof(struct kvm_sregs));\r\nfor (i = 0; i < vcpu->arch.slb_max; i++) {\r\nsregs->u.s.ppc64.slb[i].slbe = vcpu->arch.slb[i].orige;\r\nsregs->u.s.ppc64.slb[i].slbv = vcpu->arch.slb[i].origv;\r\n}\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nint i, j;\r\nkvmppc_set_pvr(vcpu, sregs->pvr);\r\nj = 0;\r\nfor (i = 0; i < vcpu->arch.slb_nr; i++) {\r\nif (sregs->u.s.ppc64.slb[i].slbe & SLB_ESID_V) {\r\nvcpu->arch.slb[j].orige = sregs->u.s.ppc64.slb[i].slbe;\r\nvcpu->arch.slb[j].origv = sregs->u.s.ppc64.slb[i].slbv;\r\n++j;\r\n}\r\n}\r\nvcpu->arch.slb_max = j;\r\nreturn 0;\r\n}\r\nint kvmppc_core_check_processor_compat(void)\r\n{\r\nif (cpu_has_feature(CPU_FTR_HVMODE))\r\nreturn 0;\r\nreturn -EIO;\r\n}\r\nstruct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm, unsigned int id)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nint err = -EINVAL;\r\nint core;\r\nstruct kvmppc_vcore *vcore;\r\ncore = id / threads_per_core;\r\nif (core >= KVM_MAX_VCORES)\r\ngoto out;\r\nerr = -ENOMEM;\r\nvcpu = kzalloc(sizeof(struct kvm_vcpu), GFP_KERNEL);\r\nif (!vcpu)\r\ngoto out;\r\nerr = kvm_vcpu_init(vcpu, kvm, id);\r\nif (err)\r\ngoto free_vcpu;\r\nvcpu->arch.shared = &vcpu->arch.shregs;\r\nvcpu->arch.last_cpu = -1;\r\nvcpu->arch.mmcr[0] = MMCR0_FC;\r\nvcpu->arch.ctrl = CTRL_RUNLATCH;\r\nvcpu->arch.pvr = mfspr(SPRN_PVR);\r\nkvmppc_set_pvr(vcpu, vcpu->arch.pvr);\r\nkvmppc_mmu_book3s_hv_init(vcpu);\r\nvcpu->arch.state = KVMPPC_VCPU_STOPPED;\r\ninit_waitqueue_head(&vcpu->arch.cpu_run);\r\nmutex_lock(&kvm->lock);\r\nvcore = kvm->arch.vcores[core];\r\nif (!vcore) {\r\nvcore = kzalloc(sizeof(struct kvmppc_vcore), GFP_KERNEL);\r\nif (vcore) {\r\nINIT_LIST_HEAD(&vcore->runnable_threads);\r\nspin_lock_init(&vcore->lock);\r\ninit_waitqueue_head(&vcore->wq);\r\n}\r\nkvm->arch.vcores[core] = vcore;\r\n}\r\nmutex_unlock(&kvm->lock);\r\nif (!vcore)\r\ngoto free_vcpu;\r\nspin_lock(&vcore->lock);\r\n++vcore->num_threads;\r\nspin_unlock(&vcore->lock);\r\nvcpu->arch.vcore = vcore;\r\nvcpu->arch.cpu_type = KVM_CPU_3S_64;\r\nkvmppc_sanity_check(vcpu);\r\nreturn vcpu;\r\nfree_vcpu:\r\nkfree(vcpu);\r\nout:\r\nreturn ERR_PTR(err);\r\n}\r\nvoid kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_vcpu_uninit(vcpu);\r\nkfree(vcpu);\r\n}\r\nstatic void kvmppc_set_timer(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long dec_nsec, now;\r\nnow = get_tb();\r\nif (now > vcpu->arch.dec_expires) {\r\nkvmppc_core_queue_dec(vcpu);\r\nkvmppc_core_deliver_interrupts(vcpu);\r\nreturn;\r\n}\r\ndec_nsec = (vcpu->arch.dec_expires - now) * NSEC_PER_SEC\r\n/ tb_ticks_per_sec;\r\nhrtimer_start(&vcpu->arch.dec_timer, ktime_set(0, dec_nsec),\r\nHRTIMER_MODE_REL);\r\nvcpu->arch.timer_running = 1;\r\n}\r\nstatic void kvmppc_end_cede(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->arch.ceded = 0;\r\nif (vcpu->arch.timer_running) {\r\nhrtimer_try_to_cancel(&vcpu->arch.dec_timer);\r\nvcpu->arch.timer_running = 0;\r\n}\r\n}\r\nstatic void kvmppc_remove_runnable(struct kvmppc_vcore *vc,\r\nstruct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_vcpu *v;\r\nif (vcpu->arch.state != KVMPPC_VCPU_RUNNABLE)\r\nreturn;\r\nvcpu->arch.state = KVMPPC_VCPU_BUSY_IN_HOST;\r\n--vc->n_runnable;\r\n++vc->n_busy;\r\nv = vcpu;\r\nlist_for_each_entry_continue(v, &vc->runnable_threads, arch.run_list)\r\n--v->arch.ptid;\r\nlist_del(&vcpu->arch.run_list);\r\n}\r\nstatic void kvmppc_start_thread(struct kvm_vcpu *vcpu)\r\n{\r\nint cpu;\r\nstruct paca_struct *tpaca;\r\nstruct kvmppc_vcore *vc = vcpu->arch.vcore;\r\nif (vcpu->arch.timer_running) {\r\nhrtimer_try_to_cancel(&vcpu->arch.dec_timer);\r\nvcpu->arch.timer_running = 0;\r\n}\r\ncpu = vc->pcpu + vcpu->arch.ptid;\r\ntpaca = &paca[cpu];\r\ntpaca->kvm_hstate.kvm_vcpu = vcpu;\r\ntpaca->kvm_hstate.kvm_vcore = vc;\r\ntpaca->kvm_hstate.napping = 0;\r\nvcpu->cpu = vc->pcpu;\r\nsmp_wmb();\r\n#if defined(CONFIG_PPC_ICP_NATIVE) && defined(CONFIG_SMP)\r\nif (vcpu->arch.ptid) {\r\ntpaca->cpu_start = 0x80;\r\nwmb();\r\nxics_wake_cpu(cpu);\r\n++vc->n_woken;\r\n}\r\n#endif\r\n}\r\nstatic void kvmppc_wait_for_nap(struct kvmppc_vcore *vc)\r\n{\r\nint i;\r\nHMT_low();\r\ni = 0;\r\nwhile (vc->nap_count < vc->n_woken) {\r\nif (++i >= 1000000) {\r\npr_err("kvmppc_wait_for_nap timeout %d %d\n",\r\nvc->nap_count, vc->n_woken);\r\nbreak;\r\n}\r\ncpu_relax();\r\n}\r\nHMT_medium();\r\n}\r\nstatic int on_primary_thread(void)\r\n{\r\nint cpu = smp_processor_id();\r\nint thr = cpu_thread_in_core(cpu);\r\nif (thr)\r\nreturn 0;\r\nwhile (++thr < threads_per_core)\r\nif (cpu_online(cpu + thr))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int kvmppc_run_core(struct kvmppc_vcore *vc)\r\n{\r\nstruct kvm_vcpu *vcpu, *vcpu0, *vnext;\r\nlong ret;\r\nu64 now;\r\nint ptid;\r\nlist_for_each_entry(vcpu, &vc->runnable_threads, arch.run_list)\r\nif (signal_pending(vcpu->arch.run_task))\r\nreturn 0;\r\nif (threads_per_core > 1 && !on_primary_thread()) {\r\nlist_for_each_entry(vcpu, &vc->runnable_threads, arch.run_list)\r\nvcpu->arch.ret = -EBUSY;\r\ngoto out;\r\n}\r\nptid = 0;\r\nvcpu0 = NULL;\r\nlist_for_each_entry(vcpu, &vc->runnable_threads, arch.run_list) {\r\nif (!vcpu->arch.ceded) {\r\nif (!ptid)\r\nvcpu0 = vcpu;\r\nvcpu->arch.ptid = ptid++;\r\n}\r\n}\r\nif (!vcpu0)\r\nreturn 0;\r\nlist_for_each_entry(vcpu, &vc->runnable_threads, arch.run_list)\r\nif (vcpu->arch.ceded)\r\nvcpu->arch.ptid = ptid++;\r\nvc->n_woken = 0;\r\nvc->nap_count = 0;\r\nvc->entry_exit_count = 0;\r\nvc->vcore_state = VCORE_RUNNING;\r\nvc->in_guest = 0;\r\nvc->pcpu = smp_processor_id();\r\nvc->napping_threads = 0;\r\nlist_for_each_entry(vcpu, &vc->runnable_threads, arch.run_list)\r\nkvmppc_start_thread(vcpu);\r\npreempt_disable();\r\nspin_unlock(&vc->lock);\r\nkvm_guest_enter();\r\n__kvmppc_vcore_entry(NULL, vcpu0);\r\nspin_lock(&vc->lock);\r\nlist_for_each_entry(vcpu, &vc->runnable_threads, arch.run_list)\r\nvcpu->cpu = -1;\r\nif (vc->nap_count < vc->n_woken)\r\nkvmppc_wait_for_nap(vc);\r\nvc->vcore_state = VCORE_EXITING;\r\nspin_unlock(&vc->lock);\r\nsmp_mb();\r\nkvm_guest_exit();\r\npreempt_enable();\r\nkvm_resched(vcpu);\r\nnow = get_tb();\r\nlist_for_each_entry(vcpu, &vc->runnable_threads, arch.run_list) {\r\nif (now < vcpu->arch.dec_expires &&\r\nkvmppc_core_pending_dec(vcpu))\r\nkvmppc_core_dequeue_dec(vcpu);\r\nret = RESUME_GUEST;\r\nif (vcpu->arch.trap)\r\nret = kvmppc_handle_exit(vcpu->arch.kvm_run, vcpu,\r\nvcpu->arch.run_task);\r\nvcpu->arch.ret = ret;\r\nvcpu->arch.trap = 0;\r\nif (vcpu->arch.ceded) {\r\nif (ret != RESUME_GUEST)\r\nkvmppc_end_cede(vcpu);\r\nelse\r\nkvmppc_set_timer(vcpu);\r\n}\r\n}\r\nspin_lock(&vc->lock);\r\nout:\r\nvc->vcore_state = VCORE_INACTIVE;\r\nlist_for_each_entry_safe(vcpu, vnext, &vc->runnable_threads,\r\narch.run_list) {\r\nif (vcpu->arch.ret != RESUME_GUEST) {\r\nkvmppc_remove_runnable(vc, vcpu);\r\nwake_up(&vcpu->arch.cpu_run);\r\n}\r\n}\r\nreturn 1;\r\n}\r\nstatic void kvmppc_wait_for_exec(struct kvm_vcpu *vcpu, int wait_state)\r\n{\r\nDEFINE_WAIT(wait);\r\nprepare_to_wait(&vcpu->arch.cpu_run, &wait, wait_state);\r\nif (vcpu->arch.state == KVMPPC_VCPU_RUNNABLE)\r\nschedule();\r\nfinish_wait(&vcpu->arch.cpu_run, &wait);\r\n}\r\nstatic void kvmppc_vcore_blocked(struct kvmppc_vcore *vc)\r\n{\r\nDEFINE_WAIT(wait);\r\nstruct kvm_vcpu *v;\r\nint all_idle = 1;\r\nprepare_to_wait(&vc->wq, &wait, TASK_INTERRUPTIBLE);\r\nvc->vcore_state = VCORE_SLEEPING;\r\nspin_unlock(&vc->lock);\r\nlist_for_each_entry(v, &vc->runnable_threads, arch.run_list) {\r\nif (!v->arch.ceded || v->arch.pending_exceptions) {\r\nall_idle = 0;\r\nbreak;\r\n}\r\n}\r\nif (all_idle)\r\nschedule();\r\nfinish_wait(&vc->wq, &wait);\r\nspin_lock(&vc->lock);\r\nvc->vcore_state = VCORE_INACTIVE;\r\n}\r\nstatic int kvmppc_run_vcpu(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)\r\n{\r\nint n_ceded;\r\nint prev_state;\r\nstruct kvmppc_vcore *vc;\r\nstruct kvm_vcpu *v, *vn;\r\nkvm_run->exit_reason = 0;\r\nvcpu->arch.ret = RESUME_GUEST;\r\nvcpu->arch.trap = 0;\r\nvc = vcpu->arch.vcore;\r\nspin_lock(&vc->lock);\r\nvcpu->arch.ceded = 0;\r\nvcpu->arch.run_task = current;\r\nvcpu->arch.kvm_run = kvm_run;\r\nprev_state = vcpu->arch.state;\r\nvcpu->arch.state = KVMPPC_VCPU_RUNNABLE;\r\nlist_add_tail(&vcpu->arch.run_list, &vc->runnable_threads);\r\n++vc->n_runnable;\r\nif (prev_state == KVMPPC_VCPU_STOPPED) {\r\nif (vc->vcore_state == VCORE_RUNNING &&\r\nVCORE_EXIT_COUNT(vc) == 0) {\r\nvcpu->arch.ptid = vc->n_runnable - 1;\r\nkvmppc_start_thread(vcpu);\r\n}\r\n} else if (prev_state == KVMPPC_VCPU_BUSY_IN_HOST)\r\n--vc->n_busy;\r\nwhile (vcpu->arch.state == KVMPPC_VCPU_RUNNABLE &&\r\n!signal_pending(current)) {\r\nif (vc->n_busy || vc->vcore_state != VCORE_INACTIVE) {\r\nspin_unlock(&vc->lock);\r\nkvmppc_wait_for_exec(vcpu, TASK_INTERRUPTIBLE);\r\nspin_lock(&vc->lock);\r\ncontinue;\r\n}\r\nn_ceded = 0;\r\nlist_for_each_entry(v, &vc->runnable_threads, arch.run_list)\r\nn_ceded += v->arch.ceded;\r\nif (n_ceded == vc->n_runnable)\r\nkvmppc_vcore_blocked(vc);\r\nelse\r\nkvmppc_run_core(vc);\r\nlist_for_each_entry_safe(v, vn, &vc->runnable_threads,\r\narch.run_list) {\r\nkvmppc_core_deliver_interrupts(v);\r\nif (signal_pending(v->arch.run_task)) {\r\nkvmppc_remove_runnable(vc, v);\r\nv->stat.signal_exits++;\r\nv->arch.kvm_run->exit_reason = KVM_EXIT_INTR;\r\nv->arch.ret = -EINTR;\r\nwake_up(&v->arch.cpu_run);\r\n}\r\n}\r\n}\r\nif (signal_pending(current)) {\r\nif (vc->vcore_state == VCORE_RUNNING ||\r\nvc->vcore_state == VCORE_EXITING) {\r\nspin_unlock(&vc->lock);\r\nkvmppc_wait_for_exec(vcpu, TASK_UNINTERRUPTIBLE);\r\nspin_lock(&vc->lock);\r\n}\r\nif (vcpu->arch.state == KVMPPC_VCPU_RUNNABLE) {\r\nkvmppc_remove_runnable(vc, vcpu);\r\nvcpu->stat.signal_exits++;\r\nkvm_run->exit_reason = KVM_EXIT_INTR;\r\nvcpu->arch.ret = -EINTR;\r\n}\r\n}\r\nspin_unlock(&vc->lock);\r\nreturn vcpu->arch.ret;\r\n}\r\nint kvmppc_vcpu_run(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nint r;\r\nif (!vcpu->arch.sane) {\r\nrun->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn -EINVAL;\r\n}\r\nif (signal_pending(current)) {\r\nrun->exit_reason = KVM_EXIT_INTR;\r\nreturn -EINTR;\r\n}\r\nif (!vcpu->kvm->arch.rma && cpu_has_feature(CPU_FTR_ARCH_201))\r\nreturn -EPERM;\r\nflush_fp_to_thread(current);\r\nflush_altivec_to_thread(current);\r\nflush_vsx_to_thread(current);\r\nvcpu->arch.wqp = &vcpu->arch.vcore->wq;\r\ndo {\r\nr = kvmppc_run_vcpu(run, vcpu);\r\nif (run->exit_reason == KVM_EXIT_PAPR_HCALL &&\r\n!(vcpu->arch.shregs.msr & MSR_PR)) {\r\nr = kvmppc_pseries_do_hcall(vcpu);\r\nkvmppc_core_deliver_interrupts(vcpu);\r\n}\r\n} while (r == RESUME_GUEST);\r\nreturn r;\r\n}\r\nstatic long kvmppc_stt_npages(unsigned long window_size)\r\n{\r\nreturn ALIGN((window_size >> SPAPR_TCE_SHIFT)\r\n* sizeof(u64), PAGE_SIZE) / PAGE_SIZE;\r\n}\r\nstatic void release_spapr_tce_table(struct kvmppc_spapr_tce_table *stt)\r\n{\r\nstruct kvm *kvm = stt->kvm;\r\nint i;\r\nmutex_lock(&kvm->lock);\r\nlist_del(&stt->list);\r\nfor (i = 0; i < kvmppc_stt_npages(stt->window_size); i++)\r\n__free_page(stt->pages[i]);\r\nkfree(stt);\r\nmutex_unlock(&kvm->lock);\r\nkvm_put_kvm(kvm);\r\n}\r\nstatic int kvm_spapr_tce_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt = vma->vm_file->private_data;\r\nstruct page *page;\r\nif (vmf->pgoff >= kvmppc_stt_npages(stt->window_size))\r\nreturn VM_FAULT_SIGBUS;\r\npage = stt->pages[vmf->pgoff];\r\nget_page(page);\r\nvmf->page = page;\r\nreturn 0;\r\n}\r\nstatic int kvm_spapr_tce_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nvma->vm_ops = &kvm_spapr_tce_vm_ops;\r\nreturn 0;\r\n}\r\nstatic int kvm_spapr_tce_release(struct inode *inode, struct file *filp)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt = filp->private_data;\r\nrelease_spapr_tce_table(stt);\r\nreturn 0;\r\n}\r\nlong kvm_vm_ioctl_create_spapr_tce(struct kvm *kvm,\r\nstruct kvm_create_spapr_tce *args)\r\n{\r\nstruct kvmppc_spapr_tce_table *stt = NULL;\r\nlong npages;\r\nint ret = -ENOMEM;\r\nint i;\r\nlist_for_each_entry(stt, &kvm->arch.spapr_tce_tables, list) {\r\nif (stt->liobn == args->liobn)\r\nreturn -EBUSY;\r\n}\r\nnpages = kvmppc_stt_npages(args->window_size);\r\nstt = kzalloc(sizeof(*stt) + npages* sizeof(struct page *),\r\nGFP_KERNEL);\r\nif (!stt)\r\ngoto fail;\r\nstt->liobn = args->liobn;\r\nstt->window_size = args->window_size;\r\nstt->kvm = kvm;\r\nfor (i = 0; i < npages; i++) {\r\nstt->pages[i] = alloc_page(GFP_KERNEL | __GFP_ZERO);\r\nif (!stt->pages[i])\r\ngoto fail;\r\n}\r\nkvm_get_kvm(kvm);\r\nmutex_lock(&kvm->lock);\r\nlist_add(&stt->list, &kvm->arch.spapr_tce_tables);\r\nmutex_unlock(&kvm->lock);\r\nreturn anon_inode_getfd("kvm-spapr-tce", &kvm_spapr_tce_fops,\r\nstt, O_RDWR);\r\nfail:\r\nif (stt) {\r\nfor (i = 0; i < npages; i++)\r\nif (stt->pages[i])\r\n__free_page(stt->pages[i]);\r\nkfree(stt);\r\n}\r\nreturn ret;\r\n}\r\nstatic inline int lpcr_rmls(unsigned long rma_size)\r\n{\r\nswitch (rma_size) {\r\ncase 32ul << 20:\r\nif (cpu_has_feature(CPU_FTR_ARCH_206))\r\nreturn 8;\r\nreturn -1;\r\ncase 64ul << 20:\r\nreturn 3;\r\ncase 128ul << 20:\r\nreturn 7;\r\ncase 256ul << 20:\r\nreturn 4;\r\ncase 1ul << 30:\r\nreturn 2;\r\ncase 16ul << 30:\r\nreturn 1;\r\ncase 256ul << 30:\r\nreturn 0;\r\ndefault:\r\nreturn -1;\r\n}\r\n}\r\nstatic int kvm_rma_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct kvmppc_rma_info *ri = vma->vm_file->private_data;\r\nstruct page *page;\r\nif (vmf->pgoff >= ri->npages)\r\nreturn VM_FAULT_SIGBUS;\r\npage = pfn_to_page(ri->base_pfn + vmf->pgoff);\r\nget_page(page);\r\nvmf->page = page;\r\nreturn 0;\r\n}\r\nstatic int kvm_rma_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nvma->vm_flags |= VM_RESERVED;\r\nvma->vm_ops = &kvm_rma_vm_ops;\r\nreturn 0;\r\n}\r\nstatic int kvm_rma_release(struct inode *inode, struct file *filp)\r\n{\r\nstruct kvmppc_rma_info *ri = filp->private_data;\r\nkvm_release_rma(ri);\r\nreturn 0;\r\n}\r\nlong kvm_vm_ioctl_allocate_rma(struct kvm *kvm, struct kvm_allocate_rma *ret)\r\n{\r\nstruct kvmppc_rma_info *ri;\r\nlong fd;\r\nri = kvm_alloc_rma();\r\nif (!ri)\r\nreturn -ENOMEM;\r\nfd = anon_inode_getfd("kvm-rma", &kvm_rma_fops, ri, O_RDWR);\r\nif (fd < 0)\r\nkvm_release_rma(ri);\r\nret->rma_size = ri->npages << PAGE_SHIFT;\r\nreturn fd;\r\n}\r\nstatic struct page *hva_to_page(unsigned long addr)\r\n{\r\nstruct page *page[1];\r\nint npages;\r\nmight_sleep();\r\nnpages = get_user_pages_fast(addr, 1, 1, page);\r\nif (unlikely(npages != 1))\r\nreturn 0;\r\nreturn page[0];\r\n}\r\nint kvmppc_core_prepare_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem)\r\n{\r\nunsigned long psize, porder;\r\nunsigned long i, npages, totalpages;\r\nunsigned long pg_ix;\r\nstruct kvmppc_pginfo *pginfo;\r\nunsigned long hva;\r\nstruct kvmppc_rma_info *ri = NULL;\r\nstruct page *page;\r\nporder = LARGE_PAGE_ORDER;\r\npsize = 1ul << porder;\r\nif ((mem->memory_size & (psize - 1)) ||\r\n(mem->guest_phys_addr & (psize - 1))) {\r\npr_err("bad memory_size=%llx @ %llx\n",\r\nmem->memory_size, mem->guest_phys_addr);\r\nreturn -EINVAL;\r\n}\r\nnpages = mem->memory_size >> porder;\r\ntotalpages = (mem->guest_phys_addr + mem->memory_size) >> porder;\r\nif (totalpages > (1ul << (MAX_MEM_ORDER - LARGE_PAGE_ORDER)))\r\nreturn -EINVAL;\r\nif (mem->guest_phys_addr == 0 && kvm->arch.rma)\r\nreturn -EINVAL;\r\nif (totalpages > kvm->arch.ram_npages)\r\nkvm->arch.ram_npages = totalpages;\r\nif (mem->guest_phys_addr == 0) {\r\nstruct vm_area_struct *vma;\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma(current->mm, mem->userspace_addr);\r\nif (vma && vma->vm_file &&\r\nvma->vm_file->f_op == &kvm_rma_fops &&\r\nmem->userspace_addr == vma->vm_start)\r\nri = vma->vm_file->private_data;\r\nup_read(&current->mm->mmap_sem);\r\nif (!ri && cpu_has_feature(CPU_FTR_ARCH_201)) {\r\npr_err("CPU requires an RMO\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (ri) {\r\nunsigned long rma_size;\r\nunsigned long lpcr;\r\nlong rmls;\r\nrma_size = ri->npages << PAGE_SHIFT;\r\nif (rma_size > mem->memory_size)\r\nrma_size = mem->memory_size;\r\nrmls = lpcr_rmls(rma_size);\r\nif (rmls < 0) {\r\npr_err("Can't use RMA of 0x%lx bytes\n", rma_size);\r\nreturn -EINVAL;\r\n}\r\natomic_inc(&ri->use_count);\r\nkvm->arch.rma = ri;\r\nkvm->arch.n_rma_pages = rma_size >> porder;\r\nlpcr = kvm->arch.lpcr;\r\nif (cpu_has_feature(CPU_FTR_ARCH_201)) {\r\nlpcr &= ~((1ul << HID4_RMLS0_SH) |\r\n(3ul << HID4_RMLS2_SH));\r\nlpcr |= ((rmls >> 2) << HID4_RMLS0_SH) |\r\n((rmls & 3) << HID4_RMLS2_SH);\r\nlpcr |= ((ri->base_pfn >> (26 - PAGE_SHIFT)) & 0xffff)\r\n<< HID4_RMOR_SH;\r\n} else {\r\nlpcr &= ~(LPCR_VPM0 | LPCR_VRMA_L);\r\nlpcr |= rmls << LPCR_RMLS_SH;\r\nkvm->arch.rmor = kvm->arch.rma->base_pfn << PAGE_SHIFT;\r\n}\r\nkvm->arch.lpcr = lpcr;\r\npr_info("Using RMO at %lx size %lx (LPCR = %lx)\n",\r\nri->base_pfn << PAGE_SHIFT, rma_size, lpcr);\r\n}\r\npg_ix = mem->guest_phys_addr >> porder;\r\npginfo = kvm->arch.ram_pginfo + pg_ix;\r\nfor (i = 0; i < npages; ++i, ++pg_ix) {\r\nif (ri && pg_ix < kvm->arch.n_rma_pages) {\r\npginfo[i].pfn = ri->base_pfn +\r\n(pg_ix << (porder - PAGE_SHIFT));\r\ncontinue;\r\n}\r\nhva = mem->userspace_addr + (i << porder);\r\npage = hva_to_page(hva);\r\nif (!page) {\r\npr_err("oops, no pfn for hva %lx\n", hva);\r\ngoto err;\r\n}\r\nif (!PageHead(page) ||\r\ncompound_order(page) != (LARGE_PAGE_ORDER - PAGE_SHIFT)) {\r\npr_err("page at %lx isn't 16MB (o=%d)\n",\r\nhva, compound_order(page));\r\ngoto err;\r\n}\r\npginfo[i].pfn = page_to_pfn(page);\r\n}\r\nreturn 0;\r\nerr:\r\nreturn -EINVAL;\r\n}\r\nvoid kvmppc_core_commit_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem)\r\n{\r\nif (mem->guest_phys_addr == 0 && mem->memory_size != 0 &&\r\n!kvm->arch.rma)\r\nkvmppc_map_vrma(kvm, mem);\r\n}\r\nint kvmppc_core_init_vm(struct kvm *kvm)\r\n{\r\nlong r;\r\nunsigned long npages = 1ul << (MAX_MEM_ORDER - LARGE_PAGE_ORDER);\r\nlong err = -ENOMEM;\r\nunsigned long lpcr;\r\nr = kvmppc_alloc_hpt(kvm);\r\nif (r)\r\nreturn r;\r\nINIT_LIST_HEAD(&kvm->arch.spapr_tce_tables);\r\nkvm->arch.ram_pginfo = kzalloc(npages * sizeof(struct kvmppc_pginfo),\r\nGFP_KERNEL);\r\nif (!kvm->arch.ram_pginfo) {\r\npr_err("kvmppc_core_init_vm: couldn't alloc %lu bytes\n",\r\nnpages * sizeof(struct kvmppc_pginfo));\r\ngoto out_free;\r\n}\r\nkvm->arch.ram_npages = 0;\r\nkvm->arch.ram_psize = 1ul << LARGE_PAGE_ORDER;\r\nkvm->arch.ram_porder = LARGE_PAGE_ORDER;\r\nkvm->arch.rma = NULL;\r\nkvm->arch.n_rma_pages = 0;\r\nkvm->arch.host_sdr1 = mfspr(SPRN_SDR1);\r\nif (cpu_has_feature(CPU_FTR_ARCH_201)) {\r\nunsigned long lpid = kvm->arch.lpid;\r\nkvm->arch.host_lpid = 0;\r\nkvm->arch.host_lpcr = lpcr = mfspr(SPRN_HID4);\r\nlpcr &= ~((3 << HID4_LPID1_SH) | (0xful << HID4_LPID5_SH));\r\nlpcr |= ((lpid >> 4) << HID4_LPID1_SH) |\r\n((lpid & 0xf) << HID4_LPID5_SH);\r\n} else {\r\nkvm->arch.host_lpid = mfspr(SPRN_LPID);\r\nkvm->arch.host_lpcr = lpcr = mfspr(SPRN_LPCR);\r\nlpcr &= LPCR_PECE | LPCR_LPES;\r\nlpcr |= (4UL << LPCR_DPFD_SH) | LPCR_HDICE |\r\nLPCR_VPM0 | LPCR_VRMA_L;\r\n}\r\nkvm->arch.lpcr = lpcr;\r\nreturn 0;\r\nout_free:\r\nkvmppc_free_hpt(kvm);\r\nreturn err;\r\n}\r\nvoid kvmppc_core_destroy_vm(struct kvm *kvm)\r\n{\r\nstruct kvmppc_pginfo *pginfo;\r\nunsigned long i;\r\nif (kvm->arch.ram_pginfo) {\r\npginfo = kvm->arch.ram_pginfo;\r\nkvm->arch.ram_pginfo = NULL;\r\nfor (i = kvm->arch.n_rma_pages; i < kvm->arch.ram_npages; ++i)\r\nif (pginfo[i].pfn)\r\nput_page(pfn_to_page(pginfo[i].pfn));\r\nkfree(pginfo);\r\n}\r\nif (kvm->arch.rma) {\r\nkvm_release_rma(kvm->arch.rma);\r\nkvm->arch.rma = NULL;\r\n}\r\nkvmppc_free_hpt(kvm);\r\nWARN_ON(!list_empty(&kvm->arch.spapr_tce_tables));\r\n}\r\nvoid kvmppc_mmu_pte_pflush(struct kvm_vcpu *vcpu, ulong pa_start, ulong pa_end)\r\n{\r\n}\r\nint kvmppc_core_emulate_op(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int inst, int *advance)\r\n{\r\nreturn EMULATE_FAIL;\r\n}\r\nint kvmppc_core_emulate_mtspr(struct kvm_vcpu *vcpu, int sprn, int rs)\r\n{\r\nreturn EMULATE_FAIL;\r\n}\r\nint kvmppc_core_emulate_mfspr(struct kvm_vcpu *vcpu, int sprn, int rt)\r\n{\r\nreturn EMULATE_FAIL;\r\n}\r\nstatic int kvmppc_book3s_hv_init(void)\r\n{\r\nint r;\r\nr = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);\r\nif (r)\r\nreturn r;\r\nr = kvmppc_mmu_hv_init();\r\nreturn r;\r\n}\r\nstatic void kvmppc_book3s_hv_exit(void)\r\n{\r\nkvm_exit();\r\n}
