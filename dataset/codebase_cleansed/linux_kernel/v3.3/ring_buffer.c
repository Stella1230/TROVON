static inline void\r\nhv_get_ringbuffer_availbytes(struct hv_ring_buffer_info *rbi,\r\nu32 *read, u32 *write)\r\n{\r\nu32 read_loc, write_loc;\r\nsmp_read_barrier_depends();\r\nread_loc = rbi->ring_buffer->read_index;\r\nwrite_loc = rbi->ring_buffer->write_index;\r\n*write = BYTES_AVAIL_TO_WRITE(read_loc, write_loc, rbi->ring_datasize);\r\n*read = rbi->ring_datasize - *write;\r\n}\r\nstatic inline u32\r\nhv_get_next_write_location(struct hv_ring_buffer_info *ring_info)\r\n{\r\nu32 next = ring_info->ring_buffer->write_index;\r\nreturn next;\r\n}\r\nstatic inline void\r\nhv_set_next_write_location(struct hv_ring_buffer_info *ring_info,\r\nu32 next_write_location)\r\n{\r\nring_info->ring_buffer->write_index = next_write_location;\r\n}\r\nstatic inline u32\r\nhv_get_next_read_location(struct hv_ring_buffer_info *ring_info)\r\n{\r\nu32 next = ring_info->ring_buffer->read_index;\r\nreturn next;\r\n}\r\nstatic inline u32\r\nhv_get_next_readlocation_withoffset(struct hv_ring_buffer_info *ring_info,\r\nu32 offset)\r\n{\r\nu32 next = ring_info->ring_buffer->read_index;\r\nnext += offset;\r\nnext %= ring_info->ring_datasize;\r\nreturn next;\r\n}\r\nstatic inline void\r\nhv_set_next_read_location(struct hv_ring_buffer_info *ring_info,\r\nu32 next_read_location)\r\n{\r\nring_info->ring_buffer->read_index = next_read_location;\r\n}\r\nstatic inline void *\r\nhv_get_ring_buffer(struct hv_ring_buffer_info *ring_info)\r\n{\r\nreturn (void *)ring_info->ring_buffer->buffer;\r\n}\r\nstatic inline u32\r\nhv_get_ring_buffersize(struct hv_ring_buffer_info *ring_info)\r\n{\r\nreturn ring_info->ring_datasize;\r\n}\r\nstatic inline u64\r\nhv_get_ring_bufferindices(struct hv_ring_buffer_info *ring_info)\r\n{\r\nreturn (u64)ring_info->ring_buffer->write_index << 32;\r\n}\r\nstatic u32 hv_copyfrom_ringbuffer(\r\nstruct hv_ring_buffer_info *ring_info,\r\nvoid *dest,\r\nu32 destlen,\r\nu32 start_read_offset)\r\n{\r\nvoid *ring_buffer = hv_get_ring_buffer(ring_info);\r\nu32 ring_buffer_size = hv_get_ring_buffersize(ring_info);\r\nu32 frag_len;\r\nif (destlen > ring_buffer_size - start_read_offset) {\r\nfrag_len = ring_buffer_size - start_read_offset;\r\nmemcpy(dest, ring_buffer + start_read_offset, frag_len);\r\nmemcpy(dest + frag_len, ring_buffer, destlen - frag_len);\r\n} else\r\nmemcpy(dest, ring_buffer + start_read_offset, destlen);\r\nstart_read_offset += destlen;\r\nstart_read_offset %= ring_buffer_size;\r\nreturn start_read_offset;\r\n}\r\nstatic u32 hv_copyto_ringbuffer(\r\nstruct hv_ring_buffer_info *ring_info,\r\nu32 start_write_offset,\r\nvoid *src,\r\nu32 srclen)\r\n{\r\nvoid *ring_buffer = hv_get_ring_buffer(ring_info);\r\nu32 ring_buffer_size = hv_get_ring_buffersize(ring_info);\r\nu32 frag_len;\r\nif (srclen > ring_buffer_size - start_write_offset) {\r\nfrag_len = ring_buffer_size - start_write_offset;\r\nmemcpy(ring_buffer + start_write_offset, src, frag_len);\r\nmemcpy(ring_buffer, src + frag_len, srclen - frag_len);\r\n} else\r\nmemcpy(ring_buffer + start_write_offset, src, srclen);\r\nstart_write_offset += srclen;\r\nstart_write_offset %= ring_buffer_size;\r\nreturn start_write_offset;\r\n}\r\nvoid hv_ringbuffer_get_debuginfo(struct hv_ring_buffer_info *ring_info,\r\nstruct hv_ring_buffer_debug_info *debug_info)\r\n{\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nif (ring_info->ring_buffer) {\r\nhv_get_ringbuffer_availbytes(ring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\ndebug_info->bytes_avail_toread = bytes_avail_toread;\r\ndebug_info->bytes_avail_towrite = bytes_avail_towrite;\r\ndebug_info->current_read_index =\r\nring_info->ring_buffer->read_index;\r\ndebug_info->current_write_index =\r\nring_info->ring_buffer->write_index;\r\ndebug_info->current_interrupt_mask =\r\nring_info->ring_buffer->interrupt_mask;\r\n}\r\n}\r\nu32 hv_get_ringbuffer_interrupt_mask(struct hv_ring_buffer_info *rbi)\r\n{\r\nreturn rbi->ring_buffer->interrupt_mask;\r\n}\r\nint hv_ringbuffer_init(struct hv_ring_buffer_info *ring_info,\r\nvoid *buffer, u32 buflen)\r\n{\r\nif (sizeof(struct hv_ring_buffer) != PAGE_SIZE)\r\nreturn -EINVAL;\r\nmemset(ring_info, 0, sizeof(struct hv_ring_buffer_info));\r\nring_info->ring_buffer = (struct hv_ring_buffer *)buffer;\r\nring_info->ring_buffer->read_index =\r\nring_info->ring_buffer->write_index = 0;\r\nring_info->ring_size = buflen;\r\nring_info->ring_datasize = buflen - sizeof(struct hv_ring_buffer);\r\nspin_lock_init(&ring_info->ring_lock);\r\nreturn 0;\r\n}\r\nvoid hv_ringbuffer_cleanup(struct hv_ring_buffer_info *ring_info)\r\n{\r\n}\r\nint hv_ringbuffer_write(struct hv_ring_buffer_info *outring_info,\r\nstruct scatterlist *sglist, u32 sgcount)\r\n{\r\nint i = 0;\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nu32 totalbytes_towrite = 0;\r\nstruct scatterlist *sg;\r\nu32 next_write_location;\r\nu64 prev_indices = 0;\r\nunsigned long flags;\r\nfor_each_sg(sglist, sg, sgcount, i)\r\n{\r\ntotalbytes_towrite += sg->length;\r\n}\r\ntotalbytes_towrite += sizeof(u64);\r\nspin_lock_irqsave(&outring_info->ring_lock, flags);\r\nhv_get_ringbuffer_availbytes(outring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\nif (bytes_avail_towrite <= totalbytes_towrite) {\r\nspin_unlock_irqrestore(&outring_info->ring_lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nnext_write_location = hv_get_next_write_location(outring_info);\r\nfor_each_sg(sglist, sg, sgcount, i)\r\n{\r\nnext_write_location = hv_copyto_ringbuffer(outring_info,\r\nnext_write_location,\r\nsg_virt(sg),\r\nsg->length);\r\n}\r\nprev_indices = hv_get_ring_bufferindices(outring_info);\r\nnext_write_location = hv_copyto_ringbuffer(outring_info,\r\nnext_write_location,\r\n&prev_indices,\r\nsizeof(u64));\r\nsmp_wmb();\r\nhv_set_next_write_location(outring_info, next_write_location);\r\nspin_unlock_irqrestore(&outring_info->ring_lock, flags);\r\nreturn 0;\r\n}\r\nint hv_ringbuffer_peek(struct hv_ring_buffer_info *Inring_info,\r\nvoid *Buffer, u32 buflen)\r\n{\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nu32 next_read_location = 0;\r\nunsigned long flags;\r\nspin_lock_irqsave(&Inring_info->ring_lock, flags);\r\nhv_get_ringbuffer_availbytes(Inring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\nif (bytes_avail_toread < buflen) {\r\nspin_unlock_irqrestore(&Inring_info->ring_lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nnext_read_location = hv_get_next_read_location(Inring_info);\r\nnext_read_location = hv_copyfrom_ringbuffer(Inring_info,\r\nBuffer,\r\nbuflen,\r\nnext_read_location);\r\nspin_unlock_irqrestore(&Inring_info->ring_lock, flags);\r\nreturn 0;\r\n}\r\nint hv_ringbuffer_read(struct hv_ring_buffer_info *inring_info, void *buffer,\r\nu32 buflen, u32 offset)\r\n{\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nu32 next_read_location = 0;\r\nu64 prev_indices = 0;\r\nunsigned long flags;\r\nif (buflen <= 0)\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&inring_info->ring_lock, flags);\r\nhv_get_ringbuffer_availbytes(inring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\nif (bytes_avail_toread < buflen) {\r\nspin_unlock_irqrestore(&inring_info->ring_lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nnext_read_location =\r\nhv_get_next_readlocation_withoffset(inring_info, offset);\r\nnext_read_location = hv_copyfrom_ringbuffer(inring_info,\r\nbuffer,\r\nbuflen,\r\nnext_read_location);\r\nnext_read_location = hv_copyfrom_ringbuffer(inring_info,\r\n&prev_indices,\r\nsizeof(u64),\r\nnext_read_location);\r\nsmp_mb();\r\nhv_set_next_read_location(inring_info, next_read_location);\r\nspin_unlock_irqrestore(&inring_info->ring_lock, flags);\r\nreturn 0;\r\n}
