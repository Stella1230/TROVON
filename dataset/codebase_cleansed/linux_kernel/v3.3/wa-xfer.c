static void wa_seg_init(struct wa_seg *seg)\r\n{\r\nkref_init(&seg->urb.kref);\r\n}\r\nstatic inline void wa_xfer_init(struct wa_xfer *xfer)\r\n{\r\nkref_init(&xfer->refcnt);\r\nINIT_LIST_HEAD(&xfer->list_node);\r\nspin_lock_init(&xfer->lock);\r\n}\r\nstatic void wa_xfer_destroy(struct kref *_xfer)\r\n{\r\nstruct wa_xfer *xfer = container_of(_xfer, struct wa_xfer, refcnt);\r\nif (xfer->seg) {\r\nunsigned cnt;\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nif (xfer->is_inbound)\r\nusb_put_urb(xfer->seg[cnt]->dto_urb);\r\nusb_put_urb(&xfer->seg[cnt]->urb);\r\n}\r\n}\r\nkfree(xfer);\r\n}\r\nstatic void wa_xfer_get(struct wa_xfer *xfer)\r\n{\r\nkref_get(&xfer->refcnt);\r\n}\r\nstatic void wa_xfer_put(struct wa_xfer *xfer)\r\n{\r\nkref_put(&xfer->refcnt, wa_xfer_destroy);\r\n}\r\nstatic void wa_xfer_giveback(struct wa_xfer *xfer)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&xfer->wa->xfer_list_lock, flags);\r\nlist_del_init(&xfer->list_node);\r\nspin_unlock_irqrestore(&xfer->wa->xfer_list_lock, flags);\r\nwusbhc_giveback_urb(xfer->wa->wusb, xfer->urb, xfer->result);\r\nwa_put(xfer->wa);\r\nwa_xfer_put(xfer);\r\n}\r\nstatic void wa_xfer_completion(struct wa_xfer *xfer)\r\n{\r\nif (xfer->wusb_dev)\r\nwusb_dev_put(xfer->wusb_dev);\r\nrpipe_put(xfer->ep->hcpriv);\r\nwa_xfer_giveback(xfer);\r\n}\r\nstatic unsigned __wa_xfer_is_done(struct wa_xfer *xfer)\r\n{\r\nstruct device *dev = &xfer->wa->usb_iface->dev;\r\nunsigned result, cnt;\r\nstruct wa_seg *seg;\r\nstruct urb *urb = xfer->urb;\r\nunsigned found_short = 0;\r\nresult = xfer->segs_done == xfer->segs_submitted;\r\nif (result == 0)\r\ngoto out;\r\nurb->actual_length = 0;\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nseg = xfer->seg[cnt];\r\nswitch (seg->status) {\r\ncase WA_SEG_DONE:\r\nif (found_short && seg->result > 0) {\r\ndev_dbg(dev, "xfer %p#%u: bad short segments (%zu)\n",\r\nxfer, cnt, seg->result);\r\nurb->status = -EINVAL;\r\ngoto out;\r\n}\r\nurb->actual_length += seg->result;\r\nif (seg->result < xfer->seg_size\r\n&& cnt != xfer->segs-1)\r\nfound_short = 1;\r\ndev_dbg(dev, "xfer %p#%u: DONE short %d "\r\n"result %zu urb->actual_length %d\n",\r\nxfer, seg->index, found_short, seg->result,\r\nurb->actual_length);\r\nbreak;\r\ncase WA_SEG_ERROR:\r\nxfer->result = seg->result;\r\ndev_dbg(dev, "xfer %p#%u: ERROR result %zu\n",\r\nxfer, seg->index, seg->result);\r\ngoto out;\r\ncase WA_SEG_ABORTED:\r\ndev_dbg(dev, "xfer %p#%u ABORTED: result %d\n",\r\nxfer, seg->index, urb->status);\r\nxfer->result = urb->status;\r\ngoto out;\r\ndefault:\r\ndev_warn(dev, "xfer %p#%u: is_done bad state %d\n",\r\nxfer, cnt, seg->status);\r\nxfer->result = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\nxfer->result = 0;\r\nout:\r\nreturn result;\r\n}\r\nstatic void wa_xfer_id_init(struct wa_xfer *xfer)\r\n{\r\nxfer->id = atomic_add_return(1, &xfer->wa->xfer_id_count);\r\n}\r\nstatic u32 wa_xfer_id(struct wa_xfer *xfer)\r\n{\r\nreturn xfer->id;\r\n}\r\nstatic struct wa_xfer *wa_xfer_get_by_id(struct wahc *wa, u32 id)\r\n{\r\nunsigned long flags;\r\nstruct wa_xfer *xfer_itr;\r\nspin_lock_irqsave(&wa->xfer_list_lock, flags);\r\nlist_for_each_entry(xfer_itr, &wa->xfer_list, list_node) {\r\nif (id == xfer_itr->id) {\r\nwa_xfer_get(xfer_itr);\r\ngoto out;\r\n}\r\n}\r\nxfer_itr = NULL;\r\nout:\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags);\r\nreturn xfer_itr;\r\n}\r\nstatic void __wa_xfer_abort_cb(struct urb *urb)\r\n{\r\nstruct wa_xfer_abort_buffer *b = urb->context;\r\nusb_put_urb(&b->urb);\r\n}\r\nstatic void __wa_xfer_abort(struct wa_xfer *xfer)\r\n{\r\nint result;\r\nstruct device *dev = &xfer->wa->usb_iface->dev;\r\nstruct wa_xfer_abort_buffer *b;\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nb = kmalloc(sizeof(*b), GFP_ATOMIC);\r\nif (b == NULL)\r\ngoto error_kmalloc;\r\nb->cmd.bLength = sizeof(b->cmd);\r\nb->cmd.bRequestType = WA_XFER_ABORT;\r\nb->cmd.wRPipe = rpipe->descr.wRPipeIndex;\r\nb->cmd.dwTransferID = wa_xfer_id(xfer);\r\nusb_init_urb(&b->urb);\r\nusb_fill_bulk_urb(&b->urb, xfer->wa->usb_dev,\r\nusb_sndbulkpipe(xfer->wa->usb_dev,\r\nxfer->wa->dto_epd->bEndpointAddress),\r\n&b->cmd, sizeof(b->cmd), __wa_xfer_abort_cb, b);\r\nresult = usb_submit_urb(&b->urb, GFP_ATOMIC);\r\nif (result < 0)\r\ngoto error_submit;\r\nreturn;\r\nerror_submit:\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p: Can't submit abort request: %d\n",\r\nxfer, result);\r\nkfree(b);\r\nerror_kmalloc:\r\nreturn;\r\n}\r\nstatic ssize_t __wa_xfer_setup_sizes(struct wa_xfer *xfer,\r\nenum wa_xfer_type *pxfer_type)\r\n{\r\nssize_t result;\r\nstruct device *dev = &xfer->wa->usb_iface->dev;\r\nsize_t maxpktsize;\r\nstruct urb *urb = xfer->urb;\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nswitch (rpipe->descr.bmAttribute & 0x3) {\r\ncase USB_ENDPOINT_XFER_CONTROL:\r\n*pxfer_type = WA_XFER_TYPE_CTL;\r\nresult = sizeof(struct wa_xfer_ctl);\r\nbreak;\r\ncase USB_ENDPOINT_XFER_INT:\r\ncase USB_ENDPOINT_XFER_BULK:\r\n*pxfer_type = WA_XFER_TYPE_BI;\r\nresult = sizeof(struct wa_xfer_bi);\r\nbreak;\r\ncase USB_ENDPOINT_XFER_ISOC:\r\ndev_err(dev, "FIXME: ISOC not implemented\n");\r\nresult = -ENOSYS;\r\ngoto error;\r\ndefault:\r\nBUG();\r\nresult = -EINVAL;\r\n};\r\nxfer->is_inbound = urb->pipe & USB_DIR_IN ? 1 : 0;\r\nxfer->is_dma = urb->transfer_flags & URB_NO_TRANSFER_DMA_MAP ? 1 : 0;\r\nxfer->seg_size = le16_to_cpu(rpipe->descr.wBlocks)\r\n* 1 << (xfer->wa->wa_descr->bRPipeBlockSize - 1);\r\nmaxpktsize = le16_to_cpu(rpipe->descr.wMaxPacketSize);\r\nif (xfer->seg_size < maxpktsize) {\r\ndev_err(dev, "HW BUG? seg_size %zu smaller than maxpktsize "\r\n"%zu\n", xfer->seg_size, maxpktsize);\r\nresult = -EINVAL;\r\ngoto error;\r\n}\r\nxfer->seg_size = (xfer->seg_size / maxpktsize) * maxpktsize;\r\nxfer->segs = (urb->transfer_buffer_length + xfer->seg_size - 1)\r\n/ xfer->seg_size;\r\nif (xfer->segs >= WA_SEGS_MAX) {\r\ndev_err(dev, "BUG? ops, number of segments %d bigger than %d\n",\r\n(int)(urb->transfer_buffer_length / xfer->seg_size),\r\nWA_SEGS_MAX);\r\nresult = -EINVAL;\r\ngoto error;\r\n}\r\nif (xfer->segs == 0 && *pxfer_type == WA_XFER_TYPE_CTL)\r\nxfer->segs = 1;\r\nerror:\r\nreturn result;\r\n}\r\nstatic void __wa_xfer_setup_hdr0(struct wa_xfer *xfer,\r\nstruct wa_xfer_hdr *xfer_hdr0,\r\nenum wa_xfer_type xfer_type,\r\nsize_t xfer_hdr_size)\r\n{\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nxfer_hdr0 = &xfer->seg[0]->xfer_hdr;\r\nxfer_hdr0->bLength = xfer_hdr_size;\r\nxfer_hdr0->bRequestType = xfer_type;\r\nxfer_hdr0->wRPipe = rpipe->descr.wRPipeIndex;\r\nxfer_hdr0->dwTransferID = wa_xfer_id(xfer);\r\nxfer_hdr0->bTransferSegment = 0;\r\nswitch (xfer_type) {\r\ncase WA_XFER_TYPE_CTL: {\r\nstruct wa_xfer_ctl *xfer_ctl =\r\ncontainer_of(xfer_hdr0, struct wa_xfer_ctl, hdr);\r\nxfer_ctl->bmAttribute = xfer->is_inbound ? 1 : 0;\r\nmemcpy(&xfer_ctl->baSetupData, xfer->urb->setup_packet,\r\nsizeof(xfer_ctl->baSetupData));\r\nbreak;\r\n}\r\ncase WA_XFER_TYPE_BI:\r\nbreak;\r\ncase WA_XFER_TYPE_ISO:\r\nprintk(KERN_ERR "FIXME: ISOC not implemented\n");\r\ndefault:\r\nBUG();\r\n};\r\n}\r\nstatic void wa_seg_dto_cb(struct urb *urb)\r\n{\r\nstruct wa_seg *seg = urb->context;\r\nstruct wa_xfer *xfer = seg->xfer;\r\nstruct wahc *wa;\r\nstruct device *dev;\r\nstruct wa_rpipe *rpipe;\r\nunsigned long flags;\r\nunsigned rpipe_ready = 0;\r\nu8 done = 0;\r\nswitch (urb->status) {\r\ncase 0:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\ndev_dbg(dev, "xfer %p#%u: data out done (%d bytes)\n",\r\nxfer, seg->index, urb->actual_length);\r\nif (seg->status < WA_SEG_PENDING)\r\nseg->status = WA_SEG_PENDING;\r\nseg->result = urb->actual_length;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nbreak;\r\ncase -ECONNRESET:\r\ncase -ENOENT:\r\nbreak;\r\ndefault:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\nrpipe = xfer->ep->hcpriv;\r\ndev_dbg(dev, "xfer %p#%u: data out error %d\n",\r\nxfer, seg->index, urb->status);\r\nif (edc_inc(&wa->nep_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)){\r\ndev_err(dev, "DTO: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nif (seg->status != WA_SEG_ERROR) {\r\nseg->status = WA_SEG_ERROR;\r\nseg->result = urb->status;\r\nxfer->segs_done++;\r\n__wa_xfer_abort(xfer);\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_is_done(xfer);\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\n}\r\n}\r\nstatic void wa_seg_cb(struct urb *urb)\r\n{\r\nstruct wa_seg *seg = urb->context;\r\nstruct wa_xfer *xfer = seg->xfer;\r\nstruct wahc *wa;\r\nstruct device *dev;\r\nstruct wa_rpipe *rpipe;\r\nunsigned long flags;\r\nunsigned rpipe_ready;\r\nu8 done = 0;\r\nswitch (urb->status) {\r\ncase 0:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\ndev_dbg(dev, "xfer %p#%u: request done\n", xfer, seg->index);\r\nif (xfer->is_inbound && seg->status < WA_SEG_PENDING)\r\nseg->status = WA_SEG_PENDING;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nbreak;\r\ncase -ECONNRESET:\r\ncase -ENOENT:\r\nbreak;\r\ndefault:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\nrpipe = xfer->ep->hcpriv;\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p#%u: request error %d\n",\r\nxfer, seg->index, urb->status);\r\nif (edc_inc(&wa->nep_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)){\r\ndev_err(dev, "DTO: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nusb_unlink_urb(seg->dto_urb);\r\nseg->status = WA_SEG_ERROR;\r\nseg->result = urb->status;\r\nxfer->segs_done++;\r\n__wa_xfer_abort(xfer);\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_is_done(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\n}\r\n}\r\nstatic int __wa_xfer_setup_segs(struct wa_xfer *xfer, size_t xfer_hdr_size)\r\n{\r\nint result, cnt;\r\nsize_t alloc_size = sizeof(*xfer->seg[0])\r\n- sizeof(xfer->seg[0]->xfer_hdr) + xfer_hdr_size;\r\nstruct usb_device *usb_dev = xfer->wa->usb_dev;\r\nconst struct usb_endpoint_descriptor *dto_epd = xfer->wa->dto_epd;\r\nstruct wa_seg *seg;\r\nsize_t buf_itr, buf_size, buf_itr_size;\r\nresult = -ENOMEM;\r\nxfer->seg = kcalloc(xfer->segs, sizeof(xfer->seg[0]), GFP_ATOMIC);\r\nif (xfer->seg == NULL)\r\ngoto error_segs_kzalloc;\r\nbuf_itr = 0;\r\nbuf_size = xfer->urb->transfer_buffer_length;\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nseg = xfer->seg[cnt] = kzalloc(alloc_size, GFP_ATOMIC);\r\nif (seg == NULL)\r\ngoto error_seg_kzalloc;\r\nwa_seg_init(seg);\r\nseg->xfer = xfer;\r\nseg->index = cnt;\r\nusb_fill_bulk_urb(&seg->urb, usb_dev,\r\nusb_sndbulkpipe(usb_dev,\r\ndto_epd->bEndpointAddress),\r\n&seg->xfer_hdr, xfer_hdr_size,\r\nwa_seg_cb, seg);\r\nbuf_itr_size = buf_size > xfer->seg_size ?\r\nxfer->seg_size : buf_size;\r\nif (xfer->is_inbound == 0 && buf_size > 0) {\r\nseg->dto_urb = usb_alloc_urb(0, GFP_ATOMIC);\r\nif (seg->dto_urb == NULL)\r\ngoto error_dto_alloc;\r\nusb_fill_bulk_urb(\r\nseg->dto_urb, usb_dev,\r\nusb_sndbulkpipe(usb_dev,\r\ndto_epd->bEndpointAddress),\r\nNULL, 0, wa_seg_dto_cb, seg);\r\nif (xfer->is_dma) {\r\nseg->dto_urb->transfer_dma =\r\nxfer->urb->transfer_dma + buf_itr;\r\nseg->dto_urb->transfer_flags |=\r\nURB_NO_TRANSFER_DMA_MAP;\r\n} else\r\nseg->dto_urb->transfer_buffer =\r\nxfer->urb->transfer_buffer + buf_itr;\r\nseg->dto_urb->transfer_buffer_length = buf_itr_size;\r\n}\r\nseg->status = WA_SEG_READY;\r\nbuf_itr += buf_itr_size;\r\nbuf_size -= buf_itr_size;\r\n}\r\nreturn 0;\r\nerror_dto_alloc:\r\nkfree(xfer->seg[cnt]);\r\ncnt--;\r\nerror_seg_kzalloc:\r\nfor (; cnt > 0; cnt--) {\r\nif (xfer->is_inbound == 0)\r\nkfree(xfer->seg[cnt]->dto_urb);\r\nkfree(xfer->seg[cnt]);\r\n}\r\nerror_segs_kzalloc:\r\nreturn result;\r\n}\r\nstatic int __wa_xfer_setup(struct wa_xfer *xfer, struct urb *urb)\r\n{\r\nint result;\r\nstruct device *dev = &xfer->wa->usb_iface->dev;\r\nenum wa_xfer_type xfer_type = 0;\r\nsize_t xfer_hdr_size, cnt, transfer_size;\r\nstruct wa_xfer_hdr *xfer_hdr0, *xfer_hdr;\r\nresult = __wa_xfer_setup_sizes(xfer, &xfer_type);\r\nif (result < 0)\r\ngoto error_setup_sizes;\r\nxfer_hdr_size = result;\r\nresult = __wa_xfer_setup_segs(xfer, xfer_hdr_size);\r\nif (result < 0) {\r\ndev_err(dev, "xfer %p: Failed to allocate %d segments: %d\n",\r\nxfer, xfer->segs, result);\r\ngoto error_setup_segs;\r\n}\r\nxfer_hdr0 = &xfer->seg[0]->xfer_hdr;\r\nwa_xfer_id_init(xfer);\r\n__wa_xfer_setup_hdr0(xfer, xfer_hdr0, xfer_type, xfer_hdr_size);\r\nxfer_hdr = xfer_hdr0;\r\ntransfer_size = urb->transfer_buffer_length;\r\nxfer_hdr0->dwTransferLength = transfer_size > xfer->seg_size ?\r\nxfer->seg_size : transfer_size;\r\ntransfer_size -= xfer->seg_size;\r\nfor (cnt = 1; cnt < xfer->segs; cnt++) {\r\nxfer_hdr = &xfer->seg[cnt]->xfer_hdr;\r\nmemcpy(xfer_hdr, xfer_hdr0, xfer_hdr_size);\r\nxfer_hdr->bTransferSegment = cnt;\r\nxfer_hdr->dwTransferLength = transfer_size > xfer->seg_size ?\r\ncpu_to_le32(xfer->seg_size)\r\n: cpu_to_le32(transfer_size);\r\nxfer->seg[cnt]->status = WA_SEG_READY;\r\ntransfer_size -= xfer->seg_size;\r\n}\r\nxfer_hdr->bTransferSegment |= 0x80;\r\nresult = 0;\r\nerror_setup_segs:\r\nerror_setup_sizes:\r\nreturn result;\r\n}\r\nstatic int __wa_seg_submit(struct wa_rpipe *rpipe, struct wa_xfer *xfer,\r\nstruct wa_seg *seg)\r\n{\r\nint result;\r\nresult = usb_submit_urb(&seg->urb, GFP_ATOMIC);\r\nif (result < 0) {\r\nprintk(KERN_ERR "xfer %p#%u: REQ submit failed: %d\n",\r\nxfer, seg->index, result);\r\ngoto error_seg_submit;\r\n}\r\nif (seg->dto_urb) {\r\nresult = usb_submit_urb(seg->dto_urb, GFP_ATOMIC);\r\nif (result < 0) {\r\nprintk(KERN_ERR "xfer %p#%u: DTO submit failed: %d\n",\r\nxfer, seg->index, result);\r\ngoto error_dto_submit;\r\n}\r\n}\r\nseg->status = WA_SEG_SUBMITTED;\r\nrpipe_avail_dec(rpipe);\r\nreturn 0;\r\nerror_dto_submit:\r\nusb_unlink_urb(&seg->urb);\r\nerror_seg_submit:\r\nseg->status = WA_SEG_ERROR;\r\nseg->result = result;\r\nreturn result;\r\n}\r\nstatic void wa_xfer_delayed_run(struct wa_rpipe *rpipe)\r\n{\r\nint result;\r\nstruct device *dev = &rpipe->wa->usb_iface->dev;\r\nstruct wa_seg *seg;\r\nstruct wa_xfer *xfer;\r\nunsigned long flags;\r\nspin_lock_irqsave(&rpipe->seg_lock, flags);\r\nwhile (atomic_read(&rpipe->segs_available) > 0\r\n&& !list_empty(&rpipe->seg_list)) {\r\nseg = list_entry(rpipe->seg_list.next, struct wa_seg,\r\nlist_node);\r\nlist_del(&seg->list_node);\r\nxfer = seg->xfer;\r\nresult = __wa_seg_submit(rpipe, xfer, seg);\r\ndev_dbg(dev, "xfer %p#%u submitted from delayed [%d segments available] %d\n",\r\nxfer, seg->index, atomic_read(&rpipe->segs_available), result);\r\nif (unlikely(result < 0)) {\r\nspin_unlock_irqrestore(&rpipe->seg_lock, flags);\r\nspin_lock_irqsave(&xfer->lock, flags);\r\n__wa_xfer_abort(xfer);\r\nxfer->segs_done++;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nspin_lock_irqsave(&rpipe->seg_lock, flags);\r\n}\r\n}\r\nspin_unlock_irqrestore(&rpipe->seg_lock, flags);\r\n}\r\nstatic int __wa_xfer_submit(struct wa_xfer *xfer)\r\n{\r\nint result;\r\nstruct wahc *wa = xfer->wa;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nunsigned cnt;\r\nstruct wa_seg *seg;\r\nunsigned long flags;\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nsize_t maxrequests = le16_to_cpu(rpipe->descr.wRequests);\r\nu8 available;\r\nu8 empty;\r\nspin_lock_irqsave(&wa->xfer_list_lock, flags);\r\nlist_add_tail(&xfer->list_node, &wa->xfer_list);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags);\r\nBUG_ON(atomic_read(&rpipe->segs_available) > maxrequests);\r\nresult = 0;\r\nspin_lock_irqsave(&rpipe->seg_lock, flags);\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\navailable = atomic_read(&rpipe->segs_available);\r\nempty = list_empty(&rpipe->seg_list);\r\nseg = xfer->seg[cnt];\r\ndev_dbg(dev, "xfer %p#%u: available %u empty %u (%s)\n",\r\nxfer, cnt, available, empty,\r\navailable == 0 || !empty ? "delayed" : "submitted");\r\nif (available == 0 || !empty) {\r\ndev_dbg(dev, "xfer %p#%u: delayed\n", xfer, cnt);\r\nseg->status = WA_SEG_DELAYED;\r\nlist_add_tail(&seg->list_node, &rpipe->seg_list);\r\n} else {\r\nresult = __wa_seg_submit(rpipe, xfer, seg);\r\nif (result < 0) {\r\n__wa_xfer_abort(xfer);\r\ngoto error_seg_submit;\r\n}\r\n}\r\nxfer->segs_submitted++;\r\n}\r\nerror_seg_submit:\r\nspin_unlock_irqrestore(&rpipe->seg_lock, flags);\r\nreturn result;\r\n}\r\nstatic void wa_urb_enqueue_b(struct wa_xfer *xfer)\r\n{\r\nint result;\r\nunsigned long flags;\r\nstruct urb *urb = xfer->urb;\r\nstruct wahc *wa = xfer->wa;\r\nstruct wusbhc *wusbhc = wa->wusb;\r\nstruct wusb_dev *wusb_dev;\r\nunsigned done;\r\nresult = rpipe_get_by_ep(wa, xfer->ep, urb, xfer->gfp);\r\nif (result < 0)\r\ngoto error_rpipe_get;\r\nresult = -ENODEV;\r\nmutex_lock(&wusbhc->mutex);\r\nif (urb->dev == NULL) {\r\nmutex_unlock(&wusbhc->mutex);\r\ngoto error_dev_gone;\r\n}\r\nwusb_dev = __wusb_dev_get_by_usb_dev(wusbhc, urb->dev);\r\nif (wusb_dev == NULL) {\r\nmutex_unlock(&wusbhc->mutex);\r\ngoto error_dev_gone;\r\n}\r\nmutex_unlock(&wusbhc->mutex);\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nxfer->wusb_dev = wusb_dev;\r\nresult = urb->status;\r\nif (urb->status != -EINPROGRESS)\r\ngoto error_dequeued;\r\nresult = __wa_xfer_setup(xfer, urb);\r\nif (result < 0)\r\ngoto error_xfer_setup;\r\nresult = __wa_xfer_submit(xfer);\r\nif (result < 0)\r\ngoto error_xfer_submit;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nreturn;\r\nerror_xfer_setup:\r\nerror_dequeued:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (wusb_dev)\r\nwusb_dev_put(wusb_dev);\r\nerror_dev_gone:\r\nrpipe_put(xfer->ep->hcpriv);\r\nerror_rpipe_get:\r\nxfer->result = result;\r\nwa_xfer_giveback(xfer);\r\nreturn;\r\nerror_xfer_submit:\r\ndone = __wa_xfer_is_done(xfer);\r\nxfer->result = result;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\n}\r\nvoid wa_urb_enqueue_run(struct work_struct *ws)\r\n{\r\nstruct wahc *wa = container_of(ws, struct wahc, xfer_work);\r\nstruct wa_xfer *xfer, *next;\r\nstruct urb *urb;\r\nspin_lock_irq(&wa->xfer_list_lock);\r\nlist_for_each_entry_safe(xfer, next, &wa->xfer_delayed_list,\r\nlist_node) {\r\nlist_del_init(&xfer->list_node);\r\nspin_unlock_irq(&wa->xfer_list_lock);\r\nurb = xfer->urb;\r\nwa_urb_enqueue_b(xfer);\r\nusb_put_urb(urb);\r\nspin_lock_irq(&wa->xfer_list_lock);\r\n}\r\nspin_unlock_irq(&wa->xfer_list_lock);\r\n}\r\nint wa_urb_enqueue(struct wahc *wa, struct usb_host_endpoint *ep,\r\nstruct urb *urb, gfp_t gfp)\r\n{\r\nint result;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nstruct wa_xfer *xfer;\r\nunsigned long my_flags;\r\nunsigned cant_sleep = irqs_disabled() | in_atomic();\r\nif (urb->transfer_buffer == NULL\r\n&& !(urb->transfer_flags & URB_NO_TRANSFER_DMA_MAP)\r\n&& urb->transfer_buffer_length != 0) {\r\ndev_err(dev, "BUG? urb %p: NULL xfer buffer & NODMA\n", urb);\r\ndump_stack();\r\n}\r\nresult = -ENOMEM;\r\nxfer = kzalloc(sizeof(*xfer), gfp);\r\nif (xfer == NULL)\r\ngoto error_kmalloc;\r\nresult = -ENOENT;\r\nif (urb->status != -EINPROGRESS)\r\ngoto error_dequeued;\r\nwa_xfer_init(xfer);\r\nxfer->wa = wa_get(wa);\r\nxfer->urb = urb;\r\nxfer->gfp = gfp;\r\nxfer->ep = ep;\r\nurb->hcpriv = xfer;\r\ndev_dbg(dev, "xfer %p urb %p pipe 0x%02x [%d bytes] %s %s %s\n",\r\nxfer, urb, urb->pipe, urb->transfer_buffer_length,\r\nurb->transfer_flags & URB_NO_TRANSFER_DMA_MAP ? "dma" : "nodma",\r\nurb->pipe & USB_DIR_IN ? "inbound" : "outbound",\r\ncant_sleep ? "deferred" : "inline");\r\nif (cant_sleep) {\r\nusb_get_urb(urb);\r\nspin_lock_irqsave(&wa->xfer_list_lock, my_flags);\r\nlist_add_tail(&xfer->list_node, &wa->xfer_delayed_list);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, my_flags);\r\nqueue_work(wusbd, &wa->xfer_work);\r\n} else {\r\nwa_urb_enqueue_b(xfer);\r\n}\r\nreturn 0;\r\nerror_dequeued:\r\nkfree(xfer);\r\nerror_kmalloc:\r\nreturn result;\r\n}\r\nint wa_urb_dequeue(struct wahc *wa, struct urb *urb)\r\n{\r\nunsigned long flags, flags2;\r\nstruct wa_xfer *xfer;\r\nstruct wa_seg *seg;\r\nstruct wa_rpipe *rpipe;\r\nunsigned cnt;\r\nunsigned rpipe_ready = 0;\r\nxfer = urb->hcpriv;\r\nif (xfer == NULL) {\r\nBUG_ON(urb->status == -EINPROGRESS);\r\ngoto out;\r\n}\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nrpipe = xfer->ep->hcpriv;\r\nspin_lock_irqsave(&wa->xfer_list_lock, flags2);\r\nif (!list_empty(&xfer->list_node) && xfer->seg == NULL)\r\ngoto dequeue_delayed;\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags2);\r\nif (xfer->seg == NULL)\r\ngoto out_unlock;\r\n__wa_xfer_abort(xfer);\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nseg = xfer->seg[cnt];\r\nswitch (seg->status) {\r\ncase WA_SEG_NOTREADY:\r\ncase WA_SEG_READY:\r\nprintk(KERN_ERR "xfer %p#%u: dequeue bad state %u\n",\r\nxfer, cnt, seg->status);\r\nWARN_ON(1);\r\nbreak;\r\ncase WA_SEG_DELAYED:\r\nseg->status = WA_SEG_ABORTED;\r\nspin_lock_irqsave(&rpipe->seg_lock, flags2);\r\nlist_del(&seg->list_node);\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\nspin_unlock_irqrestore(&rpipe->seg_lock, flags2);\r\nbreak;\r\ncase WA_SEG_SUBMITTED:\r\nseg->status = WA_SEG_ABORTED;\r\nusb_unlink_urb(&seg->urb);\r\nif (xfer->is_inbound == 0)\r\nusb_unlink_urb(seg->dto_urb);\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\nbreak;\r\ncase WA_SEG_PENDING:\r\nseg->status = WA_SEG_ABORTED;\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\nbreak;\r\ncase WA_SEG_DTI_PENDING:\r\nusb_unlink_urb(wa->dti_urb);\r\nseg->status = WA_SEG_ABORTED;\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\nbreak;\r\ncase WA_SEG_DONE:\r\ncase WA_SEG_ERROR:\r\ncase WA_SEG_ABORTED:\r\nbreak;\r\n}\r\n}\r\nxfer->result = urb->status;\r\n__wa_xfer_is_done(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nreturn 0;\r\nout_unlock:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nout:\r\nreturn 0;\r\ndequeue_delayed:\r\nlist_del_init(&xfer->list_node);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags2);\r\nxfer->result = urb->status;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nwa_xfer_giveback(xfer);\r\nusb_put_urb(urb);\r\nreturn 0;\r\n}\r\nstatic int wa_xfer_status_to_errno(u8 status)\r\n{\r\nint errno;\r\nu8 real_status = status;\r\nstatic int xlat[] = {\r\n[WA_XFER_STATUS_SUCCESS] = 0,\r\n[WA_XFER_STATUS_HALTED] = -EPIPE,\r\n[WA_XFER_STATUS_DATA_BUFFER_ERROR] = -ENOBUFS,\r\n[WA_XFER_STATUS_BABBLE] = -EOVERFLOW,\r\n[WA_XFER_RESERVED] = EINVAL,\r\n[WA_XFER_STATUS_NOT_FOUND] = 0,\r\n[WA_XFER_STATUS_INSUFFICIENT_RESOURCE] = -ENOMEM,\r\n[WA_XFER_STATUS_TRANSACTION_ERROR] = -EILSEQ,\r\n[WA_XFER_STATUS_ABORTED] = -EINTR,\r\n[WA_XFER_STATUS_RPIPE_NOT_READY] = EINVAL,\r\n[WA_XFER_INVALID_FORMAT] = EINVAL,\r\n[WA_XFER_UNEXPECTED_SEGMENT_NUMBER] = EINVAL,\r\n[WA_XFER_STATUS_RPIPE_TYPE_MISMATCH] = EINVAL,\r\n};\r\nstatus &= 0x3f;\r\nif (status == 0)\r\nreturn 0;\r\nif (status >= ARRAY_SIZE(xlat)) {\r\nprintk_ratelimited(KERN_ERR "%s(): BUG? "\r\n"Unknown WA transfer status 0x%02x\n",\r\n__func__, real_status);\r\nreturn -EINVAL;\r\n}\r\nerrno = xlat[status];\r\nif (unlikely(errno > 0)) {\r\nprintk_ratelimited(KERN_ERR "%s(): BUG? "\r\n"Inconsistent WA status: 0x%02x\n",\r\n__func__, real_status);\r\nerrno = -errno;\r\n}\r\nreturn errno;\r\n}\r\nstatic void wa_xfer_result_chew(struct wahc *wa, struct wa_xfer *xfer)\r\n{\r\nint result;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nunsigned long flags;\r\nu8 seg_idx;\r\nstruct wa_seg *seg;\r\nstruct wa_rpipe *rpipe;\r\nstruct wa_xfer_result *xfer_result = wa->xfer_result;\r\nu8 done = 0;\r\nu8 usb_status;\r\nunsigned rpipe_ready = 0;\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nseg_idx = xfer_result->bTransferSegment & 0x7f;\r\nif (unlikely(seg_idx >= xfer->segs))\r\ngoto error_bad_seg;\r\nseg = xfer->seg[seg_idx];\r\nrpipe = xfer->ep->hcpriv;\r\nusb_status = xfer_result->bTransferStatus;\r\ndev_dbg(dev, "xfer %p#%u: bTransferStatus 0x%02x (seg %u)\n",\r\nxfer, seg_idx, usb_status, seg->status);\r\nif (seg->status == WA_SEG_ABORTED\r\n|| seg->status == WA_SEG_ERROR)\r\ngoto segment_aborted;\r\nif (seg->status == WA_SEG_SUBMITTED)\r\nseg->status = WA_SEG_PENDING;\r\nif (seg->status != WA_SEG_PENDING) {\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p#%u: Bad segment state %u\n",\r\nxfer, seg_idx, seg->status);\r\nseg->status = WA_SEG_PENDING;\r\n}\r\nif (usb_status & 0x80) {\r\nseg->result = wa_xfer_status_to_errno(usb_status);\r\ndev_err(dev, "DTI: xfer %p#%u failed (0x%02x)\n",\r\nxfer, seg->index, usb_status);\r\ngoto error_complete;\r\n}\r\nif (usb_status & 0x40)\r\nusb_status = 0;\r\nif (xfer->is_inbound) {\r\nseg->status = WA_SEG_DTI_PENDING;\r\nBUG_ON(wa->buf_in_urb->status == -EINPROGRESS);\r\nif (xfer->is_dma) {\r\nwa->buf_in_urb->transfer_dma =\r\nxfer->urb->transfer_dma\r\n+ seg_idx * xfer->seg_size;\r\nwa->buf_in_urb->transfer_flags\r\n|= URB_NO_TRANSFER_DMA_MAP;\r\n} else {\r\nwa->buf_in_urb->transfer_buffer =\r\nxfer->urb->transfer_buffer\r\n+ seg_idx * xfer->seg_size;\r\nwa->buf_in_urb->transfer_flags\r\n&= ~URB_NO_TRANSFER_DMA_MAP;\r\n}\r\nwa->buf_in_urb->transfer_buffer_length =\r\nle32_to_cpu(xfer_result->dwTransferLength);\r\nwa->buf_in_urb->context = seg;\r\nresult = usb_submit_urb(wa->buf_in_urb, GFP_ATOMIC);\r\nif (result < 0)\r\ngoto error_submit_buf_in;\r\n} else {\r\nseg->status = WA_SEG_DONE;\r\nseg->result = le32_to_cpu(xfer_result->dwTransferLength);\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_is_done(xfer);\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nreturn;\r\nerror_submit_buf_in:\r\nif (edc_inc(&wa->dti_edc, EDC_MAX_ERRORS, EDC_ERROR_TIMEFRAME)) {\r\ndev_err(dev, "DTI: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p#%u: can't submit DTI data phase: %d\n",\r\nxfer, seg_idx, result);\r\nseg->result = result;\r\nerror_complete:\r\nseg->status = WA_SEG_ERROR;\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\n__wa_xfer_abort(xfer);\r\ndone = __wa_xfer_is_done(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nreturn;\r\nerror_bad_seg:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nwa_urb_dequeue(wa, xfer->urb);\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p#%u: bad segment\n", xfer, seg_idx);\r\nif (edc_inc(&wa->dti_edc, EDC_MAX_ERRORS, EDC_ERROR_TIMEFRAME)) {\r\ndev_err(dev, "DTI: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nreturn;\r\nsegment_aborted:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\n}\r\nstatic void wa_buf_in_cb(struct urb *urb)\r\n{\r\nstruct wa_seg *seg = urb->context;\r\nstruct wa_xfer *xfer = seg->xfer;\r\nstruct wahc *wa;\r\nstruct device *dev;\r\nstruct wa_rpipe *rpipe;\r\nunsigned rpipe_ready;\r\nunsigned long flags;\r\nu8 done = 0;\r\nswitch (urb->status) {\r\ncase 0:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\nrpipe = xfer->ep->hcpriv;\r\ndev_dbg(dev, "xfer %p#%u: data in done (%zu bytes)\n",\r\nxfer, seg->index, (size_t)urb->actual_length);\r\nseg->status = WA_SEG_DONE;\r\nseg->result = urb->actual_length;\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_is_done(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nbreak;\r\ncase -ECONNRESET:\r\ncase -ENOENT:\r\nbreak;\r\ndefault:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\nrpipe = xfer->ep->hcpriv;\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p#%u: data in error %d\n",\r\nxfer, seg->index, urb->status);\r\nif (edc_inc(&wa->nep_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)){\r\ndev_err(dev, "DTO: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nseg->status = WA_SEG_ERROR;\r\nseg->result = urb->status;\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\n__wa_xfer_abort(xfer);\r\ndone = __wa_xfer_is_done(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\n}\r\n}\r\nstatic void wa_xfer_result_cb(struct urb *urb)\r\n{\r\nint result;\r\nstruct wahc *wa = urb->context;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nstruct wa_xfer_result *xfer_result;\r\nu32 xfer_id;\r\nstruct wa_xfer *xfer;\r\nu8 usb_status;\r\nBUG_ON(wa->dti_urb != urb);\r\nswitch (wa->dti_urb->status) {\r\ncase 0:\r\ndev_dbg(dev, "DTI: xfer result %d bytes at %p\n",\r\nurb->actual_length, urb->transfer_buffer);\r\nif (wa->dti_urb->actual_length != sizeof(*xfer_result)) {\r\ndev_err(dev, "DTI Error: xfer result--bad size "\r\n"xfer result (%d bytes vs %zu needed)\n",\r\nurb->actual_length, sizeof(*xfer_result));\r\nbreak;\r\n}\r\nxfer_result = wa->xfer_result;\r\nif (xfer_result->hdr.bLength != sizeof(*xfer_result)) {\r\ndev_err(dev, "DTI Error: xfer result--"\r\n"bad header length %u\n",\r\nxfer_result->hdr.bLength);\r\nbreak;\r\n}\r\nif (xfer_result->hdr.bNotifyType != WA_XFER_RESULT) {\r\ndev_err(dev, "DTI Error: xfer result--"\r\n"bad header type 0x%02x\n",\r\nxfer_result->hdr.bNotifyType);\r\nbreak;\r\n}\r\nusb_status = xfer_result->bTransferStatus & 0x3f;\r\nif (usb_status == WA_XFER_STATUS_ABORTED\r\n|| usb_status == WA_XFER_STATUS_NOT_FOUND)\r\nbreak;\r\nxfer_id = xfer_result->dwTransferID;\r\nxfer = wa_xfer_get_by_id(wa, xfer_id);\r\nif (xfer == NULL) {\r\ndev_err(dev, "DTI Error: xfer result--"\r\n"unknown xfer 0x%08x (status 0x%02x)\n",\r\nxfer_id, usb_status);\r\nbreak;\r\n}\r\nwa_xfer_result_chew(wa, xfer);\r\nwa_xfer_put(xfer);\r\nbreak;\r\ncase -ENOENT:\r\ncase -ESHUTDOWN:\r\ndev_dbg(dev, "DTI: going down! %d\n", urb->status);\r\ngoto out;\r\ndefault:\r\nif (edc_inc(&wa->dti_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)) {\r\ndev_err(dev, "DTI: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\ngoto out;\r\n}\r\nif (printk_ratelimit())\r\ndev_err(dev, "DTI: URB error %d\n", urb->status);\r\nbreak;\r\n}\r\nresult = usb_submit_urb(wa->dti_urb, GFP_ATOMIC);\r\nif (result < 0) {\r\ndev_err(dev, "DTI Error: Could not submit DTI URB (%d), "\r\n"resetting\n", result);\r\nwa_reset_all(wa);\r\n}\r\nout:\r\nreturn;\r\n}\r\nvoid wa_handle_notif_xfer(struct wahc *wa, struct wa_notif_hdr *notif_hdr)\r\n{\r\nint result;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nstruct wa_notif_xfer *notif_xfer;\r\nconst struct usb_endpoint_descriptor *dti_epd = wa->dti_epd;\r\nnotif_xfer = container_of(notif_hdr, struct wa_notif_xfer, hdr);\r\nBUG_ON(notif_hdr->bNotifyType != WA_NOTIF_TRANSFER);\r\nif ((0x80 | notif_xfer->bEndpoint) != dti_epd->bEndpointAddress) {\r\ndev_err(dev, "BUG: DTI ep is %u, not %u (hack me)\n",\r\nnotif_xfer->bEndpoint, dti_epd->bEndpointAddress);\r\ngoto error;\r\n}\r\nif (wa->dti_urb != NULL)\r\ngoto out;\r\nwa->dti_urb = usb_alloc_urb(0, GFP_KERNEL);\r\nif (wa->dti_urb == NULL) {\r\ndev_err(dev, "Can't allocate DTI URB\n");\r\ngoto error_dti_urb_alloc;\r\n}\r\nusb_fill_bulk_urb(\r\nwa->dti_urb, wa->usb_dev,\r\nusb_rcvbulkpipe(wa->usb_dev, 0x80 | notif_xfer->bEndpoint),\r\nwa->xfer_result, wa->xfer_result_size,\r\nwa_xfer_result_cb, wa);\r\nwa->buf_in_urb = usb_alloc_urb(0, GFP_KERNEL);\r\nif (wa->buf_in_urb == NULL) {\r\ndev_err(dev, "Can't allocate BUF-IN URB\n");\r\ngoto error_buf_in_urb_alloc;\r\n}\r\nusb_fill_bulk_urb(\r\nwa->buf_in_urb, wa->usb_dev,\r\nusb_rcvbulkpipe(wa->usb_dev, 0x80 | notif_xfer->bEndpoint),\r\nNULL, 0, wa_buf_in_cb, wa);\r\nresult = usb_submit_urb(wa->dti_urb, GFP_KERNEL);\r\nif (result < 0) {\r\ndev_err(dev, "DTI Error: Could not submit DTI URB (%d), "\r\n"resetting\n", result);\r\ngoto error_dti_urb_submit;\r\n}\r\nout:\r\nreturn;\r\nerror_dti_urb_submit:\r\nusb_put_urb(wa->buf_in_urb);\r\nerror_buf_in_urb_alloc:\r\nusb_put_urb(wa->dti_urb);\r\nwa->dti_urb = NULL;\r\nerror_dti_urb_alloc:\r\nerror:\r\nwa_reset_all(wa);\r\n}
