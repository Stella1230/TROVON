static inline int hrtimer_clockid_to_base(clockid_t clock_id)\r\n{\r\nreturn hrtimer_clock_to_base_table[clock_id];\r\n}\r\nstatic void hrtimer_get_softirq_time(struct hrtimer_cpu_base *base)\r\n{\r\nktime_t xtim, mono, boot;\r\nstruct timespec xts, tom, slp;\r\nget_xtime_and_monotonic_and_sleep_offset(&xts, &tom, &slp);\r\nxtim = timespec_to_ktime(xts);\r\nmono = ktime_add(xtim, timespec_to_ktime(tom));\r\nboot = ktime_add(mono, timespec_to_ktime(slp));\r\nbase->clock_base[HRTIMER_BASE_REALTIME].softirq_time = xtim;\r\nbase->clock_base[HRTIMER_BASE_MONOTONIC].softirq_time = mono;\r\nbase->clock_base[HRTIMER_BASE_BOOTTIME].softirq_time = boot;\r\n}\r\nstatic\r\nstruct hrtimer_clock_base *lock_hrtimer_base(const struct hrtimer *timer,\r\nunsigned long *flags)\r\n{\r\nstruct hrtimer_clock_base *base;\r\nfor (;;) {\r\nbase = timer->base;\r\nif (likely(base != NULL)) {\r\nraw_spin_lock_irqsave(&base->cpu_base->lock, *flags);\r\nif (likely(base == timer->base))\r\nreturn base;\r\nraw_spin_unlock_irqrestore(&base->cpu_base->lock, *flags);\r\n}\r\ncpu_relax();\r\n}\r\n}\r\nstatic int hrtimer_get_target(int this_cpu, int pinned)\r\n{\r\n#ifdef CONFIG_NO_HZ\r\nif (!pinned && get_sysctl_timer_migration() && idle_cpu(this_cpu))\r\nreturn get_nohz_timer_target();\r\n#endif\r\nreturn this_cpu;\r\n}\r\nstatic int\r\nhrtimer_check_target(struct hrtimer *timer, struct hrtimer_clock_base *new_base)\r\n{\r\n#ifdef CONFIG_HIGH_RES_TIMERS\r\nktime_t expires;\r\nif (!new_base->cpu_base->hres_active)\r\nreturn 0;\r\nexpires = ktime_sub(hrtimer_get_expires(timer), new_base->offset);\r\nreturn expires.tv64 <= new_base->cpu_base->expires_next.tv64;\r\n#else\r\nreturn 0;\r\n#endif\r\n}\r\nstatic inline struct hrtimer_clock_base *\r\nswitch_hrtimer_base(struct hrtimer *timer, struct hrtimer_clock_base *base,\r\nint pinned)\r\n{\r\nstruct hrtimer_clock_base *new_base;\r\nstruct hrtimer_cpu_base *new_cpu_base;\r\nint this_cpu = smp_processor_id();\r\nint cpu = hrtimer_get_target(this_cpu, pinned);\r\nint basenum = base->index;\r\nagain:\r\nnew_cpu_base = &per_cpu(hrtimer_bases, cpu);\r\nnew_base = &new_cpu_base->clock_base[basenum];\r\nif (base != new_base) {\r\nif (unlikely(hrtimer_callback_running(timer)))\r\nreturn base;\r\ntimer->base = NULL;\r\nraw_spin_unlock(&base->cpu_base->lock);\r\nraw_spin_lock(&new_base->cpu_base->lock);\r\nif (cpu != this_cpu && hrtimer_check_target(timer, new_base)) {\r\ncpu = this_cpu;\r\nraw_spin_unlock(&new_base->cpu_base->lock);\r\nraw_spin_lock(&base->cpu_base->lock);\r\ntimer->base = base;\r\ngoto again;\r\n}\r\ntimer->base = new_base;\r\n}\r\nreturn new_base;\r\n}\r\nstatic inline struct hrtimer_clock_base *\r\nlock_hrtimer_base(const struct hrtimer *timer, unsigned long *flags)\r\n{\r\nstruct hrtimer_clock_base *base = timer->base;\r\nraw_spin_lock_irqsave(&base->cpu_base->lock, *flags);\r\nreturn base;\r\n}\r\nktime_t ktime_add_ns(const ktime_t kt, u64 nsec)\r\n{\r\nktime_t tmp;\r\nif (likely(nsec < NSEC_PER_SEC)) {\r\ntmp.tv64 = nsec;\r\n} else {\r\nunsigned long rem = do_div(nsec, NSEC_PER_SEC);\r\ntmp = ktime_set((long)nsec, rem);\r\n}\r\nreturn ktime_add(kt, tmp);\r\n}\r\nktime_t ktime_sub_ns(const ktime_t kt, u64 nsec)\r\n{\r\nktime_t tmp;\r\nif (likely(nsec < NSEC_PER_SEC)) {\r\ntmp.tv64 = nsec;\r\n} else {\r\nunsigned long rem = do_div(nsec, NSEC_PER_SEC);\r\ntmp = ktime_set((long)nsec, rem);\r\n}\r\nreturn ktime_sub(kt, tmp);\r\n}\r\nu64 ktime_divns(const ktime_t kt, s64 div)\r\n{\r\nu64 dclc;\r\nint sft = 0;\r\ndclc = ktime_to_ns(kt);\r\nwhile (div >> 32) {\r\nsft++;\r\ndiv >>= 1;\r\n}\r\ndclc >>= sft;\r\ndo_div(dclc, (unsigned long) div);\r\nreturn dclc;\r\n}\r\nktime_t ktime_add_safe(const ktime_t lhs, const ktime_t rhs)\r\n{\r\nktime_t res = ktime_add(lhs, rhs);\r\nif (res.tv64 < 0 || res.tv64 < lhs.tv64 || res.tv64 < rhs.tv64)\r\nres = ktime_set(KTIME_SEC_MAX, 0);\r\nreturn res;\r\n}\r\nstatic void *hrtimer_debug_hint(void *addr)\r\n{\r\nreturn ((struct hrtimer *) addr)->function;\r\n}\r\nstatic int hrtimer_fixup_init(void *addr, enum debug_obj_state state)\r\n{\r\nstruct hrtimer *timer = addr;\r\nswitch (state) {\r\ncase ODEBUG_STATE_ACTIVE:\r\nhrtimer_cancel(timer);\r\ndebug_object_init(timer, &hrtimer_debug_descr);\r\nreturn 1;\r\ndefault:\r\nreturn 0;\r\n}\r\n}\r\nstatic int hrtimer_fixup_activate(void *addr, enum debug_obj_state state)\r\n{\r\nswitch (state) {\r\ncase ODEBUG_STATE_NOTAVAILABLE:\r\nWARN_ON_ONCE(1);\r\nreturn 0;\r\ncase ODEBUG_STATE_ACTIVE:\r\nWARN_ON(1);\r\ndefault:\r\nreturn 0;\r\n}\r\n}\r\nstatic int hrtimer_fixup_free(void *addr, enum debug_obj_state state)\r\n{\r\nstruct hrtimer *timer = addr;\r\nswitch (state) {\r\ncase ODEBUG_STATE_ACTIVE:\r\nhrtimer_cancel(timer);\r\ndebug_object_free(timer, &hrtimer_debug_descr);\r\nreturn 1;\r\ndefault:\r\nreturn 0;\r\n}\r\n}\r\nstatic inline void debug_hrtimer_init(struct hrtimer *timer)\r\n{\r\ndebug_object_init(timer, &hrtimer_debug_descr);\r\n}\r\nstatic inline void debug_hrtimer_activate(struct hrtimer *timer)\r\n{\r\ndebug_object_activate(timer, &hrtimer_debug_descr);\r\n}\r\nstatic inline void debug_hrtimer_deactivate(struct hrtimer *timer)\r\n{\r\ndebug_object_deactivate(timer, &hrtimer_debug_descr);\r\n}\r\nstatic inline void debug_hrtimer_free(struct hrtimer *timer)\r\n{\r\ndebug_object_free(timer, &hrtimer_debug_descr);\r\n}\r\nvoid hrtimer_init_on_stack(struct hrtimer *timer, clockid_t clock_id,\r\nenum hrtimer_mode mode)\r\n{\r\ndebug_object_init_on_stack(timer, &hrtimer_debug_descr);\r\n__hrtimer_init(timer, clock_id, mode);\r\n}\r\nvoid destroy_hrtimer_on_stack(struct hrtimer *timer)\r\n{\r\ndebug_object_free(timer, &hrtimer_debug_descr);\r\n}\r\nstatic inline void debug_hrtimer_init(struct hrtimer *timer) { }\r\nstatic inline void debug_hrtimer_activate(struct hrtimer *timer) { }\r\nstatic inline void debug_hrtimer_deactivate(struct hrtimer *timer) { }\r\nstatic inline void\r\ndebug_init(struct hrtimer *timer, clockid_t clockid,\r\nenum hrtimer_mode mode)\r\n{\r\ndebug_hrtimer_init(timer);\r\ntrace_hrtimer_init(timer, clockid, mode);\r\n}\r\nstatic inline void debug_activate(struct hrtimer *timer)\r\n{\r\ndebug_hrtimer_activate(timer);\r\ntrace_hrtimer_start(timer);\r\n}\r\nstatic inline void debug_deactivate(struct hrtimer *timer)\r\n{\r\ndebug_hrtimer_deactivate(timer);\r\ntrace_hrtimer_cancel(timer);\r\n}\r\nstatic int __init setup_hrtimer_hres(char *str)\r\n{\r\nif (!strcmp(str, "off"))\r\nhrtimer_hres_enabled = 0;\r\nelse if (!strcmp(str, "on"))\r\nhrtimer_hres_enabled = 1;\r\nelse\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic inline int hrtimer_is_hres_enabled(void)\r\n{\r\nreturn hrtimer_hres_enabled;\r\n}\r\nstatic inline int hrtimer_hres_active(void)\r\n{\r\nreturn __this_cpu_read(hrtimer_bases.hres_active);\r\n}\r\nstatic void\r\nhrtimer_force_reprogram(struct hrtimer_cpu_base *cpu_base, int skip_equal)\r\n{\r\nint i;\r\nstruct hrtimer_clock_base *base = cpu_base->clock_base;\r\nktime_t expires, expires_next;\r\nexpires_next.tv64 = KTIME_MAX;\r\nfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++, base++) {\r\nstruct hrtimer *timer;\r\nstruct timerqueue_node *next;\r\nnext = timerqueue_getnext(&base->active);\r\nif (!next)\r\ncontinue;\r\ntimer = container_of(next, struct hrtimer, node);\r\nexpires = ktime_sub(hrtimer_get_expires(timer), base->offset);\r\nif (expires.tv64 < 0)\r\nexpires.tv64 = 0;\r\nif (expires.tv64 < expires_next.tv64)\r\nexpires_next = expires;\r\n}\r\nif (skip_equal && expires_next.tv64 == cpu_base->expires_next.tv64)\r\nreturn;\r\ncpu_base->expires_next.tv64 = expires_next.tv64;\r\nif (cpu_base->expires_next.tv64 != KTIME_MAX)\r\ntick_program_event(cpu_base->expires_next, 1);\r\n}\r\nstatic int hrtimer_reprogram(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = &__get_cpu_var(hrtimer_bases);\r\nktime_t expires = ktime_sub(hrtimer_get_expires(timer), base->offset);\r\nint res;\r\nWARN_ON_ONCE(hrtimer_get_expires_tv64(timer) < 0);\r\nif (hrtimer_callback_running(timer))\r\nreturn 0;\r\nif (expires.tv64 < 0)\r\nreturn -ETIME;\r\nif (expires.tv64 >= cpu_base->expires_next.tv64)\r\nreturn 0;\r\nif (cpu_base->hang_detected)\r\nreturn 0;\r\nres = tick_program_event(expires, 0);\r\nif (!IS_ERR_VALUE(res))\r\ncpu_base->expires_next = expires;\r\nreturn res;\r\n}\r\nstatic inline void hrtimer_init_hres(struct hrtimer_cpu_base *base)\r\n{\r\nbase->expires_next.tv64 = KTIME_MAX;\r\nbase->hres_active = 0;\r\n}\r\nstatic inline int hrtimer_enqueue_reprogram(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base,\r\nint wakeup)\r\n{\r\nif (base->cpu_base->hres_active && hrtimer_reprogram(timer, base)) {\r\nif (wakeup) {\r\nraw_spin_unlock(&base->cpu_base->lock);\r\nraise_softirq_irqoff(HRTIMER_SOFTIRQ);\r\nraw_spin_lock(&base->cpu_base->lock);\r\n} else\r\n__raise_softirq_irqoff(HRTIMER_SOFTIRQ);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void retrigger_next_event(void *arg)\r\n{\r\nstruct hrtimer_cpu_base *base = &__get_cpu_var(hrtimer_bases);\r\nstruct timespec realtime_offset, xtim, wtm, sleep;\r\nif (!hrtimer_hres_active())\r\nreturn;\r\nget_xtime_and_monotonic_and_sleep_offset(&xtim, &wtm, &sleep);\r\nset_normalized_timespec(&realtime_offset, -wtm.tv_sec, -wtm.tv_nsec);\r\nraw_spin_lock(&base->lock);\r\nbase->clock_base[HRTIMER_BASE_REALTIME].offset =\r\ntimespec_to_ktime(realtime_offset);\r\nbase->clock_base[HRTIMER_BASE_BOOTTIME].offset =\r\ntimespec_to_ktime(sleep);\r\nhrtimer_force_reprogram(base, 0);\r\nraw_spin_unlock(&base->lock);\r\n}\r\nstatic int hrtimer_switch_to_hres(void)\r\n{\r\nint i, cpu = smp_processor_id();\r\nstruct hrtimer_cpu_base *base = &per_cpu(hrtimer_bases, cpu);\r\nunsigned long flags;\r\nif (base->hres_active)\r\nreturn 1;\r\nlocal_irq_save(flags);\r\nif (tick_init_highres()) {\r\nlocal_irq_restore(flags);\r\nprintk(KERN_WARNING "Could not switch to high resolution "\r\n"mode on CPU %d\n", cpu);\r\nreturn 0;\r\n}\r\nbase->hres_active = 1;\r\nfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++)\r\nbase->clock_base[i].resolution = KTIME_HIGH_RES;\r\ntick_setup_sched_timer();\r\nretrigger_next_event(NULL);\r\nlocal_irq_restore(flags);\r\nreturn 1;\r\n}\r\nstatic inline int hrtimer_hres_active(void) { return 0; }\r\nstatic inline int hrtimer_is_hres_enabled(void) { return 0; }\r\nstatic inline int hrtimer_switch_to_hres(void) { return 0; }\r\nstatic inline void\r\nhrtimer_force_reprogram(struct hrtimer_cpu_base *base, int skip_equal) { }\r\nstatic inline int hrtimer_enqueue_reprogram(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base,\r\nint wakeup)\r\n{\r\nreturn 0;\r\n}\r\nstatic inline void hrtimer_init_hres(struct hrtimer_cpu_base *base) { }\r\nstatic inline void retrigger_next_event(void *arg) { }\r\nvoid clock_was_set(void)\r\n{\r\n#ifdef CONFIG_HIGH_RES_TIMERS\r\non_each_cpu(retrigger_next_event, NULL, 1);\r\n#endif\r\ntimerfd_clock_was_set();\r\n}\r\nvoid hrtimers_resume(void)\r\n{\r\nWARN_ONCE(!irqs_disabled(),\r\nKERN_INFO "hrtimers_resume() called with IRQs enabled!");\r\nretrigger_next_event(NULL);\r\ntimerfd_clock_was_set();\r\n}\r\nstatic inline void timer_stats_hrtimer_set_start_info(struct hrtimer *timer)\r\n{\r\n#ifdef CONFIG_TIMER_STATS\r\nif (timer->start_site)\r\nreturn;\r\ntimer->start_site = __builtin_return_address(0);\r\nmemcpy(timer->start_comm, current->comm, TASK_COMM_LEN);\r\ntimer->start_pid = current->pid;\r\n#endif\r\n}\r\nstatic inline void timer_stats_hrtimer_clear_start_info(struct hrtimer *timer)\r\n{\r\n#ifdef CONFIG_TIMER_STATS\r\ntimer->start_site = NULL;\r\n#endif\r\n}\r\nstatic inline void timer_stats_account_hrtimer(struct hrtimer *timer)\r\n{\r\n#ifdef CONFIG_TIMER_STATS\r\nif (likely(!timer_stats_active))\r\nreturn;\r\ntimer_stats_update_stats(timer, timer->start_pid, timer->start_site,\r\ntimer->function, timer->start_comm, 0);\r\n#endif\r\n}\r\nstatic inline\r\nvoid unlock_hrtimer_base(const struct hrtimer *timer, unsigned long *flags)\r\n{\r\nraw_spin_unlock_irqrestore(&timer->base->cpu_base->lock, *flags);\r\n}\r\nu64 hrtimer_forward(struct hrtimer *timer, ktime_t now, ktime_t interval)\r\n{\r\nu64 orun = 1;\r\nktime_t delta;\r\ndelta = ktime_sub(now, hrtimer_get_expires(timer));\r\nif (delta.tv64 < 0)\r\nreturn 0;\r\nif (interval.tv64 < timer->base->resolution.tv64)\r\ninterval.tv64 = timer->base->resolution.tv64;\r\nif (unlikely(delta.tv64 >= interval.tv64)) {\r\ns64 incr = ktime_to_ns(interval);\r\norun = ktime_divns(delta, incr);\r\nhrtimer_add_expires_ns(timer, incr * orun);\r\nif (hrtimer_get_expires_tv64(timer) > now.tv64)\r\nreturn orun;\r\norun++;\r\n}\r\nhrtimer_add_expires(timer, interval);\r\nreturn orun;\r\n}\r\nstatic int enqueue_hrtimer(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base)\r\n{\r\ndebug_activate(timer);\r\ntimerqueue_add(&base->active, &timer->node);\r\nbase->cpu_base->active_bases |= 1 << base->index;\r\ntimer->state |= HRTIMER_STATE_ENQUEUED;\r\nreturn (&timer->node == base->active.next);\r\n}\r\nstatic void __remove_hrtimer(struct hrtimer *timer,\r\nstruct hrtimer_clock_base *base,\r\nunsigned long newstate, int reprogram)\r\n{\r\nstruct timerqueue_node *next_timer;\r\nif (!(timer->state & HRTIMER_STATE_ENQUEUED))\r\ngoto out;\r\nnext_timer = timerqueue_getnext(&base->active);\r\ntimerqueue_del(&base->active, &timer->node);\r\nif (&timer->node == next_timer) {\r\n#ifdef CONFIG_HIGH_RES_TIMERS\r\nif (reprogram && hrtimer_hres_active()) {\r\nktime_t expires;\r\nexpires = ktime_sub(hrtimer_get_expires(timer),\r\nbase->offset);\r\nif (base->cpu_base->expires_next.tv64 == expires.tv64)\r\nhrtimer_force_reprogram(base->cpu_base, 1);\r\n}\r\n#endif\r\n}\r\nif (!timerqueue_getnext(&base->active))\r\nbase->cpu_base->active_bases &= ~(1 << base->index);\r\nout:\r\ntimer->state = newstate;\r\n}\r\nstatic inline int\r\nremove_hrtimer(struct hrtimer *timer, struct hrtimer_clock_base *base)\r\n{\r\nif (hrtimer_is_queued(timer)) {\r\nunsigned long state;\r\nint reprogram;\r\ndebug_deactivate(timer);\r\ntimer_stats_hrtimer_clear_start_info(timer);\r\nreprogram = base->cpu_base == &__get_cpu_var(hrtimer_bases);\r\nstate = timer->state & HRTIMER_STATE_CALLBACK;\r\n__remove_hrtimer(timer, base, state, reprogram);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nint __hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,\r\nunsigned long delta_ns, const enum hrtimer_mode mode,\r\nint wakeup)\r\n{\r\nstruct hrtimer_clock_base *base, *new_base;\r\nunsigned long flags;\r\nint ret, leftmost;\r\nbase = lock_hrtimer_base(timer, &flags);\r\nret = remove_hrtimer(timer, base);\r\nnew_base = switch_hrtimer_base(timer, base, mode & HRTIMER_MODE_PINNED);\r\nif (mode & HRTIMER_MODE_REL) {\r\ntim = ktime_add_safe(tim, new_base->get_time());\r\n#ifdef CONFIG_TIME_LOW_RES\r\ntim = ktime_add_safe(tim, base->resolution);\r\n#endif\r\n}\r\nhrtimer_set_expires_range_ns(timer, tim, delta_ns);\r\ntimer_stats_hrtimer_set_start_info(timer);\r\nleftmost = enqueue_hrtimer(timer, new_base);\r\nif (leftmost && new_base->cpu_base == &__get_cpu_var(hrtimer_bases))\r\nhrtimer_enqueue_reprogram(timer, new_base, wakeup);\r\nunlock_hrtimer_base(timer, &flags);\r\nreturn ret;\r\n}\r\nint hrtimer_start_range_ns(struct hrtimer *timer, ktime_t tim,\r\nunsigned long delta_ns, const enum hrtimer_mode mode)\r\n{\r\nreturn __hrtimer_start_range_ns(timer, tim, delta_ns, mode, 1);\r\n}\r\nint\r\nhrtimer_start(struct hrtimer *timer, ktime_t tim, const enum hrtimer_mode mode)\r\n{\r\nreturn __hrtimer_start_range_ns(timer, tim, 0, mode, 1);\r\n}\r\nint hrtimer_try_to_cancel(struct hrtimer *timer)\r\n{\r\nstruct hrtimer_clock_base *base;\r\nunsigned long flags;\r\nint ret = -1;\r\nbase = lock_hrtimer_base(timer, &flags);\r\nif (!hrtimer_callback_running(timer))\r\nret = remove_hrtimer(timer, base);\r\nunlock_hrtimer_base(timer, &flags);\r\nreturn ret;\r\n}\r\nint hrtimer_cancel(struct hrtimer *timer)\r\n{\r\nfor (;;) {\r\nint ret = hrtimer_try_to_cancel(timer);\r\nif (ret >= 0)\r\nreturn ret;\r\ncpu_relax();\r\n}\r\n}\r\nktime_t hrtimer_get_remaining(const struct hrtimer *timer)\r\n{\r\nunsigned long flags;\r\nktime_t rem;\r\nlock_hrtimer_base(timer, &flags);\r\nrem = hrtimer_expires_remaining(timer);\r\nunlock_hrtimer_base(timer, &flags);\r\nreturn rem;\r\n}\r\nktime_t hrtimer_get_next_event(void)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = &__get_cpu_var(hrtimer_bases);\r\nstruct hrtimer_clock_base *base = cpu_base->clock_base;\r\nktime_t delta, mindelta = { .tv64 = KTIME_MAX };\r\nunsigned long flags;\r\nint i;\r\nraw_spin_lock_irqsave(&cpu_base->lock, flags);\r\nif (!hrtimer_hres_active()) {\r\nfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++, base++) {\r\nstruct hrtimer *timer;\r\nstruct timerqueue_node *next;\r\nnext = timerqueue_getnext(&base->active);\r\nif (!next)\r\ncontinue;\r\ntimer = container_of(next, struct hrtimer, node);\r\ndelta.tv64 = hrtimer_get_expires_tv64(timer);\r\ndelta = ktime_sub(delta, base->get_time());\r\nif (delta.tv64 < mindelta.tv64)\r\nmindelta.tv64 = delta.tv64;\r\n}\r\n}\r\nraw_spin_unlock_irqrestore(&cpu_base->lock, flags);\r\nif (mindelta.tv64 < 0)\r\nmindelta.tv64 = 0;\r\nreturn mindelta;\r\n}\r\nstatic void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\r\nenum hrtimer_mode mode)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base;\r\nint base;\r\nmemset(timer, 0, sizeof(struct hrtimer));\r\ncpu_base = &__raw_get_cpu_var(hrtimer_bases);\r\nif (clock_id == CLOCK_REALTIME && mode != HRTIMER_MODE_ABS)\r\nclock_id = CLOCK_MONOTONIC;\r\nbase = hrtimer_clockid_to_base(clock_id);\r\ntimer->base = &cpu_base->clock_base[base];\r\ntimerqueue_init(&timer->node);\r\n#ifdef CONFIG_TIMER_STATS\r\ntimer->start_site = NULL;\r\ntimer->start_pid = -1;\r\nmemset(timer->start_comm, 0, TASK_COMM_LEN);\r\n#endif\r\n}\r\nvoid hrtimer_init(struct hrtimer *timer, clockid_t clock_id,\r\nenum hrtimer_mode mode)\r\n{\r\ndebug_init(timer, clock_id, mode);\r\n__hrtimer_init(timer, clock_id, mode);\r\n}\r\nint hrtimer_get_res(const clockid_t which_clock, struct timespec *tp)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base;\r\nint base = hrtimer_clockid_to_base(which_clock);\r\ncpu_base = &__raw_get_cpu_var(hrtimer_bases);\r\n*tp = ktime_to_timespec(cpu_base->clock_base[base].resolution);\r\nreturn 0;\r\n}\r\nstatic void __run_hrtimer(struct hrtimer *timer, ktime_t *now)\r\n{\r\nstruct hrtimer_clock_base *base = timer->base;\r\nstruct hrtimer_cpu_base *cpu_base = base->cpu_base;\r\nenum hrtimer_restart (*fn)(struct hrtimer *);\r\nint restart;\r\nWARN_ON(!irqs_disabled());\r\ndebug_deactivate(timer);\r\n__remove_hrtimer(timer, base, HRTIMER_STATE_CALLBACK, 0);\r\ntimer_stats_account_hrtimer(timer);\r\nfn = timer->function;\r\nraw_spin_unlock(&cpu_base->lock);\r\ntrace_hrtimer_expire_entry(timer, now);\r\nrestart = fn(timer);\r\ntrace_hrtimer_expire_exit(timer);\r\nraw_spin_lock(&cpu_base->lock);\r\nif (restart != HRTIMER_NORESTART) {\r\nBUG_ON(timer->state != HRTIMER_STATE_CALLBACK);\r\nenqueue_hrtimer(timer, base);\r\n}\r\nWARN_ON_ONCE(!(timer->state & HRTIMER_STATE_CALLBACK));\r\ntimer->state &= ~HRTIMER_STATE_CALLBACK;\r\n}\r\nvoid hrtimer_interrupt(struct clock_event_device *dev)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = &__get_cpu_var(hrtimer_bases);\r\nktime_t expires_next, now, entry_time, delta;\r\nint i, retries = 0;\r\nBUG_ON(!cpu_base->hres_active);\r\ncpu_base->nr_events++;\r\ndev->next_event.tv64 = KTIME_MAX;\r\nentry_time = now = ktime_get();\r\nretry:\r\nexpires_next.tv64 = KTIME_MAX;\r\nraw_spin_lock(&cpu_base->lock);\r\ncpu_base->expires_next.tv64 = KTIME_MAX;\r\nfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {\r\nstruct hrtimer_clock_base *base;\r\nstruct timerqueue_node *node;\r\nktime_t basenow;\r\nif (!(cpu_base->active_bases & (1 << i)))\r\ncontinue;\r\nbase = cpu_base->clock_base + i;\r\nbasenow = ktime_add(now, base->offset);\r\nwhile ((node = timerqueue_getnext(&base->active))) {\r\nstruct hrtimer *timer;\r\ntimer = container_of(node, struct hrtimer, node);\r\nif (basenow.tv64 < hrtimer_get_softexpires_tv64(timer)) {\r\nktime_t expires;\r\nexpires = ktime_sub(hrtimer_get_expires(timer),\r\nbase->offset);\r\nif (expires.tv64 < expires_next.tv64)\r\nexpires_next = expires;\r\nbreak;\r\n}\r\n__run_hrtimer(timer, &basenow);\r\n}\r\n}\r\ncpu_base->expires_next = expires_next;\r\nraw_spin_unlock(&cpu_base->lock);\r\nif (expires_next.tv64 == KTIME_MAX ||\r\n!tick_program_event(expires_next, 0)) {\r\ncpu_base->hang_detected = 0;\r\nreturn;\r\n}\r\nnow = ktime_get();\r\ncpu_base->nr_retries++;\r\nif (++retries < 3)\r\ngoto retry;\r\ncpu_base->nr_hangs++;\r\ncpu_base->hang_detected = 1;\r\ndelta = ktime_sub(now, entry_time);\r\nif (delta.tv64 > cpu_base->max_hang_time.tv64)\r\ncpu_base->max_hang_time = delta;\r\nif (delta.tv64 > 100 * NSEC_PER_MSEC)\r\nexpires_next = ktime_add_ns(now, 100 * NSEC_PER_MSEC);\r\nelse\r\nexpires_next = ktime_add(now, delta);\r\ntick_program_event(expires_next, 1);\r\nprintk_once(KERN_WARNING "hrtimer: interrupt took %llu ns\n",\r\nktime_to_ns(delta));\r\n}\r\nstatic void __hrtimer_peek_ahead_timers(void)\r\n{\r\nstruct tick_device *td;\r\nif (!hrtimer_hres_active())\r\nreturn;\r\ntd = &__get_cpu_var(tick_cpu_device);\r\nif (td && td->evtdev)\r\nhrtimer_interrupt(td->evtdev);\r\n}\r\nvoid hrtimer_peek_ahead_timers(void)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\n__hrtimer_peek_ahead_timers();\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void run_hrtimer_softirq(struct softirq_action *h)\r\n{\r\nhrtimer_peek_ahead_timers();\r\n}\r\nstatic inline void __hrtimer_peek_ahead_timers(void) { }\r\nvoid hrtimer_run_pending(void)\r\n{\r\nif (hrtimer_hres_active())\r\nreturn;\r\nif (tick_check_oneshot_change(!hrtimer_is_hres_enabled()))\r\nhrtimer_switch_to_hres();\r\n}\r\nvoid hrtimer_run_queues(void)\r\n{\r\nstruct timerqueue_node *node;\r\nstruct hrtimer_cpu_base *cpu_base = &__get_cpu_var(hrtimer_bases);\r\nstruct hrtimer_clock_base *base;\r\nint index, gettime = 1;\r\nif (hrtimer_hres_active())\r\nreturn;\r\nfor (index = 0; index < HRTIMER_MAX_CLOCK_BASES; index++) {\r\nbase = &cpu_base->clock_base[index];\r\nif (!timerqueue_getnext(&base->active))\r\ncontinue;\r\nif (gettime) {\r\nhrtimer_get_softirq_time(cpu_base);\r\ngettime = 0;\r\n}\r\nraw_spin_lock(&cpu_base->lock);\r\nwhile ((node = timerqueue_getnext(&base->active))) {\r\nstruct hrtimer *timer;\r\ntimer = container_of(node, struct hrtimer, node);\r\nif (base->softirq_time.tv64 <=\r\nhrtimer_get_expires_tv64(timer))\r\nbreak;\r\n__run_hrtimer(timer, &base->softirq_time);\r\n}\r\nraw_spin_unlock(&cpu_base->lock);\r\n}\r\n}\r\nstatic enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer)\r\n{\r\nstruct hrtimer_sleeper *t =\r\ncontainer_of(timer, struct hrtimer_sleeper, timer);\r\nstruct task_struct *task = t->task;\r\nt->task = NULL;\r\nif (task)\r\nwake_up_process(task);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nvoid hrtimer_init_sleeper(struct hrtimer_sleeper *sl, struct task_struct *task)\r\n{\r\nsl->timer.function = hrtimer_wakeup;\r\nsl->task = task;\r\n}\r\nstatic int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode)\r\n{\r\nhrtimer_init_sleeper(t, current);\r\ndo {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nhrtimer_start_expires(&t->timer, mode);\r\nif (!hrtimer_active(&t->timer))\r\nt->task = NULL;\r\nif (likely(t->task))\r\nschedule();\r\nhrtimer_cancel(&t->timer);\r\nmode = HRTIMER_MODE_ABS;\r\n} while (t->task && !signal_pending(current));\r\n__set_current_state(TASK_RUNNING);\r\nreturn t->task == NULL;\r\n}\r\nstatic int update_rmtp(struct hrtimer *timer, struct timespec __user *rmtp)\r\n{\r\nstruct timespec rmt;\r\nktime_t rem;\r\nrem = hrtimer_expires_remaining(timer);\r\nif (rem.tv64 <= 0)\r\nreturn 0;\r\nrmt = ktime_to_timespec(rem);\r\nif (copy_to_user(rmtp, &rmt, sizeof(*rmtp)))\r\nreturn -EFAULT;\r\nreturn 1;\r\n}\r\nlong __sched hrtimer_nanosleep_restart(struct restart_block *restart)\r\n{\r\nstruct hrtimer_sleeper t;\r\nstruct timespec __user *rmtp;\r\nint ret = 0;\r\nhrtimer_init_on_stack(&t.timer, restart->nanosleep.clockid,\r\nHRTIMER_MODE_ABS);\r\nhrtimer_set_expires_tv64(&t.timer, restart->nanosleep.expires);\r\nif (do_nanosleep(&t, HRTIMER_MODE_ABS))\r\ngoto out;\r\nrmtp = restart->nanosleep.rmtp;\r\nif (rmtp) {\r\nret = update_rmtp(&t.timer, rmtp);\r\nif (ret <= 0)\r\ngoto out;\r\n}\r\nret = -ERESTART_RESTARTBLOCK;\r\nout:\r\ndestroy_hrtimer_on_stack(&t.timer);\r\nreturn ret;\r\n}\r\nlong hrtimer_nanosleep(struct timespec *rqtp, struct timespec __user *rmtp,\r\nconst enum hrtimer_mode mode, const clockid_t clockid)\r\n{\r\nstruct restart_block *restart;\r\nstruct hrtimer_sleeper t;\r\nint ret = 0;\r\nunsigned long slack;\r\nslack = current->timer_slack_ns;\r\nif (rt_task(current))\r\nslack = 0;\r\nhrtimer_init_on_stack(&t.timer, clockid, mode);\r\nhrtimer_set_expires_range_ns(&t.timer, timespec_to_ktime(*rqtp), slack);\r\nif (do_nanosleep(&t, mode))\r\ngoto out;\r\nif (mode == HRTIMER_MODE_ABS) {\r\nret = -ERESTARTNOHAND;\r\ngoto out;\r\n}\r\nif (rmtp) {\r\nret = update_rmtp(&t.timer, rmtp);\r\nif (ret <= 0)\r\ngoto out;\r\n}\r\nrestart = &current_thread_info()->restart_block;\r\nrestart->fn = hrtimer_nanosleep_restart;\r\nrestart->nanosleep.clockid = t.timer.base->clockid;\r\nrestart->nanosleep.rmtp = rmtp;\r\nrestart->nanosleep.expires = hrtimer_get_expires_tv64(&t.timer);\r\nret = -ERESTART_RESTARTBLOCK;\r\nout:\r\ndestroy_hrtimer_on_stack(&t.timer);\r\nreturn ret;\r\n}\r\nstatic void __cpuinit init_hrtimers_cpu(int cpu)\r\n{\r\nstruct hrtimer_cpu_base *cpu_base = &per_cpu(hrtimer_bases, cpu);\r\nint i;\r\nraw_spin_lock_init(&cpu_base->lock);\r\nfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {\r\ncpu_base->clock_base[i].cpu_base = cpu_base;\r\ntimerqueue_init_head(&cpu_base->clock_base[i].active);\r\n}\r\nhrtimer_init_hres(cpu_base);\r\n}\r\nstatic void migrate_hrtimer_list(struct hrtimer_clock_base *old_base,\r\nstruct hrtimer_clock_base *new_base)\r\n{\r\nstruct hrtimer *timer;\r\nstruct timerqueue_node *node;\r\nwhile ((node = timerqueue_getnext(&old_base->active))) {\r\ntimer = container_of(node, struct hrtimer, node);\r\nBUG_ON(hrtimer_callback_running(timer));\r\ndebug_deactivate(timer);\r\n__remove_hrtimer(timer, old_base, HRTIMER_STATE_MIGRATE, 0);\r\ntimer->base = new_base;\r\nenqueue_hrtimer(timer, new_base);\r\ntimer->state &= ~HRTIMER_STATE_MIGRATE;\r\n}\r\n}\r\nstatic void migrate_hrtimers(int scpu)\r\n{\r\nstruct hrtimer_cpu_base *old_base, *new_base;\r\nint i;\r\nBUG_ON(cpu_online(scpu));\r\ntick_cancel_sched_timer(scpu);\r\nlocal_irq_disable();\r\nold_base = &per_cpu(hrtimer_bases, scpu);\r\nnew_base = &__get_cpu_var(hrtimer_bases);\r\nraw_spin_lock(&new_base->lock);\r\nraw_spin_lock_nested(&old_base->lock, SINGLE_DEPTH_NESTING);\r\nfor (i = 0; i < HRTIMER_MAX_CLOCK_BASES; i++) {\r\nmigrate_hrtimer_list(&old_base->clock_base[i],\r\n&new_base->clock_base[i]);\r\n}\r\nraw_spin_unlock(&old_base->lock);\r\nraw_spin_unlock(&new_base->lock);\r\n__hrtimer_peek_ahead_timers();\r\nlocal_irq_enable();\r\n}\r\nstatic int __cpuinit hrtimer_cpu_notify(struct notifier_block *self,\r\nunsigned long action, void *hcpu)\r\n{\r\nint scpu = (long)hcpu;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\ncase CPU_UP_PREPARE_FROZEN:\r\ninit_hrtimers_cpu(scpu);\r\nbreak;\r\n#ifdef CONFIG_HOTPLUG_CPU\r\ncase CPU_DYING:\r\ncase CPU_DYING_FROZEN:\r\nclockevents_notify(CLOCK_EVT_NOTIFY_CPU_DYING, &scpu);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_DEAD_FROZEN:\r\n{\r\nclockevents_notify(CLOCK_EVT_NOTIFY_CPU_DEAD, &scpu);\r\nmigrate_hrtimers(scpu);\r\nbreak;\r\n}\r\n#endif\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nvoid __init hrtimers_init(void)\r\n{\r\nhrtimer_cpu_notify(&hrtimers_nb, (unsigned long)CPU_UP_PREPARE,\r\n(void *)(long)smp_processor_id());\r\nregister_cpu_notifier(&hrtimers_nb);\r\n#ifdef CONFIG_HIGH_RES_TIMERS\r\nopen_softirq(HRTIMER_SOFTIRQ, run_hrtimer_softirq);\r\n#endif\r\n}\r\nint __sched\r\nschedule_hrtimeout_range_clock(ktime_t *expires, unsigned long delta,\r\nconst enum hrtimer_mode mode, int clock)\r\n{\r\nstruct hrtimer_sleeper t;\r\nif (expires && !expires->tv64) {\r\n__set_current_state(TASK_RUNNING);\r\nreturn 0;\r\n}\r\nif (!expires) {\r\nschedule();\r\n__set_current_state(TASK_RUNNING);\r\nreturn -EINTR;\r\n}\r\nhrtimer_init_on_stack(&t.timer, clock, mode);\r\nhrtimer_set_expires_range_ns(&t.timer, *expires, delta);\r\nhrtimer_init_sleeper(&t, current);\r\nhrtimer_start_expires(&t.timer, mode);\r\nif (!hrtimer_active(&t.timer))\r\nt.task = NULL;\r\nif (likely(t.task))\r\nschedule();\r\nhrtimer_cancel(&t.timer);\r\ndestroy_hrtimer_on_stack(&t.timer);\r\n__set_current_state(TASK_RUNNING);\r\nreturn !t.task ? 0 : -EINTR;\r\n}\r\nint __sched schedule_hrtimeout_range(ktime_t *expires, unsigned long delta,\r\nconst enum hrtimer_mode mode)\r\n{\r\nreturn schedule_hrtimeout_range_clock(expires, delta, mode,\r\nCLOCK_MONOTONIC);\r\n}\r\nint __sched schedule_hrtimeout(ktime_t *expires,\r\nconst enum hrtimer_mode mode)\r\n{\r\nreturn schedule_hrtimeout_range(expires, 0, mode);\r\n}
