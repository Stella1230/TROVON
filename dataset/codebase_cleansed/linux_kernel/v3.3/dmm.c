int dmm_create_tables(struct dmm_object *dmm_mgr, u32 addr, u32 size)\r\n{\r\nstruct dmm_object *dmm_obj = (struct dmm_object *)dmm_mgr;\r\nint status = 0;\r\nstatus = dmm_delete_tables(dmm_obj);\r\nif (!status) {\r\ndyn_mem_map_beg = addr;\r\ntable_size = PG_ALIGN_HIGH(size, PG_SIZE4K) / PG_SIZE4K;\r\nvirtual_mapping_table = __vmalloc(table_size *\r\nsizeof(struct map_page), GFP_KERNEL |\r\n__GFP_HIGHMEM | __GFP_ZERO, PAGE_KERNEL);\r\nif (virtual_mapping_table == NULL)\r\nstatus = -ENOMEM;\r\nelse {\r\nfree_region = 0;\r\nfree_size = table_size * PG_SIZE4K;\r\nvirtual_mapping_table[0].region_size = table_size;\r\n}\r\n}\r\nif (status)\r\npr_err("%s: failure, status 0x%x\n", __func__, status);\r\nreturn status;\r\n}\r\nint dmm_create(struct dmm_object **dmm_manager,\r\nstruct dev_object *hdev_obj,\r\nconst struct dmm_mgrattrs *mgr_attrts)\r\n{\r\nstruct dmm_object *dmm_obj = NULL;\r\nint status = 0;\r\nDBC_REQUIRE(refs > 0);\r\nDBC_REQUIRE(dmm_manager != NULL);\r\n*dmm_manager = NULL;\r\ndmm_obj = kzalloc(sizeof(struct dmm_object), GFP_KERNEL);\r\nif (dmm_obj != NULL) {\r\nspin_lock_init(&dmm_obj->dmm_lock);\r\n*dmm_manager = dmm_obj;\r\n} else {\r\nstatus = -ENOMEM;\r\n}\r\nreturn status;\r\n}\r\nint dmm_destroy(struct dmm_object *dmm_mgr)\r\n{\r\nstruct dmm_object *dmm_obj = (struct dmm_object *)dmm_mgr;\r\nint status = 0;\r\nDBC_REQUIRE(refs > 0);\r\nif (dmm_mgr) {\r\nstatus = dmm_delete_tables(dmm_obj);\r\nif (!status)\r\nkfree(dmm_obj);\r\n} else\r\nstatus = -EFAULT;\r\nreturn status;\r\n}\r\nint dmm_delete_tables(struct dmm_object *dmm_mgr)\r\n{\r\nint status = 0;\r\nDBC_REQUIRE(refs > 0);\r\nif (dmm_mgr)\r\nvfree(virtual_mapping_table);\r\nelse\r\nstatus = -EFAULT;\r\nreturn status;\r\n}\r\nvoid dmm_exit(void)\r\n{\r\nDBC_REQUIRE(refs > 0);\r\nrefs--;\r\n}\r\nint dmm_get_handle(void *hprocessor, struct dmm_object **dmm_manager)\r\n{\r\nint status = 0;\r\nstruct dev_object *hdev_obj;\r\nDBC_REQUIRE(refs > 0);\r\nDBC_REQUIRE(dmm_manager != NULL);\r\nif (hprocessor != NULL)\r\nstatus = proc_get_dev_object(hprocessor, &hdev_obj);\r\nelse\r\nhdev_obj = dev_get_first();\r\nif (!status)\r\nstatus = dev_get_dmm_mgr(hdev_obj, dmm_manager);\r\nreturn status;\r\n}\r\nbool dmm_init(void)\r\n{\r\nbool ret = true;\r\nDBC_REQUIRE(refs >= 0);\r\nif (ret)\r\nrefs++;\r\nDBC_ENSURE((ret && (refs > 0)) || (!ret && (refs >= 0)));\r\nvirtual_mapping_table = NULL;\r\ntable_size = 0;\r\nreturn ret;\r\n}\r\nint dmm_map_memory(struct dmm_object *dmm_mgr, u32 addr, u32 size)\r\n{\r\nstruct dmm_object *dmm_obj = (struct dmm_object *)dmm_mgr;\r\nstruct map_page *chunk;\r\nint status = 0;\r\nspin_lock(&dmm_obj->dmm_lock);\r\nchunk = (struct map_page *)get_region(addr);\r\nif (chunk != NULL) {\r\nchunk->mapped = true;\r\nchunk->mapped_size = (size / PG_SIZE4K);\r\n} else\r\nstatus = -ENOENT;\r\nspin_unlock(&dmm_obj->dmm_lock);\r\ndev_dbg(bridge, "%s dmm_mgr %p, addr %x, size %x\n\tstatus %x, "\r\n"chunk %p", __func__, dmm_mgr, addr, size, status, chunk);\r\nreturn status;\r\n}\r\nint dmm_reserve_memory(struct dmm_object *dmm_mgr, u32 size,\r\nu32 *prsv_addr)\r\n{\r\nint status = 0;\r\nstruct dmm_object *dmm_obj = (struct dmm_object *)dmm_mgr;\r\nstruct map_page *node;\r\nu32 rsv_addr = 0;\r\nu32 rsv_size = 0;\r\nspin_lock(&dmm_obj->dmm_lock);\r\nnode = get_free_region(size);\r\nif (node != NULL) {\r\nrsv_addr = DMM_ADDR_VIRTUAL(node);\r\nrsv_size = size / PG_SIZE4K;\r\nif (rsv_size < node->region_size) {\r\nnode[rsv_size].mapped = false;\r\nnode[rsv_size].reserved = false;\r\nnode[rsv_size].region_size =\r\nnode->region_size - rsv_size;\r\nnode[rsv_size].mapped_size = 0;\r\n}\r\nnode->mapped = false;\r\nnode->reserved = true;\r\nnode->region_size = rsv_size;\r\nnode->mapped_size = 0;\r\n*prsv_addr = rsv_addr;\r\n} else\r\nstatus = -ENOMEM;\r\nspin_unlock(&dmm_obj->dmm_lock);\r\ndev_dbg(bridge, "%s dmm_mgr %p, size %x, prsv_addr %p\n\tstatus %x, "\r\n"rsv_addr %x, rsv_size %x\n", __func__, dmm_mgr, size,\r\nprsv_addr, status, rsv_addr, rsv_size);\r\nreturn status;\r\n}\r\nint dmm_un_map_memory(struct dmm_object *dmm_mgr, u32 addr, u32 *psize)\r\n{\r\nstruct dmm_object *dmm_obj = (struct dmm_object *)dmm_mgr;\r\nstruct map_page *chunk;\r\nint status = 0;\r\nspin_lock(&dmm_obj->dmm_lock);\r\nchunk = get_mapped_region(addr);\r\nif (chunk == NULL)\r\nstatus = -ENOENT;\r\nif (!status) {\r\n*psize = chunk->mapped_size * PG_SIZE4K;\r\nchunk->mapped = false;\r\nchunk->mapped_size = 0;\r\n}\r\nspin_unlock(&dmm_obj->dmm_lock);\r\ndev_dbg(bridge, "%s: dmm_mgr %p, addr %x, psize %p\n\tstatus %x, "\r\n"chunk %p\n", __func__, dmm_mgr, addr, psize, status, chunk);\r\nreturn status;\r\n}\r\nint dmm_un_reserve_memory(struct dmm_object *dmm_mgr, u32 rsv_addr)\r\n{\r\nstruct dmm_object *dmm_obj = (struct dmm_object *)dmm_mgr;\r\nstruct map_page *chunk;\r\nu32 i;\r\nint status = 0;\r\nu32 chunk_size;\r\nspin_lock(&dmm_obj->dmm_lock);\r\nchunk = get_mapped_region(rsv_addr);\r\nif (chunk == NULL)\r\nstatus = -ENOENT;\r\nif (!status) {\r\ni = 0;\r\nwhile (i < chunk->region_size) {\r\nif (chunk[i].mapped) {\r\nchunk_size = chunk[i].mapped_size;\r\nchunk[i].mapped = false;\r\nchunk[i].mapped_size = 0;\r\ni += chunk_size;\r\n} else\r\ni++;\r\n}\r\nchunk->reserved = false;\r\n}\r\nspin_unlock(&dmm_obj->dmm_lock);\r\ndev_dbg(bridge, "%s: dmm_mgr %p, rsv_addr %x\n\tstatus %x chunk %p",\r\n__func__, dmm_mgr, rsv_addr, status, chunk);\r\nreturn status;\r\n}\r\nstatic struct map_page *get_region(u32 addr)\r\n{\r\nstruct map_page *curr_region = NULL;\r\nu32 i = 0;\r\nif (virtual_mapping_table != NULL) {\r\ni = DMM_ADDR_TO_INDEX(addr);\r\nif (i < table_size)\r\ncurr_region = virtual_mapping_table + i;\r\n}\r\ndev_dbg(bridge, "%s: curr_region %p, free_region %d, free_size %d\n",\r\n__func__, curr_region, free_region, free_size);\r\nreturn curr_region;\r\n}\r\nstatic struct map_page *get_free_region(u32 len)\r\n{\r\nstruct map_page *curr_region = NULL;\r\nu32 i = 0;\r\nu32 region_size = 0;\r\nu32 next_i = 0;\r\nif (virtual_mapping_table == NULL)\r\nreturn curr_region;\r\nif (len > free_size) {\r\nwhile (i < table_size) {\r\nregion_size = virtual_mapping_table[i].region_size;\r\nnext_i = i + region_size;\r\nif (virtual_mapping_table[i].reserved == false) {\r\nif (next_i < table_size &&\r\nvirtual_mapping_table[next_i].reserved\r\n== false) {\r\nvirtual_mapping_table[i].region_size +=\r\nvirtual_mapping_table\r\n[next_i].region_size;\r\ncontinue;\r\n}\r\nregion_size *= PG_SIZE4K;\r\nif (region_size > free_size) {\r\nfree_region = i;\r\nfree_size = region_size;\r\n}\r\n}\r\ni = next_i;\r\n}\r\n}\r\nif (len <= free_size) {\r\ncurr_region = virtual_mapping_table + free_region;\r\nfree_region += (len / PG_SIZE4K);\r\nfree_size -= len;\r\n}\r\nreturn curr_region;\r\n}\r\nstatic struct map_page *get_mapped_region(u32 addrs)\r\n{\r\nu32 i = 0;\r\nstruct map_page *curr_region = NULL;\r\nif (virtual_mapping_table == NULL)\r\nreturn curr_region;\r\ni = DMM_ADDR_TO_INDEX(addrs);\r\nif (i < table_size && (virtual_mapping_table[i].mapped ||\r\nvirtual_mapping_table[i].reserved))\r\ncurr_region = virtual_mapping_table + i;\r\nreturn curr_region;\r\n}\r\nu32 dmm_mem_map_dump(struct dmm_object *dmm_mgr)\r\n{\r\nstruct map_page *curr_node = NULL;\r\nu32 i;\r\nu32 freemem = 0;\r\nu32 bigsize = 0;\r\nspin_lock(&dmm_mgr->dmm_lock);\r\nif (virtual_mapping_table != NULL) {\r\nfor (i = 0; i < table_size; i +=\r\nvirtual_mapping_table[i].region_size) {\r\ncurr_node = virtual_mapping_table + i;\r\nif (curr_node->reserved) {\r\n} else {\r\nfreemem += (curr_node->region_size * PG_SIZE4K);\r\nif (curr_node->region_size > bigsize)\r\nbigsize = curr_node->region_size;\r\n}\r\n}\r\n}\r\nspin_unlock(&dmm_mgr->dmm_lock);\r\nprintk(KERN_INFO "Total DSP VA FREE memory = %d Mbytes\n",\r\nfreemem / (1024 * 1024));\r\nprintk(KERN_INFO "Total DSP VA USED memory= %d Mbytes \n",\r\n(((table_size * PG_SIZE4K) - freemem)) / (1024 * 1024));\r\nprintk(KERN_INFO "DSP VA - Biggest FREE block = %d Mbytes \n\n",\r\n(bigsize * PG_SIZE4K / (1024 * 1024)));\r\nreturn 0;\r\n}
