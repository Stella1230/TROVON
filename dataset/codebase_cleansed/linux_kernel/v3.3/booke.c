void kvmppc_dump_vcpu(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nprintk("pc: %08lx msr: %08llx\n", vcpu->arch.pc, vcpu->arch.shared->msr);\r\nprintk("lr: %08lx ctr: %08lx\n", vcpu->arch.lr, vcpu->arch.ctr);\r\nprintk("srr0: %08llx srr1: %08llx\n", vcpu->arch.shared->srr0,\r\nvcpu->arch.shared->srr1);\r\nprintk("exceptions: %08lx\n", vcpu->arch.pending_exceptions);\r\nfor (i = 0; i < 32; i += 4) {\r\nprintk("gpr%02d: %08lx %08lx %08lx %08lx\n", i,\r\nkvmppc_get_gpr(vcpu, i),\r\nkvmppc_get_gpr(vcpu, i+1),\r\nkvmppc_get_gpr(vcpu, i+2),\r\nkvmppc_get_gpr(vcpu, i+3));\r\n}\r\n}\r\nvoid kvmppc_vcpu_disable_spe(struct kvm_vcpu *vcpu)\r\n{\r\npreempt_disable();\r\nenable_kernel_spe();\r\nkvmppc_save_guest_spe(vcpu);\r\nvcpu->arch.shadow_msr &= ~MSR_SPE;\r\npreempt_enable();\r\n}\r\nstatic void kvmppc_vcpu_enable_spe(struct kvm_vcpu *vcpu)\r\n{\r\npreempt_disable();\r\nenable_kernel_spe();\r\nkvmppc_load_guest_spe(vcpu);\r\nvcpu->arch.shadow_msr |= MSR_SPE;\r\npreempt_enable();\r\n}\r\nstatic void kvmppc_vcpu_sync_spe(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->arch.shared->msr & MSR_SPE) {\r\nif (!(vcpu->arch.shadow_msr & MSR_SPE))\r\nkvmppc_vcpu_enable_spe(vcpu);\r\n} else if (vcpu->arch.shadow_msr & MSR_SPE) {\r\nkvmppc_vcpu_disable_spe(vcpu);\r\n}\r\n}\r\nstatic void kvmppc_vcpu_sync_spe(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nvoid kvmppc_set_msr(struct kvm_vcpu *vcpu, u32 new_msr)\r\n{\r\nu32 old_msr = vcpu->arch.shared->msr;\r\nvcpu->arch.shared->msr = new_msr;\r\nkvmppc_mmu_msr_notify(vcpu, old_msr);\r\nif (vcpu->arch.shared->msr & MSR_WE) {\r\nkvm_vcpu_block(vcpu);\r\nkvmppc_set_exit_type(vcpu, EMULATED_MTMSRWE_EXITS);\r\n};\r\nkvmppc_vcpu_sync_spe(vcpu);\r\n}\r\nstatic void kvmppc_booke_queue_irqprio(struct kvm_vcpu *vcpu,\r\nunsigned int priority)\r\n{\r\nset_bit(priority, &vcpu->arch.pending_exceptions);\r\n}\r\nstatic void kvmppc_core_queue_dtlb_miss(struct kvm_vcpu *vcpu,\r\nulong dear_flags, ulong esr_flags)\r\n{\r\nvcpu->arch.queued_dear = dear_flags;\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_DTLB_MISS);\r\n}\r\nstatic void kvmppc_core_queue_data_storage(struct kvm_vcpu *vcpu,\r\nulong dear_flags, ulong esr_flags)\r\n{\r\nvcpu->arch.queued_dear = dear_flags;\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_DATA_STORAGE);\r\n}\r\nstatic void kvmppc_core_queue_inst_storage(struct kvm_vcpu *vcpu,\r\nulong esr_flags)\r\n{\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_INST_STORAGE);\r\n}\r\nvoid kvmppc_core_queue_program(struct kvm_vcpu *vcpu, ulong esr_flags)\r\n{\r\nvcpu->arch.queued_esr = esr_flags;\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_PROGRAM);\r\n}\r\nvoid kvmppc_core_queue_dec(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_DECREMENTER);\r\n}\r\nint kvmppc_core_pending_dec(struct kvm_vcpu *vcpu)\r\n{\r\nreturn test_bit(BOOKE_IRQPRIO_DECREMENTER, &vcpu->arch.pending_exceptions);\r\n}\r\nvoid kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu)\r\n{\r\nclear_bit(BOOKE_IRQPRIO_DECREMENTER, &vcpu->arch.pending_exceptions);\r\n}\r\nvoid kvmppc_core_queue_external(struct kvm_vcpu *vcpu,\r\nstruct kvm_interrupt *irq)\r\n{\r\nunsigned int prio = BOOKE_IRQPRIO_EXTERNAL;\r\nif (irq->irq == KVM_INTERRUPT_SET_LEVEL)\r\nprio = BOOKE_IRQPRIO_EXTERNAL_LEVEL;\r\nkvmppc_booke_queue_irqprio(vcpu, prio);\r\n}\r\nvoid kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu,\r\nstruct kvm_interrupt *irq)\r\n{\r\nclear_bit(BOOKE_IRQPRIO_EXTERNAL, &vcpu->arch.pending_exceptions);\r\nclear_bit(BOOKE_IRQPRIO_EXTERNAL_LEVEL, &vcpu->arch.pending_exceptions);\r\n}\r\nstatic int kvmppc_booke_irqprio_deliver(struct kvm_vcpu *vcpu,\r\nunsigned int priority)\r\n{\r\nint allowed = 0;\r\nulong uninitialized_var(msr_mask);\r\nbool update_esr = false, update_dear = false;\r\nulong crit_raw = vcpu->arch.shared->critical;\r\nulong crit_r1 = kvmppc_get_gpr(vcpu, 1);\r\nbool crit;\r\nbool keep_irq = false;\r\nif (!(vcpu->arch.shared->msr & MSR_SF)) {\r\ncrit_raw &= 0xffffffff;\r\ncrit_r1 &= 0xffffffff;\r\n}\r\ncrit = (crit_raw == crit_r1);\r\ncrit = crit && !(vcpu->arch.shared->msr & MSR_PR);\r\nif (priority == BOOKE_IRQPRIO_EXTERNAL_LEVEL) {\r\npriority = BOOKE_IRQPRIO_EXTERNAL;\r\nkeep_irq = true;\r\n}\r\nswitch (priority) {\r\ncase BOOKE_IRQPRIO_DTLB_MISS:\r\ncase BOOKE_IRQPRIO_DATA_STORAGE:\r\nupdate_dear = true;\r\ncase BOOKE_IRQPRIO_INST_STORAGE:\r\ncase BOOKE_IRQPRIO_PROGRAM:\r\nupdate_esr = true;\r\ncase BOOKE_IRQPRIO_ITLB_MISS:\r\ncase BOOKE_IRQPRIO_SYSCALL:\r\ncase BOOKE_IRQPRIO_FP_UNAVAIL:\r\ncase BOOKE_IRQPRIO_SPE_UNAVAIL:\r\ncase BOOKE_IRQPRIO_SPE_FP_DATA:\r\ncase BOOKE_IRQPRIO_SPE_FP_ROUND:\r\ncase BOOKE_IRQPRIO_AP_UNAVAIL:\r\ncase BOOKE_IRQPRIO_ALIGNMENT:\r\nallowed = 1;\r\nmsr_mask = MSR_CE|MSR_ME|MSR_DE;\r\nbreak;\r\ncase BOOKE_IRQPRIO_CRITICAL:\r\ncase BOOKE_IRQPRIO_WATCHDOG:\r\nallowed = vcpu->arch.shared->msr & MSR_CE;\r\nmsr_mask = MSR_ME;\r\nbreak;\r\ncase BOOKE_IRQPRIO_MACHINE_CHECK:\r\nallowed = vcpu->arch.shared->msr & MSR_ME;\r\nmsr_mask = 0;\r\nbreak;\r\ncase BOOKE_IRQPRIO_EXTERNAL:\r\ncase BOOKE_IRQPRIO_DECREMENTER:\r\ncase BOOKE_IRQPRIO_FIT:\r\nallowed = vcpu->arch.shared->msr & MSR_EE;\r\nallowed = allowed && !crit;\r\nmsr_mask = MSR_CE|MSR_ME|MSR_DE;\r\nbreak;\r\ncase BOOKE_IRQPRIO_DEBUG:\r\nallowed = vcpu->arch.shared->msr & MSR_DE;\r\nmsr_mask = MSR_ME;\r\nbreak;\r\n}\r\nif (allowed) {\r\nvcpu->arch.shared->srr0 = vcpu->arch.pc;\r\nvcpu->arch.shared->srr1 = vcpu->arch.shared->msr;\r\nvcpu->arch.pc = vcpu->arch.ivpr | vcpu->arch.ivor[priority];\r\nif (update_esr == true)\r\nvcpu->arch.esr = vcpu->arch.queued_esr;\r\nif (update_dear == true)\r\nvcpu->arch.shared->dar = vcpu->arch.queued_dear;\r\nkvmppc_set_msr(vcpu, vcpu->arch.shared->msr & msr_mask);\r\nif (!keep_irq)\r\nclear_bit(priority, &vcpu->arch.pending_exceptions);\r\n}\r\nreturn allowed;\r\n}\r\nvoid kvmppc_core_deliver_interrupts(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long *pending = &vcpu->arch.pending_exceptions;\r\nunsigned long old_pending = vcpu->arch.pending_exceptions;\r\nunsigned int priority;\r\npriority = __ffs(*pending);\r\nwhile (priority <= BOOKE_IRQPRIO_MAX) {\r\nif (kvmppc_booke_irqprio_deliver(vcpu, priority))\r\nbreak;\r\npriority = find_next_bit(pending,\r\nBITS_PER_BYTE * sizeof(*pending),\r\npriority + 1);\r\n}\r\nif (*pending)\r\nvcpu->arch.shared->int_pending = 1;\r\nelse if (old_pending)\r\nvcpu->arch.shared->int_pending = 0;\r\n}\r\nint kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)\r\n{\r\nint ret;\r\nif (!vcpu->arch.sane) {\r\nkvm_run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\r\nreturn -EINVAL;\r\n}\r\nlocal_irq_disable();\r\nkvm_guest_enter();\r\nret = __kvmppc_vcpu_run(kvm_run, vcpu);\r\nkvm_guest_exit();\r\nlocal_irq_enable();\r\nreturn ret;\r\n}\r\nint kvmppc_handle_exit(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int exit_nr)\r\n{\r\nenum emulation_result er;\r\nint r = RESUME_HOST;\r\nkvmppc_update_timing_stats(vcpu);\r\nlocal_irq_enable();\r\nrun->exit_reason = KVM_EXIT_UNKNOWN;\r\nrun->ready_for_interrupt_injection = 1;\r\nswitch (exit_nr) {\r\ncase BOOKE_INTERRUPT_MACHINE_CHECK:\r\nprintk("MACHINE CHECK: %lx\n", mfspr(SPRN_MCSR));\r\nkvmppc_dump_vcpu(vcpu);\r\nr = RESUME_HOST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_EXTERNAL:\r\nkvmppc_account_exit(vcpu, EXT_INTR_EXITS);\r\nif (need_resched())\r\ncond_resched();\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_DECREMENTER:\r\nkvmppc_account_exit(vcpu, DEC_EXITS);\r\nif (need_resched())\r\ncond_resched();\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_PROGRAM:\r\nif (vcpu->arch.shared->msr & MSR_PR) {\r\nkvmppc_core_queue_program(vcpu, vcpu->arch.fault_esr);\r\nr = RESUME_GUEST;\r\nkvmppc_account_exit(vcpu, USR_PR_INST);\r\nbreak;\r\n}\r\ner = kvmppc_emulate_instruction(run, vcpu);\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nkvmppc_account_exit_stat(vcpu, EMULATED_INST_EXITS);\r\nr = RESUME_GUEST_NV;\r\nbreak;\r\ncase EMULATE_DO_DCR:\r\nrun->exit_reason = KVM_EXIT_DCR;\r\nr = RESUME_HOST;\r\nbreak;\r\ncase EMULATE_FAIL:\r\nprintk(KERN_CRIT "%s: emulation at %lx failed (%08x)\n",\r\n__func__, vcpu->arch.pc, vcpu->arch.last_inst);\r\nrun->hw.hardware_exit_reason = ~0ULL << 32;\r\nrun->hw.hardware_exit_reason |= vcpu->arch.last_inst;\r\nr = RESUME_HOST;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nbreak;\r\ncase BOOKE_INTERRUPT_FP_UNAVAIL:\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_FP_UNAVAIL);\r\nkvmppc_account_exit(vcpu, FP_UNAVAIL);\r\nr = RESUME_GUEST;\r\nbreak;\r\n#ifdef CONFIG_SPE\r\ncase BOOKE_INTERRUPT_SPE_UNAVAIL: {\r\nif (vcpu->arch.shared->msr & MSR_SPE)\r\nkvmppc_vcpu_enable_spe(vcpu);\r\nelse\r\nkvmppc_booke_queue_irqprio(vcpu,\r\nBOOKE_IRQPRIO_SPE_UNAVAIL);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\ncase BOOKE_INTERRUPT_SPE_FP_DATA:\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_SPE_FP_DATA);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_SPE_FP_ROUND:\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_SPE_FP_ROUND);\r\nr = RESUME_GUEST;\r\nbreak;\r\n#else\r\ncase BOOKE_INTERRUPT_SPE_UNAVAIL:\r\nkvmppc_core_queue_program(vcpu, ESR_PUO | ESR_SPV);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_SPE_FP_DATA:\r\ncase BOOKE_INTERRUPT_SPE_FP_ROUND:\r\nprintk(KERN_CRIT "%s: unexpected SPE interrupt %u at %08lx\n",\r\n__func__, exit_nr, vcpu->arch.pc);\r\nrun->hw.hardware_exit_reason = exit_nr;\r\nr = RESUME_HOST;\r\nbreak;\r\n#endif\r\ncase BOOKE_INTERRUPT_DATA_STORAGE:\r\nkvmppc_core_queue_data_storage(vcpu, vcpu->arch.fault_dear,\r\nvcpu->arch.fault_esr);\r\nkvmppc_account_exit(vcpu, DSI_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_INST_STORAGE:\r\nkvmppc_core_queue_inst_storage(vcpu, vcpu->arch.fault_esr);\r\nkvmppc_account_exit(vcpu, ISI_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_SYSCALL:\r\nif (!(vcpu->arch.shared->msr & MSR_PR) &&\r\n(((u32)kvmppc_get_gpr(vcpu, 0)) == KVM_SC_MAGIC_R0)) {\r\nkvmppc_set_gpr(vcpu, 3, kvmppc_kvm_pv(vcpu));\r\nr = RESUME_GUEST;\r\n} else {\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_SYSCALL);\r\n}\r\nkvmppc_account_exit(vcpu, SYSCALL_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\ncase BOOKE_INTERRUPT_DTLB_MISS: {\r\nunsigned long eaddr = vcpu->arch.fault_dear;\r\nint gtlb_index;\r\ngpa_t gpaddr;\r\ngfn_t gfn;\r\n#ifdef CONFIG_KVM_E500\r\nif (!(vcpu->arch.shared->msr & MSR_PR) &&\r\n(eaddr & PAGE_MASK) == vcpu->arch.magic_page_ea) {\r\nkvmppc_map_magic(vcpu);\r\nkvmppc_account_exit(vcpu, DTLB_VIRT_MISS_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\n#endif\r\ngtlb_index = kvmppc_mmu_dtlb_index(vcpu, eaddr);\r\nif (gtlb_index < 0) {\r\nkvmppc_core_queue_dtlb_miss(vcpu,\r\nvcpu->arch.fault_dear,\r\nvcpu->arch.fault_esr);\r\nkvmppc_mmu_dtlb_miss(vcpu);\r\nkvmppc_account_exit(vcpu, DTLB_REAL_MISS_EXITS);\r\nr = RESUME_GUEST;\r\nbreak;\r\n}\r\ngpaddr = kvmppc_mmu_xlate(vcpu, gtlb_index, eaddr);\r\ngfn = gpaddr >> PAGE_SHIFT;\r\nif (kvm_is_visible_gfn(vcpu->kvm, gfn)) {\r\nkvmppc_mmu_map(vcpu, eaddr, gpaddr, gtlb_index);\r\nkvmppc_account_exit(vcpu, DTLB_VIRT_MISS_EXITS);\r\nr = RESUME_GUEST;\r\n} else {\r\nvcpu->arch.paddr_accessed = gpaddr;\r\nr = kvmppc_emulate_mmio(run, vcpu);\r\nkvmppc_account_exit(vcpu, MMIO_EXITS);\r\n}\r\nbreak;\r\n}\r\ncase BOOKE_INTERRUPT_ITLB_MISS: {\r\nunsigned long eaddr = vcpu->arch.pc;\r\ngpa_t gpaddr;\r\ngfn_t gfn;\r\nint gtlb_index;\r\nr = RESUME_GUEST;\r\ngtlb_index = kvmppc_mmu_itlb_index(vcpu, eaddr);\r\nif (gtlb_index < 0) {\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_ITLB_MISS);\r\nkvmppc_mmu_itlb_miss(vcpu);\r\nkvmppc_account_exit(vcpu, ITLB_REAL_MISS_EXITS);\r\nbreak;\r\n}\r\nkvmppc_account_exit(vcpu, ITLB_VIRT_MISS_EXITS);\r\ngpaddr = kvmppc_mmu_xlate(vcpu, gtlb_index, eaddr);\r\ngfn = gpaddr >> PAGE_SHIFT;\r\nif (kvm_is_visible_gfn(vcpu->kvm, gfn)) {\r\nkvmppc_mmu_map(vcpu, eaddr, gpaddr, gtlb_index);\r\n} else {\r\nkvmppc_booke_queue_irqprio(vcpu, BOOKE_IRQPRIO_MACHINE_CHECK);\r\n}\r\nbreak;\r\n}\r\ncase BOOKE_INTERRUPT_DEBUG: {\r\nu32 dbsr;\r\nvcpu->arch.pc = mfspr(SPRN_CSRR0);\r\ndbsr = mfspr(SPRN_DBSR);\r\ndbsr &= DBSR_IAC1 | DBSR_IAC2 | DBSR_IAC3 | DBSR_IAC4;\r\nmtspr(SPRN_DBSR, dbsr);\r\nrun->exit_reason = KVM_EXIT_DEBUG;\r\nkvmppc_account_exit(vcpu, DEBUG_EXITS);\r\nr = RESUME_HOST;\r\nbreak;\r\n}\r\ndefault:\r\nprintk(KERN_EMERG "exit_nr %d\n", exit_nr);\r\nBUG();\r\n}\r\nlocal_irq_disable();\r\nkvmppc_core_deliver_interrupts(vcpu);\r\nif (!(r & RESUME_HOST)) {\r\nif (signal_pending(current)) {\r\nrun->exit_reason = KVM_EXIT_INTR;\r\nr = (-EINTR << 2) | RESUME_HOST | (r & RESUME_FLAG_NV);\r\nkvmppc_account_exit(vcpu, SIGNAL_EXITS);\r\n}\r\n}\r\nreturn r;\r\n}\r\nint kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)\r\n{\r\nint i;\r\nint r;\r\nvcpu->arch.pc = 0;\r\nvcpu->arch.shared->msr = 0;\r\nvcpu->arch.shadow_msr = MSR_USER | MSR_DE | MSR_IS | MSR_DS;\r\nkvmppc_set_gpr(vcpu, 1, (16<<20) - 8);\r\nvcpu->arch.shadow_pid = 1;\r\nvcpu->arch.ivpr = 0x55550000;\r\nfor (i = 0; i < BOOKE_IRQPRIO_MAX; i++)\r\nvcpu->arch.ivor[i] = 0x7700 | i * 4;\r\nkvmppc_init_timing_stats(vcpu);\r\nr = kvmppc_core_vcpu_setup(vcpu);\r\nkvmppc_sanity_check(vcpu);\r\nreturn r;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\r\n{\r\nint i;\r\nregs->pc = vcpu->arch.pc;\r\nregs->cr = kvmppc_get_cr(vcpu);\r\nregs->ctr = vcpu->arch.ctr;\r\nregs->lr = vcpu->arch.lr;\r\nregs->xer = kvmppc_get_xer(vcpu);\r\nregs->msr = vcpu->arch.shared->msr;\r\nregs->srr0 = vcpu->arch.shared->srr0;\r\nregs->srr1 = vcpu->arch.shared->srr1;\r\nregs->pid = vcpu->arch.pid;\r\nregs->sprg0 = vcpu->arch.shared->sprg0;\r\nregs->sprg1 = vcpu->arch.shared->sprg1;\r\nregs->sprg2 = vcpu->arch.shared->sprg2;\r\nregs->sprg3 = vcpu->arch.shared->sprg3;\r\nregs->sprg4 = vcpu->arch.sprg4;\r\nregs->sprg5 = vcpu->arch.sprg5;\r\nregs->sprg6 = vcpu->arch.sprg6;\r\nregs->sprg7 = vcpu->arch.sprg7;\r\nfor (i = 0; i < ARRAY_SIZE(regs->gpr); i++)\r\nregs->gpr[i] = kvmppc_get_gpr(vcpu, i);\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\r\n{\r\nint i;\r\nvcpu->arch.pc = regs->pc;\r\nkvmppc_set_cr(vcpu, regs->cr);\r\nvcpu->arch.ctr = regs->ctr;\r\nvcpu->arch.lr = regs->lr;\r\nkvmppc_set_xer(vcpu, regs->xer);\r\nkvmppc_set_msr(vcpu, regs->msr);\r\nvcpu->arch.shared->srr0 = regs->srr0;\r\nvcpu->arch.shared->srr1 = regs->srr1;\r\nkvmppc_set_pid(vcpu, regs->pid);\r\nvcpu->arch.shared->sprg0 = regs->sprg0;\r\nvcpu->arch.shared->sprg1 = regs->sprg1;\r\nvcpu->arch.shared->sprg2 = regs->sprg2;\r\nvcpu->arch.shared->sprg3 = regs->sprg3;\r\nvcpu->arch.sprg4 = regs->sprg4;\r\nvcpu->arch.sprg5 = regs->sprg5;\r\nvcpu->arch.sprg6 = regs->sprg6;\r\nvcpu->arch.sprg7 = regs->sprg7;\r\nfor (i = 0; i < ARRAY_SIZE(regs->gpr); i++)\r\nkvmppc_set_gpr(vcpu, i, regs->gpr[i]);\r\nreturn 0;\r\n}\r\nstatic void get_sregs_base(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nu64 tb = get_tb();\r\nsregs->u.e.features |= KVM_SREGS_E_BASE;\r\nsregs->u.e.csrr0 = vcpu->arch.csrr0;\r\nsregs->u.e.csrr1 = vcpu->arch.csrr1;\r\nsregs->u.e.mcsr = vcpu->arch.mcsr;\r\nsregs->u.e.esr = vcpu->arch.esr;\r\nsregs->u.e.dear = vcpu->arch.shared->dar;\r\nsregs->u.e.tsr = vcpu->arch.tsr;\r\nsregs->u.e.tcr = vcpu->arch.tcr;\r\nsregs->u.e.dec = kvmppc_get_dec(vcpu, tb);\r\nsregs->u.e.tb = tb;\r\nsregs->u.e.vrsave = vcpu->arch.vrsave;\r\n}\r\nstatic int set_sregs_base(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nif (!(sregs->u.e.features & KVM_SREGS_E_BASE))\r\nreturn 0;\r\nvcpu->arch.csrr0 = sregs->u.e.csrr0;\r\nvcpu->arch.csrr1 = sregs->u.e.csrr1;\r\nvcpu->arch.mcsr = sregs->u.e.mcsr;\r\nvcpu->arch.esr = sregs->u.e.esr;\r\nvcpu->arch.shared->dar = sregs->u.e.dear;\r\nvcpu->arch.vrsave = sregs->u.e.vrsave;\r\nvcpu->arch.tcr = sregs->u.e.tcr;\r\nif (sregs->u.e.update_special & KVM_SREGS_E_UPDATE_DEC)\r\nvcpu->arch.dec = sregs->u.e.dec;\r\nkvmppc_emulate_dec(vcpu);\r\nif (sregs->u.e.update_special & KVM_SREGS_E_UPDATE_TSR) {\r\nif ((sregs->u.e.tsr & TSR_DIS) &&\r\n(vcpu->arch.tcr & TCR_DIE))\r\nkvmppc_core_queue_dec(vcpu);\r\n}\r\nreturn 0;\r\n}\r\nstatic void get_sregs_arch206(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nsregs->u.e.features |= KVM_SREGS_E_ARCH206;\r\nsregs->u.e.pir = 0;\r\nsregs->u.e.mcsrr0 = vcpu->arch.mcsrr0;\r\nsregs->u.e.mcsrr1 = vcpu->arch.mcsrr1;\r\nsregs->u.e.decar = vcpu->arch.decar;\r\nsregs->u.e.ivpr = vcpu->arch.ivpr;\r\n}\r\nstatic int set_sregs_arch206(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nif (!(sregs->u.e.features & KVM_SREGS_E_ARCH206))\r\nreturn 0;\r\nif (sregs->u.e.pir != 0)\r\nreturn -EINVAL;\r\nvcpu->arch.mcsrr0 = sregs->u.e.mcsrr0;\r\nvcpu->arch.mcsrr1 = sregs->u.e.mcsrr1;\r\nvcpu->arch.decar = sregs->u.e.decar;\r\nvcpu->arch.ivpr = sregs->u.e.ivpr;\r\nreturn 0;\r\n}\r\nvoid kvmppc_get_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\r\n{\r\nsregs->u.e.features |= KVM_SREGS_E_IVOR;\r\nsregs->u.e.ivor_low[0] = vcpu->arch.ivor[BOOKE_IRQPRIO_CRITICAL];\r\nsregs->u.e.ivor_low[1] = vcpu->arch.ivor[BOOKE_IRQPRIO_MACHINE_CHECK];\r\nsregs->u.e.ivor_low[2] = vcpu->arch.ivor[BOOKE_IRQPRIO_DATA_STORAGE];\r\nsregs->u.e.ivor_low[3] = vcpu->arch.ivor[BOOKE_IRQPRIO_INST_STORAGE];\r\nsregs->u.e.ivor_low[4] = vcpu->arch.ivor[BOOKE_IRQPRIO_EXTERNAL];\r\nsregs->u.e.ivor_low[5] = vcpu->arch.ivor[BOOKE_IRQPRIO_ALIGNMENT];\r\nsregs->u.e.ivor_low[6] = vcpu->arch.ivor[BOOKE_IRQPRIO_PROGRAM];\r\nsregs->u.e.ivor_low[7] = vcpu->arch.ivor[BOOKE_IRQPRIO_FP_UNAVAIL];\r\nsregs->u.e.ivor_low[8] = vcpu->arch.ivor[BOOKE_IRQPRIO_SYSCALL];\r\nsregs->u.e.ivor_low[9] = vcpu->arch.ivor[BOOKE_IRQPRIO_AP_UNAVAIL];\r\nsregs->u.e.ivor_low[10] = vcpu->arch.ivor[BOOKE_IRQPRIO_DECREMENTER];\r\nsregs->u.e.ivor_low[11] = vcpu->arch.ivor[BOOKE_IRQPRIO_FIT];\r\nsregs->u.e.ivor_low[12] = vcpu->arch.ivor[BOOKE_IRQPRIO_WATCHDOG];\r\nsregs->u.e.ivor_low[13] = vcpu->arch.ivor[BOOKE_IRQPRIO_DTLB_MISS];\r\nsregs->u.e.ivor_low[14] = vcpu->arch.ivor[BOOKE_IRQPRIO_ITLB_MISS];\r\nsregs->u.e.ivor_low[15] = vcpu->arch.ivor[BOOKE_IRQPRIO_DEBUG];\r\n}\r\nint kvmppc_set_sregs_ivor(struct kvm_vcpu *vcpu, struct kvm_sregs *sregs)\r\n{\r\nif (!(sregs->u.e.features & KVM_SREGS_E_IVOR))\r\nreturn 0;\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_CRITICAL] = sregs->u.e.ivor_low[0];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_MACHINE_CHECK] = sregs->u.e.ivor_low[1];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_DATA_STORAGE] = sregs->u.e.ivor_low[2];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_INST_STORAGE] = sregs->u.e.ivor_low[3];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_EXTERNAL] = sregs->u.e.ivor_low[4];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_ALIGNMENT] = sregs->u.e.ivor_low[5];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_PROGRAM] = sregs->u.e.ivor_low[6];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_FP_UNAVAIL] = sregs->u.e.ivor_low[7];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_SYSCALL] = sregs->u.e.ivor_low[8];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_AP_UNAVAIL] = sregs->u.e.ivor_low[9];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_DECREMENTER] = sregs->u.e.ivor_low[10];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_FIT] = sregs->u.e.ivor_low[11];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_WATCHDOG] = sregs->u.e.ivor_low[12];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_DTLB_MISS] = sregs->u.e.ivor_low[13];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_ITLB_MISS] = sregs->u.e.ivor_low[14];\r\nvcpu->arch.ivor[BOOKE_IRQPRIO_DEBUG] = sregs->u.e.ivor_low[15];\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nsregs->pvr = vcpu->arch.pvr;\r\nget_sregs_base(vcpu, sregs);\r\nget_sregs_arch206(vcpu, sregs);\r\nkvmppc_core_get_sregs(vcpu, sregs);\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nint ret;\r\nif (vcpu->arch.pvr != sregs->pvr)\r\nreturn -EINVAL;\r\nret = set_sregs_base(vcpu, sregs);\r\nif (ret < 0)\r\nreturn ret;\r\nret = set_sregs_arch206(vcpu, sregs);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn kvmppc_core_set_sregs(vcpu, sregs);\r\n}\r\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\r\n{\r\nreturn -ENOTSUPP;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\r\n{\r\nreturn -ENOTSUPP;\r\n}\r\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\r\nstruct kvm_translation *tr)\r\n{\r\nint r;\r\nr = kvmppc_core_vcpu_translate(vcpu, tr);\r\nreturn r;\r\n}\r\nint kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)\r\n{\r\nreturn -ENOTSUPP;\r\n}\r\nint kvmppc_core_prepare_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvmppc_core_commit_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem)\r\n{\r\n}\r\nint kvmppc_core_init_vm(struct kvm *kvm)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvmppc_core_destroy_vm(struct kvm *kvm)\r\n{\r\n}\r\nint __init kvmppc_booke_init(void)\r\n{\r\nunsigned long ivor[16];\r\nunsigned long max_ivor = 0;\r\nint i;\r\nkvmppc_booke_handlers = __get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nVCPU_SIZE_ORDER);\r\nif (!kvmppc_booke_handlers)\r\nreturn -ENOMEM;\r\nivor[0] = mfspr(SPRN_IVOR0);\r\nivor[1] = mfspr(SPRN_IVOR1);\r\nivor[2] = mfspr(SPRN_IVOR2);\r\nivor[3] = mfspr(SPRN_IVOR3);\r\nivor[4] = mfspr(SPRN_IVOR4);\r\nivor[5] = mfspr(SPRN_IVOR5);\r\nivor[6] = mfspr(SPRN_IVOR6);\r\nivor[7] = mfspr(SPRN_IVOR7);\r\nivor[8] = mfspr(SPRN_IVOR8);\r\nivor[9] = mfspr(SPRN_IVOR9);\r\nivor[10] = mfspr(SPRN_IVOR10);\r\nivor[11] = mfspr(SPRN_IVOR11);\r\nivor[12] = mfspr(SPRN_IVOR12);\r\nivor[13] = mfspr(SPRN_IVOR13);\r\nivor[14] = mfspr(SPRN_IVOR14);\r\nivor[15] = mfspr(SPRN_IVOR15);\r\nfor (i = 0; i < 16; i++) {\r\nif (ivor[i] > max_ivor)\r\nmax_ivor = ivor[i];\r\nmemcpy((void *)kvmppc_booke_handlers + ivor[i],\r\nkvmppc_handlers_start + i * kvmppc_handler_len,\r\nkvmppc_handler_len);\r\n}\r\nflush_icache_range(kvmppc_booke_handlers,\r\nkvmppc_booke_handlers + max_ivor + kvmppc_handler_len);\r\nreturn 0;\r\n}\r\nvoid __exit kvmppc_booke_exit(void)\r\n{\r\nfree_pages(kvmppc_booke_handlers, VCPU_SIZE_ORDER);\r\nkvm_exit();\r\n}
