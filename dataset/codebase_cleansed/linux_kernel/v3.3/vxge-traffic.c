enum vxge_hw_status vxge_hw_vpath_intr_enable(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto exit;\r\n}\r\nvp_reg = vpath->vp_reg;\r\nwriteq(VXGE_HW_INTR_MASK_ALL, &vp_reg->kdfcctl_errors_reg);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->general_errors_reg);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->pci_config_errors_reg);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->mrpcim_to_vpath_alarm_reg);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->srpcim_to_vpath_alarm_reg);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->vpath_ppif_int_status);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->srpcim_msg_to_vpath_reg);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->vpath_pcipif_int_status);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->prc_alarm_reg);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->wrdma_alarm_status);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->asic_ntwk_vp_err_reg);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->xgmac_vp_int_status);\r\nval64 = readq(&vp_reg->vpath_general_int_status);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->vpath_pcipif_int_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->srpcim_msg_to_vpath_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->srpcim_to_vpath_alarm_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->mrpcim_to_vpath_alarm_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->pci_config_errors_mask);\r\nwriteq((u32)vxge_bVALn((VXGE_HW_GENERAL_ERRORS_REG_DBLGEN_FIFO1_OVRFLOW|\r\nVXGE_HW_GENERAL_ERRORS_REG_DBLGEN_FIFO2_OVRFLOW|\r\nVXGE_HW_GENERAL_ERRORS_REG_STATSB_DROP_TIMEOUT_REQ|\r\nVXGE_HW_GENERAL_ERRORS_REG_STATSB_PIF_CHAIN_ERR), 0, 32),\r\n&vp_reg->general_errors_mask);\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32)vxge_bVALn((VXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO1_OVRWR|\r\nVXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO2_OVRWR|\r\nVXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO1_POISON|\r\nVXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO2_POISON|\r\nVXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO1_DMA_ERR|\r\nVXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO2_DMA_ERR), 0, 32),\r\n&vp_reg->kdfcctl_errors_mask);\r\n__vxge_hw_pio_mem_write32_upper(0, &vp_reg->vpath_ppif_int_mask);\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32)vxge_bVALn(VXGE_HW_PRC_ALARM_REG_PRC_RING_BUMP, 0, 32),\r\n&vp_reg->prc_alarm_mask);\r\n__vxge_hw_pio_mem_write32_upper(0, &vp_reg->wrdma_alarm_mask);\r\n__vxge_hw_pio_mem_write32_upper(0, &vp_reg->xgmac_vp_int_mask);\r\nif (vpath->hldev->first_vp_id != vpath->vp_id)\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->asic_ntwk_vp_err_mask);\r\nelse\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn((\r\nVXGE_HW_ASIC_NTWK_VP_ERR_REG_XMACJ_NTWK_REAFFIRMED_FAULT |\r\nVXGE_HW_ASIC_NTWK_VP_ERR_REG_XMACJ_NTWK_REAFFIRMED_OK), 0, 32),\r\n&vp_reg->asic_ntwk_vp_err_mask);\r\n__vxge_hw_pio_mem_write32_upper(0,\r\n&vp_reg->vpath_general_int_mask);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_intr_disable(\r\nstruct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nif (vpath->vp_open == VXGE_HW_VP_NOT_OPEN) {\r\nstatus = VXGE_HW_ERR_VPATH_NOT_OPEN;\r\ngoto exit;\r\n}\r\nvp_reg = vpath->vp_reg;\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->vpath_general_int_mask);\r\nval64 = VXGE_HW_TIM_CLR_INT_EN_VP(1 << (16 - vpath->vp_id));\r\nwriteq(VXGE_HW_INTR_MASK_ALL, &vp_reg->kdfcctl_errors_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->general_errors_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->pci_config_errors_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->mrpcim_to_vpath_alarm_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->srpcim_to_vpath_alarm_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->vpath_ppif_int_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->srpcim_msg_to_vpath_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->vpath_pcipif_int_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->wrdma_alarm_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->prc_alarm_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->xgmac_vp_int_mask);\r\n__vxge_hw_pio_mem_write32_upper((u32)VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->asic_ntwk_vp_err_mask);\r\nexit:\r\nreturn status;\r\n}\r\nvoid vxge_hw_vpath_tti_ci_set(struct __vxge_hw_fifo *fifo)\r\n{\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nstruct vxge_hw_vp_config *config;\r\nu64 val64;\r\nif (fifo->config->enable != VXGE_HW_FIFO_ENABLE)\r\nreturn;\r\nvp_reg = fifo->vp_reg;\r\nconfig = container_of(fifo->config, struct vxge_hw_vp_config, fifo);\r\nif (config->tti.timer_ci_en != VXGE_HW_TIM_TIMER_CI_ENABLE) {\r\nconfig->tti.timer_ci_en = VXGE_HW_TIM_TIMER_CI_ENABLE;\r\nval64 = readq(&vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_TX]);\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_TIMER_CI;\r\nfifo->tim_tti_cfg1_saved = val64;\r\nwriteq(val64, &vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_TX]);\r\n}\r\n}\r\nvoid vxge_hw_vpath_dynamic_rti_ci_set(struct __vxge_hw_ring *ring)\r\n{\r\nu64 val64 = ring->tim_rti_cfg1_saved;\r\nval64 |= VXGE_HW_TIM_CFG1_INT_NUM_TIMER_CI;\r\nring->tim_rti_cfg1_saved = val64;\r\nwriteq(val64, &ring->vp_reg->tim_cfg1_int_num[VXGE_HW_VPATH_INTR_RX]);\r\n}\r\nvoid vxge_hw_vpath_dynamic_tti_rtimer_set(struct __vxge_hw_fifo *fifo)\r\n{\r\nu64 val64 = fifo->tim_tti_cfg3_saved;\r\nu64 timer = (fifo->rtimer * 1000) / 272;\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_RTIMER_VAL(0x3ffffff);\r\nif (timer)\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_RTIMER_VAL(timer) |\r\nVXGE_HW_TIM_CFG3_INT_NUM_RTIMER_EVENT_SF(5);\r\nwriteq(val64, &fifo->vp_reg->tim_cfg3_int_num[VXGE_HW_VPATH_INTR_TX]);\r\n}\r\nvoid vxge_hw_vpath_dynamic_rti_rtimer_set(struct __vxge_hw_ring *ring)\r\n{\r\nu64 val64 = ring->tim_rti_cfg3_saved;\r\nu64 timer = (ring->rtimer * 1000) / 272;\r\nval64 &= ~VXGE_HW_TIM_CFG3_INT_NUM_RTIMER_VAL(0x3ffffff);\r\nif (timer)\r\nval64 |= VXGE_HW_TIM_CFG3_INT_NUM_RTIMER_VAL(timer) |\r\nVXGE_HW_TIM_CFG3_INT_NUM_RTIMER_EVENT_SF(4);\r\nwriteq(val64, &ring->vp_reg->tim_cfg3_int_num[VXGE_HW_VPATH_INTR_RX]);\r\n}\r\nvoid vxge_hw_channel_msix_mask(struct __vxge_hw_channel *channel, int msix_id)\r\n{\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32)vxge_bVALn(vxge_mBIT(msix_id >> 2), 0, 32),\r\n&channel->common_reg->set_msix_mask_vect[msix_id%4]);\r\n}\r\nvoid\r\nvxge_hw_channel_msix_unmask(struct __vxge_hw_channel *channel, int msix_id)\r\n{\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32)vxge_bVALn(vxge_mBIT(msix_id >> 2), 0, 32),\r\n&channel->common_reg->clear_msix_mask_vect[msix_id%4]);\r\n}\r\nvoid vxge_hw_channel_msix_clear(struct __vxge_hw_channel *channel, int msix_id)\r\n{\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32) vxge_bVALn(vxge_mBIT(msix_id >> 2), 0, 32),\r\n&channel->common_reg->clr_msix_one_shot_vec[msix_id % 4]);\r\n}\r\nu32 vxge_hw_device_set_intr_type(struct __vxge_hw_device *hldev, u32 intr_mode)\r\n{\r\nif ((intr_mode != VXGE_HW_INTR_MODE_IRQLINE) &&\r\n(intr_mode != VXGE_HW_INTR_MODE_MSIX) &&\r\n(intr_mode != VXGE_HW_INTR_MODE_MSIX_ONE_SHOT) &&\r\n(intr_mode != VXGE_HW_INTR_MODE_DEF))\r\nintr_mode = VXGE_HW_INTR_MODE_IRQLINE;\r\nhldev->config.intr_mode = intr_mode;\r\nreturn intr_mode;\r\n}\r\nvoid vxge_hw_device_intr_enable(struct __vxge_hw_device *hldev)\r\n{\r\nu32 i;\r\nu64 val64;\r\nu32 val32;\r\nvxge_hw_device_mask_all(hldev);\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!(hldev->vpaths_deployed & vxge_mBIT(i)))\r\ncontinue;\r\nvxge_hw_vpath_intr_enable(\r\nVXGE_HW_VIRTUAL_PATH_HANDLE(&hldev->virtual_paths[i]));\r\n}\r\nif (hldev->config.intr_mode == VXGE_HW_INTR_MODE_IRQLINE) {\r\nval64 = hldev->tim_int_mask0[VXGE_HW_VPATH_INTR_TX] |\r\nhldev->tim_int_mask0[VXGE_HW_VPATH_INTR_RX];\r\nif (val64 != 0) {\r\nwriteq(val64, &hldev->common_reg->tim_int_status0);\r\nwriteq(~val64, &hldev->common_reg->tim_int_mask0);\r\n}\r\nval32 = hldev->tim_int_mask1[VXGE_HW_VPATH_INTR_TX] |\r\nhldev->tim_int_mask1[VXGE_HW_VPATH_INTR_RX];\r\nif (val32 != 0) {\r\n__vxge_hw_pio_mem_write32_upper(val32,\r\n&hldev->common_reg->tim_int_status1);\r\n__vxge_hw_pio_mem_write32_upper(~val32,\r\n&hldev->common_reg->tim_int_mask1);\r\n}\r\n}\r\nval64 = readq(&hldev->common_reg->titan_general_int_status);\r\nvxge_hw_device_unmask_all(hldev);\r\n}\r\nvoid vxge_hw_device_intr_disable(struct __vxge_hw_device *hldev)\r\n{\r\nu32 i;\r\nvxge_hw_device_mask_all(hldev);\r\nwriteq(VXGE_HW_INTR_MASK_ALL, &hldev->common_reg->tim_int_mask0);\r\n__vxge_hw_pio_mem_write32_upper(VXGE_HW_DEFAULT_32,\r\n&hldev->common_reg->tim_int_mask1);\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!(hldev->vpaths_deployed & vxge_mBIT(i)))\r\ncontinue;\r\nvxge_hw_vpath_intr_disable(\r\nVXGE_HW_VIRTUAL_PATH_HANDLE(&hldev->virtual_paths[i]));\r\n}\r\n}\r\nvoid vxge_hw_device_mask_all(struct __vxge_hw_device *hldev)\r\n{\r\nu64 val64;\r\nval64 = VXGE_HW_TITAN_MASK_ALL_INT_ALARM |\r\nVXGE_HW_TITAN_MASK_ALL_INT_TRAFFIC;\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn(val64, 0, 32),\r\n&hldev->common_reg->titan_mask_all_int);\r\n}\r\nvoid vxge_hw_device_unmask_all(struct __vxge_hw_device *hldev)\r\n{\r\nu64 val64 = 0;\r\nif (hldev->config.intr_mode == VXGE_HW_INTR_MODE_IRQLINE)\r\nval64 = VXGE_HW_TITAN_MASK_ALL_INT_TRAFFIC;\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn(val64, 0, 32),\r\n&hldev->common_reg->titan_mask_all_int);\r\n}\r\nvoid vxge_hw_device_flush_io(struct __vxge_hw_device *hldev)\r\n{\r\nu32 val32;\r\nval32 = readl(&hldev->common_reg->titan_general_int_status);\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_handle_error(struct __vxge_hw_device *hldev, u32 vp_id,\r\nenum vxge_hw_event type)\r\n{\r\nswitch (type) {\r\ncase VXGE_HW_EVENT_UNKNOWN:\r\nbreak;\r\ncase VXGE_HW_EVENT_RESET_START:\r\ncase VXGE_HW_EVENT_RESET_COMPLETE:\r\ncase VXGE_HW_EVENT_LINK_DOWN:\r\ncase VXGE_HW_EVENT_LINK_UP:\r\ngoto out;\r\ncase VXGE_HW_EVENT_ALARM_CLEARED:\r\ngoto out;\r\ncase VXGE_HW_EVENT_ECCERR:\r\ncase VXGE_HW_EVENT_MRPCIM_ECCERR:\r\ngoto out;\r\ncase VXGE_HW_EVENT_FIFO_ERR:\r\ncase VXGE_HW_EVENT_VPATH_ERR:\r\ncase VXGE_HW_EVENT_CRITICAL_ERR:\r\ncase VXGE_HW_EVENT_SERR:\r\nbreak;\r\ncase VXGE_HW_EVENT_SRPCIM_SERR:\r\ncase VXGE_HW_EVENT_MRPCIM_SERR:\r\ngoto out;\r\ncase VXGE_HW_EVENT_SLOT_FREEZE:\r\nbreak;\r\ndefault:\r\nvxge_assert(0);\r\ngoto out;\r\n}\r\nif (hldev->uld_callbacks->crit_err)\r\nhldev->uld_callbacks->crit_err(\r\n(struct __vxge_hw_device *)hldev,\r\ntype, vp_id);\r\nout:\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_handle_link_down_ind(struct __vxge_hw_device *hldev)\r\n{\r\nif (hldev->link_state == VXGE_HW_LINK_DOWN)\r\ngoto exit;\r\nhldev->link_state = VXGE_HW_LINK_DOWN;\r\nif (hldev->uld_callbacks->link_down)\r\nhldev->uld_callbacks->link_down(hldev);\r\nexit:\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_device_handle_link_up_ind(struct __vxge_hw_device *hldev)\r\n{\r\nif (hldev->link_state == VXGE_HW_LINK_UP)\r\ngoto exit;\r\nhldev->link_state = VXGE_HW_LINK_UP;\r\nif (hldev->uld_callbacks->link_up)\r\nhldev->uld_callbacks->link_up(hldev);\r\nexit:\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\n__vxge_hw_vpath_alarm_process(struct __vxge_hw_virtualpath *vpath,\r\nu32 skip_alarms)\r\n{\r\nu64 val64;\r\nu64 alarm_status;\r\nu64 pic_status;\r\nstruct __vxge_hw_device *hldev = NULL;\r\nenum vxge_hw_event alarm_event = VXGE_HW_EVENT_UNKNOWN;\r\nu64 mask64;\r\nstruct vxge_hw_vpath_stats_sw_info *sw_stats;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg;\r\nif (vpath == NULL) {\r\nalarm_event = VXGE_HW_SET_LEVEL(VXGE_HW_EVENT_UNKNOWN,\r\nalarm_event);\r\ngoto out2;\r\n}\r\nhldev = vpath->hldev;\r\nvp_reg = vpath->vp_reg;\r\nalarm_status = readq(&vp_reg->vpath_general_int_status);\r\nif (alarm_status == VXGE_HW_ALL_FOXES) {\r\nalarm_event = VXGE_HW_SET_LEVEL(VXGE_HW_EVENT_SLOT_FREEZE,\r\nalarm_event);\r\ngoto out;\r\n}\r\nsw_stats = vpath->sw_stats;\r\nif (alarm_status & ~(\r\nVXGE_HW_VPATH_GENERAL_INT_STATUS_PIC_INT |\r\nVXGE_HW_VPATH_GENERAL_INT_STATUS_PCI_INT |\r\nVXGE_HW_VPATH_GENERAL_INT_STATUS_WRDMA_INT |\r\nVXGE_HW_VPATH_GENERAL_INT_STATUS_XMAC_INT)) {\r\nsw_stats->error_stats.unknown_alarms++;\r\nalarm_event = VXGE_HW_SET_LEVEL(VXGE_HW_EVENT_UNKNOWN,\r\nalarm_event);\r\ngoto out;\r\n}\r\nif (alarm_status & VXGE_HW_VPATH_GENERAL_INT_STATUS_XMAC_INT) {\r\nval64 = readq(&vp_reg->xgmac_vp_int_status);\r\nif (val64 &\r\nVXGE_HW_XGMAC_VP_INT_STATUS_ASIC_NTWK_VP_ERR_ASIC_NTWK_VP_INT) {\r\nval64 = readq(&vp_reg->asic_ntwk_vp_err_reg);\r\nif (((val64 &\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_FLT) &&\r\n(!(val64 &\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_OK))) ||\r\n((val64 &\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_FLT_OCCURR) &&\r\n(!(val64 &\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_OK_OCCURR)\r\n))) {\r\nsw_stats->error_stats.network_sustained_fault++;\r\nwriteq(\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_FLT,\r\n&vp_reg->asic_ntwk_vp_err_mask);\r\n__vxge_hw_device_handle_link_down_ind(hldev);\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_LINK_DOWN, alarm_event);\r\n}\r\nif (((val64 &\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_OK) &&\r\n(!(val64 &\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_FLT))) ||\r\n((val64 &\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_OK_OCCURR) &&\r\n(!(val64 &\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_FLT_OCCURR)\r\n))) {\r\nsw_stats->error_stats.network_sustained_ok++;\r\nwriteq(\r\nVXGE_HW_ASIC_NW_VP_ERR_REG_XMACJ_STN_OK,\r\n&vp_reg->asic_ntwk_vp_err_mask);\r\n__vxge_hw_device_handle_link_up_ind(hldev);\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_LINK_UP, alarm_event);\r\n}\r\nwriteq(VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->asic_ntwk_vp_err_reg);\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_ALARM_CLEARED, alarm_event);\r\nif (skip_alarms)\r\nreturn VXGE_HW_OK;\r\n}\r\n}\r\nif (alarm_status & VXGE_HW_VPATH_GENERAL_INT_STATUS_PIC_INT) {\r\npic_status = readq(&vp_reg->vpath_ppif_int_status);\r\nif (pic_status &\r\nVXGE_HW_VPATH_PPIF_INT_STATUS_GENERAL_ERRORS_GENERAL_INT) {\r\nval64 = readq(&vp_reg->general_errors_reg);\r\nmask64 = readq(&vp_reg->general_errors_mask);\r\nif ((val64 &\r\nVXGE_HW_GENERAL_ERRORS_REG_INI_SERR_DET) &\r\n~mask64) {\r\nsw_stats->error_stats.ini_serr_det++;\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_SERR, alarm_event);\r\n}\r\nif ((val64 &\r\nVXGE_HW_GENERAL_ERRORS_REG_DBLGEN_FIFO0_OVRFLOW) &\r\n~mask64) {\r\nsw_stats->error_stats.dblgen_fifo0_overflow++;\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_FIFO_ERR, alarm_event);\r\n}\r\nif ((val64 &\r\nVXGE_HW_GENERAL_ERRORS_REG_STATSB_PIF_CHAIN_ERR) &\r\n~mask64)\r\nsw_stats->error_stats.statsb_pif_chain_error++;\r\nif ((val64 &\r\nVXGE_HW_GENERAL_ERRORS_REG_STATSB_DROP_TIMEOUT_REQ) &\r\n~mask64)\r\nsw_stats->error_stats.statsb_drop_timeout++;\r\nif ((val64 &\r\nVXGE_HW_GENERAL_ERRORS_REG_TGT_ILLEGAL_ACCESS) &\r\n~mask64)\r\nsw_stats->error_stats.target_illegal_access++;\r\nif (!skip_alarms) {\r\nwriteq(VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->general_errors_reg);\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_ALARM_CLEARED,\r\nalarm_event);\r\n}\r\n}\r\nif (pic_status &\r\nVXGE_HW_VPATH_PPIF_INT_STATUS_KDFCCTL_ERRORS_KDFCCTL_INT) {\r\nval64 = readq(&vp_reg->kdfcctl_errors_reg);\r\nmask64 = readq(&vp_reg->kdfcctl_errors_mask);\r\nif ((val64 &\r\nVXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO0_OVRWR) &\r\n~mask64) {\r\nsw_stats->error_stats.kdfcctl_fifo0_overwrite++;\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_FIFO_ERR,\r\nalarm_event);\r\n}\r\nif ((val64 &\r\nVXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO0_POISON) &\r\n~mask64) {\r\nsw_stats->error_stats.kdfcctl_fifo0_poison++;\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_FIFO_ERR,\r\nalarm_event);\r\n}\r\nif ((val64 &\r\nVXGE_HW_KDFCCTL_ERRORS_REG_KDFCCTL_FIFO0_DMA_ERR) &\r\n~mask64) {\r\nsw_stats->error_stats.kdfcctl_fifo0_dma_error++;\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_FIFO_ERR,\r\nalarm_event);\r\n}\r\nif (!skip_alarms) {\r\nwriteq(VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->kdfcctl_errors_reg);\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_ALARM_CLEARED,\r\nalarm_event);\r\n}\r\n}\r\n}\r\nif (alarm_status & VXGE_HW_VPATH_GENERAL_INT_STATUS_WRDMA_INT) {\r\nval64 = readq(&vp_reg->wrdma_alarm_status);\r\nif (val64 & VXGE_HW_WRDMA_ALARM_STATUS_PRC_ALARM_PRC_INT) {\r\nval64 = readq(&vp_reg->prc_alarm_reg);\r\nmask64 = readq(&vp_reg->prc_alarm_mask);\r\nif ((val64 & VXGE_HW_PRC_ALARM_REG_PRC_RING_BUMP)&\r\n~mask64)\r\nsw_stats->error_stats.prc_ring_bumps++;\r\nif ((val64 & VXGE_HW_PRC_ALARM_REG_PRC_RXDCM_SC_ERR) &\r\n~mask64) {\r\nsw_stats->error_stats.prc_rxdcm_sc_err++;\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_VPATH_ERR,\r\nalarm_event);\r\n}\r\nif ((val64 & VXGE_HW_PRC_ALARM_REG_PRC_RXDCM_SC_ABORT)\r\n& ~mask64) {\r\nsw_stats->error_stats.prc_rxdcm_sc_abort++;\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_VPATH_ERR,\r\nalarm_event);\r\n}\r\nif ((val64 & VXGE_HW_PRC_ALARM_REG_PRC_QUANTA_SIZE_ERR)\r\n& ~mask64) {\r\nsw_stats->error_stats.prc_quanta_size_err++;\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_VPATH_ERR,\r\nalarm_event);\r\n}\r\nif (!skip_alarms) {\r\nwriteq(VXGE_HW_INTR_MASK_ALL,\r\n&vp_reg->prc_alarm_reg);\r\nalarm_event = VXGE_HW_SET_LEVEL(\r\nVXGE_HW_EVENT_ALARM_CLEARED,\r\nalarm_event);\r\n}\r\n}\r\n}\r\nout:\r\nhldev->stats.sw_dev_err_stats.vpath_alarms++;\r\nout2:\r\nif ((alarm_event == VXGE_HW_EVENT_ALARM_CLEARED) ||\r\n(alarm_event == VXGE_HW_EVENT_UNKNOWN))\r\nreturn VXGE_HW_OK;\r\n__vxge_hw_device_handle_error(hldev, vpath->vp_id, alarm_event);\r\nif (alarm_event == VXGE_HW_EVENT_SERR)\r\nreturn VXGE_HW_ERR_CRITICAL;\r\nreturn (alarm_event == VXGE_HW_EVENT_SLOT_FREEZE) ?\r\nVXGE_HW_ERR_SLOT_FREEZE :\r\n(alarm_event == VXGE_HW_EVENT_FIFO_ERR) ? VXGE_HW_ERR_FIFO :\r\nVXGE_HW_ERR_VPATH;\r\n}\r\nenum vxge_hw_status vxge_hw_device_begin_irq(struct __vxge_hw_device *hldev,\r\nu32 skip_alarms, u64 *reason)\r\n{\r\nu32 i;\r\nu64 val64;\r\nu64 adapter_status;\r\nu64 vpath_mask;\r\nenum vxge_hw_status ret = VXGE_HW_OK;\r\nval64 = readq(&hldev->common_reg->titan_general_int_status);\r\nif (unlikely(!val64)) {\r\n*reason = 0;\r\nret = VXGE_HW_ERR_WRONG_IRQ;\r\ngoto exit;\r\n}\r\nif (unlikely(val64 == VXGE_HW_ALL_FOXES)) {\r\nadapter_status = readq(&hldev->common_reg->adapter_status);\r\nif (adapter_status == VXGE_HW_ALL_FOXES) {\r\n__vxge_hw_device_handle_error(hldev,\r\nNULL_VPID, VXGE_HW_EVENT_SLOT_FREEZE);\r\n*reason = 0;\r\nret = VXGE_HW_ERR_SLOT_FREEZE;\r\ngoto exit;\r\n}\r\n}\r\nhldev->stats.sw_dev_info_stats.total_intr_cnt++;\r\n*reason = val64;\r\nvpath_mask = hldev->vpaths_deployed >>\r\n(64 - VXGE_HW_MAX_VIRTUAL_PATHS);\r\nif (val64 &\r\nVXGE_HW_TITAN_GENERAL_INT_STATUS_VPATH_TRAFFIC_INT(vpath_mask)) {\r\nhldev->stats.sw_dev_info_stats.traffic_intr_cnt++;\r\nreturn VXGE_HW_OK;\r\n}\r\nhldev->stats.sw_dev_info_stats.not_traffic_intr_cnt++;\r\nif (unlikely(val64 &\r\nVXGE_HW_TITAN_GENERAL_INT_STATUS_VPATH_ALARM_INT)) {\r\nenum vxge_hw_status error_level = VXGE_HW_OK;\r\nhldev->stats.sw_dev_err_stats.vpath_alarms++;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!(hldev->vpaths_deployed & vxge_mBIT(i)))\r\ncontinue;\r\nret = __vxge_hw_vpath_alarm_process(\r\n&hldev->virtual_paths[i], skip_alarms);\r\nerror_level = VXGE_HW_SET_LEVEL(ret, error_level);\r\nif (unlikely((ret == VXGE_HW_ERR_CRITICAL) ||\r\n(ret == VXGE_HW_ERR_SLOT_FREEZE)))\r\nbreak;\r\n}\r\nret = error_level;\r\n}\r\nexit:\r\nreturn ret;\r\n}\r\nvoid vxge_hw_device_clear_tx_rx(struct __vxge_hw_device *hldev)\r\n{\r\nif ((hldev->tim_int_mask0[VXGE_HW_VPATH_INTR_TX] != 0) ||\r\n(hldev->tim_int_mask0[VXGE_HW_VPATH_INTR_RX] != 0)) {\r\nwriteq((hldev->tim_int_mask0[VXGE_HW_VPATH_INTR_TX] |\r\nhldev->tim_int_mask0[VXGE_HW_VPATH_INTR_RX]),\r\n&hldev->common_reg->tim_int_status0);\r\n}\r\nif ((hldev->tim_int_mask1[VXGE_HW_VPATH_INTR_TX] != 0) ||\r\n(hldev->tim_int_mask1[VXGE_HW_VPATH_INTR_RX] != 0)) {\r\n__vxge_hw_pio_mem_write32_upper(\r\n(hldev->tim_int_mask1[VXGE_HW_VPATH_INTR_TX] |\r\nhldev->tim_int_mask1[VXGE_HW_VPATH_INTR_RX]),\r\n&hldev->common_reg->tim_int_status1);\r\n}\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_hw_channel_dtr_alloc(struct __vxge_hw_channel *channel, void **dtrh)\r\n{\r\nvoid **tmp_arr;\r\nif (channel->reserve_ptr - channel->reserve_top > 0) {\r\n_alloc_after_swap:\r\n*dtrh = channel->reserve_arr[--channel->reserve_ptr];\r\nreturn VXGE_HW_OK;\r\n}\r\nif (channel->length - channel->free_ptr > 0) {\r\ntmp_arr = channel->reserve_arr;\r\nchannel->reserve_arr = channel->free_arr;\r\nchannel->free_arr = tmp_arr;\r\nchannel->reserve_ptr = channel->length;\r\nchannel->reserve_top = channel->free_ptr;\r\nchannel->free_ptr = channel->length;\r\nchannel->stats->reserve_free_swaps_cnt++;\r\ngoto _alloc_after_swap;\r\n}\r\nchannel->stats->full_cnt++;\r\n*dtrh = NULL;\r\nreturn VXGE_HW_INF_OUT_OF_DESCRIPTORS;\r\n}\r\nstatic void\r\nvxge_hw_channel_dtr_post(struct __vxge_hw_channel *channel, void *dtrh)\r\n{\r\nvxge_assert(channel->work_arr[channel->post_index] == NULL);\r\nchannel->work_arr[channel->post_index++] = dtrh;\r\nif (channel->post_index == channel->length)\r\nchannel->post_index = 0;\r\n}\r\nvoid\r\nvxge_hw_channel_dtr_try_complete(struct __vxge_hw_channel *channel, void **dtrh)\r\n{\r\nvxge_assert(channel->compl_index < channel->length);\r\n*dtrh = channel->work_arr[channel->compl_index];\r\nprefetch(*dtrh);\r\n}\r\nvoid vxge_hw_channel_dtr_complete(struct __vxge_hw_channel *channel)\r\n{\r\nchannel->work_arr[channel->compl_index] = NULL;\r\nif (++channel->compl_index == channel->length)\r\nchannel->compl_index = 0;\r\nchannel->stats->total_compl_cnt++;\r\n}\r\nvoid vxge_hw_channel_dtr_free(struct __vxge_hw_channel *channel, void *dtrh)\r\n{\r\nchannel->free_arr[--channel->free_ptr] = dtrh;\r\n}\r\nint vxge_hw_channel_dtr_count(struct __vxge_hw_channel *channel)\r\n{\r\nreturn (channel->reserve_ptr - channel->reserve_top) +\r\n(channel->length - channel->free_ptr);\r\n}\r\nenum vxge_hw_status vxge_hw_ring_rxd_reserve(struct __vxge_hw_ring *ring,\r\nvoid **rxdh)\r\n{\r\nenum vxge_hw_status status;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &ring->channel;\r\nstatus = vxge_hw_channel_dtr_alloc(channel, rxdh);\r\nif (status == VXGE_HW_OK) {\r\nstruct vxge_hw_ring_rxd_1 *rxdp =\r\n(struct vxge_hw_ring_rxd_1 *)*rxdh;\r\nrxdp->control_0 = rxdp->control_1 = 0;\r\n}\r\nreturn status;\r\n}\r\nvoid vxge_hw_ring_rxd_free(struct __vxge_hw_ring *ring, void *rxdh)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &ring->channel;\r\nvxge_hw_channel_dtr_free(channel, rxdh);\r\n}\r\nvoid vxge_hw_ring_rxd_pre_post(struct __vxge_hw_ring *ring, void *rxdh)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &ring->channel;\r\nvxge_hw_channel_dtr_post(channel, rxdh);\r\n}\r\nvoid vxge_hw_ring_rxd_post_post(struct __vxge_hw_ring *ring, void *rxdh)\r\n{\r\nstruct vxge_hw_ring_rxd_1 *rxdp = (struct vxge_hw_ring_rxd_1 *)rxdh;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &ring->channel;\r\nrxdp->control_0 = VXGE_HW_RING_RXD_LIST_OWN_ADAPTER;\r\nif (ring->stats->common_stats.usage_cnt > 0)\r\nring->stats->common_stats.usage_cnt--;\r\n}\r\nvoid vxge_hw_ring_rxd_post(struct __vxge_hw_ring *ring, void *rxdh)\r\n{\r\nstruct vxge_hw_ring_rxd_1 *rxdp = (struct vxge_hw_ring_rxd_1 *)rxdh;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &ring->channel;\r\nwmb();\r\nrxdp->control_0 = VXGE_HW_RING_RXD_LIST_OWN_ADAPTER;\r\nvxge_hw_channel_dtr_post(channel, rxdh);\r\nif (ring->stats->common_stats.usage_cnt > 0)\r\nring->stats->common_stats.usage_cnt--;\r\n}\r\nvoid vxge_hw_ring_rxd_post_post_wmb(struct __vxge_hw_ring *ring, void *rxdh)\r\n{\r\nwmb();\r\nvxge_hw_ring_rxd_post_post(ring, rxdh);\r\n}\r\nenum vxge_hw_status vxge_hw_ring_rxd_next_completed(\r\nstruct __vxge_hw_ring *ring, void **rxdh, u8 *t_code)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nstruct vxge_hw_ring_rxd_1 *rxdp;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nu64 control_0, own;\r\nchannel = &ring->channel;\r\nvxge_hw_channel_dtr_try_complete(channel, rxdh);\r\nrxdp = *rxdh;\r\nif (rxdp == NULL) {\r\nstatus = VXGE_HW_INF_NO_MORE_COMPLETED_DESCRIPTORS;\r\ngoto exit;\r\n}\r\ncontrol_0 = rxdp->control_0;\r\nown = control_0 & VXGE_HW_RING_RXD_LIST_OWN_ADAPTER;\r\n*t_code = (u8)VXGE_HW_RING_RXD_T_CODE_GET(control_0);\r\nif (!own || *t_code == VXGE_HW_RING_T_CODE_FRM_DROP) {\r\nvxge_assert(((struct vxge_hw_ring_rxd_1 *)rxdp)->host_control !=\r\n0);\r\n++ring->cmpl_cnt;\r\nvxge_hw_channel_dtr_complete(channel);\r\nvxge_assert(*t_code != VXGE_HW_RING_RXD_T_CODE_UNUSED);\r\nring->stats->common_stats.usage_cnt++;\r\nif (ring->stats->common_stats.usage_max <\r\nring->stats->common_stats.usage_cnt)\r\nring->stats->common_stats.usage_max =\r\nring->stats->common_stats.usage_cnt;\r\nstatus = VXGE_HW_OK;\r\ngoto exit;\r\n}\r\n*rxdh = NULL;\r\nstatus = VXGE_HW_INF_NO_MORE_COMPLETED_DESCRIPTORS;\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_ring_handle_tcode(\r\nstruct __vxge_hw_ring *ring, void *rxdh, u8 t_code)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nchannel = &ring->channel;\r\nif (t_code == VXGE_HW_RING_T_CODE_OK ||\r\nt_code == VXGE_HW_RING_T_CODE_L3_PKT_ERR) {\r\nstatus = VXGE_HW_OK;\r\ngoto exit;\r\n}\r\nif (t_code > VXGE_HW_RING_T_CODE_MULTI_ERR) {\r\nstatus = VXGE_HW_ERR_INVALID_TCODE;\r\ngoto exit;\r\n}\r\nring->stats->rxd_t_code_err_cnt[t_code]++;\r\nexit:\r\nreturn status;\r\n}\r\nstatic void __vxge_hw_non_offload_db_post(struct __vxge_hw_fifo *fifo,\r\nu64 txdl_ptr, u32 num_txds, u32 no_snoop)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &fifo->channel;\r\nwriteq(VXGE_HW_NODBW_TYPE(VXGE_HW_NODBW_TYPE_NODBW) |\r\nVXGE_HW_NODBW_LAST_TXD_NUMBER(num_txds) |\r\nVXGE_HW_NODBW_GET_NO_SNOOP(no_snoop),\r\n&fifo->nofl_db->control_0);\r\nmmiowb();\r\nwriteq(txdl_ptr, &fifo->nofl_db->txdl_ptr);\r\nmmiowb();\r\n}\r\nu32 vxge_hw_fifo_free_txdl_count_get(struct __vxge_hw_fifo *fifoh)\r\n{\r\nreturn vxge_hw_channel_dtr_count(&fifoh->channel);\r\n}\r\nenum vxge_hw_status vxge_hw_fifo_txdl_reserve(\r\nstruct __vxge_hw_fifo *fifo,\r\nvoid **txdlh, void **txdl_priv)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nenum vxge_hw_status status;\r\nint i;\r\nchannel = &fifo->channel;\r\nstatus = vxge_hw_channel_dtr_alloc(channel, txdlh);\r\nif (status == VXGE_HW_OK) {\r\nstruct vxge_hw_fifo_txd *txdp =\r\n(struct vxge_hw_fifo_txd *)*txdlh;\r\nstruct __vxge_hw_fifo_txdl_priv *priv;\r\npriv = __vxge_hw_fifo_txdl_priv(fifo, txdp);\r\npriv->align_dma_offset = 0;\r\npriv->align_vaddr_start = priv->align_vaddr;\r\npriv->align_used_frags = 0;\r\npriv->frags = 0;\r\npriv->alloc_frags = fifo->config->max_frags;\r\npriv->next_txdl_priv = NULL;\r\n*txdl_priv = (void *)(size_t)txdp->host_control;\r\nfor (i = 0; i < fifo->config->max_frags; i++) {\r\ntxdp = ((struct vxge_hw_fifo_txd *)*txdlh) + i;\r\ntxdp->control_0 = txdp->control_1 = 0;\r\n}\r\n}\r\nreturn status;\r\n}\r\nvoid vxge_hw_fifo_txdl_buffer_set(struct __vxge_hw_fifo *fifo,\r\nvoid *txdlh, u32 frag_idx,\r\ndma_addr_t dma_pointer, u32 size)\r\n{\r\nstruct __vxge_hw_fifo_txdl_priv *txdl_priv;\r\nstruct vxge_hw_fifo_txd *txdp, *txdp_last;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &fifo->channel;\r\ntxdl_priv = __vxge_hw_fifo_txdl_priv(fifo, txdlh);\r\ntxdp = (struct vxge_hw_fifo_txd *)txdlh + txdl_priv->frags;\r\nif (frag_idx != 0)\r\ntxdp->control_0 = txdp->control_1 = 0;\r\nelse {\r\ntxdp->control_0 |= VXGE_HW_FIFO_TXD_GATHER_CODE(\r\nVXGE_HW_FIFO_TXD_GATHER_CODE_FIRST);\r\ntxdp->control_1 |= fifo->interrupt_type;\r\ntxdp->control_1 |= VXGE_HW_FIFO_TXD_INT_NUMBER(\r\nfifo->tx_intr_num);\r\nif (txdl_priv->frags) {\r\ntxdp_last = (struct vxge_hw_fifo_txd *)txdlh +\r\n(txdl_priv->frags - 1);\r\ntxdp_last->control_0 |= VXGE_HW_FIFO_TXD_GATHER_CODE(\r\nVXGE_HW_FIFO_TXD_GATHER_CODE_LAST);\r\n}\r\n}\r\nvxge_assert(frag_idx < txdl_priv->alloc_frags);\r\ntxdp->buffer_pointer = (u64)dma_pointer;\r\ntxdp->control_0 |= VXGE_HW_FIFO_TXD_BUFFER_SIZE(size);\r\nfifo->stats->total_buffers++;\r\ntxdl_priv->frags++;\r\n}\r\nvoid vxge_hw_fifo_txdl_post(struct __vxge_hw_fifo *fifo, void *txdlh)\r\n{\r\nstruct __vxge_hw_fifo_txdl_priv *txdl_priv;\r\nstruct vxge_hw_fifo_txd *txdp_last;\r\nstruct vxge_hw_fifo_txd *txdp_first;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &fifo->channel;\r\ntxdl_priv = __vxge_hw_fifo_txdl_priv(fifo, txdlh);\r\ntxdp_first = txdlh;\r\ntxdp_last = (struct vxge_hw_fifo_txd *)txdlh + (txdl_priv->frags - 1);\r\ntxdp_last->control_0 |=\r\nVXGE_HW_FIFO_TXD_GATHER_CODE(VXGE_HW_FIFO_TXD_GATHER_CODE_LAST);\r\ntxdp_first->control_0 |= VXGE_HW_FIFO_TXD_LIST_OWN_ADAPTER;\r\nvxge_hw_channel_dtr_post(&fifo->channel, txdlh);\r\n__vxge_hw_non_offload_db_post(fifo,\r\n(u64)txdl_priv->dma_addr,\r\ntxdl_priv->frags - 1,\r\nfifo->no_snoop_bits);\r\nfifo->stats->total_posts++;\r\nfifo->stats->common_stats.usage_cnt++;\r\nif (fifo->stats->common_stats.usage_max <\r\nfifo->stats->common_stats.usage_cnt)\r\nfifo->stats->common_stats.usage_max =\r\nfifo->stats->common_stats.usage_cnt;\r\n}\r\nenum vxge_hw_status vxge_hw_fifo_txdl_next_completed(\r\nstruct __vxge_hw_fifo *fifo, void **txdlh,\r\nenum vxge_hw_fifo_tcode *t_code)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nstruct vxge_hw_fifo_txd *txdp;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nchannel = &fifo->channel;\r\nvxge_hw_channel_dtr_try_complete(channel, txdlh);\r\ntxdp = *txdlh;\r\nif (txdp == NULL) {\r\nstatus = VXGE_HW_INF_NO_MORE_COMPLETED_DESCRIPTORS;\r\ngoto exit;\r\n}\r\nif (!(txdp->control_0 & VXGE_HW_FIFO_TXD_LIST_OWN_ADAPTER)) {\r\nvxge_assert(txdp->host_control != 0);\r\nvxge_hw_channel_dtr_complete(channel);\r\n*t_code = (u8)VXGE_HW_FIFO_TXD_T_CODE_GET(txdp->control_0);\r\nif (fifo->stats->common_stats.usage_cnt > 0)\r\nfifo->stats->common_stats.usage_cnt--;\r\nstatus = VXGE_HW_OK;\r\ngoto exit;\r\n}\r\n*txdlh = NULL;\r\nstatus = VXGE_HW_INF_NO_MORE_COMPLETED_DESCRIPTORS;\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_fifo_handle_tcode(struct __vxge_hw_fifo *fifo,\r\nvoid *txdlh,\r\nenum vxge_hw_fifo_tcode t_code)\r\n{\r\nstruct __vxge_hw_channel *channel;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nchannel = &fifo->channel;\r\nif (((t_code & 0x7) < 0) || ((t_code & 0x7) > 0x4)) {\r\nstatus = VXGE_HW_ERR_INVALID_TCODE;\r\ngoto exit;\r\n}\r\nfifo->stats->txd_t_code_err_cnt[t_code]++;\r\nexit:\r\nreturn status;\r\n}\r\nvoid vxge_hw_fifo_txdl_free(struct __vxge_hw_fifo *fifo, void *txdlh)\r\n{\r\nstruct __vxge_hw_fifo_txdl_priv *txdl_priv;\r\nu32 max_frags;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &fifo->channel;\r\ntxdl_priv = __vxge_hw_fifo_txdl_priv(fifo,\r\n(struct vxge_hw_fifo_txd *)txdlh);\r\nmax_frags = fifo->config->max_frags;\r\nvxge_hw_channel_dtr_free(channel, txdlh);\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_vid_add(struct __vxge_hw_vpath_handle *vp, u64 vid)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_vpath_rts_table_set(vp,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_ADD_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_VID,\r\n0, VXGE_HW_RTS_ACCESS_STEER_DATA0_VLAN_ID(vid), 0);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_vid_get(struct __vxge_hw_vpath_handle *vp, u64 *vid)\r\n{\r\nu64 data;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_vpath_rts_table_get(vp,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_LIST_FIRST_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_VID,\r\n0, vid, &data);\r\n*vid = VXGE_HW_RTS_ACCESS_STEER_DATA0_GET_VLAN_ID(*vid);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_vid_delete(struct __vxge_hw_vpath_handle *vp, u64 vid)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_vpath_rts_table_set(vp,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_ACTION_DELETE_ENTRY,\r\nVXGE_HW_RTS_ACCESS_STEER_CTRL_DATA_STRUCT_SEL_VID,\r\n0, VXGE_HW_RTS_ACCESS_STEER_DATA0_VLAN_ID(vid), 0);\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_promisc_enable(\r\nstruct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((vp == NULL) || (vp->vpath->ringh == NULL)) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nif (!(vpath->hldev->access_rights &\r\nVXGE_HW_DEVICE_ACCESS_RIGHT_MRPCIM))\r\nreturn VXGE_HW_OK;\r\nval64 = readq(&vpath->vp_reg->rxmac_vcfg0);\r\nif (!(val64 & VXGE_HW_RXMAC_VCFG0_UCAST_ALL_ADDR_EN)) {\r\nval64 |= VXGE_HW_RXMAC_VCFG0_UCAST_ALL_ADDR_EN |\r\nVXGE_HW_RXMAC_VCFG0_MCAST_ALL_ADDR_EN |\r\nVXGE_HW_RXMAC_VCFG0_BCAST_EN |\r\nVXGE_HW_RXMAC_VCFG0_ALL_VID_EN;\r\nwriteq(val64, &vpath->vp_reg->rxmac_vcfg0);\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_promisc_disable(\r\nstruct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((vp == NULL) || (vp->vpath->ringh == NULL)) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nval64 = readq(&vpath->vp_reg->rxmac_vcfg0);\r\nif (val64 & VXGE_HW_RXMAC_VCFG0_UCAST_ALL_ADDR_EN) {\r\nval64 &= ~(VXGE_HW_RXMAC_VCFG0_UCAST_ALL_ADDR_EN |\r\nVXGE_HW_RXMAC_VCFG0_MCAST_ALL_ADDR_EN |\r\nVXGE_HW_RXMAC_VCFG0_ALL_VID_EN);\r\nwriteq(val64, &vpath->vp_reg->rxmac_vcfg0);\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_bcast_enable(\r\nstruct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((vp == NULL) || (vp->vpath->ringh == NULL)) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nval64 = readq(&vpath->vp_reg->rxmac_vcfg0);\r\nif (!(val64 & VXGE_HW_RXMAC_VCFG0_BCAST_EN)) {\r\nval64 |= VXGE_HW_RXMAC_VCFG0_BCAST_EN;\r\nwriteq(val64, &vpath->vp_reg->rxmac_vcfg0);\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_mcast_enable(\r\nstruct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((vp == NULL) || (vp->vpath->ringh == NULL)) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nval64 = readq(&vpath->vp_reg->rxmac_vcfg0);\r\nif (!(val64 & VXGE_HW_RXMAC_VCFG0_MCAST_ALL_ADDR_EN)) {\r\nval64 |= VXGE_HW_RXMAC_VCFG0_MCAST_ALL_ADDR_EN;\r\nwriteq(val64, &vpath->vp_reg->rxmac_vcfg0);\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status\r\nvxge_hw_vpath_mcast_disable(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif ((vp == NULL) || (vp->vpath->ringh == NULL)) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nvpath = vp->vpath;\r\nval64 = readq(&vpath->vp_reg->rxmac_vcfg0);\r\nif (val64 & VXGE_HW_RXMAC_VCFG0_MCAST_ALL_ADDR_EN) {\r\nval64 &= ~VXGE_HW_RXMAC_VCFG0_MCAST_ALL_ADDR_EN;\r\nwriteq(val64, &vpath->vp_reg->rxmac_vcfg0);\r\n}\r\nexit:\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_alarm_process(\r\nstruct __vxge_hw_vpath_handle *vp,\r\nu32 skip_alarms)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nif (vp == NULL) {\r\nstatus = VXGE_HW_ERR_INVALID_HANDLE;\r\ngoto exit;\r\n}\r\nstatus = __vxge_hw_vpath_alarm_process(vp->vpath, skip_alarms);\r\nexit:\r\nreturn status;\r\n}\r\nvoid\r\nvxge_hw_vpath_msix_set(struct __vxge_hw_vpath_handle *vp, int *tim_msix_id,\r\nint alarm_msix_id)\r\n{\r\nu64 val64;\r\nstruct __vxge_hw_virtualpath *vpath = vp->vpath;\r\nstruct vxge_hw_vpath_reg __iomem *vp_reg = vpath->vp_reg;\r\nu32 vp_id = vp->vpath->vp_id;\r\nval64 = VXGE_HW_INTERRUPT_CFG0_GROUP0_MSIX_FOR_TXTI(\r\n(vp_id * 4) + tim_msix_id[0]) |\r\nVXGE_HW_INTERRUPT_CFG0_GROUP1_MSIX_FOR_TXTI(\r\n(vp_id * 4) + tim_msix_id[1]);\r\nwriteq(val64, &vp_reg->interrupt_cfg0);\r\nwriteq(VXGE_HW_INTERRUPT_CFG2_ALARM_MAP_TO_MSG(\r\n(vpath->hldev->first_vp_id * 4) + alarm_msix_id),\r\n&vp_reg->interrupt_cfg2);\r\nif (vpath->hldev->config.intr_mode ==\r\nVXGE_HW_INTR_MODE_MSIX_ONE_SHOT) {\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn(\r\nVXGE_HW_ONE_SHOT_VECT0_EN_ONE_SHOT_VECT0_EN,\r\n0, 32), &vp_reg->one_shot_vect0_en);\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn(\r\nVXGE_HW_ONE_SHOT_VECT1_EN_ONE_SHOT_VECT1_EN,\r\n0, 32), &vp_reg->one_shot_vect1_en);\r\n__vxge_hw_pio_mem_write32_upper((u32)vxge_bVALn(\r\nVXGE_HW_ONE_SHOT_VECT2_EN_ONE_SHOT_VECT2_EN,\r\n0, 32), &vp_reg->one_shot_vect2_en);\r\n}\r\n}\r\nvoid\r\nvxge_hw_vpath_msix_mask(struct __vxge_hw_vpath_handle *vp, int msix_id)\r\n{\r\nstruct __vxge_hw_device *hldev = vp->vpath->hldev;\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32) vxge_bVALn(vxge_mBIT(msix_id >> 2), 0, 32),\r\n&hldev->common_reg->set_msix_mask_vect[msix_id % 4]);\r\n}\r\nvoid vxge_hw_vpath_msix_clear(struct __vxge_hw_vpath_handle *vp, int msix_id)\r\n{\r\nstruct __vxge_hw_device *hldev = vp->vpath->hldev;\r\nif ((hldev->config.intr_mode == VXGE_HW_INTR_MODE_MSIX_ONE_SHOT))\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32) vxge_bVALn(vxge_mBIT((msix_id >> 2)), 0, 32),\r\n&hldev->common_reg->clr_msix_one_shot_vec[msix_id % 4]);\r\nelse\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32) vxge_bVALn(vxge_mBIT((msix_id >> 2)), 0, 32),\r\n&hldev->common_reg->clear_msix_mask_vect[msix_id % 4]);\r\n}\r\nvoid\r\nvxge_hw_vpath_msix_unmask(struct __vxge_hw_vpath_handle *vp, int msix_id)\r\n{\r\nstruct __vxge_hw_device *hldev = vp->vpath->hldev;\r\n__vxge_hw_pio_mem_write32_upper(\r\n(u32)vxge_bVALn(vxge_mBIT(msix_id >> 2), 0, 32),\r\n&hldev->common_reg->clear_msix_mask_vect[msix_id%4]);\r\n}\r\nvoid vxge_hw_vpath_inta_mask_tx_rx(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 tim_int_mask0[4] = {[0 ...3] = 0};\r\nu32 tim_int_mask1[4] = {[0 ...3] = 0};\r\nu64 val64;\r\nstruct __vxge_hw_device *hldev = vp->vpath->hldev;\r\nVXGE_HW_DEVICE_TIM_INT_MASK_SET(tim_int_mask0,\r\ntim_int_mask1, vp->vpath->vp_id);\r\nval64 = readq(&hldev->common_reg->tim_int_mask0);\r\nif ((tim_int_mask0[VXGE_HW_VPATH_INTR_TX] != 0) ||\r\n(tim_int_mask0[VXGE_HW_VPATH_INTR_RX] != 0)) {\r\nwriteq((tim_int_mask0[VXGE_HW_VPATH_INTR_TX] |\r\ntim_int_mask0[VXGE_HW_VPATH_INTR_RX] | val64),\r\n&hldev->common_reg->tim_int_mask0);\r\n}\r\nval64 = readl(&hldev->common_reg->tim_int_mask1);\r\nif ((tim_int_mask1[VXGE_HW_VPATH_INTR_TX] != 0) ||\r\n(tim_int_mask1[VXGE_HW_VPATH_INTR_RX] != 0)) {\r\n__vxge_hw_pio_mem_write32_upper(\r\n(tim_int_mask1[VXGE_HW_VPATH_INTR_TX] |\r\ntim_int_mask1[VXGE_HW_VPATH_INTR_RX] | val64),\r\n&hldev->common_reg->tim_int_mask1);\r\n}\r\n}\r\nvoid vxge_hw_vpath_inta_unmask_tx_rx(struct __vxge_hw_vpath_handle *vp)\r\n{\r\nu64 tim_int_mask0[4] = {[0 ...3] = 0};\r\nu32 tim_int_mask1[4] = {[0 ...3] = 0};\r\nu64 val64;\r\nstruct __vxge_hw_device *hldev = vp->vpath->hldev;\r\nVXGE_HW_DEVICE_TIM_INT_MASK_SET(tim_int_mask0,\r\ntim_int_mask1, vp->vpath->vp_id);\r\nval64 = readq(&hldev->common_reg->tim_int_mask0);\r\nif ((tim_int_mask0[VXGE_HW_VPATH_INTR_TX] != 0) ||\r\n(tim_int_mask0[VXGE_HW_VPATH_INTR_RX] != 0)) {\r\nwriteq((~(tim_int_mask0[VXGE_HW_VPATH_INTR_TX] |\r\ntim_int_mask0[VXGE_HW_VPATH_INTR_RX])) & val64,\r\n&hldev->common_reg->tim_int_mask0);\r\n}\r\nif ((tim_int_mask1[VXGE_HW_VPATH_INTR_TX] != 0) ||\r\n(tim_int_mask1[VXGE_HW_VPATH_INTR_RX] != 0)) {\r\n__vxge_hw_pio_mem_write32_upper(\r\n(~(tim_int_mask1[VXGE_HW_VPATH_INTR_TX] |\r\ntim_int_mask1[VXGE_HW_VPATH_INTR_RX])) & val64,\r\n&hldev->common_reg->tim_int_mask1);\r\n}\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_poll_rx(struct __vxge_hw_ring *ring)\r\n{\r\nu8 t_code;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nvoid *first_rxdh;\r\nu64 val64 = 0;\r\nint new_count = 0;\r\nring->cmpl_cnt = 0;\r\nstatus = vxge_hw_ring_rxd_next_completed(ring, &first_rxdh, &t_code);\r\nif (status == VXGE_HW_OK)\r\nring->callback(ring, first_rxdh,\r\nt_code, ring->channel.userdata);\r\nif (ring->cmpl_cnt != 0) {\r\nring->doorbell_cnt += ring->cmpl_cnt;\r\nif (ring->doorbell_cnt >= ring->rxds_limit) {\r\nnew_count = (ring->doorbell_cnt * 4);\r\nring->total_db_cnt += ring->doorbell_cnt;\r\nif (ring->total_db_cnt >= ring->rxds_per_block) {\r\nnew_count += 4;\r\nring->total_db_cnt %= ring->rxds_per_block;\r\n}\r\nwriteq(VXGE_HW_PRC_RXD_DOORBELL_NEW_QW_CNT(new_count),\r\n&ring->vp_reg->prc_rxd_doorbell);\r\nval64 =\r\nreadl(&ring->common_reg->titan_general_int_status);\r\nring->doorbell_cnt = 0;\r\n}\r\n}\r\nreturn status;\r\n}\r\nenum vxge_hw_status vxge_hw_vpath_poll_tx(struct __vxge_hw_fifo *fifo,\r\nstruct sk_buff ***skb_ptr, int nr_skb,\r\nint *more)\r\n{\r\nenum vxge_hw_fifo_tcode t_code;\r\nvoid *first_txdlh;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct __vxge_hw_channel *channel;\r\nchannel = &fifo->channel;\r\nstatus = vxge_hw_fifo_txdl_next_completed(fifo,\r\n&first_txdlh, &t_code);\r\nif (status == VXGE_HW_OK)\r\nif (fifo->callback(fifo, first_txdlh, t_code,\r\nchannel->userdata, skb_ptr, nr_skb, more) != VXGE_HW_OK)\r\nstatus = VXGE_HW_COMPLETIONS_REMAIN;\r\nreturn status;\r\n}
