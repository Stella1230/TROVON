int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)\r\n{\r\nreturn !(v->arch.shared->msr & MSR_WE) ||\r\n!!(v->arch.pending_exceptions);\r\n}\r\nint kvmppc_kvm_pv(struct kvm_vcpu *vcpu)\r\n{\r\nint nr = kvmppc_get_gpr(vcpu, 11);\r\nint r;\r\nunsigned long __maybe_unused param1 = kvmppc_get_gpr(vcpu, 3);\r\nunsigned long __maybe_unused param2 = kvmppc_get_gpr(vcpu, 4);\r\nunsigned long __maybe_unused param3 = kvmppc_get_gpr(vcpu, 5);\r\nunsigned long __maybe_unused param4 = kvmppc_get_gpr(vcpu, 6);\r\nunsigned long r2 = 0;\r\nif (!(vcpu->arch.shared->msr & MSR_SF)) {\r\nparam1 &= 0xffffffff;\r\nparam2 &= 0xffffffff;\r\nparam3 &= 0xffffffff;\r\nparam4 &= 0xffffffff;\r\n}\r\nswitch (nr) {\r\ncase HC_VENDOR_KVM | KVM_HC_PPC_MAP_MAGIC_PAGE:\r\n{\r\nvcpu->arch.magic_page_pa = param1;\r\nvcpu->arch.magic_page_ea = param2;\r\nr2 = KVM_MAGIC_FEAT_SR;\r\nr = HC_EV_SUCCESS;\r\nbreak;\r\n}\r\ncase HC_VENDOR_KVM | KVM_HC_FEATURES:\r\nr = HC_EV_SUCCESS;\r\n#if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500)\r\nr2 |= (1 << KVM_FEATURE_MAGIC_PAGE);\r\n#endif\r\nbreak;\r\ndefault:\r\nr = HC_EV_UNIMPLEMENTED;\r\nbreak;\r\n}\r\nkvmppc_set_gpr(vcpu, 4, r2);\r\nreturn r;\r\n}\r\nint kvmppc_sanity_check(struct kvm_vcpu *vcpu)\r\n{\r\nint r = false;\r\nif (!vcpu->arch.pvr)\r\ngoto out;\r\nif ((vcpu->arch.cpu_type != KVM_CPU_3S_64) && vcpu->arch.papr_enabled)\r\ngoto out;\r\n#ifdef CONFIG_KVM_BOOK3S_64_HV\r\nif (!vcpu->arch.papr_enabled)\r\ngoto out;\r\n#endif\r\nr = true;\r\nout:\r\nvcpu->arch.sane = r;\r\nreturn r ? 0 : -EINVAL;\r\n}\r\nint kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)\r\n{\r\nenum emulation_result er;\r\nint r;\r\ner = kvmppc_emulate_instruction(run, vcpu);\r\nswitch (er) {\r\ncase EMULATE_DONE:\r\nr = RESUME_GUEST_NV;\r\nbreak;\r\ncase EMULATE_DO_MMIO:\r\nrun->exit_reason = KVM_EXIT_MMIO;\r\nr = RESUME_HOST_NV;\r\nbreak;\r\ncase EMULATE_FAIL:\r\nprintk(KERN_EMERG "%s: emulation failed (%08x)\n", __func__,\r\nkvmppc_get_last_inst(vcpu));\r\nr = RESUME_HOST;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn r;\r\n}\r\nint kvm_arch_hardware_enable(void *garbage)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_hardware_disable(void *garbage)\r\n{\r\n}\r\nint kvm_arch_hardware_setup(void)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_hardware_unsetup(void)\r\n{\r\n}\r\nvoid kvm_arch_check_processor_compat(void *rtn)\r\n{\r\n*(int *)rtn = kvmppc_core_check_processor_compat();\r\n}\r\nint kvm_arch_init_vm(struct kvm *kvm)\r\n{\r\nreturn kvmppc_core_init_vm(kvm);\r\n}\r\nvoid kvm_arch_destroy_vm(struct kvm *kvm)\r\n{\r\nunsigned int i;\r\nstruct kvm_vcpu *vcpu;\r\nkvm_for_each_vcpu(i, vcpu, kvm)\r\nkvm_arch_vcpu_free(vcpu);\r\nmutex_lock(&kvm->lock);\r\nfor (i = 0; i < atomic_read(&kvm->online_vcpus); i++)\r\nkvm->vcpus[i] = NULL;\r\natomic_set(&kvm->online_vcpus, 0);\r\nkvmppc_core_destroy_vm(kvm);\r\nmutex_unlock(&kvm->lock);\r\n}\r\nvoid kvm_arch_sync_events(struct kvm *kvm)\r\n{\r\n}\r\nint kvm_dev_ioctl_check_extension(long ext)\r\n{\r\nint r;\r\nswitch (ext) {\r\n#ifdef CONFIG_BOOKE\r\ncase KVM_CAP_PPC_BOOKE_SREGS:\r\n#else\r\ncase KVM_CAP_PPC_SEGSTATE:\r\ncase KVM_CAP_PPC_PAPR:\r\n#endif\r\ncase KVM_CAP_PPC_UNSET_IRQ:\r\ncase KVM_CAP_PPC_IRQ_LEVEL:\r\ncase KVM_CAP_ENABLE_CAP:\r\nr = 1;\r\nbreak;\r\n#ifndef CONFIG_KVM_BOOK3S_64_HV\r\ncase KVM_CAP_PPC_PAIRED_SINGLES:\r\ncase KVM_CAP_PPC_OSI:\r\ncase KVM_CAP_PPC_GET_PVINFO:\r\nr = 1;\r\nbreak;\r\ncase KVM_CAP_COALESCED_MMIO:\r\nr = KVM_COALESCED_MMIO_PAGE_OFFSET;\r\nbreak;\r\n#endif\r\n#ifdef CONFIG_KVM_BOOK3S_64_HV\r\ncase KVM_CAP_SPAPR_TCE:\r\nr = 1;\r\nbreak;\r\ncase KVM_CAP_PPC_SMT:\r\nr = threads_per_core;\r\nbreak;\r\ncase KVM_CAP_PPC_RMA:\r\nr = 1;\r\nif (cpu_has_feature(CPU_FTR_ARCH_201))\r\nr = 2;\r\nbreak;\r\n#endif\r\ndefault:\r\nr = 0;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nlong kvm_arch_dev_ioctl(struct file *filp,\r\nunsigned int ioctl, unsigned long arg)\r\n{\r\nreturn -EINVAL;\r\n}\r\nint kvm_arch_prepare_memory_region(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot,\r\nstruct kvm_memory_slot old,\r\nstruct kvm_userspace_memory_region *mem,\r\nint user_alloc)\r\n{\r\nreturn kvmppc_core_prepare_memory_region(kvm, mem);\r\n}\r\nvoid kvm_arch_commit_memory_region(struct kvm *kvm,\r\nstruct kvm_userspace_memory_region *mem,\r\nstruct kvm_memory_slot old,\r\nint user_alloc)\r\n{\r\nkvmppc_core_commit_memory_region(kvm, mem);\r\n}\r\nvoid kvm_arch_flush_shadow(struct kvm *kvm)\r\n{\r\n}\r\nstruct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nvcpu = kvmppc_core_vcpu_create(kvm, id);\r\nvcpu->arch.wqp = &vcpu->wq;\r\nif (!IS_ERR(vcpu))\r\nkvmppc_create_vcpu_debugfs(vcpu, id);\r\nreturn vcpu;\r\n}\r\nvoid kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)\r\n{\r\nhrtimer_cancel(&vcpu->arch.dec_timer);\r\ntasklet_kill(&vcpu->arch.tasklet);\r\nkvmppc_remove_vcpu_debugfs(vcpu);\r\nkvmppc_core_vcpu_free(vcpu);\r\n}\r\nvoid kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nkvm_arch_vcpu_free(vcpu);\r\n}\r\nint kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvmppc_core_pending_dec(vcpu);\r\n}\r\nstatic void kvmppc_decrementer_func(unsigned long data)\r\n{\r\nstruct kvm_vcpu *vcpu = (struct kvm_vcpu *)data;\r\nkvmppc_core_queue_dec(vcpu);\r\nif (waitqueue_active(vcpu->arch.wqp)) {\r\nwake_up_interruptible(vcpu->arch.wqp);\r\nvcpu->stat.halt_wakeup++;\r\n}\r\n}\r\nenum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)\r\n{\r\nstruct kvm_vcpu *vcpu;\r\nvcpu = container_of(timer, struct kvm_vcpu, arch.dec_timer);\r\ntasklet_schedule(&vcpu->arch.tasklet);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nint kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nhrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);\r\ntasklet_init(&vcpu->arch.tasklet, kvmppc_decrementer_func, (ulong)vcpu);\r\nvcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;\r\nvcpu->arch.dec_expires = ~(u64)0;\r\n#ifdef CONFIG_KVM_EXIT_TIMING\r\nmutex_init(&vcpu->arch.exit_timing_lock);\r\n#endif\r\nreturn 0;\r\n}\r\nvoid kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_mmu_destroy(vcpu);\r\n}\r\nvoid kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\n#ifdef CONFIG_BOOKE\r\nmtspr(SPRN_VRSAVE, vcpu->arch.vrsave);\r\n#endif\r\nkvmppc_core_vcpu_load(vcpu, cpu);\r\nvcpu->cpu = smp_processor_id();\r\n}\r\nvoid kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_core_vcpu_put(vcpu);\r\n#ifdef CONFIG_BOOKE\r\nvcpu->arch.vrsave = mfspr(SPRN_VRSAVE);\r\n#endif\r\nvcpu->cpu = -1;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\r\nstruct kvm_guest_debug *dbg)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,\r\nstruct kvm_run *run)\r\n{\r\nkvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, run->dcr.data);\r\n}\r\nstatic void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,\r\nstruct kvm_run *run)\r\n{\r\nu64 uninitialized_var(gpr);\r\nif (run->mmio.len > sizeof(gpr)) {\r\nprintk(KERN_ERR "bad MMIO length: %d\n", run->mmio.len);\r\nreturn;\r\n}\r\nif (vcpu->arch.mmio_is_bigendian) {\r\nswitch (run->mmio.len) {\r\ncase 8: gpr = *(u64 *)run->mmio.data; break;\r\ncase 4: gpr = *(u32 *)run->mmio.data; break;\r\ncase 2: gpr = *(u16 *)run->mmio.data; break;\r\ncase 1: gpr = *(u8 *)run->mmio.data; break;\r\n}\r\n} else {\r\nswitch (run->mmio.len) {\r\ncase 4: gpr = ld_le32((u32 *)run->mmio.data); break;\r\ncase 2: gpr = ld_le16((u16 *)run->mmio.data); break;\r\ncase 1: gpr = *(u8 *)run->mmio.data; break;\r\n}\r\n}\r\nif (vcpu->arch.mmio_sign_extend) {\r\nswitch (run->mmio.len) {\r\n#ifdef CONFIG_PPC64\r\ncase 4:\r\ngpr = (s64)(s32)gpr;\r\nbreak;\r\n#endif\r\ncase 2:\r\ngpr = (s64)(s16)gpr;\r\nbreak;\r\ncase 1:\r\ngpr = (s64)(s8)gpr;\r\nbreak;\r\n}\r\n}\r\nkvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);\r\nswitch (vcpu->arch.io_gpr & KVM_REG_EXT_MASK) {\r\ncase KVM_REG_GPR:\r\nkvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);\r\nbreak;\r\ncase KVM_REG_FPR:\r\nvcpu->arch.fpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;\r\nbreak;\r\n#ifdef CONFIG_PPC_BOOK3S\r\ncase KVM_REG_QPR:\r\nvcpu->arch.qpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;\r\nbreak;\r\ncase KVM_REG_FQPR:\r\nvcpu->arch.fpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;\r\nvcpu->arch.qpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;\r\nbreak;\r\n#endif\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nint kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int rt, unsigned int bytes, int is_bigendian)\r\n{\r\nif (bytes > sizeof(run->mmio.data)) {\r\nprintk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\n}\r\nrun->mmio.phys_addr = vcpu->arch.paddr_accessed;\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 0;\r\nvcpu->arch.io_gpr = rt;\r\nvcpu->arch.mmio_is_bigendian = is_bigendian;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 0;\r\nvcpu->arch.mmio_sign_extend = 0;\r\nreturn EMULATE_DO_MMIO;\r\n}\r\nint kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int rt, unsigned int bytes, int is_bigendian)\r\n{\r\nint r;\r\nr = kvmppc_handle_load(run, vcpu, rt, bytes, is_bigendian);\r\nvcpu->arch.mmio_sign_extend = 1;\r\nreturn r;\r\n}\r\nint kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nu64 val, unsigned int bytes, int is_bigendian)\r\n{\r\nvoid *data = run->mmio.data;\r\nif (bytes > sizeof(run->mmio.data)) {\r\nprintk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,\r\nrun->mmio.len);\r\n}\r\nrun->mmio.phys_addr = vcpu->arch.paddr_accessed;\r\nrun->mmio.len = bytes;\r\nrun->mmio.is_write = 1;\r\nvcpu->mmio_needed = 1;\r\nvcpu->mmio_is_write = 1;\r\nif (is_bigendian) {\r\nswitch (bytes) {\r\ncase 8: *(u64 *)data = val; break;\r\ncase 4: *(u32 *)data = val; break;\r\ncase 2: *(u16 *)data = val; break;\r\ncase 1: *(u8 *)data = val; break;\r\n}\r\n} else {\r\nswitch (bytes) {\r\ncase 4: st_le32(data, val); break;\r\ncase 2: st_le16(data, val); break;\r\ncase 1: *(u8 *)data = val; break;\r\n}\r\n}\r\nreturn EMULATE_DO_MMIO;\r\n}\r\nint kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)\r\n{\r\nint r;\r\nsigset_t sigsaved;\r\nif (vcpu->sigset_active)\r\nsigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);\r\nif (vcpu->mmio_needed) {\r\nif (!vcpu->mmio_is_write)\r\nkvmppc_complete_mmio_load(vcpu, run);\r\nvcpu->mmio_needed = 0;\r\n} else if (vcpu->arch.dcr_needed) {\r\nif (!vcpu->arch.dcr_is_write)\r\nkvmppc_complete_dcr_load(vcpu, run);\r\nvcpu->arch.dcr_needed = 0;\r\n} else if (vcpu->arch.osi_needed) {\r\nu64 *gprs = run->osi.gprs;\r\nint i;\r\nfor (i = 0; i < 32; i++)\r\nkvmppc_set_gpr(vcpu, i, gprs[i]);\r\nvcpu->arch.osi_needed = 0;\r\n} else if (vcpu->arch.hcall_needed) {\r\nint i;\r\nkvmppc_set_gpr(vcpu, 3, run->papr_hcall.ret);\r\nfor (i = 0; i < 9; ++i)\r\nkvmppc_set_gpr(vcpu, 4 + i, run->papr_hcall.args[i]);\r\nvcpu->arch.hcall_needed = 0;\r\n}\r\nkvmppc_core_deliver_interrupts(vcpu);\r\nr = kvmppc_vcpu_run(run, vcpu);\r\nif (vcpu->sigset_active)\r\nsigprocmask(SIG_SETMASK, &sigsaved, NULL);\r\nreturn r;\r\n}\r\nint kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)\r\n{\r\nif (irq->irq == KVM_INTERRUPT_UNSET) {\r\nkvmppc_core_dequeue_external(vcpu, irq);\r\nreturn 0;\r\n}\r\nkvmppc_core_queue_external(vcpu, irq);\r\nif (waitqueue_active(vcpu->arch.wqp)) {\r\nwake_up_interruptible(vcpu->arch.wqp);\r\nvcpu->stat.halt_wakeup++;\r\n} else if (vcpu->cpu != -1) {\r\nsmp_send_reschedule(vcpu->cpu);\r\n}\r\nreturn 0;\r\n}\r\nstatic int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,\r\nstruct kvm_enable_cap *cap)\r\n{\r\nint r;\r\nif (cap->flags)\r\nreturn -EINVAL;\r\nswitch (cap->cap) {\r\ncase KVM_CAP_PPC_OSI:\r\nr = 0;\r\nvcpu->arch.osi_enabled = true;\r\nbreak;\r\ncase KVM_CAP_PPC_PAPR:\r\nr = 0;\r\nvcpu->arch.papr_enabled = true;\r\nbreak;\r\ndefault:\r\nr = -EINVAL;\r\nbreak;\r\n}\r\nif (!r)\r\nr = kvmppc_sanity_check(vcpu);\r\nreturn r;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,\r\nstruct kvm_mp_state *mp_state)\r\n{\r\nreturn -EINVAL;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,\r\nstruct kvm_mp_state *mp_state)\r\n{\r\nreturn -EINVAL;\r\n}\r\nlong kvm_arch_vcpu_ioctl(struct file *filp,\r\nunsigned int ioctl, unsigned long arg)\r\n{\r\nstruct kvm_vcpu *vcpu = filp->private_data;\r\nvoid __user *argp = (void __user *)arg;\r\nlong r;\r\nswitch (ioctl) {\r\ncase KVM_INTERRUPT: {\r\nstruct kvm_interrupt irq;\r\nr = -EFAULT;\r\nif (copy_from_user(&irq, argp, sizeof(irq)))\r\ngoto out;\r\nr = kvm_vcpu_ioctl_interrupt(vcpu, &irq);\r\ngoto out;\r\n}\r\ncase KVM_ENABLE_CAP:\r\n{\r\nstruct kvm_enable_cap cap;\r\nr = -EFAULT;\r\nif (copy_from_user(&cap, argp, sizeof(cap)))\r\ngoto out;\r\nr = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);\r\nbreak;\r\n}\r\ndefault:\r\nr = -EINVAL;\r\n}\r\nout:\r\nreturn r;\r\n}\r\nstatic int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)\r\n{\r\nu32 inst_lis = 0x3c000000;\r\nu32 inst_ori = 0x60000000;\r\nu32 inst_nop = 0x60000000;\r\nu32 inst_sc = 0x44000002;\r\nu32 inst_imm_mask = 0xffff;\r\npvinfo->hcall[0] = inst_lis | ((KVM_SC_MAGIC_R0 >> 16) & inst_imm_mask);\r\npvinfo->hcall[1] = inst_ori | (KVM_SC_MAGIC_R0 & inst_imm_mask);\r\npvinfo->hcall[2] = inst_sc;\r\npvinfo->hcall[3] = inst_nop;\r\nreturn 0;\r\n}\r\nlong kvm_arch_vm_ioctl(struct file *filp,\r\nunsigned int ioctl, unsigned long arg)\r\n{\r\nvoid __user *argp = (void __user *)arg;\r\nlong r;\r\nswitch (ioctl) {\r\ncase KVM_PPC_GET_PVINFO: {\r\nstruct kvm_ppc_pvinfo pvinfo;\r\nmemset(&pvinfo, 0, sizeof(pvinfo));\r\nr = kvm_vm_ioctl_get_pvinfo(&pvinfo);\r\nif (copy_to_user(argp, &pvinfo, sizeof(pvinfo))) {\r\nr = -EFAULT;\r\ngoto out;\r\n}\r\nbreak;\r\n}\r\n#ifdef CONFIG_KVM_BOOK3S_64_HV\r\ncase KVM_CREATE_SPAPR_TCE: {\r\nstruct kvm_create_spapr_tce create_tce;\r\nstruct kvm *kvm = filp->private_data;\r\nr = -EFAULT;\r\nif (copy_from_user(&create_tce, argp, sizeof(create_tce)))\r\ngoto out;\r\nr = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce);\r\ngoto out;\r\n}\r\ncase KVM_ALLOCATE_RMA: {\r\nstruct kvm *kvm = filp->private_data;\r\nstruct kvm_allocate_rma rma;\r\nr = kvm_vm_ioctl_allocate_rma(kvm, &rma);\r\nif (r >= 0 && copy_to_user(argp, &rma, sizeof(rma)))\r\nr = -EFAULT;\r\nbreak;\r\n}\r\n#endif\r\ndefault:\r\nr = -ENOTTY;\r\n}\r\nout:\r\nreturn r;\r\n}\r\nint kvm_arch_init(void *opaque)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvm_arch_exit(void)\r\n{\r\n}
