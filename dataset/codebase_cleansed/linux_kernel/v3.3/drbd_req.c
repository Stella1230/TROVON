static void _drbd_start_io_acct(struct drbd_conf *mdev, struct drbd_request *req, struct bio *bio)\r\n{\r\nconst int rw = bio_data_dir(bio);\r\nint cpu;\r\ncpu = part_stat_lock();\r\npart_stat_inc(cpu, &mdev->vdisk->part0, ios[rw]);\r\npart_stat_add(cpu, &mdev->vdisk->part0, sectors[rw], bio_sectors(bio));\r\npart_inc_in_flight(&mdev->vdisk->part0, rw);\r\npart_stat_unlock();\r\n}\r\nstatic void _drbd_end_io_acct(struct drbd_conf *mdev, struct drbd_request *req)\r\n{\r\nint rw = bio_data_dir(req->master_bio);\r\nunsigned long duration = jiffies - req->start_time;\r\nint cpu;\r\ncpu = part_stat_lock();\r\npart_stat_add(cpu, &mdev->vdisk->part0, ticks[rw], duration);\r\npart_round_stats(cpu, &mdev->vdisk->part0);\r\npart_dec_in_flight(&mdev->vdisk->part0, rw);\r\npart_stat_unlock();\r\n}\r\nstatic void _req_is_done(struct drbd_conf *mdev, struct drbd_request *req, const int rw)\r\n{\r\nconst unsigned long s = req->rq_state;\r\nlist_del(&req->tl_requests);\r\nif (rw == WRITE) {\r\nif (!(s & RQ_NET_OK) || !(s & RQ_LOCAL_OK))\r\ndrbd_set_out_of_sync(mdev, req->sector, req->size);\r\nif ((s & RQ_NET_OK) && (s & RQ_LOCAL_OK) && (s & RQ_NET_SIS))\r\ndrbd_set_in_sync(mdev, req->sector, req->size);\r\nif (s & RQ_LOCAL_MASK) {\r\nif (get_ldev_if_state(mdev, D_FAILED)) {\r\nif (s & RQ_IN_ACT_LOG)\r\ndrbd_al_complete_io(mdev, req->sector);\r\nput_ldev(mdev);\r\n} else if (__ratelimit(&drbd_ratelimit_state)) {\r\ndev_warn(DEV, "Should have called drbd_al_complete_io(, %llu), "\r\n"but my Disk seems to have failed :(\n",\r\n(unsigned long long) req->sector);\r\n}\r\n}\r\n}\r\ndrbd_req_free(req);\r\n}\r\nstatic void queue_barrier(struct drbd_conf *mdev)\r\n{\r\nstruct drbd_tl_epoch *b;\r\nif (test_bit(CREATE_BARRIER, &mdev->flags))\r\nreturn;\r\nb = mdev->newest_tle;\r\nb->w.cb = w_send_barrier;\r\ninc_ap_pending(mdev);\r\ndrbd_queue_work(&mdev->data.work, &b->w);\r\nset_bit(CREATE_BARRIER, &mdev->flags);\r\n}\r\nstatic void _about_to_complete_local_write(struct drbd_conf *mdev,\r\nstruct drbd_request *req)\r\n{\r\nconst unsigned long s = req->rq_state;\r\nstruct drbd_request *i;\r\nstruct drbd_epoch_entry *e;\r\nstruct hlist_node *n;\r\nstruct hlist_head *slot;\r\nif (mdev->state.conn >= C_CONNECTED &&\r\n(s & RQ_NET_SENT) != 0 &&\r\nreq->epoch == mdev->newest_tle->br_number)\r\nqueue_barrier(mdev);\r\nif ((s & RQ_NET_DONE) && mdev->ee_hash != NULL) {\r\nconst sector_t sector = req->sector;\r\nconst int size = req->size;\r\n#define OVERLAPS overlaps(sector, size, i->sector, i->size)\r\nslot = tl_hash_slot(mdev, sector);\r\nhlist_for_each_entry(i, n, slot, collision) {\r\nif (OVERLAPS) {\r\ndev_alert(DEV, "LOGIC BUG: completed: %p %llus +%u; "\r\n"other: %p %llus +%u\n",\r\nreq, (unsigned long long)sector, size,\r\ni, (unsigned long long)i->sector, i->size);\r\n}\r\n}\r\n#undef OVERLAPS\r\n#define OVERLAPS overlaps(sector, size, e->sector, e->size)\r\nslot = ee_hash_slot(mdev, req->sector);\r\nhlist_for_each_entry(e, n, slot, collision) {\r\nif (OVERLAPS) {\r\nwake_up(&mdev->misc_wait);\r\nbreak;\r\n}\r\n}\r\n}\r\n#undef OVERLAPS\r\n}\r\nvoid complete_master_bio(struct drbd_conf *mdev,\r\nstruct bio_and_error *m)\r\n{\r\nbio_endio(m->bio, m->error);\r\ndec_ap_bio(mdev);\r\n}\r\nvoid _req_may_be_done(struct drbd_request *req, struct bio_and_error *m)\r\n{\r\nconst unsigned long s = req->rq_state;\r\nstruct drbd_conf *mdev = req->mdev;\r\nint rw = req->master_bio ? bio_data_dir(req->master_bio) : WRITE;\r\nif (s & RQ_NET_QUEUED)\r\nreturn;\r\nif (s & RQ_NET_PENDING)\r\nreturn;\r\nif (s & RQ_LOCAL_PENDING)\r\nreturn;\r\nif (req->master_bio) {\r\nint ok = (s & RQ_LOCAL_OK) || (s & RQ_NET_OK);\r\nint error = PTR_ERR(req->private_bio);\r\nif (!hlist_unhashed(&req->collision))\r\nhlist_del(&req->collision);\r\nelse\r\nD_ASSERT((s & (RQ_NET_MASK & ~RQ_NET_DONE)) == 0);\r\nif (rw == WRITE)\r\n_about_to_complete_local_write(mdev, req);\r\n_drbd_end_io_acct(mdev, req);\r\nm->error = ok ? 0 : (error ?: -EIO);\r\nm->bio = req->master_bio;\r\nreq->master_bio = NULL;\r\n}\r\nif ((s & RQ_NET_MASK) == 0 || (s & RQ_NET_DONE)) {\r\n_req_is_done(mdev, req, rw);\r\n}\r\n}\r\nstatic void _req_may_be_done_not_susp(struct drbd_request *req, struct bio_and_error *m)\r\n{\r\nstruct drbd_conf *mdev = req->mdev;\r\nif (!is_susp(mdev->state))\r\n_req_may_be_done(req, m);\r\n}\r\nstatic int _req_conflicts(struct drbd_request *req)\r\n{\r\nstruct drbd_conf *mdev = req->mdev;\r\nconst sector_t sector = req->sector;\r\nconst int size = req->size;\r\nstruct drbd_request *i;\r\nstruct drbd_epoch_entry *e;\r\nstruct hlist_node *n;\r\nstruct hlist_head *slot;\r\nD_ASSERT(hlist_unhashed(&req->collision));\r\nif (!get_net_conf(mdev))\r\nreturn 0;\r\nERR_IF (mdev->tl_hash_s == 0)\r\ngoto out_no_conflict;\r\nBUG_ON(mdev->tl_hash == NULL);\r\n#define OVERLAPS overlaps(i->sector, i->size, sector, size)\r\nslot = tl_hash_slot(mdev, sector);\r\nhlist_for_each_entry(i, n, slot, collision) {\r\nif (OVERLAPS) {\r\ndev_alert(DEV, "%s[%u] Concurrent local write detected! "\r\n"[DISCARD L] new: %llus +%u; "\r\n"pending: %llus +%u\n",\r\ncurrent->comm, current->pid,\r\n(unsigned long long)sector, size,\r\n(unsigned long long)i->sector, i->size);\r\ngoto out_conflict;\r\n}\r\n}\r\nif (mdev->ee_hash_s) {\r\nBUG_ON(mdev->ee_hash == NULL);\r\n#undef OVERLAPS\r\n#define OVERLAPS overlaps(e->sector, e->size, sector, size)\r\nslot = ee_hash_slot(mdev, sector);\r\nhlist_for_each_entry(e, n, slot, collision) {\r\nif (OVERLAPS) {\r\ndev_alert(DEV, "%s[%u] Concurrent remote write detected!"\r\n" [DISCARD L] new: %llus +%u; "\r\n"pending: %llus +%u\n",\r\ncurrent->comm, current->pid,\r\n(unsigned long long)sector, size,\r\n(unsigned long long)e->sector, e->size);\r\ngoto out_conflict;\r\n}\r\n}\r\n}\r\n#undef OVERLAPS\r\nout_no_conflict:\r\nput_net_conf(mdev);\r\nreturn 0;\r\nout_conflict:\r\nput_net_conf(mdev);\r\nreturn 1;\r\n}\r\nint __req_mod(struct drbd_request *req, enum drbd_req_event what,\r\nstruct bio_and_error *m)\r\n{\r\nstruct drbd_conf *mdev = req->mdev;\r\nint rv = 0;\r\nm->bio = NULL;\r\nswitch (what) {\r\ndefault:\r\ndev_err(DEV, "LOGIC BUG in %s:%u\n", __FILE__ , __LINE__);\r\nbreak;\r\ncase to_be_send:\r\nD_ASSERT(!(req->rq_state & RQ_NET_MASK));\r\nreq->rq_state |= RQ_NET_PENDING;\r\ninc_ap_pending(mdev);\r\nbreak;\r\ncase to_be_submitted:\r\nD_ASSERT(!(req->rq_state & RQ_LOCAL_MASK));\r\nreq->rq_state |= RQ_LOCAL_PENDING;\r\nbreak;\r\ncase completed_ok:\r\nif (bio_data_dir(req->master_bio) == WRITE)\r\nmdev->writ_cnt += req->size>>9;\r\nelse\r\nmdev->read_cnt += req->size>>9;\r\nreq->rq_state |= (RQ_LOCAL_COMPLETED|RQ_LOCAL_OK);\r\nreq->rq_state &= ~RQ_LOCAL_PENDING;\r\n_req_may_be_done_not_susp(req, m);\r\nput_ldev(mdev);\r\nbreak;\r\ncase write_completed_with_error:\r\nreq->rq_state |= RQ_LOCAL_COMPLETED;\r\nreq->rq_state &= ~RQ_LOCAL_PENDING;\r\n__drbd_chk_io_error(mdev, false);\r\n_req_may_be_done_not_susp(req, m);\r\nput_ldev(mdev);\r\nbreak;\r\ncase read_ahead_completed_with_error:\r\nreq->rq_state |= RQ_LOCAL_COMPLETED;\r\nreq->rq_state &= ~RQ_LOCAL_PENDING;\r\n_req_may_be_done_not_susp(req, m);\r\nput_ldev(mdev);\r\nbreak;\r\ncase read_completed_with_error:\r\ndrbd_set_out_of_sync(mdev, req->sector, req->size);\r\nreq->rq_state |= RQ_LOCAL_COMPLETED;\r\nreq->rq_state &= ~RQ_LOCAL_PENDING;\r\nD_ASSERT(!(req->rq_state & RQ_NET_MASK));\r\n__drbd_chk_io_error(mdev, false);\r\nput_ldev(mdev);\r\nif (mdev->state.pdsk != D_UP_TO_DATE) {\r\n_req_may_be_done_not_susp(req, m);\r\nbreak;\r\n}\r\nreq->rq_state |= RQ_NET_PENDING;\r\ninc_ap_pending(mdev);\r\ncase queue_for_net_read:\r\nhlist_add_head(&req->collision, ar_hash_slot(mdev, req->sector));\r\nset_bit(UNPLUG_REMOTE, &mdev->flags);\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\nreq->rq_state |= RQ_NET_QUEUED;\r\nreq->w.cb = (req->rq_state & RQ_LOCAL_MASK)\r\n? w_read_retry_remote\r\n: w_send_read_req;\r\ndrbd_queue_work(&mdev->data.work, &req->w);\r\nbreak;\r\ncase queue_for_net_write:\r\nhlist_add_head(&req->collision, tl_hash_slot(mdev, req->sector));\r\nset_bit(UNPLUG_REMOTE, &mdev->flags);\r\nD_ASSERT(test_bit(CREATE_BARRIER, &mdev->flags) == 0);\r\nreq->epoch = mdev->newest_tle->br_number;\r\nmdev->newest_tle->n_writes++;\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\nreq->rq_state |= RQ_NET_QUEUED;\r\nreq->w.cb = w_send_dblock;\r\ndrbd_queue_work(&mdev->data.work, &req->w);\r\nif (mdev->newest_tle->n_writes >= mdev->net_conf->max_epoch_size)\r\nqueue_barrier(mdev);\r\nbreak;\r\ncase queue_for_send_oos:\r\nreq->rq_state |= RQ_NET_QUEUED;\r\nreq->w.cb = w_send_oos;\r\ndrbd_queue_work(&mdev->data.work, &req->w);\r\nbreak;\r\ncase oos_handed_to_network:\r\ncase send_canceled:\r\ncase send_failed:\r\nreq->rq_state &= ~RQ_NET_QUEUED;\r\n_req_may_be_done_not_susp(req, m);\r\nbreak;\r\ncase handed_over_to_network:\r\nif (bio_data_dir(req->master_bio) == WRITE)\r\natomic_add(req->size>>9, &mdev->ap_in_flight);\r\nif (bio_data_dir(req->master_bio) == WRITE &&\r\nmdev->net_conf->wire_protocol == DRBD_PROT_A) {\r\nif (req->rq_state & RQ_NET_PENDING) {\r\ndec_ap_pending(mdev);\r\nreq->rq_state &= ~RQ_NET_PENDING;\r\nreq->rq_state |= RQ_NET_OK;\r\n}\r\n}\r\nreq->rq_state &= ~RQ_NET_QUEUED;\r\nreq->rq_state |= RQ_NET_SENT;\r\n_req_may_be_done_not_susp(req, m);\r\nbreak;\r\ncase read_retry_remote_canceled:\r\nreq->rq_state &= ~RQ_NET_QUEUED;\r\ncase connection_lost_while_pending:\r\nif (req->rq_state & RQ_NET_PENDING)\r\ndec_ap_pending(mdev);\r\nreq->rq_state &= ~(RQ_NET_OK|RQ_NET_PENDING);\r\nreq->rq_state |= RQ_NET_DONE;\r\nif (req->rq_state & RQ_NET_SENT && req->rq_state & RQ_WRITE)\r\natomic_sub(req->size>>9, &mdev->ap_in_flight);\r\nif (!(req->rq_state & RQ_NET_QUEUED))\r\n_req_may_be_done(req, m);\r\nbreak;\r\ncase write_acked_by_peer_and_sis:\r\nreq->rq_state |= RQ_NET_SIS;\r\ncase conflict_discarded_by_peer:\r\nif (what == conflict_discarded_by_peer)\r\ndev_alert(DEV, "Got DiscardAck packet %llus +%u!"\r\n" DRBD is not a random data generator!\n",\r\n(unsigned long long)req->sector, req->size);\r\nreq->rq_state |= RQ_NET_DONE;\r\ncase write_acked_by_peer:\r\ncase recv_acked_by_peer:\r\nreq->rq_state |= RQ_NET_OK;\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\ndec_ap_pending(mdev);\r\natomic_sub(req->size>>9, &mdev->ap_in_flight);\r\nreq->rq_state &= ~RQ_NET_PENDING;\r\n_req_may_be_done_not_susp(req, m);\r\nbreak;\r\ncase neg_acked:\r\nif (req->rq_state & RQ_NET_PENDING) {\r\ndec_ap_pending(mdev);\r\natomic_sub(req->size>>9, &mdev->ap_in_flight);\r\n}\r\nreq->rq_state &= ~(RQ_NET_OK|RQ_NET_PENDING);\r\nreq->rq_state |= RQ_NET_DONE;\r\n_req_may_be_done_not_susp(req, m);\r\nbreak;\r\ncase fail_frozen_disk_io:\r\nif (!(req->rq_state & RQ_LOCAL_COMPLETED))\r\nbreak;\r\n_req_may_be_done(req, m);\r\nbreak;\r\ncase restart_frozen_disk_io:\r\nif (!(req->rq_state & RQ_LOCAL_COMPLETED))\r\nbreak;\r\nreq->rq_state &= ~RQ_LOCAL_COMPLETED;\r\nrv = MR_READ;\r\nif (bio_data_dir(req->master_bio) == WRITE)\r\nrv = MR_WRITE;\r\nget_ldev(mdev);\r\nreq->w.cb = w_restart_disk_io;\r\ndrbd_queue_work(&mdev->data.work, &req->w);\r\nbreak;\r\ncase resend:\r\nif (!(req->rq_state & RQ_NET_OK)) {\r\nif (req->w.cb) {\r\ndrbd_queue_work(&mdev->data.work, &req->w);\r\nrv = req->rq_state & RQ_WRITE ? MR_WRITE : MR_READ;\r\n}\r\nbreak;\r\n}\r\ncase barrier_acked:\r\nif (!(req->rq_state & RQ_WRITE))\r\nbreak;\r\nif (req->rq_state & RQ_NET_PENDING) {\r\ndev_err(DEV, "FIXME (barrier_acked but pending)\n");\r\nlist_move(&req->tl_requests, &mdev->out_of_sequence_requests);\r\n}\r\nif ((req->rq_state & RQ_NET_MASK) != 0) {\r\nreq->rq_state |= RQ_NET_DONE;\r\nif (mdev->net_conf->wire_protocol == DRBD_PROT_A)\r\natomic_sub(req->size>>9, &mdev->ap_in_flight);\r\n}\r\n_req_may_be_done(req, m);\r\nbreak;\r\ncase data_received:\r\nD_ASSERT(req->rq_state & RQ_NET_PENDING);\r\ndec_ap_pending(mdev);\r\nreq->rq_state &= ~RQ_NET_PENDING;\r\nreq->rq_state |= (RQ_NET_OK|RQ_NET_DONE);\r\n_req_may_be_done_not_susp(req, m);\r\nbreak;\r\n};\r\nreturn rv;\r\n}\r\nstatic int drbd_may_do_local_read(struct drbd_conf *mdev, sector_t sector, int size)\r\n{\r\nunsigned long sbnr, ebnr;\r\nsector_t esector, nr_sectors;\r\nif (mdev->state.disk == D_UP_TO_DATE)\r\nreturn 1;\r\nif (mdev->state.disk >= D_OUTDATED)\r\nreturn 0;\r\nif (mdev->state.disk < D_INCONSISTENT)\r\nreturn 0;\r\nnr_sectors = drbd_get_capacity(mdev->this_bdev);\r\nesector = sector + (size >> 9) - 1;\r\nD_ASSERT(sector < nr_sectors);\r\nD_ASSERT(esector < nr_sectors);\r\nsbnr = BM_SECT_TO_BIT(sector);\r\nebnr = BM_SECT_TO_BIT(esector);\r\nreturn 0 == drbd_bm_count_bits(mdev, sbnr, ebnr);\r\n}\r\nstatic int drbd_make_request_common(struct drbd_conf *mdev, struct bio *bio, unsigned long start_time)\r\n{\r\nconst int rw = bio_rw(bio);\r\nconst int size = bio->bi_size;\r\nconst sector_t sector = bio->bi_sector;\r\nstruct drbd_tl_epoch *b = NULL;\r\nstruct drbd_request *req;\r\nint local, remote, send_oos = 0;\r\nint err = -EIO;\r\nint ret = 0;\r\nreq = drbd_req_new(mdev, bio);\r\nif (!req) {\r\ndec_ap_bio(mdev);\r\ndev_err(DEV, "could not kmalloc() req\n");\r\nbio_endio(bio, -ENOMEM);\r\nreturn 0;\r\n}\r\nreq->start_time = start_time;\r\nlocal = get_ldev(mdev);\r\nif (!local) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\n}\r\nif (rw == WRITE) {\r\nremote = 1;\r\n} else {\r\nif (local) {\r\nif (!drbd_may_do_local_read(mdev, sector, size)) {\r\nlocal = 0;\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\nput_ldev(mdev);\r\n}\r\n}\r\nremote = !local && mdev->state.pdsk >= D_UP_TO_DATE;\r\n}\r\nif (rw == READA && mdev->state.disk >= D_INCONSISTENT && !local) {\r\nerr = -EWOULDBLOCK;\r\ngoto fail_and_free_req;\r\n}\r\nif (rw == WRITE && local && !test_bit(AL_SUSPENDED, &mdev->flags)) {\r\nreq->rq_state |= RQ_IN_ACT_LOG;\r\ndrbd_al_begin_io(mdev, sector);\r\n}\r\nremote = remote && drbd_should_do_remote(mdev->state);\r\nsend_oos = rw == WRITE && drbd_should_send_oos(mdev->state);\r\nD_ASSERT(!(remote && send_oos));\r\nif (!(local || remote) && !is_susp(mdev->state)) {\r\nif (__ratelimit(&drbd_ratelimit_state))\r\ndev_err(DEV, "IO ERROR: neither local nor remote disk\n");\r\ngoto fail_free_complete;\r\n}\r\nif (rw == WRITE && (remote || send_oos) &&\r\nmdev->unused_spare_tle == NULL &&\r\ntest_bit(CREATE_BARRIER, &mdev->flags)) {\r\nallocate_barrier:\r\nb = kmalloc(sizeof(struct drbd_tl_epoch), GFP_NOIO);\r\nif (!b) {\r\ndev_err(DEV, "Failed to alloc barrier.\n");\r\nerr = -ENOMEM;\r\ngoto fail_free_complete;\r\n}\r\n}\r\nspin_lock_irq(&mdev->req_lock);\r\nif (is_susp(mdev->state)) {\r\nret = 1;\r\nspin_unlock_irq(&mdev->req_lock);\r\ngoto fail_free_complete;\r\n}\r\nif (remote || send_oos) {\r\nremote = drbd_should_do_remote(mdev->state);\r\nsend_oos = rw == WRITE && drbd_should_send_oos(mdev->state);\r\nD_ASSERT(!(remote && send_oos));\r\nif (!(remote || send_oos))\r\ndev_warn(DEV, "lost connection while grabbing the req_lock!\n");\r\nif (!(local || remote)) {\r\ndev_err(DEV, "IO ERROR: neither local nor remote disk\n");\r\nspin_unlock_irq(&mdev->req_lock);\r\ngoto fail_free_complete;\r\n}\r\n}\r\nif (b && mdev->unused_spare_tle == NULL) {\r\nmdev->unused_spare_tle = b;\r\nb = NULL;\r\n}\r\nif (rw == WRITE && (remote || send_oos) &&\r\nmdev->unused_spare_tle == NULL &&\r\ntest_bit(CREATE_BARRIER, &mdev->flags)) {\r\nspin_unlock_irq(&mdev->req_lock);\r\ngoto allocate_barrier;\r\n}\r\n_drbd_start_io_acct(mdev, req, bio);\r\nif ((remote || send_oos) && mdev->unused_spare_tle &&\r\ntest_and_clear_bit(CREATE_BARRIER, &mdev->flags)) {\r\n_tl_add_barrier(mdev, mdev->unused_spare_tle);\r\nmdev->unused_spare_tle = NULL;\r\n} else {\r\nD_ASSERT(!(remote && rw == WRITE &&\r\ntest_bit(CREATE_BARRIER, &mdev->flags)));\r\n}\r\nif (remote)\r\n_req_mod(req, to_be_send);\r\nif (local)\r\n_req_mod(req, to_be_submitted);\r\nif (rw == WRITE && _req_conflicts(req))\r\ngoto fail_conflicting;\r\nlist_add_tail(&req->tl_requests, &mdev->newest_tle->requests);\r\nif (remote) {\r\n_req_mod(req, (rw == WRITE)\r\n? queue_for_net_write\r\n: queue_for_net_read);\r\n}\r\nif (send_oos && drbd_set_out_of_sync(mdev, sector, size))\r\n_req_mod(req, queue_for_send_oos);\r\nif (remote &&\r\nmdev->net_conf->on_congestion != OC_BLOCK && mdev->agreed_pro_version >= 96) {\r\nint congested = 0;\r\nif (mdev->net_conf->cong_fill &&\r\natomic_read(&mdev->ap_in_flight) >= mdev->net_conf->cong_fill) {\r\ndev_info(DEV, "Congestion-fill threshold reached\n");\r\ncongested = 1;\r\n}\r\nif (mdev->act_log->used >= mdev->net_conf->cong_extents) {\r\ndev_info(DEV, "Congestion-extents threshold reached\n");\r\ncongested = 1;\r\n}\r\nif (congested) {\r\nqueue_barrier(mdev);\r\nif (mdev->net_conf->on_congestion == OC_PULL_AHEAD)\r\n_drbd_set_state(_NS(mdev, conn, C_AHEAD), 0, NULL);\r\nelse\r\n_drbd_set_state(_NS(mdev, conn, C_DISCONNECTING), 0, NULL);\r\n}\r\n}\r\nspin_unlock_irq(&mdev->req_lock);\r\nkfree(b);\r\nif (local) {\r\nreq->private_bio->bi_bdev = mdev->ldev->backing_bdev;\r\nif (get_ldev(mdev)) {\r\nif (drbd_insert_fault(mdev, rw == WRITE ? DRBD_FAULT_DT_WR\r\n: rw == READ ? DRBD_FAULT_DT_RD\r\n: DRBD_FAULT_DT_RA))\r\nbio_endio(req->private_bio, -EIO);\r\nelse\r\ngeneric_make_request(req->private_bio);\r\nput_ldev(mdev);\r\n} else\r\nbio_endio(req->private_bio, -EIO);\r\n}\r\nreturn 0;\r\nfail_conflicting:\r\n_drbd_end_io_acct(mdev, req);\r\nspin_unlock_irq(&mdev->req_lock);\r\nif (remote)\r\ndec_ap_pending(mdev);\r\nerr = 0;\r\nfail_free_complete:\r\nif (req->rq_state & RQ_IN_ACT_LOG)\r\ndrbd_al_complete_io(mdev, sector);\r\nfail_and_free_req:\r\nif (local) {\r\nbio_put(req->private_bio);\r\nreq->private_bio = NULL;\r\nput_ldev(mdev);\r\n}\r\nif (!ret)\r\nbio_endio(bio, err);\r\ndrbd_req_free(req);\r\ndec_ap_bio(mdev);\r\nkfree(b);\r\nreturn ret;\r\n}\r\nstatic int drbd_fail_request_early(struct drbd_conf *mdev, int is_write)\r\n{\r\nif (mdev->state.role != R_PRIMARY &&\r\n(!allow_oos || is_write)) {\r\nif (__ratelimit(&drbd_ratelimit_state)) {\r\ndev_err(DEV, "Process %s[%u] tried to %s; "\r\n"since we are not in Primary state, "\r\n"we cannot allow this\n",\r\ncurrent->comm, current->pid,\r\nis_write ? "WRITE" : "READ");\r\n}\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nvoid drbd_make_request(struct request_queue *q, struct bio *bio)\r\n{\r\nunsigned int s_enr, e_enr;\r\nstruct drbd_conf *mdev = (struct drbd_conf *) q->queuedata;\r\nunsigned long start_time;\r\nif (drbd_fail_request_early(mdev, bio_data_dir(bio) & WRITE)) {\r\nbio_endio(bio, -EPERM);\r\nreturn;\r\n}\r\nstart_time = jiffies;\r\nD_ASSERT(bio->bi_size > 0);\r\nD_ASSERT((bio->bi_size & 0x1ff) == 0);\r\nD_ASSERT(bio->bi_idx == 0);\r\ns_enr = bio->bi_sector >> HT_SHIFT;\r\ne_enr = (bio->bi_sector+(bio->bi_size>>9)-1) >> HT_SHIFT;\r\nif (likely(s_enr == e_enr)) {\r\ninc_ap_bio(mdev, 1);\r\ndrbd_make_request_common(mdev, bio, start_time);\r\nreturn;\r\n}\r\nif (bio->bi_vcnt != 1 || bio->bi_idx != 0 || bio->bi_size > DRBD_MAX_BIO_SIZE) {\r\ndev_err(DEV, "bio would need to, but cannot, be split: "\r\n"(vcnt=%u,idx=%u,size=%u,sector=%llu)\n",\r\nbio->bi_vcnt, bio->bi_idx, bio->bi_size,\r\n(unsigned long long)bio->bi_sector);\r\nbio_endio(bio, -EINVAL);\r\n} else {\r\nstruct bio_pair *bp;\r\nconst sector_t sect = bio->bi_sector;\r\nconst int sps = 1 << HT_SHIFT;\r\nconst int mask = sps - 1;\r\nconst sector_t first_sectors = sps - (sect & mask);\r\nbp = bio_split(bio, first_sectors);\r\ninc_ap_bio(mdev, 3);\r\nD_ASSERT(e_enr == s_enr + 1);\r\nwhile (drbd_make_request_common(mdev, &bp->bio1, start_time))\r\ninc_ap_bio(mdev, 1);\r\nwhile (drbd_make_request_common(mdev, &bp->bio2, start_time))\r\ninc_ap_bio(mdev, 1);\r\ndec_ap_bio(mdev);\r\nbio_pair_release(bp);\r\n}\r\n}\r\nint drbd_merge_bvec(struct request_queue *q, struct bvec_merge_data *bvm, struct bio_vec *bvec)\r\n{\r\nstruct drbd_conf *mdev = (struct drbd_conf *) q->queuedata;\r\nunsigned int bio_offset =\r\n(unsigned int)bvm->bi_sector << 9;\r\nunsigned int bio_size = bvm->bi_size;\r\nint limit, backing_limit;\r\nlimit = DRBD_MAX_BIO_SIZE\r\n- ((bio_offset & (DRBD_MAX_BIO_SIZE-1)) + bio_size);\r\nif (limit < 0)\r\nlimit = 0;\r\nif (bio_size == 0) {\r\nif (limit <= bvec->bv_len)\r\nlimit = bvec->bv_len;\r\n} else if (limit && get_ldev(mdev)) {\r\nstruct request_queue * const b =\r\nmdev->ldev->backing_bdev->bd_disk->queue;\r\nif (b->merge_bvec_fn) {\r\nbacking_limit = b->merge_bvec_fn(b, bvm, bvec);\r\nlimit = min(limit, backing_limit);\r\n}\r\nput_ldev(mdev);\r\n}\r\nreturn limit;\r\n}\r\nvoid request_timer_fn(unsigned long data)\r\n{\r\nstruct drbd_conf *mdev = (struct drbd_conf *) data;\r\nstruct drbd_request *req;\r\nstruct list_head *le;\r\nunsigned long et = 0;\r\nif (get_net_conf(mdev)) {\r\net = mdev->net_conf->timeout*HZ/10 * mdev->net_conf->ko_count;\r\nput_net_conf(mdev);\r\n}\r\nif (!et || mdev->state.conn < C_WF_REPORT_PARAMS)\r\nreturn;\r\nspin_lock_irq(&mdev->req_lock);\r\nle = &mdev->oldest_tle->requests;\r\nif (list_empty(le)) {\r\nspin_unlock_irq(&mdev->req_lock);\r\nmod_timer(&mdev->request_timer, jiffies + et);\r\nreturn;\r\n}\r\nle = le->prev;\r\nreq = list_entry(le, struct drbd_request, tl_requests);\r\nif (time_is_before_eq_jiffies(req->start_time + et)) {\r\nif (req->rq_state & RQ_NET_PENDING) {\r\ndev_warn(DEV, "Remote failed to finish a request within ko-count * timeout\n");\r\n_drbd_set_state(_NS(mdev, conn, C_TIMEOUT), CS_VERBOSE, NULL);\r\n} else {\r\ndev_warn(DEV, "Local backing block device frozen?\n");\r\nmod_timer(&mdev->request_timer, jiffies + et);\r\n}\r\n} else {\r\nmod_timer(&mdev->request_timer, req->start_time + et);\r\n}\r\nspin_unlock_irq(&mdev->req_lock);\r\n}
