static int\r\nnv40_graph_context_new(struct nouveau_channel *chan, int engine)\r\n{\r\nstruct nv40_graph_engine *pgraph = nv_engine(chan->dev, engine);\r\nstruct drm_device *dev = chan->dev;\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_gpuobj *grctx = NULL;\r\nstruct nouveau_grctx ctx = {};\r\nunsigned long flags;\r\nint ret;\r\nret = nouveau_gpuobj_new(dev, NULL, pgraph->grctx_size, 16,\r\nNVOBJ_FLAG_ZERO_ALLOC, &grctx);\r\nif (ret)\r\nreturn ret;\r\nctx.dev = chan->dev;\r\nctx.mode = NOUVEAU_GRCTX_VALS;\r\nctx.data = grctx;\r\nnv40_grctx_init(&ctx);\r\nnv_wo32(grctx, 0, grctx->vinst);\r\nspin_lock_irqsave(&dev_priv->context_switch_lock, flags);\r\nnv_wo32(chan->ramfc, 0x38, grctx->vinst >> 4);\r\nnv_mask(dev, 0x002500, 0x00000001, 0x00000000);\r\nif ((nv_rd32(dev, 0x003204) & 0x0000001f) == chan->id)\r\nnv_wr32(dev, 0x0032e0, grctx->vinst >> 4);\r\nnv_mask(dev, 0x002500, 0x00000001, 0x00000001);\r\nspin_unlock_irqrestore(&dev_priv->context_switch_lock, flags);\r\nchan->engctx[engine] = grctx;\r\nreturn 0;\r\n}\r\nstatic void\r\nnv40_graph_context_del(struct nouveau_channel *chan, int engine)\r\n{\r\nstruct nouveau_gpuobj *grctx = chan->engctx[engine];\r\nstruct drm_device *dev = chan->dev;\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nu32 inst = 0x01000000 | (grctx->pinst >> 4);\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev_priv->context_switch_lock, flags);\r\nnv_mask(dev, 0x400720, 0x00000000, 0x00000001);\r\nif (nv_rd32(dev, 0x40032c) == inst)\r\nnv_mask(dev, 0x40032c, 0x01000000, 0x00000000);\r\nif (nv_rd32(dev, 0x400330) == inst)\r\nnv_mask(dev, 0x400330, 0x01000000, 0x00000000);\r\nnv_mask(dev, 0x400720, 0x00000001, 0x00000001);\r\nspin_unlock_irqrestore(&dev_priv->context_switch_lock, flags);\r\nnouveau_gpuobj_ref(NULL, &grctx);\r\nchan->engctx[engine] = NULL;\r\n}\r\nint\r\nnv40_graph_object_new(struct nouveau_channel *chan, int engine,\r\nu32 handle, u16 class)\r\n{\r\nstruct drm_device *dev = chan->dev;\r\nstruct nouveau_gpuobj *obj = NULL;\r\nint ret;\r\nret = nouveau_gpuobj_new(dev, chan, 20, 16, NVOBJ_FLAG_ZERO_FREE, &obj);\r\nif (ret)\r\nreturn ret;\r\nobj->engine = 1;\r\nobj->class = class;\r\nnv_wo32(obj, 0x00, class);\r\nnv_wo32(obj, 0x04, 0x00000000);\r\n#ifndef __BIG_ENDIAN\r\nnv_wo32(obj, 0x08, 0x00000000);\r\n#else\r\nnv_wo32(obj, 0x08, 0x01000000);\r\n#endif\r\nnv_wo32(obj, 0x0c, 0x00000000);\r\nnv_wo32(obj, 0x10, 0x00000000);\r\nret = nouveau_ramht_insert(chan, handle, obj);\r\nnouveau_gpuobj_ref(NULL, &obj);\r\nreturn ret;\r\n}\r\nstatic void\r\nnv40_graph_set_tile_region(struct drm_device *dev, int i)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_tile_reg *tile = &dev_priv->tile.reg[i];\r\nswitch (dev_priv->chipset) {\r\ncase 0x40:\r\ncase 0x41:\r\ncase 0x42:\r\ncase 0x43:\r\ncase 0x45:\r\ncase 0x4e:\r\nnv_wr32(dev, NV20_PGRAPH_TSIZE(i), tile->pitch);\r\nnv_wr32(dev, NV20_PGRAPH_TLIMIT(i), tile->limit);\r\nnv_wr32(dev, NV20_PGRAPH_TILE(i), tile->addr);\r\nnv_wr32(dev, NV40_PGRAPH_TSIZE1(i), tile->pitch);\r\nnv_wr32(dev, NV40_PGRAPH_TLIMIT1(i), tile->limit);\r\nnv_wr32(dev, NV40_PGRAPH_TILE1(i), tile->addr);\r\nbreak;\r\ncase 0x44:\r\ncase 0x4a:\r\nnv_wr32(dev, NV20_PGRAPH_TSIZE(i), tile->pitch);\r\nnv_wr32(dev, NV20_PGRAPH_TLIMIT(i), tile->limit);\r\nnv_wr32(dev, NV20_PGRAPH_TILE(i), tile->addr);\r\nbreak;\r\ncase 0x46:\r\ncase 0x47:\r\ncase 0x49:\r\ncase 0x4b:\r\ncase 0x4c:\r\ncase 0x67:\r\ndefault:\r\nnv_wr32(dev, NV47_PGRAPH_TSIZE(i), tile->pitch);\r\nnv_wr32(dev, NV47_PGRAPH_TLIMIT(i), tile->limit);\r\nnv_wr32(dev, NV47_PGRAPH_TILE(i), tile->addr);\r\nnv_wr32(dev, NV40_PGRAPH_TSIZE1(i), tile->pitch);\r\nnv_wr32(dev, NV40_PGRAPH_TLIMIT1(i), tile->limit);\r\nnv_wr32(dev, NV40_PGRAPH_TILE1(i), tile->addr);\r\nbreak;\r\n}\r\n}\r\nint\r\nnv40_graph_init(struct drm_device *dev, int engine)\r\n{\r\nstruct nv40_graph_engine *pgraph = nv_engine(dev, engine);\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_fb_engine *pfb = &dev_priv->engine.fb;\r\nstruct nouveau_grctx ctx = {};\r\nuint32_t vramsz, *cp;\r\nint i, j;\r\nnv_wr32(dev, NV03_PMC_ENABLE, nv_rd32(dev, NV03_PMC_ENABLE) &\r\n~NV_PMC_ENABLE_PGRAPH);\r\nnv_wr32(dev, NV03_PMC_ENABLE, nv_rd32(dev, NV03_PMC_ENABLE) |\r\nNV_PMC_ENABLE_PGRAPH);\r\ncp = kmalloc(sizeof(*cp) * 256, GFP_KERNEL);\r\nif (!cp)\r\nreturn -ENOMEM;\r\nctx.dev = dev;\r\nctx.mode = NOUVEAU_GRCTX_PROG;\r\nctx.data = cp;\r\nctx.ctxprog_max = 256;\r\nnv40_grctx_init(&ctx);\r\npgraph->grctx_size = ctx.ctxvals_pos * 4;\r\nnv_wr32(dev, NV40_PGRAPH_CTXCTL_UCODE_INDEX, 0);\r\nfor (i = 0; i < ctx.ctxprog_len; i++)\r\nnv_wr32(dev, NV40_PGRAPH_CTXCTL_UCODE_DATA, cp[i]);\r\nkfree(cp);\r\nnv_wr32(dev, NV40_PGRAPH_CTXCTL_CUR, 0x00000000);\r\nnv_wr32(dev, NV03_PGRAPH_INTR , 0xFFFFFFFF);\r\nnv_wr32(dev, NV40_PGRAPH_INTR_EN, 0xFFFFFFFF);\r\nnv_wr32(dev, NV04_PGRAPH_DEBUG_0, 0xFFFFFFFF);\r\nnv_wr32(dev, NV04_PGRAPH_DEBUG_0, 0x00000000);\r\nnv_wr32(dev, NV04_PGRAPH_DEBUG_1, 0x401287c0);\r\nnv_wr32(dev, NV04_PGRAPH_DEBUG_3, 0xe0de8055);\r\nnv_wr32(dev, NV10_PGRAPH_DEBUG_4, 0x00008000);\r\nnv_wr32(dev, NV04_PGRAPH_LIMIT_VIOL_PIX, 0x00be3c5f);\r\nnv_wr32(dev, NV10_PGRAPH_CTX_CONTROL, 0x10010100);\r\nnv_wr32(dev, NV10_PGRAPH_STATE , 0xFFFFFFFF);\r\nj = nv_rd32(dev, 0x1540) & 0xff;\r\nif (j) {\r\nfor (i = 0; !(j & 1); j >>= 1, i++)\r\n;\r\nnv_wr32(dev, 0x405000, i);\r\n}\r\nif (dev_priv->chipset == 0x40) {\r\nnv_wr32(dev, 0x4009b0, 0x83280fff);\r\nnv_wr32(dev, 0x4009b4, 0x000000a0);\r\n} else {\r\nnv_wr32(dev, 0x400820, 0x83280eff);\r\nnv_wr32(dev, 0x400824, 0x000000a0);\r\n}\r\nswitch (dev_priv->chipset) {\r\ncase 0x40:\r\ncase 0x45:\r\nnv_wr32(dev, 0x4009b8, 0x0078e366);\r\nnv_wr32(dev, 0x4009bc, 0x0000014c);\r\nbreak;\r\ncase 0x41:\r\ncase 0x42:\r\nnv_wr32(dev, 0x400828, 0x007596ff);\r\nnv_wr32(dev, 0x40082c, 0x00000108);\r\nbreak;\r\ncase 0x43:\r\nnv_wr32(dev, 0x400828, 0x0072cb77);\r\nnv_wr32(dev, 0x40082c, 0x00000108);\r\nbreak;\r\ncase 0x44:\r\ncase 0x46:\r\ncase 0x4a:\r\ncase 0x4c:\r\ncase 0x4e:\r\nnv_wr32(dev, 0x400860, 0);\r\nnv_wr32(dev, 0x400864, 0);\r\nbreak;\r\ncase 0x47:\r\ncase 0x49:\r\ncase 0x4b:\r\nnv_wr32(dev, 0x400828, 0x07830610);\r\nnv_wr32(dev, 0x40082c, 0x0000016A);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nnv_wr32(dev, 0x400b38, 0x2ffff800);\r\nnv_wr32(dev, 0x400b3c, 0x00006000);\r\nswitch (dev_priv->chipset) {\r\ncase 0x44:\r\ncase 0x4a:\r\nnv_wr32(dev, 0x400bc4, 0x1003d888);\r\nnv_wr32(dev, 0x400bbc, 0xb7a7b500);\r\nbreak;\r\ncase 0x46:\r\nnv_wr32(dev, 0x400bc4, 0x0000e024);\r\nnv_wr32(dev, 0x400bbc, 0xb7a7b520);\r\nbreak;\r\ncase 0x4c:\r\ncase 0x4e:\r\ncase 0x67:\r\nnv_wr32(dev, 0x400bc4, 0x1003d888);\r\nnv_wr32(dev, 0x400bbc, 0xb7a7b540);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nfor (i = 0; i < pfb->num_tiles; i++)\r\nnv40_graph_set_tile_region(dev, i);\r\nvramsz = pci_resource_len(dev->pdev, 0) - 1;\r\nswitch (dev_priv->chipset) {\r\ncase 0x40:\r\nnv_wr32(dev, 0x4009A4, nv_rd32(dev, NV04_PFB_CFG0));\r\nnv_wr32(dev, 0x4009A8, nv_rd32(dev, NV04_PFB_CFG1));\r\nnv_wr32(dev, 0x4069A4, nv_rd32(dev, NV04_PFB_CFG0));\r\nnv_wr32(dev, 0x4069A8, nv_rd32(dev, NV04_PFB_CFG1));\r\nnv_wr32(dev, 0x400820, 0);\r\nnv_wr32(dev, 0x400824, 0);\r\nnv_wr32(dev, 0x400864, vramsz);\r\nnv_wr32(dev, 0x400868, vramsz);\r\nbreak;\r\ndefault:\r\nswitch (dev_priv->chipset) {\r\ncase 0x41:\r\ncase 0x42:\r\ncase 0x43:\r\ncase 0x45:\r\ncase 0x4e:\r\ncase 0x44:\r\ncase 0x4a:\r\nnv_wr32(dev, 0x4009F0, nv_rd32(dev, NV04_PFB_CFG0));\r\nnv_wr32(dev, 0x4009F4, nv_rd32(dev, NV04_PFB_CFG1));\r\nbreak;\r\ndefault:\r\nnv_wr32(dev, 0x400DF0, nv_rd32(dev, NV04_PFB_CFG0));\r\nnv_wr32(dev, 0x400DF4, nv_rd32(dev, NV04_PFB_CFG1));\r\nbreak;\r\n}\r\nnv_wr32(dev, 0x4069F0, nv_rd32(dev, NV04_PFB_CFG0));\r\nnv_wr32(dev, 0x4069F4, nv_rd32(dev, NV04_PFB_CFG1));\r\nnv_wr32(dev, 0x400840, 0);\r\nnv_wr32(dev, 0x400844, 0);\r\nnv_wr32(dev, 0x4008A0, vramsz);\r\nnv_wr32(dev, 0x4008A4, vramsz);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv40_graph_fini(struct drm_device *dev, int engine, bool suspend)\r\n{\r\nu32 inst = nv_rd32(dev, 0x40032c);\r\nif (inst & 0x01000000) {\r\nnv_wr32(dev, 0x400720, 0x00000000);\r\nnv_wr32(dev, 0x400784, inst);\r\nnv_mask(dev, 0x400310, 0x00000020, 0x00000020);\r\nnv_mask(dev, 0x400304, 0x00000001, 0x00000001);\r\nif (!nv_wait(dev, 0x400300, 0x00000001, 0x00000000)) {\r\nu32 insn = nv_rd32(dev, 0x400308);\r\nNV_ERROR(dev, "PGRAPH: ctxprog timeout 0x%08x\n", insn);\r\n}\r\nnv_mask(dev, 0x40032c, 0x01000000, 0x00000000);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nnv40_graph_isr_chid(struct drm_device *dev, u32 inst)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_gpuobj *grctx;\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&dev_priv->channels.lock, flags);\r\nfor (i = 0; i < dev_priv->engine.fifo.channels; i++) {\r\nif (!dev_priv->channels.ptr[i])\r\ncontinue;\r\ngrctx = dev_priv->channels.ptr[i]->engctx[NVOBJ_ENGINE_GR];\r\nif (grctx && grctx->pinst == inst)\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&dev_priv->channels.lock, flags);\r\nreturn i;\r\n}\r\nstatic void\r\nnv40_graph_isr(struct drm_device *dev)\r\n{\r\nu32 stat;\r\nwhile ((stat = nv_rd32(dev, NV03_PGRAPH_INTR))) {\r\nu32 nsource = nv_rd32(dev, NV03_PGRAPH_NSOURCE);\r\nu32 nstatus = nv_rd32(dev, NV03_PGRAPH_NSTATUS);\r\nu32 inst = (nv_rd32(dev, 0x40032c) & 0x000fffff) << 4;\r\nu32 chid = nv40_graph_isr_chid(dev, inst);\r\nu32 addr = nv_rd32(dev, NV04_PGRAPH_TRAPPED_ADDR);\r\nu32 subc = (addr & 0x00070000) >> 16;\r\nu32 mthd = (addr & 0x00001ffc);\r\nu32 data = nv_rd32(dev, NV04_PGRAPH_TRAPPED_DATA);\r\nu32 class = nv_rd32(dev, 0x400160 + subc * 4) & 0xffff;\r\nu32 show = stat;\r\nif (stat & NV_PGRAPH_INTR_ERROR) {\r\nif (nsource & NV03_PGRAPH_NSOURCE_ILLEGAL_MTHD) {\r\nif (!nouveau_gpuobj_mthd_call2(dev, chid, class, mthd, data))\r\nshow &= ~NV_PGRAPH_INTR_ERROR;\r\n} else\r\nif (nsource & NV03_PGRAPH_NSOURCE_DMA_VTX_PROTECTION) {\r\nnv_mask(dev, 0x402000, 0, 0);\r\n}\r\n}\r\nnv_wr32(dev, NV03_PGRAPH_INTR, stat);\r\nnv_wr32(dev, NV04_PGRAPH_FIFO, 0x00000001);\r\nif (show && nouveau_ratelimit()) {\r\nNV_INFO(dev, "PGRAPH -");\r\nnouveau_bitfield_print(nv10_graph_intr, show);\r\nprintk(" nsource:");\r\nnouveau_bitfield_print(nv04_graph_nsource, nsource);\r\nprintk(" nstatus:");\r\nnouveau_bitfield_print(nv10_graph_nstatus, nstatus);\r\nprintk("\n");\r\nNV_INFO(dev, "PGRAPH - ch %d (0x%08x) subc %d "\r\n"class 0x%04x mthd 0x%04x data 0x%08x\n",\r\nchid, inst, subc, class, mthd, data);\r\n}\r\n}\r\n}\r\nstatic void\r\nnv40_graph_destroy(struct drm_device *dev, int engine)\r\n{\r\nstruct nv40_graph_engine *pgraph = nv_engine(dev, engine);\r\nnouveau_irq_unregister(dev, 12);\r\nNVOBJ_ENGINE_DEL(dev, GR);\r\nkfree(pgraph);\r\n}\r\nint\r\nnv40_graph_create(struct drm_device *dev)\r\n{\r\nstruct nv40_graph_engine *pgraph;\r\npgraph = kzalloc(sizeof(*pgraph), GFP_KERNEL);\r\nif (!pgraph)\r\nreturn -ENOMEM;\r\npgraph->base.destroy = nv40_graph_destroy;\r\npgraph->base.init = nv40_graph_init;\r\npgraph->base.fini = nv40_graph_fini;\r\npgraph->base.context_new = nv40_graph_context_new;\r\npgraph->base.context_del = nv40_graph_context_del;\r\npgraph->base.object_new = nv40_graph_object_new;\r\npgraph->base.set_tile_region = nv40_graph_set_tile_region;\r\nNVOBJ_ENGINE_ADD(dev, GR, &pgraph->base);\r\nnouveau_irq_register(dev, 12, nv40_graph_isr);\r\nNVOBJ_CLASS(dev, 0x506e, SW);\r\nNVOBJ_CLASS(dev, 0x0030, GR);\r\nNVOBJ_CLASS(dev, 0x0039, GR);\r\nNVOBJ_CLASS(dev, 0x004a, GR);\r\nNVOBJ_CLASS(dev, 0x009f, GR);\r\nNVOBJ_CLASS(dev, 0x008a, GR);\r\nNVOBJ_CLASS(dev, 0x0089, GR);\r\nNVOBJ_CLASS(dev, 0x3089, GR);\r\nNVOBJ_CLASS(dev, 0x0062, GR);\r\nNVOBJ_CLASS(dev, 0x3062, GR);\r\nNVOBJ_CLASS(dev, 0x0043, GR);\r\nNVOBJ_CLASS(dev, 0x0012, GR);\r\nNVOBJ_CLASS(dev, 0x0072, GR);\r\nNVOBJ_CLASS(dev, 0x0019, GR);\r\nNVOBJ_CLASS(dev, 0x0044, GR);\r\nNVOBJ_CLASS(dev, 0x309e, GR);\r\nif (nv44_graph_class(dev))\r\nNVOBJ_CLASS(dev, 0x4497, GR);\r\nelse\r\nNVOBJ_CLASS(dev, 0x4097, GR);\r\nNVOBJ_CLASS(dev, 0x506e, SW);\r\nNVOBJ_MTHD (dev, 0x506e, 0x0500, nv04_graph_mthd_page_flip);\r\nreturn 0;\r\n}
