static int make_ste(unsigned long stab, unsigned long esid, unsigned long vsid)\r\n{\r\nunsigned long esid_data, vsid_data;\r\nunsigned long entry, group, old_esid, castout_entry, i;\r\nunsigned int global_entry;\r\nstruct stab_entry *ste, *castout_ste;\r\nunsigned long kernel_segment = (esid << SID_SHIFT) >= PAGE_OFFSET;\r\nvsid_data = vsid << STE_VSID_SHIFT;\r\nesid_data = esid << SID_SHIFT | STE_ESID_KP | STE_ESID_V;\r\nif (! kernel_segment)\r\nesid_data |= STE_ESID_KS;\r\nglobal_entry = (esid & 0x1f) << 3;\r\nste = (struct stab_entry *)(stab | ((esid & 0x1f) << 7));\r\nfor (group = 0; group < 2; group++) {\r\nfor (entry = 0; entry < 8; entry++, ste++) {\r\nif (!(ste->esid_data & STE_ESID_V)) {\r\nste->vsid_data = vsid_data;\r\neieio();\r\nste->esid_data = esid_data;\r\nreturn (global_entry | entry);\r\n}\r\n}\r\nglobal_entry = ((~esid) & 0x1f) << 3;\r\nste = (struct stab_entry *)(stab | (((~esid) & 0x1f) << 7));\r\n}\r\ncastout_entry = get_paca()->stab_rr;\r\nfor (i = 0; i < 16; i++) {\r\nif (castout_entry < 8) {\r\nglobal_entry = (esid & 0x1f) << 3;\r\nste = (struct stab_entry *)(stab | ((esid & 0x1f) << 7));\r\ncastout_ste = ste + castout_entry;\r\n} else {\r\nglobal_entry = ((~esid) & 0x1f) << 3;\r\nste = (struct stab_entry *)(stab | (((~esid) & 0x1f) << 7));\r\ncastout_ste = ste + (castout_entry - 8);\r\n}\r\nif ((castout_ste->esid_data & ESID_MASK) != PAGE_OFFSET)\r\nbreak;\r\ncastout_entry = (castout_entry + 1) & 0xf;\r\n}\r\nget_paca()->stab_rr = (castout_entry + 1) & 0xf;\r\nasm volatile("isync" : : : "memory");\r\nold_esid = castout_ste->esid_data >> SID_SHIFT;\r\ncastout_ste->esid_data = 0;\r\nasm volatile("sync" : : : "memory");\r\ncastout_ste->vsid_data = vsid_data;\r\neieio();\r\ncastout_ste->esid_data = esid_data;\r\nasm volatile("slbie %0" : : "r" (old_esid << SID_SHIFT));\r\nasm volatile("sync" : : : "memory");\r\nreturn (global_entry | (castout_entry & 0x7));\r\n}\r\nstatic int __ste_allocate(unsigned long ea, struct mm_struct *mm)\r\n{\r\nunsigned long vsid;\r\nunsigned char stab_entry;\r\nunsigned long offset;\r\nif (is_kernel_addr(ea)) {\r\nvsid = get_kernel_vsid(ea, MMU_SEGSIZE_256M);\r\n} else {\r\nif ((ea >= TASK_SIZE_USER64) || (! mm))\r\nreturn 1;\r\nvsid = get_vsid(mm->context.id, ea, MMU_SEGSIZE_256M);\r\n}\r\nstab_entry = make_ste(get_paca()->stab_addr, GET_ESID(ea), vsid);\r\nif (!is_kernel_addr(ea)) {\r\noffset = __get_cpu_var(stab_cache_ptr);\r\nif (offset < NR_STAB_CACHE_ENTRIES)\r\n__get_cpu_var(stab_cache[offset++]) = stab_entry;\r\nelse\r\noffset = NR_STAB_CACHE_ENTRIES+1;\r\n__get_cpu_var(stab_cache_ptr) = offset;\r\nasm volatile("sync":::"memory");\r\n}\r\nreturn 0;\r\n}\r\nint ste_allocate(unsigned long ea)\r\n{\r\nreturn __ste_allocate(ea, current->mm);\r\n}\r\nvoid switch_stab(struct task_struct *tsk, struct mm_struct *mm)\r\n{\r\nstruct stab_entry *stab = (struct stab_entry *) get_paca()->stab_addr;\r\nstruct stab_entry *ste;\r\nunsigned long offset;\r\nunsigned long pc = KSTK_EIP(tsk);\r\nunsigned long stack = KSTK_ESP(tsk);\r\nunsigned long unmapped_base;\r\nasm volatile("isync" : : : "memory");\r\nhard_irq_disable();\r\noffset = __get_cpu_var(stab_cache_ptr);\r\nif (offset <= NR_STAB_CACHE_ENTRIES) {\r\nint i;\r\nfor (i = 0; i < offset; i++) {\r\nste = stab + __get_cpu_var(stab_cache[i]);\r\nste->esid_data = 0;\r\n}\r\n} else {\r\nunsigned long entry;\r\nste = stab;\r\nste += 1;\r\nfor (entry = 1;\r\nentry < (HW_PAGE_SIZE / sizeof(struct stab_entry));\r\nentry++, ste++) {\r\nunsigned long ea;\r\nea = ste->esid_data & ESID_MASK;\r\nif (!is_kernel_addr(ea)) {\r\nste->esid_data = 0;\r\n}\r\n}\r\n}\r\nasm volatile("sync; slbia; sync":::"memory");\r\n__get_cpu_var(stab_cache_ptr) = 0;\r\nif (test_tsk_thread_flag(tsk, TIF_32BIT))\r\nunmapped_base = TASK_UNMAPPED_BASE_USER32;\r\nelse\r\nunmapped_base = TASK_UNMAPPED_BASE_USER64;\r\n__ste_allocate(pc, mm);\r\nif (GET_ESID(pc) == GET_ESID(stack))\r\nreturn;\r\n__ste_allocate(stack, mm);\r\nif ((GET_ESID(pc) == GET_ESID(unmapped_base))\r\n|| (GET_ESID(stack) == GET_ESID(unmapped_base)))\r\nreturn;\r\n__ste_allocate(unmapped_base, mm);\r\nasm volatile("sync" : : : "memory");\r\n}\r\nvoid __init stabs_alloc(void)\r\n{\r\nint cpu;\r\nif (mmu_has_feature(MMU_FTR_SLB))\r\nreturn;\r\nfor_each_possible_cpu(cpu) {\r\nunsigned long newstab;\r\nif (cpu == 0)\r\ncontinue;\r\nnewstab = memblock_alloc_base(HW_PAGE_SIZE, HW_PAGE_SIZE,\r\n1<<SID_SHIFT);\r\nnewstab = (unsigned long)__va(newstab);\r\nmemset((void *)newstab, 0, HW_PAGE_SIZE);\r\npaca[cpu].stab_addr = newstab;\r\npaca[cpu].stab_real = virt_to_abs(newstab);\r\nprintk(KERN_INFO "Segment table for CPU %d at 0x%llx "\r\n"virtual, 0x%llx absolute\n",\r\ncpu, paca[cpu].stab_addr, paca[cpu].stab_real);\r\n}\r\n}\r\nvoid stab_initialize(unsigned long stab)\r\n{\r\nunsigned long vsid = get_kernel_vsid(PAGE_OFFSET, MMU_SEGSIZE_256M);\r\nunsigned long stabreal;\r\nasm volatile("isync; slbia; isync":::"memory");\r\nmake_ste(stab, GET_ESID(PAGE_OFFSET), vsid);\r\nasm volatile("sync":::"memory");\r\nstabreal = get_paca()->stab_real | 0x1ul;\r\n#ifdef CONFIG_PPC_ISERIES\r\nif (firmware_has_feature(FW_FEATURE_ISERIES)) {\r\nHvCall1(HvCallBaseSetASR, stabreal);\r\nreturn;\r\n}\r\n#endif\r\nmtspr(SPRN_ASR, stabreal);\r\n}
