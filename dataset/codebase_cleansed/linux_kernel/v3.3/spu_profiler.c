void set_spu_profiling_frequency(unsigned int freq_khz, unsigned int cycles_reset)\r\n{\r\nunsigned long ns_per_cyc;\r\nif (!freq_khz)\r\nfreq_khz = ppc_proc_freq/1000;\r\nns_per_cyc = (USEC_PER_SEC << SCALE_SHIFT)/freq_khz;\r\nprofiling_interval = (ns_per_cyc * cycles_reset) >> SCALE_SHIFT;\r\n}\r\nstatic void spu_pc_extract(int cpu, int entry)\r\n{\r\nu64 trace_buffer[2];\r\nu64 spu_mask;\r\nint spu;\r\nspu_mask = SPU_PC_MASK;\r\ncbe_read_trace_buffer(cpu, trace_buffer);\r\nfor (spu = SPUS_PER_TB_ENTRY-1; spu >= 0; spu--) {\r\nsamples[spu * TRACE_ARRAY_SIZE + entry]\r\n= (spu_mask & trace_buffer[0]) << 2;\r\nsamples[(spu + SPUS_PER_TB_ENTRY) * TRACE_ARRAY_SIZE + entry]\r\n= (spu_mask & trace_buffer[1]) << 2;\r\ntrace_buffer[0] = trace_buffer[0] >> NUM_SPU_BITS_TRBUF;\r\ntrace_buffer[1] = trace_buffer[1] >> NUM_SPU_BITS_TRBUF;\r\n}\r\n}\r\nstatic int cell_spu_pc_collection(int cpu)\r\n{\r\nu32 trace_addr;\r\nint entry;\r\nentry = 0;\r\ntrace_addr = cbe_read_pm(cpu, trace_address);\r\nwhile (!(trace_addr & CBE_PM_TRACE_BUF_EMPTY)) {\r\nspu_pc_extract(cpu, entry);\r\nentry++;\r\nif (entry >= TRACE_ARRAY_SIZE)\r\nbreak;\r\ntrace_addr = cbe_read_pm(cpu, trace_address);\r\n}\r\nreturn entry;\r\n}\r\nstatic enum hrtimer_restart profile_spus(struct hrtimer *timer)\r\n{\r\nktime_t kt;\r\nint cpu, node, k, num_samples, spu_num;\r\nif (!spu_prof_running)\r\ngoto stop;\r\nfor_each_online_cpu(cpu) {\r\nif (cbe_get_hw_thread_id(cpu))\r\ncontinue;\r\nnode = cbe_cpu_to_node(cpu);\r\nspin_lock_irqsave(&oprof_spu_smpl_arry_lck,\r\noprof_spu_smpl_arry_lck_flags);\r\nnum_samples = cell_spu_pc_collection(cpu);\r\nif (num_samples == 0) {\r\nspin_unlock_irqrestore(&oprof_spu_smpl_arry_lck,\r\noprof_spu_smpl_arry_lck_flags);\r\ncontinue;\r\n}\r\nfor (k = 0; k < SPUS_PER_NODE; k++) {\r\nspu_num = k + (node * SPUS_PER_NODE);\r\nspu_sync_buffer(spu_num,\r\nsamples + (k * TRACE_ARRAY_SIZE),\r\nnum_samples);\r\n}\r\nspin_unlock_irqrestore(&oprof_spu_smpl_arry_lck,\r\noprof_spu_smpl_arry_lck_flags);\r\n}\r\nsmp_wmb();\r\nkt = ktime_set(0, profiling_interval);\r\nif (!spu_prof_running)\r\ngoto stop;\r\nhrtimer_forward(timer, timer->base->get_time(), kt);\r\nreturn HRTIMER_RESTART;\r\nstop:\r\nprintk(KERN_INFO "SPU_PROF: spu-prof timer ending\n");\r\nreturn HRTIMER_NORESTART;\r\n}\r\nint start_spu_profiling_cycles(unsigned int cycles_reset)\r\n{\r\nktime_t kt;\r\npr_debug("timer resolution: %lu\n", TICK_NSEC);\r\nkt = ktime_set(0, profiling_interval);\r\nhrtimer_init(&timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nhrtimer_set_expires(&timer, kt);\r\ntimer.function = profile_spus;\r\nsamples = kzalloc(SPUS_PER_NODE *\r\nTRACE_ARRAY_SIZE * sizeof(u32), GFP_KERNEL);\r\nif (!samples)\r\nreturn -ENOMEM;\r\nspu_prof_running = 1;\r\nhrtimer_start(&timer, kt, HRTIMER_MODE_REL);\r\nschedule_delayed_work(&spu_work, DEFAULT_TIMER_EXPIRE);\r\nreturn 0;\r\n}\r\nvoid start_spu_profiling_events(void)\r\n{\r\nspu_prof_running = 1;\r\nschedule_delayed_work(&spu_work, DEFAULT_TIMER_EXPIRE);\r\nreturn;\r\n}\r\nvoid stop_spu_profiling_cycles(void)\r\n{\r\nspu_prof_running = 0;\r\nhrtimer_cancel(&timer);\r\nkfree(samples);\r\npr_debug("SPU_PROF: stop_spu_profiling_cycles issued\n");\r\n}\r\nvoid stop_spu_profiling_events(void)\r\n{\r\nspu_prof_running = 0;\r\n}
