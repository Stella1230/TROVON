static inline struct ipath_ucontext *to_iucontext(struct ib_ucontext\r\n*ibucontext)\r\n{\r\nreturn container_of(ibucontext, struct ipath_ucontext, ibucontext);\r\n}\r\nvoid ipath_copy_sge(struct ipath_sge_state *ss, void *data, u32 length)\r\n{\r\nstruct ipath_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nmemcpy(sge->vaddr, data, len);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr != NULL) {\r\nif (++sge->n >= IPATH_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\ndata += len;\r\nlength -= len;\r\n}\r\n}\r\nvoid ipath_skip_sge(struct ipath_sge_state *ss, u32 length)\r\n{\r\nstruct ipath_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr != NULL) {\r\nif (++sge->n >= IPATH_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\nlength -= len;\r\n}\r\n}\r\nstatic u32 ipath_count_sge(struct ipath_sge_state *ss, u32 length)\r\n{\r\nstruct ipath_sge *sg_list = ss->sg_list;\r\nstruct ipath_sge sge = ss->sge;\r\nu8 num_sge = ss->num_sge;\r\nu32 ndesc = 1;\r\nwhile (length) {\r\nu32 len = sge.length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge.sge_length)\r\nlen = sge.sge_length;\r\nBUG_ON(len == 0);\r\nif (((long) sge.vaddr & (sizeof(u32) - 1)) ||\r\n(len != length && (len & (sizeof(u32) - 1)))) {\r\nndesc = 0;\r\nbreak;\r\n}\r\nndesc++;\r\nsge.vaddr += len;\r\nsge.length -= len;\r\nsge.sge_length -= len;\r\nif (sge.sge_length == 0) {\r\nif (--num_sge)\r\nsge = *sg_list++;\r\n} else if (sge.length == 0 && sge.mr != NULL) {\r\nif (++sge.n >= IPATH_SEGSZ) {\r\nif (++sge.m >= sge.mr->mapsz)\r\nbreak;\r\nsge.n = 0;\r\n}\r\nsge.vaddr =\r\nsge.mr->map[sge.m]->segs[sge.n].vaddr;\r\nsge.length =\r\nsge.mr->map[sge.m]->segs[sge.n].length;\r\n}\r\nlength -= len;\r\n}\r\nreturn ndesc;\r\n}\r\nstatic void ipath_copy_from_sge(void *data, struct ipath_sge_state *ss,\r\nu32 length)\r\n{\r\nstruct ipath_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nmemcpy(data, sge->vaddr, len);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr != NULL) {\r\nif (++sge->n >= IPATH_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\ndata += len;\r\nlength -= len;\r\n}\r\n}\r\nstatic int ipath_post_one_send(struct ipath_qp *qp, struct ib_send_wr *wr)\r\n{\r\nstruct ipath_swqe *wqe;\r\nu32 next;\r\nint i;\r\nint j;\r\nint acc;\r\nint ret;\r\nunsigned long flags;\r\nstruct ipath_devdata *dd = to_idev(qp->ibqp.device)->dd;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->ibqp.qp_type != IB_QPT_SMI &&\r\n!(dd->ipath_flags & IPATH_LINKACTIVE)) {\r\nret = -ENETDOWN;\r\ngoto bail;\r\n}\r\nif (unlikely(!(ib_ipath_state_ops[qp->state] & IPATH_POST_SEND_OK)))\r\ngoto bail_inval;\r\nif (wr->num_sge > qp->s_max_sge)\r\ngoto bail_inval;\r\nif (qp->ibqp.qp_type == IB_QPT_UC) {\r\nif ((unsigned) wr->opcode >= IB_WR_RDMA_READ)\r\ngoto bail_inval;\r\n} else if (qp->ibqp.qp_type == IB_QPT_UD) {\r\nif (wr->opcode != IB_WR_SEND &&\r\nwr->opcode != IB_WR_SEND_WITH_IMM)\r\ngoto bail_inval;\r\nif (qp->ibqp.pd != wr->wr.ud.ah->pd)\r\ngoto bail_inval;\r\n} else if ((unsigned) wr->opcode > IB_WR_ATOMIC_FETCH_AND_ADD)\r\ngoto bail_inval;\r\nelse if (wr->opcode >= IB_WR_ATOMIC_CMP_AND_SWP &&\r\n(wr->num_sge == 0 ||\r\nwr->sg_list[0].length < sizeof(u64) ||\r\nwr->sg_list[0].addr & (sizeof(u64) - 1)))\r\ngoto bail_inval;\r\nelse if (wr->opcode >= IB_WR_RDMA_READ && !qp->s_max_rd_atomic)\r\ngoto bail_inval;\r\nnext = qp->s_head + 1;\r\nif (next >= qp->s_size)\r\nnext = 0;\r\nif (next == qp->s_last) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nwqe = get_swqe_ptr(qp, qp->s_head);\r\nwqe->wr = *wr;\r\nwqe->length = 0;\r\nif (wr->num_sge) {\r\nacc = wr->opcode >= IB_WR_RDMA_READ ?\r\nIB_ACCESS_LOCAL_WRITE : 0;\r\nfor (i = 0, j = 0; i < wr->num_sge; i++) {\r\nu32 length = wr->sg_list[i].length;\r\nint ok;\r\nif (length == 0)\r\ncontinue;\r\nok = ipath_lkey_ok(qp, &wqe->sg_list[j],\r\n&wr->sg_list[i], acc);\r\nif (!ok)\r\ngoto bail_inval;\r\nwqe->length += length;\r\nj++;\r\n}\r\nwqe->wr.num_sge = j;\r\n}\r\nif (qp->ibqp.qp_type == IB_QPT_UC ||\r\nqp->ibqp.qp_type == IB_QPT_RC) {\r\nif (wqe->length > 0x80000000U)\r\ngoto bail_inval;\r\n} else if (wqe->length > to_idev(qp->ibqp.device)->dd->ipath_ibmtu)\r\ngoto bail_inval;\r\nwqe->ssn = qp->s_ssn++;\r\nqp->s_head = next;\r\nret = 0;\r\ngoto bail;\r\nbail_inval:\r\nret = -EINVAL;\r\nbail:\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int ipath_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,\r\nstruct ib_send_wr **bad_wr)\r\n{\r\nstruct ipath_qp *qp = to_iqp(ibqp);\r\nint err = 0;\r\nfor (; wr; wr = wr->next) {\r\nerr = ipath_post_one_send(qp, wr);\r\nif (err) {\r\n*bad_wr = wr;\r\ngoto bail;\r\n}\r\n}\r\nipath_do_send((unsigned long) qp);\r\nbail:\r\nreturn err;\r\n}\r\nstatic int ipath_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct ipath_qp *qp = to_iqp(ibqp);\r\nstruct ipath_rwq *wq = qp->r_rq.wq;\r\nunsigned long flags;\r\nint ret;\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_POST_RECV_OK) || !wq) {\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nfor (; wr; wr = wr->next) {\r\nstruct ipath_rwqe *wqe;\r\nu32 next;\r\nint i;\r\nif ((unsigned) wr->num_sge > qp->r_rq.max_sge) {\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nspin_lock_irqsave(&qp->r_rq.lock, flags);\r\nnext = wq->head + 1;\r\nif (next >= qp->r_rq.size)\r\nnext = 0;\r\nif (next == wq->tail) {\r\nspin_unlock_irqrestore(&qp->r_rq.lock, flags);\r\n*bad_wr = wr;\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nwqe = get_rwqe_ptr(&qp->r_rq, wq->head);\r\nwqe->wr_id = wr->wr_id;\r\nwqe->num_sge = wr->num_sge;\r\nfor (i = 0; i < wr->num_sge; i++)\r\nwqe->sg_list[i] = wr->sg_list[i];\r\nsmp_wmb();\r\nwq->head = next;\r\nspin_unlock_irqrestore(&qp->r_rq.lock, flags);\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void ipath_qp_rcv(struct ipath_ibdev *dev,\r\nstruct ipath_ib_header *hdr, int has_grh,\r\nvoid *data, u32 tlen, struct ipath_qp *qp)\r\n{\r\nif (!(ib_ipath_state_ops[qp->state] & IPATH_PROCESS_RECV_OK)) {\r\ndev->n_pkt_drops++;\r\nreturn;\r\n}\r\nswitch (qp->ibqp.qp_type) {\r\ncase IB_QPT_SMI:\r\ncase IB_QPT_GSI:\r\nif (ib_ipath_disable_sma)\r\nbreak;\r\ncase IB_QPT_UD:\r\nipath_ud_rcv(dev, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ncase IB_QPT_RC:\r\nipath_rc_rcv(dev, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ncase IB_QPT_UC:\r\nipath_uc_rcv(dev, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nvoid ipath_ib_rcv(struct ipath_ibdev *dev, void *rhdr, void *data,\r\nu32 tlen)\r\n{\r\nstruct ipath_ib_header *hdr = rhdr;\r\nstruct ipath_other_headers *ohdr;\r\nstruct ipath_qp *qp;\r\nu32 qp_num;\r\nint lnh;\r\nu8 opcode;\r\nu16 lid;\r\nif (unlikely(dev == NULL))\r\ngoto bail;\r\nif (unlikely(tlen < 24)) {\r\ndev->rcv_errors++;\r\ngoto bail;\r\n}\r\nlid = be16_to_cpu(hdr->lrh[1]);\r\nif (lid < IPATH_MULTICAST_LID_BASE) {\r\nlid &= ~((1 << dev->dd->ipath_lmc) - 1);\r\nif (unlikely(lid != dev->dd->ipath_lid)) {\r\ndev->rcv_errors++;\r\ngoto bail;\r\n}\r\n}\r\nlnh = be16_to_cpu(hdr->lrh[0]) & 3;\r\nif (lnh == IPATH_LRH_BTH)\r\nohdr = &hdr->u.oth;\r\nelse if (lnh == IPATH_LRH_GRH)\r\nohdr = &hdr->u.l.oth;\r\nelse {\r\ndev->rcv_errors++;\r\ngoto bail;\r\n}\r\nopcode = be32_to_cpu(ohdr->bth[0]) >> 24;\r\ndev->opstats[opcode].n_bytes += tlen;\r\ndev->opstats[opcode].n_packets++;\r\nqp_num = be32_to_cpu(ohdr->bth[1]) & IPATH_QPN_MASK;\r\nif (qp_num == IPATH_MULTICAST_QPN) {\r\nstruct ipath_mcast *mcast;\r\nstruct ipath_mcast_qp *p;\r\nif (lnh != IPATH_LRH_GRH) {\r\ndev->n_pkt_drops++;\r\ngoto bail;\r\n}\r\nmcast = ipath_mcast_find(&hdr->u.l.grh.dgid);\r\nif (mcast == NULL) {\r\ndev->n_pkt_drops++;\r\ngoto bail;\r\n}\r\ndev->n_multicast_rcv++;\r\nlist_for_each_entry_rcu(p, &mcast->qp_list, list)\r\nipath_qp_rcv(dev, hdr, 1, data, tlen, p->qp);\r\nif (atomic_dec_return(&mcast->refcount) <= 1)\r\nwake_up(&mcast->wait);\r\n} else {\r\nqp = ipath_lookup_qpn(&dev->qp_table, qp_num);\r\nif (qp) {\r\ndev->n_unicast_rcv++;\r\nipath_qp_rcv(dev, hdr, lnh == IPATH_LRH_GRH, data,\r\ntlen, qp);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n} else\r\ndev->n_pkt_drops++;\r\n}\r\nbail:;\r\n}\r\nstatic void ipath_ib_timer(struct ipath_ibdev *dev)\r\n{\r\nstruct ipath_qp *resend = NULL;\r\nstruct ipath_qp *rnr = NULL;\r\nstruct list_head *last;\r\nstruct ipath_qp *qp;\r\nunsigned long flags;\r\nif (dev == NULL)\r\nreturn;\r\nspin_lock_irqsave(&dev->pending_lock, flags);\r\nif (++dev->pending_index >= ARRAY_SIZE(dev->pending))\r\ndev->pending_index = 0;\r\nlast = &dev->pending[dev->pending_index];\r\nwhile (!list_empty(last)) {\r\nqp = list_entry(last->next, struct ipath_qp, timerwait);\r\nlist_del_init(&qp->timerwait);\r\nqp->timer_next = resend;\r\nresend = qp;\r\natomic_inc(&qp->refcount);\r\n}\r\nlast = &dev->rnrwait;\r\nif (!list_empty(last)) {\r\nqp = list_entry(last->next, struct ipath_qp, timerwait);\r\nif (--qp->s_rnr_timeout == 0) {\r\ndo {\r\nlist_del_init(&qp->timerwait);\r\nqp->timer_next = rnr;\r\nrnr = qp;\r\natomic_inc(&qp->refcount);\r\nif (list_empty(last))\r\nbreak;\r\nqp = list_entry(last->next, struct ipath_qp,\r\ntimerwait);\r\n} while (qp->s_rnr_timeout == 0);\r\n}\r\n}\r\nif (dev->pma_sample_status == IB_PMA_SAMPLE_STATUS_STARTED &&\r\n--dev->pma_sample_start == 0) {\r\ndev->pma_sample_status = IB_PMA_SAMPLE_STATUS_RUNNING;\r\nipath_snapshot_counters(dev->dd, &dev->ipath_sword,\r\n&dev->ipath_rword,\r\n&dev->ipath_spkts,\r\n&dev->ipath_rpkts,\r\n&dev->ipath_xmit_wait);\r\n}\r\nif (dev->pma_sample_status == IB_PMA_SAMPLE_STATUS_RUNNING) {\r\nif (dev->pma_sample_interval == 0) {\r\nu64 ta, tb, tc, td, te;\r\ndev->pma_sample_status = IB_PMA_SAMPLE_STATUS_DONE;\r\nipath_snapshot_counters(dev->dd, &ta, &tb,\r\n&tc, &td, &te);\r\ndev->ipath_sword = ta - dev->ipath_sword;\r\ndev->ipath_rword = tb - dev->ipath_rword;\r\ndev->ipath_spkts = tc - dev->ipath_spkts;\r\ndev->ipath_rpkts = td - dev->ipath_rpkts;\r\ndev->ipath_xmit_wait = te - dev->ipath_xmit_wait;\r\n}\r\nelse\r\ndev->pma_sample_interval--;\r\n}\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\nwhile (resend != NULL) {\r\nqp = resend;\r\nresend = qp->timer_next;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->s_last != qp->s_tail &&\r\nib_ipath_state_ops[qp->state] & IPATH_PROCESS_SEND_OK) {\r\ndev->n_timeouts++;\r\nipath_restart_rc(qp, qp->s_last_psn + 1);\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\nwhile (rnr != NULL) {\r\nqp = rnr;\r\nrnr = qp->timer_next;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (ib_ipath_state_ops[qp->state] & IPATH_PROCESS_SEND_OK)\r\nipath_schedule_send(qp);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\n}\r\nstatic void update_sge(struct ipath_sge_state *ss, u32 length)\r\n{\r\nstruct ipath_sge *sge = &ss->sge;\r\nsge->vaddr += length;\r\nsge->length -= length;\r\nsge->sge_length -= length;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr != NULL) {\r\nif (++sge->n >= IPATH_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nreturn;\r\nsge->n = 0;\r\n}\r\nsge->vaddr = sge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length = sge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\n}\r\nstatic inline u32 get_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data >> shift;\r\n}\r\nstatic inline u32 set_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data << shift;\r\n}\r\nstatic inline u32 clear_upper_bytes(u32 data, u32 n, u32 off)\r\n{\r\ndata <<= ((sizeof(u32) - n) * BITS_PER_BYTE);\r\ndata >>= ((sizeof(u32) - n - off) * BITS_PER_BYTE);\r\nreturn data;\r\n}\r\nstatic inline u32 get_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data << shift;\r\n}\r\nstatic inline u32 set_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data >> shift;\r\n}\r\nstatic inline u32 clear_upper_bytes(u32 data, u32 n, u32 off)\r\n{\r\ndata >>= ((sizeof(u32) - n) * BITS_PER_BYTE);\r\ndata <<= ((sizeof(u32) - n - off) * BITS_PER_BYTE);\r\nreturn data;\r\n}\r\nstatic void copy_io(u32 __iomem *piobuf, struct ipath_sge_state *ss,\r\nu32 length, unsigned flush_wc)\r\n{\r\nu32 extra = 0;\r\nu32 data = 0;\r\nu32 last;\r\nwhile (1) {\r\nu32 len = ss->sge.length;\r\nu32 off;\r\nif (len > length)\r\nlen = length;\r\nif (len > ss->sge.sge_length)\r\nlen = ss->sge.sge_length;\r\nBUG_ON(len == 0);\r\noff = (unsigned long)ss->sge.vaddr & (sizeof(u32) - 1);\r\nif (off) {\r\nu32 *addr = (u32 *)((unsigned long)ss->sge.vaddr &\r\n~(sizeof(u32) - 1));\r\nu32 v = get_upper_bits(*addr, off * BITS_PER_BYTE);\r\nu32 y;\r\ny = sizeof(u32) - off;\r\nif (len > y)\r\nlen = y;\r\nif (len + extra >= sizeof(u32)) {\r\ndata |= set_upper_bits(v, extra *\r\nBITS_PER_BYTE);\r\nlen = sizeof(u32) - extra;\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n__raw_writel(data, piobuf);\r\npiobuf++;\r\nextra = 0;\r\ndata = 0;\r\n} else {\r\ndata |= clear_upper_bytes(v, len, extra);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\nextra += len;\r\n}\r\n} else if (extra) {\r\nu32 *addr = (u32 *) ss->sge.vaddr;\r\nint shift = extra * BITS_PER_BYTE;\r\nint ushift = 32 - shift;\r\nu32 l = len;\r\nwhile (l >= sizeof(u32)) {\r\nu32 v = *addr;\r\ndata |= set_upper_bits(v, shift);\r\n__raw_writel(data, piobuf);\r\ndata = get_upper_bits(v, ushift);\r\npiobuf++;\r\naddr++;\r\nl -= sizeof(u32);\r\n}\r\nif (l) {\r\nu32 v = *addr;\r\nif (l + extra >= sizeof(u32)) {\r\ndata |= set_upper_bits(v, shift);\r\nlen -= l + extra - sizeof(u32);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n__raw_writel(data, piobuf);\r\npiobuf++;\r\nextra = 0;\r\ndata = 0;\r\n} else {\r\ndata |= clear_upper_bytes(v, l,\r\nextra);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\nextra += l;\r\n}\r\n} else if (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n} else if (len == length) {\r\nu32 w;\r\nw = (len + 3) >> 2;\r\n__iowrite32_copy(piobuf, ss->sge.vaddr, w - 1);\r\npiobuf += w - 1;\r\nlast = ((u32 *) ss->sge.vaddr)[w - 1];\r\nbreak;\r\n} else {\r\nu32 w = len >> 2;\r\n__iowrite32_copy(piobuf, ss->sge.vaddr, w);\r\npiobuf += w;\r\nextra = len & (sizeof(u32) - 1);\r\nif (extra) {\r\nu32 v = ((u32 *) ss->sge.vaddr)[w];\r\ndata = clear_upper_bytes(v, extra, 0);\r\n}\r\n}\r\nupdate_sge(ss, len);\r\nlength -= len;\r\n}\r\nupdate_sge(ss, length);\r\nif (flush_wc) {\r\nipath_flush_wc();\r\n__raw_writel(last, piobuf);\r\nipath_flush_wc();\r\n} else\r\n__raw_writel(last, piobuf);\r\n}\r\nunsigned ipath_ib_rate_to_mult(enum ib_rate rate)\r\n{\r\nswitch (rate) {\r\ncase IB_RATE_2_5_GBPS: return 8;\r\ncase IB_RATE_5_GBPS: return 4;\r\ncase IB_RATE_10_GBPS: return 2;\r\ncase IB_RATE_20_GBPS: return 1;\r\ndefault: return 0;\r\n}\r\n}\r\nstatic enum ib_rate ipath_mult_to_ib_rate(unsigned mult)\r\n{\r\nswitch (mult) {\r\ncase 8: return IB_RATE_2_5_GBPS;\r\ncase 4: return IB_RATE_5_GBPS;\r\ncase 2: return IB_RATE_10_GBPS;\r\ncase 1: return IB_RATE_20_GBPS;\r\ndefault: return IB_RATE_PORT_CURRENT;\r\n}\r\n}\r\nstatic inline struct ipath_verbs_txreq *get_txreq(struct ipath_ibdev *dev)\r\n{\r\nstruct ipath_verbs_txreq *tx = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->pending_lock, flags);\r\nif (!list_empty(&dev->txreq_free)) {\r\nstruct list_head *l = dev->txreq_free.next;\r\nlist_del(l);\r\ntx = list_entry(l, struct ipath_verbs_txreq, txreq.list);\r\n}\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\nreturn tx;\r\n}\r\nstatic inline void put_txreq(struct ipath_ibdev *dev,\r\nstruct ipath_verbs_txreq *tx)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->pending_lock, flags);\r\nlist_add(&tx->txreq.list, &dev->txreq_free);\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\n}\r\nstatic void sdma_complete(void *cookie, int status)\r\n{\r\nstruct ipath_verbs_txreq *tx = cookie;\r\nstruct ipath_qp *qp = tx->qp;\r\nstruct ipath_ibdev *dev = to_idev(qp->ibqp.device);\r\nunsigned long flags;\r\nenum ib_wc_status ibs = status == IPATH_SDMA_TXREQ_S_OK ?\r\nIB_WC_SUCCESS : IB_WC_WR_FLUSH_ERR;\r\nif (atomic_dec_and_test(&qp->s_dma_busy)) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (tx->wqe)\r\nipath_send_complete(qp, tx->wqe, ibs);\r\nif ((ib_ipath_state_ops[qp->state] & IPATH_FLUSH_SEND &&\r\nqp->s_last != qp->s_head) ||\r\n(qp->s_flags & IPATH_S_WAIT_DMA))\r\nipath_schedule_send(qp);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nwake_up(&qp->wait_dma);\r\n} else if (tx->wqe) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nipath_send_complete(qp, tx->wqe, ibs);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\n}\r\nif (tx->txreq.flags & IPATH_SDMA_TXREQ_F_FREEBUF)\r\nkfree(tx->txreq.map_addr);\r\nput_txreq(dev, tx);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\nstatic void decrement_dma_busy(struct ipath_qp *qp)\r\n{\r\nunsigned long flags;\r\nif (atomic_dec_and_test(&qp->s_dma_busy)) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif ((ib_ipath_state_ops[qp->state] & IPATH_FLUSH_SEND &&\r\nqp->s_last != qp->s_head) ||\r\n(qp->s_flags & IPATH_S_WAIT_DMA))\r\nipath_schedule_send(qp);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nwake_up(&qp->wait_dma);\r\n}\r\n}\r\nstatic inline unsigned ipath_pkt_delay(u32 plen, u8 snd_mult, u8 rcv_mult)\r\n{\r\nreturn (rcv_mult > snd_mult) ?\r\n(plen * (rcv_mult - snd_mult) + 1) >> 1 : 0;\r\n}\r\nstatic int ipath_verbs_send_dma(struct ipath_qp *qp,\r\nstruct ipath_ib_header *hdr, u32 hdrwords,\r\nstruct ipath_sge_state *ss, u32 len,\r\nu32 plen, u32 dwords)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct ipath_devdata *dd = dev->dd;\r\nstruct ipath_verbs_txreq *tx;\r\nu32 *piobuf;\r\nu32 control;\r\nu32 ndesc;\r\nint ret;\r\ntx = qp->s_tx;\r\nif (tx) {\r\nqp->s_tx = NULL;\r\natomic_inc(&qp->s_dma_busy);\r\nret = ipath_sdma_verbs_send(dd, tx->ss, tx->len, tx);\r\nif (ret) {\r\nqp->s_tx = tx;\r\ndecrement_dma_busy(qp);\r\n}\r\ngoto bail;\r\n}\r\ntx = get_txreq(dev);\r\nif (!tx) {\r\nret = -EBUSY;\r\ngoto bail;\r\n}\r\ncontrol = qp->s_pkt_delay;\r\nqp->s_pkt_delay = ipath_pkt_delay(plen, dd->delay_mult, qp->s_dmult);\r\ntx->qp = qp;\r\natomic_inc(&qp->refcount);\r\ntx->wqe = qp->s_wqe;\r\ntx->txreq.callback = sdma_complete;\r\ntx->txreq.callback_cookie = tx;\r\ntx->txreq.flags = IPATH_SDMA_TXREQ_F_HEADTOHOST |\r\nIPATH_SDMA_TXREQ_F_INTREQ | IPATH_SDMA_TXREQ_F_FREEDESC;\r\nif (plen + 1 >= IPATH_SMALLBUF_DWORDS)\r\ntx->txreq.flags |= IPATH_SDMA_TXREQ_F_USELARGEBUF;\r\nif ((be16_to_cpu(hdr->lrh[0]) >> 12) == 15) {\r\ncontrol |= 1ULL << 31;\r\ntx->txreq.flags |= IPATH_SDMA_TXREQ_F_VL15;\r\n}\r\nif (len) {\r\nndesc = ipath_count_sge(ss, len);\r\nif (ndesc >= dd->ipath_sdma_descq_cnt)\r\nndesc = 0;\r\n} else\r\nndesc = 1;\r\nif (ndesc) {\r\ntx->hdr.pbc[0] = cpu_to_le32(plen);\r\ntx->hdr.pbc[1] = cpu_to_le32(control);\r\nmemcpy(&tx->hdr.hdr, hdr, hdrwords << 2);\r\ntx->txreq.sg_count = ndesc;\r\ntx->map_len = (hdrwords + 2) << 2;\r\ntx->txreq.map_addr = &tx->hdr;\r\natomic_inc(&qp->s_dma_busy);\r\nret = ipath_sdma_verbs_send(dd, ss, dwords, tx);\r\nif (ret) {\r\ntx->ss = ss;\r\ntx->len = dwords;\r\nqp->s_tx = tx;\r\ndecrement_dma_busy(qp);\r\n}\r\ngoto bail;\r\n}\r\ntx->map_len = (plen + 1) << 2;\r\npiobuf = kmalloc(tx->map_len, GFP_ATOMIC);\r\nif (unlikely(piobuf == NULL)) {\r\nret = -EBUSY;\r\ngoto err_tx;\r\n}\r\ntx->txreq.map_addr = piobuf;\r\ntx->txreq.flags |= IPATH_SDMA_TXREQ_F_FREEBUF;\r\ntx->txreq.sg_count = 1;\r\n*piobuf++ = (__force u32) cpu_to_le32(plen);\r\n*piobuf++ = (__force u32) cpu_to_le32(control);\r\nmemcpy(piobuf, hdr, hdrwords << 2);\r\nipath_copy_from_sge(piobuf + hdrwords, ss, len);\r\natomic_inc(&qp->s_dma_busy);\r\nret = ipath_sdma_verbs_send(dd, NULL, 0, tx);\r\nif (ret) {\r\ntx->ss = NULL;\r\ntx->len = 0;\r\nqp->s_tx = tx;\r\ndecrement_dma_busy(qp);\r\n}\r\ndev->n_unaligned++;\r\ngoto bail;\r\nerr_tx:\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\nput_txreq(dev, tx);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int ipath_verbs_send_pio(struct ipath_qp *qp,\r\nstruct ipath_ib_header *ibhdr, u32 hdrwords,\r\nstruct ipath_sge_state *ss, u32 len,\r\nu32 plen, u32 dwords)\r\n{\r\nstruct ipath_devdata *dd = to_idev(qp->ibqp.device)->dd;\r\nu32 *hdr = (u32 *) ibhdr;\r\nu32 __iomem *piobuf;\r\nunsigned flush_wc;\r\nu32 control;\r\nint ret;\r\nunsigned long flags;\r\npiobuf = ipath_getpiobuf(dd, plen, NULL);\r\nif (unlikely(piobuf == NULL)) {\r\nret = -EBUSY;\r\ngoto bail;\r\n}\r\ncontrol = qp->s_pkt_delay;\r\nqp->s_pkt_delay = ipath_pkt_delay(plen, dd->delay_mult, qp->s_dmult);\r\nif ((be16_to_cpu(ibhdr->lrh[0]) >> 12) == 15)\r\ncontrol |= 1ULL << 31;\r\nwriteq(((u64) control << 32) | plen, piobuf);\r\npiobuf += 2;\r\nflush_wc = dd->ipath_flags & IPATH_PIO_FLUSH_WC;\r\nif (len == 0) {\r\nif (flush_wc) {\r\nipath_flush_wc();\r\n__iowrite32_copy(piobuf, hdr, hdrwords - 1);\r\nipath_flush_wc();\r\n__raw_writel(hdr[hdrwords - 1], piobuf + hdrwords - 1);\r\nipath_flush_wc();\r\n} else\r\n__iowrite32_copy(piobuf, hdr, hdrwords);\r\ngoto done;\r\n}\r\nif (flush_wc)\r\nipath_flush_wc();\r\n__iowrite32_copy(piobuf, hdr, hdrwords);\r\npiobuf += hdrwords;\r\nif (likely(ss->num_sge == 1 && len <= ss->sge.length &&\r\n!((unsigned long)ss->sge.vaddr & (sizeof(u32) - 1)))) {\r\nu32 *addr = (u32 *) ss->sge.vaddr;\r\nupdate_sge(ss, len);\r\nif (flush_wc) {\r\n__iowrite32_copy(piobuf, addr, dwords - 1);\r\nipath_flush_wc();\r\n__raw_writel(addr[dwords - 1], piobuf + dwords - 1);\r\nipath_flush_wc();\r\n} else\r\n__iowrite32_copy(piobuf, addr, dwords);\r\ngoto done;\r\n}\r\ncopy_io(piobuf, ss, len, flush_wc);\r\ndone:\r\nif (qp->s_wqe) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nipath_send_complete(qp, qp->s_wqe, IB_WC_SUCCESS);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_verbs_send(struct ipath_qp *qp, struct ipath_ib_header *hdr,\r\nu32 hdrwords, struct ipath_sge_state *ss, u32 len)\r\n{\r\nstruct ipath_devdata *dd = to_idev(qp->ibqp.device)->dd;\r\nu32 plen;\r\nint ret;\r\nu32 dwords = (len + 3) >> 2;\r\nplen = hdrwords + dwords + 1;\r\nif (qp->ibqp.qp_type == IB_QPT_SMI ||\r\n!(dd->ipath_flags & IPATH_HAS_SEND_DMA))\r\nret = ipath_verbs_send_pio(qp, hdr, hdrwords, ss, len,\r\nplen, dwords);\r\nelse\r\nret = ipath_verbs_send_dma(qp, hdr, hdrwords, ss, len,\r\nplen, dwords);\r\nreturn ret;\r\n}\r\nint ipath_snapshot_counters(struct ipath_devdata *dd, u64 *swords,\r\nu64 *rwords, u64 *spkts, u64 *rpkts,\r\nu64 *xmit_wait)\r\n{\r\nint ret;\r\nif (!(dd->ipath_flags & IPATH_INITTED)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\n*swords = ipath_snap_cntr(dd, dd->ipath_cregs->cr_wordsendcnt);\r\n*rwords = ipath_snap_cntr(dd, dd->ipath_cregs->cr_wordrcvcnt);\r\n*spkts = ipath_snap_cntr(dd, dd->ipath_cregs->cr_pktsendcnt);\r\n*rpkts = ipath_snap_cntr(dd, dd->ipath_cregs->cr_pktrcvcnt);\r\n*xmit_wait = ipath_snap_cntr(dd, dd->ipath_cregs->cr_sendstallcnt);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_get_counters(struct ipath_devdata *dd,\r\nstruct ipath_verbs_counters *cntrs)\r\n{\r\nstruct ipath_cregs const *crp = dd->ipath_cregs;\r\nint ret;\r\nif (!(dd->ipath_flags & IPATH_INITTED)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\ncntrs->symbol_error_counter =\r\nipath_snap_cntr(dd, crp->cr_ibsymbolerrcnt);\r\ncntrs->link_error_recovery_counter =\r\nipath_snap_cntr(dd, crp->cr_iblinkerrrecovcnt);\r\ncntrs->link_downed_counter =\r\nipath_snap_cntr(dd, crp->cr_iblinkdowncnt);\r\ncntrs->port_rcv_errors =\r\nipath_snap_cntr(dd, crp->cr_rxdroppktcnt) +\r\nipath_snap_cntr(dd, crp->cr_rcvovflcnt) +\r\nipath_snap_cntr(dd, crp->cr_portovflcnt) +\r\nipath_snap_cntr(dd, crp->cr_err_rlencnt) +\r\nipath_snap_cntr(dd, crp->cr_invalidrlencnt) +\r\nipath_snap_cntr(dd, crp->cr_errlinkcnt) +\r\nipath_snap_cntr(dd, crp->cr_erricrccnt) +\r\nipath_snap_cntr(dd, crp->cr_errvcrccnt) +\r\nipath_snap_cntr(dd, crp->cr_errlpcrccnt) +\r\nipath_snap_cntr(dd, crp->cr_badformatcnt) +\r\ndd->ipath_rxfc_unsupvl_errs;\r\nif (crp->cr_rxotherlocalphyerrcnt)\r\ncntrs->port_rcv_errors +=\r\nipath_snap_cntr(dd, crp->cr_rxotherlocalphyerrcnt);\r\nif (crp->cr_rxvlerrcnt)\r\ncntrs->port_rcv_errors +=\r\nipath_snap_cntr(dd, crp->cr_rxvlerrcnt);\r\ncntrs->port_rcv_remphys_errors =\r\nipath_snap_cntr(dd, crp->cr_rcvebpcnt);\r\ncntrs->port_xmit_discards = ipath_snap_cntr(dd, crp->cr_unsupvlcnt);\r\ncntrs->port_xmit_data = ipath_snap_cntr(dd, crp->cr_wordsendcnt);\r\ncntrs->port_rcv_data = ipath_snap_cntr(dd, crp->cr_wordrcvcnt);\r\ncntrs->port_xmit_packets = ipath_snap_cntr(dd, crp->cr_pktsendcnt);\r\ncntrs->port_rcv_packets = ipath_snap_cntr(dd, crp->cr_pktrcvcnt);\r\ncntrs->local_link_integrity_errors =\r\ncrp->cr_locallinkintegrityerrcnt ?\r\nipath_snap_cntr(dd, crp->cr_locallinkintegrityerrcnt) :\r\n((dd->ipath_flags & IPATH_GPIO_ERRINTRS) ?\r\ndd->ipath_lli_errs : dd->ipath_lli_errors);\r\ncntrs->excessive_buffer_overrun_errors =\r\ncrp->cr_excessbufferovflcnt ?\r\nipath_snap_cntr(dd, crp->cr_excessbufferovflcnt) :\r\ndd->ipath_overrun_thresh_errs;\r\ncntrs->vl15_dropped = crp->cr_vl15droppedpktcnt ?\r\nipath_snap_cntr(dd, crp->cr_vl15droppedpktcnt) : 0;\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_ib_piobufavail(struct ipath_ibdev *dev)\r\n{\r\nstruct list_head *list;\r\nstruct ipath_qp *qplist;\r\nstruct ipath_qp *qp;\r\nunsigned long flags;\r\nif (dev == NULL)\r\ngoto bail;\r\nlist = &dev->piowait;\r\nqplist = NULL;\r\nspin_lock_irqsave(&dev->pending_lock, flags);\r\nwhile (!list_empty(list)) {\r\nqp = list_entry(list->next, struct ipath_qp, piowait);\r\nlist_del_init(&qp->piowait);\r\nqp->pio_next = qplist;\r\nqplist = qp;\r\natomic_inc(&qp->refcount);\r\n}\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\nwhile (qplist != NULL) {\r\nqp = qplist;\r\nqplist = qp->pio_next;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (ib_ipath_state_ops[qp->state] & IPATH_PROCESS_SEND_OK)\r\nipath_schedule_send(qp);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\nbail:\r\nreturn 0;\r\n}\r\nstatic int ipath_query_device(struct ib_device *ibdev,\r\nstruct ib_device_attr *props)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibdev);\r\nmemset(props, 0, sizeof(*props));\r\nprops->device_cap_flags = IB_DEVICE_BAD_PKEY_CNTR |\r\nIB_DEVICE_BAD_QKEY_CNTR | IB_DEVICE_SHUTDOWN_PORT |\r\nIB_DEVICE_SYS_IMAGE_GUID | IB_DEVICE_RC_RNR_NAK_GEN |\r\nIB_DEVICE_PORT_ACTIVE_EVENT | IB_DEVICE_SRQ_RESIZE;\r\nprops->page_size_cap = PAGE_SIZE;\r\nprops->vendor_id =\r\nIPATH_SRC_OUI_1 << 16 | IPATH_SRC_OUI_2 << 8 | IPATH_SRC_OUI_3;\r\nprops->vendor_part_id = dev->dd->ipath_deviceid;\r\nprops->hw_ver = dev->dd->ipath_pcirev;\r\nprops->sys_image_guid = dev->sys_image_guid;\r\nprops->max_mr_size = ~0ull;\r\nprops->max_qp = ib_ipath_max_qps;\r\nprops->max_qp_wr = ib_ipath_max_qp_wrs;\r\nprops->max_sge = ib_ipath_max_sges;\r\nprops->max_cq = ib_ipath_max_cqs;\r\nprops->max_ah = ib_ipath_max_ahs;\r\nprops->max_cqe = ib_ipath_max_cqes;\r\nprops->max_mr = dev->lk_table.max;\r\nprops->max_fmr = dev->lk_table.max;\r\nprops->max_map_per_fmr = 32767;\r\nprops->max_pd = ib_ipath_max_pds;\r\nprops->max_qp_rd_atom = IPATH_MAX_RDMA_ATOMIC;\r\nprops->max_qp_init_rd_atom = 255;\r\nprops->max_srq = ib_ipath_max_srqs;\r\nprops->max_srq_wr = ib_ipath_max_srq_wrs;\r\nprops->max_srq_sge = ib_ipath_max_srq_sges;\r\nprops->atomic_cap = IB_ATOMIC_GLOB;\r\nprops->max_pkeys = ipath_get_npkeys(dev->dd);\r\nprops->max_mcast_grp = ib_ipath_max_mcast_grps;\r\nprops->max_mcast_qp_attach = ib_ipath_max_mcast_qp_attached;\r\nprops->max_total_mcast_qp_attach = props->max_mcast_qp_attach *\r\nprops->max_mcast_grp;\r\nreturn 0;\r\n}\r\nu32 ipath_get_cr_errpkey(struct ipath_devdata *dd)\r\n{\r\nreturn ipath_read_creg32(dd, dd->ipath_cregs->cr_errpkey);\r\n}\r\nstatic int ipath_query_port(struct ib_device *ibdev,\r\nu8 port, struct ib_port_attr *props)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibdev);\r\nstruct ipath_devdata *dd = dev->dd;\r\nenum ib_mtu mtu;\r\nu16 lid = dd->ipath_lid;\r\nu64 ibcstat;\r\nmemset(props, 0, sizeof(*props));\r\nprops->lid = lid ? lid : be16_to_cpu(IB_LID_PERMISSIVE);\r\nprops->lmc = dd->ipath_lmc;\r\nprops->sm_lid = dev->sm_lid;\r\nprops->sm_sl = dev->sm_sl;\r\nibcstat = dd->ipath_lastibcstat;\r\nprops->state = ipath_ib_linkstate(dd, ibcstat) + 1;\r\nprops->phys_state =\r\nipath_cvt_physportstate[dd->ipath_lastibcstat &\r\ndd->ibcs_lts_mask];\r\nprops->port_cap_flags = dev->port_cap_flags;\r\nprops->gid_tbl_len = 1;\r\nprops->max_msg_sz = 0x80000000;\r\nprops->pkey_tbl_len = ipath_get_npkeys(dd);\r\nprops->bad_pkey_cntr = ipath_get_cr_errpkey(dd) -\r\ndev->z_pkey_violations;\r\nprops->qkey_viol_cntr = dev->qkey_violations;\r\nprops->active_width = dd->ipath_link_width_active;\r\nprops->active_speed = dd->ipath_link_speed_active;\r\nprops->max_vl_num = 1;\r\nprops->init_type_reply = 0;\r\nprops->max_mtu = ipath_mtu4096 ? IB_MTU_4096 : IB_MTU_2048;\r\nswitch (dd->ipath_ibmtu) {\r\ncase 4096:\r\nmtu = IB_MTU_4096;\r\nbreak;\r\ncase 2048:\r\nmtu = IB_MTU_2048;\r\nbreak;\r\ncase 1024:\r\nmtu = IB_MTU_1024;\r\nbreak;\r\ncase 512:\r\nmtu = IB_MTU_512;\r\nbreak;\r\ncase 256:\r\nmtu = IB_MTU_256;\r\nbreak;\r\ndefault:\r\nmtu = IB_MTU_2048;\r\n}\r\nprops->active_mtu = mtu;\r\nprops->subnet_timeout = dev->subnet_timeout;\r\nreturn 0;\r\n}\r\nstatic int ipath_modify_device(struct ib_device *device,\r\nint device_modify_mask,\r\nstruct ib_device_modify *device_modify)\r\n{\r\nint ret;\r\nif (device_modify_mask & ~(IB_DEVICE_MODIFY_SYS_IMAGE_GUID |\r\nIB_DEVICE_MODIFY_NODE_DESC)) {\r\nret = -EOPNOTSUPP;\r\ngoto bail;\r\n}\r\nif (device_modify_mask & IB_DEVICE_MODIFY_NODE_DESC)\r\nmemcpy(device->node_desc, device_modify->node_desc, 64);\r\nif (device_modify_mask & IB_DEVICE_MODIFY_SYS_IMAGE_GUID)\r\nto_idev(device)->sys_image_guid =\r\ncpu_to_be64(device_modify->sys_image_guid);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int ipath_modify_port(struct ib_device *ibdev,\r\nu8 port, int port_modify_mask,\r\nstruct ib_port_modify *props)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibdev);\r\ndev->port_cap_flags |= props->set_port_cap_mask;\r\ndev->port_cap_flags &= ~props->clr_port_cap_mask;\r\nif (port_modify_mask & IB_PORT_SHUTDOWN)\r\nipath_set_linkstate(dev->dd, IPATH_IB_LINKDOWN);\r\nif (port_modify_mask & IB_PORT_RESET_QKEY_CNTR)\r\ndev->qkey_violations = 0;\r\nreturn 0;\r\n}\r\nstatic int ipath_query_gid(struct ib_device *ibdev, u8 port,\r\nint index, union ib_gid *gid)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibdev);\r\nint ret;\r\nif (index >= 1) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\ngid->global.subnet_prefix = dev->gid_prefix;\r\ngid->global.interface_id = dev->dd->ipath_guid;\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic struct ib_pd *ipath_alloc_pd(struct ib_device *ibdev,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibdev);\r\nstruct ipath_pd *pd;\r\nstruct ib_pd *ret;\r\npd = kmalloc(sizeof *pd, GFP_KERNEL);\r\nif (!pd) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nspin_lock(&dev->n_pds_lock);\r\nif (dev->n_pds_allocated == ib_ipath_max_pds) {\r\nspin_unlock(&dev->n_pds_lock);\r\nkfree(pd);\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\ndev->n_pds_allocated++;\r\nspin_unlock(&dev->n_pds_lock);\r\npd->user = udata != NULL;\r\nret = &pd->ibpd;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int ipath_dealloc_pd(struct ib_pd *ibpd)\r\n{\r\nstruct ipath_pd *pd = to_ipd(ibpd);\r\nstruct ipath_ibdev *dev = to_idev(ibpd->device);\r\nspin_lock(&dev->n_pds_lock);\r\ndev->n_pds_allocated--;\r\nspin_unlock(&dev->n_pds_lock);\r\nkfree(pd);\r\nreturn 0;\r\n}\r\nstatic struct ib_ah *ipath_create_ah(struct ib_pd *pd,\r\nstruct ib_ah_attr *ah_attr)\r\n{\r\nstruct ipath_ah *ah;\r\nstruct ib_ah *ret;\r\nstruct ipath_ibdev *dev = to_idev(pd->device);\r\nunsigned long flags;\r\nif (ah_attr->dlid >= IPATH_MULTICAST_LID_BASE &&\r\nah_attr->dlid != IPATH_PERMISSIVE_LID &&\r\n!(ah_attr->ah_flags & IB_AH_GRH)) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nif (ah_attr->dlid == 0) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nif (ah_attr->port_num < 1 ||\r\nah_attr->port_num > pd->device->phys_port_cnt) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nah = kmalloc(sizeof *ah, GFP_ATOMIC);\r\nif (!ah) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nspin_lock_irqsave(&dev->n_ahs_lock, flags);\r\nif (dev->n_ahs_allocated == ib_ipath_max_ahs) {\r\nspin_unlock_irqrestore(&dev->n_ahs_lock, flags);\r\nkfree(ah);\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\ndev->n_ahs_allocated++;\r\nspin_unlock_irqrestore(&dev->n_ahs_lock, flags);\r\nah->attr = *ah_attr;\r\nah->attr.static_rate = ipath_ib_rate_to_mult(ah_attr->static_rate);\r\nret = &ah->ibah;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int ipath_destroy_ah(struct ib_ah *ibah)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibah->device);\r\nstruct ipath_ah *ah = to_iah(ibah);\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->n_ahs_lock, flags);\r\ndev->n_ahs_allocated--;\r\nspin_unlock_irqrestore(&dev->n_ahs_lock, flags);\r\nkfree(ah);\r\nreturn 0;\r\n}\r\nstatic int ipath_query_ah(struct ib_ah *ibah, struct ib_ah_attr *ah_attr)\r\n{\r\nstruct ipath_ah *ah = to_iah(ibah);\r\n*ah_attr = ah->attr;\r\nah_attr->static_rate = ipath_mult_to_ib_rate(ah->attr.static_rate);\r\nreturn 0;\r\n}\r\nunsigned ipath_get_npkeys(struct ipath_devdata *dd)\r\n{\r\nreturn ARRAY_SIZE(dd->ipath_pd[0]->port_pkeys);\r\n}\r\nunsigned ipath_get_pkey(struct ipath_devdata *dd, unsigned index)\r\n{\r\nunsigned ret;\r\nif (index >= ARRAY_SIZE(dd->ipath_pd[0]->port_pkeys))\r\nret = 0;\r\nelse\r\nret = dd->ipath_pd[0]->port_pkeys[index];\r\nreturn ret;\r\n}\r\nstatic int ipath_query_pkey(struct ib_device *ibdev, u8 port, u16 index,\r\nu16 *pkey)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibdev);\r\nint ret;\r\nif (index >= ipath_get_npkeys(dev->dd)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\n*pkey = ipath_get_pkey(dev->dd, index);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic struct ib_ucontext *ipath_alloc_ucontext(struct ib_device *ibdev,\r\nstruct ib_udata *udata)\r\n{\r\nstruct ipath_ucontext *context;\r\nstruct ib_ucontext *ret;\r\ncontext = kmalloc(sizeof *context, GFP_KERNEL);\r\nif (!context) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nret = &context->ibucontext;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int ipath_dealloc_ucontext(struct ib_ucontext *context)\r\n{\r\nkfree(to_iucontext(context));\r\nreturn 0;\r\n}\r\nstatic void __verbs_timer(unsigned long arg)\r\n{\r\nstruct ipath_devdata *dd = (struct ipath_devdata *) arg;\r\nipath_ib_timer(dd->verbs_dev);\r\nmod_timer(&dd->verbs_timer, jiffies + 1);\r\n}\r\nstatic int enable_timer(struct ipath_devdata *dd)\r\n{\r\nif (dd->ipath_flags & IPATH_GPIO_INTR) {\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_debugportselect,\r\n0x2074076542310ULL);\r\ndd->ipath_gpio_mask |= (u64) (1 << IPATH_GPIO_PORT0_BIT);\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_gpio_mask,\r\ndd->ipath_gpio_mask);\r\n}\r\ninit_timer(&dd->verbs_timer);\r\ndd->verbs_timer.function = __verbs_timer;\r\ndd->verbs_timer.data = (unsigned long)dd;\r\ndd->verbs_timer.expires = jiffies + 1;\r\nadd_timer(&dd->verbs_timer);\r\nreturn 0;\r\n}\r\nstatic int disable_timer(struct ipath_devdata *dd)\r\n{\r\nif (dd->ipath_flags & IPATH_GPIO_INTR) {\r\ndd->ipath_gpio_mask &= ~((u64) (1 << IPATH_GPIO_PORT0_BIT));\r\nipath_write_kreg(dd, dd->ipath_kregs->kr_gpio_mask,\r\ndd->ipath_gpio_mask);\r\n}\r\ndel_timer_sync(&dd->verbs_timer);\r\nreturn 0;\r\n}\r\nint ipath_register_ib_device(struct ipath_devdata *dd)\r\n{\r\nstruct ipath_verbs_counters cntrs;\r\nstruct ipath_ibdev *idev;\r\nstruct ib_device *dev;\r\nstruct ipath_verbs_txreq *tx;\r\nunsigned i;\r\nint ret;\r\nidev = (struct ipath_ibdev *)ib_alloc_device(sizeof *idev);\r\nif (idev == NULL) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\ndev = &idev->ibdev;\r\nif (dd->ipath_sdma_descq_cnt) {\r\ntx = kmalloc(dd->ipath_sdma_descq_cnt * sizeof *tx,\r\nGFP_KERNEL);\r\nif (tx == NULL) {\r\nret = -ENOMEM;\r\ngoto err_tx;\r\n}\r\n} else\r\ntx = NULL;\r\nidev->txreq_bufs = tx;\r\nspin_lock_init(&idev->n_pds_lock);\r\nspin_lock_init(&idev->n_ahs_lock);\r\nspin_lock_init(&idev->n_cqs_lock);\r\nspin_lock_init(&idev->n_qps_lock);\r\nspin_lock_init(&idev->n_srqs_lock);\r\nspin_lock_init(&idev->n_mcast_grps_lock);\r\nspin_lock_init(&idev->qp_table.lock);\r\nspin_lock_init(&idev->lk_table.lock);\r\nidev->sm_lid = __constant_be16_to_cpu(IB_LID_PERMISSIVE);\r\nidev->gid_prefix = __constant_cpu_to_be64(0xfe80000000000000ULL);\r\nret = ipath_init_qp_table(idev, ib_ipath_qp_table_size);\r\nif (ret)\r\ngoto err_qp;\r\nidev->lk_table.max = 1 << ib_ipath_lkey_table_size;\r\nidev->lk_table.table = kzalloc(idev->lk_table.max *\r\nsizeof(*idev->lk_table.table),\r\nGFP_KERNEL);\r\nif (idev->lk_table.table == NULL) {\r\nret = -ENOMEM;\r\ngoto err_lk;\r\n}\r\nINIT_LIST_HEAD(&idev->pending_mmaps);\r\nspin_lock_init(&idev->pending_lock);\r\nidev->mmap_offset = PAGE_SIZE;\r\nspin_lock_init(&idev->mmap_offset_lock);\r\nINIT_LIST_HEAD(&idev->pending[0]);\r\nINIT_LIST_HEAD(&idev->pending[1]);\r\nINIT_LIST_HEAD(&idev->pending[2]);\r\nINIT_LIST_HEAD(&idev->piowait);\r\nINIT_LIST_HEAD(&idev->rnrwait);\r\nINIT_LIST_HEAD(&idev->txreq_free);\r\nidev->pending_index = 0;\r\nidev->port_cap_flags =\r\nIB_PORT_SYS_IMAGE_GUID_SUP | IB_PORT_CLIENT_REG_SUP;\r\nif (dd->ipath_flags & IPATH_HAS_LINK_LATENCY)\r\nidev->port_cap_flags |= IB_PORT_LINK_LATENCY_SUP;\r\nidev->pma_counter_select[0] = IB_PMA_PORT_XMIT_DATA;\r\nidev->pma_counter_select[1] = IB_PMA_PORT_RCV_DATA;\r\nidev->pma_counter_select[2] = IB_PMA_PORT_XMIT_PKTS;\r\nidev->pma_counter_select[3] = IB_PMA_PORT_RCV_PKTS;\r\nidev->pma_counter_select[4] = IB_PMA_PORT_XMIT_WAIT;\r\nipath_get_counters(dd, &cntrs);\r\nidev->z_symbol_error_counter = cntrs.symbol_error_counter;\r\nidev->z_link_error_recovery_counter =\r\ncntrs.link_error_recovery_counter;\r\nidev->z_link_downed_counter = cntrs.link_downed_counter;\r\nidev->z_port_rcv_errors = cntrs.port_rcv_errors;\r\nidev->z_port_rcv_remphys_errors =\r\ncntrs.port_rcv_remphys_errors;\r\nidev->z_port_xmit_discards = cntrs.port_xmit_discards;\r\nidev->z_port_xmit_data = cntrs.port_xmit_data;\r\nidev->z_port_rcv_data = cntrs.port_rcv_data;\r\nidev->z_port_xmit_packets = cntrs.port_xmit_packets;\r\nidev->z_port_rcv_packets = cntrs.port_rcv_packets;\r\nidev->z_local_link_integrity_errors =\r\ncntrs.local_link_integrity_errors;\r\nidev->z_excessive_buffer_overrun_errors =\r\ncntrs.excessive_buffer_overrun_errors;\r\nidev->z_vl15_dropped = cntrs.vl15_dropped;\r\nfor (i = 0; i < dd->ipath_sdma_descq_cnt; i++, tx++)\r\nlist_add(&tx->txreq.list, &idev->txreq_free);\r\nif (!sys_image_guid)\r\nsys_image_guid = dd->ipath_guid;\r\nidev->sys_image_guid = sys_image_guid;\r\nidev->ib_unit = dd->ipath_unit;\r\nidev->dd = dd;\r\nstrlcpy(dev->name, "ipath%d", IB_DEVICE_NAME_MAX);\r\ndev->owner = THIS_MODULE;\r\ndev->node_guid = dd->ipath_guid;\r\ndev->uverbs_abi_ver = IPATH_UVERBS_ABI_VERSION;\r\ndev->uverbs_cmd_mask =\r\n(1ull << IB_USER_VERBS_CMD_GET_CONTEXT) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_DEVICE) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_PORT) |\r\n(1ull << IB_USER_VERBS_CMD_ALLOC_PD) |\r\n(1ull << IB_USER_VERBS_CMD_DEALLOC_PD) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_AH) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_AH) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_AH) |\r\n(1ull << IB_USER_VERBS_CMD_REG_MR) |\r\n(1ull << IB_USER_VERBS_CMD_DEREG_MR) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_RESIZE_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_POLL_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_REQ_NOTIFY_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_QP) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_MODIFY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_POST_SEND) |\r\n(1ull << IB_USER_VERBS_CMD_POST_RECV) |\r\n(1ull << IB_USER_VERBS_CMD_ATTACH_MCAST) |\r\n(1ull << IB_USER_VERBS_CMD_DETACH_MCAST) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_MODIFY_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_POST_SRQ_RECV);\r\ndev->node_type = RDMA_NODE_IB_CA;\r\ndev->phys_port_cnt = 1;\r\ndev->num_comp_vectors = 1;\r\ndev->dma_device = &dd->pcidev->dev;\r\ndev->query_device = ipath_query_device;\r\ndev->modify_device = ipath_modify_device;\r\ndev->query_port = ipath_query_port;\r\ndev->modify_port = ipath_modify_port;\r\ndev->query_pkey = ipath_query_pkey;\r\ndev->query_gid = ipath_query_gid;\r\ndev->alloc_ucontext = ipath_alloc_ucontext;\r\ndev->dealloc_ucontext = ipath_dealloc_ucontext;\r\ndev->alloc_pd = ipath_alloc_pd;\r\ndev->dealloc_pd = ipath_dealloc_pd;\r\ndev->create_ah = ipath_create_ah;\r\ndev->destroy_ah = ipath_destroy_ah;\r\ndev->query_ah = ipath_query_ah;\r\ndev->create_srq = ipath_create_srq;\r\ndev->modify_srq = ipath_modify_srq;\r\ndev->query_srq = ipath_query_srq;\r\ndev->destroy_srq = ipath_destroy_srq;\r\ndev->create_qp = ipath_create_qp;\r\ndev->modify_qp = ipath_modify_qp;\r\ndev->query_qp = ipath_query_qp;\r\ndev->destroy_qp = ipath_destroy_qp;\r\ndev->post_send = ipath_post_send;\r\ndev->post_recv = ipath_post_receive;\r\ndev->post_srq_recv = ipath_post_srq_receive;\r\ndev->create_cq = ipath_create_cq;\r\ndev->destroy_cq = ipath_destroy_cq;\r\ndev->resize_cq = ipath_resize_cq;\r\ndev->poll_cq = ipath_poll_cq;\r\ndev->req_notify_cq = ipath_req_notify_cq;\r\ndev->get_dma_mr = ipath_get_dma_mr;\r\ndev->reg_phys_mr = ipath_reg_phys_mr;\r\ndev->reg_user_mr = ipath_reg_user_mr;\r\ndev->dereg_mr = ipath_dereg_mr;\r\ndev->alloc_fmr = ipath_alloc_fmr;\r\ndev->map_phys_fmr = ipath_map_phys_fmr;\r\ndev->unmap_fmr = ipath_unmap_fmr;\r\ndev->dealloc_fmr = ipath_dealloc_fmr;\r\ndev->attach_mcast = ipath_multicast_attach;\r\ndev->detach_mcast = ipath_multicast_detach;\r\ndev->process_mad = ipath_process_mad;\r\ndev->mmap = ipath_mmap;\r\ndev->dma_ops = &ipath_dma_mapping_ops;\r\nsnprintf(dev->node_desc, sizeof(dev->node_desc),\r\nIPATH_IDSTR " %s", init_utsname()->nodename);\r\nret = ib_register_device(dev, NULL);\r\nif (ret)\r\ngoto err_reg;\r\nif (ipath_verbs_register_sysfs(dev))\r\ngoto err_class;\r\nenable_timer(dd);\r\ngoto bail;\r\nerr_class:\r\nib_unregister_device(dev);\r\nerr_reg:\r\nkfree(idev->lk_table.table);\r\nerr_lk:\r\nkfree(idev->qp_table.table);\r\nerr_qp:\r\nkfree(idev->txreq_bufs);\r\nerr_tx:\r\nib_dealloc_device(dev);\r\nipath_dev_err(dd, "cannot register verbs: %d!\n", -ret);\r\nidev = NULL;\r\nbail:\r\ndd->verbs_dev = idev;\r\nreturn ret;\r\n}\r\nvoid ipath_unregister_ib_device(struct ipath_ibdev *dev)\r\n{\r\nstruct ib_device *ibdev = &dev->ibdev;\r\nu32 qps_inuse;\r\nib_unregister_device(ibdev);\r\ndisable_timer(dev->dd);\r\nif (!list_empty(&dev->pending[0]) ||\r\n!list_empty(&dev->pending[1]) ||\r\n!list_empty(&dev->pending[2]))\r\nipath_dev_err(dev->dd, "pending list not empty!\n");\r\nif (!list_empty(&dev->piowait))\r\nipath_dev_err(dev->dd, "piowait list not empty!\n");\r\nif (!list_empty(&dev->rnrwait))\r\nipath_dev_err(dev->dd, "rnrwait list not empty!\n");\r\nif (!ipath_mcast_tree_empty())\r\nipath_dev_err(dev->dd, "multicast table memory leak!\n");\r\nqps_inuse = ipath_free_all_qps(&dev->qp_table);\r\nif (qps_inuse)\r\nipath_dev_err(dev->dd, "QP memory leak! %u still in use\n",\r\nqps_inuse);\r\nkfree(dev->qp_table.table);\r\nkfree(dev->lk_table.table);\r\nkfree(dev->txreq_bufs);\r\nib_dealloc_device(ibdev);\r\n}\r\nstatic ssize_t show_rev(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct ipath_ibdev *dev =\r\ncontainer_of(device, struct ipath_ibdev, ibdev.dev);\r\nreturn sprintf(buf, "%x\n", dev->dd->ipath_pcirev);\r\n}\r\nstatic ssize_t show_hca(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct ipath_ibdev *dev =\r\ncontainer_of(device, struct ipath_ibdev, ibdev.dev);\r\nint ret;\r\nret = dev->dd->ipath_f_get_boardname(dev->dd, buf, 128);\r\nif (ret < 0)\r\ngoto bail;\r\nstrcat(buf, "\n");\r\nret = strlen(buf);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic ssize_t show_stats(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct ipath_ibdev *dev =\r\ncontainer_of(device, struct ipath_ibdev, ibdev.dev);\r\nint i;\r\nint len;\r\nlen = sprintf(buf,\r\n"RC resends %d\n"\r\n"RC no QACK %d\n"\r\n"RC ACKs %d\n"\r\n"RC SEQ NAKs %d\n"\r\n"RC RDMA seq %d\n"\r\n"RC RNR NAKs %d\n"\r\n"RC OTH NAKs %d\n"\r\n"RC timeouts %d\n"\r\n"RC RDMA dup %d\n"\r\n"piobuf wait %d\n"\r\n"unaligned %d\n"\r\n"PKT drops %d\n"\r\n"WQE errs %d\n",\r\ndev->n_rc_resends, dev->n_rc_qacks, dev->n_rc_acks,\r\ndev->n_seq_naks, dev->n_rdma_seq, dev->n_rnr_naks,\r\ndev->n_other_naks, dev->n_timeouts,\r\ndev->n_rdma_dup_busy, dev->n_piowait, dev->n_unaligned,\r\ndev->n_pkt_drops, dev->n_wqe_errs);\r\nfor (i = 0; i < ARRAY_SIZE(dev->opstats); i++) {\r\nconst struct ipath_opcode_stats *si = &dev->opstats[i];\r\nif (!si->n_packets && !si->n_bytes)\r\ncontinue;\r\nlen += sprintf(buf + len, "%02x %llu/%llu\n", i,\r\n(unsigned long long) si->n_packets,\r\n(unsigned long long) si->n_bytes);\r\n}\r\nreturn len;\r\n}\r\nstatic int ipath_verbs_register_sysfs(struct ib_device *dev)\r\n{\r\nint i;\r\nint ret;\r\nfor (i = 0; i < ARRAY_SIZE(ipath_class_attributes); ++i)\r\nif (device_create_file(&dev->dev,\r\nipath_class_attributes[i])) {\r\nret = 1;\r\ngoto bail;\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}
