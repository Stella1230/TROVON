static uint32_t calc_nr_buckets(unsigned nr_cells)\r\n{\r\nuint32_t n = 128;\r\nnr_cells /= 4;\r\nnr_cells = min(nr_cells, 8192u);\r\nwhile (n < nr_cells)\r\nn <<= 1;\r\nreturn n;\r\n}\r\nstatic struct bio_prison *prison_create(unsigned nr_cells)\r\n{\r\nunsigned i;\r\nuint32_t nr_buckets = calc_nr_buckets(nr_cells);\r\nsize_t len = sizeof(struct bio_prison) +\r\n(sizeof(struct hlist_head) * nr_buckets);\r\nstruct bio_prison *prison = kmalloc(len, GFP_KERNEL);\r\nif (!prison)\r\nreturn NULL;\r\nspin_lock_init(&prison->lock);\r\nprison->cell_pool = mempool_create_kmalloc_pool(nr_cells,\r\nsizeof(struct cell));\r\nif (!prison->cell_pool) {\r\nkfree(prison);\r\nreturn NULL;\r\n}\r\nprison->nr_buckets = nr_buckets;\r\nprison->hash_mask = nr_buckets - 1;\r\nprison->cells = (struct hlist_head *) (prison + 1);\r\nfor (i = 0; i < nr_buckets; i++)\r\nINIT_HLIST_HEAD(prison->cells + i);\r\nreturn prison;\r\n}\r\nstatic void prison_destroy(struct bio_prison *prison)\r\n{\r\nmempool_destroy(prison->cell_pool);\r\nkfree(prison);\r\n}\r\nstatic uint32_t hash_key(struct bio_prison *prison, struct cell_key *key)\r\n{\r\nconst unsigned long BIG_PRIME = 4294967291UL;\r\nuint64_t hash = key->block * BIG_PRIME;\r\nreturn (uint32_t) (hash & prison->hash_mask);\r\n}\r\nstatic int keys_equal(struct cell_key *lhs, struct cell_key *rhs)\r\n{\r\nreturn (lhs->virtual == rhs->virtual) &&\r\n(lhs->dev == rhs->dev) &&\r\n(lhs->block == rhs->block);\r\n}\r\nstatic struct cell *__search_bucket(struct hlist_head *bucket,\r\nstruct cell_key *key)\r\n{\r\nstruct cell *cell;\r\nstruct hlist_node *tmp;\r\nhlist_for_each_entry(cell, tmp, bucket, list)\r\nif (keys_equal(&cell->key, key))\r\nreturn cell;\r\nreturn NULL;\r\n}\r\nstatic int bio_detain(struct bio_prison *prison, struct cell_key *key,\r\nstruct bio *inmate, struct cell **ref)\r\n{\r\nint r;\r\nunsigned long flags;\r\nuint32_t hash = hash_key(prison, key);\r\nstruct cell *uninitialized_var(cell), *cell2 = NULL;\r\nBUG_ON(hash > prison->nr_buckets);\r\nspin_lock_irqsave(&prison->lock, flags);\r\ncell = __search_bucket(prison->cells + hash, key);\r\nif (!cell) {\r\nspin_unlock_irqrestore(&prison->lock, flags);\r\ncell2 = mempool_alloc(prison->cell_pool, GFP_NOIO);\r\nspin_lock_irqsave(&prison->lock, flags);\r\ncell = __search_bucket(prison->cells + hash, key);\r\nif (!cell) {\r\ncell = cell2;\r\ncell2 = NULL;\r\ncell->prison = prison;\r\nmemcpy(&cell->key, key, sizeof(cell->key));\r\ncell->count = 0;\r\nbio_list_init(&cell->bios);\r\nhlist_add_head(&cell->list, prison->cells + hash);\r\n}\r\n}\r\nr = cell->count++;\r\nbio_list_add(&cell->bios, inmate);\r\nspin_unlock_irqrestore(&prison->lock, flags);\r\nif (cell2)\r\nmempool_free(cell2, prison->cell_pool);\r\n*ref = cell;\r\nreturn r;\r\n}\r\nstatic void __cell_release(struct cell *cell, struct bio_list *inmates)\r\n{\r\nstruct bio_prison *prison = cell->prison;\r\nhlist_del(&cell->list);\r\nif (inmates)\r\nbio_list_merge(inmates, &cell->bios);\r\nmempool_free(cell, prison->cell_pool);\r\n}\r\nstatic void cell_release(struct cell *cell, struct bio_list *bios)\r\n{\r\nunsigned long flags;\r\nstruct bio_prison *prison = cell->prison;\r\nspin_lock_irqsave(&prison->lock, flags);\r\n__cell_release(cell, bios);\r\nspin_unlock_irqrestore(&prison->lock, flags);\r\n}\r\nstatic void cell_release_singleton(struct cell *cell, struct bio *bio)\r\n{\r\nstruct bio_prison *prison = cell->prison;\r\nstruct bio_list bios;\r\nstruct bio *b;\r\nunsigned long flags;\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&prison->lock, flags);\r\n__cell_release(cell, &bios);\r\nspin_unlock_irqrestore(&prison->lock, flags);\r\nb = bio_list_pop(&bios);\r\nBUG_ON(b != bio);\r\nBUG_ON(!bio_list_empty(&bios));\r\n}\r\nstatic void cell_error(struct cell *cell)\r\n{\r\nstruct bio_prison *prison = cell->prison;\r\nstruct bio_list bios;\r\nstruct bio *bio;\r\nunsigned long flags;\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&prison->lock, flags);\r\n__cell_release(cell, &bios);\r\nspin_unlock_irqrestore(&prison->lock, flags);\r\nwhile ((bio = bio_list_pop(&bios)))\r\nbio_io_error(bio);\r\n}\r\nstatic void ds_init(struct deferred_set *ds)\r\n{\r\nint i;\r\nspin_lock_init(&ds->lock);\r\nds->current_entry = 0;\r\nds->sweeper = 0;\r\nfor (i = 0; i < DEFERRED_SET_SIZE; i++) {\r\nds->entries[i].ds = ds;\r\nds->entries[i].count = 0;\r\nINIT_LIST_HEAD(&ds->entries[i].work_items);\r\n}\r\n}\r\nstatic struct deferred_entry *ds_inc(struct deferred_set *ds)\r\n{\r\nunsigned long flags;\r\nstruct deferred_entry *entry;\r\nspin_lock_irqsave(&ds->lock, flags);\r\nentry = ds->entries + ds->current_entry;\r\nentry->count++;\r\nspin_unlock_irqrestore(&ds->lock, flags);\r\nreturn entry;\r\n}\r\nstatic unsigned ds_next(unsigned index)\r\n{\r\nreturn (index + 1) % DEFERRED_SET_SIZE;\r\n}\r\nstatic void __sweep(struct deferred_set *ds, struct list_head *head)\r\n{\r\nwhile ((ds->sweeper != ds->current_entry) &&\r\n!ds->entries[ds->sweeper].count) {\r\nlist_splice_init(&ds->entries[ds->sweeper].work_items, head);\r\nds->sweeper = ds_next(ds->sweeper);\r\n}\r\nif ((ds->sweeper == ds->current_entry) && !ds->entries[ds->sweeper].count)\r\nlist_splice_init(&ds->entries[ds->sweeper].work_items, head);\r\n}\r\nstatic void ds_dec(struct deferred_entry *entry, struct list_head *head)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&entry->ds->lock, flags);\r\nBUG_ON(!entry->count);\r\n--entry->count;\r\n__sweep(entry->ds, head);\r\nspin_unlock_irqrestore(&entry->ds->lock, flags);\r\n}\r\nstatic int ds_add_work(struct deferred_set *ds, struct list_head *work)\r\n{\r\nint r = 1;\r\nunsigned long flags;\r\nunsigned next_entry;\r\nspin_lock_irqsave(&ds->lock, flags);\r\nif ((ds->sweeper == ds->current_entry) &&\r\n!ds->entries[ds->current_entry].count)\r\nr = 0;\r\nelse {\r\nlist_add(work, &ds->entries[ds->current_entry].work_items);\r\nnext_entry = ds_next(ds->current_entry);\r\nif (!ds->entries[next_entry].count)\r\nds->current_entry = next_entry;\r\n}\r\nspin_unlock_irqrestore(&ds->lock, flags);\r\nreturn r;\r\n}\r\nstatic void build_data_key(struct dm_thin_device *td,\r\ndm_block_t b, struct cell_key *key)\r\n{\r\nkey->virtual = 0;\r\nkey->dev = dm_thin_dev_id(td);\r\nkey->block = b;\r\n}\r\nstatic void build_virtual_key(struct dm_thin_device *td, dm_block_t b,\r\nstruct cell_key *key)\r\n{\r\nkey->virtual = 1;\r\nkey->dev = dm_thin_dev_id(td);\r\nkey->block = b;\r\n}\r\nstatic void pool_table_init(void)\r\n{\r\nmutex_init(&dm_thin_pool_table.mutex);\r\nINIT_LIST_HEAD(&dm_thin_pool_table.pools);\r\n}\r\nstatic void __pool_table_insert(struct pool *pool)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nlist_add(&pool->list, &dm_thin_pool_table.pools);\r\n}\r\nstatic void __pool_table_remove(struct pool *pool)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nlist_del(&pool->list);\r\n}\r\nstatic struct pool *__pool_table_lookup(struct mapped_device *md)\r\n{\r\nstruct pool *pool = NULL, *tmp;\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nlist_for_each_entry(tmp, &dm_thin_pool_table.pools, list) {\r\nif (tmp->pool_md == md) {\r\npool = tmp;\r\nbreak;\r\n}\r\n}\r\nreturn pool;\r\n}\r\nstatic struct pool *__pool_table_lookup_metadata_dev(struct block_device *md_dev)\r\n{\r\nstruct pool *pool = NULL, *tmp;\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nlist_for_each_entry(tmp, &dm_thin_pool_table.pools, list) {\r\nif (tmp->md_dev == md_dev) {\r\npool = tmp;\r\nbreak;\r\n}\r\n}\r\nreturn pool;\r\n}\r\nstatic void __requeue_bio_list(struct thin_c *tc, struct bio_list *master)\r\n{\r\nstruct bio *bio;\r\nstruct bio_list bios;\r\nbio_list_init(&bios);\r\nbio_list_merge(&bios, master);\r\nbio_list_init(master);\r\nwhile ((bio = bio_list_pop(&bios))) {\r\nif (dm_get_mapinfo(bio)->ptr == tc)\r\nbio_endio(bio, DM_ENDIO_REQUEUE);\r\nelse\r\nbio_list_add(master, bio);\r\n}\r\n}\r\nstatic void requeue_io(struct thin_c *tc)\r\n{\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\n__requeue_bio_list(tc, &pool->deferred_bios);\r\n__requeue_bio_list(tc, &pool->retry_on_resume_list);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic dm_block_t get_bio_block(struct thin_c *tc, struct bio *bio)\r\n{\r\nreturn bio->bi_sector >> tc->pool->block_shift;\r\n}\r\nstatic void remap(struct thin_c *tc, struct bio *bio, dm_block_t block)\r\n{\r\nstruct pool *pool = tc->pool;\r\nbio->bi_bdev = tc->pool_dev->bdev;\r\nbio->bi_sector = (block << pool->block_shift) +\r\n(bio->bi_sector & pool->offset_mask);\r\n}\r\nstatic void remap_and_issue(struct thin_c *tc, struct bio *bio,\r\ndm_block_t block)\r\n{\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nremap(tc, bio, block);\r\nif (bio->bi_rw & (REQ_FLUSH | REQ_FUA)) {\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio_list_add(&pool->deferred_flush_bios, bio);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n} else\r\ngeneric_make_request(bio);\r\n}\r\nstatic void wake_worker(struct pool *pool)\r\n{\r\nqueue_work(pool->wq, &pool->worker);\r\n}\r\nstatic void __maybe_add_mapping(struct new_mapping *m)\r\n{\r\nstruct pool *pool = m->tc->pool;\r\nif (list_empty(&m->list) && m->prepared) {\r\nlist_add(&m->list, &pool->prepared_mappings);\r\nwake_worker(pool);\r\n}\r\n}\r\nstatic void copy_complete(int read_err, unsigned long write_err, void *context)\r\n{\r\nunsigned long flags;\r\nstruct new_mapping *m = context;\r\nstruct pool *pool = m->tc->pool;\r\nm->err = read_err || write_err ? -EIO : 0;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nm->prepared = 1;\r\n__maybe_add_mapping(m);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic void overwrite_endio(struct bio *bio, int err)\r\n{\r\nunsigned long flags;\r\nstruct new_mapping *m = dm_get_mapinfo(bio)->ptr;\r\nstruct pool *pool = m->tc->pool;\r\nm->err = err;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nm->prepared = 1;\r\n__maybe_add_mapping(m);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic void shared_read_endio(struct bio *bio, int err)\r\n{\r\nstruct list_head mappings;\r\nstruct new_mapping *m, *tmp;\r\nstruct endio_hook *h = dm_get_mapinfo(bio)->ptr;\r\nunsigned long flags;\r\nstruct pool *pool = h->tc->pool;\r\nbio->bi_end_io = h->saved_bi_end_io;\r\nbio_endio(bio, err);\r\nINIT_LIST_HEAD(&mappings);\r\nds_dec(h->entry, &mappings);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nlist_for_each_entry_safe(m, tmp, &mappings, list) {\r\nlist_del(&m->list);\r\nINIT_LIST_HEAD(&m->list);\r\n__maybe_add_mapping(m);\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nmempool_free(h, pool->endio_hook_pool);\r\n}\r\nstatic void cell_defer(struct thin_c *tc, struct cell *cell,\r\ndm_block_t data_block)\r\n{\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\ncell_release(cell, &pool->deferred_bios);\r\nspin_unlock_irqrestore(&tc->pool->lock, flags);\r\nwake_worker(pool);\r\n}\r\nstatic void cell_defer_except(struct thin_c *tc, struct cell *cell,\r\nstruct bio *exception)\r\n{\r\nstruct bio_list bios;\r\nstruct bio *bio;\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nbio_list_init(&bios);\r\ncell_release(cell, &bios);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nwhile ((bio = bio_list_pop(&bios)))\r\nif (bio != exception)\r\nbio_list_add(&pool->deferred_bios, bio);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nwake_worker(pool);\r\n}\r\nstatic void process_prepared_mapping(struct new_mapping *m)\r\n{\r\nstruct thin_c *tc = m->tc;\r\nstruct bio *bio;\r\nint r;\r\nbio = m->bio;\r\nif (bio)\r\nbio->bi_end_io = m->saved_bi_end_io;\r\nif (m->err) {\r\ncell_error(m->cell);\r\nreturn;\r\n}\r\nr = dm_thin_insert_block(tc->td, m->virt_block, m->data_block);\r\nif (r) {\r\nDMERR("dm_thin_insert_block() failed");\r\ncell_error(m->cell);\r\nreturn;\r\n}\r\nif (bio) {\r\ncell_defer_except(tc, m->cell, bio);\r\nbio_endio(bio, 0);\r\n} else\r\ncell_defer(tc, m->cell, m->data_block);\r\nlist_del(&m->list);\r\nmempool_free(m, tc->pool->mapping_pool);\r\n}\r\nstatic void process_prepared_mappings(struct pool *pool)\r\n{\r\nunsigned long flags;\r\nstruct list_head maps;\r\nstruct new_mapping *m, *tmp;\r\nINIT_LIST_HEAD(&maps);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nlist_splice_init(&pool->prepared_mappings, &maps);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nlist_for_each_entry_safe(m, tmp, &maps, list)\r\nprocess_prepared_mapping(m);\r\n}\r\nstatic int io_overwrites_block(struct pool *pool, struct bio *bio)\r\n{\r\nreturn ((bio_data_dir(bio) == WRITE) &&\r\n!(bio->bi_sector & pool->offset_mask)) &&\r\n(bio->bi_size == (pool->sectors_per_block << SECTOR_SHIFT));\r\n}\r\nstatic void save_and_set_endio(struct bio *bio, bio_end_io_t **save,\r\nbio_end_io_t *fn)\r\n{\r\n*save = bio->bi_end_io;\r\nbio->bi_end_io = fn;\r\n}\r\nstatic int ensure_next_mapping(struct pool *pool)\r\n{\r\nif (pool->next_mapping)\r\nreturn 0;\r\npool->next_mapping = mempool_alloc(pool->mapping_pool, GFP_ATOMIC);\r\nreturn pool->next_mapping ? 0 : -ENOMEM;\r\n}\r\nstatic struct new_mapping *get_next_mapping(struct pool *pool)\r\n{\r\nstruct new_mapping *r = pool->next_mapping;\r\nBUG_ON(!pool->next_mapping);\r\npool->next_mapping = NULL;\r\nreturn r;\r\n}\r\nstatic void schedule_copy(struct thin_c *tc, dm_block_t virt_block,\r\ndm_block_t data_origin, dm_block_t data_dest,\r\nstruct cell *cell, struct bio *bio)\r\n{\r\nint r;\r\nstruct pool *pool = tc->pool;\r\nstruct new_mapping *m = get_next_mapping(pool);\r\nINIT_LIST_HEAD(&m->list);\r\nm->prepared = 0;\r\nm->tc = tc;\r\nm->virt_block = virt_block;\r\nm->data_block = data_dest;\r\nm->cell = cell;\r\nm->err = 0;\r\nm->bio = NULL;\r\nds_add_work(&pool->ds, &m->list);\r\nif (io_overwrites_block(pool, bio)) {\r\nm->bio = bio;\r\nsave_and_set_endio(bio, &m->saved_bi_end_io, overwrite_endio);\r\ndm_get_mapinfo(bio)->ptr = m;\r\nremap_and_issue(tc, bio, data_dest);\r\n} else {\r\nstruct dm_io_region from, to;\r\nfrom.bdev = tc->pool_dev->bdev;\r\nfrom.sector = data_origin * pool->sectors_per_block;\r\nfrom.count = pool->sectors_per_block;\r\nto.bdev = tc->pool_dev->bdev;\r\nto.sector = data_dest * pool->sectors_per_block;\r\nto.count = pool->sectors_per_block;\r\nr = dm_kcopyd_copy(pool->copier, &from, 1, &to,\r\n0, copy_complete, m);\r\nif (r < 0) {\r\nmempool_free(m, pool->mapping_pool);\r\nDMERR("dm_kcopyd_copy() failed");\r\ncell_error(cell);\r\n}\r\n}\r\n}\r\nstatic void schedule_zero(struct thin_c *tc, dm_block_t virt_block,\r\ndm_block_t data_block, struct cell *cell,\r\nstruct bio *bio)\r\n{\r\nstruct pool *pool = tc->pool;\r\nstruct new_mapping *m = get_next_mapping(pool);\r\nINIT_LIST_HEAD(&m->list);\r\nm->prepared = 0;\r\nm->tc = tc;\r\nm->virt_block = virt_block;\r\nm->data_block = data_block;\r\nm->cell = cell;\r\nm->err = 0;\r\nm->bio = NULL;\r\nif (!pool->zero_new_blocks)\r\nprocess_prepared_mapping(m);\r\nelse if (io_overwrites_block(pool, bio)) {\r\nm->bio = bio;\r\nsave_and_set_endio(bio, &m->saved_bi_end_io, overwrite_endio);\r\ndm_get_mapinfo(bio)->ptr = m;\r\nremap_and_issue(tc, bio, data_block);\r\n} else {\r\nint r;\r\nstruct dm_io_region to;\r\nto.bdev = tc->pool_dev->bdev;\r\nto.sector = data_block * pool->sectors_per_block;\r\nto.count = pool->sectors_per_block;\r\nr = dm_kcopyd_zero(pool->copier, 1, &to, 0, copy_complete, m);\r\nif (r < 0) {\r\nmempool_free(m, pool->mapping_pool);\r\nDMERR("dm_kcopyd_zero() failed");\r\ncell_error(cell);\r\n}\r\n}\r\n}\r\nstatic int alloc_data_block(struct thin_c *tc, dm_block_t *result)\r\n{\r\nint r;\r\ndm_block_t free_blocks;\r\nunsigned long flags;\r\nstruct pool *pool = tc->pool;\r\nr = dm_pool_get_free_block_count(pool->pmd, &free_blocks);\r\nif (r)\r\nreturn r;\r\nif (free_blocks <= pool->low_water_blocks && !pool->low_water_triggered) {\r\nDMWARN("%s: reached low water mark, sending event.",\r\ndm_device_name(pool->pool_md));\r\nspin_lock_irqsave(&pool->lock, flags);\r\npool->low_water_triggered = 1;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\ndm_table_event(pool->ti->table);\r\n}\r\nif (!free_blocks) {\r\nif (pool->no_free_space)\r\nreturn -ENOSPC;\r\nelse {\r\nr = dm_pool_commit_metadata(pool->pmd);\r\nif (r) {\r\nDMERR("%s: dm_pool_commit_metadata() failed, error = %d",\r\n__func__, r);\r\nreturn r;\r\n}\r\nr = dm_pool_get_free_block_count(pool->pmd, &free_blocks);\r\nif (r)\r\nreturn r;\r\nif (!free_blocks) {\r\nDMWARN("%s: no free space available.",\r\ndm_device_name(pool->pool_md));\r\nspin_lock_irqsave(&pool->lock, flags);\r\npool->no_free_space = 1;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn -ENOSPC;\r\n}\r\n}\r\n}\r\nr = dm_pool_alloc_data_block(pool->pmd, result);\r\nif (r)\r\nreturn r;\r\nreturn 0;\r\n}\r\nstatic void retry_on_resume(struct bio *bio)\r\n{\r\nstruct thin_c *tc = dm_get_mapinfo(bio)->ptr;\r\nstruct pool *pool = tc->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio_list_add(&pool->retry_on_resume_list, bio);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic void no_space(struct cell *cell)\r\n{\r\nstruct bio *bio;\r\nstruct bio_list bios;\r\nbio_list_init(&bios);\r\ncell_release(cell, &bios);\r\nwhile ((bio = bio_list_pop(&bios)))\r\nretry_on_resume(bio);\r\n}\r\nstatic void break_sharing(struct thin_c *tc, struct bio *bio, dm_block_t block,\r\nstruct cell_key *key,\r\nstruct dm_thin_lookup_result *lookup_result,\r\nstruct cell *cell)\r\n{\r\nint r;\r\ndm_block_t data_block;\r\nr = alloc_data_block(tc, &data_block);\r\nswitch (r) {\r\ncase 0:\r\nschedule_copy(tc, block, lookup_result->block,\r\ndata_block, cell, bio);\r\nbreak;\r\ncase -ENOSPC:\r\nno_space(cell);\r\nbreak;\r\ndefault:\r\nDMERR("%s: alloc_data_block() failed, error = %d", __func__, r);\r\ncell_error(cell);\r\nbreak;\r\n}\r\n}\r\nstatic void process_shared_bio(struct thin_c *tc, struct bio *bio,\r\ndm_block_t block,\r\nstruct dm_thin_lookup_result *lookup_result)\r\n{\r\nstruct cell *cell;\r\nstruct pool *pool = tc->pool;\r\nstruct cell_key key;\r\nbuild_data_key(tc->td, lookup_result->block, &key);\r\nif (bio_detain(pool->prison, &key, bio, &cell))\r\nreturn;\r\nif (bio_data_dir(bio) == WRITE)\r\nbreak_sharing(tc, bio, block, &key, lookup_result, cell);\r\nelse {\r\nstruct endio_hook *h;\r\nh = mempool_alloc(pool->endio_hook_pool, GFP_NOIO);\r\nh->tc = tc;\r\nh->entry = ds_inc(&pool->ds);\r\nsave_and_set_endio(bio, &h->saved_bi_end_io, shared_read_endio);\r\ndm_get_mapinfo(bio)->ptr = h;\r\ncell_release_singleton(cell, bio);\r\nremap_and_issue(tc, bio, lookup_result->block);\r\n}\r\n}\r\nstatic void provision_block(struct thin_c *tc, struct bio *bio, dm_block_t block,\r\nstruct cell *cell)\r\n{\r\nint r;\r\ndm_block_t data_block;\r\nif (!bio->bi_size) {\r\ncell_release_singleton(cell, bio);\r\nremap_and_issue(tc, bio, 0);\r\nreturn;\r\n}\r\nif (bio_data_dir(bio) == READ) {\r\nzero_fill_bio(bio);\r\ncell_release_singleton(cell, bio);\r\nbio_endio(bio, 0);\r\nreturn;\r\n}\r\nr = alloc_data_block(tc, &data_block);\r\nswitch (r) {\r\ncase 0:\r\nschedule_zero(tc, block, data_block, cell, bio);\r\nbreak;\r\ncase -ENOSPC:\r\nno_space(cell);\r\nbreak;\r\ndefault:\r\nDMERR("%s: alloc_data_block() failed, error = %d", __func__, r);\r\ncell_error(cell);\r\nbreak;\r\n}\r\n}\r\nstatic void process_bio(struct thin_c *tc, struct bio *bio)\r\n{\r\nint r;\r\ndm_block_t block = get_bio_block(tc, bio);\r\nstruct cell *cell;\r\nstruct cell_key key;\r\nstruct dm_thin_lookup_result lookup_result;\r\nbuild_virtual_key(tc->td, block, &key);\r\nif (bio_detain(tc->pool->prison, &key, bio, &cell))\r\nreturn;\r\nr = dm_thin_find_block(tc->td, block, 1, &lookup_result);\r\nswitch (r) {\r\ncase 0:\r\ncell_release_singleton(cell, bio);\r\nif (lookup_result.shared)\r\nprocess_shared_bio(tc, bio, block, &lookup_result);\r\nelse\r\nremap_and_issue(tc, bio, lookup_result.block);\r\nbreak;\r\ncase -ENODATA:\r\nprovision_block(tc, bio, block, cell);\r\nbreak;\r\ndefault:\r\nDMERR("dm_thin_find_block() failed, error = %d", r);\r\nbio_io_error(bio);\r\nbreak;\r\n}\r\n}\r\nstatic void process_deferred_bios(struct pool *pool)\r\n{\r\nunsigned long flags;\r\nstruct bio *bio;\r\nstruct bio_list bios;\r\nint r;\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio_list_merge(&bios, &pool->deferred_bios);\r\nbio_list_init(&pool->deferred_bios);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nwhile ((bio = bio_list_pop(&bios))) {\r\nstruct thin_c *tc = dm_get_mapinfo(bio)->ptr;\r\nif (ensure_next_mapping(pool)) {\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio_list_merge(&pool->deferred_bios, &bios);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nbreak;\r\n}\r\nprocess_bio(tc, bio);\r\n}\r\nbio_list_init(&bios);\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio_list_merge(&bios, &pool->deferred_flush_bios);\r\nbio_list_init(&pool->deferred_flush_bios);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nif (bio_list_empty(&bios))\r\nreturn;\r\nr = dm_pool_commit_metadata(pool->pmd);\r\nif (r) {\r\nDMERR("%s: dm_pool_commit_metadata() failed, error = %d",\r\n__func__, r);\r\nwhile ((bio = bio_list_pop(&bios)))\r\nbio_io_error(bio);\r\nreturn;\r\n}\r\nwhile ((bio = bio_list_pop(&bios)))\r\ngeneric_make_request(bio);\r\n}\r\nstatic void do_worker(struct work_struct *ws)\r\n{\r\nstruct pool *pool = container_of(ws, struct pool, worker);\r\nprocess_prepared_mappings(pool);\r\nprocess_deferred_bios(pool);\r\n}\r\nstatic void thin_defer_bio(struct thin_c *tc, struct bio *bio)\r\n{\r\nunsigned long flags;\r\nstruct pool *pool = tc->pool;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio_list_add(&pool->deferred_bios, bio);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nwake_worker(pool);\r\n}\r\nstatic int thin_bio_map(struct dm_target *ti, struct bio *bio,\r\nunion map_info *map_context)\r\n{\r\nint r;\r\nstruct thin_c *tc = ti->private;\r\ndm_block_t block = get_bio_block(tc, bio);\r\nstruct dm_thin_device *td = tc->td;\r\nstruct dm_thin_lookup_result result;\r\nmap_context->ptr = tc;\r\nif (bio->bi_rw & (REQ_FLUSH | REQ_FUA)) {\r\nthin_defer_bio(tc, bio);\r\nreturn DM_MAPIO_SUBMITTED;\r\n}\r\nr = dm_thin_find_block(td, block, 0, &result);\r\nswitch (r) {\r\ncase 0:\r\nif (unlikely(result.shared)) {\r\nthin_defer_bio(tc, bio);\r\nr = DM_MAPIO_SUBMITTED;\r\n} else {\r\nremap(tc, bio, result.block);\r\nr = DM_MAPIO_REMAPPED;\r\n}\r\nbreak;\r\ncase -ENODATA:\r\ncase -EWOULDBLOCK:\r\nthin_defer_bio(tc, bio);\r\nr = DM_MAPIO_SUBMITTED;\r\nbreak;\r\n}\r\nreturn r;\r\n}\r\nstatic int pool_is_congested(struct dm_target_callbacks *cb, int bdi_bits)\r\n{\r\nint r;\r\nunsigned long flags;\r\nstruct pool_c *pt = container_of(cb, struct pool_c, callbacks);\r\nspin_lock_irqsave(&pt->pool->lock, flags);\r\nr = !bio_list_empty(&pt->pool->retry_on_resume_list);\r\nspin_unlock_irqrestore(&pt->pool->lock, flags);\r\nif (!r) {\r\nstruct request_queue *q = bdev_get_queue(pt->data_dev->bdev);\r\nr = bdi_congested(&q->backing_dev_info, bdi_bits);\r\n}\r\nreturn r;\r\n}\r\nstatic void __requeue_bios(struct pool *pool)\r\n{\r\nbio_list_merge(&pool->deferred_bios, &pool->retry_on_resume_list);\r\nbio_list_init(&pool->retry_on_resume_list);\r\n}\r\nstatic int bind_control_target(struct pool *pool, struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\npool->ti = ti;\r\npool->low_water_blocks = pt->low_water_blocks;\r\npool->zero_new_blocks = pt->zero_new_blocks;\r\nreturn 0;\r\n}\r\nstatic void unbind_control_target(struct pool *pool, struct dm_target *ti)\r\n{\r\nif (pool->ti == ti)\r\npool->ti = NULL;\r\n}\r\nstatic void __pool_destroy(struct pool *pool)\r\n{\r\n__pool_table_remove(pool);\r\nif (dm_pool_metadata_close(pool->pmd) < 0)\r\nDMWARN("%s: dm_pool_metadata_close() failed.", __func__);\r\nprison_destroy(pool->prison);\r\ndm_kcopyd_client_destroy(pool->copier);\r\nif (pool->wq)\r\ndestroy_workqueue(pool->wq);\r\nif (pool->next_mapping)\r\nmempool_free(pool->next_mapping, pool->mapping_pool);\r\nmempool_destroy(pool->mapping_pool);\r\nmempool_destroy(pool->endio_hook_pool);\r\nkfree(pool);\r\n}\r\nstatic struct pool *pool_create(struct mapped_device *pool_md,\r\nstruct block_device *metadata_dev,\r\nunsigned long block_size, char **error)\r\n{\r\nint r;\r\nvoid *err_p;\r\nstruct pool *pool;\r\nstruct dm_pool_metadata *pmd;\r\npmd = dm_pool_metadata_open(metadata_dev, block_size);\r\nif (IS_ERR(pmd)) {\r\n*error = "Error creating metadata object";\r\nreturn (struct pool *)pmd;\r\n}\r\npool = kmalloc(sizeof(*pool), GFP_KERNEL);\r\nif (!pool) {\r\n*error = "Error allocating memory for pool";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_pool;\r\n}\r\npool->pmd = pmd;\r\npool->sectors_per_block = block_size;\r\npool->block_shift = ffs(block_size) - 1;\r\npool->offset_mask = block_size - 1;\r\npool->low_water_blocks = 0;\r\npool->zero_new_blocks = 1;\r\npool->prison = prison_create(PRISON_CELLS);\r\nif (!pool->prison) {\r\n*error = "Error creating pool's bio prison";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_prison;\r\n}\r\npool->copier = dm_kcopyd_client_create();\r\nif (IS_ERR(pool->copier)) {\r\nr = PTR_ERR(pool->copier);\r\n*error = "Error creating pool's kcopyd client";\r\nerr_p = ERR_PTR(r);\r\ngoto bad_kcopyd_client;\r\n}\r\npool->wq = alloc_ordered_workqueue("dm-" DM_MSG_PREFIX, WQ_MEM_RECLAIM);\r\nif (!pool->wq) {\r\n*error = "Error creating pool's workqueue";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_wq;\r\n}\r\nINIT_WORK(&pool->worker, do_worker);\r\nspin_lock_init(&pool->lock);\r\nbio_list_init(&pool->deferred_bios);\r\nbio_list_init(&pool->deferred_flush_bios);\r\nINIT_LIST_HEAD(&pool->prepared_mappings);\r\npool->low_water_triggered = 0;\r\npool->no_free_space = 0;\r\nbio_list_init(&pool->retry_on_resume_list);\r\nds_init(&pool->ds);\r\npool->next_mapping = NULL;\r\npool->mapping_pool =\r\nmempool_create_kmalloc_pool(MAPPING_POOL_SIZE, sizeof(struct new_mapping));\r\nif (!pool->mapping_pool) {\r\n*error = "Error creating pool's mapping mempool";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_mapping_pool;\r\n}\r\npool->endio_hook_pool =\r\nmempool_create_kmalloc_pool(ENDIO_HOOK_POOL_SIZE, sizeof(struct endio_hook));\r\nif (!pool->endio_hook_pool) {\r\n*error = "Error creating pool's endio_hook mempool";\r\nerr_p = ERR_PTR(-ENOMEM);\r\ngoto bad_endio_hook_pool;\r\n}\r\npool->ref_count = 1;\r\npool->pool_md = pool_md;\r\npool->md_dev = metadata_dev;\r\n__pool_table_insert(pool);\r\nreturn pool;\r\nbad_endio_hook_pool:\r\nmempool_destroy(pool->mapping_pool);\r\nbad_mapping_pool:\r\ndestroy_workqueue(pool->wq);\r\nbad_wq:\r\ndm_kcopyd_client_destroy(pool->copier);\r\nbad_kcopyd_client:\r\nprison_destroy(pool->prison);\r\nbad_prison:\r\nkfree(pool);\r\nbad_pool:\r\nif (dm_pool_metadata_close(pmd))\r\nDMWARN("%s: dm_pool_metadata_close() failed.", __func__);\r\nreturn err_p;\r\n}\r\nstatic void __pool_inc(struct pool *pool)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\npool->ref_count++;\r\n}\r\nstatic void __pool_dec(struct pool *pool)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\r\nBUG_ON(!pool->ref_count);\r\nif (!--pool->ref_count)\r\n__pool_destroy(pool);\r\n}\r\nstatic struct pool *__pool_find(struct mapped_device *pool_md,\r\nstruct block_device *metadata_dev,\r\nunsigned long block_size, char **error)\r\n{\r\nstruct pool *pool = __pool_table_lookup_metadata_dev(metadata_dev);\r\nif (pool) {\r\nif (pool->pool_md != pool_md)\r\nreturn ERR_PTR(-EBUSY);\r\n__pool_inc(pool);\r\n} else {\r\npool = __pool_table_lookup(pool_md);\r\nif (pool) {\r\nif (pool->md_dev != metadata_dev)\r\nreturn ERR_PTR(-EINVAL);\r\n__pool_inc(pool);\r\n} else\r\npool = pool_create(pool_md, metadata_dev, block_size, error);\r\n}\r\nreturn pool;\r\n}\r\nstatic void pool_dtr(struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\nunbind_control_target(pt->pool, ti);\r\n__pool_dec(pt->pool);\r\ndm_put_device(ti, pt->metadata_dev);\r\ndm_put_device(ti, pt->data_dev);\r\nkfree(pt);\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\n}\r\nstatic int parse_pool_features(struct dm_arg_set *as, struct pool_features *pf,\r\nstruct dm_target *ti)\r\n{\r\nint r;\r\nunsigned argc;\r\nconst char *arg_name;\r\nstatic struct dm_arg _args[] = {\r\n{0, 1, "Invalid number of pool feature arguments"},\r\n};\r\nif (!as->argc)\r\nreturn 0;\r\nr = dm_read_arg_group(_args, as, &argc, &ti->error);\r\nif (r)\r\nreturn -EINVAL;\r\nwhile (argc && !r) {\r\narg_name = dm_shift_arg(as);\r\nargc--;\r\nif (!strcasecmp(arg_name, "skip_block_zeroing")) {\r\npf->zero_new_blocks = 0;\r\ncontinue;\r\n}\r\nti->error = "Unrecognised pool feature requested";\r\nr = -EINVAL;\r\n}\r\nreturn r;\r\n}\r\nstatic int pool_ctr(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint r;\r\nstruct pool_c *pt;\r\nstruct pool *pool;\r\nstruct pool_features pf;\r\nstruct dm_arg_set as;\r\nstruct dm_dev *data_dev;\r\nunsigned long block_size;\r\ndm_block_t low_water_blocks;\r\nstruct dm_dev *metadata_dev;\r\nsector_t metadata_dev_size;\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\nif (argc < 4) {\r\nti->error = "Invalid argument count";\r\nr = -EINVAL;\r\ngoto out_unlock;\r\n}\r\nas.argc = argc;\r\nas.argv = argv;\r\nr = dm_get_device(ti, argv[0], FMODE_READ | FMODE_WRITE, &metadata_dev);\r\nif (r) {\r\nti->error = "Error opening metadata block device";\r\ngoto out_unlock;\r\n}\r\nmetadata_dev_size = i_size_read(metadata_dev->bdev->bd_inode) >> SECTOR_SHIFT;\r\nif (metadata_dev_size > METADATA_DEV_MAX_SECTORS) {\r\nti->error = "Metadata device is too large";\r\nr = -EINVAL;\r\ngoto out_metadata;\r\n}\r\nr = dm_get_device(ti, argv[1], FMODE_READ | FMODE_WRITE, &data_dev);\r\nif (r) {\r\nti->error = "Error getting data device";\r\ngoto out_metadata;\r\n}\r\nif (kstrtoul(argv[2], 10, &block_size) || !block_size ||\r\nblock_size < DATA_DEV_BLOCK_SIZE_MIN_SECTORS ||\r\nblock_size > DATA_DEV_BLOCK_SIZE_MAX_SECTORS ||\r\n!is_power_of_2(block_size)) {\r\nti->error = "Invalid block size";\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nif (kstrtoull(argv[3], 10, (unsigned long long *)&low_water_blocks)) {\r\nti->error = "Invalid low water mark";\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nmemset(&pf, 0, sizeof(pf));\r\npf.zero_new_blocks = 1;\r\ndm_consume_args(&as, 4);\r\nr = parse_pool_features(&as, &pf, ti);\r\nif (r)\r\ngoto out;\r\npt = kzalloc(sizeof(*pt), GFP_KERNEL);\r\nif (!pt) {\r\nr = -ENOMEM;\r\ngoto out;\r\n}\r\npool = __pool_find(dm_table_get_md(ti->table), metadata_dev->bdev,\r\nblock_size, &ti->error);\r\nif (IS_ERR(pool)) {\r\nr = PTR_ERR(pool);\r\ngoto out_free_pt;\r\n}\r\npt->pool = pool;\r\npt->ti = ti;\r\npt->metadata_dev = metadata_dev;\r\npt->data_dev = data_dev;\r\npt->low_water_blocks = low_water_blocks;\r\npt->zero_new_blocks = pf.zero_new_blocks;\r\nti->num_flush_requests = 1;\r\nti->num_discard_requests = 0;\r\nti->private = pt;\r\npt->callbacks.congested_fn = pool_is_congested;\r\ndm_table_add_target_callbacks(ti->table, &pt->callbacks);\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\nreturn 0;\r\nout_free_pt:\r\nkfree(pt);\r\nout:\r\ndm_put_device(ti, data_dev);\r\nout_metadata:\r\ndm_put_device(ti, metadata_dev);\r\nout_unlock:\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\nreturn r;\r\n}\r\nstatic int pool_map(struct dm_target *ti, struct bio *bio,\r\nunion map_info *map_context)\r\n{\r\nint r;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nbio->bi_bdev = pt->data_dev->bdev;\r\nr = DM_MAPIO_REMAPPED;\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn r;\r\n}\r\nstatic int pool_preresume(struct dm_target *ti)\r\n{\r\nint r;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\ndm_block_t data_size, sb_data_size;\r\nr = bind_control_target(pool, ti);\r\nif (r)\r\nreturn r;\r\ndata_size = ti->len >> pool->block_shift;\r\nr = dm_pool_get_data_dev_size(pool->pmd, &sb_data_size);\r\nif (r) {\r\nDMERR("failed to retrieve data device size");\r\nreturn r;\r\n}\r\nif (data_size < sb_data_size) {\r\nDMERR("pool target too small, is %llu blocks (expected %llu)",\r\ndata_size, sb_data_size);\r\nreturn -EINVAL;\r\n} else if (data_size > sb_data_size) {\r\nr = dm_pool_resize_data_dev(pool->pmd, data_size);\r\nif (r) {\r\nDMERR("failed to resize data device");\r\nreturn r;\r\n}\r\nr = dm_pool_commit_metadata(pool->pmd);\r\nif (r) {\r\nDMERR("%s: dm_pool_commit_metadata() failed, error = %d",\r\n__func__, r);\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void pool_resume(struct dm_target *ti)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\npool->low_water_triggered = 0;\r\npool->no_free_space = 0;\r\n__requeue_bios(pool);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nwake_worker(pool);\r\n}\r\nstatic void pool_postsuspend(struct dm_target *ti)\r\n{\r\nint r;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nflush_workqueue(pool->wq);\r\nr = dm_pool_commit_metadata(pool->pmd);\r\nif (r < 0) {\r\nDMERR("%s: dm_pool_commit_metadata() failed, error = %d",\r\n__func__, r);\r\n}\r\n}\r\nstatic int check_arg_count(unsigned argc, unsigned args_required)\r\n{\r\nif (argc != args_required) {\r\nDMWARN("Message received with %u arguments instead of %u.",\r\nargc, args_required);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int read_dev_id(char *arg, dm_thin_id *dev_id, int warning)\r\n{\r\nif (!kstrtoull(arg, 10, (unsigned long long *)dev_id) &&\r\n*dev_id <= MAX_DEV_ID)\r\nreturn 0;\r\nif (warning)\r\nDMWARN("Message received with invalid device id: %s", arg);\r\nreturn -EINVAL;\r\n}\r\nstatic int process_create_thin_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\ndm_thin_id dev_id;\r\nint r;\r\nr = check_arg_count(argc, 2);\r\nif (r)\r\nreturn r;\r\nr = read_dev_id(argv[1], &dev_id, 1);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_create_thin(pool->pmd, dev_id);\r\nif (r) {\r\nDMWARN("Creation of new thinly-provisioned device with id %s failed.",\r\nargv[1]);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic int process_create_snap_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\ndm_thin_id dev_id;\r\ndm_thin_id origin_dev_id;\r\nint r;\r\nr = check_arg_count(argc, 3);\r\nif (r)\r\nreturn r;\r\nr = read_dev_id(argv[1], &dev_id, 1);\r\nif (r)\r\nreturn r;\r\nr = read_dev_id(argv[2], &origin_dev_id, 1);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_create_snap(pool->pmd, dev_id, origin_dev_id);\r\nif (r) {\r\nDMWARN("Creation of new snapshot %s of device %s failed.",\r\nargv[1], argv[2]);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic int process_delete_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\ndm_thin_id dev_id;\r\nint r;\r\nr = check_arg_count(argc, 2);\r\nif (r)\r\nreturn r;\r\nr = read_dev_id(argv[1], &dev_id, 1);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_delete_thin_device(pool->pmd, dev_id);\r\nif (r)\r\nDMWARN("Deletion of thin device %s failed.", argv[1]);\r\nreturn r;\r\n}\r\nstatic int process_set_transaction_id_mesg(unsigned argc, char **argv, struct pool *pool)\r\n{\r\ndm_thin_id old_id, new_id;\r\nint r;\r\nr = check_arg_count(argc, 3);\r\nif (r)\r\nreturn r;\r\nif (kstrtoull(argv[1], 10, (unsigned long long *)&old_id)) {\r\nDMWARN("set_transaction_id message: Unrecognised id %s.", argv[1]);\r\nreturn -EINVAL;\r\n}\r\nif (kstrtoull(argv[2], 10, (unsigned long long *)&new_id)) {\r\nDMWARN("set_transaction_id message: Unrecognised new id %s.", argv[2]);\r\nreturn -EINVAL;\r\n}\r\nr = dm_pool_set_metadata_transaction_id(pool->pmd, old_id, new_id);\r\nif (r) {\r\nDMWARN("Failed to change transaction id from %s to %s.",\r\nargv[1], argv[2]);\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pool_message(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint r = -EINVAL;\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nif (!strcasecmp(argv[0], "create_thin"))\r\nr = process_create_thin_mesg(argc, argv, pool);\r\nelse if (!strcasecmp(argv[0], "create_snap"))\r\nr = process_create_snap_mesg(argc, argv, pool);\r\nelse if (!strcasecmp(argv[0], "delete"))\r\nr = process_delete_mesg(argc, argv, pool);\r\nelse if (!strcasecmp(argv[0], "set_transaction_id"))\r\nr = process_set_transaction_id_mesg(argc, argv, pool);\r\nelse\r\nDMWARN("Unrecognised thin pool target message received: %s", argv[0]);\r\nif (!r) {\r\nr = dm_pool_commit_metadata(pool->pmd);\r\nif (r)\r\nDMERR("%s message: dm_pool_commit_metadata() failed, error = %d",\r\nargv[0], r);\r\n}\r\nreturn r;\r\n}\r\nstatic int pool_status(struct dm_target *ti, status_type_t type,\r\nchar *result, unsigned maxlen)\r\n{\r\nint r;\r\nunsigned sz = 0;\r\nuint64_t transaction_id;\r\ndm_block_t nr_free_blocks_data;\r\ndm_block_t nr_free_blocks_metadata;\r\ndm_block_t nr_blocks_data;\r\ndm_block_t nr_blocks_metadata;\r\ndm_block_t held_root;\r\nchar buf[BDEVNAME_SIZE];\r\nchar buf2[BDEVNAME_SIZE];\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\nr = dm_pool_get_metadata_transaction_id(pool->pmd,\r\n&transaction_id);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_get_free_metadata_block_count(pool->pmd,\r\n&nr_free_blocks_metadata);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_get_metadata_dev_size(pool->pmd, &nr_blocks_metadata);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_get_free_block_count(pool->pmd,\r\n&nr_free_blocks_data);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_get_data_dev_size(pool->pmd, &nr_blocks_data);\r\nif (r)\r\nreturn r;\r\nr = dm_pool_get_held_metadata_root(pool->pmd, &held_root);\r\nif (r)\r\nreturn r;\r\nDMEMIT("%llu %llu/%llu %llu/%llu ",\r\n(unsigned long long)transaction_id,\r\n(unsigned long long)(nr_blocks_metadata - nr_free_blocks_metadata),\r\n(unsigned long long)nr_blocks_metadata,\r\n(unsigned long long)(nr_blocks_data - nr_free_blocks_data),\r\n(unsigned long long)nr_blocks_data);\r\nif (held_root)\r\nDMEMIT("%llu", held_root);\r\nelse\r\nDMEMIT("-");\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nDMEMIT("%s %s %lu %llu ",\r\nformat_dev_t(buf, pt->metadata_dev->bdev->bd_dev),\r\nformat_dev_t(buf2, pt->data_dev->bdev->bd_dev),\r\n(unsigned long)pool->sectors_per_block,\r\n(unsigned long long)pt->low_water_blocks);\r\nDMEMIT("%u ", !pool->zero_new_blocks);\r\nif (!pool->zero_new_blocks)\r\nDMEMIT("skip_block_zeroing ");\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pool_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nreturn fn(ti, pt->data_dev, 0, ti->len, data);\r\n}\r\nstatic int pool_merge(struct dm_target *ti, struct bvec_merge_data *bvm,\r\nstruct bio_vec *biovec, int max_size)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct request_queue *q = bdev_get_queue(pt->data_dev->bdev);\r\nif (!q->merge_bvec_fn)\r\nreturn max_size;\r\nbvm->bi_bdev = pt->data_dev->bdev;\r\nreturn min(max_size, q->merge_bvec_fn(q, bvm, biovec));\r\n}\r\nstatic void pool_io_hints(struct dm_target *ti, struct queue_limits *limits)\r\n{\r\nstruct pool_c *pt = ti->private;\r\nstruct pool *pool = pt->pool;\r\nblk_limits_io_min(limits, 0);\r\nblk_limits_io_opt(limits, pool->sectors_per_block << SECTOR_SHIFT);\r\n}\r\nstatic void thin_dtr(struct dm_target *ti)\r\n{\r\nstruct thin_c *tc = ti->private;\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\n__pool_dec(tc->pool);\r\ndm_pool_close_thin_device(tc->td);\r\ndm_put_device(ti, tc->pool_dev);\r\nkfree(tc);\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\n}\r\nstatic int thin_ctr(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint r;\r\nstruct thin_c *tc;\r\nstruct dm_dev *pool_dev;\r\nstruct mapped_device *pool_md;\r\nmutex_lock(&dm_thin_pool_table.mutex);\r\nif (argc != 2) {\r\nti->error = "Invalid argument count";\r\nr = -EINVAL;\r\ngoto out_unlock;\r\n}\r\ntc = ti->private = kzalloc(sizeof(*tc), GFP_KERNEL);\r\nif (!tc) {\r\nti->error = "Out of memory";\r\nr = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\nr = dm_get_device(ti, argv[0], dm_table_get_mode(ti->table), &pool_dev);\r\nif (r) {\r\nti->error = "Error opening pool device";\r\ngoto bad_pool_dev;\r\n}\r\ntc->pool_dev = pool_dev;\r\nif (read_dev_id(argv[1], (unsigned long long *)&tc->dev_id, 0)) {\r\nti->error = "Invalid device id";\r\nr = -EINVAL;\r\ngoto bad_common;\r\n}\r\npool_md = dm_get_md(tc->pool_dev->bdev->bd_dev);\r\nif (!pool_md) {\r\nti->error = "Couldn't get pool mapped device";\r\nr = -EINVAL;\r\ngoto bad_common;\r\n}\r\ntc->pool = __pool_table_lookup(pool_md);\r\nif (!tc->pool) {\r\nti->error = "Couldn't find pool object";\r\nr = -EINVAL;\r\ngoto bad_pool_lookup;\r\n}\r\n__pool_inc(tc->pool);\r\nr = dm_pool_open_thin_device(tc->pool->pmd, tc->dev_id, &tc->td);\r\nif (r) {\r\nti->error = "Couldn't open thin internal device";\r\ngoto bad_thin_open;\r\n}\r\nti->split_io = tc->pool->sectors_per_block;\r\nti->num_flush_requests = 1;\r\nti->num_discard_requests = 0;\r\nti->discards_supported = 0;\r\ndm_put(pool_md);\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\nreturn 0;\r\nbad_thin_open:\r\n__pool_dec(tc->pool);\r\nbad_pool_lookup:\r\ndm_put(pool_md);\r\nbad_common:\r\ndm_put_device(ti, tc->pool_dev);\r\nbad_pool_dev:\r\nkfree(tc);\r\nout_unlock:\r\nmutex_unlock(&dm_thin_pool_table.mutex);\r\nreturn r;\r\n}\r\nstatic int thin_map(struct dm_target *ti, struct bio *bio,\r\nunion map_info *map_context)\r\n{\r\nbio->bi_sector -= ti->begin;\r\nreturn thin_bio_map(ti, bio, map_context);\r\n}\r\nstatic void thin_postsuspend(struct dm_target *ti)\r\n{\r\nif (dm_noflush_suspending(ti))\r\nrequeue_io((struct thin_c *)ti->private);\r\n}\r\nstatic int thin_status(struct dm_target *ti, status_type_t type,\r\nchar *result, unsigned maxlen)\r\n{\r\nint r;\r\nssize_t sz = 0;\r\ndm_block_t mapped, highest;\r\nchar buf[BDEVNAME_SIZE];\r\nstruct thin_c *tc = ti->private;\r\nif (!tc->td)\r\nDMEMIT("-");\r\nelse {\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\nr = dm_thin_get_mapped_count(tc->td, &mapped);\r\nif (r)\r\nreturn r;\r\nr = dm_thin_get_highest_mapped_block(tc->td, &highest);\r\nif (r < 0)\r\nreturn r;\r\nDMEMIT("%llu ", mapped * tc->pool->sectors_per_block);\r\nif (r)\r\nDMEMIT("%llu", ((highest + 1) *\r\ntc->pool->sectors_per_block) - 1);\r\nelse\r\nDMEMIT("-");\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nDMEMIT("%s %lu",\r\nformat_dev_t(buf, tc->pool_dev->bdev->bd_dev),\r\n(unsigned long) tc->dev_id);\r\nbreak;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int thin_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\ndm_block_t blocks;\r\nstruct thin_c *tc = ti->private;\r\nif (!tc->pool->ti)\r\nreturn 0;\r\nblocks = tc->pool->ti->len >> tc->pool->block_shift;\r\nif (blocks)\r\nreturn fn(ti, tc->pool_dev, 0, tc->pool->sectors_per_block * blocks, data);\r\nreturn 0;\r\n}\r\nstatic void thin_io_hints(struct dm_target *ti, struct queue_limits *limits)\r\n{\r\nstruct thin_c *tc = ti->private;\r\nblk_limits_io_min(limits, 0);\r\nblk_limits_io_opt(limits, tc->pool->sectors_per_block << SECTOR_SHIFT);\r\n}\r\nstatic int __init dm_thin_init(void)\r\n{\r\nint r;\r\npool_table_init();\r\nr = dm_register_target(&thin_target);\r\nif (r)\r\nreturn r;\r\nr = dm_register_target(&pool_target);\r\nif (r)\r\ndm_unregister_target(&thin_target);\r\nreturn r;\r\n}\r\nstatic void dm_thin_exit(void)\r\n{\r\ndm_unregister_target(&thin_target);\r\ndm_unregister_target(&pool_target);\r\n}
