static int iser_start_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nint dma_nents;\r\nstruct ib_device *dev;\r\nchar *mem = NULL;\r\nstruct iser_data_buf *data = &iser_task->data[cmd_dir];\r\nunsigned long cmd_data_len = data->data_len;\r\nif (cmd_data_len > ISER_KMALLOC_THRESHOLD)\r\nmem = (void *)__get_free_pages(GFP_ATOMIC,\r\nilog2(roundup_pow_of_two(cmd_data_len)) - PAGE_SHIFT);\r\nelse\r\nmem = kmalloc(cmd_data_len, GFP_ATOMIC);\r\nif (mem == NULL) {\r\niser_err("Failed to allocate mem size %d %d for copying sglist\n",\r\ndata->size,(int)cmd_data_len);\r\nreturn -ENOMEM;\r\n}\r\nif (cmd_dir == ISER_DIR_OUT) {\r\nstruct scatterlist *sgl = (struct scatterlist *)data->buf;\r\nstruct scatterlist *sg;\r\nint i;\r\nchar *p, *from;\r\np = mem;\r\nfor_each_sg(sgl, sg, data->size, i) {\r\nfrom = kmap_atomic(sg_page(sg), KM_USER0);\r\nmemcpy(p,\r\nfrom + sg->offset,\r\nsg->length);\r\nkunmap_atomic(from, KM_USER0);\r\np += sg->length;\r\n}\r\n}\r\nsg_init_one(&iser_task->data_copy[cmd_dir].sg_single, mem, cmd_data_len);\r\niser_task->data_copy[cmd_dir].buf =\r\n&iser_task->data_copy[cmd_dir].sg_single;\r\niser_task->data_copy[cmd_dir].size = 1;\r\niser_task->data_copy[cmd_dir].copy_buf = mem;\r\ndev = iser_task->iser_conn->ib_conn->device->ib_device;\r\ndma_nents = ib_dma_map_sg(dev,\r\n&iser_task->data_copy[cmd_dir].sg_single,\r\n1,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nBUG_ON(dma_nents == 0);\r\niser_task->data_copy[cmd_dir].dma_nents = dma_nents;\r\nreturn 0;\r\n}\r\nvoid iser_finalize_rdma_unaligned_sg(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct ib_device *dev;\r\nstruct iser_data_buf *mem_copy;\r\nunsigned long cmd_data_len;\r\ndev = iser_task->iser_conn->ib_conn->device->ib_device;\r\nmem_copy = &iser_task->data_copy[cmd_dir];\r\nib_dma_unmap_sg(dev, &mem_copy->sg_single, 1,\r\n(cmd_dir == ISER_DIR_OUT) ?\r\nDMA_TO_DEVICE : DMA_FROM_DEVICE);\r\nif (cmd_dir == ISER_DIR_IN) {\r\nchar *mem;\r\nstruct scatterlist *sgl, *sg;\r\nunsigned char *p, *to;\r\nunsigned int sg_size;\r\nint i;\r\nmem = mem_copy->copy_buf;\r\nsgl = (struct scatterlist *)iser_task->data[ISER_DIR_IN].buf;\r\nsg_size = iser_task->data[ISER_DIR_IN].size;\r\np = mem;\r\nfor_each_sg(sgl, sg, sg_size, i) {\r\nto = kmap_atomic(sg_page(sg), KM_SOFTIRQ0);\r\nmemcpy(to + sg->offset,\r\np,\r\nsg->length);\r\nkunmap_atomic(to, KM_SOFTIRQ0);\r\np += sg->length;\r\n}\r\n}\r\ncmd_data_len = iser_task->data[cmd_dir].data_len;\r\nif (cmd_data_len > ISER_KMALLOC_THRESHOLD)\r\nfree_pages((unsigned long)mem_copy->copy_buf,\r\nilog2(roundup_pow_of_two(cmd_data_len)) - PAGE_SHIFT);\r\nelse\r\nkfree(mem_copy->copy_buf);\r\nmem_copy->copy_buf = NULL;\r\n}\r\nstatic int iser_sg_to_page_vec(struct iser_data_buf *data,\r\nstruct iser_page_vec *page_vec,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sg, *sgl = (struct scatterlist *)data->buf;\r\nu64 start_addr, end_addr, page, chunk_start = 0;\r\nunsigned long total_sz = 0;\r\nunsigned int dma_len;\r\nint i, new_chunk, cur_page, last_ent = data->dma_nents - 1;\r\npage_vec->offset = (u64) sgl[0].offset & ~MASK_4K;\r\nnew_chunk = 1;\r\ncur_page = 0;\r\nfor_each_sg(sgl, sg, data->dma_nents, i) {\r\nstart_addr = ib_sg_dma_address(ibdev, sg);\r\nif (new_chunk)\r\nchunk_start = start_addr;\r\ndma_len = ib_sg_dma_len(ibdev, sg);\r\nend_addr = start_addr + dma_len;\r\ntotal_sz += dma_len;\r\nif (!IS_4K_ALIGNED(end_addr) && i < last_ent) {\r\nnew_chunk = 0;\r\ncontinue;\r\n}\r\nnew_chunk = 1;\r\npage = chunk_start & MASK_4K;\r\ndo {\r\npage_vec->pages[cur_page++] = page;\r\npage += SIZE_4K;\r\n} while (page < end_addr);\r\n}\r\npage_vec->data_size = total_sz;\r\niser_dbg("page_vec->data_size:%d cur_page %d\n", page_vec->data_size,cur_page);\r\nreturn cur_page;\r\n}\r\nstatic int iser_data_buf_aligned_len(struct iser_data_buf *data,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sgl, *sg, *next_sg = NULL;\r\nu64 start_addr, end_addr;\r\nint i, ret_len, start_check = 0;\r\nif (data->dma_nents == 1)\r\nreturn 1;\r\nsgl = (struct scatterlist *)data->buf;\r\nstart_addr = ib_sg_dma_address(ibdev, sgl);\r\nfor_each_sg(sgl, sg, data->dma_nents, i) {\r\nif (start_check && !IS_4K_ALIGNED(start_addr))\r\nbreak;\r\nnext_sg = sg_next(sg);\r\nif (!next_sg)\r\nbreak;\r\nend_addr = start_addr + ib_sg_dma_len(ibdev, sg);\r\nstart_addr = ib_sg_dma_address(ibdev, next_sg);\r\nif (end_addr == start_addr) {\r\nstart_check = 0;\r\ncontinue;\r\n} else\r\nstart_check = 1;\r\nif (!IS_4K_ALIGNED(end_addr))\r\nbreak;\r\n}\r\nret_len = (next_sg) ? i : i+1;\r\niser_dbg("Found %d aligned entries out of %d in sg:0x%p\n",\r\nret_len, data->dma_nents, data);\r\nreturn ret_len;\r\n}\r\nstatic void iser_data_buf_dump(struct iser_data_buf *data,\r\nstruct ib_device *ibdev)\r\n{\r\nstruct scatterlist *sgl = (struct scatterlist *)data->buf;\r\nstruct scatterlist *sg;\r\nint i;\r\nif (iser_debug_level == 0)\r\nreturn;\r\nfor_each_sg(sgl, sg, data->dma_nents, i)\r\niser_warn("sg[%d] dma_addr:0x%lX page:0x%p "\r\n"off:0x%x sz:0x%x dma_len:0x%x\n",\r\ni, (unsigned long)ib_sg_dma_address(ibdev, sg),\r\nsg_page(sg), sg->offset,\r\nsg->length, ib_sg_dma_len(ibdev, sg));\r\n}\r\nstatic void iser_dump_page_vec(struct iser_page_vec *page_vec)\r\n{\r\nint i;\r\niser_err("page vec length %d data size %d\n",\r\npage_vec->length, page_vec->data_size);\r\nfor (i = 0; i < page_vec->length; i++)\r\niser_err("%d %lx\n",i,(unsigned long)page_vec->pages[i]);\r\n}\r\nstatic void iser_page_vec_build(struct iser_data_buf *data,\r\nstruct iser_page_vec *page_vec,\r\nstruct ib_device *ibdev)\r\n{\r\nint page_vec_len = 0;\r\npage_vec->length = 0;\r\npage_vec->offset = 0;\r\niser_dbg("Translating sg sz: %d\n", data->dma_nents);\r\npage_vec_len = iser_sg_to_page_vec(data, page_vec, ibdev);\r\niser_dbg("sg len %d page_vec_len %d\n", data->dma_nents,page_vec_len);\r\npage_vec->length = page_vec_len;\r\nif (page_vec_len * SIZE_4K < page_vec->data_size) {\r\niser_err("page_vec too short to hold this SG\n");\r\niser_data_buf_dump(data, ibdev);\r\niser_dump_page_vec(page_vec);\r\nBUG();\r\n}\r\n}\r\nint iser_dma_map_task_data(struct iscsi_iser_task *iser_task,\r\nstruct iser_data_buf *data,\r\nenum iser_data_dir iser_dir,\r\nenum dma_data_direction dma_dir)\r\n{\r\nstruct ib_device *dev;\r\niser_task->dir[iser_dir] = 1;\r\ndev = iser_task->iser_conn->ib_conn->device->ib_device;\r\ndata->dma_nents = ib_dma_map_sg(dev, data->buf, data->size, dma_dir);\r\nif (data->dma_nents == 0) {\r\niser_err("dma_map_sg failed!!!\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nvoid iser_dma_unmap_task_data(struct iscsi_iser_task *iser_task)\r\n{\r\nstruct ib_device *dev;\r\nstruct iser_data_buf *data;\r\ndev = iser_task->iser_conn->ib_conn->device->ib_device;\r\nif (iser_task->dir[ISER_DIR_IN]) {\r\ndata = &iser_task->data[ISER_DIR_IN];\r\nib_dma_unmap_sg(dev, data->buf, data->size, DMA_FROM_DEVICE);\r\n}\r\nif (iser_task->dir[ISER_DIR_OUT]) {\r\ndata = &iser_task->data[ISER_DIR_OUT];\r\nib_dma_unmap_sg(dev, data->buf, data->size, DMA_TO_DEVICE);\r\n}\r\n}\r\nint iser_reg_rdma_mem(struct iscsi_iser_task *iser_task,\r\nenum iser_data_dir cmd_dir)\r\n{\r\nstruct iscsi_conn *iscsi_conn = iser_task->iser_conn->iscsi_conn;\r\nstruct iser_conn *ib_conn = iser_task->iser_conn->ib_conn;\r\nstruct iser_device *device = ib_conn->device;\r\nstruct ib_device *ibdev = device->ib_device;\r\nstruct iser_data_buf *mem = &iser_task->data[cmd_dir];\r\nstruct iser_regd_buf *regd_buf;\r\nint aligned_len;\r\nint err;\r\nint i;\r\nstruct scatterlist *sg;\r\nregd_buf = &iser_task->rdma_regd[cmd_dir];\r\naligned_len = iser_data_buf_aligned_len(mem, ibdev);\r\nif (aligned_len != mem->dma_nents) {\r\niscsi_conn->fmr_unalign_cnt++;\r\niser_warn("rdma alignment violation %d/%d aligned\n",\r\naligned_len, mem->size);\r\niser_data_buf_dump(mem, ibdev);\r\niser_dma_unmap_task_data(iser_task);\r\nif (iser_start_rdma_unaligned_sg(iser_task, cmd_dir) != 0)\r\nreturn -ENOMEM;\r\nmem = &iser_task->data_copy[cmd_dir];\r\n}\r\nif (mem->dma_nents == 1) {\r\nsg = (struct scatterlist *)mem->buf;\r\nregd_buf->reg.lkey = device->mr->lkey;\r\nregd_buf->reg.rkey = device->mr->rkey;\r\nregd_buf->reg.len = ib_sg_dma_len(ibdev, &sg[0]);\r\nregd_buf->reg.va = ib_sg_dma_address(ibdev, &sg[0]);\r\nregd_buf->reg.is_fmr = 0;\r\niser_dbg("PHYSICAL Mem.register: lkey: 0x%08X rkey: 0x%08X "\r\n"va: 0x%08lX sz: %ld]\n",\r\n(unsigned int)regd_buf->reg.lkey,\r\n(unsigned int)regd_buf->reg.rkey,\r\n(unsigned long)regd_buf->reg.va,\r\n(unsigned long)regd_buf->reg.len);\r\n} else {\r\niser_page_vec_build(mem, ib_conn->page_vec, ibdev);\r\nerr = iser_reg_page_vec(ib_conn, ib_conn->page_vec, &regd_buf->reg);\r\nif (err) {\r\niser_data_buf_dump(mem, ibdev);\r\niser_err("mem->dma_nents = %d (dlength = 0x%x)\n",\r\nmem->dma_nents,\r\nntoh24(iser_task->desc.iscsi_header.dlength));\r\niser_err("page_vec: data_size = 0x%x, length = %d, offset = 0x%x\n",\r\nib_conn->page_vec->data_size, ib_conn->page_vec->length,\r\nib_conn->page_vec->offset);\r\nfor (i=0 ; i<ib_conn->page_vec->length ; i++)\r\niser_err("page_vec[%d] = 0x%llx\n", i,\r\n(unsigned long long) ib_conn->page_vec->pages[i]);\r\nreturn err;\r\n}\r\n}\r\nreturn 0;\r\n}
