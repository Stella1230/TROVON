static dma_addr_t xor_get_src(struct ioat_raw_descriptor *descs[2], int idx)\r\n{\r\nstruct ioat_raw_descriptor *raw = descs[xor_idx_to_desc >> idx & 1];\r\nreturn raw->field[xor_idx_to_field[idx]];\r\n}\r\nstatic void xor_set_src(struct ioat_raw_descriptor *descs[2],\r\ndma_addr_t addr, u32 offset, int idx)\r\n{\r\nstruct ioat_raw_descriptor *raw = descs[xor_idx_to_desc >> idx & 1];\r\nraw->field[xor_idx_to_field[idx]] = addr + offset;\r\n}\r\nstatic dma_addr_t pq_get_src(struct ioat_raw_descriptor *descs[2], int idx)\r\n{\r\nstruct ioat_raw_descriptor *raw = descs[pq_idx_to_desc >> idx & 1];\r\nreturn raw->field[pq_idx_to_field[idx]];\r\n}\r\nstatic void pq_set_src(struct ioat_raw_descriptor *descs[2],\r\ndma_addr_t addr, u32 offset, u8 coef, int idx)\r\n{\r\nstruct ioat_pq_descriptor *pq = (struct ioat_pq_descriptor *) descs[0];\r\nstruct ioat_raw_descriptor *raw = descs[pq_idx_to_desc >> idx & 1];\r\nraw->field[pq_idx_to_field[idx]] = addr + offset;\r\npq->coef[idx] = coef;\r\n}\r\nstatic void ioat3_dma_unmap(struct ioat2_dma_chan *ioat,\r\nstruct ioat_ring_ent *desc, int idx)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nstruct pci_dev *pdev = chan->device->pdev;\r\nsize_t len = desc->len;\r\nsize_t offset = len - desc->hw->size;\r\nstruct dma_async_tx_descriptor *tx = &desc->txd;\r\nenum dma_ctrl_flags flags = tx->flags;\r\nswitch (desc->hw->ctl_f.op) {\r\ncase IOAT_OP_COPY:\r\nif (!desc->hw->ctl_f.null)\r\nioat_dma_unmap(chan, flags, len, desc->hw);\r\nbreak;\r\ncase IOAT_OP_FILL: {\r\nstruct ioat_fill_descriptor *hw = desc->fill;\r\nif (!(flags & DMA_COMPL_SKIP_DEST_UNMAP))\r\nioat_unmap(pdev, hw->dst_addr - offset, len,\r\nPCI_DMA_FROMDEVICE, flags, 1);\r\nbreak;\r\n}\r\ncase IOAT_OP_XOR_VAL:\r\ncase IOAT_OP_XOR: {\r\nstruct ioat_xor_descriptor *xor = desc->xor;\r\nstruct ioat_ring_ent *ext;\r\nstruct ioat_xor_ext_descriptor *xor_ex = NULL;\r\nint src_cnt = src_cnt_to_sw(xor->ctl_f.src_cnt);\r\nstruct ioat_raw_descriptor *descs[2];\r\nint i;\r\nif (src_cnt > 5) {\r\next = ioat2_get_ring_ent(ioat, idx + 1);\r\nxor_ex = ext->xor_ex;\r\n}\r\nif (!(flags & DMA_COMPL_SKIP_SRC_UNMAP)) {\r\ndescs[0] = (struct ioat_raw_descriptor *) xor;\r\ndescs[1] = (struct ioat_raw_descriptor *) xor_ex;\r\nfor (i = 0; i < src_cnt; i++) {\r\ndma_addr_t src = xor_get_src(descs, i);\r\nioat_unmap(pdev, src - offset, len,\r\nPCI_DMA_TODEVICE, flags, 0);\r\n}\r\nif (xor->ctl_f.op == IOAT_OP_XOR_VAL) {\r\nioat_unmap(pdev, xor->dst_addr - offset, len,\r\nPCI_DMA_TODEVICE, flags, 1);\r\nbreak;\r\n}\r\n}\r\nif (!(flags & DMA_COMPL_SKIP_DEST_UNMAP))\r\nioat_unmap(pdev, xor->dst_addr - offset, len,\r\nPCI_DMA_FROMDEVICE, flags, 1);\r\nbreak;\r\n}\r\ncase IOAT_OP_PQ_VAL:\r\ncase IOAT_OP_PQ: {\r\nstruct ioat_pq_descriptor *pq = desc->pq;\r\nstruct ioat_ring_ent *ext;\r\nstruct ioat_pq_ext_descriptor *pq_ex = NULL;\r\nint src_cnt = src_cnt_to_sw(pq->ctl_f.src_cnt);\r\nstruct ioat_raw_descriptor *descs[2];\r\nint i;\r\nif (src_cnt > 3) {\r\next = ioat2_get_ring_ent(ioat, idx + 1);\r\npq_ex = ext->pq_ex;\r\n}\r\nif (dmaf_p_disabled_continue(flags))\r\nsrc_cnt--;\r\nelse if (dmaf_continue(flags))\r\nsrc_cnt -= 3;\r\nif (!(flags & DMA_COMPL_SKIP_SRC_UNMAP)) {\r\ndescs[0] = (struct ioat_raw_descriptor *) pq;\r\ndescs[1] = (struct ioat_raw_descriptor *) pq_ex;\r\nfor (i = 0; i < src_cnt; i++) {\r\ndma_addr_t src = pq_get_src(descs, i);\r\nioat_unmap(pdev, src - offset, len,\r\nPCI_DMA_TODEVICE, flags, 0);\r\n}\r\nif (pq->ctl_f.op == IOAT_OP_XOR_VAL) {\r\nif (!(flags & DMA_PREP_PQ_DISABLE_P))\r\nioat_unmap(pdev, pq->p_addr - offset,\r\nlen, PCI_DMA_TODEVICE, flags, 0);\r\nif (!(flags & DMA_PREP_PQ_DISABLE_Q))\r\nioat_unmap(pdev, pq->q_addr - offset,\r\nlen, PCI_DMA_TODEVICE, flags, 0);\r\nbreak;\r\n}\r\n}\r\nif (!(flags & DMA_COMPL_SKIP_DEST_UNMAP)) {\r\nif (!(flags & DMA_PREP_PQ_DISABLE_P))\r\nioat_unmap(pdev, pq->p_addr - offset, len,\r\nPCI_DMA_BIDIRECTIONAL, flags, 1);\r\nif (!(flags & DMA_PREP_PQ_DISABLE_Q))\r\nioat_unmap(pdev, pq->q_addr - offset, len,\r\nPCI_DMA_BIDIRECTIONAL, flags, 1);\r\n}\r\nbreak;\r\n}\r\ndefault:\r\ndev_err(&pdev->dev, "%s: unknown op type: %#x\n",\r\n__func__, desc->hw->ctl_f.op);\r\n}\r\n}\r\nstatic bool desc_has_ext(struct ioat_ring_ent *desc)\r\n{\r\nstruct ioat_dma_descriptor *hw = desc->hw;\r\nif (hw->ctl_f.op == IOAT_OP_XOR ||\r\nhw->ctl_f.op == IOAT_OP_XOR_VAL) {\r\nstruct ioat_xor_descriptor *xor = desc->xor;\r\nif (src_cnt_to_sw(xor->ctl_f.src_cnt) > 5)\r\nreturn true;\r\n} else if (hw->ctl_f.op == IOAT_OP_PQ ||\r\nhw->ctl_f.op == IOAT_OP_PQ_VAL) {\r\nstruct ioat_pq_descriptor *pq = desc->pq;\r\nif (src_cnt_to_sw(pq->ctl_f.src_cnt) > 3)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic void __cleanup(struct ioat2_dma_chan *ioat, unsigned long phys_complete)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nstruct ioat_ring_ent *desc;\r\nbool seen_current = false;\r\nint idx = ioat->tail, i;\r\nu16 active;\r\ndev_dbg(to_dev(chan), "%s: head: %#x tail: %#x issued: %#x\n",\r\n__func__, ioat->head, ioat->tail, ioat->issued);\r\nactive = ioat2_ring_active(ioat);\r\nfor (i = 0; i < active && !seen_current; i++) {\r\nstruct dma_async_tx_descriptor *tx;\r\nsmp_read_barrier_depends();\r\nprefetch(ioat2_get_ring_ent(ioat, idx + i + 1));\r\ndesc = ioat2_get_ring_ent(ioat, idx + i);\r\ndump_desc_dbg(ioat, desc);\r\ntx = &desc->txd;\r\nif (tx->cookie) {\r\nchan->completed_cookie = tx->cookie;\r\nioat3_dma_unmap(ioat, desc, idx + i);\r\ntx->cookie = 0;\r\nif (tx->callback) {\r\ntx->callback(tx->callback_param);\r\ntx->callback = NULL;\r\n}\r\n}\r\nif (tx->phys == phys_complete)\r\nseen_current = true;\r\nif (desc_has_ext(desc)) {\r\nBUG_ON(i + 1 >= active);\r\ni++;\r\n}\r\n}\r\nsmp_mb();\r\nioat->tail = idx + i;\r\nBUG_ON(active && !seen_current);\r\nchan->last_completion = phys_complete;\r\nif (active - i == 0) {\r\ndev_dbg(to_dev(chan), "%s: cancel completion timeout\n",\r\n__func__);\r\nclear_bit(IOAT_COMPLETION_PENDING, &chan->state);\r\nmod_timer(&chan->timer, jiffies + IDLE_TIMEOUT);\r\n}\r\nwritew(min((5 * (active - i)), IOAT_INTRDELAY_MASK),\r\nchan->device->reg_base + IOAT_INTRDELAY_OFFSET);\r\n}\r\nstatic void ioat3_cleanup(struct ioat2_dma_chan *ioat)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nunsigned long phys_complete;\r\nspin_lock_bh(&chan->cleanup_lock);\r\nif (ioat_cleanup_preamble(chan, &phys_complete))\r\n__cleanup(ioat, phys_complete);\r\nspin_unlock_bh(&chan->cleanup_lock);\r\n}\r\nstatic void ioat3_cleanup_event(unsigned long data)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan((void *) data);\r\nioat3_cleanup(ioat);\r\nwritew(IOAT_CHANCTRL_RUN, ioat->base.reg_base + IOAT_CHANCTRL_OFFSET);\r\n}\r\nstatic void ioat3_restart_channel(struct ioat2_dma_chan *ioat)\r\n{\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nunsigned long phys_complete;\r\nioat2_quiesce(chan, 0);\r\nif (ioat_cleanup_preamble(chan, &phys_complete))\r\n__cleanup(ioat, phys_complete);\r\n__ioat2_restart_chan(ioat);\r\n}\r\nstatic void ioat3_timer_event(unsigned long data)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan((void *) data);\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nif (test_bit(IOAT_COMPLETION_PENDING, &chan->state)) {\r\nunsigned long phys_complete;\r\nu64 status;\r\nstatus = ioat_chansts(chan);\r\nif (is_ioat_halted(status)) {\r\nu32 chanerr;\r\nchanerr = readl(chan->reg_base + IOAT_CHANERR_OFFSET);\r\ndev_err(to_dev(chan), "%s: Channel halted (%x)\n",\r\n__func__, chanerr);\r\nif (test_bit(IOAT_RUN, &chan->state))\r\nBUG_ON(is_ioat_bug(chanerr));\r\nelse\r\nreturn;\r\n}\r\nspin_lock_bh(&chan->cleanup_lock);\r\nif (ioat_cleanup_preamble(chan, &phys_complete))\r\n__cleanup(ioat, phys_complete);\r\nelse if (test_bit(IOAT_COMPLETION_ACK, &chan->state)) {\r\nspin_lock_bh(&ioat->prep_lock);\r\nioat3_restart_channel(ioat);\r\nspin_unlock_bh(&ioat->prep_lock);\r\n} else {\r\nset_bit(IOAT_COMPLETION_ACK, &chan->state);\r\nmod_timer(&chan->timer, jiffies + COMPLETION_TIMEOUT);\r\n}\r\nspin_unlock_bh(&chan->cleanup_lock);\r\n} else {\r\nu16 active;\r\nspin_lock_bh(&chan->cleanup_lock);\r\nspin_lock_bh(&ioat->prep_lock);\r\nactive = ioat2_ring_active(ioat);\r\nif (active == 0 && ioat->alloc_order > ioat_get_alloc_order())\r\nreshape_ring(ioat, ioat->alloc_order-1);\r\nspin_unlock_bh(&ioat->prep_lock);\r\nspin_unlock_bh(&chan->cleanup_lock);\r\nif (ioat->alloc_order > ioat_get_alloc_order())\r\nmod_timer(&chan->timer, jiffies + IDLE_TIMEOUT);\r\n}\r\n}\r\nstatic enum dma_status\r\nioat3_tx_status(struct dma_chan *c, dma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nif (ioat_tx_status(c, cookie, txstate) == DMA_SUCCESS)\r\nreturn DMA_SUCCESS;\r\nioat3_cleanup(ioat);\r\nreturn ioat_tx_status(c, cookie, txstate);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nioat3_prep_memset_lock(struct dma_chan *c, dma_addr_t dest, int value,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nstruct ioat_ring_ent *desc;\r\nsize_t total_len = len;\r\nstruct ioat_fill_descriptor *fill;\r\nu64 src_data = (0x0101010101010101ULL) * (value & 0xff);\r\nint num_descs, idx, i;\r\nnum_descs = ioat2_xferlen_to_descs(ioat, len);\r\nif (likely(num_descs) && ioat2_check_space_lock(ioat, num_descs) == 0)\r\nidx = ioat->head;\r\nelse\r\nreturn NULL;\r\ni = 0;\r\ndo {\r\nsize_t xfer_size = min_t(size_t, len, 1 << ioat->xfercap_log);\r\ndesc = ioat2_get_ring_ent(ioat, idx + i);\r\nfill = desc->fill;\r\nfill->size = xfer_size;\r\nfill->src_data = src_data;\r\nfill->dst_addr = dest;\r\nfill->ctl = 0;\r\nfill->ctl_f.op = IOAT_OP_FILL;\r\nlen -= xfer_size;\r\ndest += xfer_size;\r\ndump_desc_dbg(ioat, desc);\r\n} while (++i < num_descs);\r\ndesc->txd.flags = flags;\r\ndesc->len = total_len;\r\nfill->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);\r\nfill->ctl_f.fence = !!(flags & DMA_PREP_FENCE);\r\nfill->ctl_f.compl_write = 1;\r\ndump_desc_dbg(ioat, desc);\r\nreturn &desc->txd;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\n__ioat3_prep_xor_lock(struct dma_chan *c, enum sum_check_flags *result,\r\ndma_addr_t dest, dma_addr_t *src, unsigned int src_cnt,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nstruct ioat_ring_ent *compl_desc;\r\nstruct ioat_ring_ent *desc;\r\nstruct ioat_ring_ent *ext;\r\nsize_t total_len = len;\r\nstruct ioat_xor_descriptor *xor;\r\nstruct ioat_xor_ext_descriptor *xor_ex = NULL;\r\nstruct ioat_dma_descriptor *hw;\r\nint num_descs, with_ext, idx, i;\r\nu32 offset = 0;\r\nu8 op = result ? IOAT_OP_XOR_VAL : IOAT_OP_XOR;\r\nBUG_ON(src_cnt < 2);\r\nnum_descs = ioat2_xferlen_to_descs(ioat, len);\r\nif (src_cnt > 5) {\r\nwith_ext = 1;\r\nnum_descs *= 2;\r\n} else\r\nwith_ext = 0;\r\nif (likely(num_descs) && ioat2_check_space_lock(ioat, num_descs+1) == 0)\r\nidx = ioat->head;\r\nelse\r\nreturn NULL;\r\ni = 0;\r\ndo {\r\nstruct ioat_raw_descriptor *descs[2];\r\nsize_t xfer_size = min_t(size_t, len, 1 << ioat->xfercap_log);\r\nint s;\r\ndesc = ioat2_get_ring_ent(ioat, idx + i);\r\nxor = desc->xor;\r\next = ioat2_get_ring_ent(ioat, idx + i + 1);\r\nxor_ex = ext->xor_ex;\r\ndescs[0] = (struct ioat_raw_descriptor *) xor;\r\ndescs[1] = (struct ioat_raw_descriptor *) xor_ex;\r\nfor (s = 0; s < src_cnt; s++)\r\nxor_set_src(descs, src[s], offset, s);\r\nxor->size = xfer_size;\r\nxor->dst_addr = dest + offset;\r\nxor->ctl = 0;\r\nxor->ctl_f.op = op;\r\nxor->ctl_f.src_cnt = src_cnt_to_hw(src_cnt);\r\nlen -= xfer_size;\r\noffset += xfer_size;\r\ndump_desc_dbg(ioat, desc);\r\n} while ((i += 1 + with_ext) < num_descs);\r\ndesc->txd.flags = flags;\r\ndesc->len = total_len;\r\nif (result)\r\ndesc->result = result;\r\nxor->ctl_f.fence = !!(flags & DMA_PREP_FENCE);\r\ncompl_desc = ioat2_get_ring_ent(ioat, idx + i);\r\ncompl_desc->txd.flags = flags & DMA_PREP_INTERRUPT;\r\nhw = compl_desc->hw;\r\nhw->ctl = 0;\r\nhw->ctl_f.null = 1;\r\nhw->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);\r\nhw->ctl_f.compl_write = 1;\r\nhw->size = NULL_DESC_BUFFER_SIZE;\r\ndump_desc_dbg(ioat, compl_desc);\r\nreturn &compl_desc->txd;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nioat3_prep_xor(struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,\r\nunsigned int src_cnt, size_t len, unsigned long flags)\r\n{\r\nreturn __ioat3_prep_xor_lock(chan, NULL, dest, src, src_cnt, len, flags);\r\n}\r\nstruct dma_async_tx_descriptor *\r\nioat3_prep_xor_val(struct dma_chan *chan, dma_addr_t *src,\r\nunsigned int src_cnt, size_t len,\r\nenum sum_check_flags *result, unsigned long flags)\r\n{\r\n*result = 0;\r\nreturn __ioat3_prep_xor_lock(chan, result, src[0], &src[1],\r\nsrc_cnt - 1, len, flags);\r\n}\r\nstatic void\r\ndump_pq_desc_dbg(struct ioat2_dma_chan *ioat, struct ioat_ring_ent *desc, struct ioat_ring_ent *ext)\r\n{\r\nstruct device *dev = to_dev(&ioat->base);\r\nstruct ioat_pq_descriptor *pq = desc->pq;\r\nstruct ioat_pq_ext_descriptor *pq_ex = ext ? ext->pq_ex : NULL;\r\nstruct ioat_raw_descriptor *descs[] = { (void *) pq, (void *) pq_ex };\r\nint src_cnt = src_cnt_to_sw(pq->ctl_f.src_cnt);\r\nint i;\r\ndev_dbg(dev, "desc[%d]: (%#llx->%#llx) flags: %#x"\r\n" sz: %#x ctl: %#x (op: %d int: %d compl: %d pq: '%s%s' src_cnt: %d)\n",\r\ndesc_id(desc), (unsigned long long) desc->txd.phys,\r\n(unsigned long long) (pq_ex ? pq_ex->next : pq->next),\r\ndesc->txd.flags, pq->size, pq->ctl, pq->ctl_f.op, pq->ctl_f.int_en,\r\npq->ctl_f.compl_write,\r\npq->ctl_f.p_disable ? "" : "p", pq->ctl_f.q_disable ? "" : "q",\r\npq->ctl_f.src_cnt);\r\nfor (i = 0; i < src_cnt; i++)\r\ndev_dbg(dev, "\tsrc[%d]: %#llx coef: %#x\n", i,\r\n(unsigned long long) pq_get_src(descs, i), pq->coef[i]);\r\ndev_dbg(dev, "\tP: %#llx\n", pq->p_addr);\r\ndev_dbg(dev, "\tQ: %#llx\n", pq->q_addr);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\n__ioat3_prep_pq_lock(struct dma_chan *c, enum sum_check_flags *result,\r\nconst dma_addr_t *dst, const dma_addr_t *src,\r\nunsigned int src_cnt, const unsigned char *scf,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nstruct ioat_chan_common *chan = &ioat->base;\r\nstruct ioat_ring_ent *compl_desc;\r\nstruct ioat_ring_ent *desc;\r\nstruct ioat_ring_ent *ext;\r\nsize_t total_len = len;\r\nstruct ioat_pq_descriptor *pq;\r\nstruct ioat_pq_ext_descriptor *pq_ex = NULL;\r\nstruct ioat_dma_descriptor *hw;\r\nu32 offset = 0;\r\nu8 op = result ? IOAT_OP_PQ_VAL : IOAT_OP_PQ;\r\nint i, s, idx, with_ext, num_descs;\r\ndev_dbg(to_dev(chan), "%s\n", __func__);\r\nBUG_ON(src_cnt + dmaf_continue(flags) < 2);\r\nnum_descs = ioat2_xferlen_to_descs(ioat, len);\r\nif (src_cnt + dmaf_p_disabled_continue(flags) > 3 ||\r\n(dmaf_continue(flags) && !dmaf_p_disabled_continue(flags))) {\r\nwith_ext = 1;\r\nnum_descs *= 2;\r\n} else\r\nwith_ext = 0;\r\nif (likely(num_descs) &&\r\nioat2_check_space_lock(ioat, num_descs+1) == 0)\r\nidx = ioat->head;\r\nelse\r\nreturn NULL;\r\ni = 0;\r\ndo {\r\nstruct ioat_raw_descriptor *descs[2];\r\nsize_t xfer_size = min_t(size_t, len, 1 << ioat->xfercap_log);\r\ndesc = ioat2_get_ring_ent(ioat, idx + i);\r\npq = desc->pq;\r\next = ioat2_get_ring_ent(ioat, idx + i + with_ext);\r\npq_ex = ext->pq_ex;\r\ndescs[0] = (struct ioat_raw_descriptor *) pq;\r\ndescs[1] = (struct ioat_raw_descriptor *) pq_ex;\r\nfor (s = 0; s < src_cnt; s++)\r\npq_set_src(descs, src[s], offset, scf[s], s);\r\nif (dmaf_p_disabled_continue(flags))\r\npq_set_src(descs, dst[1], offset, 1, s++);\r\nelse if (dmaf_continue(flags)) {\r\npq_set_src(descs, dst[0], offset, 0, s++);\r\npq_set_src(descs, dst[1], offset, 1, s++);\r\npq_set_src(descs, dst[1], offset, 0, s++);\r\n}\r\npq->size = xfer_size;\r\npq->p_addr = dst[0] + offset;\r\npq->q_addr = dst[1] + offset;\r\npq->ctl = 0;\r\npq->ctl_f.op = op;\r\npq->ctl_f.src_cnt = src_cnt_to_hw(s);\r\npq->ctl_f.p_disable = !!(flags & DMA_PREP_PQ_DISABLE_P);\r\npq->ctl_f.q_disable = !!(flags & DMA_PREP_PQ_DISABLE_Q);\r\nlen -= xfer_size;\r\noffset += xfer_size;\r\n} while ((i += 1 + with_ext) < num_descs);\r\ndesc->txd.flags = flags;\r\ndesc->len = total_len;\r\nif (result)\r\ndesc->result = result;\r\npq->ctl_f.fence = !!(flags & DMA_PREP_FENCE);\r\ndump_pq_desc_dbg(ioat, desc, ext);\r\ncompl_desc = ioat2_get_ring_ent(ioat, idx + i);\r\ncompl_desc->txd.flags = flags & DMA_PREP_INTERRUPT;\r\nhw = compl_desc->hw;\r\nhw->ctl = 0;\r\nhw->ctl_f.null = 1;\r\nhw->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);\r\nhw->ctl_f.compl_write = 1;\r\nhw->size = NULL_DESC_BUFFER_SIZE;\r\ndump_desc_dbg(ioat, compl_desc);\r\nreturn &compl_desc->txd;\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nioat3_prep_pq(struct dma_chan *chan, dma_addr_t *dst, dma_addr_t *src,\r\nunsigned int src_cnt, const unsigned char *scf, size_t len,\r\nunsigned long flags)\r\n{\r\nif (flags & DMA_PREP_PQ_DISABLE_P)\r\ndst[0] = dst[1];\r\nif (flags & DMA_PREP_PQ_DISABLE_Q)\r\ndst[1] = dst[0];\r\nif ((flags & DMA_PREP_PQ_DISABLE_P) && src_cnt == 1) {\r\ndma_addr_t single_source[2];\r\nunsigned char single_source_coef[2];\r\nBUG_ON(flags & DMA_PREP_PQ_DISABLE_Q);\r\nsingle_source[0] = src[0];\r\nsingle_source[1] = src[0];\r\nsingle_source_coef[0] = scf[0];\r\nsingle_source_coef[1] = 0;\r\nreturn __ioat3_prep_pq_lock(chan, NULL, dst, single_source, 2,\r\nsingle_source_coef, len, flags);\r\n} else\r\nreturn __ioat3_prep_pq_lock(chan, NULL, dst, src, src_cnt, scf,\r\nlen, flags);\r\n}\r\nstruct dma_async_tx_descriptor *\r\nioat3_prep_pq_val(struct dma_chan *chan, dma_addr_t *pq, dma_addr_t *src,\r\nunsigned int src_cnt, const unsigned char *scf, size_t len,\r\nenum sum_check_flags *pqres, unsigned long flags)\r\n{\r\nif (flags & DMA_PREP_PQ_DISABLE_P)\r\npq[0] = pq[1];\r\nif (flags & DMA_PREP_PQ_DISABLE_Q)\r\npq[1] = pq[0];\r\n*pqres = 0;\r\nreturn __ioat3_prep_pq_lock(chan, pqres, pq, src, src_cnt, scf, len,\r\nflags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nioat3_prep_pqxor(struct dma_chan *chan, dma_addr_t dst, dma_addr_t *src,\r\nunsigned int src_cnt, size_t len, unsigned long flags)\r\n{\r\nunsigned char scf[src_cnt];\r\ndma_addr_t pq[2];\r\nmemset(scf, 0, src_cnt);\r\npq[0] = dst;\r\nflags |= DMA_PREP_PQ_DISABLE_Q;\r\npq[1] = dst;\r\nreturn __ioat3_prep_pq_lock(chan, NULL, pq, src, src_cnt, scf, len,\r\nflags);\r\n}\r\nstruct dma_async_tx_descriptor *\r\nioat3_prep_pqxor_val(struct dma_chan *chan, dma_addr_t *src,\r\nunsigned int src_cnt, size_t len,\r\nenum sum_check_flags *result, unsigned long flags)\r\n{\r\nunsigned char scf[src_cnt];\r\ndma_addr_t pq[2];\r\n*result = 0;\r\nmemset(scf, 0, src_cnt);\r\npq[0] = src[0];\r\nflags |= DMA_PREP_PQ_DISABLE_Q;\r\npq[1] = pq[0];\r\nreturn __ioat3_prep_pq_lock(chan, result, pq, &src[1], src_cnt - 1, scf,\r\nlen, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *\r\nioat3_prep_interrupt_lock(struct dma_chan *c, unsigned long flags)\r\n{\r\nstruct ioat2_dma_chan *ioat = to_ioat2_chan(c);\r\nstruct ioat_ring_ent *desc;\r\nstruct ioat_dma_descriptor *hw;\r\nif (ioat2_check_space_lock(ioat, 1) == 0)\r\ndesc = ioat2_get_ring_ent(ioat, ioat->head);\r\nelse\r\nreturn NULL;\r\nhw = desc->hw;\r\nhw->ctl = 0;\r\nhw->ctl_f.null = 1;\r\nhw->ctl_f.int_en = 1;\r\nhw->ctl_f.fence = !!(flags & DMA_PREP_FENCE);\r\nhw->ctl_f.compl_write = 1;\r\nhw->size = NULL_DESC_BUFFER_SIZE;\r\nhw->src_addr = 0;\r\nhw->dst_addr = 0;\r\ndesc->txd.flags = flags;\r\ndesc->len = 1;\r\ndump_desc_dbg(ioat, desc);\r\nreturn &desc->txd;\r\n}\r\nstatic void __devinit ioat3_dma_test_callback(void *dma_async_param)\r\n{\r\nstruct completion *cmp = dma_async_param;\r\ncomplete(cmp);\r\n}\r\nstatic int __devinit ioat_xor_val_self_test(struct ioatdma_device *device)\r\n{\r\nint i, src_idx;\r\nstruct page *dest;\r\nstruct page *xor_srcs[IOAT_NUM_SRC_TEST];\r\nstruct page *xor_val_srcs[IOAT_NUM_SRC_TEST + 1];\r\ndma_addr_t dma_srcs[IOAT_NUM_SRC_TEST + 1];\r\ndma_addr_t dma_addr, dest_dma;\r\nstruct dma_async_tx_descriptor *tx;\r\nstruct dma_chan *dma_chan;\r\ndma_cookie_t cookie;\r\nu8 cmp_byte = 0;\r\nu32 cmp_word;\r\nu32 xor_val_result;\r\nint err = 0;\r\nstruct completion cmp;\r\nunsigned long tmo;\r\nstruct device *dev = &device->pdev->dev;\r\nstruct dma_device *dma = &device->common;\r\ndev_dbg(dev, "%s\n", __func__);\r\nif (!dma_has_cap(DMA_XOR, dma->cap_mask))\r\nreturn 0;\r\nfor (src_idx = 0; src_idx < IOAT_NUM_SRC_TEST; src_idx++) {\r\nxor_srcs[src_idx] = alloc_page(GFP_KERNEL);\r\nif (!xor_srcs[src_idx]) {\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\nreturn -ENOMEM;\r\n}\r\n}\r\ndest = alloc_page(GFP_KERNEL);\r\nif (!dest) {\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\nreturn -ENOMEM;\r\n}\r\nfor (src_idx = 0; src_idx < IOAT_NUM_SRC_TEST; src_idx++) {\r\nu8 *ptr = page_address(xor_srcs[src_idx]);\r\nfor (i = 0; i < PAGE_SIZE; i++)\r\nptr[i] = (1 << src_idx);\r\n}\r\nfor (src_idx = 0; src_idx < IOAT_NUM_SRC_TEST; src_idx++)\r\ncmp_byte ^= (u8) (1 << src_idx);\r\ncmp_word = (cmp_byte << 24) | (cmp_byte << 16) |\r\n(cmp_byte << 8) | cmp_byte;\r\nmemset(page_address(dest), 0, PAGE_SIZE);\r\ndma_chan = container_of(dma->channels.next, struct dma_chan,\r\ndevice_node);\r\nif (dma->device_alloc_chan_resources(dma_chan) < 1) {\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\ndest_dma = dma_map_page(dev, dest, 0, PAGE_SIZE, DMA_FROM_DEVICE);\r\nfor (i = 0; i < IOAT_NUM_SRC_TEST; i++)\r\ndma_srcs[i] = dma_map_page(dev, xor_srcs[i], 0, PAGE_SIZE,\r\nDMA_TO_DEVICE);\r\ntx = dma->device_prep_dma_xor(dma_chan, dest_dma, dma_srcs,\r\nIOAT_NUM_SRC_TEST, PAGE_SIZE,\r\nDMA_PREP_INTERRUPT);\r\nif (!tx) {\r\ndev_err(dev, "Self-test xor prep failed\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nasync_tx_ack(tx);\r\ninit_completion(&cmp);\r\ntx->callback = ioat3_dma_test_callback;\r\ntx->callback_param = &cmp;\r\ncookie = tx->tx_submit(tx);\r\nif (cookie < 0) {\r\ndev_err(dev, "Self-test xor setup failed\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ndma->device_issue_pending(dma_chan);\r\ntmo = wait_for_completion_timeout(&cmp, msecs_to_jiffies(3000));\r\nif (dma->device_tx_status(dma_chan, cookie, NULL) != DMA_SUCCESS) {\r\ndev_err(dev, "Self-test xor timed out\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ndma_sync_single_for_cpu(dev, dest_dma, PAGE_SIZE, DMA_FROM_DEVICE);\r\nfor (i = 0; i < (PAGE_SIZE / sizeof(u32)); i++) {\r\nu32 *ptr = page_address(dest);\r\nif (ptr[i] != cmp_word) {\r\ndev_err(dev, "Self-test xor failed compare\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\n}\r\ndma_sync_single_for_device(dev, dest_dma, PAGE_SIZE, DMA_TO_DEVICE);\r\nif (!dma_has_cap(DMA_XOR_VAL, dma_chan->device->cap_mask))\r\ngoto free_resources;\r\nfor (i = 0; i < IOAT_NUM_SRC_TEST; i++)\r\nxor_val_srcs[i] = xor_srcs[i];\r\nxor_val_srcs[i] = dest;\r\nxor_val_result = 1;\r\nfor (i = 0; i < IOAT_NUM_SRC_TEST + 1; i++)\r\ndma_srcs[i] = dma_map_page(dev, xor_val_srcs[i], 0, PAGE_SIZE,\r\nDMA_TO_DEVICE);\r\ntx = dma->device_prep_dma_xor_val(dma_chan, dma_srcs,\r\nIOAT_NUM_SRC_TEST + 1, PAGE_SIZE,\r\n&xor_val_result, DMA_PREP_INTERRUPT);\r\nif (!tx) {\r\ndev_err(dev, "Self-test zero prep failed\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nasync_tx_ack(tx);\r\ninit_completion(&cmp);\r\ntx->callback = ioat3_dma_test_callback;\r\ntx->callback_param = &cmp;\r\ncookie = tx->tx_submit(tx);\r\nif (cookie < 0) {\r\ndev_err(dev, "Self-test zero setup failed\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ndma->device_issue_pending(dma_chan);\r\ntmo = wait_for_completion_timeout(&cmp, msecs_to_jiffies(3000));\r\nif (dma->device_tx_status(dma_chan, cookie, NULL) != DMA_SUCCESS) {\r\ndev_err(dev, "Self-test validate timed out\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nif (xor_val_result != 0) {\r\ndev_err(dev, "Self-test validate failed compare\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nif (!dma_has_cap(DMA_MEMSET, dma_chan->device->cap_mask))\r\ngoto free_resources;\r\ndma_addr = dma_map_page(dev, dest, 0,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\ntx = dma->device_prep_dma_memset(dma_chan, dma_addr, 0, PAGE_SIZE,\r\nDMA_PREP_INTERRUPT);\r\nif (!tx) {\r\ndev_err(dev, "Self-test memset prep failed\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nasync_tx_ack(tx);\r\ninit_completion(&cmp);\r\ntx->callback = ioat3_dma_test_callback;\r\ntx->callback_param = &cmp;\r\ncookie = tx->tx_submit(tx);\r\nif (cookie < 0) {\r\ndev_err(dev, "Self-test memset setup failed\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ndma->device_issue_pending(dma_chan);\r\ntmo = wait_for_completion_timeout(&cmp, msecs_to_jiffies(3000));\r\nif (dma->device_tx_status(dma_chan, cookie, NULL) != DMA_SUCCESS) {\r\ndev_err(dev, "Self-test memset timed out\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nfor (i = 0; i < PAGE_SIZE/sizeof(u32); i++) {\r\nu32 *ptr = page_address(dest);\r\nif (ptr[i]) {\r\ndev_err(dev, "Self-test memset failed compare\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\n}\r\nxor_val_result = 0;\r\nfor (i = 0; i < IOAT_NUM_SRC_TEST + 1; i++)\r\ndma_srcs[i] = dma_map_page(dev, xor_val_srcs[i], 0, PAGE_SIZE,\r\nDMA_TO_DEVICE);\r\ntx = dma->device_prep_dma_xor_val(dma_chan, dma_srcs,\r\nIOAT_NUM_SRC_TEST + 1, PAGE_SIZE,\r\n&xor_val_result, DMA_PREP_INTERRUPT);\r\nif (!tx) {\r\ndev_err(dev, "Self-test 2nd zero prep failed\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nasync_tx_ack(tx);\r\ninit_completion(&cmp);\r\ntx->callback = ioat3_dma_test_callback;\r\ntx->callback_param = &cmp;\r\ncookie = tx->tx_submit(tx);\r\nif (cookie < 0) {\r\ndev_err(dev, "Self-test 2nd zero setup failed\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\ndma->device_issue_pending(dma_chan);\r\ntmo = wait_for_completion_timeout(&cmp, msecs_to_jiffies(3000));\r\nif (dma->device_tx_status(dma_chan, cookie, NULL) != DMA_SUCCESS) {\r\ndev_err(dev, "Self-test 2nd validate timed out\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nif (xor_val_result != SUM_CHECK_P_RESULT) {\r\ndev_err(dev, "Self-test validate failed compare\n");\r\nerr = -ENODEV;\r\ngoto free_resources;\r\n}\r\nfree_resources:\r\ndma->device_free_chan_resources(dma_chan);\r\nout:\r\nsrc_idx = IOAT_NUM_SRC_TEST;\r\nwhile (src_idx--)\r\n__free_page(xor_srcs[src_idx]);\r\n__free_page(dest);\r\nreturn err;\r\n}\r\nstatic int __devinit ioat3_dma_self_test(struct ioatdma_device *device)\r\n{\r\nint rc = ioat_dma_self_test(device);\r\nif (rc)\r\nreturn rc;\r\nrc = ioat_xor_val_self_test(device);\r\nif (rc)\r\nreturn rc;\r\nreturn 0;\r\n}\r\nstatic int ioat3_reset_hw(struct ioat_chan_common *chan)\r\n{\r\nstruct ioatdma_device *device = chan->device;\r\nstruct pci_dev *pdev = device->pdev;\r\nu32 chanerr;\r\nu16 dev_id;\r\nint err;\r\nioat2_quiesce(chan, msecs_to_jiffies(100));\r\nchanerr = readl(chan->reg_base + IOAT_CHANERR_OFFSET);\r\nwritel(chanerr, chan->reg_base + IOAT_CHANERR_OFFSET);\r\npci_write_config_dword(pdev, IOAT_PCI_CHANERRMASK_INT_OFFSET, 0x3e07);\r\nerr = pci_read_config_dword(pdev, IOAT_PCI_CHANERR_INT_OFFSET, &chanerr);\r\nif (err) {\r\ndev_err(&pdev->dev, "channel error register unreachable\n");\r\nreturn err;\r\n}\r\npci_write_config_dword(pdev, IOAT_PCI_CHANERR_INT_OFFSET, chanerr);\r\npci_read_config_word(pdev, IOAT_PCI_DEVICE_ID_OFFSET, &dev_id);\r\nif (dev_id == PCI_DEVICE_ID_INTEL_IOAT_TBG0)\r\npci_write_config_dword(pdev, IOAT_PCI_DMAUNCERRSTS_OFFSET, 0x10);\r\nreturn ioat2_reset_sync(chan, msecs_to_jiffies(200));\r\n}\r\nint __devinit ioat3_dma_probe(struct ioatdma_device *device, int dca)\r\n{\r\nstruct pci_dev *pdev = device->pdev;\r\nint dca_en = system_has_dca_enabled(pdev);\r\nstruct dma_device *dma;\r\nstruct dma_chan *c;\r\nstruct ioat_chan_common *chan;\r\nbool is_raid_device = false;\r\nint err;\r\nu32 cap;\r\ndevice->enumerate_channels = ioat2_enumerate_channels;\r\ndevice->reset_hw = ioat3_reset_hw;\r\ndevice->self_test = ioat3_dma_self_test;\r\ndma = &device->common;\r\ndma->device_prep_dma_memcpy = ioat2_dma_prep_memcpy_lock;\r\ndma->device_issue_pending = ioat2_issue_pending;\r\ndma->device_alloc_chan_resources = ioat2_alloc_chan_resources;\r\ndma->device_free_chan_resources = ioat2_free_chan_resources;\r\ndma_cap_set(DMA_INTERRUPT, dma->cap_mask);\r\ndma->device_prep_dma_interrupt = ioat3_prep_interrupt_lock;\r\ncap = readl(device->reg_base + IOAT_DMA_CAP_OFFSET);\r\nif (dca_en && (cap & (IOAT_CAP_XOR|IOAT_CAP_PQ)))\r\ncap &= ~(IOAT_CAP_XOR|IOAT_CAP_PQ);\r\nif (cap & IOAT_CAP_XOR) {\r\nis_raid_device = true;\r\ndma->max_xor = 8;\r\ndma->xor_align = 6;\r\ndma_cap_set(DMA_XOR, dma->cap_mask);\r\ndma->device_prep_dma_xor = ioat3_prep_xor;\r\ndma_cap_set(DMA_XOR_VAL, dma->cap_mask);\r\ndma->device_prep_dma_xor_val = ioat3_prep_xor_val;\r\n}\r\nif (cap & IOAT_CAP_PQ) {\r\nis_raid_device = true;\r\ndma_set_maxpq(dma, 8, 0);\r\ndma->pq_align = 6;\r\ndma_cap_set(DMA_PQ, dma->cap_mask);\r\ndma->device_prep_dma_pq = ioat3_prep_pq;\r\ndma_cap_set(DMA_PQ_VAL, dma->cap_mask);\r\ndma->device_prep_dma_pq_val = ioat3_prep_pq_val;\r\nif (!(cap & IOAT_CAP_XOR)) {\r\ndma->max_xor = 8;\r\ndma->xor_align = 6;\r\ndma_cap_set(DMA_XOR, dma->cap_mask);\r\ndma->device_prep_dma_xor = ioat3_prep_pqxor;\r\ndma_cap_set(DMA_XOR_VAL, dma->cap_mask);\r\ndma->device_prep_dma_xor_val = ioat3_prep_pqxor_val;\r\n}\r\n}\r\nif (is_raid_device && (cap & IOAT_CAP_FILL_BLOCK)) {\r\ndma_cap_set(DMA_MEMSET, dma->cap_mask);\r\ndma->device_prep_dma_memset = ioat3_prep_memset_lock;\r\n}\r\nif (is_raid_device) {\r\ndma->device_tx_status = ioat3_tx_status;\r\ndevice->cleanup_fn = ioat3_cleanup_event;\r\ndevice->timer_fn = ioat3_timer_event;\r\n} else {\r\ndma->device_tx_status = ioat_dma_tx_status;\r\ndevice->cleanup_fn = ioat2_cleanup_event;\r\ndevice->timer_fn = ioat2_timer_event;\r\n}\r\n#ifdef CONFIG_ASYNC_TX_DISABLE_PQ_VAL_DMA\r\ndma_cap_clear(DMA_PQ_VAL, dma->cap_mask);\r\ndma->device_prep_dma_pq_val = NULL;\r\n#endif\r\n#ifdef CONFIG_ASYNC_TX_DISABLE_XOR_VAL_DMA\r\ndma_cap_clear(DMA_XOR_VAL, dma->cap_mask);\r\ndma->device_prep_dma_xor_val = NULL;\r\n#endif\r\nerr = ioat_probe(device);\r\nif (err)\r\nreturn err;\r\nioat_set_tcp_copy_break(262144);\r\nlist_for_each_entry(c, &dma->channels, device_node) {\r\nchan = to_chan_common(c);\r\nwritel(IOAT_DMA_DCA_ANY_CPU,\r\nchan->reg_base + IOAT_DCACTRL_OFFSET);\r\n}\r\nerr = ioat_register(device);\r\nif (err)\r\nreturn err;\r\nioat_kobject_add(device, &ioat2_ktype);\r\nif (dca)\r\ndevice->dca = ioat3_dca_init(pdev, device->reg_base);\r\nreturn 0;\r\n}
