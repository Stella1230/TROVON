static unsigned long htab_convert_pte_flags(unsigned long pteflags)\r\n{\r\nunsigned long rflags = pteflags & 0x1fa;\r\nif ((pteflags & _PAGE_EXEC) == 0)\r\nrflags |= HPTE_R_N;\r\nif ((pteflags & _PAGE_USER) && !((pteflags & _PAGE_RW) &&\r\n(pteflags & _PAGE_DIRTY)))\r\nrflags |= 1;\r\nreturn rflags | HPTE_R_C;\r\n}\r\nint htab_bolt_mapping(unsigned long vstart, unsigned long vend,\r\nunsigned long pstart, unsigned long prot,\r\nint psize, int ssize)\r\n{\r\nunsigned long vaddr, paddr;\r\nunsigned int step, shift;\r\nint ret = 0;\r\nshift = mmu_psize_defs[psize].shift;\r\nstep = 1 << shift;\r\nprot = htab_convert_pte_flags(prot);\r\nDBG("htab_bolt_mapping(%lx..%lx -> %lx (%lx,%d,%d)\n",\r\nvstart, vend, pstart, prot, psize, ssize);\r\nfor (vaddr = vstart, paddr = pstart; vaddr < vend;\r\nvaddr += step, paddr += step) {\r\nunsigned long hash, hpteg;\r\nunsigned long vsid = get_kernel_vsid(vaddr, ssize);\r\nunsigned long va = hpt_va(vaddr, vsid, ssize);\r\nunsigned long tprot = prot;\r\nif (overlaps_kernel_text(vaddr, vaddr + step))\r\ntprot &= ~HPTE_R_N;\r\nhash = hpt_hash(va, shift, ssize);\r\nhpteg = ((hash & htab_hash_mask) * HPTES_PER_GROUP);\r\nBUG_ON(!ppc_md.hpte_insert);\r\nret = ppc_md.hpte_insert(hpteg, va, paddr, tprot,\r\nHPTE_V_BOLTED, psize, ssize);\r\nif (ret < 0)\r\nbreak;\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nif ((paddr >> PAGE_SHIFT) < linear_map_hash_count)\r\nlinear_map_hash_slots[paddr >> PAGE_SHIFT] = ret | 0x80;\r\n#endif\r\n}\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nstatic int htab_remove_mapping(unsigned long vstart, unsigned long vend,\r\nint psize, int ssize)\r\n{\r\nunsigned long vaddr;\r\nunsigned int step, shift;\r\nshift = mmu_psize_defs[psize].shift;\r\nstep = 1 << shift;\r\nif (!ppc_md.hpte_removebolted) {\r\nprintk(KERN_WARNING "Platform doesn't implement "\r\n"hpte_removebolted\n");\r\nreturn -EINVAL;\r\n}\r\nfor (vaddr = vstart; vaddr < vend; vaddr += step)\r\nppc_md.hpte_removebolted(vaddr, psize, ssize);\r\nreturn 0;\r\n}\r\nstatic int __init htab_dt_scan_seg_sizes(unsigned long node,\r\nconst char *uname, int depth,\r\nvoid *data)\r\n{\r\nchar *type = of_get_flat_dt_prop(node, "device_type", NULL);\r\nu32 *prop;\r\nunsigned long size = 0;\r\nif (type == NULL || strcmp(type, "cpu") != 0)\r\nreturn 0;\r\nprop = (u32 *)of_get_flat_dt_prop(node, "ibm,processor-segment-sizes",\r\n&size);\r\nif (prop == NULL)\r\nreturn 0;\r\nfor (; size >= 4; size -= 4, ++prop) {\r\nif (prop[0] == 40) {\r\nDBG("1T segment support detected\n");\r\ncur_cpu_spec->mmu_features |= MMU_FTR_1T_SEGMENT;\r\nreturn 1;\r\n}\r\n}\r\ncur_cpu_spec->mmu_features &= ~MMU_FTR_NO_SLBIE_B;\r\nreturn 0;\r\n}\r\nstatic void __init htab_init_seg_sizes(void)\r\n{\r\nof_scan_flat_dt(htab_dt_scan_seg_sizes, NULL);\r\n}\r\nstatic int __init htab_dt_scan_page_sizes(unsigned long node,\r\nconst char *uname, int depth,\r\nvoid *data)\r\n{\r\nchar *type = of_get_flat_dt_prop(node, "device_type", NULL);\r\nu32 *prop;\r\nunsigned long size = 0;\r\nif (type == NULL || strcmp(type, "cpu") != 0)\r\nreturn 0;\r\nprop = (u32 *)of_get_flat_dt_prop(node,\r\n"ibm,segment-page-sizes", &size);\r\nif (prop != NULL) {\r\nDBG("Page sizes from device-tree:\n");\r\nsize /= 4;\r\ncur_cpu_spec->mmu_features &= ~(MMU_FTR_16M_PAGE);\r\nwhile(size > 0) {\r\nunsigned int shift = prop[0];\r\nunsigned int slbenc = prop[1];\r\nunsigned int lpnum = prop[2];\r\nunsigned int lpenc = 0;\r\nstruct mmu_psize_def *def;\r\nint idx = -1;\r\nsize -= 3; prop += 3;\r\nwhile(size > 0 && lpnum) {\r\nif (prop[0] == shift)\r\nlpenc = prop[1];\r\nprop += 2; size -= 2;\r\nlpnum--;\r\n}\r\nswitch(shift) {\r\ncase 0xc:\r\nidx = MMU_PAGE_4K;\r\nbreak;\r\ncase 0x10:\r\nidx = MMU_PAGE_64K;\r\nbreak;\r\ncase 0x14:\r\nidx = MMU_PAGE_1M;\r\nbreak;\r\ncase 0x18:\r\nidx = MMU_PAGE_16M;\r\ncur_cpu_spec->mmu_features |= MMU_FTR_16M_PAGE;\r\nbreak;\r\ncase 0x22:\r\nidx = MMU_PAGE_16G;\r\nbreak;\r\n}\r\nif (idx < 0)\r\ncontinue;\r\ndef = &mmu_psize_defs[idx];\r\ndef->shift = shift;\r\nif (shift <= 23)\r\ndef->avpnm = 0;\r\nelse\r\ndef->avpnm = (1 << (shift - 23)) - 1;\r\ndef->sllp = slbenc;\r\ndef->penc = lpenc;\r\nif (idx == MMU_PAGE_4K || idx == MMU_PAGE_64K)\r\ndef->tlbiel = 1;\r\nelse\r\ndef->tlbiel = 0;\r\nDBG(" %d: shift=%02x, sllp=%04lx, avpnm=%08lx, "\r\n"tlbiel=%d, penc=%d\n",\r\nidx, shift, def->sllp, def->avpnm, def->tlbiel,\r\ndef->penc);\r\n}\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init htab_dt_scan_hugepage_blocks(unsigned long node,\r\nconst char *uname, int depth,\r\nvoid *data) {\r\nchar *type = of_get_flat_dt_prop(node, "device_type", NULL);\r\nunsigned long *addr_prop;\r\nu32 *page_count_prop;\r\nunsigned int expected_pages;\r\nlong unsigned int phys_addr;\r\nlong unsigned int block_size;\r\nif (type == NULL || strcmp(type, "memory") != 0)\r\nreturn 0;\r\npage_count_prop = of_get_flat_dt_prop(node, "ibm,expected#pages", NULL);\r\nif (page_count_prop == NULL)\r\nreturn 0;\r\nexpected_pages = (1 << page_count_prop[0]);\r\naddr_prop = of_get_flat_dt_prop(node, "reg", NULL);\r\nif (addr_prop == NULL)\r\nreturn 0;\r\nphys_addr = addr_prop[0];\r\nblock_size = addr_prop[1];\r\nif (block_size != (16 * GB))\r\nreturn 0;\r\nprintk(KERN_INFO "Huge page(16GB) memory: "\r\n"addr = 0x%lX size = 0x%lX pages = %d\n",\r\nphys_addr, block_size, expected_pages);\r\nif (phys_addr + (16 * GB) <= memblock_end_of_DRAM()) {\r\nmemblock_reserve(phys_addr, block_size * expected_pages);\r\nadd_gpage(phys_addr, block_size, expected_pages);\r\n}\r\nreturn 0;\r\n}\r\nstatic void __init htab_init_page_sizes(void)\r\n{\r\nint rc;\r\nmemcpy(mmu_psize_defs, mmu_psize_defaults_old,\r\nsizeof(mmu_psize_defaults_old));\r\nrc = of_scan_flat_dt(htab_dt_scan_page_sizes, NULL);\r\nif (rc != 0)\r\ngoto found;\r\nif (mmu_has_feature(MMU_FTR_16M_PAGE))\r\nmemcpy(mmu_psize_defs, mmu_psize_defaults_gp,\r\nsizeof(mmu_psize_defaults_gp));\r\nfound:\r\n#ifndef CONFIG_DEBUG_PAGEALLOC\r\nif (mmu_psize_defs[MMU_PAGE_16M].shift)\r\nmmu_linear_psize = MMU_PAGE_16M;\r\nelse if (mmu_psize_defs[MMU_PAGE_1M].shift)\r\nmmu_linear_psize = MMU_PAGE_1M;\r\n#endif\r\n#ifdef CONFIG_PPC_64K_PAGES\r\nif (mmu_psize_defs[MMU_PAGE_64K].shift) {\r\nmmu_virtual_psize = MMU_PAGE_64K;\r\nmmu_vmalloc_psize = MMU_PAGE_64K;\r\nif (mmu_linear_psize == MMU_PAGE_4K)\r\nmmu_linear_psize = MMU_PAGE_64K;\r\nif (mmu_has_feature(MMU_FTR_CI_LARGE_PAGE)) {\r\nif (!machine_is(pseries))\r\nmmu_io_psize = MMU_PAGE_64K;\r\n} else\r\nmmu_ci_restrictions = 1;\r\n}\r\n#endif\r\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\r\nif (mmu_psize_defs[MMU_PAGE_16M].shift &&\r\nmemblock_phys_mem_size() >= 0x40000000)\r\nmmu_vmemmap_psize = MMU_PAGE_16M;\r\nelse if (mmu_psize_defs[MMU_PAGE_64K].shift)\r\nmmu_vmemmap_psize = MMU_PAGE_64K;\r\nelse\r\nmmu_vmemmap_psize = MMU_PAGE_4K;\r\n#endif\r\nprintk(KERN_DEBUG "Page orders: linear mapping = %d, "\r\n"virtual = %d, io = %d"\r\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\r\n", vmemmap = %d"\r\n#endif\r\n"\n",\r\nmmu_psize_defs[mmu_linear_psize].shift,\r\nmmu_psize_defs[mmu_virtual_psize].shift,\r\nmmu_psize_defs[mmu_io_psize].shift\r\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\r\n,mmu_psize_defs[mmu_vmemmap_psize].shift\r\n#endif\r\n);\r\n#ifdef CONFIG_HUGETLB_PAGE\r\nof_scan_flat_dt(htab_dt_scan_hugepage_blocks, NULL);\r\n#endif\r\n}\r\nstatic int __init htab_dt_scan_pftsize(unsigned long node,\r\nconst char *uname, int depth,\r\nvoid *data)\r\n{\r\nchar *type = of_get_flat_dt_prop(node, "device_type", NULL);\r\nu32 *prop;\r\nif (type == NULL || strcmp(type, "cpu") != 0)\r\nreturn 0;\r\nprop = (u32 *)of_get_flat_dt_prop(node, "ibm,pft-size", NULL);\r\nif (prop != NULL) {\r\nppc64_pft_size = prop[1];\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned long __init htab_get_table_size(void)\r\n{\r\nunsigned long mem_size, rnd_mem_size, pteg_count, psize;\r\nif (ppc64_pft_size == 0)\r\nof_scan_flat_dt(htab_dt_scan_pftsize, NULL);\r\nif (ppc64_pft_size)\r\nreturn 1UL << ppc64_pft_size;\r\nmem_size = memblock_phys_mem_size();\r\nrnd_mem_size = 1UL << __ilog2(mem_size);\r\nif (rnd_mem_size < mem_size)\r\nrnd_mem_size <<= 1;\r\npsize = mmu_psize_defs[mmu_virtual_psize].shift;\r\npteg_count = max(rnd_mem_size >> (psize + 1), 1UL << 11);\r\nreturn pteg_count << 7;\r\n}\r\nint create_section_mapping(unsigned long start, unsigned long end)\r\n{\r\nreturn htab_bolt_mapping(start, end, __pa(start),\r\npgprot_val(PAGE_KERNEL), mmu_linear_psize,\r\nmmu_kernel_ssize);\r\n}\r\nint remove_section_mapping(unsigned long start, unsigned long end)\r\n{\r\nreturn htab_remove_mapping(start, end, mmu_linear_psize,\r\nmmu_kernel_ssize);\r\n}\r\nstatic void __init htab_finish_init(void)\r\n{\r\nextern unsigned int *htab_call_hpte_insert1;\r\nextern unsigned int *htab_call_hpte_insert2;\r\nextern unsigned int *htab_call_hpte_remove;\r\nextern unsigned int *htab_call_hpte_updatepp;\r\n#ifdef CONFIG_PPC_HAS_HASH_64K\r\nextern unsigned int *ht64_call_hpte_insert1;\r\nextern unsigned int *ht64_call_hpte_insert2;\r\nextern unsigned int *ht64_call_hpte_remove;\r\nextern unsigned int *ht64_call_hpte_updatepp;\r\npatch_branch(ht64_call_hpte_insert1,\r\nFUNCTION_TEXT(ppc_md.hpte_insert),\r\nBRANCH_SET_LINK);\r\npatch_branch(ht64_call_hpte_insert2,\r\nFUNCTION_TEXT(ppc_md.hpte_insert),\r\nBRANCH_SET_LINK);\r\npatch_branch(ht64_call_hpte_remove,\r\nFUNCTION_TEXT(ppc_md.hpte_remove),\r\nBRANCH_SET_LINK);\r\npatch_branch(ht64_call_hpte_updatepp,\r\nFUNCTION_TEXT(ppc_md.hpte_updatepp),\r\nBRANCH_SET_LINK);\r\n#endif\r\npatch_branch(htab_call_hpte_insert1,\r\nFUNCTION_TEXT(ppc_md.hpte_insert),\r\nBRANCH_SET_LINK);\r\npatch_branch(htab_call_hpte_insert2,\r\nFUNCTION_TEXT(ppc_md.hpte_insert),\r\nBRANCH_SET_LINK);\r\npatch_branch(htab_call_hpte_remove,\r\nFUNCTION_TEXT(ppc_md.hpte_remove),\r\nBRANCH_SET_LINK);\r\npatch_branch(htab_call_hpte_updatepp,\r\nFUNCTION_TEXT(ppc_md.hpte_updatepp),\r\nBRANCH_SET_LINK);\r\n}\r\nstatic void __init htab_initialize(void)\r\n{\r\nunsigned long table;\r\nunsigned long pteg_count;\r\nunsigned long prot;\r\nunsigned long base = 0, size = 0, limit;\r\nstruct memblock_region *reg;\r\nDBG(" -> htab_initialize()\n");\r\nhtab_init_seg_sizes();\r\nhtab_init_page_sizes();\r\nif (mmu_has_feature(MMU_FTR_1T_SEGMENT)) {\r\nmmu_kernel_ssize = MMU_SEGSIZE_1T;\r\nmmu_highuser_ssize = MMU_SEGSIZE_1T;\r\nprintk(KERN_INFO "Using 1TB segments\n");\r\n}\r\nhtab_size_bytes = htab_get_table_size();\r\npteg_count = htab_size_bytes >> 7;\r\nhtab_hash_mask = pteg_count - 1;\r\nif (firmware_has_feature(FW_FEATURE_LPAR)) {\r\nhtab_address = NULL;\r\n_SDR1 = 0;\r\n} else {\r\nif (machine_is(cell))\r\nlimit = 0x80000000;\r\nelse\r\nlimit = MEMBLOCK_ALLOC_ANYWHERE;\r\ntable = memblock_alloc_base(htab_size_bytes, htab_size_bytes, limit);\r\nDBG("Hash table allocated at %lx, size: %lx\n", table,\r\nhtab_size_bytes);\r\nhtab_address = abs_to_virt(table);\r\n_SDR1 = table + __ilog2(pteg_count) - 11;\r\nmemset((void *)table, 0, htab_size_bytes);\r\nmtspr(SPRN_SDR1, _SDR1);\r\n}\r\nprot = pgprot_val(PAGE_KERNEL);\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nlinear_map_hash_count = memblock_end_of_DRAM() >> PAGE_SHIFT;\r\nlinear_map_hash_slots = __va(memblock_alloc_base(linear_map_hash_count,\r\n1, ppc64_rma_size));\r\nmemset(linear_map_hash_slots, 0, linear_map_hash_count);\r\n#endif\r\nfor_each_memblock(memory, reg) {\r\nbase = (unsigned long)__va(reg->base);\r\nsize = reg->size;\r\nDBG("creating mapping for region: %lx..%lx (prot: %lx)\n",\r\nbase, size, prot);\r\n#ifdef CONFIG_U3_DART\r\nDBG("DART base: %lx\n", dart_tablebase);\r\nif (dart_tablebase != 0 && dart_tablebase >= base\r\n&& dart_tablebase < (base + size)) {\r\nunsigned long dart_table_end = dart_tablebase + 16 * MB;\r\nif (base != dart_tablebase)\r\nBUG_ON(htab_bolt_mapping(base, dart_tablebase,\r\n__pa(base), prot,\r\nmmu_linear_psize,\r\nmmu_kernel_ssize));\r\nif ((base + size) > dart_table_end)\r\nBUG_ON(htab_bolt_mapping(dart_tablebase+16*MB,\r\nbase + size,\r\n__pa(dart_table_end),\r\nprot,\r\nmmu_linear_psize,\r\nmmu_kernel_ssize));\r\ncontinue;\r\n}\r\n#endif\r\nBUG_ON(htab_bolt_mapping(base, base + size, __pa(base),\r\nprot, mmu_linear_psize, mmu_kernel_ssize));\r\n}\r\nmemblock_set_current_limit(MEMBLOCK_ALLOC_ANYWHERE);\r\nif (tce_alloc_start) {\r\ntce_alloc_start = (unsigned long)__va(tce_alloc_start);\r\ntce_alloc_end = (unsigned long)__va(tce_alloc_end);\r\nif (base + size >= tce_alloc_start)\r\ntce_alloc_start = base + size + 1;\r\nBUG_ON(htab_bolt_mapping(tce_alloc_start, tce_alloc_end,\r\n__pa(tce_alloc_start), prot,\r\nmmu_linear_psize, mmu_kernel_ssize));\r\n}\r\nhtab_finish_init();\r\nDBG(" <- htab_initialize()\n");\r\n}\r\nvoid __init early_init_mmu(void)\r\n{\r\nget_paca()->stab_real = __pa((u64)&initial_stab);\r\nget_paca()->stab_addr = (u64)&initial_stab;\r\nhtab_initialize();\r\nif (mmu_has_feature(MMU_FTR_SLB))\r\nslb_initialize();\r\nelse if (!firmware_has_feature(FW_FEATURE_ISERIES))\r\nstab_initialize(get_paca()->stab_real);\r\n}\r\nvoid __cpuinit early_init_mmu_secondary(void)\r\n{\r\nif (!firmware_has_feature(FW_FEATURE_LPAR))\r\nmtspr(SPRN_SDR1, _SDR1);\r\nif (mmu_has_feature(MMU_FTR_SLB))\r\nslb_initialize();\r\nelse\r\nstab_initialize(get_paca()->stab_addr);\r\n}\r\nunsigned int hash_page_do_lazy_icache(unsigned int pp, pte_t pte, int trap)\r\n{\r\nstruct page *page;\r\nif (!pfn_valid(pte_pfn(pte)))\r\nreturn pp;\r\npage = pte_page(pte);\r\nif (!test_bit(PG_arch_1, &page->flags) && !PageReserved(page)) {\r\nif (trap == 0x400) {\r\nflush_dcache_icache_page(page);\r\nset_bit(PG_arch_1, &page->flags);\r\n} else\r\npp |= HPTE_R_N;\r\n}\r\nreturn pp;\r\n}\r\nunsigned int get_paca_psize(unsigned long addr)\r\n{\r\nunsigned long index, slices;\r\nif (addr < SLICE_LOW_TOP) {\r\nslices = get_paca()->context.low_slices_psize;\r\nindex = GET_LOW_SLICE_INDEX(addr);\r\n} else {\r\nslices = get_paca()->context.high_slices_psize;\r\nindex = GET_HIGH_SLICE_INDEX(addr);\r\n}\r\nreturn (slices >> (index * 4)) & 0xF;\r\n}\r\nunsigned int get_paca_psize(unsigned long addr)\r\n{\r\nreturn get_paca()->context.user_psize;\r\n}\r\nvoid demote_segment_4k(struct mm_struct *mm, unsigned long addr)\r\n{\r\nif (get_slice_psize(mm, addr) == MMU_PAGE_4K)\r\nreturn;\r\nslice_set_range_psize(mm, addr, 1, MMU_PAGE_4K);\r\n#ifdef CONFIG_SPU_BASE\r\nspu_flush_all_slbs(mm);\r\n#endif\r\nif (get_paca_psize(addr) != MMU_PAGE_4K) {\r\nget_paca()->context = mm->context;\r\nslb_flush_and_rebolt();\r\n}\r\n}\r\nstatic int subpage_protection(struct mm_struct *mm, unsigned long ea)\r\n{\r\nstruct subpage_prot_table *spt = &mm->context.spt;\r\nu32 spp = 0;\r\nu32 **sbpm, *sbpp;\r\nif (ea >= spt->maxaddr)\r\nreturn 0;\r\nif (ea < 0x100000000) {\r\nsbpm = spt->low_prot;\r\n} else {\r\nsbpm = spt->protptrs[ea >> SBP_L3_SHIFT];\r\nif (!sbpm)\r\nreturn 0;\r\n}\r\nsbpp = sbpm[(ea >> SBP_L2_SHIFT) & (SBP_L2_COUNT - 1)];\r\nif (!sbpp)\r\nreturn 0;\r\nspp = sbpp[(ea >> PAGE_SHIFT) & (SBP_L1_COUNT - 1)];\r\nspp >>= 30 - 2 * ((ea >> 12) & 0xf);\r\nspp = ((spp & 2) ? _PAGE_USER : 0) | ((spp & 1) ? _PAGE_RW : 0);\r\nreturn spp;\r\n}\r\nstatic inline int subpage_protection(struct mm_struct *mm, unsigned long ea)\r\n{\r\nreturn 0;\r\n}\r\nvoid hash_failure_debug(unsigned long ea, unsigned long access,\r\nunsigned long vsid, unsigned long trap,\r\nint ssize, int psize, unsigned long pte)\r\n{\r\nif (!printk_ratelimit())\r\nreturn;\r\npr_info("mm: Hashing failure ! EA=0x%lx access=0x%lx current=%s\n",\r\nea, access, current->comm);\r\npr_info(" trap=0x%lx vsid=0x%lx ssize=%d psize=%d pte=0x%lx\n",\r\ntrap, vsid, ssize, psize, pte);\r\n}\r\nint hash_page(unsigned long ea, unsigned long access, unsigned long trap)\r\n{\r\npgd_t *pgdir;\r\nunsigned long vsid;\r\nstruct mm_struct *mm;\r\npte_t *ptep;\r\nunsigned hugeshift;\r\nconst struct cpumask *tmp;\r\nint rc, user_region = 0, local = 0;\r\nint psize, ssize;\r\nDBG_LOW("hash_page(ea=%016lx, access=%lx, trap=%lx\n",\r\nea, access, trap);\r\nif ((ea & ~REGION_MASK) >= PGTABLE_RANGE) {\r\nDBG_LOW(" out of pgtable range !\n");\r\nreturn 1;\r\n}\r\nswitch (REGION_ID(ea)) {\r\ncase USER_REGION_ID:\r\nuser_region = 1;\r\nmm = current->mm;\r\nif (! mm) {\r\nDBG_LOW(" user region with no mm !\n");\r\nreturn 1;\r\n}\r\npsize = get_slice_psize(mm, ea);\r\nssize = user_segment_size(ea);\r\nvsid = get_vsid(mm->context.id, ea, ssize);\r\nbreak;\r\ncase VMALLOC_REGION_ID:\r\nmm = &init_mm;\r\nvsid = get_kernel_vsid(ea, mmu_kernel_ssize);\r\nif (ea < VMALLOC_END)\r\npsize = mmu_vmalloc_psize;\r\nelse\r\npsize = mmu_io_psize;\r\nssize = mmu_kernel_ssize;\r\nbreak;\r\ndefault:\r\nreturn 1;\r\n}\r\nDBG_LOW(" mm=%p, mm->pgdir=%p, vsid=%016lx\n", mm, mm->pgd, vsid);\r\npgdir = mm->pgd;\r\nif (pgdir == NULL)\r\nreturn 1;\r\ntmp = cpumask_of(smp_processor_id());\r\nif (user_region && cpumask_equal(mm_cpumask(mm), tmp))\r\nlocal = 1;\r\n#ifndef CONFIG_PPC_64K_PAGES\r\nif (psize != MMU_PAGE_4K)\r\nea &= ~((1ul << mmu_psize_defs[psize].shift) - 1);\r\n#endif\r\nptep = find_linux_pte_or_hugepte(pgdir, ea, &hugeshift);\r\nif (ptep == NULL || !pte_present(*ptep)) {\r\nDBG_LOW(" no PTE !\n");\r\nreturn 1;\r\n}\r\naccess |= _PAGE_PRESENT;\r\nif (access & ~pte_val(*ptep)) {\r\nDBG_LOW(" no access !\n");\r\nreturn 1;\r\n}\r\n#ifdef CONFIG_HUGETLB_PAGE\r\nif (hugeshift)\r\nreturn __hash_page_huge(ea, access, vsid, ptep, trap, local,\r\nssize, hugeshift, psize);\r\n#endif\r\n#ifndef CONFIG_PPC_64K_PAGES\r\nDBG_LOW(" i-pte: %016lx\n", pte_val(*ptep));\r\n#else\r\nDBG_LOW(" i-pte: %016lx %016lx\n", pte_val(*ptep),\r\npte_val(*(ptep + PTRS_PER_PTE)));\r\n#endif\r\n#ifdef CONFIG_PPC_64K_PAGES\r\nif ((pte_val(*ptep) & _PAGE_4K_PFN) && psize == MMU_PAGE_64K) {\r\ndemote_segment_4k(mm, ea);\r\npsize = MMU_PAGE_4K;\r\n}\r\nif (mmu_ci_restrictions && psize == MMU_PAGE_64K &&\r\n(pte_val(*ptep) & _PAGE_NO_CACHE)) {\r\nif (user_region) {\r\ndemote_segment_4k(mm, ea);\r\npsize = MMU_PAGE_4K;\r\n} else if (ea < VMALLOC_END) {\r\nprintk(KERN_ALERT "Reducing vmalloc segment "\r\n"to 4kB pages because of "\r\n"non-cacheable mapping\n");\r\npsize = mmu_vmalloc_psize = MMU_PAGE_4K;\r\n#ifdef CONFIG_SPU_BASE\r\nspu_flush_all_slbs(mm);\r\n#endif\r\n}\r\n}\r\nif (user_region) {\r\nif (psize != get_paca_psize(ea)) {\r\nget_paca()->context = mm->context;\r\nslb_flush_and_rebolt();\r\n}\r\n} else if (get_paca()->vmalloc_sllp !=\r\nmmu_psize_defs[mmu_vmalloc_psize].sllp) {\r\nget_paca()->vmalloc_sllp =\r\nmmu_psize_defs[mmu_vmalloc_psize].sllp;\r\nslb_vmalloc_update();\r\n}\r\n#endif\r\n#ifdef CONFIG_PPC_HAS_HASH_64K\r\nif (psize == MMU_PAGE_64K)\r\nrc = __hash_page_64K(ea, access, vsid, ptep, trap, local, ssize);\r\nelse\r\n#endif\r\n{\r\nint spp = subpage_protection(mm, ea);\r\nif (access & spp)\r\nrc = -2;\r\nelse\r\nrc = __hash_page_4K(ea, access, vsid, ptep, trap,\r\nlocal, ssize, spp);\r\n}\r\nif (rc == -1)\r\nhash_failure_debug(ea, access, vsid, trap, ssize, psize,\r\npte_val(*ptep));\r\n#ifndef CONFIG_PPC_64K_PAGES\r\nDBG_LOW(" o-pte: %016lx\n", pte_val(*ptep));\r\n#else\r\nDBG_LOW(" o-pte: %016lx %016lx\n", pte_val(*ptep),\r\npte_val(*(ptep + PTRS_PER_PTE)));\r\n#endif\r\nDBG_LOW(" -> rc=%d\n", rc);\r\nreturn rc;\r\n}\r\nvoid hash_preload(struct mm_struct *mm, unsigned long ea,\r\nunsigned long access, unsigned long trap)\r\n{\r\nunsigned long vsid;\r\npgd_t *pgdir;\r\npte_t *ptep;\r\nunsigned long flags;\r\nint rc, ssize, local = 0;\r\nBUG_ON(REGION_ID(ea) != USER_REGION_ID);\r\n#ifdef CONFIG_PPC_MM_SLICES\r\nif (unlikely(get_slice_psize(mm, ea) != mm->context.user_psize))\r\nreturn;\r\n#endif\r\nDBG_LOW("hash_preload(mm=%p, mm->pgdir=%p, ea=%016lx, access=%lx,"\r\n" trap=%lx\n", mm, mm->pgd, ea, access, trap);\r\npgdir = mm->pgd;\r\nif (pgdir == NULL)\r\nreturn;\r\nptep = find_linux_pte(pgdir, ea);\r\nif (!ptep)\r\nreturn;\r\n#ifdef CONFIG_PPC_64K_PAGES\r\nif (pte_val(*ptep) & (_PAGE_4K_PFN | _PAGE_NO_CACHE))\r\nreturn;\r\n#endif\r\nssize = user_segment_size(ea);\r\nvsid = get_vsid(mm->context.id, ea, ssize);\r\nlocal_irq_save(flags);\r\nif (cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))\r\nlocal = 1;\r\n#ifdef CONFIG_PPC_HAS_HASH_64K\r\nif (mm->context.user_psize == MMU_PAGE_64K)\r\nrc = __hash_page_64K(ea, access, vsid, ptep, trap, local, ssize);\r\nelse\r\n#endif\r\nrc = __hash_page_4K(ea, access, vsid, ptep, trap, local, ssize,\r\nsubpage_protection(mm, ea));\r\nif (rc == -1)\r\nhash_failure_debug(ea, access, vsid, trap, ssize,\r\nmm->context.user_psize, pte_val(*ptep));\r\nlocal_irq_restore(flags);\r\n}\r\nvoid flush_hash_page(unsigned long va, real_pte_t pte, int psize, int ssize,\r\nint local)\r\n{\r\nunsigned long hash, index, shift, hidx, slot;\r\nDBG_LOW("flush_hash_page(va=%016lx)\n", va);\r\npte_iterate_hashed_subpages(pte, psize, va, index, shift) {\r\nhash = hpt_hash(va, shift, ssize);\r\nhidx = __rpte_to_hidx(pte, index);\r\nif (hidx & _PTEIDX_SECONDARY)\r\nhash = ~hash;\r\nslot = (hash & htab_hash_mask) * HPTES_PER_GROUP;\r\nslot += hidx & _PTEIDX_GROUP_IX;\r\nDBG_LOW(" sub %ld: hash=%lx, hidx=%lx\n", index, slot, hidx);\r\nppc_md.hpte_invalidate(slot, va, psize, ssize, local);\r\n} pte_iterate_hashed_end();\r\n}\r\nvoid flush_hash_range(unsigned long number, int local)\r\n{\r\nif (ppc_md.flush_hash_range)\r\nppc_md.flush_hash_range(number, local);\r\nelse {\r\nint i;\r\nstruct ppc64_tlb_batch *batch =\r\n&__get_cpu_var(ppc64_tlb_batch);\r\nfor (i = 0; i < number; i++)\r\nflush_hash_page(batch->vaddr[i], batch->pte[i],\r\nbatch->psize, batch->ssize, local);\r\n}\r\n}\r\nvoid low_hash_fault(struct pt_regs *regs, unsigned long address, int rc)\r\n{\r\nif (user_mode(regs)) {\r\n#ifdef CONFIG_PPC_SUBPAGE_PROT\r\nif (rc == -2)\r\n_exception(SIGSEGV, regs, SEGV_ACCERR, address);\r\nelse\r\n#endif\r\n_exception(SIGBUS, regs, BUS_ADRERR, address);\r\n} else\r\nbad_page_fault(regs, address, SIGBUS);\r\n}\r\nstatic void kernel_map_linear_page(unsigned long vaddr, unsigned long lmi)\r\n{\r\nunsigned long hash, hpteg;\r\nunsigned long vsid = get_kernel_vsid(vaddr, mmu_kernel_ssize);\r\nunsigned long va = hpt_va(vaddr, vsid, mmu_kernel_ssize);\r\nunsigned long mode = htab_convert_pte_flags(PAGE_KERNEL);\r\nint ret;\r\nhash = hpt_hash(va, PAGE_SHIFT, mmu_kernel_ssize);\r\nhpteg = ((hash & htab_hash_mask) * HPTES_PER_GROUP);\r\nret = ppc_md.hpte_insert(hpteg, va, __pa(vaddr),\r\nmode, HPTE_V_BOLTED,\r\nmmu_linear_psize, mmu_kernel_ssize);\r\nBUG_ON (ret < 0);\r\nspin_lock(&linear_map_hash_lock);\r\nBUG_ON(linear_map_hash_slots[lmi] & 0x80);\r\nlinear_map_hash_slots[lmi] = ret | 0x80;\r\nspin_unlock(&linear_map_hash_lock);\r\n}\r\nstatic void kernel_unmap_linear_page(unsigned long vaddr, unsigned long lmi)\r\n{\r\nunsigned long hash, hidx, slot;\r\nunsigned long vsid = get_kernel_vsid(vaddr, mmu_kernel_ssize);\r\nunsigned long va = hpt_va(vaddr, vsid, mmu_kernel_ssize);\r\nhash = hpt_hash(va, PAGE_SHIFT, mmu_kernel_ssize);\r\nspin_lock(&linear_map_hash_lock);\r\nBUG_ON(!(linear_map_hash_slots[lmi] & 0x80));\r\nhidx = linear_map_hash_slots[lmi] & 0x7f;\r\nlinear_map_hash_slots[lmi] = 0;\r\nspin_unlock(&linear_map_hash_lock);\r\nif (hidx & _PTEIDX_SECONDARY)\r\nhash = ~hash;\r\nslot = (hash & htab_hash_mask) * HPTES_PER_GROUP;\r\nslot += hidx & _PTEIDX_GROUP_IX;\r\nppc_md.hpte_invalidate(slot, va, mmu_linear_psize, mmu_kernel_ssize, 0);\r\n}\r\nvoid kernel_map_pages(struct page *page, int numpages, int enable)\r\n{\r\nunsigned long flags, vaddr, lmi;\r\nint i;\r\nlocal_irq_save(flags);\r\nfor (i = 0; i < numpages; i++, page++) {\r\nvaddr = (unsigned long)page_address(page);\r\nlmi = __pa(vaddr) >> PAGE_SHIFT;\r\nif (lmi >= linear_map_hash_count)\r\ncontinue;\r\nif (enable)\r\nkernel_map_linear_page(vaddr, lmi);\r\nelse\r\nkernel_unmap_linear_page(vaddr, lmi);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nvoid setup_initial_memory_limit(phys_addr_t first_memblock_base,\r\nphys_addr_t first_memblock_size)\r\n{\r\nBUG_ON(first_memblock_base != 0);\r\nppc64_rma_size = min_t(u64, first_memblock_size, 0x40000000);\r\nmemblock_set_current_limit(ppc64_rma_size);\r\n}
