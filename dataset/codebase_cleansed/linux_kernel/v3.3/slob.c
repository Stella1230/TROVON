static inline void struct_slob_page_wrong_size(void)\r\n{ BUILD_BUG_ON(sizeof(struct slob_page) != sizeof(struct page)); }\r\nstatic inline void free_slob_page(struct slob_page *sp)\r\n{\r\nreset_page_mapcount(&sp->page);\r\nsp->page.mapping = NULL;\r\n}\r\nstatic inline int is_slob_page(struct slob_page *sp)\r\n{\r\nreturn PageSlab((struct page *)sp);\r\n}\r\nstatic inline void set_slob_page(struct slob_page *sp)\r\n{\r\n__SetPageSlab((struct page *)sp);\r\n}\r\nstatic inline void clear_slob_page(struct slob_page *sp)\r\n{\r\n__ClearPageSlab((struct page *)sp);\r\n}\r\nstatic inline struct slob_page *slob_page(const void *addr)\r\n{\r\nreturn (struct slob_page *)virt_to_page(addr);\r\n}\r\nstatic inline int slob_page_free(struct slob_page *sp)\r\n{\r\nreturn PageSlobFree((struct page *)sp);\r\n}\r\nstatic void set_slob_page_free(struct slob_page *sp, struct list_head *list)\r\n{\r\nlist_add(&sp->list, list);\r\n__SetPageSlobFree((struct page *)sp);\r\n}\r\nstatic inline void clear_slob_page_free(struct slob_page *sp)\r\n{\r\nlist_del(&sp->list);\r\n__ClearPageSlobFree((struct page *)sp);\r\n}\r\nstatic void set_slob(slob_t *s, slobidx_t size, slob_t *next)\r\n{\r\nslob_t *base = (slob_t *)((unsigned long)s & PAGE_MASK);\r\nslobidx_t offset = next - base;\r\nif (size > 1) {\r\ns[0].units = size;\r\ns[1].units = offset;\r\n} else\r\ns[0].units = -offset;\r\n}\r\nstatic slobidx_t slob_units(slob_t *s)\r\n{\r\nif (s->units > 0)\r\nreturn s->units;\r\nreturn 1;\r\n}\r\nstatic slob_t *slob_next(slob_t *s)\r\n{\r\nslob_t *base = (slob_t *)((unsigned long)s & PAGE_MASK);\r\nslobidx_t next;\r\nif (s[0].units < 0)\r\nnext = -s[0].units;\r\nelse\r\nnext = s[1].units;\r\nreturn base+next;\r\n}\r\nstatic int slob_last(slob_t *s)\r\n{\r\nreturn !((unsigned long)slob_next(s) & ~PAGE_MASK);\r\n}\r\nstatic void *slob_new_pages(gfp_t gfp, int order, int node)\r\n{\r\nvoid *page;\r\n#ifdef CONFIG_NUMA\r\nif (node != -1)\r\npage = alloc_pages_exact_node(node, gfp, order);\r\nelse\r\n#endif\r\npage = alloc_pages(gfp, order);\r\nif (!page)\r\nreturn NULL;\r\nreturn page_address(page);\r\n}\r\nstatic void slob_free_pages(void *b, int order)\r\n{\r\nif (current->reclaim_state)\r\ncurrent->reclaim_state->reclaimed_slab += 1 << order;\r\nfree_pages((unsigned long)b, order);\r\n}\r\nstatic void *slob_page_alloc(struct slob_page *sp, size_t size, int align)\r\n{\r\nslob_t *prev, *cur, *aligned = NULL;\r\nint delta = 0, units = SLOB_UNITS(size);\r\nfor (prev = NULL, cur = sp->free; ; prev = cur, cur = slob_next(cur)) {\r\nslobidx_t avail = slob_units(cur);\r\nif (align) {\r\naligned = (slob_t *)ALIGN((unsigned long)cur, align);\r\ndelta = aligned - cur;\r\n}\r\nif (avail >= units + delta) {\r\nslob_t *next;\r\nif (delta) {\r\nnext = slob_next(cur);\r\nset_slob(aligned, avail - delta, next);\r\nset_slob(cur, delta, aligned);\r\nprev = cur;\r\ncur = aligned;\r\navail = slob_units(cur);\r\n}\r\nnext = slob_next(cur);\r\nif (avail == units) {\r\nif (prev)\r\nset_slob(prev, slob_units(prev), next);\r\nelse\r\nsp->free = next;\r\n} else {\r\nif (prev)\r\nset_slob(prev, slob_units(prev), cur + units);\r\nelse\r\nsp->free = cur + units;\r\nset_slob(cur + units, avail - units, next);\r\n}\r\nsp->units -= units;\r\nif (!sp->units)\r\nclear_slob_page_free(sp);\r\nreturn cur;\r\n}\r\nif (slob_last(cur))\r\nreturn NULL;\r\n}\r\n}\r\nstatic void *slob_alloc(size_t size, gfp_t gfp, int align, int node)\r\n{\r\nstruct slob_page *sp;\r\nstruct list_head *prev;\r\nstruct list_head *slob_list;\r\nslob_t *b = NULL;\r\nunsigned long flags;\r\nif (size < SLOB_BREAK1)\r\nslob_list = &free_slob_small;\r\nelse if (size < SLOB_BREAK2)\r\nslob_list = &free_slob_medium;\r\nelse\r\nslob_list = &free_slob_large;\r\nspin_lock_irqsave(&slob_lock, flags);\r\nlist_for_each_entry(sp, slob_list, list) {\r\n#ifdef CONFIG_NUMA\r\nif (node != -1 && page_to_nid(&sp->page) != node)\r\ncontinue;\r\n#endif\r\nif (sp->units < SLOB_UNITS(size))\r\ncontinue;\r\nprev = sp->list.prev;\r\nb = slob_page_alloc(sp, size, align);\r\nif (!b)\r\ncontinue;\r\nif (prev != slob_list->prev &&\r\nslob_list->next != prev->next)\r\nlist_move_tail(slob_list, prev->next);\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&slob_lock, flags);\r\nif (!b) {\r\nb = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);\r\nif (!b)\r\nreturn NULL;\r\nsp = slob_page(b);\r\nset_slob_page(sp);\r\nspin_lock_irqsave(&slob_lock, flags);\r\nsp->units = SLOB_UNITS(PAGE_SIZE);\r\nsp->free = b;\r\nINIT_LIST_HEAD(&sp->list);\r\nset_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));\r\nset_slob_page_free(sp, slob_list);\r\nb = slob_page_alloc(sp, size, align);\r\nBUG_ON(!b);\r\nspin_unlock_irqrestore(&slob_lock, flags);\r\n}\r\nif (unlikely((gfp & __GFP_ZERO) && b))\r\nmemset(b, 0, size);\r\nreturn b;\r\n}\r\nstatic void slob_free(void *block, int size)\r\n{\r\nstruct slob_page *sp;\r\nslob_t *prev, *next, *b = (slob_t *)block;\r\nslobidx_t units;\r\nunsigned long flags;\r\nstruct list_head *slob_list;\r\nif (unlikely(ZERO_OR_NULL_PTR(block)))\r\nreturn;\r\nBUG_ON(!size);\r\nsp = slob_page(block);\r\nunits = SLOB_UNITS(size);\r\nspin_lock_irqsave(&slob_lock, flags);\r\nif (sp->units + units == SLOB_UNITS(PAGE_SIZE)) {\r\nif (slob_page_free(sp))\r\nclear_slob_page_free(sp);\r\nspin_unlock_irqrestore(&slob_lock, flags);\r\nclear_slob_page(sp);\r\nfree_slob_page(sp);\r\nslob_free_pages(b, 0);\r\nreturn;\r\n}\r\nif (!slob_page_free(sp)) {\r\nsp->units = units;\r\nsp->free = b;\r\nset_slob(b, units,\r\n(void *)((unsigned long)(b +\r\nSLOB_UNITS(PAGE_SIZE)) & PAGE_MASK));\r\nif (size < SLOB_BREAK1)\r\nslob_list = &free_slob_small;\r\nelse if (size < SLOB_BREAK2)\r\nslob_list = &free_slob_medium;\r\nelse\r\nslob_list = &free_slob_large;\r\nset_slob_page_free(sp, slob_list);\r\ngoto out;\r\n}\r\nsp->units += units;\r\nif (b < sp->free) {\r\nif (b + units == sp->free) {\r\nunits += slob_units(sp->free);\r\nsp->free = slob_next(sp->free);\r\n}\r\nset_slob(b, units, sp->free);\r\nsp->free = b;\r\n} else {\r\nprev = sp->free;\r\nnext = slob_next(prev);\r\nwhile (b > next) {\r\nprev = next;\r\nnext = slob_next(prev);\r\n}\r\nif (!slob_last(prev) && b + units == next) {\r\nunits += slob_units(next);\r\nset_slob(b, units, slob_next(next));\r\n} else\r\nset_slob(b, units, next);\r\nif (prev + slob_units(prev) == b) {\r\nunits = slob_units(b) + slob_units(prev);\r\nset_slob(prev, units, slob_next(b));\r\n} else\r\nset_slob(prev, slob_units(prev), b);\r\n}\r\nout:\r\nspin_unlock_irqrestore(&slob_lock, flags);\r\n}\r\nvoid *__kmalloc_node(size_t size, gfp_t gfp, int node)\r\n{\r\nunsigned int *m;\r\nint align = max(ARCH_KMALLOC_MINALIGN, ARCH_SLAB_MINALIGN);\r\nvoid *ret;\r\ngfp &= gfp_allowed_mask;\r\nlockdep_trace_alloc(gfp);\r\nif (size < PAGE_SIZE - align) {\r\nif (!size)\r\nreturn ZERO_SIZE_PTR;\r\nm = slob_alloc(size + align, gfp, align, node);\r\nif (!m)\r\nreturn NULL;\r\n*m = size;\r\nret = (void *)m + align;\r\ntrace_kmalloc_node(_RET_IP_, ret,\r\nsize, size + align, gfp, node);\r\n} else {\r\nunsigned int order = get_order(size);\r\nif (likely(order))\r\ngfp |= __GFP_COMP;\r\nret = slob_new_pages(gfp, order, node);\r\nif (ret) {\r\nstruct page *page;\r\npage = virt_to_page(ret);\r\npage->private = size;\r\n}\r\ntrace_kmalloc_node(_RET_IP_, ret,\r\nsize, PAGE_SIZE << order, gfp, node);\r\n}\r\nkmemleak_alloc(ret, size, 1, gfp);\r\nreturn ret;\r\n}\r\nvoid kfree(const void *block)\r\n{\r\nstruct slob_page *sp;\r\ntrace_kfree(_RET_IP_, block);\r\nif (unlikely(ZERO_OR_NULL_PTR(block)))\r\nreturn;\r\nkmemleak_free(block);\r\nsp = slob_page(block);\r\nif (is_slob_page(sp)) {\r\nint align = max(ARCH_KMALLOC_MINALIGN, ARCH_SLAB_MINALIGN);\r\nunsigned int *m = (unsigned int *)(block - align);\r\nslob_free(m, *m + align);\r\n} else\r\nput_page(&sp->page);\r\n}\r\nsize_t ksize(const void *block)\r\n{\r\nstruct slob_page *sp;\r\nBUG_ON(!block);\r\nif (unlikely(block == ZERO_SIZE_PTR))\r\nreturn 0;\r\nsp = slob_page(block);\r\nif (is_slob_page(sp)) {\r\nint align = max(ARCH_KMALLOC_MINALIGN, ARCH_SLAB_MINALIGN);\r\nunsigned int *m = (unsigned int *)(block - align);\r\nreturn SLOB_UNITS(*m) * SLOB_UNIT;\r\n} else\r\nreturn sp->page.private;\r\n}\r\nstruct kmem_cache *kmem_cache_create(const char *name, size_t size,\r\nsize_t align, unsigned long flags, void (*ctor)(void *))\r\n{\r\nstruct kmem_cache *c;\r\nc = slob_alloc(sizeof(struct kmem_cache),\r\nGFP_KERNEL, ARCH_KMALLOC_MINALIGN, -1);\r\nif (c) {\r\nc->name = name;\r\nc->size = size;\r\nif (flags & SLAB_DESTROY_BY_RCU) {\r\nc->size += sizeof(struct slob_rcu);\r\n}\r\nc->flags = flags;\r\nc->ctor = ctor;\r\nc->align = (flags & SLAB_HWCACHE_ALIGN) ? SLOB_ALIGN : 0;\r\nif (c->align < ARCH_SLAB_MINALIGN)\r\nc->align = ARCH_SLAB_MINALIGN;\r\nif (c->align < align)\r\nc->align = align;\r\n} else if (flags & SLAB_PANIC)\r\npanic("Cannot create slab cache %s\n", name);\r\nkmemleak_alloc(c, sizeof(struct kmem_cache), 1, GFP_KERNEL);\r\nreturn c;\r\n}\r\nvoid kmem_cache_destroy(struct kmem_cache *c)\r\n{\r\nkmemleak_free(c);\r\nif (c->flags & SLAB_DESTROY_BY_RCU)\r\nrcu_barrier();\r\nslob_free(c, sizeof(struct kmem_cache));\r\n}\r\nvoid *kmem_cache_alloc_node(struct kmem_cache *c, gfp_t flags, int node)\r\n{\r\nvoid *b;\r\nflags &= gfp_allowed_mask;\r\nlockdep_trace_alloc(flags);\r\nif (c->size < PAGE_SIZE) {\r\nb = slob_alloc(c->size, flags, c->align, node);\r\ntrace_kmem_cache_alloc_node(_RET_IP_, b, c->size,\r\nSLOB_UNITS(c->size) * SLOB_UNIT,\r\nflags, node);\r\n} else {\r\nb = slob_new_pages(flags, get_order(c->size), node);\r\ntrace_kmem_cache_alloc_node(_RET_IP_, b, c->size,\r\nPAGE_SIZE << get_order(c->size),\r\nflags, node);\r\n}\r\nif (c->ctor)\r\nc->ctor(b);\r\nkmemleak_alloc_recursive(b, c->size, 1, c->flags, flags);\r\nreturn b;\r\n}\r\nstatic void __kmem_cache_free(void *b, int size)\r\n{\r\nif (size < PAGE_SIZE)\r\nslob_free(b, size);\r\nelse\r\nslob_free_pages(b, get_order(size));\r\n}\r\nstatic void kmem_rcu_free(struct rcu_head *head)\r\n{\r\nstruct slob_rcu *slob_rcu = (struct slob_rcu *)head;\r\nvoid *b = (void *)slob_rcu - (slob_rcu->size - sizeof(struct slob_rcu));\r\n__kmem_cache_free(b, slob_rcu->size);\r\n}\r\nvoid kmem_cache_free(struct kmem_cache *c, void *b)\r\n{\r\nkmemleak_free_recursive(b, c->flags);\r\nif (unlikely(c->flags & SLAB_DESTROY_BY_RCU)) {\r\nstruct slob_rcu *slob_rcu;\r\nslob_rcu = b + (c->size - sizeof(struct slob_rcu));\r\nslob_rcu->size = c->size;\r\ncall_rcu(&slob_rcu->head, kmem_rcu_free);\r\n} else {\r\n__kmem_cache_free(b, c->size);\r\n}\r\ntrace_kmem_cache_free(_RET_IP_, b);\r\n}\r\nunsigned int kmem_cache_size(struct kmem_cache *c)\r\n{\r\nreturn c->size;\r\n}\r\nint kmem_cache_shrink(struct kmem_cache *d)\r\n{\r\nreturn 0;\r\n}\r\nint slab_is_available(void)\r\n{\r\nreturn slob_ready;\r\n}\r\nvoid __init kmem_cache_init(void)\r\n{\r\nslob_ready = 1;\r\n}\r\nvoid __init kmem_cache_init_late(void)\r\n{\r\n}
