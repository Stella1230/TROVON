static int __init set_noallocl2(char *str)\r\n{\r\nnoallocl2 = 1;\r\nreturn 0;\r\n}\r\nstatic void hv_flush_update(const struct cpumask *cache_cpumask,\r\nstruct cpumask *tlb_cpumask,\r\nunsigned long tlb_va, unsigned long tlb_length,\r\nHV_Remote_ASID *asids, int asidcount)\r\n{\r\nstruct cpumask mask;\r\nint i, cpu;\r\ncpumask_clear(&mask);\r\nif (cache_cpumask)\r\ncpumask_or(&mask, &mask, cache_cpumask);\r\nif (tlb_cpumask && tlb_length) {\r\ncpumask_or(&mask, &mask, tlb_cpumask);\r\n}\r\nfor (i = 0; i < asidcount; ++i)\r\ncpumask_set_cpu(asids[i].y * smp_width + asids[i].x, &mask);\r\nfor_each_cpu(cpu, &mask)\r\n++per_cpu(irq_stat, cpu).irq_hv_flush_count;\r\n}\r\nvoid flush_remote(unsigned long cache_pfn, unsigned long cache_control,\r\nconst struct cpumask *cache_cpumask_orig,\r\nHV_VirtAddr tlb_va, unsigned long tlb_length,\r\nunsigned long tlb_pgsize,\r\nconst struct cpumask *tlb_cpumask_orig,\r\nHV_Remote_ASID *asids, int asidcount)\r\n{\r\nint rc;\r\nint timestamp = 0;\r\nstruct cpumask cache_cpumask_copy, tlb_cpumask_copy;\r\nstruct cpumask *cache_cpumask, *tlb_cpumask;\r\nHV_PhysAddr cache_pa;\r\nchar cache_buf[NR_CPUS*5], tlb_buf[NR_CPUS*5];\r\nmb();\r\nif (cache_cpumask_orig && cache_control) {\r\ncpumask_copy(&cache_cpumask_copy, cache_cpumask_orig);\r\ncache_cpumask = &cache_cpumask_copy;\r\n} else {\r\ncpumask_clear(&cache_cpumask_copy);\r\ncache_cpumask = NULL;\r\n}\r\nif (cache_cpumask == NULL)\r\ncache_control = 0;\r\nif (tlb_cpumask_orig && tlb_length) {\r\ncpumask_copy(&tlb_cpumask_copy, tlb_cpumask_orig);\r\ntlb_cpumask = &tlb_cpumask_copy;\r\n} else {\r\ncpumask_clear(&tlb_cpumask_copy);\r\ntlb_cpumask = NULL;\r\n}\r\nhv_flush_update(cache_cpumask, tlb_cpumask, tlb_va, tlb_length,\r\nasids, asidcount);\r\ncache_pa = (HV_PhysAddr)cache_pfn << PAGE_SHIFT;\r\nif (cache_control & HV_FLUSH_EVICT_L2)\r\ntimestamp = mark_caches_evicted_start();\r\nrc = hv_flush_remote(cache_pa, cache_control,\r\ncpumask_bits(cache_cpumask),\r\ntlb_va, tlb_length, tlb_pgsize,\r\ncpumask_bits(tlb_cpumask),\r\nasids, asidcount);\r\nif (cache_control & HV_FLUSH_EVICT_L2)\r\nmark_caches_evicted_finish(cache_cpumask, timestamp);\r\nif (rc == 0)\r\nreturn;\r\ncpumask_scnprintf(cache_buf, sizeof(cache_buf), &cache_cpumask_copy);\r\ncpumask_scnprintf(tlb_buf, sizeof(tlb_buf), &tlb_cpumask_copy);\r\npr_err("hv_flush_remote(%#llx, %#lx, %p [%s],"\r\n" %#lx, %#lx, %#lx, %p [%s], %p, %d) = %d\n",\r\ncache_pa, cache_control, cache_cpumask, cache_buf,\r\n(unsigned long)tlb_va, tlb_length, tlb_pgsize,\r\ntlb_cpumask, tlb_buf,\r\nasids, asidcount, rc);\r\npanic("Unsafe to continue.");\r\n}\r\nvoid flush_remote_page(struct page *page, int order)\r\n{\r\nint i, pages = (1 << order);\r\nfor (i = 0; i < pages; ++i, ++page) {\r\nvoid *p = kmap_atomic(page);\r\nint hfh = 0;\r\nint home = page_home(page);\r\n#if CHIP_HAS_CBOX_HOME_MAP()\r\nif (home == PAGE_HOME_HASH)\r\nhfh = 1;\r\nelse\r\n#endif\r\nBUG_ON(home < 0 || home >= NR_CPUS);\r\nfinv_buffer_remote(p, PAGE_SIZE, hfh);\r\nkunmap_atomic(p);\r\n}\r\n}\r\nvoid homecache_evict(const struct cpumask *mask)\r\n{\r\nflush_remote(0, HV_FLUSH_EVICT_L2, mask, 0, 0, 0, NULL, NULL, 0);\r\n}\r\nstatic int homecache_mask(struct page *page, int pages,\r\nstruct cpumask *home_mask)\r\n{\r\nint i;\r\nint cached_coherently = 1;\r\ncpumask_clear(home_mask);\r\nfor (i = 0; i < pages; ++i) {\r\nint home = page_home(&page[i]);\r\nif (home == PAGE_HOME_IMMUTABLE ||\r\nhome == PAGE_HOME_INCOHERENT) {\r\ncpumask_copy(home_mask, cpu_possible_mask);\r\nreturn 0;\r\n}\r\n#if CHIP_HAS_CBOX_HOME_MAP()\r\nif (home == PAGE_HOME_HASH) {\r\ncpumask_or(home_mask, home_mask, &hash_for_home_map);\r\ncontinue;\r\n}\r\n#endif\r\nif (home == PAGE_HOME_UNCACHED) {\r\ncached_coherently = 0;\r\ncontinue;\r\n}\r\nBUG_ON(home < 0 || home >= NR_CPUS);\r\ncpumask_set_cpu(home, home_mask);\r\n}\r\nreturn cached_coherently;\r\n}\r\nstatic unsigned long cache_flush_length(unsigned long length)\r\n{\r\nreturn (length >= CHIP_L2_CACHE_SIZE()) ? HV_FLUSH_EVICT_L2 : length;\r\n}\r\nvoid homecache_flush_cache(struct page *page, int order)\r\n{\r\nint pages = 1 << order;\r\nint length = cache_flush_length(pages * PAGE_SIZE);\r\nunsigned long pfn = page_to_pfn(page);\r\nstruct cpumask home_mask;\r\nhomecache_mask(page, pages, &home_mask);\r\nflush_remote(pfn, length, &home_mask, 0, 0, 0, NULL, NULL, 0);\r\nsim_validate_lines_evicted(PFN_PHYS(pfn), pages * PAGE_SIZE);\r\n}\r\nstatic int pte_to_home(pte_t pte)\r\n{\r\nif (hv_pte_get_nc(pte))\r\nreturn PAGE_HOME_IMMUTABLE;\r\nswitch (hv_pte_get_mode(pte)) {\r\ncase HV_PTE_MODE_CACHE_TILE_L3:\r\nreturn get_remote_cache_cpu(pte);\r\ncase HV_PTE_MODE_CACHE_NO_L3:\r\nreturn PAGE_HOME_INCOHERENT;\r\ncase HV_PTE_MODE_UNCACHED:\r\nreturn PAGE_HOME_UNCACHED;\r\n#if CHIP_HAS_CBOX_HOME_MAP()\r\ncase HV_PTE_MODE_CACHE_HASH_L3:\r\nreturn PAGE_HOME_HASH;\r\n#endif\r\n}\r\npanic("Bad PTE %#llx\n", pte.val);\r\n}\r\npte_t pte_set_home(pte_t pte, int home)\r\n{\r\nif (pte_file(pte))\r\nreturn pte;\r\n#if CHIP_HAS_MMIO()\r\nif (hv_pte_get_mode(pte) == HV_PTE_MODE_MMIO)\r\nreturn pte;\r\n#endif\r\nif (hv_pte_get_nc(pte) && home != PAGE_HOME_IMMUTABLE) {\r\npte = hv_pte_clear_nc(pte);\r\npr_err("non-immutable page incoherently referenced: %#llx\n",\r\npte.val);\r\n}\r\nswitch (home) {\r\ncase PAGE_HOME_UNCACHED:\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_UNCACHED);\r\nbreak;\r\ncase PAGE_HOME_INCOHERENT:\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_NO_L3);\r\nbreak;\r\ncase PAGE_HOME_IMMUTABLE:\r\nBUG_ON(hv_pte_get_writable(pte));\r\nif (pte_get_forcecache(pte)) {\r\nif (hv_pte_get_mode(pte) == HV_PTE_MODE_CACHE_TILE_L3\r\n&& pte_get_anyhome(pte)) {\r\npte = hv_pte_set_mode(pte,\r\nHV_PTE_MODE_CACHE_NO_L3);\r\n}\r\n} else\r\n#if CHIP_HAS_CBOX_HOME_MAP()\r\nif (hash_default)\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_HASH_L3);\r\nelse\r\n#endif\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_NO_L3);\r\npte = hv_pte_set_nc(pte);\r\nbreak;\r\n#if CHIP_HAS_CBOX_HOME_MAP()\r\ncase PAGE_HOME_HASH:\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_HASH_L3);\r\nbreak;\r\n#endif\r\ndefault:\r\nBUG_ON(home < 0 || home >= NR_CPUS ||\r\n!cpu_is_valid_lotar(home));\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_CACHE_TILE_L3);\r\npte = set_remote_cache_cpu(pte, home);\r\nbreak;\r\n}\r\n#if CHIP_HAS_NC_AND_NOALLOC_BITS()\r\nif (noallocl2)\r\npte = hv_pte_set_no_alloc_l2(pte);\r\nif (hv_pte_get_no_alloc_l2(pte) && hv_pte_get_no_alloc_l1(pte) &&\r\nhv_pte_get_mode(pte) == HV_PTE_MODE_CACHE_NO_L3) {\r\npte = hv_pte_set_mode(pte, HV_PTE_MODE_UNCACHED);\r\n}\r\n#endif\r\nBUG_ON(hv_pte_get_mode(pte) == 0);\r\nreturn pte;\r\n}\r\nstatic inline int initial_page_home(void) { return PAGE_HOME_HASH; }\r\nstatic inline int initial_page_home(void) { return 0; }\r\nint page_home(struct page *page)\r\n{\r\nif (PageHighMem(page)) {\r\nreturn initial_page_home();\r\n} else {\r\nunsigned long kva = (unsigned long)page_address(page);\r\nreturn pte_to_home(*virt_to_pte(NULL, kva));\r\n}\r\n}\r\nvoid homecache_change_page_home(struct page *page, int order, int home)\r\n{\r\nint i, pages = (1 << order);\r\nunsigned long kva;\r\nBUG_ON(PageHighMem(page));\r\nBUG_ON(page_count(page) > 1);\r\nBUG_ON(page_mapcount(page) != 0);\r\nkva = (unsigned long) page_address(page);\r\nflush_remote(0, HV_FLUSH_EVICT_L2, &cpu_cacheable_map,\r\nkva, pages * PAGE_SIZE, PAGE_SIZE, cpu_online_mask,\r\nNULL, 0);\r\nfor (i = 0; i < pages; ++i, kva += PAGE_SIZE) {\r\npte_t *ptep = virt_to_pte(NULL, kva);\r\npte_t pteval = *ptep;\r\nBUG_ON(!pte_present(pteval) || pte_huge(pteval));\r\n__set_pte(ptep, pte_set_home(pteval, home));\r\n}\r\n}\r\nstruct page *homecache_alloc_pages(gfp_t gfp_mask,\r\nunsigned int order, int home)\r\n{\r\nstruct page *page;\r\nBUG_ON(gfp_mask & __GFP_HIGHMEM);\r\npage = alloc_pages(gfp_mask, order);\r\nif (page)\r\nhomecache_change_page_home(page, order, home);\r\nreturn page;\r\n}\r\nstruct page *homecache_alloc_pages_node(int nid, gfp_t gfp_mask,\r\nunsigned int order, int home)\r\n{\r\nstruct page *page;\r\nBUG_ON(gfp_mask & __GFP_HIGHMEM);\r\npage = alloc_pages_node(nid, gfp_mask, order);\r\nif (page)\r\nhomecache_change_page_home(page, order, home);\r\nreturn page;\r\n}\r\nvoid homecache_free_pages(unsigned long addr, unsigned int order)\r\n{\r\nstruct page *page;\r\nif (addr == 0)\r\nreturn;\r\nVM_BUG_ON(!virt_addr_valid((void *)addr));\r\npage = virt_to_page((void *)addr);\r\nif (put_page_testzero(page)) {\r\nhomecache_change_page_home(page, order, initial_page_home());\r\nif (order == 0) {\r\nfree_hot_cold_page(page, 0);\r\n} else {\r\ninit_page_count(page);\r\n__free_pages(page, order);\r\n}\r\n}\r\n}
