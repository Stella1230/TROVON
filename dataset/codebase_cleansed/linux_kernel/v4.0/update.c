void __rcu_read_lock(void)\r\n{\r\ncurrent->rcu_read_lock_nesting++;\r\nbarrier();\r\n}\r\nvoid __rcu_read_unlock(void)\r\n{\r\nstruct task_struct *t = current;\r\nif (t->rcu_read_lock_nesting != 1) {\r\n--t->rcu_read_lock_nesting;\r\n} else {\r\nbarrier();\r\nt->rcu_read_lock_nesting = INT_MIN;\r\nbarrier();\r\nif (unlikely(ACCESS_ONCE(t->rcu_read_unlock_special.s)))\r\nrcu_read_unlock_special(t);\r\nbarrier();\r\nt->rcu_read_lock_nesting = 0;\r\n}\r\n#ifdef CONFIG_PROVE_LOCKING\r\n{\r\nint rrln = ACCESS_ONCE(t->rcu_read_lock_nesting);\r\nWARN_ON_ONCE(rrln < 0 && rrln > INT_MIN / 2);\r\n}\r\n#endif\r\n}\r\nint notrace debug_lockdep_rcu_enabled(void)\r\n{\r\nreturn rcu_scheduler_active && debug_locks &&\r\ncurrent->lockdep_recursion == 0;\r\n}\r\nint rcu_read_lock_held(void)\r\n{\r\nif (!debug_lockdep_rcu_enabled())\r\nreturn 1;\r\nif (!rcu_is_watching())\r\nreturn 0;\r\nif (!rcu_lockdep_current_cpu_online())\r\nreturn 0;\r\nreturn lock_is_held(&rcu_lock_map);\r\n}\r\nint rcu_read_lock_bh_held(void)\r\n{\r\nif (!debug_lockdep_rcu_enabled())\r\nreturn 1;\r\nif (!rcu_is_watching())\r\nreturn 0;\r\nif (!rcu_lockdep_current_cpu_online())\r\nreturn 0;\r\nreturn in_softirq() || irqs_disabled();\r\n}\r\nstatic void wakeme_after_rcu(struct rcu_head *head)\r\n{\r\nstruct rcu_synchronize *rcu;\r\nrcu = container_of(head, struct rcu_synchronize, head);\r\ncomplete(&rcu->completion);\r\n}\r\nvoid wait_rcu_gp(call_rcu_func_t crf)\r\n{\r\nstruct rcu_synchronize rcu;\r\ninit_rcu_head_on_stack(&rcu.head);\r\ninit_completion(&rcu.completion);\r\ncrf(&rcu.head, wakeme_after_rcu);\r\nwait_for_completion(&rcu.completion);\r\ndestroy_rcu_head_on_stack(&rcu.head);\r\n}\r\nvoid init_rcu_head(struct rcu_head *head)\r\n{\r\ndebug_object_init(head, &rcuhead_debug_descr);\r\n}\r\nvoid destroy_rcu_head(struct rcu_head *head)\r\n{\r\ndebug_object_free(head, &rcuhead_debug_descr);\r\n}\r\nstatic int rcuhead_fixup_activate(void *addr, enum debug_obj_state state)\r\n{\r\nstruct rcu_head *head = addr;\r\nswitch (state) {\r\ncase ODEBUG_STATE_NOTAVAILABLE:\r\ndebug_object_init(head, &rcuhead_debug_descr);\r\ndebug_object_activate(head, &rcuhead_debug_descr);\r\nreturn 0;\r\ndefault:\r\nreturn 1;\r\n}\r\n}\r\nvoid init_rcu_head_on_stack(struct rcu_head *head)\r\n{\r\ndebug_object_init_on_stack(head, &rcuhead_debug_descr);\r\n}\r\nvoid destroy_rcu_head_on_stack(struct rcu_head *head)\r\n{\r\ndebug_object_free(head, &rcuhead_debug_descr);\r\n}\r\nvoid do_trace_rcu_torture_read(const char *rcutorturename, struct rcu_head *rhp,\r\nunsigned long secs,\r\nunsigned long c_old, unsigned long c)\r\n{\r\ntrace_rcu_torture_read(rcutorturename, rhp, secs, c_old, c);\r\n}\r\nint rcu_jiffies_till_stall_check(void)\r\n{\r\nint till_stall_check = ACCESS_ONCE(rcu_cpu_stall_timeout);\r\nif (till_stall_check < 3) {\r\nACCESS_ONCE(rcu_cpu_stall_timeout) = 3;\r\ntill_stall_check = 3;\r\n} else if (till_stall_check > 300) {\r\nACCESS_ONCE(rcu_cpu_stall_timeout) = 300;\r\ntill_stall_check = 300;\r\n}\r\nreturn till_stall_check * HZ + RCU_STALL_DELAY_DELTA;\r\n}\r\nvoid rcu_sysrq_start(void)\r\n{\r\nif (!rcu_cpu_stall_suppress)\r\nrcu_cpu_stall_suppress = 2;\r\n}\r\nvoid rcu_sysrq_end(void)\r\n{\r\nif (rcu_cpu_stall_suppress == 2)\r\nrcu_cpu_stall_suppress = 0;\r\n}\r\nstatic int rcu_panic(struct notifier_block *this, unsigned long ev, void *ptr)\r\n{\r\nrcu_cpu_stall_suppress = 1;\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic int __init check_cpu_stall_init(void)\r\n{\r\natomic_notifier_chain_register(&panic_notifier_list, &rcu_panic_block);\r\nreturn 0;\r\n}\r\nvoid call_rcu_tasks(struct rcu_head *rhp, void (*func)(struct rcu_head *rhp))\r\n{\r\nunsigned long flags;\r\nbool needwake;\r\nrhp->next = NULL;\r\nrhp->func = func;\r\nraw_spin_lock_irqsave(&rcu_tasks_cbs_lock, flags);\r\nneedwake = !rcu_tasks_cbs_head;\r\n*rcu_tasks_cbs_tail = rhp;\r\nrcu_tasks_cbs_tail = &rhp->next;\r\nraw_spin_unlock_irqrestore(&rcu_tasks_cbs_lock, flags);\r\nif (needwake) {\r\nrcu_spawn_tasks_kthread();\r\nwake_up(&rcu_tasks_cbs_wq);\r\n}\r\n}\r\nvoid synchronize_rcu_tasks(void)\r\n{\r\nrcu_lockdep_assert(!rcu_scheduler_active,\r\n"synchronize_rcu_tasks called too soon");\r\nwait_rcu_gp(call_rcu_tasks);\r\n}\r\nvoid rcu_barrier_tasks(void)\r\n{\r\nsynchronize_rcu_tasks();\r\n}\r\nstatic void check_holdout_task(struct task_struct *t,\r\nbool needreport, bool *firstreport)\r\n{\r\nint cpu;\r\nif (!ACCESS_ONCE(t->rcu_tasks_holdout) ||\r\nt->rcu_tasks_nvcsw != ACCESS_ONCE(t->nvcsw) ||\r\n!ACCESS_ONCE(t->on_rq) ||\r\n(IS_ENABLED(CONFIG_NO_HZ_FULL) &&\r\n!is_idle_task(t) && t->rcu_tasks_idle_cpu >= 0)) {\r\nACCESS_ONCE(t->rcu_tasks_holdout) = false;\r\nlist_del_init(&t->rcu_tasks_holdout_list);\r\nput_task_struct(t);\r\nreturn;\r\n}\r\nif (!needreport)\r\nreturn;\r\nif (*firstreport) {\r\npr_err("INFO: rcu_tasks detected stalls on tasks:\n");\r\n*firstreport = false;\r\n}\r\ncpu = task_cpu(t);\r\npr_alert("%p: %c%c nvcsw: %lu/%lu holdout: %d idle_cpu: %d/%d\n",\r\nt, ".I"[is_idle_task(t)],\r\n"N."[cpu < 0 || !tick_nohz_full_cpu(cpu)],\r\nt->rcu_tasks_nvcsw, t->nvcsw, t->rcu_tasks_holdout,\r\nt->rcu_tasks_idle_cpu, cpu);\r\nsched_show_task(t);\r\n}\r\nstatic int __noreturn rcu_tasks_kthread(void *arg)\r\n{\r\nunsigned long flags;\r\nstruct task_struct *g, *t;\r\nunsigned long lastreport;\r\nstruct rcu_head *list;\r\nstruct rcu_head *next;\r\nLIST_HEAD(rcu_tasks_holdouts);\r\nhousekeeping_affine(current);\r\nfor (;;) {\r\nraw_spin_lock_irqsave(&rcu_tasks_cbs_lock, flags);\r\nlist = rcu_tasks_cbs_head;\r\nrcu_tasks_cbs_head = NULL;\r\nrcu_tasks_cbs_tail = &rcu_tasks_cbs_head;\r\nraw_spin_unlock_irqrestore(&rcu_tasks_cbs_lock, flags);\r\nif (!list) {\r\nwait_event_interruptible(rcu_tasks_cbs_wq,\r\nrcu_tasks_cbs_head);\r\nif (!rcu_tasks_cbs_head) {\r\nWARN_ON(signal_pending(current));\r\nschedule_timeout_interruptible(HZ/10);\r\n}\r\ncontinue;\r\n}\r\nsynchronize_sched();\r\nrcu_read_lock();\r\nfor_each_process_thread(g, t) {\r\nif (t != current && ACCESS_ONCE(t->on_rq) &&\r\n!is_idle_task(t)) {\r\nget_task_struct(t);\r\nt->rcu_tasks_nvcsw = ACCESS_ONCE(t->nvcsw);\r\nACCESS_ONCE(t->rcu_tasks_holdout) = true;\r\nlist_add(&t->rcu_tasks_holdout_list,\r\n&rcu_tasks_holdouts);\r\n}\r\n}\r\nrcu_read_unlock();\r\nsynchronize_srcu(&tasks_rcu_exit_srcu);\r\nlastreport = jiffies;\r\nwhile (!list_empty(&rcu_tasks_holdouts)) {\r\nbool firstreport;\r\nbool needreport;\r\nint rtst;\r\nstruct task_struct *t1;\r\nschedule_timeout_interruptible(HZ);\r\nrtst = ACCESS_ONCE(rcu_task_stall_timeout);\r\nneedreport = rtst > 0 &&\r\ntime_after(jiffies, lastreport + rtst);\r\nif (needreport)\r\nlastreport = jiffies;\r\nfirstreport = true;\r\nWARN_ON(signal_pending(current));\r\nlist_for_each_entry_safe(t, t1, &rcu_tasks_holdouts,\r\nrcu_tasks_holdout_list) {\r\ncheck_holdout_task(t, needreport, &firstreport);\r\ncond_resched();\r\n}\r\n}\r\nsynchronize_sched();\r\nwhile (list) {\r\nnext = list->next;\r\nlocal_bh_disable();\r\nlist->func(list);\r\nlocal_bh_enable();\r\nlist = next;\r\ncond_resched();\r\n}\r\nschedule_timeout_uninterruptible(HZ/10);\r\n}\r\n}\r\nstatic void rcu_spawn_tasks_kthread(void)\r\n{\r\nstatic DEFINE_MUTEX(rcu_tasks_kthread_mutex);\r\nstatic struct task_struct *rcu_tasks_kthread_ptr;\r\nstruct task_struct *t;\r\nif (ACCESS_ONCE(rcu_tasks_kthread_ptr)) {\r\nsmp_mb();\r\nreturn;\r\n}\r\nmutex_lock(&rcu_tasks_kthread_mutex);\r\nif (rcu_tasks_kthread_ptr) {\r\nmutex_unlock(&rcu_tasks_kthread_mutex);\r\nreturn;\r\n}\r\nt = kthread_run(rcu_tasks_kthread, NULL, "rcu_tasks_kthread");\r\nBUG_ON(IS_ERR(t));\r\nsmp_mb();\r\nACCESS_ONCE(rcu_tasks_kthread_ptr) = t;\r\nmutex_unlock(&rcu_tasks_kthread_mutex);\r\n}\r\nstatic void test_callback(struct rcu_head *r)\r\n{\r\nrcu_self_test_counter++;\r\npr_info("RCU test callback executed %d\n", rcu_self_test_counter);\r\n}\r\nstatic void early_boot_test_call_rcu(void)\r\n{\r\nstatic struct rcu_head head;\r\ncall_rcu(&head, test_callback);\r\n}\r\nstatic void early_boot_test_call_rcu_bh(void)\r\n{\r\nstatic struct rcu_head head;\r\ncall_rcu_bh(&head, test_callback);\r\n}\r\nstatic void early_boot_test_call_rcu_sched(void)\r\n{\r\nstatic struct rcu_head head;\r\ncall_rcu_sched(&head, test_callback);\r\n}\r\nvoid rcu_early_boot_tests(void)\r\n{\r\npr_info("Running RCU self tests\n");\r\nif (rcu_self_test)\r\nearly_boot_test_call_rcu();\r\nif (rcu_self_test_bh)\r\nearly_boot_test_call_rcu_bh();\r\nif (rcu_self_test_sched)\r\nearly_boot_test_call_rcu_sched();\r\n}\r\nstatic int rcu_verify_early_boot_tests(void)\r\n{\r\nint ret = 0;\r\nint early_boot_test_counter = 0;\r\nif (rcu_self_test) {\r\nearly_boot_test_counter++;\r\nrcu_barrier();\r\n}\r\nif (rcu_self_test_bh) {\r\nearly_boot_test_counter++;\r\nrcu_barrier_bh();\r\n}\r\nif (rcu_self_test_sched) {\r\nearly_boot_test_counter++;\r\nrcu_barrier_sched();\r\n}\r\nif (rcu_self_test_counter != early_boot_test_counter) {\r\nWARN_ON(1);\r\nret = -1;\r\n}\r\nreturn ret;\r\n}\r\nvoid rcu_early_boot_tests(void) {}
