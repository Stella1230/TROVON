static void snbep_uncore_pci_disable_box(struct intel_uncore_box *box)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nint box_ctl = uncore_pci_box_ctl(box);\r\nu32 config = 0;\r\nif (!pci_read_config_dword(pdev, box_ctl, &config)) {\r\nconfig |= SNBEP_PMON_BOX_CTL_FRZ;\r\npci_write_config_dword(pdev, box_ctl, config);\r\n}\r\n}\r\nstatic void snbep_uncore_pci_enable_box(struct intel_uncore_box *box)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nint box_ctl = uncore_pci_box_ctl(box);\r\nu32 config = 0;\r\nif (!pci_read_config_dword(pdev, box_ctl, &config)) {\r\nconfig &= ~SNBEP_PMON_BOX_CTL_FRZ;\r\npci_write_config_dword(pdev, box_ctl, config);\r\n}\r\n}\r\nstatic void snbep_uncore_pci_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\npci_write_config_dword(pdev, hwc->config_base, hwc->config | SNBEP_PMON_CTL_EN);\r\n}\r\nstatic void snbep_uncore_pci_disable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\npci_write_config_dword(pdev, hwc->config_base, hwc->config);\r\n}\r\nstatic u64 snbep_uncore_pci_read_counter(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 count = 0;\r\npci_read_config_dword(pdev, hwc->event_base, (u32 *)&count);\r\npci_read_config_dword(pdev, hwc->event_base + 4, (u32 *)&count + 1);\r\nreturn count;\r\n}\r\nstatic void snbep_uncore_pci_init_box(struct intel_uncore_box *box)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\npci_write_config_dword(pdev, SNBEP_PCI_PMON_BOX_CTL, SNBEP_PMON_BOX_CTL_INT);\r\n}\r\nstatic void snbep_uncore_msr_disable_box(struct intel_uncore_box *box)\r\n{\r\nu64 config;\r\nunsigned msr;\r\nmsr = uncore_msr_box_ctl(box);\r\nif (msr) {\r\nrdmsrl(msr, config);\r\nconfig |= SNBEP_PMON_BOX_CTL_FRZ;\r\nwrmsrl(msr, config);\r\n}\r\n}\r\nstatic void snbep_uncore_msr_enable_box(struct intel_uncore_box *box)\r\n{\r\nu64 config;\r\nunsigned msr;\r\nmsr = uncore_msr_box_ctl(box);\r\nif (msr) {\r\nrdmsrl(msr, config);\r\nconfig &= ~SNBEP_PMON_BOX_CTL_FRZ;\r\nwrmsrl(msr, config);\r\n}\r\n}\r\nstatic void snbep_uncore_msr_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nif (reg1->idx != EXTRA_REG_NONE)\r\nwrmsrl(reg1->reg, uncore_shared_reg_config(box, 0));\r\nwrmsrl(hwc->config_base, hwc->config | SNBEP_PMON_CTL_EN);\r\n}\r\nstatic void snbep_uncore_msr_disable_event(struct intel_uncore_box *box,\r\nstruct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nwrmsrl(hwc->config_base, hwc->config);\r\n}\r\nstatic void snbep_uncore_msr_init_box(struct intel_uncore_box *box)\r\n{\r\nunsigned msr = uncore_msr_box_ctl(box);\r\nif (msr)\r\nwrmsrl(msr, SNBEP_PMON_BOX_CTL_INT);\r\n}\r\nstatic void snbep_cbox_put_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct intel_uncore_extra_reg *er = &box->shared_regs[0];\r\nint i;\r\nif (uncore_box_is_fake(box))\r\nreturn;\r\nfor (i = 0; i < 5; i++) {\r\nif (reg1->alloc & (0x1 << i))\r\natomic_sub(1 << (i * 6), &er->ref);\r\n}\r\nreg1->alloc = 0;\r\n}\r\nstatic struct event_constraint *\r\n__snbep_cbox_get_constraint(struct intel_uncore_box *box, struct perf_event *event,\r\nu64 (*cbox_filter_mask)(int fields))\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct intel_uncore_extra_reg *er = &box->shared_regs[0];\r\nint i, alloc = 0;\r\nunsigned long flags;\r\nu64 mask;\r\nif (reg1->idx == EXTRA_REG_NONE)\r\nreturn NULL;\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nfor (i = 0; i < 5; i++) {\r\nif (!(reg1->idx & (0x1 << i)))\r\ncontinue;\r\nif (!uncore_box_is_fake(box) && (reg1->alloc & (0x1 << i)))\r\ncontinue;\r\nmask = cbox_filter_mask(0x1 << i);\r\nif (!__BITS_VALUE(atomic_read(&er->ref), i, 6) ||\r\n!((reg1->config ^ er->config) & mask)) {\r\natomic_add(1 << (i * 6), &er->ref);\r\ner->config &= ~mask;\r\ner->config |= reg1->config & mask;\r\nalloc |= (0x1 << i);\r\n} else {\r\nbreak;\r\n}\r\n}\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nif (i < 5)\r\ngoto fail;\r\nif (!uncore_box_is_fake(box))\r\nreg1->alloc |= alloc;\r\nreturn NULL;\r\nfail:\r\nfor (; i >= 0; i--) {\r\nif (alloc & (0x1 << i))\r\natomic_sub(1 << (i * 6), &er->ref);\r\n}\r\nreturn &uncore_constraint_empty;\r\n}\r\nstatic u64 snbep_cbox_filter_mask(int fields)\r\n{\r\nu64 mask = 0;\r\nif (fields & 0x1)\r\nmask |= SNBEP_CB0_MSR_PMON_BOX_FILTER_TID;\r\nif (fields & 0x2)\r\nmask |= SNBEP_CB0_MSR_PMON_BOX_FILTER_NID;\r\nif (fields & 0x4)\r\nmask |= SNBEP_CB0_MSR_PMON_BOX_FILTER_STATE;\r\nif (fields & 0x8)\r\nmask |= SNBEP_CB0_MSR_PMON_BOX_FILTER_OPC;\r\nreturn mask;\r\n}\r\nstatic struct event_constraint *\r\nsnbep_cbox_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nreturn __snbep_cbox_get_constraint(box, event, snbep_cbox_filter_mask);\r\n}\r\nstatic int snbep_cbox_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct extra_reg *er;\r\nint idx = 0;\r\nfor (er = snbep_uncore_cbox_extra_regs; er->msr; er++) {\r\nif (er->event != (event->hw.config & er->config_mask))\r\ncontinue;\r\nidx |= er->idx;\r\n}\r\nif (idx) {\r\nreg1->reg = SNBEP_C0_MSR_PMON_BOX_FILTER +\r\nSNBEP_CBO_MSR_OFFSET * box->pmu->pmu_idx;\r\nreg1->config = event->attr.config1 & snbep_cbox_filter_mask(idx);\r\nreg1->idx = idx;\r\n}\r\nreturn 0;\r\n}\r\nstatic u64 snbep_pcu_alter_er(struct perf_event *event, int new_idx, bool modify)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nu64 config = reg1->config;\r\nif (new_idx > reg1->idx)\r\nconfig <<= 8 * (new_idx - reg1->idx);\r\nelse\r\nconfig >>= 8 * (reg1->idx - new_idx);\r\nif (modify) {\r\nhwc->config += new_idx - reg1->idx;\r\nreg1->config = config;\r\nreg1->idx = new_idx;\r\n}\r\nreturn config;\r\n}\r\nstatic struct event_constraint *\r\nsnbep_pcu_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct intel_uncore_extra_reg *er = &box->shared_regs[0];\r\nunsigned long flags;\r\nint idx = reg1->idx;\r\nu64 mask, config1 = reg1->config;\r\nbool ok = false;\r\nif (reg1->idx == EXTRA_REG_NONE ||\r\n(!uncore_box_is_fake(box) && reg1->alloc))\r\nreturn NULL;\r\nagain:\r\nmask = 0xffULL << (idx * 8);\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nif (!__BITS_VALUE(atomic_read(&er->ref), idx, 8) ||\r\n!((config1 ^ er->config) & mask)) {\r\natomic_add(1 << (idx * 8), &er->ref);\r\ner->config &= ~mask;\r\ner->config |= config1 & mask;\r\nok = true;\r\n}\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nif (!ok) {\r\nidx = (idx + 1) % 4;\r\nif (idx != reg1->idx) {\r\nconfig1 = snbep_pcu_alter_er(event, idx, false);\r\ngoto again;\r\n}\r\nreturn &uncore_constraint_empty;\r\n}\r\nif (!uncore_box_is_fake(box)) {\r\nif (idx != reg1->idx)\r\nsnbep_pcu_alter_er(event, idx, true);\r\nreg1->alloc = 1;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void snbep_pcu_put_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct intel_uncore_extra_reg *er = &box->shared_regs[0];\r\nif (uncore_box_is_fake(box) || !reg1->alloc)\r\nreturn;\r\natomic_sub(1 << (reg1->idx * 8), &er->ref);\r\nreg1->alloc = 0;\r\n}\r\nstatic int snbep_pcu_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nint ev_sel = hwc->config & SNBEP_PMON_CTL_EV_SEL_MASK;\r\nif (ev_sel >= 0xb && ev_sel <= 0xe) {\r\nreg1->reg = SNBEP_PCU_MSR_PMON_BOX_FILTER;\r\nreg1->idx = ev_sel - 0xb;\r\nreg1->config = event->attr.config1 & (0xff << (reg1->idx * 8));\r\n}\r\nreturn 0;\r\n}\r\nvoid snbep_uncore_cpu_init(void)\r\n{\r\nif (snbep_uncore_cbox.num_boxes > boot_cpu_data.x86_max_cores)\r\nsnbep_uncore_cbox.num_boxes = boot_cpu_data.x86_max_cores;\r\nuncore_msr_uncores = snbep_msr_uncores;\r\n}\r\nstatic int snbep_qpi_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nif ((hwc->config & SNBEP_PMON_CTL_EV_SEL_MASK) == 0x38) {\r\nreg1->idx = 0;\r\nreg1->reg = SNBEP_Q_Py_PCI_PMON_PKT_MATCH0;\r\nreg1->config = event->attr.config1;\r\nreg2->reg = SNBEP_Q_Py_PCI_PMON_PKT_MASK0;\r\nreg2->config = event->attr.config2;\r\n}\r\nreturn 0;\r\n}\r\nstatic void snbep_qpi_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &hwc->branch_reg;\r\nif (reg1->idx != EXTRA_REG_NONE) {\r\nint idx = box->pmu->pmu_idx + SNBEP_PCI_QPI_PORT0_FILTER;\r\nstruct pci_dev *filter_pdev = uncore_extra_pci_dev[box->phys_id][idx];\r\nif (filter_pdev) {\r\npci_write_config_dword(filter_pdev, reg1->reg,\r\n(u32)reg1->config);\r\npci_write_config_dword(filter_pdev, reg1->reg + 4,\r\n(u32)(reg1->config >> 32));\r\npci_write_config_dword(filter_pdev, reg2->reg,\r\n(u32)reg2->config);\r\npci_write_config_dword(filter_pdev, reg2->reg + 4,\r\n(u32)(reg2->config >> 32));\r\n}\r\n}\r\npci_write_config_dword(pdev, hwc->config_base, hwc->config | SNBEP_PMON_CTL_EN);\r\n}\r\nstatic int snbep_pci2phy_map_init(int devid)\r\n{\r\nstruct pci_dev *ubox_dev = NULL;\r\nint i, bus, nodeid;\r\nint err = 0;\r\nu32 config = 0;\r\nwhile (1) {\r\nubox_dev = pci_get_device(PCI_VENDOR_ID_INTEL, devid, ubox_dev);\r\nif (!ubox_dev)\r\nbreak;\r\nbus = ubox_dev->bus->number;\r\nerr = pci_read_config_dword(ubox_dev, 0x40, &config);\r\nif (err)\r\nbreak;\r\nnodeid = config;\r\nerr = pci_read_config_dword(ubox_dev, 0x54, &config);\r\nif (err)\r\nbreak;\r\nfor (i = 0; i < 8; i++) {\r\nif (nodeid == ((config >> (3 * i)) & 0x7)) {\r\nuncore_pcibus_to_physid[bus] = i;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (!err) {\r\ni = -1;\r\nfor (bus = 255; bus >= 0; bus--) {\r\nif (uncore_pcibus_to_physid[bus] >= 0)\r\ni = uncore_pcibus_to_physid[bus];\r\nelse\r\nuncore_pcibus_to_physid[bus] = i;\r\n}\r\n}\r\nif (ubox_dev)\r\npci_dev_put(ubox_dev);\r\nreturn err ? pcibios_err_to_errno(err) : 0;\r\n}\r\nint snbep_uncore_pci_init(void)\r\n{\r\nint ret = snbep_pci2phy_map_init(0x3ce0);\r\nif (ret)\r\nreturn ret;\r\nuncore_pci_uncores = snbep_pci_uncores;\r\nuncore_pci_driver = &snbep_uncore_pci_driver;\r\nreturn 0;\r\n}\r\nstatic void ivbep_uncore_msr_init_box(struct intel_uncore_box *box)\r\n{\r\nunsigned msr = uncore_msr_box_ctl(box);\r\nif (msr)\r\nwrmsrl(msr, IVBEP_PMON_BOX_CTL_INT);\r\n}\r\nstatic void ivbep_uncore_pci_init_box(struct intel_uncore_box *box)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\npci_write_config_dword(pdev, SNBEP_PCI_PMON_BOX_CTL, IVBEP_PMON_BOX_CTL_INT);\r\n}\r\nstatic u64 ivbep_cbox_filter_mask(int fields)\r\n{\r\nu64 mask = 0;\r\nif (fields & 0x1)\r\nmask |= IVBEP_CB0_MSR_PMON_BOX_FILTER_TID;\r\nif (fields & 0x2)\r\nmask |= IVBEP_CB0_MSR_PMON_BOX_FILTER_LINK;\r\nif (fields & 0x4)\r\nmask |= IVBEP_CB0_MSR_PMON_BOX_FILTER_STATE;\r\nif (fields & 0x8)\r\nmask |= IVBEP_CB0_MSR_PMON_BOX_FILTER_NID;\r\nif (fields & 0x10) {\r\nmask |= IVBEP_CB0_MSR_PMON_BOX_FILTER_OPC;\r\nmask |= IVBEP_CB0_MSR_PMON_BOX_FILTER_NC;\r\nmask |= IVBEP_CB0_MSR_PMON_BOX_FILTER_C6;\r\nmask |= IVBEP_CB0_MSR_PMON_BOX_FILTER_ISOC;\r\n}\r\nreturn mask;\r\n}\r\nstatic struct event_constraint *\r\nivbep_cbox_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nreturn __snbep_cbox_get_constraint(box, event, ivbep_cbox_filter_mask);\r\n}\r\nstatic int ivbep_cbox_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct extra_reg *er;\r\nint idx = 0;\r\nfor (er = ivbep_uncore_cbox_extra_regs; er->msr; er++) {\r\nif (er->event != (event->hw.config & er->config_mask))\r\ncontinue;\r\nidx |= er->idx;\r\n}\r\nif (idx) {\r\nreg1->reg = SNBEP_C0_MSR_PMON_BOX_FILTER +\r\nSNBEP_CBO_MSR_OFFSET * box->pmu->pmu_idx;\r\nreg1->config = event->attr.config1 & ivbep_cbox_filter_mask(idx);\r\nreg1->idx = idx;\r\n}\r\nreturn 0;\r\n}\r\nstatic void ivbep_cbox_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nif (reg1->idx != EXTRA_REG_NONE) {\r\nu64 filter = uncore_shared_reg_config(box, 0);\r\nwrmsrl(reg1->reg, filter & 0xffffffff);\r\nwrmsrl(reg1->reg + 6, filter >> 32);\r\n}\r\nwrmsrl(hwc->config_base, hwc->config | SNBEP_PMON_CTL_EN);\r\n}\r\nvoid ivbep_uncore_cpu_init(void)\r\n{\r\nif (ivbep_uncore_cbox.num_boxes > boot_cpu_data.x86_max_cores)\r\nivbep_uncore_cbox.num_boxes = boot_cpu_data.x86_max_cores;\r\nuncore_msr_uncores = ivbep_msr_uncores;\r\n}\r\nstatic void ivbep_uncore_irp_enable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\npci_write_config_dword(pdev, ivbep_uncore_irp_ctls[hwc->idx],\r\nhwc->config | SNBEP_PMON_CTL_EN);\r\n}\r\nstatic void ivbep_uncore_irp_disable_event(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\npci_write_config_dword(pdev, ivbep_uncore_irp_ctls[hwc->idx], hwc->config);\r\n}\r\nstatic u64 ivbep_uncore_irp_read_counter(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 count = 0;\r\npci_read_config_dword(pdev, ivbep_uncore_irp_ctrs[hwc->idx], (u32 *)&count);\r\npci_read_config_dword(pdev, ivbep_uncore_irp_ctrs[hwc->idx] + 4, (u32 *)&count + 1);\r\nreturn count;\r\n}\r\nint ivbep_uncore_pci_init(void)\r\n{\r\nint ret = snbep_pci2phy_map_init(0x0e1e);\r\nif (ret)\r\nreturn ret;\r\nuncore_pci_uncores = ivbep_pci_uncores;\r\nuncore_pci_driver = &ivbep_uncore_pci_driver;\r\nreturn 0;\r\n}\r\nstatic int hswep_ubox_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nreg1->reg = HSWEP_U_MSR_PMON_FILTER;\r\nreg1->config = event->attr.config1 & HSWEP_U_MSR_PMON_BOX_FILTER_MASK;\r\nreg1->idx = 0;\r\nreturn 0;\r\n}\r\nstatic u64 hswep_cbox_filter_mask(int fields)\r\n{\r\nu64 mask = 0;\r\nif (fields & 0x1)\r\nmask |= HSWEP_CB0_MSR_PMON_BOX_FILTER_TID;\r\nif (fields & 0x2)\r\nmask |= HSWEP_CB0_MSR_PMON_BOX_FILTER_LINK;\r\nif (fields & 0x4)\r\nmask |= HSWEP_CB0_MSR_PMON_BOX_FILTER_STATE;\r\nif (fields & 0x8)\r\nmask |= HSWEP_CB0_MSR_PMON_BOX_FILTER_NID;\r\nif (fields & 0x10) {\r\nmask |= HSWEP_CB0_MSR_PMON_BOX_FILTER_OPC;\r\nmask |= HSWEP_CB0_MSR_PMON_BOX_FILTER_NC;\r\nmask |= HSWEP_CB0_MSR_PMON_BOX_FILTER_C6;\r\nmask |= HSWEP_CB0_MSR_PMON_BOX_FILTER_ISOC;\r\n}\r\nreturn mask;\r\n}\r\nstatic struct event_constraint *\r\nhswep_cbox_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nreturn __snbep_cbox_get_constraint(box, event, hswep_cbox_filter_mask);\r\n}\r\nstatic int hswep_cbox_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct extra_reg *er;\r\nint idx = 0;\r\nfor (er = hswep_uncore_cbox_extra_regs; er->msr; er++) {\r\nif (er->event != (event->hw.config & er->config_mask))\r\ncontinue;\r\nidx |= er->idx;\r\n}\r\nif (idx) {\r\nreg1->reg = HSWEP_C0_MSR_PMON_BOX_FILTER0 +\r\nHSWEP_CBO_MSR_OFFSET * box->pmu->pmu_idx;\r\nreg1->config = event->attr.config1 & hswep_cbox_filter_mask(idx);\r\nreg1->idx = idx;\r\n}\r\nreturn 0;\r\n}\r\nstatic void hswep_cbox_enable_event(struct intel_uncore_box *box,\r\nstruct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nif (reg1->idx != EXTRA_REG_NONE) {\r\nu64 filter = uncore_shared_reg_config(box, 0);\r\nwrmsrl(reg1->reg, filter & 0xffffffff);\r\nwrmsrl(reg1->reg + 1, filter >> 32);\r\n}\r\nwrmsrl(hwc->config_base, hwc->config | SNBEP_PMON_CTL_EN);\r\n}\r\nstatic void hswep_uncore_sbox_msr_init_box(struct intel_uncore_box *box)\r\n{\r\nunsigned msr = uncore_msr_box_ctl(box);\r\nif (msr) {\r\nu64 init = SNBEP_PMON_BOX_CTL_INT;\r\nu64 flags = 0;\r\nint i;\r\nfor_each_set_bit(i, (unsigned long *)&init, 64) {\r\nflags |= (1ULL << i);\r\nwrmsrl(msr, flags);\r\n}\r\n}\r\n}\r\nstatic int hswep_pcu_hw_config(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nstruct hw_perf_event_extra *reg1 = &hwc->extra_reg;\r\nint ev_sel = hwc->config & SNBEP_PMON_CTL_EV_SEL_MASK;\r\nif (ev_sel >= 0xb && ev_sel <= 0xe) {\r\nreg1->reg = HSWEP_PCU_MSR_PMON_BOX_FILTER;\r\nreg1->idx = ev_sel - 0xb;\r\nreg1->config = event->attr.config1 & (0xff << reg1->idx);\r\n}\r\nreturn 0;\r\n}\r\nvoid hswep_uncore_cpu_init(void)\r\n{\r\nif (hswep_uncore_cbox.num_boxes > boot_cpu_data.x86_max_cores)\r\nhswep_uncore_cbox.num_boxes = boot_cpu_data.x86_max_cores;\r\nif (uncore_extra_pci_dev[0][HSWEP_PCI_PCU_3]) {\r\nu32 capid4;\r\npci_read_config_dword(uncore_extra_pci_dev[0][HSWEP_PCI_PCU_3],\r\n0x94, &capid4);\r\nif (((capid4 >> 6) & 0x3) == 0)\r\nhswep_uncore_sbox.num_boxes = 2;\r\n}\r\nuncore_msr_uncores = hswep_msr_uncores;\r\n}\r\nstatic u64 hswep_uncore_irp_read_counter(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct pci_dev *pdev = box->pci_dev;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 count = 0;\r\npci_read_config_dword(pdev, hswep_uncore_irp_ctrs[hwc->idx], (u32 *)&count);\r\npci_read_config_dword(pdev, hswep_uncore_irp_ctrs[hwc->idx] + 4, (u32 *)&count + 1);\r\nreturn count;\r\n}\r\nint hswep_uncore_pci_init(void)\r\n{\r\nint ret = snbep_pci2phy_map_init(0x2f1e);\r\nif (ret)\r\nreturn ret;\r\nuncore_pci_uncores = hswep_pci_uncores;\r\nuncore_pci_driver = &hswep_uncore_pci_driver;\r\nreturn 0;\r\n}
