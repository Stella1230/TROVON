static const char *mpx_mapping_name(struct vm_area_struct *vma)\r\n{\r\nreturn "[mpx]";\r\n}\r\nstatic int is_mpx_vma(struct vm_area_struct *vma)\r\n{\r\nreturn (vma->vm_ops == &mpx_vma_ops);\r\n}\r\nstatic unsigned long mpx_mmap(unsigned long len)\r\n{\r\nunsigned long ret;\r\nunsigned long addr, pgoff;\r\nstruct mm_struct *mm = current->mm;\r\nvm_flags_t vm_flags;\r\nstruct vm_area_struct *vma;\r\nif (len != MPX_BD_SIZE_BYTES && len != MPX_BT_SIZE_BYTES)\r\nreturn -EINVAL;\r\ndown_write(&mm->mmap_sem);\r\nif (mm->map_count > sysctl_max_map_count) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\naddr = get_unmapped_area(NULL, 0, len, 0, MAP_ANONYMOUS | MAP_PRIVATE);\r\nif (addr & ~PAGE_MASK) {\r\nret = addr;\r\ngoto out;\r\n}\r\nvm_flags = VM_READ | VM_WRITE | VM_MPX |\r\nmm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;\r\npgoff = addr >> PAGE_SHIFT;\r\nret = mmap_region(NULL, addr, len, vm_flags, pgoff);\r\nif (IS_ERR_VALUE(ret))\r\ngoto out;\r\nvma = find_vma(mm, ret);\r\nif (!vma) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nvma->vm_ops = &mpx_vma_ops;\r\nif (vm_flags & VM_LOCKED) {\r\nup_write(&mm->mmap_sem);\r\nmm_populate(ret, len);\r\nreturn ret;\r\n}\r\nout:\r\nup_write(&mm->mmap_sem);\r\nreturn ret;\r\n}\r\nstatic int get_reg_offset(struct insn *insn, struct pt_regs *regs,\r\nenum reg_type type)\r\n{\r\nint regno = 0;\r\nstatic const int regoff[] = {\r\noffsetof(struct pt_regs, ax),\r\noffsetof(struct pt_regs, cx),\r\noffsetof(struct pt_regs, dx),\r\noffsetof(struct pt_regs, bx),\r\noffsetof(struct pt_regs, sp),\r\noffsetof(struct pt_regs, bp),\r\noffsetof(struct pt_regs, si),\r\noffsetof(struct pt_regs, di),\r\n#ifdef CONFIG_X86_64\r\noffsetof(struct pt_regs, r8),\r\noffsetof(struct pt_regs, r9),\r\noffsetof(struct pt_regs, r10),\r\noffsetof(struct pt_regs, r11),\r\noffsetof(struct pt_regs, r12),\r\noffsetof(struct pt_regs, r13),\r\noffsetof(struct pt_regs, r14),\r\noffsetof(struct pt_regs, r15),\r\n#endif\r\n};\r\nint nr_registers = ARRAY_SIZE(regoff);\r\nif (IS_ENABLED(CONFIG_X86_64) && !insn->x86_64)\r\nnr_registers -= 8;\r\nswitch (type) {\r\ncase REG_TYPE_RM:\r\nregno = X86_MODRM_RM(insn->modrm.value);\r\nif (X86_REX_B(insn->rex_prefix.value) == 1)\r\nregno += 8;\r\nbreak;\r\ncase REG_TYPE_INDEX:\r\nregno = X86_SIB_INDEX(insn->sib.value);\r\nif (X86_REX_X(insn->rex_prefix.value) == 1)\r\nregno += 8;\r\nbreak;\r\ncase REG_TYPE_BASE:\r\nregno = X86_SIB_BASE(insn->sib.value);\r\nif (X86_REX_B(insn->rex_prefix.value) == 1)\r\nregno += 8;\r\nbreak;\r\ndefault:\r\npr_err("invalid register type");\r\nBUG();\r\nbreak;\r\n}\r\nif (regno > nr_registers) {\r\nWARN_ONCE(1, "decoded an instruction with an invalid register");\r\nreturn -EINVAL;\r\n}\r\nreturn regoff[regno];\r\n}\r\nstatic void __user *mpx_get_addr_ref(struct insn *insn, struct pt_regs *regs)\r\n{\r\nunsigned long addr, base, indx;\r\nint addr_offset, base_offset, indx_offset;\r\ninsn_byte_t sib;\r\ninsn_get_modrm(insn);\r\ninsn_get_sib(insn);\r\nsib = insn->sib.value;\r\nif (X86_MODRM_MOD(insn->modrm.value) == 3) {\r\naddr_offset = get_reg_offset(insn, regs, REG_TYPE_RM);\r\nif (addr_offset < 0)\r\ngoto out_err;\r\naddr = regs_get_register(regs, addr_offset);\r\n} else {\r\nif (insn->sib.nbytes) {\r\nbase_offset = get_reg_offset(insn, regs, REG_TYPE_BASE);\r\nif (base_offset < 0)\r\ngoto out_err;\r\nindx_offset = get_reg_offset(insn, regs, REG_TYPE_INDEX);\r\nif (indx_offset < 0)\r\ngoto out_err;\r\nbase = regs_get_register(regs, base_offset);\r\nindx = regs_get_register(regs, indx_offset);\r\naddr = base + indx * (1 << X86_SIB_SCALE(sib));\r\n} else {\r\naddr_offset = get_reg_offset(insn, regs, REG_TYPE_RM);\r\nif (addr_offset < 0)\r\ngoto out_err;\r\naddr = regs_get_register(regs, addr_offset);\r\n}\r\naddr += insn->displacement.value;\r\n}\r\nreturn (void __user *)addr;\r\nout_err:\r\nreturn (void __user *)-1;\r\n}\r\nstatic int mpx_insn_decode(struct insn *insn,\r\nstruct pt_regs *regs)\r\n{\r\nunsigned char buf[MAX_INSN_SIZE];\r\nint x86_64 = !test_thread_flag(TIF_IA32);\r\nint not_copied;\r\nint nr_copied;\r\nnot_copied = copy_from_user(buf, (void __user *)regs->ip, sizeof(buf));\r\nnr_copied = sizeof(buf) - not_copied;\r\nif (!nr_copied)\r\nreturn -EFAULT;\r\ninsn_init(insn, buf, nr_copied, x86_64);\r\ninsn_get_length(insn);\r\nif (nr_copied < insn->length)\r\nreturn -EFAULT;\r\ninsn_get_opcode(insn);\r\nif (insn->opcode.bytes[0] != 0x0f)\r\ngoto bad_opcode;\r\nif ((insn->opcode.bytes[1] != 0x1a) &&\r\n(insn->opcode.bytes[1] != 0x1b))\r\ngoto bad_opcode;\r\nreturn 0;\r\nbad_opcode:\r\nreturn -EINVAL;\r\n}\r\nsiginfo_t *mpx_generate_siginfo(struct pt_regs *regs,\r\nstruct xsave_struct *xsave_buf)\r\n{\r\nstruct bndreg *bndregs, *bndreg;\r\nsiginfo_t *info = NULL;\r\nstruct insn insn;\r\nuint8_t bndregno;\r\nint err;\r\nerr = mpx_insn_decode(&insn, regs);\r\nif (err)\r\ngoto err_out;\r\ninsn_get_modrm(&insn);\r\nbndregno = X86_MODRM_REG(insn.modrm.value);\r\nif (bndregno > 3) {\r\nerr = -EINVAL;\r\ngoto err_out;\r\n}\r\nbndregs = get_xsave_addr(xsave_buf, XSTATE_BNDREGS);\r\nif (!bndregs) {\r\nerr = -EINVAL;\r\ngoto err_out;\r\n}\r\nbndreg = &bndregs[bndregno];\r\ninfo = kzalloc(sizeof(*info), GFP_KERNEL);\r\nif (!info) {\r\nerr = -ENOMEM;\r\ngoto err_out;\r\n}\r\ninfo->si_lower = (void __user *)(unsigned long)bndreg->lower_bound;\r\ninfo->si_upper = (void __user *)(unsigned long)~bndreg->upper_bound;\r\ninfo->si_addr_lsb = 0;\r\ninfo->si_signo = SIGSEGV;\r\ninfo->si_errno = 0;\r\ninfo->si_code = SEGV_BNDERR;\r\ninfo->si_addr = mpx_get_addr_ref(&insn, regs);\r\nif (info->si_addr == (void *)-1) {\r\nerr = -EINVAL;\r\ngoto err_out;\r\n}\r\nreturn info;\r\nerr_out:\r\nkfree(info);\r\nreturn ERR_PTR(err);\r\n}\r\nstatic __user void *task_get_bounds_dir(struct task_struct *tsk)\r\n{\r\nstruct bndcsr *bndcsr;\r\nif (!cpu_feature_enabled(X86_FEATURE_MPX))\r\nreturn MPX_INVALID_BOUNDS_DIR;\r\nif (IS_ENABLED(CONFIG_X86_64) && test_thread_flag(TIF_IA32))\r\nreturn MPX_INVALID_BOUNDS_DIR;\r\nfpu_save_init(&tsk->thread.fpu);\r\nbndcsr = get_xsave_addr(&tsk->thread.fpu.state->xsave, XSTATE_BNDCSR);\r\nif (!bndcsr)\r\nreturn MPX_INVALID_BOUNDS_DIR;\r\nif (!(bndcsr->bndcfgu & MPX_BNDCFG_ENABLE_FLAG))\r\nreturn MPX_INVALID_BOUNDS_DIR;\r\nreturn (void __user *)(unsigned long)\r\n(bndcsr->bndcfgu & MPX_BNDCFG_ADDR_MASK);\r\n}\r\nint mpx_enable_management(struct task_struct *tsk)\r\n{\r\nvoid __user *bd_base = MPX_INVALID_BOUNDS_DIR;\r\nstruct mm_struct *mm = tsk->mm;\r\nint ret = 0;\r\nbd_base = task_get_bounds_dir(tsk);\r\ndown_write(&mm->mmap_sem);\r\nmm->bd_addr = bd_base;\r\nif (mm->bd_addr == MPX_INVALID_BOUNDS_DIR)\r\nret = -ENXIO;\r\nup_write(&mm->mmap_sem);\r\nreturn ret;\r\n}\r\nint mpx_disable_management(struct task_struct *tsk)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nif (!cpu_feature_enabled(X86_FEATURE_MPX))\r\nreturn -ENXIO;\r\ndown_write(&mm->mmap_sem);\r\nmm->bd_addr = MPX_INVALID_BOUNDS_DIR;\r\nup_write(&mm->mmap_sem);\r\nreturn 0;\r\n}\r\nstatic int allocate_bt(long __user *bd_entry)\r\n{\r\nunsigned long expected_old_val = 0;\r\nunsigned long actual_old_val = 0;\r\nunsigned long bt_addr;\r\nint ret = 0;\r\nbt_addr = mpx_mmap(MPX_BT_SIZE_BYTES);\r\nif (IS_ERR((void *)bt_addr))\r\nreturn PTR_ERR((void *)bt_addr);\r\nbt_addr = bt_addr | MPX_BD_ENTRY_VALID_FLAG;\r\nret = user_atomic_cmpxchg_inatomic(&actual_old_val, bd_entry,\r\nexpected_old_val, bt_addr);\r\nif (ret)\r\ngoto out_unmap;\r\nif (actual_old_val & MPX_BD_ENTRY_VALID_FLAG) {\r\nret = 0;\r\ngoto out_unmap;\r\n}\r\nif (expected_old_val != actual_old_val) {\r\nret = -EINVAL;\r\ngoto out_unmap;\r\n}\r\nreturn 0;\r\nout_unmap:\r\nvm_munmap(bt_addr & MPX_BT_ADDR_MASK, MPX_BT_SIZE_BYTES);\r\nreturn ret;\r\n}\r\nstatic int do_mpx_bt_fault(struct xsave_struct *xsave_buf)\r\n{\r\nunsigned long bd_entry, bd_base;\r\nstruct bndcsr *bndcsr;\r\nbndcsr = get_xsave_addr(xsave_buf, XSTATE_BNDCSR);\r\nif (!bndcsr)\r\nreturn -EINVAL;\r\nbd_base = bndcsr->bndcfgu & MPX_BNDCFG_ADDR_MASK;\r\nbd_entry = bndcsr->bndstatus & MPX_BNDSTA_ADDR_MASK;\r\nif ((bd_entry < bd_base) ||\r\n(bd_entry >= bd_base + MPX_BD_SIZE_BYTES))\r\nreturn -EINVAL;\r\nreturn allocate_bt((long __user *)bd_entry);\r\n}\r\nint mpx_handle_bd_fault(struct xsave_struct *xsave_buf)\r\n{\r\nif (!kernel_managing_mpx_tables(current->mm))\r\nreturn -EINVAL;\r\nif (do_mpx_bt_fault(xsave_buf)) {\r\nforce_sig(SIGSEGV, current);\r\n}\r\nreturn 0;\r\n}\r\nstatic int mpx_resolve_fault(long __user *addr, int write)\r\n{\r\nlong gup_ret;\r\nint nr_pages = 1;\r\nint force = 0;\r\ngup_ret = get_user_pages(current, current->mm, (unsigned long)addr,\r\nnr_pages, write, force, NULL, NULL);\r\nif (!gup_ret)\r\nreturn -EFAULT;\r\nif (gup_ret < 0)\r\nreturn gup_ret;\r\nreturn 0;\r\n}\r\nstatic int get_bt_addr(struct mm_struct *mm,\r\nlong __user *bd_entry, unsigned long *bt_addr)\r\n{\r\nint ret;\r\nint valid_bit;\r\nif (!access_ok(VERIFY_READ, (bd_entry), sizeof(*bd_entry)))\r\nreturn -EFAULT;\r\nwhile (1) {\r\nint need_write = 0;\r\npagefault_disable();\r\nret = get_user(*bt_addr, bd_entry);\r\npagefault_enable();\r\nif (!ret)\r\nbreak;\r\nif (ret == -EFAULT)\r\nret = mpx_resolve_fault(bd_entry, need_write);\r\nif (ret)\r\nreturn ret;\r\n}\r\nvalid_bit = *bt_addr & MPX_BD_ENTRY_VALID_FLAG;\r\n*bt_addr &= MPX_BT_ADDR_MASK;\r\nif (!valid_bit && *bt_addr)\r\nreturn -EINVAL;\r\nif (!valid_bit)\r\nreturn -ENOENT;\r\nreturn 0;\r\n}\r\nstatic int zap_bt_entries(struct mm_struct *mm,\r\nunsigned long bt_addr,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct vm_area_struct *vma;\r\nunsigned long addr, len;\r\nvma = find_vma(mm, start);\r\nif (!vma || vma->vm_start > start)\r\nreturn -EINVAL;\r\naddr = start;\r\nwhile (vma && vma->vm_start < end) {\r\nif (!is_mpx_vma(vma))\r\nreturn -EINVAL;\r\nlen = min(vma->vm_end, end) - addr;\r\nzap_page_range(vma, addr, len, NULL);\r\nvma = vma->vm_next;\r\naddr = vma->vm_start;\r\n}\r\nreturn 0;\r\n}\r\nstatic int unmap_single_bt(struct mm_struct *mm,\r\nlong __user *bd_entry, unsigned long bt_addr)\r\n{\r\nunsigned long expected_old_val = bt_addr | MPX_BD_ENTRY_VALID_FLAG;\r\nunsigned long actual_old_val = 0;\r\nint ret;\r\nwhile (1) {\r\nint need_write = 1;\r\npagefault_disable();\r\nret = user_atomic_cmpxchg_inatomic(&actual_old_val, bd_entry,\r\nexpected_old_val, 0);\r\npagefault_enable();\r\nif (!ret)\r\nbreak;\r\nif (ret == -EFAULT)\r\nret = mpx_resolve_fault(bd_entry, need_write);\r\nif (ret)\r\nreturn ret;\r\n}\r\nif (actual_old_val != expected_old_val) {\r\nif (!actual_old_val)\r\nreturn 0;\r\nreturn -EINVAL;\r\n}\r\nreturn do_munmap(mm, bt_addr, MPX_BT_SIZE_BYTES);\r\n}\r\nstatic int unmap_shared_bt(struct mm_struct *mm,\r\nlong __user *bd_entry, unsigned long start,\r\nunsigned long end, bool prev_shared, bool next_shared)\r\n{\r\nunsigned long bt_addr;\r\nint ret;\r\nret = get_bt_addr(mm, bd_entry, &bt_addr);\r\nif (ret)\r\nreturn ret;\r\nif (prev_shared && next_shared)\r\nret = zap_bt_entries(mm, bt_addr,\r\nbt_addr+MPX_GET_BT_ENTRY_OFFSET(start),\r\nbt_addr+MPX_GET_BT_ENTRY_OFFSET(end));\r\nelse if (prev_shared)\r\nret = zap_bt_entries(mm, bt_addr,\r\nbt_addr+MPX_GET_BT_ENTRY_OFFSET(start),\r\nbt_addr+MPX_BT_SIZE_BYTES);\r\nelse if (next_shared)\r\nret = zap_bt_entries(mm, bt_addr, bt_addr,\r\nbt_addr+MPX_GET_BT_ENTRY_OFFSET(end));\r\nelse\r\nret = unmap_single_bt(mm, bd_entry, bt_addr);\r\nreturn ret;\r\n}\r\nstatic int unmap_edge_bts(struct mm_struct *mm,\r\nunsigned long start, unsigned long end)\r\n{\r\nint ret;\r\nlong __user *bde_start, *bde_end;\r\nstruct vm_area_struct *prev, *next;\r\nbool prev_shared = false, next_shared = false;\r\nbde_start = mm->bd_addr + MPX_GET_BD_ENTRY_OFFSET(start);\r\nbde_end = mm->bd_addr + MPX_GET_BD_ENTRY_OFFSET(end-1);\r\nnext = find_vma_prev(mm, start, &prev);\r\nif (prev && (mm->bd_addr + MPX_GET_BD_ENTRY_OFFSET(prev->vm_end-1))\r\n== bde_start)\r\nprev_shared = true;\r\nif (next && (mm->bd_addr + MPX_GET_BD_ENTRY_OFFSET(next->vm_start))\r\n== bde_end)\r\nnext_shared = true;\r\nif (bde_start == bde_end) {\r\nreturn unmap_shared_bt(mm, bde_start, start, end,\r\nprev_shared, next_shared);\r\n}\r\nret = unmap_shared_bt(mm, bde_start, start, end, prev_shared, false);\r\nif (ret)\r\nreturn ret;\r\nret = unmap_shared_bt(mm, bde_end, start, end, false, next_shared);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nstatic int mpx_unmap_tables(struct mm_struct *mm,\r\nunsigned long start, unsigned long end)\r\n{\r\nint ret;\r\nlong __user *bd_entry, *bde_start, *bde_end;\r\nunsigned long bt_addr;\r\nret = unmap_edge_bts(mm, start, end);\r\nswitch (ret) {\r\ncase 0:\r\ncase -ENOENT:\r\nbreak;\r\ncase -EINVAL:\r\ncase -EFAULT:\r\ndefault:\r\nreturn ret;\r\n}\r\nbde_start = mm->bd_addr + MPX_GET_BD_ENTRY_OFFSET(start);\r\nbde_end = mm->bd_addr + MPX_GET_BD_ENTRY_OFFSET(end-1);\r\nfor (bd_entry = bde_start + 1; bd_entry < bde_end; bd_entry++) {\r\nret = get_bt_addr(mm, bd_entry, &bt_addr);\r\nswitch (ret) {\r\ncase 0:\r\nbreak;\r\ncase -ENOENT:\r\ncontinue;\r\ncase -EINVAL:\r\ncase -EFAULT:\r\ndefault:\r\nreturn ret;\r\n}\r\nret = unmap_single_bt(mm, bd_entry, bt_addr);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nvoid mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end)\r\n{\r\nint ret;\r\nif (!kernel_managing_mpx_tables(current->mm))\r\nreturn;\r\ndo {\r\nif (vma->vm_flags & VM_MPX)\r\nreturn;\r\nvma = vma->vm_next;\r\n} while (vma && vma->vm_start < end);\r\nret = mpx_unmap_tables(mm, start, end);\r\nif (ret)\r\nforce_sig(SIGSEGV, current);\r\n}
