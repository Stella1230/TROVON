static void verbose(const char *fmt, ...)\r\n{\r\nva_list args;\r\nif (log_level == 0 || log_len >= log_size - 1)\r\nreturn;\r\nva_start(args, fmt);\r\nlog_len += vscnprintf(log_buf + log_len, log_size - log_len, fmt, args);\r\nva_end(args);\r\n}\r\nstatic void print_verifier_state(struct verifier_env *env)\r\n{\r\nenum bpf_reg_type t;\r\nint i;\r\nfor (i = 0; i < MAX_BPF_REG; i++) {\r\nt = env->cur_state.regs[i].type;\r\nif (t == NOT_INIT)\r\ncontinue;\r\nverbose(" R%d=%s", i, reg_type_str[t]);\r\nif (t == CONST_IMM || t == PTR_TO_STACK)\r\nverbose("%d", env->cur_state.regs[i].imm);\r\nelse if (t == CONST_PTR_TO_MAP || t == PTR_TO_MAP_VALUE ||\r\nt == PTR_TO_MAP_VALUE_OR_NULL)\r\nverbose("(ks=%d,vs=%d)",\r\nenv->cur_state.regs[i].map_ptr->key_size,\r\nenv->cur_state.regs[i].map_ptr->value_size);\r\n}\r\nfor (i = 0; i < MAX_BPF_STACK; i += BPF_REG_SIZE) {\r\nif (env->cur_state.stack_slot_type[i] == STACK_SPILL)\r\nverbose(" fp%d=%s", -MAX_BPF_STACK + i,\r\nreg_type_str[env->cur_state.spilled_regs[i / BPF_REG_SIZE].type]);\r\n}\r\nverbose("\n");\r\n}\r\nstatic void print_bpf_insn(struct bpf_insn *insn)\r\n{\r\nu8 class = BPF_CLASS(insn->code);\r\nif (class == BPF_ALU || class == BPF_ALU64) {\r\nif (BPF_SRC(insn->code) == BPF_X)\r\nverbose("(%02x) %sr%d %s %sr%d\n",\r\ninsn->code, class == BPF_ALU ? "(u32) " : "",\r\ninsn->dst_reg,\r\nbpf_alu_string[BPF_OP(insn->code) >> 4],\r\nclass == BPF_ALU ? "(u32) " : "",\r\ninsn->src_reg);\r\nelse\r\nverbose("(%02x) %sr%d %s %s%d\n",\r\ninsn->code, class == BPF_ALU ? "(u32) " : "",\r\ninsn->dst_reg,\r\nbpf_alu_string[BPF_OP(insn->code) >> 4],\r\nclass == BPF_ALU ? "(u32) " : "",\r\ninsn->imm);\r\n} else if (class == BPF_STX) {\r\nif (BPF_MODE(insn->code) == BPF_MEM)\r\nverbose("(%02x) *(%s *)(r%d %+d) = r%d\n",\r\ninsn->code,\r\nbpf_ldst_string[BPF_SIZE(insn->code) >> 3],\r\ninsn->dst_reg,\r\ninsn->off, insn->src_reg);\r\nelse if (BPF_MODE(insn->code) == BPF_XADD)\r\nverbose("(%02x) lock *(%s *)(r%d %+d) += r%d\n",\r\ninsn->code,\r\nbpf_ldst_string[BPF_SIZE(insn->code) >> 3],\r\ninsn->dst_reg, insn->off,\r\ninsn->src_reg);\r\nelse\r\nverbose("BUG_%02x\n", insn->code);\r\n} else if (class == BPF_ST) {\r\nif (BPF_MODE(insn->code) != BPF_MEM) {\r\nverbose("BUG_st_%02x\n", insn->code);\r\nreturn;\r\n}\r\nverbose("(%02x) *(%s *)(r%d %+d) = %d\n",\r\ninsn->code,\r\nbpf_ldst_string[BPF_SIZE(insn->code) >> 3],\r\ninsn->dst_reg,\r\ninsn->off, insn->imm);\r\n} else if (class == BPF_LDX) {\r\nif (BPF_MODE(insn->code) != BPF_MEM) {\r\nverbose("BUG_ldx_%02x\n", insn->code);\r\nreturn;\r\n}\r\nverbose("(%02x) r%d = *(%s *)(r%d %+d)\n",\r\ninsn->code, insn->dst_reg,\r\nbpf_ldst_string[BPF_SIZE(insn->code) >> 3],\r\ninsn->src_reg, insn->off);\r\n} else if (class == BPF_LD) {\r\nif (BPF_MODE(insn->code) == BPF_ABS) {\r\nverbose("(%02x) r0 = *(%s *)skb[%d]\n",\r\ninsn->code,\r\nbpf_ldst_string[BPF_SIZE(insn->code) >> 3],\r\ninsn->imm);\r\n} else if (BPF_MODE(insn->code) == BPF_IND) {\r\nverbose("(%02x) r0 = *(%s *)skb[r%d + %d]\n",\r\ninsn->code,\r\nbpf_ldst_string[BPF_SIZE(insn->code) >> 3],\r\ninsn->src_reg, insn->imm);\r\n} else if (BPF_MODE(insn->code) == BPF_IMM) {\r\nverbose("(%02x) r%d = 0x%x\n",\r\ninsn->code, insn->dst_reg, insn->imm);\r\n} else {\r\nverbose("BUG_ld_%02x\n", insn->code);\r\nreturn;\r\n}\r\n} else if (class == BPF_JMP) {\r\nu8 opcode = BPF_OP(insn->code);\r\nif (opcode == BPF_CALL) {\r\nverbose("(%02x) call %d\n", insn->code, insn->imm);\r\n} else if (insn->code == (BPF_JMP | BPF_JA)) {\r\nverbose("(%02x) goto pc%+d\n",\r\ninsn->code, insn->off);\r\n} else if (insn->code == (BPF_JMP | BPF_EXIT)) {\r\nverbose("(%02x) exit\n", insn->code);\r\n} else if (BPF_SRC(insn->code) == BPF_X) {\r\nverbose("(%02x) if r%d %s r%d goto pc%+d\n",\r\ninsn->code, insn->dst_reg,\r\nbpf_jmp_string[BPF_OP(insn->code) >> 4],\r\ninsn->src_reg, insn->off);\r\n} else {\r\nverbose("(%02x) if r%d %s 0x%x goto pc%+d\n",\r\ninsn->code, insn->dst_reg,\r\nbpf_jmp_string[BPF_OP(insn->code) >> 4],\r\ninsn->imm, insn->off);\r\n}\r\n} else {\r\nverbose("(%02x) %s\n", insn->code, bpf_class_string[class]);\r\n}\r\n}\r\nstatic int pop_stack(struct verifier_env *env, int *prev_insn_idx)\r\n{\r\nstruct verifier_stack_elem *elem;\r\nint insn_idx;\r\nif (env->head == NULL)\r\nreturn -1;\r\nmemcpy(&env->cur_state, &env->head->st, sizeof(env->cur_state));\r\ninsn_idx = env->head->insn_idx;\r\nif (prev_insn_idx)\r\n*prev_insn_idx = env->head->prev_insn_idx;\r\nelem = env->head->next;\r\nkfree(env->head);\r\nenv->head = elem;\r\nenv->stack_size--;\r\nreturn insn_idx;\r\n}\r\nstatic struct verifier_state *push_stack(struct verifier_env *env, int insn_idx,\r\nint prev_insn_idx)\r\n{\r\nstruct verifier_stack_elem *elem;\r\nelem = kmalloc(sizeof(struct verifier_stack_elem), GFP_KERNEL);\r\nif (!elem)\r\ngoto err;\r\nmemcpy(&elem->st, &env->cur_state, sizeof(env->cur_state));\r\nelem->insn_idx = insn_idx;\r\nelem->prev_insn_idx = prev_insn_idx;\r\nelem->next = env->head;\r\nenv->head = elem;\r\nenv->stack_size++;\r\nif (env->stack_size > 1024) {\r\nverbose("BPF program is too complex\n");\r\ngoto err;\r\n}\r\nreturn &elem->st;\r\nerr:\r\nwhile (pop_stack(env, NULL) >= 0);\r\nreturn NULL;\r\n}\r\nstatic void init_reg_state(struct reg_state *regs)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_BPF_REG; i++) {\r\nregs[i].type = NOT_INIT;\r\nregs[i].imm = 0;\r\nregs[i].map_ptr = NULL;\r\n}\r\nregs[BPF_REG_FP].type = FRAME_PTR;\r\nregs[BPF_REG_1].type = PTR_TO_CTX;\r\n}\r\nstatic void mark_reg_unknown_value(struct reg_state *regs, u32 regno)\r\n{\r\nBUG_ON(regno >= MAX_BPF_REG);\r\nregs[regno].type = UNKNOWN_VALUE;\r\nregs[regno].imm = 0;\r\nregs[regno].map_ptr = NULL;\r\n}\r\nstatic int check_reg_arg(struct reg_state *regs, u32 regno,\r\nenum reg_arg_type t)\r\n{\r\nif (regno >= MAX_BPF_REG) {\r\nverbose("R%d is invalid\n", regno);\r\nreturn -EINVAL;\r\n}\r\nif (t == SRC_OP) {\r\nif (regs[regno].type == NOT_INIT) {\r\nverbose("R%d !read_ok\n", regno);\r\nreturn -EACCES;\r\n}\r\n} else {\r\nif (regno == BPF_REG_FP) {\r\nverbose("frame pointer is read only\n");\r\nreturn -EACCES;\r\n}\r\nif (t == DST_OP)\r\nmark_reg_unknown_value(regs, regno);\r\n}\r\nreturn 0;\r\n}\r\nstatic int bpf_size_to_bytes(int bpf_size)\r\n{\r\nif (bpf_size == BPF_W)\r\nreturn 4;\r\nelse if (bpf_size == BPF_H)\r\nreturn 2;\r\nelse if (bpf_size == BPF_B)\r\nreturn 1;\r\nelse if (bpf_size == BPF_DW)\r\nreturn 8;\r\nelse\r\nreturn -EINVAL;\r\n}\r\nstatic int check_stack_write(struct verifier_state *state, int off, int size,\r\nint value_regno)\r\n{\r\nint i;\r\nif (value_regno >= 0 &&\r\n(state->regs[value_regno].type == PTR_TO_MAP_VALUE ||\r\nstate->regs[value_regno].type == PTR_TO_STACK ||\r\nstate->regs[value_regno].type == PTR_TO_CTX)) {\r\nif (size != BPF_REG_SIZE) {\r\nverbose("invalid size of register spill\n");\r\nreturn -EACCES;\r\n}\r\nstate->spilled_regs[(MAX_BPF_STACK + off) / BPF_REG_SIZE] =\r\nstate->regs[value_regno];\r\nfor (i = 0; i < BPF_REG_SIZE; i++)\r\nstate->stack_slot_type[MAX_BPF_STACK + off + i] = STACK_SPILL;\r\n} else {\r\nstate->spilled_regs[(MAX_BPF_STACK + off) / BPF_REG_SIZE] =\r\n(struct reg_state) {};\r\nfor (i = 0; i < size; i++)\r\nstate->stack_slot_type[MAX_BPF_STACK + off + i] = STACK_MISC;\r\n}\r\nreturn 0;\r\n}\r\nstatic int check_stack_read(struct verifier_state *state, int off, int size,\r\nint value_regno)\r\n{\r\nu8 *slot_type;\r\nint i;\r\nslot_type = &state->stack_slot_type[MAX_BPF_STACK + off];\r\nif (slot_type[0] == STACK_SPILL) {\r\nif (size != BPF_REG_SIZE) {\r\nverbose("invalid size of register spill\n");\r\nreturn -EACCES;\r\n}\r\nfor (i = 1; i < BPF_REG_SIZE; i++) {\r\nif (slot_type[i] != STACK_SPILL) {\r\nverbose("corrupted spill memory\n");\r\nreturn -EACCES;\r\n}\r\n}\r\nif (value_regno >= 0)\r\nstate->regs[value_regno] =\r\nstate->spilled_regs[(MAX_BPF_STACK + off) / BPF_REG_SIZE];\r\nreturn 0;\r\n} else {\r\nfor (i = 0; i < size; i++) {\r\nif (slot_type[i] != STACK_MISC) {\r\nverbose("invalid read from stack off %d+%d size %d\n",\r\noff, i, size);\r\nreturn -EACCES;\r\n}\r\n}\r\nif (value_regno >= 0)\r\nmark_reg_unknown_value(state->regs, value_regno);\r\nreturn 0;\r\n}\r\n}\r\nstatic int check_map_access(struct verifier_env *env, u32 regno, int off,\r\nint size)\r\n{\r\nstruct bpf_map *map = env->cur_state.regs[regno].map_ptr;\r\nif (off < 0 || off + size > map->value_size) {\r\nverbose("invalid access to map value, value_size=%d off=%d size=%d\n",\r\nmap->value_size, off, size);\r\nreturn -EACCES;\r\n}\r\nreturn 0;\r\n}\r\nstatic int check_ctx_access(struct verifier_env *env, int off, int size,\r\nenum bpf_access_type t)\r\n{\r\nif (env->prog->aux->ops->is_valid_access &&\r\nenv->prog->aux->ops->is_valid_access(off, size, t))\r\nreturn 0;\r\nverbose("invalid bpf_context access off=%d size=%d\n", off, size);\r\nreturn -EACCES;\r\n}\r\nstatic int check_mem_access(struct verifier_env *env, u32 regno, int off,\r\nint bpf_size, enum bpf_access_type t,\r\nint value_regno)\r\n{\r\nstruct verifier_state *state = &env->cur_state;\r\nint size, err = 0;\r\nsize = bpf_size_to_bytes(bpf_size);\r\nif (size < 0)\r\nreturn size;\r\nif (off % size != 0) {\r\nverbose("misaligned access off %d size %d\n", off, size);\r\nreturn -EACCES;\r\n}\r\nif (state->regs[regno].type == PTR_TO_MAP_VALUE) {\r\nerr = check_map_access(env, regno, off, size);\r\nif (!err && t == BPF_READ && value_regno >= 0)\r\nmark_reg_unknown_value(state->regs, value_regno);\r\n} else if (state->regs[regno].type == PTR_TO_CTX) {\r\nerr = check_ctx_access(env, off, size, t);\r\nif (!err && t == BPF_READ && value_regno >= 0)\r\nmark_reg_unknown_value(state->regs, value_regno);\r\n} else if (state->regs[regno].type == FRAME_PTR) {\r\nif (off >= 0 || off < -MAX_BPF_STACK) {\r\nverbose("invalid stack off=%d size=%d\n", off, size);\r\nreturn -EACCES;\r\n}\r\nif (t == BPF_WRITE)\r\nerr = check_stack_write(state, off, size, value_regno);\r\nelse\r\nerr = check_stack_read(state, off, size, value_regno);\r\n} else {\r\nverbose("R%d invalid mem access '%s'\n",\r\nregno, reg_type_str[state->regs[regno].type]);\r\nreturn -EACCES;\r\n}\r\nreturn err;\r\n}\r\nstatic int check_xadd(struct verifier_env *env, struct bpf_insn *insn)\r\n{\r\nstruct reg_state *regs = env->cur_state.regs;\r\nint err;\r\nif ((BPF_SIZE(insn->code) != BPF_W && BPF_SIZE(insn->code) != BPF_DW) ||\r\ninsn->imm != 0) {\r\nverbose("BPF_XADD uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, insn->src_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nerr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nerr = check_mem_access(env, insn->dst_reg, insn->off,\r\nBPF_SIZE(insn->code), BPF_READ, -1);\r\nif (err)\r\nreturn err;\r\nreturn check_mem_access(env, insn->dst_reg, insn->off,\r\nBPF_SIZE(insn->code), BPF_WRITE, -1);\r\n}\r\nstatic int check_stack_boundary(struct verifier_env *env,\r\nint regno, int access_size)\r\n{\r\nstruct verifier_state *state = &env->cur_state;\r\nstruct reg_state *regs = state->regs;\r\nint off, i;\r\nif (regs[regno].type != PTR_TO_STACK)\r\nreturn -EACCES;\r\noff = regs[regno].imm;\r\nif (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||\r\naccess_size <= 0) {\r\nverbose("invalid stack type R%d off=%d access_size=%d\n",\r\nregno, off, access_size);\r\nreturn -EACCES;\r\n}\r\nfor (i = 0; i < access_size; i++) {\r\nif (state->stack_slot_type[MAX_BPF_STACK + off + i] != STACK_MISC) {\r\nverbose("invalid indirect read from stack off %d+%d size %d\n",\r\noff, i, access_size);\r\nreturn -EACCES;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int check_func_arg(struct verifier_env *env, u32 regno,\r\nenum bpf_arg_type arg_type, struct bpf_map **mapp)\r\n{\r\nstruct reg_state *reg = env->cur_state.regs + regno;\r\nenum bpf_reg_type expected_type;\r\nint err = 0;\r\nif (arg_type == ARG_ANYTHING)\r\nreturn 0;\r\nif (reg->type == NOT_INIT) {\r\nverbose("R%d !read_ok\n", regno);\r\nreturn -EACCES;\r\n}\r\nif (arg_type == ARG_PTR_TO_STACK || arg_type == ARG_PTR_TO_MAP_KEY ||\r\narg_type == ARG_PTR_TO_MAP_VALUE) {\r\nexpected_type = PTR_TO_STACK;\r\n} else if (arg_type == ARG_CONST_STACK_SIZE) {\r\nexpected_type = CONST_IMM;\r\n} else if (arg_type == ARG_CONST_MAP_PTR) {\r\nexpected_type = CONST_PTR_TO_MAP;\r\n} else {\r\nverbose("unsupported arg_type %d\n", arg_type);\r\nreturn -EFAULT;\r\n}\r\nif (reg->type != expected_type) {\r\nverbose("R%d type=%s expected=%s\n", regno,\r\nreg_type_str[reg->type], reg_type_str[expected_type]);\r\nreturn -EACCES;\r\n}\r\nif (arg_type == ARG_CONST_MAP_PTR) {\r\n*mapp = reg->map_ptr;\r\n} else if (arg_type == ARG_PTR_TO_MAP_KEY) {\r\nif (!*mapp) {\r\nverbose("invalid map_ptr to access map->key\n");\r\nreturn -EACCES;\r\n}\r\nerr = check_stack_boundary(env, regno, (*mapp)->key_size);\r\n} else if (arg_type == ARG_PTR_TO_MAP_VALUE) {\r\nif (!*mapp) {\r\nverbose("invalid map_ptr to access map->value\n");\r\nreturn -EACCES;\r\n}\r\nerr = check_stack_boundary(env, regno, (*mapp)->value_size);\r\n} else if (arg_type == ARG_CONST_STACK_SIZE) {\r\nif (regno == 0) {\r\nverbose("ARG_CONST_STACK_SIZE cannot be first argument\n");\r\nreturn -EACCES;\r\n}\r\nerr = check_stack_boundary(env, regno - 1, reg->imm);\r\n}\r\nreturn err;\r\n}\r\nstatic int check_call(struct verifier_env *env, int func_id)\r\n{\r\nstruct verifier_state *state = &env->cur_state;\r\nconst struct bpf_func_proto *fn = NULL;\r\nstruct reg_state *regs = state->regs;\r\nstruct bpf_map *map = NULL;\r\nstruct reg_state *reg;\r\nint i, err;\r\nif (func_id < 0 || func_id >= __BPF_FUNC_MAX_ID) {\r\nverbose("invalid func %d\n", func_id);\r\nreturn -EINVAL;\r\n}\r\nif (env->prog->aux->ops->get_func_proto)\r\nfn = env->prog->aux->ops->get_func_proto(func_id);\r\nif (!fn) {\r\nverbose("unknown func %d\n", func_id);\r\nreturn -EINVAL;\r\n}\r\nif (!env->prog->aux->is_gpl_compatible && fn->gpl_only) {\r\nverbose("cannot call GPL only function from proprietary program\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_func_arg(env, BPF_REG_1, fn->arg1_type, &map);\r\nif (err)\r\nreturn err;\r\nerr = check_func_arg(env, BPF_REG_2, fn->arg2_type, &map);\r\nif (err)\r\nreturn err;\r\nerr = check_func_arg(env, BPF_REG_3, fn->arg3_type, &map);\r\nif (err)\r\nreturn err;\r\nerr = check_func_arg(env, BPF_REG_4, fn->arg4_type, &map);\r\nif (err)\r\nreturn err;\r\nerr = check_func_arg(env, BPF_REG_5, fn->arg5_type, &map);\r\nif (err)\r\nreturn err;\r\nfor (i = 0; i < CALLER_SAVED_REGS; i++) {\r\nreg = regs + caller_saved[i];\r\nreg->type = NOT_INIT;\r\nreg->imm = 0;\r\n}\r\nif (fn->ret_type == RET_INTEGER) {\r\nregs[BPF_REG_0].type = UNKNOWN_VALUE;\r\n} else if (fn->ret_type == RET_VOID) {\r\nregs[BPF_REG_0].type = NOT_INIT;\r\n} else if (fn->ret_type == RET_PTR_TO_MAP_VALUE_OR_NULL) {\r\nregs[BPF_REG_0].type = PTR_TO_MAP_VALUE_OR_NULL;\r\nif (map == NULL) {\r\nverbose("kernel subsystem misconfigured verifier\n");\r\nreturn -EINVAL;\r\n}\r\nregs[BPF_REG_0].map_ptr = map;\r\n} else {\r\nverbose("unknown return type %d of func %d\n",\r\nfn->ret_type, func_id);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int check_alu_op(struct reg_state *regs, struct bpf_insn *insn)\r\n{\r\nu8 opcode = BPF_OP(insn->code);\r\nint err;\r\nif (opcode == BPF_END || opcode == BPF_NEG) {\r\nif (opcode == BPF_NEG) {\r\nif (BPF_SRC(insn->code) != 0 ||\r\ninsn->src_reg != BPF_REG_0 ||\r\ninsn->off != 0 || insn->imm != 0) {\r\nverbose("BPF_NEG uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nif (insn->src_reg != BPF_REG_0 || insn->off != 0 ||\r\n(insn->imm != 16 && insn->imm != 32 && insn->imm != 64)) {\r\nverbose("BPF_END uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nerr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nerr = check_reg_arg(regs, insn->dst_reg, DST_OP);\r\nif (err)\r\nreturn err;\r\n} else if (opcode == BPF_MOV) {\r\nif (BPF_SRC(insn->code) == BPF_X) {\r\nif (insn->imm != 0 || insn->off != 0) {\r\nverbose("BPF_MOV uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, insn->src_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\n} else {\r\nif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\r\nverbose("BPF_MOV uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nerr = check_reg_arg(regs, insn->dst_reg, DST_OP);\r\nif (err)\r\nreturn err;\r\nif (BPF_SRC(insn->code) == BPF_X) {\r\nif (BPF_CLASS(insn->code) == BPF_ALU64) {\r\nregs[insn->dst_reg] = regs[insn->src_reg];\r\n} else {\r\nregs[insn->dst_reg].type = UNKNOWN_VALUE;\r\nregs[insn->dst_reg].map_ptr = NULL;\r\n}\r\n} else {\r\nregs[insn->dst_reg].type = CONST_IMM;\r\nregs[insn->dst_reg].imm = insn->imm;\r\n}\r\n} else if (opcode > BPF_END) {\r\nverbose("invalid BPF_ALU opcode %x\n", opcode);\r\nreturn -EINVAL;\r\n} else {\r\nbool stack_relative = false;\r\nif (BPF_SRC(insn->code) == BPF_X) {\r\nif (insn->imm != 0 || insn->off != 0) {\r\nverbose("BPF_ALU uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, insn->src_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\n} else {\r\nif (insn->src_reg != BPF_REG_0 || insn->off != 0) {\r\nverbose("BPF_ALU uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nerr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nif ((opcode == BPF_MOD || opcode == BPF_DIV) &&\r\nBPF_SRC(insn->code) == BPF_K && insn->imm == 0) {\r\nverbose("div by zero\n");\r\nreturn -EINVAL;\r\n}\r\nif (opcode == BPF_ADD && BPF_CLASS(insn->code) == BPF_ALU64 &&\r\nregs[insn->dst_reg].type == FRAME_PTR &&\r\nBPF_SRC(insn->code) == BPF_K)\r\nstack_relative = true;\r\nerr = check_reg_arg(regs, insn->dst_reg, DST_OP);\r\nif (err)\r\nreturn err;\r\nif (stack_relative) {\r\nregs[insn->dst_reg].type = PTR_TO_STACK;\r\nregs[insn->dst_reg].imm = insn->imm;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int check_cond_jmp_op(struct verifier_env *env,\r\nstruct bpf_insn *insn, int *insn_idx)\r\n{\r\nstruct reg_state *regs = env->cur_state.regs;\r\nstruct verifier_state *other_branch;\r\nu8 opcode = BPF_OP(insn->code);\r\nint err;\r\nif (opcode > BPF_EXIT) {\r\nverbose("invalid BPF_JMP opcode %x\n", opcode);\r\nreturn -EINVAL;\r\n}\r\nif (BPF_SRC(insn->code) == BPF_X) {\r\nif (insn->imm != 0) {\r\nverbose("BPF_JMP uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, insn->src_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\n} else {\r\nif (insn->src_reg != BPF_REG_0) {\r\nverbose("BPF_JMP uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\n}\r\nerr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nif (BPF_SRC(insn->code) == BPF_K &&\r\n(opcode == BPF_JEQ || opcode == BPF_JNE) &&\r\nregs[insn->dst_reg].type == CONST_IMM &&\r\nregs[insn->dst_reg].imm == insn->imm) {\r\nif (opcode == BPF_JEQ) {\r\n*insn_idx += insn->off;\r\nreturn 0;\r\n} else {\r\nreturn 0;\r\n}\r\n}\r\nother_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx);\r\nif (!other_branch)\r\nreturn -EFAULT;\r\nif (BPF_SRC(insn->code) == BPF_K &&\r\ninsn->imm == 0 && (opcode == BPF_JEQ ||\r\nopcode == BPF_JNE) &&\r\nregs[insn->dst_reg].type == PTR_TO_MAP_VALUE_OR_NULL) {\r\nif (opcode == BPF_JEQ) {\r\nregs[insn->dst_reg].type = PTR_TO_MAP_VALUE;\r\nother_branch->regs[insn->dst_reg].type = CONST_IMM;\r\nother_branch->regs[insn->dst_reg].imm = 0;\r\n} else {\r\nother_branch->regs[insn->dst_reg].type = PTR_TO_MAP_VALUE;\r\nregs[insn->dst_reg].type = CONST_IMM;\r\nregs[insn->dst_reg].imm = 0;\r\n}\r\n} else if (BPF_SRC(insn->code) == BPF_K &&\r\n(opcode == BPF_JEQ || opcode == BPF_JNE)) {\r\nif (opcode == BPF_JEQ) {\r\nother_branch->regs[insn->dst_reg].type = CONST_IMM;\r\nother_branch->regs[insn->dst_reg].imm = insn->imm;\r\n} else {\r\nregs[insn->dst_reg].type = CONST_IMM;\r\nregs[insn->dst_reg].imm = insn->imm;\r\n}\r\n}\r\nif (log_level)\r\nprint_verifier_state(env);\r\nreturn 0;\r\n}\r\nstatic struct bpf_map *ld_imm64_to_map_ptr(struct bpf_insn *insn)\r\n{\r\nu64 imm64 = ((u64) (u32) insn[0].imm) | ((u64) (u32) insn[1].imm) << 32;\r\nreturn (struct bpf_map *) (unsigned long) imm64;\r\n}\r\nstatic int check_ld_imm(struct verifier_env *env, struct bpf_insn *insn)\r\n{\r\nstruct reg_state *regs = env->cur_state.regs;\r\nint err;\r\nif (BPF_SIZE(insn->code) != BPF_DW) {\r\nverbose("invalid BPF_LD_IMM insn\n");\r\nreturn -EINVAL;\r\n}\r\nif (insn->off != 0) {\r\nverbose("BPF_LD_IMM64 uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, insn->dst_reg, DST_OP);\r\nif (err)\r\nreturn err;\r\nif (insn->src_reg == 0)\r\nreturn 0;\r\nBUG_ON(insn->src_reg != BPF_PSEUDO_MAP_FD);\r\nregs[insn->dst_reg].type = CONST_PTR_TO_MAP;\r\nregs[insn->dst_reg].map_ptr = ld_imm64_to_map_ptr(insn);\r\nreturn 0;\r\n}\r\nstatic int check_ld_abs(struct verifier_env *env, struct bpf_insn *insn)\r\n{\r\nstruct reg_state *regs = env->cur_state.regs;\r\nu8 mode = BPF_MODE(insn->code);\r\nstruct reg_state *reg;\r\nint i, err;\r\nif (env->prog->aux->prog_type != BPF_PROG_TYPE_SOCKET_FILTER) {\r\nverbose("BPF_LD_ABS|IND instructions are only allowed in socket filters\n");\r\nreturn -EINVAL;\r\n}\r\nif (insn->dst_reg != BPF_REG_0 || insn->off != 0 ||\r\n(mode == BPF_ABS && insn->src_reg != BPF_REG_0)) {\r\nverbose("BPF_LD_ABS uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, BPF_REG_6, SRC_OP);\r\nif (err)\r\nreturn err;\r\nif (regs[BPF_REG_6].type != PTR_TO_CTX) {\r\nverbose("at the time of BPF_LD_ABS|IND R6 != pointer to skb\n");\r\nreturn -EINVAL;\r\n}\r\nif (mode == BPF_IND) {\r\nerr = check_reg_arg(regs, insn->src_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\n}\r\nfor (i = 0; i < CALLER_SAVED_REGS; i++) {\r\nreg = regs + caller_saved[i];\r\nreg->type = NOT_INIT;\r\nreg->imm = 0;\r\n}\r\nregs[BPF_REG_0].type = UNKNOWN_VALUE;\r\nreturn 0;\r\n}\r\nstatic int push_insn(int t, int w, int e, struct verifier_env *env)\r\n{\r\nif (e == FALLTHROUGH && insn_state[t] >= (DISCOVERED | FALLTHROUGH))\r\nreturn 0;\r\nif (e == BRANCH && insn_state[t] >= (DISCOVERED | BRANCH))\r\nreturn 0;\r\nif (w < 0 || w >= env->prog->len) {\r\nverbose("jump out of range from insn %d to %d\n", t, w);\r\nreturn -EINVAL;\r\n}\r\nif (e == BRANCH)\r\nenv->explored_states[w] = STATE_LIST_MARK;\r\nif (insn_state[w] == 0) {\r\ninsn_state[t] = DISCOVERED | e;\r\ninsn_state[w] = DISCOVERED;\r\nif (cur_stack >= env->prog->len)\r\nreturn -E2BIG;\r\ninsn_stack[cur_stack++] = w;\r\nreturn 1;\r\n} else if ((insn_state[w] & 0xF0) == DISCOVERED) {\r\nverbose("back-edge from insn %d to %d\n", t, w);\r\nreturn -EINVAL;\r\n} else if (insn_state[w] == EXPLORED) {\r\ninsn_state[t] = DISCOVERED | e;\r\n} else {\r\nverbose("insn state internal bug\n");\r\nreturn -EFAULT;\r\n}\r\nreturn 0;\r\n}\r\nstatic int check_cfg(struct verifier_env *env)\r\n{\r\nstruct bpf_insn *insns = env->prog->insnsi;\r\nint insn_cnt = env->prog->len;\r\nint ret = 0;\r\nint i, t;\r\ninsn_state = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\r\nif (!insn_state)\r\nreturn -ENOMEM;\r\ninsn_stack = kcalloc(insn_cnt, sizeof(int), GFP_KERNEL);\r\nif (!insn_stack) {\r\nkfree(insn_state);\r\nreturn -ENOMEM;\r\n}\r\ninsn_state[0] = DISCOVERED;\r\ninsn_stack[0] = 0;\r\ncur_stack = 1;\r\npeek_stack:\r\nif (cur_stack == 0)\r\ngoto check_state;\r\nt = insn_stack[cur_stack - 1];\r\nif (BPF_CLASS(insns[t].code) == BPF_JMP) {\r\nu8 opcode = BPF_OP(insns[t].code);\r\nif (opcode == BPF_EXIT) {\r\ngoto mark_explored;\r\n} else if (opcode == BPF_CALL) {\r\nret = push_insn(t, t + 1, FALLTHROUGH, env);\r\nif (ret == 1)\r\ngoto peek_stack;\r\nelse if (ret < 0)\r\ngoto err_free;\r\n} else if (opcode == BPF_JA) {\r\nif (BPF_SRC(insns[t].code) != BPF_K) {\r\nret = -EINVAL;\r\ngoto err_free;\r\n}\r\nret = push_insn(t, t + insns[t].off + 1,\r\nFALLTHROUGH, env);\r\nif (ret == 1)\r\ngoto peek_stack;\r\nelse if (ret < 0)\r\ngoto err_free;\r\nenv->explored_states[t + 1] = STATE_LIST_MARK;\r\n} else {\r\nret = push_insn(t, t + 1, FALLTHROUGH, env);\r\nif (ret == 1)\r\ngoto peek_stack;\r\nelse if (ret < 0)\r\ngoto err_free;\r\nret = push_insn(t, t + insns[t].off + 1, BRANCH, env);\r\nif (ret == 1)\r\ngoto peek_stack;\r\nelse if (ret < 0)\r\ngoto err_free;\r\n}\r\n} else {\r\nret = push_insn(t, t + 1, FALLTHROUGH, env);\r\nif (ret == 1)\r\ngoto peek_stack;\r\nelse if (ret < 0)\r\ngoto err_free;\r\n}\r\nmark_explored:\r\ninsn_state[t] = EXPLORED;\r\nif (cur_stack-- <= 0) {\r\nverbose("pop stack internal bug\n");\r\nret = -EFAULT;\r\ngoto err_free;\r\n}\r\ngoto peek_stack;\r\ncheck_state:\r\nfor (i = 0; i < insn_cnt; i++) {\r\nif (insn_state[i] != EXPLORED) {\r\nverbose("unreachable insn %d\n", i);\r\nret = -EINVAL;\r\ngoto err_free;\r\n}\r\n}\r\nret = 0;\r\nerr_free:\r\nkfree(insn_state);\r\nkfree(insn_stack);\r\nreturn ret;\r\n}\r\nstatic bool states_equal(struct verifier_state *old, struct verifier_state *cur)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_BPF_REG; i++) {\r\nif (memcmp(&old->regs[i], &cur->regs[i],\r\nsizeof(old->regs[0])) != 0) {\r\nif (old->regs[i].type == NOT_INIT ||\r\n(old->regs[i].type == UNKNOWN_VALUE &&\r\ncur->regs[i].type != NOT_INIT))\r\ncontinue;\r\nreturn false;\r\n}\r\n}\r\nfor (i = 0; i < MAX_BPF_STACK; i++) {\r\nif (old->stack_slot_type[i] == STACK_INVALID)\r\ncontinue;\r\nif (old->stack_slot_type[i] != cur->stack_slot_type[i])\r\nreturn false;\r\nif (i % BPF_REG_SIZE)\r\ncontinue;\r\nif (memcmp(&old->spilled_regs[i / BPF_REG_SIZE],\r\n&cur->spilled_regs[i / BPF_REG_SIZE],\r\nsizeof(old->spilled_regs[0])))\r\nreturn false;\r\nelse\r\ncontinue;\r\n}\r\nreturn true;\r\n}\r\nstatic int is_state_visited(struct verifier_env *env, int insn_idx)\r\n{\r\nstruct verifier_state_list *new_sl;\r\nstruct verifier_state_list *sl;\r\nsl = env->explored_states[insn_idx];\r\nif (!sl)\r\nreturn 0;\r\nwhile (sl != STATE_LIST_MARK) {\r\nif (states_equal(&sl->state, &env->cur_state))\r\nreturn 1;\r\nsl = sl->next;\r\n}\r\nnew_sl = kmalloc(sizeof(struct verifier_state_list), GFP_USER);\r\nif (!new_sl)\r\nreturn -ENOMEM;\r\nmemcpy(&new_sl->state, &env->cur_state, sizeof(env->cur_state));\r\nnew_sl->next = env->explored_states[insn_idx];\r\nenv->explored_states[insn_idx] = new_sl;\r\nreturn 0;\r\n}\r\nstatic int do_check(struct verifier_env *env)\r\n{\r\nstruct verifier_state *state = &env->cur_state;\r\nstruct bpf_insn *insns = env->prog->insnsi;\r\nstruct reg_state *regs = state->regs;\r\nint insn_cnt = env->prog->len;\r\nint insn_idx, prev_insn_idx = 0;\r\nint insn_processed = 0;\r\nbool do_print_state = false;\r\ninit_reg_state(regs);\r\ninsn_idx = 0;\r\nfor (;;) {\r\nstruct bpf_insn *insn;\r\nu8 class;\r\nint err;\r\nif (insn_idx >= insn_cnt) {\r\nverbose("invalid insn idx %d insn_cnt %d\n",\r\ninsn_idx, insn_cnt);\r\nreturn -EFAULT;\r\n}\r\ninsn = &insns[insn_idx];\r\nclass = BPF_CLASS(insn->code);\r\nif (++insn_processed > 32768) {\r\nverbose("BPF program is too large. Proccessed %d insn\n",\r\ninsn_processed);\r\nreturn -E2BIG;\r\n}\r\nerr = is_state_visited(env, insn_idx);\r\nif (err < 0)\r\nreturn err;\r\nif (err == 1) {\r\nif (log_level) {\r\nif (do_print_state)\r\nverbose("\nfrom %d to %d: safe\n",\r\nprev_insn_idx, insn_idx);\r\nelse\r\nverbose("%d: safe\n", insn_idx);\r\n}\r\ngoto process_bpf_exit;\r\n}\r\nif (log_level && do_print_state) {\r\nverbose("\nfrom %d to %d:", prev_insn_idx, insn_idx);\r\nprint_verifier_state(env);\r\ndo_print_state = false;\r\n}\r\nif (log_level) {\r\nverbose("%d: ", insn_idx);\r\nprint_bpf_insn(insn);\r\n}\r\nif (class == BPF_ALU || class == BPF_ALU64) {\r\nerr = check_alu_op(regs, insn);\r\nif (err)\r\nreturn err;\r\n} else if (class == BPF_LDX) {\r\nif (BPF_MODE(insn->code) != BPF_MEM ||\r\ninsn->imm != 0) {\r\nverbose("BPF_LDX uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, insn->src_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nerr = check_reg_arg(regs, insn->dst_reg, DST_OP_NO_MARK);\r\nif (err)\r\nreturn err;\r\nerr = check_mem_access(env, insn->src_reg, insn->off,\r\nBPF_SIZE(insn->code), BPF_READ,\r\ninsn->dst_reg);\r\nif (err)\r\nreturn err;\r\n} else if (class == BPF_STX) {\r\nif (BPF_MODE(insn->code) == BPF_XADD) {\r\nerr = check_xadd(env, insn);\r\nif (err)\r\nreturn err;\r\ninsn_idx++;\r\ncontinue;\r\n}\r\nif (BPF_MODE(insn->code) != BPF_MEM ||\r\ninsn->imm != 0) {\r\nverbose("BPF_STX uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, insn->src_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nerr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nerr = check_mem_access(env, insn->dst_reg, insn->off,\r\nBPF_SIZE(insn->code), BPF_WRITE,\r\ninsn->src_reg);\r\nif (err)\r\nreturn err;\r\n} else if (class == BPF_ST) {\r\nif (BPF_MODE(insn->code) != BPF_MEM ||\r\ninsn->src_reg != BPF_REG_0) {\r\nverbose("BPF_ST uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, insn->dst_reg, SRC_OP);\r\nif (err)\r\nreturn err;\r\nerr = check_mem_access(env, insn->dst_reg, insn->off,\r\nBPF_SIZE(insn->code), BPF_WRITE,\r\n-1);\r\nif (err)\r\nreturn err;\r\n} else if (class == BPF_JMP) {\r\nu8 opcode = BPF_OP(insn->code);\r\nif (opcode == BPF_CALL) {\r\nif (BPF_SRC(insn->code) != BPF_K ||\r\ninsn->off != 0 ||\r\ninsn->src_reg != BPF_REG_0 ||\r\ninsn->dst_reg != BPF_REG_0) {\r\nverbose("BPF_CALL uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_call(env, insn->imm);\r\nif (err)\r\nreturn err;\r\n} else if (opcode == BPF_JA) {\r\nif (BPF_SRC(insn->code) != BPF_K ||\r\ninsn->imm != 0 ||\r\ninsn->src_reg != BPF_REG_0 ||\r\ninsn->dst_reg != BPF_REG_0) {\r\nverbose("BPF_JA uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\ninsn_idx += insn->off + 1;\r\ncontinue;\r\n} else if (opcode == BPF_EXIT) {\r\nif (BPF_SRC(insn->code) != BPF_K ||\r\ninsn->imm != 0 ||\r\ninsn->src_reg != BPF_REG_0 ||\r\ninsn->dst_reg != BPF_REG_0) {\r\nverbose("BPF_EXIT uses reserved fields\n");\r\nreturn -EINVAL;\r\n}\r\nerr = check_reg_arg(regs, BPF_REG_0, SRC_OP);\r\nif (err)\r\nreturn err;\r\nprocess_bpf_exit:\r\ninsn_idx = pop_stack(env, &prev_insn_idx);\r\nif (insn_idx < 0) {\r\nbreak;\r\n} else {\r\ndo_print_state = true;\r\ncontinue;\r\n}\r\n} else {\r\nerr = check_cond_jmp_op(env, insn, &insn_idx);\r\nif (err)\r\nreturn err;\r\n}\r\n} else if (class == BPF_LD) {\r\nu8 mode = BPF_MODE(insn->code);\r\nif (mode == BPF_ABS || mode == BPF_IND) {\r\nerr = check_ld_abs(env, insn);\r\nif (err)\r\nreturn err;\r\n} else if (mode == BPF_IMM) {\r\nerr = check_ld_imm(env, insn);\r\nif (err)\r\nreturn err;\r\ninsn_idx++;\r\n} else {\r\nverbose("invalid BPF_LD mode\n");\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nverbose("unknown insn class %d\n", class);\r\nreturn -EINVAL;\r\n}\r\ninsn_idx++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int replace_map_fd_with_map_ptr(struct verifier_env *env)\r\n{\r\nstruct bpf_insn *insn = env->prog->insnsi;\r\nint insn_cnt = env->prog->len;\r\nint i, j;\r\nfor (i = 0; i < insn_cnt; i++, insn++) {\r\nif (insn[0].code == (BPF_LD | BPF_IMM | BPF_DW)) {\r\nstruct bpf_map *map;\r\nstruct fd f;\r\nif (i == insn_cnt - 1 || insn[1].code != 0 ||\r\ninsn[1].dst_reg != 0 || insn[1].src_reg != 0 ||\r\ninsn[1].off != 0) {\r\nverbose("invalid bpf_ld_imm64 insn\n");\r\nreturn -EINVAL;\r\n}\r\nif (insn->src_reg == 0)\r\ngoto next_insn;\r\nif (insn->src_reg != BPF_PSEUDO_MAP_FD) {\r\nverbose("unrecognized bpf_ld_imm64 insn\n");\r\nreturn -EINVAL;\r\n}\r\nf = fdget(insn->imm);\r\nmap = bpf_map_get(f);\r\nif (IS_ERR(map)) {\r\nverbose("fd %d is not pointing to valid bpf_map\n",\r\ninsn->imm);\r\nfdput(f);\r\nreturn PTR_ERR(map);\r\n}\r\ninsn[0].imm = (u32) (unsigned long) map;\r\ninsn[1].imm = ((u64) (unsigned long) map) >> 32;\r\nfor (j = 0; j < env->used_map_cnt; j++)\r\nif (env->used_maps[j] == map) {\r\nfdput(f);\r\ngoto next_insn;\r\n}\r\nif (env->used_map_cnt >= MAX_USED_MAPS) {\r\nfdput(f);\r\nreturn -E2BIG;\r\n}\r\nenv->used_maps[env->used_map_cnt++] = map;\r\natomic_inc(&map->refcnt);\r\nfdput(f);\r\nnext_insn:\r\ninsn++;\r\ni++;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void release_maps(struct verifier_env *env)\r\n{\r\nint i;\r\nfor (i = 0; i < env->used_map_cnt; i++)\r\nbpf_map_put(env->used_maps[i]);\r\n}\r\nstatic void convert_pseudo_ld_imm64(struct verifier_env *env)\r\n{\r\nstruct bpf_insn *insn = env->prog->insnsi;\r\nint insn_cnt = env->prog->len;\r\nint i;\r\nfor (i = 0; i < insn_cnt; i++, insn++)\r\nif (insn->code == (BPF_LD | BPF_IMM | BPF_DW))\r\ninsn->src_reg = 0;\r\n}\r\nstatic void free_states(struct verifier_env *env)\r\n{\r\nstruct verifier_state_list *sl, *sln;\r\nint i;\r\nif (!env->explored_states)\r\nreturn;\r\nfor (i = 0; i < env->prog->len; i++) {\r\nsl = env->explored_states[i];\r\nif (sl)\r\nwhile (sl != STATE_LIST_MARK) {\r\nsln = sl->next;\r\nkfree(sl);\r\nsl = sln;\r\n}\r\n}\r\nkfree(env->explored_states);\r\n}\r\nint bpf_check(struct bpf_prog *prog, union bpf_attr *attr)\r\n{\r\nchar __user *log_ubuf = NULL;\r\nstruct verifier_env *env;\r\nint ret = -EINVAL;\r\nif (prog->len <= 0 || prog->len > BPF_MAXINSNS)\r\nreturn -E2BIG;\r\nenv = kzalloc(sizeof(struct verifier_env), GFP_KERNEL);\r\nif (!env)\r\nreturn -ENOMEM;\r\nenv->prog = prog;\r\nmutex_lock(&bpf_verifier_lock);\r\nif (attr->log_level || attr->log_buf || attr->log_size) {\r\nlog_level = attr->log_level;\r\nlog_ubuf = (char __user *) (unsigned long) attr->log_buf;\r\nlog_size = attr->log_size;\r\nlog_len = 0;\r\nret = -EINVAL;\r\nif (log_size < 128 || log_size > UINT_MAX >> 8 ||\r\nlog_level == 0 || log_ubuf == NULL)\r\ngoto free_env;\r\nret = -ENOMEM;\r\nlog_buf = vmalloc(log_size);\r\nif (!log_buf)\r\ngoto free_env;\r\n} else {\r\nlog_level = 0;\r\n}\r\nret = replace_map_fd_with_map_ptr(env);\r\nif (ret < 0)\r\ngoto skip_full_check;\r\nenv->explored_states = kcalloc(prog->len,\r\nsizeof(struct verifier_state_list *),\r\nGFP_USER);\r\nret = -ENOMEM;\r\nif (!env->explored_states)\r\ngoto skip_full_check;\r\nret = check_cfg(env);\r\nif (ret < 0)\r\ngoto skip_full_check;\r\nret = do_check(env);\r\nskip_full_check:\r\nwhile (pop_stack(env, NULL) >= 0);\r\nfree_states(env);\r\nif (log_level && log_len >= log_size - 1) {\r\nBUG_ON(log_len >= log_size);\r\nret = -ENOSPC;\r\n}\r\nif (log_level && copy_to_user(log_ubuf, log_buf, log_len + 1) != 0) {\r\nret = -EFAULT;\r\ngoto free_log_buf;\r\n}\r\nif (ret == 0 && env->used_map_cnt) {\r\nprog->aux->used_maps = kmalloc_array(env->used_map_cnt,\r\nsizeof(env->used_maps[0]),\r\nGFP_KERNEL);\r\nif (!prog->aux->used_maps) {\r\nret = -ENOMEM;\r\ngoto free_log_buf;\r\n}\r\nmemcpy(prog->aux->used_maps, env->used_maps,\r\nsizeof(env->used_maps[0]) * env->used_map_cnt);\r\nprog->aux->used_map_cnt = env->used_map_cnt;\r\nconvert_pseudo_ld_imm64(env);\r\n}\r\nfree_log_buf:\r\nif (log_level)\r\nvfree(log_buf);\r\nfree_env:\r\nif (!prog->aux->used_maps)\r\nrelease_maps(env);\r\nkfree(env);\r\nmutex_unlock(&bpf_verifier_lock);\r\nreturn ret;\r\n}
