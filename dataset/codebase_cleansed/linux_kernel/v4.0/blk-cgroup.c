static bool blkcg_policy_enabled(struct request_queue *q,\r\nconst struct blkcg_policy *pol)\r\n{\r\nreturn pol && test_bit(pol->plid, q->blkcg_pols);\r\n}\r\nstatic void blkg_free(struct blkcg_gq *blkg)\r\n{\r\nint i;\r\nif (!blkg)\r\nreturn;\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++)\r\nkfree(blkg->pd[i]);\r\nblk_exit_rl(&blkg->rl);\r\nkfree(blkg);\r\n}\r\nstatic struct blkcg_gq *blkg_alloc(struct blkcg *blkcg, struct request_queue *q,\r\ngfp_t gfp_mask)\r\n{\r\nstruct blkcg_gq *blkg;\r\nint i;\r\nblkg = kzalloc_node(sizeof(*blkg), gfp_mask, q->node);\r\nif (!blkg)\r\nreturn NULL;\r\nblkg->q = q;\r\nINIT_LIST_HEAD(&blkg->q_node);\r\nblkg->blkcg = blkcg;\r\natomic_set(&blkg->refcnt, 1);\r\nif (blkcg != &blkcg_root) {\r\nif (blk_init_rl(&blkg->rl, q, gfp_mask))\r\ngoto err_free;\r\nblkg->rl.blkg = blkg;\r\n}\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nstruct blkg_policy_data *pd;\r\nif (!blkcg_policy_enabled(q, pol))\r\ncontinue;\r\npd = kzalloc_node(pol->pd_size, gfp_mask, q->node);\r\nif (!pd)\r\ngoto err_free;\r\nblkg->pd[i] = pd;\r\npd->blkg = blkg;\r\npd->plid = i;\r\n}\r\nreturn blkg;\r\nerr_free:\r\nblkg_free(blkg);\r\nreturn NULL;\r\n}\r\nstruct blkcg_gq *__blkg_lookup(struct blkcg *blkcg, struct request_queue *q,\r\nbool update_hint)\r\n{\r\nstruct blkcg_gq *blkg;\r\nblkg = rcu_dereference(blkcg->blkg_hint);\r\nif (blkg && blkg->q == q)\r\nreturn blkg;\r\nblkg = radix_tree_lookup(&blkcg->blkg_tree, q->id);\r\nif (blkg && blkg->q == q) {\r\nif (update_hint) {\r\nlockdep_assert_held(q->queue_lock);\r\nrcu_assign_pointer(blkcg->blkg_hint, blkg);\r\n}\r\nreturn blkg;\r\n}\r\nreturn NULL;\r\n}\r\nstruct blkcg_gq *blkg_lookup(struct blkcg *blkcg, struct request_queue *q)\r\n{\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nif (unlikely(blk_queue_bypass(q)))\r\nreturn NULL;\r\nreturn __blkg_lookup(blkcg, q, false);\r\n}\r\nstatic struct blkcg_gq *blkg_create(struct blkcg *blkcg,\r\nstruct request_queue *q,\r\nstruct blkcg_gq *new_blkg)\r\n{\r\nstruct blkcg_gq *blkg;\r\nint i, ret;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nlockdep_assert_held(q->queue_lock);\r\nif (!css_tryget_online(&blkcg->css)) {\r\nret = -EINVAL;\r\ngoto err_free_blkg;\r\n}\r\nif (!new_blkg) {\r\nnew_blkg = blkg_alloc(blkcg, q, GFP_ATOMIC);\r\nif (unlikely(!new_blkg)) {\r\nret = -ENOMEM;\r\ngoto err_put_css;\r\n}\r\n}\r\nblkg = new_blkg;\r\nif (blkcg_parent(blkcg)) {\r\nblkg->parent = __blkg_lookup(blkcg_parent(blkcg), q, false);\r\nif (WARN_ON_ONCE(!blkg->parent)) {\r\nret = -EINVAL;\r\ngoto err_put_css;\r\n}\r\nblkg_get(blkg->parent);\r\n}\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkg->pd[i] && pol->pd_init_fn)\r\npol->pd_init_fn(blkg);\r\n}\r\nspin_lock(&blkcg->lock);\r\nret = radix_tree_insert(&blkcg->blkg_tree, q->id, blkg);\r\nif (likely(!ret)) {\r\nhlist_add_head_rcu(&blkg->blkcg_node, &blkcg->blkg_list);\r\nlist_add(&blkg->q_node, &q->blkg_list);\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkg->pd[i] && pol->pd_online_fn)\r\npol->pd_online_fn(blkg);\r\n}\r\n}\r\nblkg->online = true;\r\nspin_unlock(&blkcg->lock);\r\nif (!ret) {\r\nif (blkcg == &blkcg_root) {\r\nq->root_blkg = blkg;\r\nq->root_rl.blkg = blkg;\r\n}\r\nreturn blkg;\r\n}\r\nblkg_put(blkg);\r\nreturn ERR_PTR(ret);\r\nerr_put_css:\r\ncss_put(&blkcg->css);\r\nerr_free_blkg:\r\nblkg_free(new_blkg);\r\nreturn ERR_PTR(ret);\r\n}\r\nstruct blkcg_gq *blkg_lookup_create(struct blkcg *blkcg,\r\nstruct request_queue *q)\r\n{\r\nstruct blkcg_gq *blkg;\r\nWARN_ON_ONCE(!rcu_read_lock_held());\r\nlockdep_assert_held(q->queue_lock);\r\nif (unlikely(blk_queue_bypass(q)))\r\nreturn ERR_PTR(blk_queue_dying(q) ? -EINVAL : -EBUSY);\r\nblkg = __blkg_lookup(blkcg, q, true);\r\nif (blkg)\r\nreturn blkg;\r\nwhile (true) {\r\nstruct blkcg *pos = blkcg;\r\nstruct blkcg *parent = blkcg_parent(blkcg);\r\nwhile (parent && !__blkg_lookup(parent, q, false)) {\r\npos = parent;\r\nparent = blkcg_parent(parent);\r\n}\r\nblkg = blkg_create(pos, q, NULL);\r\nif (pos == blkcg || IS_ERR(blkg))\r\nreturn blkg;\r\n}\r\n}\r\nstatic void blkg_destroy(struct blkcg_gq *blkg)\r\n{\r\nstruct blkcg *blkcg = blkg->blkcg;\r\nint i;\r\nlockdep_assert_held(blkg->q->queue_lock);\r\nlockdep_assert_held(&blkcg->lock);\r\nWARN_ON_ONCE(list_empty(&blkg->q_node));\r\nWARN_ON_ONCE(hlist_unhashed(&blkg->blkcg_node));\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkg->pd[i] && pol->pd_offline_fn)\r\npol->pd_offline_fn(blkg);\r\n}\r\nblkg->online = false;\r\nradix_tree_delete(&blkcg->blkg_tree, blkg->q->id);\r\nlist_del_init(&blkg->q_node);\r\nhlist_del_init_rcu(&blkg->blkcg_node);\r\nif (rcu_access_pointer(blkcg->blkg_hint) == blkg)\r\nrcu_assign_pointer(blkcg->blkg_hint, NULL);\r\nif (blkcg == &blkcg_root) {\r\nblkg->q->root_blkg = NULL;\r\nblkg->q->root_rl.blkg = NULL;\r\n}\r\nblkg_put(blkg);\r\n}\r\nstatic void blkg_destroy_all(struct request_queue *q)\r\n{\r\nstruct blkcg_gq *blkg, *n;\r\nlockdep_assert_held(q->queue_lock);\r\nlist_for_each_entry_safe(blkg, n, &q->blkg_list, q_node) {\r\nstruct blkcg *blkcg = blkg->blkcg;\r\nspin_lock(&blkcg->lock);\r\nblkg_destroy(blkg);\r\nspin_unlock(&blkcg->lock);\r\n}\r\n}\r\nvoid __blkg_release_rcu(struct rcu_head *rcu_head)\r\n{\r\nstruct blkcg_gq *blkg = container_of(rcu_head, struct blkcg_gq, rcu_head);\r\nint i;\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkg->pd[i] && pol->pd_exit_fn)\r\npol->pd_exit_fn(blkg);\r\n}\r\ncss_put(&blkg->blkcg->css);\r\nif (blkg->parent)\r\nblkg_put(blkg->parent);\r\nblkg_free(blkg);\r\n}\r\nstruct request_list *__blk_queue_next_rl(struct request_list *rl,\r\nstruct request_queue *q)\r\n{\r\nstruct list_head *ent;\r\nstruct blkcg_gq *blkg;\r\nif (rl == &q->root_rl) {\r\nent = &q->blkg_list;\r\nif (list_empty(ent))\r\nreturn NULL;\r\n} else {\r\nblkg = container_of(rl, struct blkcg_gq, rl);\r\nent = &blkg->q_node;\r\n}\r\nent = ent->next;\r\nif (ent == &q->root_blkg->q_node)\r\nent = ent->next;\r\nif (ent == &q->blkg_list)\r\nreturn NULL;\r\nblkg = container_of(ent, struct blkcg_gq, q_node);\r\nreturn &blkg->rl;\r\n}\r\nstatic int blkcg_reset_stats(struct cgroup_subsys_state *css,\r\nstruct cftype *cftype, u64 val)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(css);\r\nstruct blkcg_gq *blkg;\r\nint i;\r\nif (!mutex_trylock(&blkcg_pol_mutex))\r\nreturn restart_syscall();\r\nspin_lock_irq(&blkcg->lock);\r\nhlist_for_each_entry(blkg, &blkcg->blkg_list, blkcg_node) {\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++) {\r\nstruct blkcg_policy *pol = blkcg_policy[i];\r\nif (blkcg_policy_enabled(blkg->q, pol) &&\r\npol->pd_reset_stats_fn)\r\npol->pd_reset_stats_fn(blkg);\r\n}\r\n}\r\nspin_unlock_irq(&blkcg->lock);\r\nmutex_unlock(&blkcg_pol_mutex);\r\nreturn 0;\r\n}\r\nstatic const char *blkg_dev_name(struct blkcg_gq *blkg)\r\n{\r\nif (blkg->q->backing_dev_info.dev)\r\nreturn dev_name(blkg->q->backing_dev_info.dev);\r\nreturn NULL;\r\n}\r\nvoid blkcg_print_blkgs(struct seq_file *sf, struct blkcg *blkcg,\r\nu64 (*prfill)(struct seq_file *,\r\nstruct blkg_policy_data *, int),\r\nconst struct blkcg_policy *pol, int data,\r\nbool show_total)\r\n{\r\nstruct blkcg_gq *blkg;\r\nu64 total = 0;\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(blkg, &blkcg->blkg_list, blkcg_node) {\r\nspin_lock_irq(blkg->q->queue_lock);\r\nif (blkcg_policy_enabled(blkg->q, pol))\r\ntotal += prfill(sf, blkg->pd[pol->plid], data);\r\nspin_unlock_irq(blkg->q->queue_lock);\r\n}\r\nrcu_read_unlock();\r\nif (show_total)\r\nseq_printf(sf, "Total %llu\n", (unsigned long long)total);\r\n}\r\nu64 __blkg_prfill_u64(struct seq_file *sf, struct blkg_policy_data *pd, u64 v)\r\n{\r\nconst char *dname = blkg_dev_name(pd->blkg);\r\nif (!dname)\r\nreturn 0;\r\nseq_printf(sf, "%s %llu\n", dname, (unsigned long long)v);\r\nreturn v;\r\n}\r\nu64 __blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,\r\nconst struct blkg_rwstat *rwstat)\r\n{\r\nstatic const char *rwstr[] = {\r\n[BLKG_RWSTAT_READ] = "Read",\r\n[BLKG_RWSTAT_WRITE] = "Write",\r\n[BLKG_RWSTAT_SYNC] = "Sync",\r\n[BLKG_RWSTAT_ASYNC] = "Async",\r\n};\r\nconst char *dname = blkg_dev_name(pd->blkg);\r\nu64 v;\r\nint i;\r\nif (!dname)\r\nreturn 0;\r\nfor (i = 0; i < BLKG_RWSTAT_NR; i++)\r\nseq_printf(sf, "%s %s %llu\n", dname, rwstr[i],\r\n(unsigned long long)rwstat->cnt[i]);\r\nv = rwstat->cnt[BLKG_RWSTAT_READ] + rwstat->cnt[BLKG_RWSTAT_WRITE];\r\nseq_printf(sf, "%s Total %llu\n", dname, (unsigned long long)v);\r\nreturn v;\r\n}\r\nu64 blkg_prfill_stat(struct seq_file *sf, struct blkg_policy_data *pd, int off)\r\n{\r\nreturn __blkg_prfill_u64(sf, pd, blkg_stat_read((void *)pd + off));\r\n}\r\nu64 blkg_prfill_rwstat(struct seq_file *sf, struct blkg_policy_data *pd,\r\nint off)\r\n{\r\nstruct blkg_rwstat rwstat = blkg_rwstat_read((void *)pd + off);\r\nreturn __blkg_prfill_rwstat(sf, pd, &rwstat);\r\n}\r\nu64 blkg_stat_recursive_sum(struct blkg_policy_data *pd, int off)\r\n{\r\nstruct blkcg_policy *pol = blkcg_policy[pd->plid];\r\nstruct blkcg_gq *pos_blkg;\r\nstruct cgroup_subsys_state *pos_css;\r\nu64 sum = 0;\r\nlockdep_assert_held(pd->blkg->q->queue_lock);\r\nrcu_read_lock();\r\nblkg_for_each_descendant_pre(pos_blkg, pos_css, pd_to_blkg(pd)) {\r\nstruct blkg_policy_data *pos_pd = blkg_to_pd(pos_blkg, pol);\r\nstruct blkg_stat *stat = (void *)pos_pd + off;\r\nif (pos_blkg->online)\r\nsum += blkg_stat_read(stat);\r\n}\r\nrcu_read_unlock();\r\nreturn sum;\r\n}\r\nstruct blkg_rwstat blkg_rwstat_recursive_sum(struct blkg_policy_data *pd,\r\nint off)\r\n{\r\nstruct blkcg_policy *pol = blkcg_policy[pd->plid];\r\nstruct blkcg_gq *pos_blkg;\r\nstruct cgroup_subsys_state *pos_css;\r\nstruct blkg_rwstat sum = { };\r\nint i;\r\nlockdep_assert_held(pd->blkg->q->queue_lock);\r\nrcu_read_lock();\r\nblkg_for_each_descendant_pre(pos_blkg, pos_css, pd_to_blkg(pd)) {\r\nstruct blkg_policy_data *pos_pd = blkg_to_pd(pos_blkg, pol);\r\nstruct blkg_rwstat *rwstat = (void *)pos_pd + off;\r\nstruct blkg_rwstat tmp;\r\nif (!pos_blkg->online)\r\ncontinue;\r\ntmp = blkg_rwstat_read(rwstat);\r\nfor (i = 0; i < BLKG_RWSTAT_NR; i++)\r\nsum.cnt[i] += tmp.cnt[i];\r\n}\r\nrcu_read_unlock();\r\nreturn sum;\r\n}\r\nint blkg_conf_prep(struct blkcg *blkcg, const struct blkcg_policy *pol,\r\nconst char *input, struct blkg_conf_ctx *ctx)\r\n__acquires(rcu) __acquires(disk->queue->queue_lock)\r\n{\r\nstruct gendisk *disk;\r\nstruct blkcg_gq *blkg;\r\nunsigned int major, minor;\r\nunsigned long long v;\r\nint part, ret;\r\nif (sscanf(input, "%u:%u %llu", &major, &minor, &v) != 3)\r\nreturn -EINVAL;\r\ndisk = get_gendisk(MKDEV(major, minor), &part);\r\nif (!disk || part)\r\nreturn -EINVAL;\r\nrcu_read_lock();\r\nspin_lock_irq(disk->queue->queue_lock);\r\nif (blkcg_policy_enabled(disk->queue, pol))\r\nblkg = blkg_lookup_create(blkcg, disk->queue);\r\nelse\r\nblkg = ERR_PTR(-EINVAL);\r\nif (IS_ERR(blkg)) {\r\nret = PTR_ERR(blkg);\r\nrcu_read_unlock();\r\nspin_unlock_irq(disk->queue->queue_lock);\r\nput_disk(disk);\r\nif (ret == -EBUSY) {\r\nmsleep(10);\r\nret = restart_syscall();\r\n}\r\nreturn ret;\r\n}\r\nctx->disk = disk;\r\nctx->blkg = blkg;\r\nctx->v = v;\r\nreturn 0;\r\n}\r\nvoid blkg_conf_finish(struct blkg_conf_ctx *ctx)\r\n__releases(ctx->disk->queue->queue_lock) __releases(rcu)\r\n{\r\nspin_unlock_irq(ctx->disk->queue->queue_lock);\r\nrcu_read_unlock();\r\nput_disk(ctx->disk);\r\n}\r\nstatic void blkcg_css_offline(struct cgroup_subsys_state *css)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(css);\r\nspin_lock_irq(&blkcg->lock);\r\nwhile (!hlist_empty(&blkcg->blkg_list)) {\r\nstruct blkcg_gq *blkg = hlist_entry(blkcg->blkg_list.first,\r\nstruct blkcg_gq, blkcg_node);\r\nstruct request_queue *q = blkg->q;\r\nif (spin_trylock(q->queue_lock)) {\r\nblkg_destroy(blkg);\r\nspin_unlock(q->queue_lock);\r\n} else {\r\nspin_unlock_irq(&blkcg->lock);\r\ncpu_relax();\r\nspin_lock_irq(&blkcg->lock);\r\n}\r\n}\r\nspin_unlock_irq(&blkcg->lock);\r\n}\r\nstatic void blkcg_css_free(struct cgroup_subsys_state *css)\r\n{\r\nstruct blkcg *blkcg = css_to_blkcg(css);\r\nif (blkcg != &blkcg_root)\r\nkfree(blkcg);\r\n}\r\nstatic struct cgroup_subsys_state *\r\nblkcg_css_alloc(struct cgroup_subsys_state *parent_css)\r\n{\r\nstruct blkcg *blkcg;\r\nif (!parent_css) {\r\nblkcg = &blkcg_root;\r\ngoto done;\r\n}\r\nblkcg = kzalloc(sizeof(*blkcg), GFP_KERNEL);\r\nif (!blkcg)\r\nreturn ERR_PTR(-ENOMEM);\r\nblkcg->cfq_weight = CFQ_WEIGHT_DEFAULT;\r\nblkcg->cfq_leaf_weight = CFQ_WEIGHT_DEFAULT;\r\ndone:\r\nspin_lock_init(&blkcg->lock);\r\nINIT_RADIX_TREE(&blkcg->blkg_tree, GFP_ATOMIC);\r\nINIT_HLIST_HEAD(&blkcg->blkg_list);\r\nreturn &blkcg->css;\r\n}\r\nint blkcg_init_queue(struct request_queue *q)\r\n{\r\nmight_sleep();\r\nreturn blk_throtl_init(q);\r\n}\r\nvoid blkcg_drain_queue(struct request_queue *q)\r\n{\r\nlockdep_assert_held(q->queue_lock);\r\nif (!q->root_blkg)\r\nreturn;\r\nblk_throtl_drain(q);\r\n}\r\nvoid blkcg_exit_queue(struct request_queue *q)\r\n{\r\nspin_lock_irq(q->queue_lock);\r\nblkg_destroy_all(q);\r\nspin_unlock_irq(q->queue_lock);\r\nblk_throtl_exit(q);\r\n}\r\nstatic int blkcg_can_attach(struct cgroup_subsys_state *css,\r\nstruct cgroup_taskset *tset)\r\n{\r\nstruct task_struct *task;\r\nstruct io_context *ioc;\r\nint ret = 0;\r\ncgroup_taskset_for_each(task, tset) {\r\ntask_lock(task);\r\nioc = task->io_context;\r\nif (ioc && atomic_read(&ioc->nr_tasks) > 1)\r\nret = -EINVAL;\r\ntask_unlock(task);\r\nif (ret)\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nint blkcg_activate_policy(struct request_queue *q,\r\nconst struct blkcg_policy *pol)\r\n{\r\nLIST_HEAD(pds);\r\nstruct blkcg_gq *blkg, *new_blkg;\r\nstruct blkg_policy_data *pd, *n;\r\nint cnt = 0, ret;\r\nbool preloaded;\r\nif (blkcg_policy_enabled(q, pol))\r\nreturn 0;\r\nnew_blkg = blkg_alloc(&blkcg_root, q, GFP_KERNEL);\r\nif (!new_blkg)\r\nreturn -ENOMEM;\r\nblk_queue_bypass_start(q);\r\npreloaded = !radix_tree_preload(GFP_KERNEL);\r\nspin_lock_irq(q->queue_lock);\r\nrcu_read_lock();\r\nblkg = __blkg_lookup(&blkcg_root, q, false);\r\nif (blkg)\r\nblkg_free(new_blkg);\r\nelse\r\nblkg = blkg_create(&blkcg_root, q, new_blkg);\r\nrcu_read_unlock();\r\nif (preloaded)\r\nradix_tree_preload_end();\r\nif (IS_ERR(blkg)) {\r\nret = PTR_ERR(blkg);\r\ngoto out_unlock;\r\n}\r\nlist_for_each_entry(blkg, &q->blkg_list, q_node)\r\ncnt++;\r\nspin_unlock_irq(q->queue_lock);\r\nwhile (cnt--) {\r\npd = kzalloc_node(pol->pd_size, GFP_KERNEL, q->node);\r\nif (!pd) {\r\nret = -ENOMEM;\r\ngoto out_free;\r\n}\r\nlist_add_tail(&pd->alloc_node, &pds);\r\n}\r\nspin_lock_irq(q->queue_lock);\r\nlist_for_each_entry(blkg, &q->blkg_list, q_node) {\r\nif (WARN_ON(list_empty(&pds))) {\r\nret = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\npd = list_first_entry(&pds, struct blkg_policy_data, alloc_node);\r\nlist_del_init(&pd->alloc_node);\r\nspin_lock(&blkg->blkcg->lock);\r\nblkg->pd[pol->plid] = pd;\r\npd->blkg = blkg;\r\npd->plid = pol->plid;\r\npol->pd_init_fn(blkg);\r\nspin_unlock(&blkg->blkcg->lock);\r\n}\r\n__set_bit(pol->plid, q->blkcg_pols);\r\nret = 0;\r\nout_unlock:\r\nspin_unlock_irq(q->queue_lock);\r\nout_free:\r\nblk_queue_bypass_end(q);\r\nlist_for_each_entry_safe(pd, n, &pds, alloc_node)\r\nkfree(pd);\r\nreturn ret;\r\n}\r\nvoid blkcg_deactivate_policy(struct request_queue *q,\r\nconst struct blkcg_policy *pol)\r\n{\r\nstruct blkcg_gq *blkg;\r\nif (!blkcg_policy_enabled(q, pol))\r\nreturn;\r\nblk_queue_bypass_start(q);\r\nspin_lock_irq(q->queue_lock);\r\n__clear_bit(pol->plid, q->blkcg_pols);\r\nif (bitmap_empty(q->blkcg_pols, BLKCG_MAX_POLS))\r\nblkg_destroy_all(q);\r\nlist_for_each_entry(blkg, &q->blkg_list, q_node) {\r\nspin_lock(&blkg->blkcg->lock);\r\nif (pol->pd_offline_fn)\r\npol->pd_offline_fn(blkg);\r\nif (pol->pd_exit_fn)\r\npol->pd_exit_fn(blkg);\r\nkfree(blkg->pd[pol->plid]);\r\nblkg->pd[pol->plid] = NULL;\r\nspin_unlock(&blkg->blkcg->lock);\r\n}\r\nspin_unlock_irq(q->queue_lock);\r\nblk_queue_bypass_end(q);\r\n}\r\nint blkcg_policy_register(struct blkcg_policy *pol)\r\n{\r\nint i, ret;\r\nif (WARN_ON(pol->pd_size < sizeof(struct blkg_policy_data)))\r\nreturn -EINVAL;\r\nmutex_lock(&blkcg_pol_mutex);\r\nret = -ENOSPC;\r\nfor (i = 0; i < BLKCG_MAX_POLS; i++)\r\nif (!blkcg_policy[i])\r\nbreak;\r\nif (i >= BLKCG_MAX_POLS)\r\ngoto out_unlock;\r\npol->plid = i;\r\nblkcg_policy[i] = pol;\r\nif (pol->cftypes)\r\nWARN_ON(cgroup_add_legacy_cftypes(&blkio_cgrp_subsys,\r\npol->cftypes));\r\nret = 0;\r\nout_unlock:\r\nmutex_unlock(&blkcg_pol_mutex);\r\nreturn ret;\r\n}\r\nvoid blkcg_policy_unregister(struct blkcg_policy *pol)\r\n{\r\nmutex_lock(&blkcg_pol_mutex);\r\nif (WARN_ON(blkcg_policy[pol->plid] != pol))\r\ngoto out_unlock;\r\nif (pol->cftypes)\r\ncgroup_rm_cftypes(pol->cftypes);\r\nblkcg_policy[pol->plid] = NULL;\r\nout_unlock:\r\nmutex_unlock(&blkcg_pol_mutex);\r\n}
