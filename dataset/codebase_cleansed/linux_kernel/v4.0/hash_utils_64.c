static unsigned long htab_convert_pte_flags(unsigned long pteflags)\r\n{\r\nunsigned long rflags = pteflags & 0x1fa;\r\nif ((pteflags & _PAGE_EXEC) == 0)\r\nrflags |= HPTE_R_N;\r\nif ((pteflags & _PAGE_USER) && !((pteflags & _PAGE_RW) &&\r\n(pteflags & _PAGE_DIRTY)))\r\nrflags |= 1;\r\nreturn rflags | HPTE_R_C | HPTE_R_M;\r\n}\r\nint htab_bolt_mapping(unsigned long vstart, unsigned long vend,\r\nunsigned long pstart, unsigned long prot,\r\nint psize, int ssize)\r\n{\r\nunsigned long vaddr, paddr;\r\nunsigned int step, shift;\r\nint ret = 0;\r\nshift = mmu_psize_defs[psize].shift;\r\nstep = 1 << shift;\r\nprot = htab_convert_pte_flags(prot);\r\nDBG("htab_bolt_mapping(%lx..%lx -> %lx (%lx,%d,%d)\n",\r\nvstart, vend, pstart, prot, psize, ssize);\r\nfor (vaddr = vstart, paddr = pstart; vaddr < vend;\r\nvaddr += step, paddr += step) {\r\nunsigned long hash, hpteg;\r\nunsigned long vsid = get_kernel_vsid(vaddr, ssize);\r\nunsigned long vpn = hpt_vpn(vaddr, vsid, ssize);\r\nunsigned long tprot = prot;\r\nif (!vsid)\r\nreturn -1;\r\nif (overlaps_kernel_text(vaddr, vaddr + step))\r\ntprot &= ~HPTE_R_N;\r\nif (overlaps_kvm_tmp(vaddr, vaddr + step))\r\ntprot &= ~HPTE_R_N;\r\nif ((PHYSICAL_START > MEMORY_START) &&\r\noverlaps_interrupt_vector_text(vaddr, vaddr + step))\r\ntprot &= ~HPTE_R_N;\r\nhash = hpt_hash(vpn, shift, ssize);\r\nhpteg = ((hash & htab_hash_mask) * HPTES_PER_GROUP);\r\nBUG_ON(!ppc_md.hpte_insert);\r\nret = ppc_md.hpte_insert(hpteg, vpn, paddr, tprot,\r\nHPTE_V_BOLTED, psize, psize, ssize);\r\nif (ret < 0)\r\nbreak;\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nif ((paddr >> PAGE_SHIFT) < linear_map_hash_count)\r\nlinear_map_hash_slots[paddr >> PAGE_SHIFT] = ret | 0x80;\r\n#endif\r\n}\r\nreturn ret < 0 ? ret : 0;\r\n}\r\nint htab_remove_mapping(unsigned long vstart, unsigned long vend,\r\nint psize, int ssize)\r\n{\r\nunsigned long vaddr;\r\nunsigned int step, shift;\r\nshift = mmu_psize_defs[psize].shift;\r\nstep = 1 << shift;\r\nif (!ppc_md.hpte_removebolted) {\r\nprintk(KERN_WARNING "Platform doesn't implement "\r\n"hpte_removebolted\n");\r\nreturn -EINVAL;\r\n}\r\nfor (vaddr = vstart; vaddr < vend; vaddr += step)\r\nppc_md.hpte_removebolted(vaddr, psize, ssize);\r\nreturn 0;\r\n}\r\nstatic int __init htab_dt_scan_seg_sizes(unsigned long node,\r\nconst char *uname, int depth,\r\nvoid *data)\r\n{\r\nconst char *type = of_get_flat_dt_prop(node, "device_type", NULL);\r\nconst __be32 *prop;\r\nint size = 0;\r\nif (type == NULL || strcmp(type, "cpu") != 0)\r\nreturn 0;\r\nprop = of_get_flat_dt_prop(node, "ibm,processor-segment-sizes", &size);\r\nif (prop == NULL)\r\nreturn 0;\r\nfor (; size >= 4; size -= 4, ++prop) {\r\nif (be32_to_cpu(prop[0]) == 40) {\r\nDBG("1T segment support detected\n");\r\ncur_cpu_spec->mmu_features |= MMU_FTR_1T_SEGMENT;\r\nreturn 1;\r\n}\r\n}\r\ncur_cpu_spec->mmu_features &= ~MMU_FTR_NO_SLBIE_B;\r\nreturn 0;\r\n}\r\nstatic void __init htab_init_seg_sizes(void)\r\n{\r\nof_scan_flat_dt(htab_dt_scan_seg_sizes, NULL);\r\n}\r\nstatic int __init get_idx_from_shift(unsigned int shift)\r\n{\r\nint idx = -1;\r\nswitch (shift) {\r\ncase 0xc:\r\nidx = MMU_PAGE_4K;\r\nbreak;\r\ncase 0x10:\r\nidx = MMU_PAGE_64K;\r\nbreak;\r\ncase 0x14:\r\nidx = MMU_PAGE_1M;\r\nbreak;\r\ncase 0x18:\r\nidx = MMU_PAGE_16M;\r\nbreak;\r\ncase 0x22:\r\nidx = MMU_PAGE_16G;\r\nbreak;\r\n}\r\nreturn idx;\r\n}\r\nstatic int __init htab_dt_scan_page_sizes(unsigned long node,\r\nconst char *uname, int depth,\r\nvoid *data)\r\n{\r\nconst char *type = of_get_flat_dt_prop(node, "device_type", NULL);\r\nconst __be32 *prop;\r\nint size = 0;\r\nif (type == NULL || strcmp(type, "cpu") != 0)\r\nreturn 0;\r\nprop = of_get_flat_dt_prop(node, "ibm,segment-page-sizes", &size);\r\nif (!prop)\r\nreturn 0;\r\npr_info("Page sizes from device-tree:\n");\r\nsize /= 4;\r\ncur_cpu_spec->mmu_features &= ~(MMU_FTR_16M_PAGE);\r\nwhile(size > 0) {\r\nunsigned int base_shift = be32_to_cpu(prop[0]);\r\nunsigned int slbenc = be32_to_cpu(prop[1]);\r\nunsigned int lpnum = be32_to_cpu(prop[2]);\r\nstruct mmu_psize_def *def;\r\nint idx, base_idx;\r\nsize -= 3; prop += 3;\r\nbase_idx = get_idx_from_shift(base_shift);\r\nif (base_idx < 0) {\r\nprop += lpnum * 2; size -= lpnum * 2;\r\ncontinue;\r\n}\r\ndef = &mmu_psize_defs[base_idx];\r\nif (base_idx == MMU_PAGE_16M)\r\ncur_cpu_spec->mmu_features |= MMU_FTR_16M_PAGE;\r\ndef->shift = base_shift;\r\nif (base_shift <= 23)\r\ndef->avpnm = 0;\r\nelse\r\ndef->avpnm = (1 << (base_shift - 23)) - 1;\r\ndef->sllp = slbenc;\r\nif (base_idx == MMU_PAGE_4K || base_idx == MMU_PAGE_64K)\r\ndef->tlbiel = 1;\r\nelse\r\ndef->tlbiel = 0;\r\nwhile (size > 0 && lpnum) {\r\nunsigned int shift = be32_to_cpu(prop[0]);\r\nint penc = be32_to_cpu(prop[1]);\r\nprop += 2; size -= 2;\r\nlpnum--;\r\nidx = get_idx_from_shift(shift);\r\nif (idx < 0)\r\ncontinue;\r\nif (penc == -1)\r\npr_err("Invalid penc for base_shift=%d "\r\n"shift=%d\n", base_shift, shift);\r\ndef->penc[idx] = penc;\r\npr_info("base_shift=%d: shift=%d, sllp=0x%04lx,"\r\n" avpnm=0x%08lx, tlbiel=%d, penc=%d\n",\r\nbase_shift, shift, def->sllp,\r\ndef->avpnm, def->tlbiel, def->penc[idx]);\r\n}\r\n}\r\nreturn 1;\r\n}\r\nstatic int __init htab_dt_scan_hugepage_blocks(unsigned long node,\r\nconst char *uname, int depth,\r\nvoid *data) {\r\nconst char *type = of_get_flat_dt_prop(node, "device_type", NULL);\r\nconst __be64 *addr_prop;\r\nconst __be32 *page_count_prop;\r\nunsigned int expected_pages;\r\nlong unsigned int phys_addr;\r\nlong unsigned int block_size;\r\nif (type == NULL || strcmp(type, "memory") != 0)\r\nreturn 0;\r\npage_count_prop = of_get_flat_dt_prop(node, "ibm,expected#pages", NULL);\r\nif (page_count_prop == NULL)\r\nreturn 0;\r\nexpected_pages = (1 << be32_to_cpu(page_count_prop[0]));\r\naddr_prop = of_get_flat_dt_prop(node, "reg", NULL);\r\nif (addr_prop == NULL)\r\nreturn 0;\r\nphys_addr = be64_to_cpu(addr_prop[0]);\r\nblock_size = be64_to_cpu(addr_prop[1]);\r\nif (block_size != (16 * GB))\r\nreturn 0;\r\nprintk(KERN_INFO "Huge page(16GB) memory: "\r\n"addr = 0x%lX size = 0x%lX pages = %d\n",\r\nphys_addr, block_size, expected_pages);\r\nif (phys_addr + (16 * GB) <= memblock_end_of_DRAM()) {\r\nmemblock_reserve(phys_addr, block_size * expected_pages);\r\nadd_gpage(phys_addr, block_size, expected_pages);\r\n}\r\nreturn 0;\r\n}\r\nstatic void mmu_psize_set_default_penc(void)\r\n{\r\nint bpsize, apsize;\r\nfor (bpsize = 0; bpsize < MMU_PAGE_COUNT; bpsize++)\r\nfor (apsize = 0; apsize < MMU_PAGE_COUNT; apsize++)\r\nmmu_psize_defs[bpsize].penc[apsize] = -1;\r\n}\r\nstatic bool might_have_hea(void)\r\n{\r\n#ifdef CONFIG_IBMEBUS\r\nreturn !cpu_has_feature(CPU_FTR_ARCH_207S);\r\n#else\r\nreturn false;\r\n#endif\r\n}\r\nstatic void __init htab_init_page_sizes(void)\r\n{\r\nint rc;\r\nmmu_psize_set_default_penc();\r\nmemcpy(mmu_psize_defs, mmu_psize_defaults_old,\r\nsizeof(mmu_psize_defaults_old));\r\nrc = of_scan_flat_dt(htab_dt_scan_page_sizes, NULL);\r\nif (rc != 0)\r\ngoto found;\r\nif (mmu_has_feature(MMU_FTR_16M_PAGE))\r\nmemcpy(mmu_psize_defs, mmu_psize_defaults_gp,\r\nsizeof(mmu_psize_defaults_gp));\r\nfound:\r\n#ifndef CONFIG_DEBUG_PAGEALLOC\r\nif (mmu_psize_defs[MMU_PAGE_16M].shift)\r\nmmu_linear_psize = MMU_PAGE_16M;\r\nelse if (mmu_psize_defs[MMU_PAGE_1M].shift)\r\nmmu_linear_psize = MMU_PAGE_1M;\r\n#endif\r\n#ifdef CONFIG_PPC_64K_PAGES\r\nif (mmu_psize_defs[MMU_PAGE_64K].shift) {\r\nmmu_virtual_psize = MMU_PAGE_64K;\r\nmmu_vmalloc_psize = MMU_PAGE_64K;\r\nif (mmu_linear_psize == MMU_PAGE_4K)\r\nmmu_linear_psize = MMU_PAGE_64K;\r\nif (mmu_has_feature(MMU_FTR_CI_LARGE_PAGE)) {\r\nif (!might_have_hea() || !machine_is(pseries))\r\nmmu_io_psize = MMU_PAGE_64K;\r\n} else\r\nmmu_ci_restrictions = 1;\r\n}\r\n#endif\r\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\r\nif (mmu_psize_defs[MMU_PAGE_16M].shift &&\r\nmemblock_phys_mem_size() >= 0x40000000)\r\nmmu_vmemmap_psize = MMU_PAGE_16M;\r\nelse if (mmu_psize_defs[MMU_PAGE_64K].shift)\r\nmmu_vmemmap_psize = MMU_PAGE_64K;\r\nelse\r\nmmu_vmemmap_psize = MMU_PAGE_4K;\r\n#endif\r\nprintk(KERN_DEBUG "Page orders: linear mapping = %d, "\r\n"virtual = %d, io = %d"\r\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\r\n", vmemmap = %d"\r\n#endif\r\n"\n",\r\nmmu_psize_defs[mmu_linear_psize].shift,\r\nmmu_psize_defs[mmu_virtual_psize].shift,\r\nmmu_psize_defs[mmu_io_psize].shift\r\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\r\n,mmu_psize_defs[mmu_vmemmap_psize].shift\r\n#endif\r\n);\r\n#ifdef CONFIG_HUGETLB_PAGE\r\nof_scan_flat_dt(htab_dt_scan_hugepage_blocks, NULL);\r\n#endif\r\n}\r\nstatic int __init htab_dt_scan_pftsize(unsigned long node,\r\nconst char *uname, int depth,\r\nvoid *data)\r\n{\r\nconst char *type = of_get_flat_dt_prop(node, "device_type", NULL);\r\nconst __be32 *prop;\r\nif (type == NULL || strcmp(type, "cpu") != 0)\r\nreturn 0;\r\nprop = of_get_flat_dt_prop(node, "ibm,pft-size", NULL);\r\nif (prop != NULL) {\r\nppc64_pft_size = be32_to_cpu(prop[1]);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic unsigned long __init htab_get_table_size(void)\r\n{\r\nunsigned long mem_size, rnd_mem_size, pteg_count, psize;\r\nif (ppc64_pft_size == 0)\r\nof_scan_flat_dt(htab_dt_scan_pftsize, NULL);\r\nif (ppc64_pft_size)\r\nreturn 1UL << ppc64_pft_size;\r\nmem_size = memblock_phys_mem_size();\r\nrnd_mem_size = 1UL << __ilog2(mem_size);\r\nif (rnd_mem_size < mem_size)\r\nrnd_mem_size <<= 1;\r\npsize = mmu_psize_defs[mmu_virtual_psize].shift;\r\npteg_count = max(rnd_mem_size >> (psize + 1), 1UL << 11);\r\nreturn pteg_count << 7;\r\n}\r\nint create_section_mapping(unsigned long start, unsigned long end)\r\n{\r\nreturn htab_bolt_mapping(start, end, __pa(start),\r\npgprot_val(PAGE_KERNEL), mmu_linear_psize,\r\nmmu_kernel_ssize);\r\n}\r\nint remove_section_mapping(unsigned long start, unsigned long end)\r\n{\r\nreturn htab_remove_mapping(start, end, mmu_linear_psize,\r\nmmu_kernel_ssize);\r\n}\r\nstatic void __init htab_finish_init(void)\r\n{\r\n#ifdef CONFIG_PPC_HAS_HASH_64K\r\npatch_branch(ht64_call_hpte_insert1,\r\nppc_function_entry(ppc_md.hpte_insert),\r\nBRANCH_SET_LINK);\r\npatch_branch(ht64_call_hpte_insert2,\r\nppc_function_entry(ppc_md.hpte_insert),\r\nBRANCH_SET_LINK);\r\npatch_branch(ht64_call_hpte_remove,\r\nppc_function_entry(ppc_md.hpte_remove),\r\nBRANCH_SET_LINK);\r\npatch_branch(ht64_call_hpte_updatepp,\r\nppc_function_entry(ppc_md.hpte_updatepp),\r\nBRANCH_SET_LINK);\r\n#endif\r\npatch_branch(htab_call_hpte_insert1,\r\nppc_function_entry(ppc_md.hpte_insert),\r\nBRANCH_SET_LINK);\r\npatch_branch(htab_call_hpte_insert2,\r\nppc_function_entry(ppc_md.hpte_insert),\r\nBRANCH_SET_LINK);\r\npatch_branch(htab_call_hpte_remove,\r\nppc_function_entry(ppc_md.hpte_remove),\r\nBRANCH_SET_LINK);\r\npatch_branch(htab_call_hpte_updatepp,\r\nppc_function_entry(ppc_md.hpte_updatepp),\r\nBRANCH_SET_LINK);\r\n}\r\nstatic void __init htab_initialize(void)\r\n{\r\nunsigned long table;\r\nunsigned long pteg_count;\r\nunsigned long prot;\r\nunsigned long base = 0, size = 0, limit;\r\nstruct memblock_region *reg;\r\nDBG(" -> htab_initialize()\n");\r\nhtab_init_seg_sizes();\r\nhtab_init_page_sizes();\r\nif (mmu_has_feature(MMU_FTR_1T_SEGMENT)) {\r\nmmu_kernel_ssize = MMU_SEGSIZE_1T;\r\nmmu_highuser_ssize = MMU_SEGSIZE_1T;\r\nprintk(KERN_INFO "Using 1TB segments\n");\r\n}\r\nhtab_size_bytes = htab_get_table_size();\r\npteg_count = htab_size_bytes >> 7;\r\nhtab_hash_mask = pteg_count - 1;\r\nif (firmware_has_feature(FW_FEATURE_LPAR)) {\r\nhtab_address = NULL;\r\n_SDR1 = 0;\r\n#ifdef CONFIG_FA_DUMP\r\nif (is_fadump_active() && ppc_md.hpte_clear_all)\r\nppc_md.hpte_clear_all();\r\n#endif\r\n} else {\r\nif (machine_is(cell))\r\nlimit = 0x80000000;\r\nelse\r\nlimit = MEMBLOCK_ALLOC_ANYWHERE;\r\ntable = memblock_alloc_base(htab_size_bytes, htab_size_bytes, limit);\r\nDBG("Hash table allocated at %lx, size: %lx\n", table,\r\nhtab_size_bytes);\r\nhtab_address = __va(table);\r\n_SDR1 = table + __ilog2(pteg_count) - 11;\r\nmemset((void *)table, 0, htab_size_bytes);\r\nmtspr(SPRN_SDR1, _SDR1);\r\n}\r\nprot = pgprot_val(PAGE_KERNEL);\r\n#ifdef CONFIG_DEBUG_PAGEALLOC\r\nlinear_map_hash_count = memblock_end_of_DRAM() >> PAGE_SHIFT;\r\nlinear_map_hash_slots = __va(memblock_alloc_base(linear_map_hash_count,\r\n1, ppc64_rma_size));\r\nmemset(linear_map_hash_slots, 0, linear_map_hash_count);\r\n#endif\r\nfor_each_memblock(memory, reg) {\r\nbase = (unsigned long)__va(reg->base);\r\nsize = reg->size;\r\nDBG("creating mapping for region: %lx..%lx (prot: %lx)\n",\r\nbase, size, prot);\r\n#ifdef CONFIG_U3_DART\r\nDBG("DART base: %lx\n", dart_tablebase);\r\nif (dart_tablebase != 0 && dart_tablebase >= base\r\n&& dart_tablebase < (base + size)) {\r\nunsigned long dart_table_end = dart_tablebase + 16 * MB;\r\nif (base != dart_tablebase)\r\nBUG_ON(htab_bolt_mapping(base, dart_tablebase,\r\n__pa(base), prot,\r\nmmu_linear_psize,\r\nmmu_kernel_ssize));\r\nif ((base + size) > dart_table_end)\r\nBUG_ON(htab_bolt_mapping(dart_tablebase+16*MB,\r\nbase + size,\r\n__pa(dart_table_end),\r\nprot,\r\nmmu_linear_psize,\r\nmmu_kernel_ssize));\r\ncontinue;\r\n}\r\n#endif\r\nBUG_ON(htab_bolt_mapping(base, base + size, __pa(base),\r\nprot, mmu_linear_psize, mmu_kernel_ssize));\r\n}\r\nmemblock_set_current_limit(MEMBLOCK_ALLOC_ANYWHERE);\r\nif (tce_alloc_start) {\r\ntce_alloc_start = (unsigned long)__va(tce_alloc_start);\r\ntce_alloc_end = (unsigned long)__va(tce_alloc_end);\r\nif (base + size >= tce_alloc_start)\r\ntce_alloc_start = base + size + 1;\r\nBUG_ON(htab_bolt_mapping(tce_alloc_start, tce_alloc_end,\r\n__pa(tce_alloc_start), prot,\r\nmmu_linear_psize, mmu_kernel_ssize));\r\n}\r\nhtab_finish_init();\r\nDBG(" <- htab_initialize()\n");\r\n}\r\nvoid __init early_init_mmu(void)\r\n{\r\nhtab_initialize();\r\nslb_initialize();\r\n}\r\nvoid early_init_mmu_secondary(void)\r\n{\r\nif (!firmware_has_feature(FW_FEATURE_LPAR))\r\nmtspr(SPRN_SDR1, _SDR1);\r\nslb_initialize();\r\n}\r\nunsigned int hash_page_do_lazy_icache(unsigned int pp, pte_t pte, int trap)\r\n{\r\nstruct page *page;\r\nif (!pfn_valid(pte_pfn(pte)))\r\nreturn pp;\r\npage = pte_page(pte);\r\nif (!test_bit(PG_arch_1, &page->flags) && !PageReserved(page)) {\r\nif (trap == 0x400) {\r\nflush_dcache_icache_page(page);\r\nset_bit(PG_arch_1, &page->flags);\r\n} else\r\npp |= HPTE_R_N;\r\n}\r\nreturn pp;\r\n}\r\nstatic unsigned int get_paca_psize(unsigned long addr)\r\n{\r\nu64 lpsizes;\r\nunsigned char *hpsizes;\r\nunsigned long index, mask_index;\r\nif (addr < SLICE_LOW_TOP) {\r\nlpsizes = get_paca()->context.low_slices_psize;\r\nindex = GET_LOW_SLICE_INDEX(addr);\r\nreturn (lpsizes >> (index * 4)) & 0xF;\r\n}\r\nhpsizes = get_paca()->context.high_slices_psize;\r\nindex = GET_HIGH_SLICE_INDEX(addr);\r\nmask_index = index & 0x1;\r\nreturn (hpsizes[index >> 1] >> (mask_index * 4)) & 0xF;\r\n}\r\nunsigned int get_paca_psize(unsigned long addr)\r\n{\r\nreturn get_paca()->context.user_psize;\r\n}\r\nvoid demote_segment_4k(struct mm_struct *mm, unsigned long addr)\r\n{\r\nif (get_slice_psize(mm, addr) == MMU_PAGE_4K)\r\nreturn;\r\nslice_set_range_psize(mm, addr, 1, MMU_PAGE_4K);\r\ncopro_flush_all_slbs(mm);\r\nif ((get_paca_psize(addr) != MMU_PAGE_4K) && (current->mm == mm)) {\r\nget_paca()->context = mm->context;\r\nslb_flush_and_rebolt();\r\n}\r\n}\r\nstatic int subpage_protection(struct mm_struct *mm, unsigned long ea)\r\n{\r\nstruct subpage_prot_table *spt = &mm->context.spt;\r\nu32 spp = 0;\r\nu32 **sbpm, *sbpp;\r\nif (ea >= spt->maxaddr)\r\nreturn 0;\r\nif (ea < 0x100000000UL) {\r\nsbpm = spt->low_prot;\r\n} else {\r\nsbpm = spt->protptrs[ea >> SBP_L3_SHIFT];\r\nif (!sbpm)\r\nreturn 0;\r\n}\r\nsbpp = sbpm[(ea >> SBP_L2_SHIFT) & (SBP_L2_COUNT - 1)];\r\nif (!sbpp)\r\nreturn 0;\r\nspp = sbpp[(ea >> PAGE_SHIFT) & (SBP_L1_COUNT - 1)];\r\nspp >>= 30 - 2 * ((ea >> 12) & 0xf);\r\nspp = ((spp & 2) ? _PAGE_USER : 0) | ((spp & 1) ? _PAGE_RW : 0);\r\nreturn spp;\r\n}\r\nstatic inline int subpage_protection(struct mm_struct *mm, unsigned long ea)\r\n{\r\nreturn 0;\r\n}\r\nvoid hash_failure_debug(unsigned long ea, unsigned long access,\r\nunsigned long vsid, unsigned long trap,\r\nint ssize, int psize, int lpsize, unsigned long pte)\r\n{\r\nif (!printk_ratelimit())\r\nreturn;\r\npr_info("mm: Hashing failure ! EA=0x%lx access=0x%lx current=%s\n",\r\nea, access, current->comm);\r\npr_info(" trap=0x%lx vsid=0x%lx ssize=%d base psize=%d psize %d pte=0x%lx\n",\r\ntrap, vsid, ssize, psize, lpsize, pte);\r\n}\r\nstatic void check_paca_psize(unsigned long ea, struct mm_struct *mm,\r\nint psize, bool user_region)\r\n{\r\nif (user_region) {\r\nif (psize != get_paca_psize(ea)) {\r\nget_paca()->context = mm->context;\r\nslb_flush_and_rebolt();\r\n}\r\n} else if (get_paca()->vmalloc_sllp !=\r\nmmu_psize_defs[mmu_vmalloc_psize].sllp) {\r\nget_paca()->vmalloc_sllp =\r\nmmu_psize_defs[mmu_vmalloc_psize].sllp;\r\nslb_vmalloc_update();\r\n}\r\n}\r\nint hash_page_mm(struct mm_struct *mm, unsigned long ea,\r\nunsigned long access, unsigned long trap,\r\nunsigned long flags)\r\n{\r\nenum ctx_state prev_state = exception_enter();\r\npgd_t *pgdir;\r\nunsigned long vsid;\r\npte_t *ptep;\r\nunsigned hugeshift;\r\nconst struct cpumask *tmp;\r\nint rc, user_region = 0;\r\nint psize, ssize;\r\nDBG_LOW("hash_page(ea=%016lx, access=%lx, trap=%lx\n",\r\nea, access, trap);\r\nswitch (REGION_ID(ea)) {\r\ncase USER_REGION_ID:\r\nuser_region = 1;\r\nif (! mm) {\r\nDBG_LOW(" user region with no mm !\n");\r\nrc = 1;\r\ngoto bail;\r\n}\r\npsize = get_slice_psize(mm, ea);\r\nssize = user_segment_size(ea);\r\nvsid = get_vsid(mm->context.id, ea, ssize);\r\nbreak;\r\ncase VMALLOC_REGION_ID:\r\nvsid = get_kernel_vsid(ea, mmu_kernel_ssize);\r\nif (ea < VMALLOC_END)\r\npsize = mmu_vmalloc_psize;\r\nelse\r\npsize = mmu_io_psize;\r\nssize = mmu_kernel_ssize;\r\nbreak;\r\ndefault:\r\nrc = 1;\r\ngoto bail;\r\n}\r\nDBG_LOW(" mm=%p, mm->pgdir=%p, vsid=%016lx\n", mm, mm->pgd, vsid);\r\nif (!vsid) {\r\nDBG_LOW("Bad address!\n");\r\nrc = 1;\r\ngoto bail;\r\n}\r\npgdir = mm->pgd;\r\nif (pgdir == NULL) {\r\nrc = 1;\r\ngoto bail;\r\n}\r\ntmp = cpumask_of(smp_processor_id());\r\nif (user_region && cpumask_equal(mm_cpumask(mm), tmp))\r\nflags |= HPTE_LOCAL_UPDATE;\r\n#ifndef CONFIG_PPC_64K_PAGES\r\nif (psize != MMU_PAGE_4K)\r\nea &= ~((1ul << mmu_psize_defs[psize].shift) - 1);\r\n#endif\r\nptep = find_linux_pte_or_hugepte(pgdir, ea, &hugeshift);\r\nif (ptep == NULL || !pte_present(*ptep)) {\r\nDBG_LOW(" no PTE !\n");\r\nrc = 1;\r\ngoto bail;\r\n}\r\naccess |= _PAGE_PRESENT;\r\nif (access & ~pte_val(*ptep)) {\r\nDBG_LOW(" no access !\n");\r\nrc = 1;\r\ngoto bail;\r\n}\r\nif (hugeshift) {\r\nif (pmd_trans_huge(*(pmd_t *)ptep))\r\nrc = __hash_page_thp(ea, access, vsid, (pmd_t *)ptep,\r\ntrap, flags, ssize, psize);\r\n#ifdef CONFIG_HUGETLB_PAGE\r\nelse\r\nrc = __hash_page_huge(ea, access, vsid, ptep, trap,\r\nflags, ssize, hugeshift, psize);\r\n#else\r\nelse {\r\nrc = 1;\r\nWARN_ON(1);\r\n}\r\n#endif\r\nif (current->mm == mm)\r\ncheck_paca_psize(ea, mm, psize, user_region);\r\ngoto bail;\r\n}\r\n#ifndef CONFIG_PPC_64K_PAGES\r\nDBG_LOW(" i-pte: %016lx\n", pte_val(*ptep));\r\n#else\r\nDBG_LOW(" i-pte: %016lx %016lx\n", pte_val(*ptep),\r\npte_val(*(ptep + PTRS_PER_PTE)));\r\n#endif\r\n#ifdef CONFIG_PPC_64K_PAGES\r\nif ((pte_val(*ptep) & _PAGE_4K_PFN) && psize == MMU_PAGE_64K) {\r\ndemote_segment_4k(mm, ea);\r\npsize = MMU_PAGE_4K;\r\n}\r\nif (mmu_ci_restrictions && psize == MMU_PAGE_64K &&\r\n(pte_val(*ptep) & _PAGE_NO_CACHE)) {\r\nif (user_region) {\r\ndemote_segment_4k(mm, ea);\r\npsize = MMU_PAGE_4K;\r\n} else if (ea < VMALLOC_END) {\r\nprintk(KERN_ALERT "Reducing vmalloc segment "\r\n"to 4kB pages because of "\r\n"non-cacheable mapping\n");\r\npsize = mmu_vmalloc_psize = MMU_PAGE_4K;\r\ncopro_flush_all_slbs(mm);\r\n}\r\n}\r\nif (current->mm == mm)\r\ncheck_paca_psize(ea, mm, psize, user_region);\r\n#endif\r\n#ifdef CONFIG_PPC_HAS_HASH_64K\r\nif (psize == MMU_PAGE_64K)\r\nrc = __hash_page_64K(ea, access, vsid, ptep, trap,\r\nflags, ssize);\r\nelse\r\n#endif\r\n{\r\nint spp = subpage_protection(mm, ea);\r\nif (access & spp)\r\nrc = -2;\r\nelse\r\nrc = __hash_page_4K(ea, access, vsid, ptep, trap,\r\nflags, ssize, spp);\r\n}\r\nif (rc == -1)\r\nhash_failure_debug(ea, access, vsid, trap, ssize, psize,\r\npsize, pte_val(*ptep));\r\n#ifndef CONFIG_PPC_64K_PAGES\r\nDBG_LOW(" o-pte: %016lx\n", pte_val(*ptep));\r\n#else\r\nDBG_LOW(" o-pte: %016lx %016lx\n", pte_val(*ptep),\r\npte_val(*(ptep + PTRS_PER_PTE)));\r\n#endif\r\nDBG_LOW(" -> rc=%d\n", rc);\r\nbail:\r\nexception_exit(prev_state);\r\nreturn rc;\r\n}\r\nint hash_page(unsigned long ea, unsigned long access, unsigned long trap,\r\nunsigned long dsisr)\r\n{\r\nunsigned long flags = 0;\r\nstruct mm_struct *mm = current->mm;\r\nif (REGION_ID(ea) == VMALLOC_REGION_ID)\r\nmm = &init_mm;\r\nif (dsisr & DSISR_NOHPTE)\r\nflags |= HPTE_NOHPTE_UPDATE;\r\nreturn hash_page_mm(mm, ea, access, trap, flags);\r\n}\r\nvoid hash_preload(struct mm_struct *mm, unsigned long ea,\r\nunsigned long access, unsigned long trap)\r\n{\r\nint hugepage_shift;\r\nunsigned long vsid;\r\npgd_t *pgdir;\r\npte_t *ptep;\r\nunsigned long flags;\r\nint rc, ssize, update_flags = 0;\r\nBUG_ON(REGION_ID(ea) != USER_REGION_ID);\r\n#ifdef CONFIG_PPC_MM_SLICES\r\nif (unlikely(get_slice_psize(mm, ea) != mm->context.user_psize))\r\nreturn;\r\n#endif\r\nDBG_LOW("hash_preload(mm=%p, mm->pgdir=%p, ea=%016lx, access=%lx,"\r\n" trap=%lx\n", mm, mm->pgd, ea, access, trap);\r\npgdir = mm->pgd;\r\nif (pgdir == NULL)\r\nreturn;\r\nssize = user_segment_size(ea);\r\nvsid = get_vsid(mm->context.id, ea, ssize);\r\nif (!vsid)\r\nreturn;\r\nlocal_irq_save(flags);\r\nptep = find_linux_pte_or_hugepte(pgdir, ea, &hugepage_shift);\r\nif (!ptep)\r\ngoto out_exit;\r\nWARN_ON(hugepage_shift);\r\n#ifdef CONFIG_PPC_64K_PAGES\r\nif (pte_val(*ptep) & (_PAGE_4K_PFN | _PAGE_NO_CACHE))\r\ngoto out_exit;\r\n#endif\r\nif (cpumask_equal(mm_cpumask(mm), cpumask_of(smp_processor_id())))\r\nupdate_flags |= HPTE_LOCAL_UPDATE;\r\n#ifdef CONFIG_PPC_HAS_HASH_64K\r\nif (mm->context.user_psize == MMU_PAGE_64K)\r\nrc = __hash_page_64K(ea, access, vsid, ptep, trap,\r\nupdate_flags, ssize);\r\nelse\r\n#endif\r\nrc = __hash_page_4K(ea, access, vsid, ptep, trap, update_flags,\r\nssize, subpage_protection(mm, ea));\r\nif (rc == -1)\r\nhash_failure_debug(ea, access, vsid, trap, ssize,\r\nmm->context.user_psize,\r\nmm->context.user_psize,\r\npte_val(*ptep));\r\nout_exit:\r\nlocal_irq_restore(flags);\r\n}\r\nvoid flush_hash_page(unsigned long vpn, real_pte_t pte, int psize, int ssize,\r\nunsigned long flags)\r\n{\r\nunsigned long hash, index, shift, hidx, slot;\r\nint local = flags & HPTE_LOCAL_UPDATE;\r\nDBG_LOW("flush_hash_page(vpn=%016lx)\n", vpn);\r\npte_iterate_hashed_subpages(pte, psize, vpn, index, shift) {\r\nhash = hpt_hash(vpn, shift, ssize);\r\nhidx = __rpte_to_hidx(pte, index);\r\nif (hidx & _PTEIDX_SECONDARY)\r\nhash = ~hash;\r\nslot = (hash & htab_hash_mask) * HPTES_PER_GROUP;\r\nslot += hidx & _PTEIDX_GROUP_IX;\r\nDBG_LOW(" sub %ld: hash=%lx, hidx=%lx\n", index, slot, hidx);\r\nppc_md.hpte_invalidate(slot, vpn, psize, psize, ssize, local);\r\n} pte_iterate_hashed_end();\r\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\r\nif (local && cpu_has_feature(CPU_FTR_TM) &&\r\ncurrent->thread.regs &&\r\nMSR_TM_ACTIVE(current->thread.regs->msr)) {\r\ntm_enable();\r\ntm_abort(TM_CAUSE_TLBI);\r\n}\r\n#endif\r\n}\r\nvoid flush_hash_hugepage(unsigned long vsid, unsigned long addr,\r\npmd_t *pmdp, unsigned int psize, int ssize,\r\nunsigned long flags)\r\n{\r\nint i, max_hpte_count, valid;\r\nunsigned long s_addr;\r\nunsigned char *hpte_slot_array;\r\nunsigned long hidx, shift, vpn, hash, slot;\r\nint local = flags & HPTE_LOCAL_UPDATE;\r\ns_addr = addr & HPAGE_PMD_MASK;\r\nhpte_slot_array = get_hpte_slot_array(pmdp);\r\nif (!hpte_slot_array)\r\nreturn;\r\nif (ppc_md.hugepage_invalidate) {\r\nppc_md.hugepage_invalidate(vsid, s_addr, hpte_slot_array,\r\npsize, ssize, local);\r\ngoto tm_abort;\r\n}\r\nshift = mmu_psize_defs[psize].shift;\r\nmax_hpte_count = HPAGE_PMD_SIZE >> shift;\r\nfor (i = 0; i < max_hpte_count; i++) {\r\nvalid = hpte_valid(hpte_slot_array, i);\r\nif (!valid)\r\ncontinue;\r\nhidx = hpte_hash_index(hpte_slot_array, i);\r\naddr = s_addr + (i * (1ul << shift));\r\nvpn = hpt_vpn(addr, vsid, ssize);\r\nhash = hpt_hash(vpn, shift, ssize);\r\nif (hidx & _PTEIDX_SECONDARY)\r\nhash = ~hash;\r\nslot = (hash & htab_hash_mask) * HPTES_PER_GROUP;\r\nslot += hidx & _PTEIDX_GROUP_IX;\r\nppc_md.hpte_invalidate(slot, vpn, psize,\r\nMMU_PAGE_16M, ssize, local);\r\n}\r\ntm_abort:\r\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\r\nif (local && cpu_has_feature(CPU_FTR_TM) &&\r\ncurrent->thread.regs &&\r\nMSR_TM_ACTIVE(current->thread.regs->msr)) {\r\ntm_enable();\r\ntm_abort(TM_CAUSE_TLBI);\r\n}\r\n#endif\r\n}\r\nvoid flush_hash_range(unsigned long number, int local)\r\n{\r\nif (ppc_md.flush_hash_range)\r\nppc_md.flush_hash_range(number, local);\r\nelse {\r\nint i;\r\nstruct ppc64_tlb_batch *batch =\r\nthis_cpu_ptr(&ppc64_tlb_batch);\r\nfor (i = 0; i < number; i++)\r\nflush_hash_page(batch->vpn[i], batch->pte[i],\r\nbatch->psize, batch->ssize, local);\r\n}\r\n}\r\nvoid low_hash_fault(struct pt_regs *regs, unsigned long address, int rc)\r\n{\r\nenum ctx_state prev_state = exception_enter();\r\nif (user_mode(regs)) {\r\n#ifdef CONFIG_PPC_SUBPAGE_PROT\r\nif (rc == -2)\r\n_exception(SIGSEGV, regs, SEGV_ACCERR, address);\r\nelse\r\n#endif\r\n_exception(SIGBUS, regs, BUS_ADRERR, address);\r\n} else\r\nbad_page_fault(regs, address, SIGBUS);\r\nexception_exit(prev_state);\r\n}\r\nlong hpte_insert_repeating(unsigned long hash, unsigned long vpn,\r\nunsigned long pa, unsigned long rflags,\r\nunsigned long vflags, int psize, int ssize)\r\n{\r\nunsigned long hpte_group;\r\nlong slot;\r\nrepeat:\r\nhpte_group = ((hash & htab_hash_mask) *\r\nHPTES_PER_GROUP) & ~0x7UL;\r\nslot = ppc_md.hpte_insert(hpte_group, vpn, pa, rflags, vflags,\r\npsize, psize, ssize);\r\nif (unlikely(slot == -1)) {\r\nhpte_group = ((~hash & htab_hash_mask) *\r\nHPTES_PER_GROUP) & ~0x7UL;\r\nslot = ppc_md.hpte_insert(hpte_group, vpn, pa, rflags,\r\nvflags | HPTE_V_SECONDARY,\r\npsize, psize, ssize);\r\nif (slot == -1) {\r\nif (mftb() & 0x1)\r\nhpte_group = ((hash & htab_hash_mask) *\r\nHPTES_PER_GROUP)&~0x7UL;\r\nppc_md.hpte_remove(hpte_group);\r\ngoto repeat;\r\n}\r\n}\r\nreturn slot;\r\n}\r\nstatic void kernel_map_linear_page(unsigned long vaddr, unsigned long lmi)\r\n{\r\nunsigned long hash;\r\nunsigned long vsid = get_kernel_vsid(vaddr, mmu_kernel_ssize);\r\nunsigned long vpn = hpt_vpn(vaddr, vsid, mmu_kernel_ssize);\r\nunsigned long mode = htab_convert_pte_flags(PAGE_KERNEL);\r\nlong ret;\r\nhash = hpt_hash(vpn, PAGE_SHIFT, mmu_kernel_ssize);\r\nif (!vsid)\r\nreturn;\r\nret = hpte_insert_repeating(hash, vpn, __pa(vaddr), mode,\r\nHPTE_V_BOLTED,\r\nmmu_linear_psize, mmu_kernel_ssize);\r\nBUG_ON (ret < 0);\r\nspin_lock(&linear_map_hash_lock);\r\nBUG_ON(linear_map_hash_slots[lmi] & 0x80);\r\nlinear_map_hash_slots[lmi] = ret | 0x80;\r\nspin_unlock(&linear_map_hash_lock);\r\n}\r\nstatic void kernel_unmap_linear_page(unsigned long vaddr, unsigned long lmi)\r\n{\r\nunsigned long hash, hidx, slot;\r\nunsigned long vsid = get_kernel_vsid(vaddr, mmu_kernel_ssize);\r\nunsigned long vpn = hpt_vpn(vaddr, vsid, mmu_kernel_ssize);\r\nhash = hpt_hash(vpn, PAGE_SHIFT, mmu_kernel_ssize);\r\nspin_lock(&linear_map_hash_lock);\r\nBUG_ON(!(linear_map_hash_slots[lmi] & 0x80));\r\nhidx = linear_map_hash_slots[lmi] & 0x7f;\r\nlinear_map_hash_slots[lmi] = 0;\r\nspin_unlock(&linear_map_hash_lock);\r\nif (hidx & _PTEIDX_SECONDARY)\r\nhash = ~hash;\r\nslot = (hash & htab_hash_mask) * HPTES_PER_GROUP;\r\nslot += hidx & _PTEIDX_GROUP_IX;\r\nppc_md.hpte_invalidate(slot, vpn, mmu_linear_psize, mmu_linear_psize,\r\nmmu_kernel_ssize, 0);\r\n}\r\nvoid __kernel_map_pages(struct page *page, int numpages, int enable)\r\n{\r\nunsigned long flags, vaddr, lmi;\r\nint i;\r\nlocal_irq_save(flags);\r\nfor (i = 0; i < numpages; i++, page++) {\r\nvaddr = (unsigned long)page_address(page);\r\nlmi = __pa(vaddr) >> PAGE_SHIFT;\r\nif (lmi >= linear_map_hash_count)\r\ncontinue;\r\nif (enable)\r\nkernel_map_linear_page(vaddr, lmi);\r\nelse\r\nkernel_unmap_linear_page(vaddr, lmi);\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\nvoid setup_initial_memory_limit(phys_addr_t first_memblock_base,\r\nphys_addr_t first_memblock_size)\r\n{\r\nBUG_ON(first_memblock_base != 0);\r\nppc64_rma_size = min_t(u64, first_memblock_size, 0x40000000);\r\nmemblock_set_current_limit(ppc64_rma_size);\r\n}
