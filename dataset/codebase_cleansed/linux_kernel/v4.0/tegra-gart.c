static inline void gart_set_pte(struct gart_device *gart,\r\nunsigned long offs, u32 pte)\r\n{\r\nwritel(offs, gart->regs + GART_ENTRY_ADDR);\r\nwritel(pte, gart->regs + GART_ENTRY_DATA);\r\ndev_dbg(gart->dev, "%s %08lx:%08x\n",\r\npte ? "map" : "unmap", offs, pte & GART_PAGE_MASK);\r\n}\r\nstatic inline unsigned long gart_read_pte(struct gart_device *gart,\r\nunsigned long offs)\r\n{\r\nunsigned long pte;\r\nwritel(offs, gart->regs + GART_ENTRY_ADDR);\r\npte = readl(gart->regs + GART_ENTRY_DATA);\r\nreturn pte;\r\n}\r\nstatic void do_gart_setup(struct gart_device *gart, const u32 *data)\r\n{\r\nunsigned long iova;\r\nfor_each_gart_pte(gart, iova)\r\ngart_set_pte(gart, iova, data ? *(data++) : 0);\r\nwritel(1, gart->regs + GART_CONFIG);\r\nFLUSH_GART_REGS(gart);\r\n}\r\nstatic void gart_dump_table(struct gart_device *gart)\r\n{\r\nunsigned long iova;\r\nunsigned long flags;\r\nspin_lock_irqsave(&gart->pte_lock, flags);\r\nfor_each_gart_pte(gart, iova) {\r\nunsigned long pte;\r\npte = gart_read_pte(gart, iova);\r\ndev_dbg(gart->dev, "%s %08lx:%08lx\n",\r\n(GART_ENTRY_PHYS_ADDR_VALID & pte) ? "v" : " ",\r\niova, pte & GART_PAGE_MASK);\r\n}\r\nspin_unlock_irqrestore(&gart->pte_lock, flags);\r\n}\r\nstatic inline void gart_dump_table(struct gart_device *gart)\r\n{\r\n}\r\nstatic inline bool gart_iova_range_valid(struct gart_device *gart,\r\nunsigned long iova, size_t bytes)\r\n{\r\nunsigned long iova_start, iova_end, gart_start, gart_end;\r\niova_start = iova;\r\niova_end = iova_start + bytes - 1;\r\ngart_start = gart->iovmm_base;\r\ngart_end = gart_start + gart->page_count * GART_PAGE_SIZE - 1;\r\nif (iova_start < gart_start)\r\nreturn false;\r\nif (iova_end > gart_end)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int gart_iommu_attach_dev(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct gart_device *gart;\r\nstruct gart_client *client, *c;\r\nint err = 0;\r\ngart = gart_handle;\r\nif (!gart)\r\nreturn -EINVAL;\r\ndomain->priv = gart;\r\ndomain->geometry.aperture_start = gart->iovmm_base;\r\ndomain->geometry.aperture_end = gart->iovmm_base +\r\ngart->page_count * GART_PAGE_SIZE - 1;\r\ndomain->geometry.force_aperture = true;\r\nclient = devm_kzalloc(gart->dev, sizeof(*c), GFP_KERNEL);\r\nif (!client)\r\nreturn -ENOMEM;\r\nclient->dev = dev;\r\nspin_lock(&gart->client_lock);\r\nlist_for_each_entry(c, &gart->client, list) {\r\nif (c->dev == dev) {\r\ndev_err(gart->dev,\r\n"%s is already attached\n", dev_name(dev));\r\nerr = -EINVAL;\r\ngoto fail;\r\n}\r\n}\r\nlist_add(&client->list, &gart->client);\r\nspin_unlock(&gart->client_lock);\r\ndev_dbg(gart->dev, "Attached %s\n", dev_name(dev));\r\nreturn 0;\r\nfail:\r\ndevm_kfree(gart->dev, client);\r\nspin_unlock(&gart->client_lock);\r\nreturn err;\r\n}\r\nstatic void gart_iommu_detach_dev(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct gart_device *gart = domain->priv;\r\nstruct gart_client *c;\r\nspin_lock(&gart->client_lock);\r\nlist_for_each_entry(c, &gart->client, list) {\r\nif (c->dev == dev) {\r\nlist_del(&c->list);\r\ndevm_kfree(gart->dev, c);\r\ndev_dbg(gart->dev, "Detached %s\n", dev_name(dev));\r\ngoto out;\r\n}\r\n}\r\ndev_err(gart->dev, "Couldn't find\n");\r\nout:\r\nspin_unlock(&gart->client_lock);\r\n}\r\nstatic int gart_iommu_domain_init(struct iommu_domain *domain)\r\n{\r\nreturn 0;\r\n}\r\nstatic void gart_iommu_domain_destroy(struct iommu_domain *domain)\r\n{\r\nstruct gart_device *gart = domain->priv;\r\nif (!gart)\r\nreturn;\r\nspin_lock(&gart->client_lock);\r\nif (!list_empty(&gart->client)) {\r\nstruct gart_client *c;\r\nlist_for_each_entry(c, &gart->client, list)\r\ngart_iommu_detach_dev(domain, c->dev);\r\n}\r\nspin_unlock(&gart->client_lock);\r\ndomain->priv = NULL;\r\n}\r\nstatic int gart_iommu_map(struct iommu_domain *domain, unsigned long iova,\r\nphys_addr_t pa, size_t bytes, int prot)\r\n{\r\nstruct gart_device *gart = domain->priv;\r\nunsigned long flags;\r\nunsigned long pfn;\r\nif (!gart_iova_range_valid(gart, iova, bytes))\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&gart->pte_lock, flags);\r\npfn = __phys_to_pfn(pa);\r\nif (!pfn_valid(pfn)) {\r\ndev_err(gart->dev, "Invalid page: %pa\n", &pa);\r\nspin_unlock_irqrestore(&gart->pte_lock, flags);\r\nreturn -EINVAL;\r\n}\r\ngart_set_pte(gart, iova, GART_PTE(pfn));\r\nFLUSH_GART_REGS(gart);\r\nspin_unlock_irqrestore(&gart->pte_lock, flags);\r\nreturn 0;\r\n}\r\nstatic size_t gart_iommu_unmap(struct iommu_domain *domain, unsigned long iova,\r\nsize_t bytes)\r\n{\r\nstruct gart_device *gart = domain->priv;\r\nunsigned long flags;\r\nif (!gart_iova_range_valid(gart, iova, bytes))\r\nreturn 0;\r\nspin_lock_irqsave(&gart->pte_lock, flags);\r\ngart_set_pte(gart, iova, 0);\r\nFLUSH_GART_REGS(gart);\r\nspin_unlock_irqrestore(&gart->pte_lock, flags);\r\nreturn 0;\r\n}\r\nstatic phys_addr_t gart_iommu_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct gart_device *gart = domain->priv;\r\nunsigned long pte;\r\nphys_addr_t pa;\r\nunsigned long flags;\r\nif (!gart_iova_range_valid(gart, iova, 0))\r\nreturn -EINVAL;\r\nspin_lock_irqsave(&gart->pte_lock, flags);\r\npte = gart_read_pte(gart, iova);\r\nspin_unlock_irqrestore(&gart->pte_lock, flags);\r\npa = (pte & GART_PAGE_MASK);\r\nif (!pfn_valid(__phys_to_pfn(pa))) {\r\ndev_err(gart->dev, "No entry for %08llx:%pa\n",\r\n(unsigned long long)iova, &pa);\r\ngart_dump_table(gart);\r\nreturn -EINVAL;\r\n}\r\nreturn pa;\r\n}\r\nstatic bool gart_iommu_capable(enum iommu_cap cap)\r\n{\r\nreturn false;\r\n}\r\nstatic int tegra_gart_suspend(struct device *dev)\r\n{\r\nstruct gart_device *gart = dev_get_drvdata(dev);\r\nunsigned long iova;\r\nu32 *data = gart->savedata;\r\nunsigned long flags;\r\nspin_lock_irqsave(&gart->pte_lock, flags);\r\nfor_each_gart_pte(gart, iova)\r\n*(data++) = gart_read_pte(gart, iova);\r\nspin_unlock_irqrestore(&gart->pte_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int tegra_gart_resume(struct device *dev)\r\n{\r\nstruct gart_device *gart = dev_get_drvdata(dev);\r\nunsigned long flags;\r\nspin_lock_irqsave(&gart->pte_lock, flags);\r\ndo_gart_setup(gart, gart->savedata);\r\nspin_unlock_irqrestore(&gart->pte_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int tegra_gart_probe(struct platform_device *pdev)\r\n{\r\nstruct gart_device *gart;\r\nstruct resource *res, *res_remap;\r\nvoid __iomem *gart_regs;\r\nstruct device *dev = &pdev->dev;\r\nif (gart_handle)\r\nreturn -EIO;\r\nBUILD_BUG_ON(PAGE_SHIFT != GART_PAGE_SHIFT);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nres_remap = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nif (!res || !res_remap) {\r\ndev_err(dev, "GART memory aperture expected\n");\r\nreturn -ENXIO;\r\n}\r\ngart = devm_kzalloc(dev, sizeof(*gart), GFP_KERNEL);\r\nif (!gart) {\r\ndev_err(dev, "failed to allocate gart_device\n");\r\nreturn -ENOMEM;\r\n}\r\ngart_regs = devm_ioremap(dev, res->start, resource_size(res));\r\nif (!gart_regs) {\r\ndev_err(dev, "failed to remap GART registers\n");\r\nreturn -ENXIO;\r\n}\r\ngart->dev = &pdev->dev;\r\nspin_lock_init(&gart->pte_lock);\r\nspin_lock_init(&gart->client_lock);\r\nINIT_LIST_HEAD(&gart->client);\r\ngart->regs = gart_regs;\r\ngart->iovmm_base = (dma_addr_t)res_remap->start;\r\ngart->page_count = (resource_size(res_remap) >> GART_PAGE_SHIFT);\r\ngart->savedata = vmalloc(sizeof(u32) * gart->page_count);\r\nif (!gart->savedata) {\r\ndev_err(dev, "failed to allocate context save area\n");\r\nreturn -ENOMEM;\r\n}\r\nplatform_set_drvdata(pdev, gart);\r\ndo_gart_setup(gart, NULL);\r\ngart_handle = gart;\r\nreturn 0;\r\n}\r\nstatic int tegra_gart_remove(struct platform_device *pdev)\r\n{\r\nstruct gart_device *gart = platform_get_drvdata(pdev);\r\nwritel(0, gart->regs + GART_CONFIG);\r\nif (gart->savedata)\r\nvfree(gart->savedata);\r\ngart_handle = NULL;\r\nreturn 0;\r\n}\r\nstatic int tegra_gart_init(void)\r\n{\r\nreturn platform_driver_register(&tegra_gart_driver);\r\n}\r\nstatic void __exit tegra_gart_exit(void)\r\n{\r\nplatform_driver_unregister(&tegra_gart_driver);\r\n}
