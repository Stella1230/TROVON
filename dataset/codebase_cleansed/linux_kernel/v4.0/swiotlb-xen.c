static unsigned long dma_alloc_coherent_mask(struct device *dev,\r\ngfp_t gfp)\r\n{\r\nunsigned long dma_mask = 0;\r\ndma_mask = dev->coherent_dma_mask;\r\nif (!dma_mask)\r\ndma_mask = (gfp & GFP_DMA) ? DMA_BIT_MASK(24) : DMA_BIT_MASK(32);\r\nreturn dma_mask;\r\n}\r\nstatic inline dma_addr_t xen_phys_to_bus(phys_addr_t paddr)\r\n{\r\nunsigned long mfn = pfn_to_mfn(PFN_DOWN(paddr));\r\ndma_addr_t dma = (dma_addr_t)mfn << PAGE_SHIFT;\r\ndma |= paddr & ~PAGE_MASK;\r\nreturn dma;\r\n}\r\nstatic inline phys_addr_t xen_bus_to_phys(dma_addr_t baddr)\r\n{\r\nunsigned long pfn = mfn_to_pfn(PFN_DOWN(baddr));\r\ndma_addr_t dma = (dma_addr_t)pfn << PAGE_SHIFT;\r\nphys_addr_t paddr = dma;\r\npaddr |= baddr & ~PAGE_MASK;\r\nreturn paddr;\r\n}\r\nstatic inline dma_addr_t xen_virt_to_bus(void *address)\r\n{\r\nreturn xen_phys_to_bus(virt_to_phys(address));\r\n}\r\nstatic int check_pages_physically_contiguous(unsigned long pfn,\r\nunsigned int offset,\r\nsize_t length)\r\n{\r\nunsigned long next_mfn;\r\nint i;\r\nint nr_pages;\r\nnext_mfn = pfn_to_mfn(pfn);\r\nnr_pages = (offset + length + PAGE_SIZE-1) >> PAGE_SHIFT;\r\nfor (i = 1; i < nr_pages; i++) {\r\nif (pfn_to_mfn(++pfn) != ++next_mfn)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic inline int range_straddles_page_boundary(phys_addr_t p, size_t size)\r\n{\r\nunsigned long pfn = PFN_DOWN(p);\r\nunsigned int offset = p & ~PAGE_MASK;\r\nif (offset + size <= PAGE_SIZE)\r\nreturn 0;\r\nif (check_pages_physically_contiguous(pfn, offset, size))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int is_xen_swiotlb_buffer(dma_addr_t dma_addr)\r\n{\r\nunsigned long mfn = PFN_DOWN(dma_addr);\r\nunsigned long pfn = mfn_to_local_pfn(mfn);\r\nphys_addr_t paddr;\r\nif (pfn_valid(pfn)) {\r\npaddr = PFN_PHYS(pfn);\r\nreturn paddr >= virt_to_phys(xen_io_tlb_start) &&\r\npaddr < virt_to_phys(xen_io_tlb_end);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nxen_swiotlb_fixup(void *buf, size_t size, unsigned long nslabs)\r\n{\r\nint i, rc;\r\nint dma_bits;\r\ndma_addr_t dma_handle;\r\nphys_addr_t p = virt_to_phys(buf);\r\ndma_bits = get_order(IO_TLB_SEGSIZE << IO_TLB_SHIFT) + PAGE_SHIFT;\r\ni = 0;\r\ndo {\r\nint slabs = min(nslabs - i, (unsigned long)IO_TLB_SEGSIZE);\r\ndo {\r\nrc = xen_create_contiguous_region(\r\np + (i << IO_TLB_SHIFT),\r\nget_order(slabs << IO_TLB_SHIFT),\r\ndma_bits, &dma_handle);\r\n} while (rc && dma_bits++ < max_dma_bits);\r\nif (rc)\r\nreturn rc;\r\ni += slabs;\r\n} while (i < nslabs);\r\nreturn 0;\r\n}\r\nstatic unsigned long xen_set_nslabs(unsigned long nr_tbl)\r\n{\r\nif (!nr_tbl) {\r\nxen_io_tlb_nslabs = (64 * 1024 * 1024 >> IO_TLB_SHIFT);\r\nxen_io_tlb_nslabs = ALIGN(xen_io_tlb_nslabs, IO_TLB_SEGSIZE);\r\n} else\r\nxen_io_tlb_nslabs = nr_tbl;\r\nreturn xen_io_tlb_nslabs << IO_TLB_SHIFT;\r\n}\r\nstatic const char *xen_swiotlb_error(enum xen_swiotlb_err err)\r\n{\r\nswitch (err) {\r\ncase XEN_SWIOTLB_ENOMEM:\r\nreturn "Cannot allocate Xen-SWIOTLB buffer\n";\r\ncase XEN_SWIOTLB_EFIXUP:\r\nreturn "Failed to get contiguous memory for DMA from Xen!\n"\\r\n"You either: don't have the permissions, do not have"\\r\n" enough free memory under 4GB, or the hypervisor memory"\\r\n" is too fragmented!";\r\ndefault:\r\nbreak;\r\n}\r\nreturn "";\r\n}\r\nint __ref xen_swiotlb_init(int verbose, bool early)\r\n{\r\nunsigned long bytes, order;\r\nint rc = -ENOMEM;\r\nenum xen_swiotlb_err m_ret = XEN_SWIOTLB_UNKNOWN;\r\nunsigned int repeat = 3;\r\nxen_io_tlb_nslabs = swiotlb_nr_tbl();\r\nretry:\r\nbytes = xen_set_nslabs(xen_io_tlb_nslabs);\r\norder = get_order(xen_io_tlb_nslabs << IO_TLB_SHIFT);\r\nif (early)\r\nxen_io_tlb_start = alloc_bootmem_pages(PAGE_ALIGN(bytes));\r\nelse {\r\n#define SLABS_PER_PAGE (1 << (PAGE_SHIFT - IO_TLB_SHIFT))\r\n#define IO_TLB_MIN_SLABS ((1<<20) >> IO_TLB_SHIFT)\r\nwhile ((SLABS_PER_PAGE << order) > IO_TLB_MIN_SLABS) {\r\nxen_io_tlb_start = (void *)__get_free_pages(__GFP_NOWARN, order);\r\nif (xen_io_tlb_start)\r\nbreak;\r\norder--;\r\n}\r\nif (order != get_order(bytes)) {\r\npr_warn("Warning: only able to allocate %ld MB for software IO TLB\n",\r\n(PAGE_SIZE << order) >> 20);\r\nxen_io_tlb_nslabs = SLABS_PER_PAGE << order;\r\nbytes = xen_io_tlb_nslabs << IO_TLB_SHIFT;\r\n}\r\n}\r\nif (!xen_io_tlb_start) {\r\nm_ret = XEN_SWIOTLB_ENOMEM;\r\ngoto error;\r\n}\r\nxen_io_tlb_end = xen_io_tlb_start + bytes;\r\nrc = xen_swiotlb_fixup(xen_io_tlb_start,\r\nbytes,\r\nxen_io_tlb_nslabs);\r\nif (rc) {\r\nif (early)\r\nfree_bootmem(__pa(xen_io_tlb_start), PAGE_ALIGN(bytes));\r\nelse {\r\nfree_pages((unsigned long)xen_io_tlb_start, order);\r\nxen_io_tlb_start = NULL;\r\n}\r\nm_ret = XEN_SWIOTLB_EFIXUP;\r\ngoto error;\r\n}\r\nstart_dma_addr = xen_virt_to_bus(xen_io_tlb_start);\r\nif (early) {\r\nif (swiotlb_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs,\r\nverbose))\r\npanic("Cannot allocate SWIOTLB buffer");\r\nrc = 0;\r\n} else\r\nrc = swiotlb_late_init_with_tbl(xen_io_tlb_start, xen_io_tlb_nslabs);\r\nreturn rc;\r\nerror:\r\nif (repeat--) {\r\nxen_io_tlb_nslabs = max(1024UL,\r\n(xen_io_tlb_nslabs >> 1));\r\npr_info("Lowering to %luMB\n",\r\n(xen_io_tlb_nslabs << IO_TLB_SHIFT) >> 20);\r\ngoto retry;\r\n}\r\npr_err("%s (rc:%d)\n", xen_swiotlb_error(m_ret), rc);\r\nif (early)\r\npanic("%s (rc:%d)", xen_swiotlb_error(m_ret), rc);\r\nelse\r\nfree_pages((unsigned long)xen_io_tlb_start, order);\r\nreturn rc;\r\n}\r\nvoid *\r\nxen_swiotlb_alloc_coherent(struct device *hwdev, size_t size,\r\ndma_addr_t *dma_handle, gfp_t flags,\r\nstruct dma_attrs *attrs)\r\n{\r\nvoid *ret;\r\nint order = get_order(size);\r\nu64 dma_mask = DMA_BIT_MASK(32);\r\nphys_addr_t phys;\r\ndma_addr_t dev_addr;\r\nflags &= ~(__GFP_DMA | __GFP_HIGHMEM);\r\nif (dma_alloc_from_coherent(hwdev, size, dma_handle, &ret))\r\nreturn ret;\r\nret = xen_alloc_coherent_pages(hwdev, size, dma_handle, flags, attrs);\r\nif (!ret)\r\nreturn ret;\r\nif (hwdev && hwdev->coherent_dma_mask)\r\ndma_mask = dma_alloc_coherent_mask(hwdev, flags);\r\nphys = *dma_handle;\r\ndev_addr = xen_phys_to_bus(phys);\r\nif (((dev_addr + size - 1 <= dma_mask)) &&\r\n!range_straddles_page_boundary(phys, size))\r\n*dma_handle = dev_addr;\r\nelse {\r\nif (xen_create_contiguous_region(phys, order,\r\nfls64(dma_mask), dma_handle) != 0) {\r\nxen_free_coherent_pages(hwdev, size, ret, (dma_addr_t)phys, attrs);\r\nreturn NULL;\r\n}\r\n}\r\nmemset(ret, 0, size);\r\nreturn ret;\r\n}\r\nvoid\r\nxen_swiotlb_free_coherent(struct device *hwdev, size_t size, void *vaddr,\r\ndma_addr_t dev_addr, struct dma_attrs *attrs)\r\n{\r\nint order = get_order(size);\r\nphys_addr_t phys;\r\nu64 dma_mask = DMA_BIT_MASK(32);\r\nif (dma_release_from_coherent(hwdev, order, vaddr))\r\nreturn;\r\nif (hwdev && hwdev->coherent_dma_mask)\r\ndma_mask = hwdev->coherent_dma_mask;\r\nphys = xen_bus_to_phys(dev_addr);\r\nif (((dev_addr + size - 1 > dma_mask)) ||\r\nrange_straddles_page_boundary(phys, size))\r\nxen_destroy_contiguous_region(phys, order);\r\nxen_free_coherent_pages(hwdev, size, vaddr, (dma_addr_t)phys, attrs);\r\n}\r\ndma_addr_t xen_swiotlb_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nphys_addr_t map, phys = page_to_phys(page) + offset;\r\ndma_addr_t dev_addr = xen_phys_to_bus(phys);\r\nBUG_ON(dir == DMA_NONE);\r\nif (dma_capable(dev, dev_addr, size) &&\r\n!range_straddles_page_boundary(phys, size) &&\r\n!xen_arch_need_swiotlb(dev, PFN_DOWN(phys), PFN_DOWN(dev_addr)) &&\r\n!swiotlb_force) {\r\nxen_dma_map_page(dev, page, dev_addr, offset, size, dir, attrs);\r\nreturn dev_addr;\r\n}\r\ntrace_swiotlb_bounced(dev, dev_addr, size, swiotlb_force);\r\nmap = swiotlb_tbl_map_single(dev, start_dma_addr, phys, size, dir);\r\nif (map == SWIOTLB_MAP_ERROR)\r\nreturn DMA_ERROR_CODE;\r\nxen_dma_map_page(dev, pfn_to_page(map >> PAGE_SHIFT),\r\ndev_addr, map & ~PAGE_MASK, size, dir, attrs);\r\ndev_addr = xen_phys_to_bus(map);\r\nif (!dma_capable(dev, dev_addr, size)) {\r\nswiotlb_tbl_unmap_single(dev, map, size, dir);\r\ndev_addr = 0;\r\n}\r\nreturn dev_addr;\r\n}\r\nstatic void xen_unmap_single(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nphys_addr_t paddr = xen_bus_to_phys(dev_addr);\r\nBUG_ON(dir == DMA_NONE);\r\nxen_dma_unmap_page(hwdev, dev_addr, size, dir, attrs);\r\nif (is_xen_swiotlb_buffer(dev_addr)) {\r\nswiotlb_tbl_unmap_single(hwdev, paddr, size, dir);\r\nreturn;\r\n}\r\nif (dir != DMA_FROM_DEVICE)\r\nreturn;\r\ndma_mark_clean(phys_to_virt(paddr), size);\r\n}\r\nvoid xen_swiotlb_unmap_page(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nxen_unmap_single(hwdev, dev_addr, size, dir, attrs);\r\n}\r\nstatic void\r\nxen_swiotlb_sync_single(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nphys_addr_t paddr = xen_bus_to_phys(dev_addr);\r\nBUG_ON(dir == DMA_NONE);\r\nif (target == SYNC_FOR_CPU)\r\nxen_dma_sync_single_for_cpu(hwdev, dev_addr, size, dir);\r\nif (is_xen_swiotlb_buffer(dev_addr))\r\nswiotlb_tbl_sync_single(hwdev, paddr, size, dir, target);\r\nif (target == SYNC_FOR_DEVICE)\r\nxen_dma_sync_single_for_device(hwdev, dev_addr, size, dir);\r\nif (dir != DMA_FROM_DEVICE)\r\nreturn;\r\ndma_mark_clean(phys_to_virt(paddr), size);\r\n}\r\nvoid\r\nxen_swiotlb_sync_single_for_cpu(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nxen_swiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_CPU);\r\n}\r\nvoid\r\nxen_swiotlb_sync_single_for_device(struct device *hwdev, dma_addr_t dev_addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nxen_swiotlb_sync_single(hwdev, dev_addr, size, dir, SYNC_FOR_DEVICE);\r\n}\r\nint\r\nxen_swiotlb_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nBUG_ON(dir == DMA_NONE);\r\nfor_each_sg(sgl, sg, nelems, i) {\r\nphys_addr_t paddr = sg_phys(sg);\r\ndma_addr_t dev_addr = xen_phys_to_bus(paddr);\r\nif (swiotlb_force ||\r\nxen_arch_need_swiotlb(hwdev, PFN_DOWN(paddr), PFN_DOWN(dev_addr)) ||\r\n!dma_capable(hwdev, dev_addr, sg->length) ||\r\nrange_straddles_page_boundary(paddr, sg->length)) {\r\nphys_addr_t map = swiotlb_tbl_map_single(hwdev,\r\nstart_dma_addr,\r\nsg_phys(sg),\r\nsg->length,\r\ndir);\r\nif (map == SWIOTLB_MAP_ERROR) {\r\ndev_warn(hwdev, "swiotlb buffer is full\n");\r\nxen_swiotlb_unmap_sg_attrs(hwdev, sgl, i, dir,\r\nattrs);\r\nsg_dma_len(sgl) = 0;\r\nreturn 0;\r\n}\r\nxen_dma_map_page(hwdev, pfn_to_page(map >> PAGE_SHIFT),\r\ndev_addr,\r\nmap & ~PAGE_MASK,\r\nsg->length,\r\ndir,\r\nattrs);\r\nsg->dma_address = xen_phys_to_bus(map);\r\n} else {\r\nxen_dma_map_page(hwdev, pfn_to_page(paddr >> PAGE_SHIFT),\r\ndev_addr,\r\npaddr & ~PAGE_MASK,\r\nsg->length,\r\ndir,\r\nattrs);\r\nsg->dma_address = dev_addr;\r\n}\r\nsg_dma_len(sg) = sg->length;\r\n}\r\nreturn nelems;\r\n}\r\nvoid\r\nxen_swiotlb_unmap_sg_attrs(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nstruct dma_attrs *attrs)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nBUG_ON(dir == DMA_NONE);\r\nfor_each_sg(sgl, sg, nelems, i)\r\nxen_unmap_single(hwdev, sg->dma_address, sg_dma_len(sg), dir, attrs);\r\n}\r\nstatic void\r\nxen_swiotlb_sync_sg(struct device *hwdev, struct scatterlist *sgl,\r\nint nelems, enum dma_data_direction dir,\r\nenum dma_sync_target target)\r\n{\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(sgl, sg, nelems, i)\r\nxen_swiotlb_sync_single(hwdev, sg->dma_address,\r\nsg_dma_len(sg), dir, target);\r\n}\r\nvoid\r\nxen_swiotlb_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,\r\nint nelems, enum dma_data_direction dir)\r\n{\r\nxen_swiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_CPU);\r\n}\r\nvoid\r\nxen_swiotlb_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,\r\nint nelems, enum dma_data_direction dir)\r\n{\r\nxen_swiotlb_sync_sg(hwdev, sg, nelems, dir, SYNC_FOR_DEVICE);\r\n}\r\nint\r\nxen_swiotlb_dma_mapping_error(struct device *hwdev, dma_addr_t dma_addr)\r\n{\r\nreturn !dma_addr;\r\n}\r\nint\r\nxen_swiotlb_dma_supported(struct device *hwdev, u64 mask)\r\n{\r\nreturn xen_virt_to_bus(xen_io_tlb_end - 1) <= mask;\r\n}\r\nint\r\nxen_swiotlb_set_dma_mask(struct device *dev, u64 dma_mask)\r\n{\r\nif (!dev->dma_mask || !xen_swiotlb_dma_supported(dev, dma_mask))\r\nreturn -EIO;\r\n*dev->dma_mask = dma_mask;\r\nreturn 0;\r\n}
