static void qxl_ttm_bo_destroy(struct ttm_buffer_object *tbo)\r\n{\r\nstruct qxl_bo *bo;\r\nstruct qxl_device *qdev;\r\nbo = container_of(tbo, struct qxl_bo, tbo);\r\nqdev = (struct qxl_device *)bo->gem_base.dev->dev_private;\r\nqxl_surface_evict(qdev, bo, false);\r\nmutex_lock(&qdev->gem.mutex);\r\nlist_del_init(&bo->list);\r\nmutex_unlock(&qdev->gem.mutex);\r\ndrm_gem_object_release(&bo->gem_base);\r\nkfree(bo);\r\n}\r\nbool qxl_ttm_bo_is_qxl_bo(struct ttm_buffer_object *bo)\r\n{\r\nif (bo->destroy == &qxl_ttm_bo_destroy)\r\nreturn true;\r\nreturn false;\r\n}\r\nvoid qxl_ttm_placement_from_domain(struct qxl_bo *qbo, u32 domain, bool pinned)\r\n{\r\nu32 c = 0;\r\nu32 pflag = pinned ? TTM_PL_FLAG_NO_EVICT : 0;\r\nunsigned i;\r\nqbo->placement.placement = qbo->placements;\r\nqbo->placement.busy_placement = qbo->placements;\r\nif (domain == QXL_GEM_DOMAIN_VRAM)\r\nqbo->placements[c++].flags = TTM_PL_FLAG_CACHED | TTM_PL_FLAG_VRAM | pflag;\r\nif (domain == QXL_GEM_DOMAIN_SURFACE)\r\nqbo->placements[c++].flags = TTM_PL_FLAG_CACHED | TTM_PL_FLAG_PRIV0 | pflag;\r\nif (domain == QXL_GEM_DOMAIN_CPU)\r\nqbo->placements[c++].flags = TTM_PL_MASK_CACHING | TTM_PL_FLAG_SYSTEM | pflag;\r\nif (!c)\r\nqbo->placements[c++].flags = TTM_PL_MASK_CACHING | TTM_PL_FLAG_SYSTEM;\r\nqbo->placement.num_placement = c;\r\nqbo->placement.num_busy_placement = c;\r\nfor (i = 0; i < c; ++i) {\r\nqbo->placements[i].fpfn = 0;\r\nqbo->placements[i].lpfn = 0;\r\n}\r\n}\r\nint qxl_bo_create(struct qxl_device *qdev,\r\nunsigned long size, bool kernel, bool pinned, u32 domain,\r\nstruct qxl_surface *surf,\r\nstruct qxl_bo **bo_ptr)\r\n{\r\nstruct qxl_bo *bo;\r\nenum ttm_bo_type type;\r\nint r;\r\nif (kernel)\r\ntype = ttm_bo_type_kernel;\r\nelse\r\ntype = ttm_bo_type_device;\r\n*bo_ptr = NULL;\r\nbo = kzalloc(sizeof(struct qxl_bo), GFP_KERNEL);\r\nif (bo == NULL)\r\nreturn -ENOMEM;\r\nsize = roundup(size, PAGE_SIZE);\r\nr = drm_gem_object_init(qdev->ddev, &bo->gem_base, size);\r\nif (unlikely(r)) {\r\nkfree(bo);\r\nreturn r;\r\n}\r\nbo->type = domain;\r\nbo->pin_count = pinned ? 1 : 0;\r\nbo->surface_id = 0;\r\nINIT_LIST_HEAD(&bo->list);\r\nif (surf)\r\nbo->surf = *surf;\r\nqxl_ttm_placement_from_domain(bo, domain, pinned);\r\nr = ttm_bo_init(&qdev->mman.bdev, &bo->tbo, size, type,\r\n&bo->placement, 0, !kernel, NULL, size,\r\nNULL, NULL, &qxl_ttm_bo_destroy);\r\nif (unlikely(r != 0)) {\r\nif (r != -ERESTARTSYS)\r\ndev_err(qdev->dev,\r\n"object_init failed for (%lu, 0x%08X)\n",\r\nsize, domain);\r\nreturn r;\r\n}\r\n*bo_ptr = bo;\r\nreturn 0;\r\n}\r\nint qxl_bo_kmap(struct qxl_bo *bo, void **ptr)\r\n{\r\nbool is_iomem;\r\nint r;\r\nif (bo->kptr) {\r\nif (ptr)\r\n*ptr = bo->kptr;\r\nreturn 0;\r\n}\r\nr = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages, &bo->kmap);\r\nif (r)\r\nreturn r;\r\nbo->kptr = ttm_kmap_obj_virtual(&bo->kmap, &is_iomem);\r\nif (ptr)\r\n*ptr = bo->kptr;\r\nreturn 0;\r\n}\r\nvoid *qxl_bo_kmap_atomic_page(struct qxl_device *qdev,\r\nstruct qxl_bo *bo, int page_offset)\r\n{\r\nstruct ttm_mem_type_manager *man = &bo->tbo.bdev->man[bo->tbo.mem.mem_type];\r\nvoid *rptr;\r\nint ret;\r\nstruct io_mapping *map;\r\nif (bo->tbo.mem.mem_type == TTM_PL_VRAM)\r\nmap = qdev->vram_mapping;\r\nelse if (bo->tbo.mem.mem_type == TTM_PL_PRIV0)\r\nmap = qdev->surface_mapping;\r\nelse\r\ngoto fallback;\r\n(void) ttm_mem_io_lock(man, false);\r\nret = ttm_mem_io_reserve(bo->tbo.bdev, &bo->tbo.mem);\r\nttm_mem_io_unlock(man);\r\nreturn io_mapping_map_atomic_wc(map, bo->tbo.mem.bus.offset + page_offset);\r\nfallback:\r\nif (bo->kptr) {\r\nrptr = bo->kptr + (page_offset * PAGE_SIZE);\r\nreturn rptr;\r\n}\r\nret = qxl_bo_kmap(bo, &rptr);\r\nif (ret)\r\nreturn NULL;\r\nrptr += page_offset * PAGE_SIZE;\r\nreturn rptr;\r\n}\r\nvoid qxl_bo_kunmap(struct qxl_bo *bo)\r\n{\r\nif (bo->kptr == NULL)\r\nreturn;\r\nbo->kptr = NULL;\r\nttm_bo_kunmap(&bo->kmap);\r\n}\r\nvoid qxl_bo_kunmap_atomic_page(struct qxl_device *qdev,\r\nstruct qxl_bo *bo, void *pmap)\r\n{\r\nstruct ttm_mem_type_manager *man = &bo->tbo.bdev->man[bo->tbo.mem.mem_type];\r\nstruct io_mapping *map;\r\nif (bo->tbo.mem.mem_type == TTM_PL_VRAM)\r\nmap = qdev->vram_mapping;\r\nelse if (bo->tbo.mem.mem_type == TTM_PL_PRIV0)\r\nmap = qdev->surface_mapping;\r\nelse\r\ngoto fallback;\r\nio_mapping_unmap_atomic(pmap);\r\n(void) ttm_mem_io_lock(man, false);\r\nttm_mem_io_free(bo->tbo.bdev, &bo->tbo.mem);\r\nttm_mem_io_unlock(man);\r\nreturn ;\r\nfallback:\r\nqxl_bo_kunmap(bo);\r\n}\r\nvoid qxl_bo_unref(struct qxl_bo **bo)\r\n{\r\nstruct ttm_buffer_object *tbo;\r\nif ((*bo) == NULL)\r\nreturn;\r\ntbo = &((*bo)->tbo);\r\nttm_bo_unref(&tbo);\r\nif (tbo == NULL)\r\n*bo = NULL;\r\n}\r\nstruct qxl_bo *qxl_bo_ref(struct qxl_bo *bo)\r\n{\r\nttm_bo_reference(&bo->tbo);\r\nreturn bo;\r\n}\r\nint qxl_bo_pin(struct qxl_bo *bo, u32 domain, u64 *gpu_addr)\r\n{\r\nstruct qxl_device *qdev = (struct qxl_device *)bo->gem_base.dev->dev_private;\r\nint r;\r\nif (bo->pin_count) {\r\nbo->pin_count++;\r\nif (gpu_addr)\r\n*gpu_addr = qxl_bo_gpu_offset(bo);\r\nreturn 0;\r\n}\r\nqxl_ttm_placement_from_domain(bo, domain, true);\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (likely(r == 0)) {\r\nbo->pin_count = 1;\r\nif (gpu_addr != NULL)\r\n*gpu_addr = qxl_bo_gpu_offset(bo);\r\n}\r\nif (unlikely(r != 0))\r\ndev_err(qdev->dev, "%p pin failed\n", bo);\r\nreturn r;\r\n}\r\nint qxl_bo_unpin(struct qxl_bo *bo)\r\n{\r\nstruct qxl_device *qdev = (struct qxl_device *)bo->gem_base.dev->dev_private;\r\nint r, i;\r\nif (!bo->pin_count) {\r\ndev_warn(qdev->dev, "%p unpin not necessary\n", bo);\r\nreturn 0;\r\n}\r\nbo->pin_count--;\r\nif (bo->pin_count)\r\nreturn 0;\r\nfor (i = 0; i < bo->placement.num_placement; i++)\r\nbo->placements[i].flags &= ~TTM_PL_FLAG_NO_EVICT;\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (unlikely(r != 0))\r\ndev_err(qdev->dev, "%p validate failed for unpin\n", bo);\r\nreturn r;\r\n}\r\nvoid qxl_bo_force_delete(struct qxl_device *qdev)\r\n{\r\nstruct qxl_bo *bo, *n;\r\nif (list_empty(&qdev->gem.objects))\r\nreturn;\r\ndev_err(qdev->dev, "Userspace still has active objects !\n");\r\nlist_for_each_entry_safe(bo, n, &qdev->gem.objects, list) {\r\nmutex_lock(&qdev->ddev->struct_mutex);\r\ndev_err(qdev->dev, "%p %p %lu %lu force free\n",\r\n&bo->gem_base, bo, (unsigned long)bo->gem_base.size,\r\n*((unsigned long *)&bo->gem_base.refcount));\r\nmutex_lock(&qdev->gem.mutex);\r\nlist_del_init(&bo->list);\r\nmutex_unlock(&qdev->gem.mutex);\r\ndrm_gem_object_unreference(&bo->gem_base);\r\nmutex_unlock(&qdev->ddev->struct_mutex);\r\n}\r\n}\r\nint qxl_bo_init(struct qxl_device *qdev)\r\n{\r\nreturn qxl_ttm_init(qdev);\r\n}\r\nvoid qxl_bo_fini(struct qxl_device *qdev)\r\n{\r\nqxl_ttm_fini(qdev);\r\n}\r\nint qxl_bo_check_id(struct qxl_device *qdev, struct qxl_bo *bo)\r\n{\r\nint ret;\r\nif (bo->type == QXL_GEM_DOMAIN_SURFACE && bo->surface_id == 0) {\r\nret = qxl_surface_id_alloc(qdev, bo);\r\nif (ret)\r\nreturn ret;\r\nret = qxl_hw_surface_alloc(qdev, bo, NULL);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nint qxl_surf_evict(struct qxl_device *qdev)\r\n{\r\nreturn ttm_bo_evict_mm(&qdev->mman.bdev, TTM_PL_PRIV0);\r\n}\r\nint qxl_vram_evict(struct qxl_device *qdev)\r\n{\r\nreturn ttm_bo_evict_mm(&qdev->mman.bdev, TTM_PL_VRAM);\r\n}
