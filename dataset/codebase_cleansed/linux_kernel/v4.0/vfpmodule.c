static bool vfp_state_in_hw(unsigned int cpu, struct thread_info *thread)\r\n{\r\n#ifdef CONFIG_SMP\r\nif (thread->vfpstate.hard.cpu != cpu)\r\nreturn false;\r\n#endif\r\nreturn vfp_current_hw_state[cpu] == &thread->vfpstate;\r\n}\r\nstatic void vfp_force_reload(unsigned int cpu, struct thread_info *thread)\r\n{\r\nif (vfp_state_in_hw(cpu, thread)) {\r\nfmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);\r\nvfp_current_hw_state[cpu] = NULL;\r\n}\r\n#ifdef CONFIG_SMP\r\nthread->vfpstate.hard.cpu = NR_CPUS;\r\n#endif\r\n}\r\nstatic void vfp_thread_flush(struct thread_info *thread)\r\n{\r\nunion vfp_state *vfp = &thread->vfpstate;\r\nunsigned int cpu;\r\ncpu = get_cpu();\r\nif (vfp_current_hw_state[cpu] == vfp)\r\nvfp_current_hw_state[cpu] = NULL;\r\nfmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);\r\nput_cpu();\r\nmemset(vfp, 0, sizeof(union vfp_state));\r\nvfp->hard.fpexc = FPEXC_EN;\r\nvfp->hard.fpscr = FPSCR_ROUND_NEAREST;\r\n#ifdef CONFIG_SMP\r\nvfp->hard.cpu = NR_CPUS;\r\n#endif\r\n}\r\nstatic void vfp_thread_exit(struct thread_info *thread)\r\n{\r\nunion vfp_state *vfp = &thread->vfpstate;\r\nunsigned int cpu = get_cpu();\r\nif (vfp_current_hw_state[cpu] == vfp)\r\nvfp_current_hw_state[cpu] = NULL;\r\nput_cpu();\r\n}\r\nstatic void vfp_thread_copy(struct thread_info *thread)\r\n{\r\nstruct thread_info *parent = current_thread_info();\r\nvfp_sync_hwstate(parent);\r\nthread->vfpstate = parent->vfpstate;\r\n#ifdef CONFIG_SMP\r\nthread->vfpstate.hard.cpu = NR_CPUS;\r\n#endif\r\n}\r\nstatic int vfp_notifier(struct notifier_block *self, unsigned long cmd, void *v)\r\n{\r\nstruct thread_info *thread = v;\r\nu32 fpexc;\r\n#ifdef CONFIG_SMP\r\nunsigned int cpu;\r\n#endif\r\nswitch (cmd) {\r\ncase THREAD_NOTIFY_SWITCH:\r\nfpexc = fmrx(FPEXC);\r\n#ifdef CONFIG_SMP\r\ncpu = thread->cpu;\r\nif ((fpexc & FPEXC_EN) && vfp_current_hw_state[cpu])\r\nvfp_save_state(vfp_current_hw_state[cpu], fpexc);\r\n#endif\r\nfmxr(FPEXC, fpexc & ~FPEXC_EN);\r\nbreak;\r\ncase THREAD_NOTIFY_FLUSH:\r\nvfp_thread_flush(thread);\r\nbreak;\r\ncase THREAD_NOTIFY_EXIT:\r\nvfp_thread_exit(thread);\r\nbreak;\r\ncase THREAD_NOTIFY_COPY:\r\nvfp_thread_copy(thread);\r\nbreak;\r\n}\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic void vfp_raise_sigfpe(unsigned int sicode, struct pt_regs *regs)\r\n{\r\nsiginfo_t info;\r\nmemset(&info, 0, sizeof(info));\r\ninfo.si_signo = SIGFPE;\r\ninfo.si_code = sicode;\r\ninfo.si_addr = (void __user *)(instruction_pointer(regs) - 4);\r\ncurrent->thread.error_code = 0;\r\ncurrent->thread.trap_no = 6;\r\nsend_sig_info(SIGFPE, &info, current);\r\n}\r\nstatic void vfp_panic(char *reason, u32 inst)\r\n{\r\nint i;\r\npr_err("VFP: Error: %s\n", reason);\r\npr_err("VFP: EXC 0x%08x SCR 0x%08x INST 0x%08x\n",\r\nfmrx(FPEXC), fmrx(FPSCR), inst);\r\nfor (i = 0; i < 32; i += 2)\r\npr_err("VFP: s%2u: 0x%08x s%2u: 0x%08x\n",\r\ni, vfp_get_float(i), i+1, vfp_get_float(i+1));\r\n}\r\nstatic void vfp_raise_exceptions(u32 exceptions, u32 inst, u32 fpscr, struct pt_regs *regs)\r\n{\r\nint si_code = 0;\r\npr_debug("VFP: raising exceptions %08x\n", exceptions);\r\nif (exceptions == VFP_EXCEPTION_ERROR) {\r\nvfp_panic("unhandled bounce", inst);\r\nvfp_raise_sigfpe(0, regs);\r\nreturn;\r\n}\r\nif (exceptions & (FPSCR_N|FPSCR_Z|FPSCR_C|FPSCR_V))\r\nfpscr &= ~(FPSCR_N|FPSCR_Z|FPSCR_C|FPSCR_V);\r\nfpscr |= exceptions;\r\nfmxr(FPSCR, fpscr);\r\n#define RAISE(stat,en,sig) \\r\nif (exceptions & stat && fpscr & en) \\r\nsi_code = sig;\r\nRAISE(FPSCR_DZC, FPSCR_DZE, FPE_FLTDIV);\r\nRAISE(FPSCR_IXC, FPSCR_IXE, FPE_FLTRES);\r\nRAISE(FPSCR_UFC, FPSCR_UFE, FPE_FLTUND);\r\nRAISE(FPSCR_OFC, FPSCR_OFE, FPE_FLTOVF);\r\nRAISE(FPSCR_IOC, FPSCR_IOE, FPE_FLTINV);\r\nif (si_code)\r\nvfp_raise_sigfpe(si_code, regs);\r\n}\r\nstatic u32 vfp_emulate_instruction(u32 inst, u32 fpscr, struct pt_regs *regs)\r\n{\r\nu32 exceptions = VFP_EXCEPTION_ERROR;\r\npr_debug("VFP: emulate: INST=0x%08x SCR=0x%08x\n", inst, fpscr);\r\nif (INST_CPRTDO(inst)) {\r\nif (!INST_CPRT(inst)) {\r\nif (vfp_single(inst)) {\r\nexceptions = vfp_single_cpdo(inst, fpscr);\r\n} else {\r\nexceptions = vfp_double_cpdo(inst, fpscr);\r\n}\r\n} else {\r\n}\r\n} else {\r\n}\r\nreturn exceptions & ~VFP_NAN_FLAG;\r\n}\r\nvoid VFP_bounce(u32 trigger, u32 fpexc, struct pt_regs *regs)\r\n{\r\nu32 fpscr, orig_fpscr, fpsid, exceptions;\r\npr_debug("VFP: bounce: trigger %08x fpexc %08x\n", trigger, fpexc);\r\nfmxr(FPEXC, fpexc & ~(FPEXC_EX|FPEXC_DEX|FPEXC_FP2V|FPEXC_VV|FPEXC_TRAP_MASK));\r\nfpsid = fmrx(FPSID);\r\norig_fpscr = fpscr = fmrx(FPSCR);\r\nif ((fpsid & FPSID_ARCH_MASK) == (1 << FPSID_ARCH_BIT)\r\n&& (fpscr & FPSCR_IXE)) {\r\ngoto emulate;\r\n}\r\nif (fpexc & FPEXC_EX) {\r\n#ifndef CONFIG_CPU_FEROCEON\r\ntrigger = fmrx(FPINST);\r\nregs->ARM_pc -= 4;\r\n#endif\r\n} else if (!(fpexc & FPEXC_DEX)) {\r\nvfp_raise_exceptions(VFP_EXCEPTION_ERROR, trigger, fpscr, regs);\r\ngoto exit;\r\n}\r\nif (fpexc & (FPEXC_EX | FPEXC_VV)) {\r\nu32 len;\r\nlen = fpexc + (1 << FPEXC_LENGTH_BIT);\r\nfpscr &= ~FPSCR_LENGTH_MASK;\r\nfpscr |= (len & FPEXC_LENGTH_MASK) << (FPSCR_LENGTH_BIT - FPEXC_LENGTH_BIT);\r\n}\r\nexceptions = vfp_emulate_instruction(trigger, fpscr, regs);\r\nif (exceptions)\r\nvfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);\r\nif ((fpexc & (FPEXC_EX | FPEXC_FP2V)) != (FPEXC_EX | FPEXC_FP2V))\r\ngoto exit;\r\nbarrier();\r\ntrigger = fmrx(FPINST2);\r\nemulate:\r\nexceptions = vfp_emulate_instruction(trigger, orig_fpscr, regs);\r\nif (exceptions)\r\nvfp_raise_exceptions(exceptions, trigger, orig_fpscr, regs);\r\nexit:\r\npreempt_enable();\r\n}\r\nstatic void vfp_enable(void *unused)\r\n{\r\nu32 access;\r\nBUG_ON(preemptible());\r\naccess = get_copro_access();\r\nset_copro_access(access | CPACC_FULL(10) | CPACC_FULL(11));\r\n}\r\nstatic int vfp_pm_suspend(void)\r\n{\r\nstruct thread_info *ti = current_thread_info();\r\nu32 fpexc = fmrx(FPEXC);\r\nif (fpexc & FPEXC_EN) {\r\npr_debug("%s: saving vfp state\n", __func__);\r\nvfp_save_state(&ti->vfpstate, fpexc);\r\nfmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);\r\n} else if (vfp_current_hw_state[ti->cpu]) {\r\n#ifndef CONFIG_SMP\r\nfmxr(FPEXC, fpexc | FPEXC_EN);\r\nvfp_save_state(vfp_current_hw_state[ti->cpu], fpexc);\r\nfmxr(FPEXC, fpexc);\r\n#endif\r\n}\r\nvfp_current_hw_state[ti->cpu] = NULL;\r\nreturn 0;\r\n}\r\nstatic void vfp_pm_resume(void)\r\n{\r\nvfp_enable(NULL);\r\nfmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);\r\n}\r\nstatic int vfp_cpu_pm_notifier(struct notifier_block *self, unsigned long cmd,\r\nvoid *v)\r\n{\r\nswitch (cmd) {\r\ncase CPU_PM_ENTER:\r\nvfp_pm_suspend();\r\nbreak;\r\ncase CPU_PM_ENTER_FAILED:\r\ncase CPU_PM_EXIT:\r\nvfp_pm_resume();\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void vfp_pm_init(void)\r\n{\r\ncpu_pm_register_notifier(&vfp_cpu_pm_notifier_block);\r\n}\r\nstatic inline void vfp_pm_init(void) { }\r\nvoid vfp_sync_hwstate(struct thread_info *thread)\r\n{\r\nunsigned int cpu = get_cpu();\r\nif (vfp_state_in_hw(cpu, thread)) {\r\nu32 fpexc = fmrx(FPEXC);\r\nfmxr(FPEXC, fpexc | FPEXC_EN);\r\nvfp_save_state(&thread->vfpstate, fpexc | FPEXC_EN);\r\nfmxr(FPEXC, fpexc);\r\n}\r\nput_cpu();\r\n}\r\nvoid vfp_flush_hwstate(struct thread_info *thread)\r\n{\r\nunsigned int cpu = get_cpu();\r\nvfp_force_reload(cpu, thread);\r\nput_cpu();\r\n}\r\nint vfp_preserve_user_clear_hwstate(struct user_vfp __user *ufp,\r\nstruct user_vfp_exc __user *ufp_exc)\r\n{\r\nstruct thread_info *thread = current_thread_info();\r\nstruct vfp_hard_struct *hwstate = &thread->vfpstate.hard;\r\nint err = 0;\r\nvfp_sync_hwstate(thread);\r\nerr |= __copy_to_user(&ufp->fpregs, &hwstate->fpregs,\r\nsizeof(hwstate->fpregs));\r\n__put_user_error(hwstate->fpscr, &ufp->fpscr, err);\r\n__put_user_error(hwstate->fpexc, &ufp_exc->fpexc, err);\r\n__put_user_error(hwstate->fpinst, &ufp_exc->fpinst, err);\r\n__put_user_error(hwstate->fpinst2, &ufp_exc->fpinst2, err);\r\nif (err)\r\nreturn -EFAULT;\r\nvfp_flush_hwstate(thread);\r\nhwstate->fpscr &= ~(FPSCR_LENGTH_MASK | FPSCR_STRIDE_MASK);\r\nreturn 0;\r\n}\r\nint vfp_restore_user_hwstate(struct user_vfp __user *ufp,\r\nstruct user_vfp_exc __user *ufp_exc)\r\n{\r\nstruct thread_info *thread = current_thread_info();\r\nstruct vfp_hard_struct *hwstate = &thread->vfpstate.hard;\r\nunsigned long fpexc;\r\nint err = 0;\r\nvfp_flush_hwstate(thread);\r\nerr |= __copy_from_user(&hwstate->fpregs, &ufp->fpregs,\r\nsizeof(hwstate->fpregs));\r\n__get_user_error(hwstate->fpscr, &ufp->fpscr, err);\r\n__get_user_error(fpexc, &ufp_exc->fpexc, err);\r\nfpexc |= FPEXC_EN;\r\nfpexc &= ~(FPEXC_EX | FPEXC_FP2V);\r\nhwstate->fpexc = fpexc;\r\n__get_user_error(hwstate->fpinst, &ufp_exc->fpinst, err);\r\n__get_user_error(hwstate->fpinst2, &ufp_exc->fpinst2, err);\r\nreturn err ? -EFAULT : 0;\r\n}\r\nstatic int vfp_hotplug(struct notifier_block *b, unsigned long action,\r\nvoid *hcpu)\r\n{\r\nif (action == CPU_DYING || action == CPU_DYING_FROZEN)\r\nvfp_current_hw_state[(long)hcpu] = NULL;\r\nelse if (action == CPU_STARTING || action == CPU_STARTING_FROZEN)\r\nvfp_enable(NULL);\r\nreturn NOTIFY_OK;\r\n}\r\nvoid vfp_kmode_exception(void)\r\n{\r\nif (fmrx(FPEXC) & FPEXC_EN)\r\npr_crit("BUG: unsupported FP instruction in kernel mode\n");\r\nelse\r\npr_crit("BUG: FP instruction issued in kernel mode with FP unit disabled\n");\r\n}\r\nvoid kernel_neon_begin(void)\r\n{\r\nstruct thread_info *thread = current_thread_info();\r\nunsigned int cpu;\r\nu32 fpexc;\r\nBUG_ON(in_interrupt());\r\ncpu = get_cpu();\r\nfpexc = fmrx(FPEXC) | FPEXC_EN;\r\nfmxr(FPEXC, fpexc);\r\nif (vfp_state_in_hw(cpu, thread))\r\nvfp_save_state(&thread->vfpstate, fpexc);\r\n#ifndef CONFIG_SMP\r\nelse if (vfp_current_hw_state[cpu] != NULL)\r\nvfp_save_state(vfp_current_hw_state[cpu], fpexc);\r\n#endif\r\nvfp_current_hw_state[cpu] = NULL;\r\n}\r\nvoid kernel_neon_end(void)\r\n{\r\nfmxr(FPEXC, fmrx(FPEXC) & ~FPEXC_EN);\r\nput_cpu();\r\n}\r\nstatic int __init vfp_init(void)\r\n{\r\nunsigned int vfpsid;\r\nunsigned int cpu_arch = cpu_architecture();\r\nif (cpu_arch >= CPU_ARCH_ARMv6)\r\non_each_cpu(vfp_enable, NULL, 1);\r\nvfp_vector = vfp_testing_entry;\r\nbarrier();\r\nvfpsid = fmrx(FPSID);\r\nbarrier();\r\nvfp_vector = vfp_null_entry;\r\npr_info("VFP support v0.3: ");\r\nif (VFP_arch) {\r\npr_cont("not present\n");\r\nreturn 0;\r\n} else if ((read_cpuid_id() & 0x000f0000) == 0x000f0000) {\r\nVFP_arch = vfpsid & FPSID_CPUID_ARCH_MASK;\r\nVFP_arch >>= FPSID_ARCH_BIT;\r\nif (IS_ENABLED(CONFIG_NEON) &&\r\n(fmrx(MVFR1) & 0x000fff00) == 0x00011100)\r\nelf_hwcap |= HWCAP_NEON;\r\nif (IS_ENABLED(CONFIG_VFPv3)) {\r\nu32 mvfr0 = fmrx(MVFR0);\r\nif (((mvfr0 & MVFR0_DP_MASK) >> MVFR0_DP_BIT) == 0x2 ||\r\n((mvfr0 & MVFR0_SP_MASK) >> MVFR0_SP_BIT) == 0x2) {\r\nelf_hwcap |= HWCAP_VFPv3;\r\nif ((mvfr0 & MVFR0_A_SIMD_MASK) == 1)\r\nelf_hwcap |= HWCAP_VFPv3D16;\r\nelse\r\nelf_hwcap |= HWCAP_VFPD32;\r\n}\r\nif ((fmrx(MVFR1) & 0xf0000000) == 0x10000000)\r\nelf_hwcap |= HWCAP_VFPv4;\r\n}\r\n} else {\r\nif (vfpsid & FPSID_NODOUBLE) {\r\npr_cont("no double precision support\n");\r\nreturn 0;\r\n}\r\nVFP_arch = (vfpsid & FPSID_ARCH_MASK) >> FPSID_ARCH_BIT;\r\n}\r\nhotcpu_notifier(vfp_hotplug, 0);\r\nvfp_vector = vfp_support_entry;\r\nthread_register_notifier(&vfp_notifier_block);\r\nvfp_pm_init();\r\nelf_hwcap |= HWCAP_VFP;\r\npr_cont("implementor %02x architecture %d part %02x variant %x rev %x\n",\r\n(vfpsid & FPSID_IMPLEMENTER_MASK) >> FPSID_IMPLEMENTER_BIT,\r\nVFP_arch,\r\n(vfpsid & FPSID_PART_MASK) >> FPSID_PART_BIT,\r\n(vfpsid & FPSID_VARIANT_MASK) >> FPSID_VARIANT_BIT,\r\n(vfpsid & FPSID_REV_MASK) >> FPSID_REV_BIT);\r\nreturn 0;\r\n}
