int\r\nLNetEQAlloc(unsigned int count, lnet_eq_handler_t callback,\r\nlnet_handle_eq_t *handle)\r\n{\r\nlnet_eq_t *eq;\r\nLASSERT(the_lnet.ln_init);\r\nLASSERT(the_lnet.ln_refcount > 0);\r\ncount = cfs_power2_roundup(count);\r\nif (callback != LNET_EQ_HANDLER_NONE && count != 0) {\r\nCWARN("EQ callback is guaranteed to get every event, do you still want to set eqcount %d for polling event which will have locking overhead? Please contact with developer to confirm\n", count);\r\n}\r\nif (count == 0 && callback == LNET_EQ_HANDLER_NONE)\r\nreturn -EINVAL;\r\neq = lnet_eq_alloc();\r\nif (eq == NULL)\r\nreturn -ENOMEM;\r\nif (count != 0) {\r\nLIBCFS_ALLOC(eq->eq_events, count * sizeof(lnet_event_t));\r\nif (eq->eq_events == NULL)\r\ngoto failed;\r\n}\r\neq->eq_deq_seq = 1;\r\neq->eq_enq_seq = 1;\r\neq->eq_size = count;\r\neq->eq_callback = callback;\r\neq->eq_refs = cfs_percpt_alloc(lnet_cpt_table(),\r\nsizeof(*eq->eq_refs[0]));\r\nif (eq->eq_refs == NULL)\r\ngoto failed;\r\nlnet_res_lock(LNET_LOCK_EX);\r\nlnet_eq_wait_lock();\r\nlnet_res_lh_initialize(&the_lnet.ln_eq_container, &eq->eq_lh);\r\nlist_add(&eq->eq_list, &the_lnet.ln_eq_container.rec_active);\r\nlnet_eq_wait_unlock();\r\nlnet_res_unlock(LNET_LOCK_EX);\r\nlnet_eq2handle(handle, eq);\r\nreturn 0;\r\nfailed:\r\nif (eq->eq_events != NULL)\r\nLIBCFS_FREE(eq->eq_events, count * sizeof(lnet_event_t));\r\nif (eq->eq_refs != NULL)\r\ncfs_percpt_free(eq->eq_refs);\r\nlnet_eq_free(eq);\r\nreturn -ENOMEM;\r\n}\r\nint\r\nLNetEQFree(lnet_handle_eq_t eqh)\r\n{\r\nstruct lnet_eq *eq;\r\nlnet_event_t *events = NULL;\r\nint **refs = NULL;\r\nint *ref;\r\nint rc = 0;\r\nint size = 0;\r\nint i;\r\nLASSERT(the_lnet.ln_init);\r\nLASSERT(the_lnet.ln_refcount > 0);\r\nlnet_res_lock(LNET_LOCK_EX);\r\nlnet_eq_wait_lock();\r\neq = lnet_handle2eq(&eqh);\r\nif (eq == NULL) {\r\nrc = -ENOENT;\r\ngoto out;\r\n}\r\ncfs_percpt_for_each(ref, i, eq->eq_refs) {\r\nLASSERT(*ref >= 0);\r\nif (*ref == 0)\r\ncontinue;\r\nCDEBUG(D_NET, "Event equeue (%d: %d) busy on destroy.\n",\r\ni, *ref);\r\nrc = -EBUSY;\r\ngoto out;\r\n}\r\nevents = eq->eq_events;\r\nsize = eq->eq_size;\r\nrefs = eq->eq_refs;\r\nlnet_res_lh_invalidate(&eq->eq_lh);\r\nlist_del(&eq->eq_list);\r\nlnet_eq_free_locked(eq);\r\nout:\r\nlnet_eq_wait_unlock();\r\nlnet_res_unlock(LNET_LOCK_EX);\r\nif (events != NULL)\r\nLIBCFS_FREE(events, size * sizeof(lnet_event_t));\r\nif (refs != NULL)\r\ncfs_percpt_free(refs);\r\nreturn rc;\r\n}\r\nvoid\r\nlnet_eq_enqueue_event(lnet_eq_t *eq, lnet_event_t *ev)\r\n{\r\nint index;\r\nif (eq->eq_size == 0) {\r\nLASSERT(eq->eq_callback != LNET_EQ_HANDLER_NONE);\r\neq->eq_callback(ev);\r\nreturn;\r\n}\r\nlnet_eq_wait_lock();\r\nev->sequence = eq->eq_enq_seq++;\r\nLASSERT(eq->eq_size == LOWEST_BIT_SET(eq->eq_size));\r\nindex = ev->sequence & (eq->eq_size - 1);\r\neq->eq_events[index] = *ev;\r\nif (eq->eq_callback != LNET_EQ_HANDLER_NONE)\r\neq->eq_callback(ev);\r\nif (waitqueue_active(&the_lnet.ln_eq_waitq))\r\nwake_up_all(&the_lnet.ln_eq_waitq);\r\nlnet_eq_wait_unlock();\r\n}\r\nstatic int\r\nlnet_eq_dequeue_event(lnet_eq_t *eq, lnet_event_t *ev)\r\n{\r\nint new_index = eq->eq_deq_seq & (eq->eq_size - 1);\r\nlnet_event_t *new_event = &eq->eq_events[new_index];\r\nint rc;\r\nif (LNET_SEQ_GT(eq->eq_deq_seq, new_event->sequence))\r\nreturn 0;\r\n*ev = *new_event;\r\nCDEBUG(D_INFO, "event: %p, sequence: %lu, eq->size: %u\n",\r\nnew_event, eq->eq_deq_seq, eq->eq_size);\r\nif (eq->eq_deq_seq == new_event->sequence) {\r\nrc = 1;\r\n} else {\r\nCDEBUG(D_NET, "Event Queue Overflow: eq seq %lu ev seq %lu\n",\r\neq->eq_deq_seq, new_event->sequence);\r\nrc = -EOVERFLOW;\r\n}\r\neq->eq_deq_seq = new_event->sequence + 1;\r\nreturn rc;\r\n}\r\nint\r\nLNetEQGet(lnet_handle_eq_t eventq, lnet_event_t *event)\r\n{\r\nint which;\r\nreturn LNetEQPoll(&eventq, 1, 0,\r\nevent, &which);\r\n}\r\nint\r\nLNetEQWait(lnet_handle_eq_t eventq, lnet_event_t *event)\r\n{\r\nint which;\r\nreturn LNetEQPoll(&eventq, 1, LNET_TIME_FOREVER,\r\nevent, &which);\r\n}\r\nstatic int\r\nlnet_eq_wait_locked(int *timeout_ms)\r\n__must_hold(&the_lnet.ln_eq_wait_lock\r\nint\r\nLNetEQPoll(lnet_handle_eq_t *eventqs, int neq, int timeout_ms,\r\nlnet_event_t *event, int *which)\r\n{\r\nint wait = 1;\r\nint rc;\r\nint i;\r\nLASSERT(the_lnet.ln_init);\r\nLASSERT(the_lnet.ln_refcount > 0);\r\nif (neq < 1)\r\nreturn -ENOENT;\r\nlnet_eq_wait_lock();\r\nfor (;;) {\r\nfor (i = 0; i < neq; i++) {\r\nlnet_eq_t *eq = lnet_handle2eq(&eventqs[i]);\r\nif (eq == NULL) {\r\nlnet_eq_wait_unlock();\r\nreturn -ENOENT;\r\n}\r\nrc = lnet_eq_dequeue_event(eq, event);\r\nif (rc != 0) {\r\nlnet_eq_wait_unlock();\r\n*which = i;\r\nreturn rc;\r\n}\r\n}\r\nif (wait == 0)\r\nbreak;\r\nwait = lnet_eq_wait_locked(&timeout_ms);\r\nif (wait < 0)\r\nbreak;\r\n}\r\nlnet_eq_wait_unlock();\r\nreturn 0;\r\n}
