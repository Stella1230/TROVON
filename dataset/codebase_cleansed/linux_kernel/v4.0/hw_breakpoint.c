static struct bp_cpuinfo *get_bp_info(int cpu, enum bp_type_idx type)\r\n{\r\nreturn per_cpu_ptr(bp_cpuinfo + type, cpu);\r\n}\r\n__weak int hw_breakpoint_weight(struct perf_event *bp)\r\n{\r\nreturn 1;\r\n}\r\nstatic inline enum bp_type_idx find_slot_idx(struct perf_event *bp)\r\n{\r\nif (bp->attr.bp_type & HW_BREAKPOINT_RW)\r\nreturn TYPE_DATA;\r\nreturn TYPE_INST;\r\n}\r\nstatic unsigned int max_task_bp_pinned(int cpu, enum bp_type_idx type)\r\n{\r\nunsigned int *tsk_pinned = get_bp_info(cpu, type)->tsk_pinned;\r\nint i;\r\nfor (i = nr_slots[type] - 1; i >= 0; i--) {\r\nif (tsk_pinned[i] > 0)\r\nreturn i + 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int task_bp_pinned(int cpu, struct perf_event *bp, enum bp_type_idx type)\r\n{\r\nstruct task_struct *tsk = bp->hw.bp_target;\r\nstruct perf_event *iter;\r\nint count = 0;\r\nlist_for_each_entry(iter, &bp_task_head, hw.bp_list) {\r\nif (iter->hw.bp_target == tsk &&\r\nfind_slot_idx(iter) == type &&\r\n(iter->cpu < 0 || cpu == iter->cpu))\r\ncount += hw_breakpoint_weight(iter);\r\n}\r\nreturn count;\r\n}\r\nstatic const struct cpumask *cpumask_of_bp(struct perf_event *bp)\r\n{\r\nif (bp->cpu >= 0)\r\nreturn cpumask_of(bp->cpu);\r\nreturn cpu_possible_mask;\r\n}\r\nstatic void\r\nfetch_bp_busy_slots(struct bp_busy_slots *slots, struct perf_event *bp,\r\nenum bp_type_idx type)\r\n{\r\nconst struct cpumask *cpumask = cpumask_of_bp(bp);\r\nint cpu;\r\nfor_each_cpu(cpu, cpumask) {\r\nstruct bp_cpuinfo *info = get_bp_info(cpu, type);\r\nint nr;\r\nnr = info->cpu_pinned;\r\nif (!bp->hw.bp_target)\r\nnr += max_task_bp_pinned(cpu, type);\r\nelse\r\nnr += task_bp_pinned(cpu, bp, type);\r\nif (nr > slots->pinned)\r\nslots->pinned = nr;\r\nnr = info->flexible;\r\nif (nr > slots->flexible)\r\nslots->flexible = nr;\r\n}\r\n}\r\nstatic void\r\nfetch_this_slot(struct bp_busy_slots *slots, int weight)\r\n{\r\nslots->pinned += weight;\r\n}\r\nstatic void toggle_bp_task_slot(struct perf_event *bp, int cpu,\r\nenum bp_type_idx type, int weight)\r\n{\r\nunsigned int *tsk_pinned = get_bp_info(cpu, type)->tsk_pinned;\r\nint old_idx, new_idx;\r\nold_idx = task_bp_pinned(cpu, bp, type) - 1;\r\nnew_idx = old_idx + weight;\r\nif (old_idx >= 0)\r\ntsk_pinned[old_idx]--;\r\nif (new_idx >= 0)\r\ntsk_pinned[new_idx]++;\r\n}\r\nstatic void\r\ntoggle_bp_slot(struct perf_event *bp, bool enable, enum bp_type_idx type,\r\nint weight)\r\n{\r\nconst struct cpumask *cpumask = cpumask_of_bp(bp);\r\nint cpu;\r\nif (!enable)\r\nweight = -weight;\r\nif (!bp->hw.bp_target) {\r\nget_bp_info(bp->cpu, type)->cpu_pinned += weight;\r\nreturn;\r\n}\r\nfor_each_cpu(cpu, cpumask)\r\ntoggle_bp_task_slot(bp, cpu, type, weight);\r\nif (enable)\r\nlist_add_tail(&bp->hw.bp_list, &bp_task_head);\r\nelse\r\nlist_del(&bp->hw.bp_list);\r\n}\r\n__weak void arch_unregister_hw_breakpoint(struct perf_event *bp)\r\n{\r\n}\r\nstatic int __reserve_bp_slot(struct perf_event *bp)\r\n{\r\nstruct bp_busy_slots slots = {0};\r\nenum bp_type_idx type;\r\nint weight;\r\nif (!constraints_initialized)\r\nreturn -ENOMEM;\r\nif (bp->attr.bp_type == HW_BREAKPOINT_EMPTY ||\r\nbp->attr.bp_type == HW_BREAKPOINT_INVALID)\r\nreturn -EINVAL;\r\ntype = find_slot_idx(bp);\r\nweight = hw_breakpoint_weight(bp);\r\nfetch_bp_busy_slots(&slots, bp, type);\r\nfetch_this_slot(&slots, weight);\r\nif (slots.pinned + (!!slots.flexible) > nr_slots[type])\r\nreturn -ENOSPC;\r\ntoggle_bp_slot(bp, true, type, weight);\r\nreturn 0;\r\n}\r\nint reserve_bp_slot(struct perf_event *bp)\r\n{\r\nint ret;\r\nmutex_lock(&nr_bp_mutex);\r\nret = __reserve_bp_slot(bp);\r\nmutex_unlock(&nr_bp_mutex);\r\nreturn ret;\r\n}\r\nstatic void __release_bp_slot(struct perf_event *bp)\r\n{\r\nenum bp_type_idx type;\r\nint weight;\r\ntype = find_slot_idx(bp);\r\nweight = hw_breakpoint_weight(bp);\r\ntoggle_bp_slot(bp, false, type, weight);\r\n}\r\nvoid release_bp_slot(struct perf_event *bp)\r\n{\r\nmutex_lock(&nr_bp_mutex);\r\narch_unregister_hw_breakpoint(bp);\r\n__release_bp_slot(bp);\r\nmutex_unlock(&nr_bp_mutex);\r\n}\r\nint dbg_reserve_bp_slot(struct perf_event *bp)\r\n{\r\nif (mutex_is_locked(&nr_bp_mutex))\r\nreturn -1;\r\nreturn __reserve_bp_slot(bp);\r\n}\r\nint dbg_release_bp_slot(struct perf_event *bp)\r\n{\r\nif (mutex_is_locked(&nr_bp_mutex))\r\nreturn -1;\r\n__release_bp_slot(bp);\r\nreturn 0;\r\n}\r\nstatic int validate_hw_breakpoint(struct perf_event *bp)\r\n{\r\nint ret;\r\nret = arch_validate_hwbkpt_settings(bp);\r\nif (ret)\r\nreturn ret;\r\nif (arch_check_bp_in_kernelspace(bp)) {\r\nif (bp->attr.exclude_kernel)\r\nreturn -EINVAL;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\n}\r\nreturn 0;\r\n}\r\nint register_perf_hw_breakpoint(struct perf_event *bp)\r\n{\r\nint ret;\r\nret = reserve_bp_slot(bp);\r\nif (ret)\r\nreturn ret;\r\nret = validate_hw_breakpoint(bp);\r\nif (ret)\r\nrelease_bp_slot(bp);\r\nreturn ret;\r\n}\r\nstruct perf_event *\r\nregister_user_hw_breakpoint(struct perf_event_attr *attr,\r\nperf_overflow_handler_t triggered,\r\nvoid *context,\r\nstruct task_struct *tsk)\r\n{\r\nreturn perf_event_create_kernel_counter(attr, -1, tsk, triggered,\r\ncontext);\r\n}\r\nint modify_user_hw_breakpoint(struct perf_event *bp, struct perf_event_attr *attr)\r\n{\r\nu64 old_addr = bp->attr.bp_addr;\r\nu64 old_len = bp->attr.bp_len;\r\nint old_type = bp->attr.bp_type;\r\nint err = 0;\r\nif (irqs_disabled() && bp->ctx && bp->ctx->task == current)\r\n__perf_event_disable(bp);\r\nelse\r\nperf_event_disable(bp);\r\nbp->attr.bp_addr = attr->bp_addr;\r\nbp->attr.bp_type = attr->bp_type;\r\nbp->attr.bp_len = attr->bp_len;\r\nif (attr->disabled)\r\ngoto end;\r\nerr = validate_hw_breakpoint(bp);\r\nif (!err)\r\nperf_event_enable(bp);\r\nif (err) {\r\nbp->attr.bp_addr = old_addr;\r\nbp->attr.bp_type = old_type;\r\nbp->attr.bp_len = old_len;\r\nif (!bp->attr.disabled)\r\nperf_event_enable(bp);\r\nreturn err;\r\n}\r\nend:\r\nbp->attr.disabled = attr->disabled;\r\nreturn 0;\r\n}\r\nvoid unregister_hw_breakpoint(struct perf_event *bp)\r\n{\r\nif (!bp)\r\nreturn;\r\nperf_event_release_kernel(bp);\r\n}\r\nstruct perf_event * __percpu *\r\nregister_wide_hw_breakpoint(struct perf_event_attr *attr,\r\nperf_overflow_handler_t triggered,\r\nvoid *context)\r\n{\r\nstruct perf_event * __percpu *cpu_events, *bp;\r\nlong err = 0;\r\nint cpu;\r\ncpu_events = alloc_percpu(typeof(*cpu_events));\r\nif (!cpu_events)\r\nreturn (void __percpu __force *)ERR_PTR(-ENOMEM);\r\nget_online_cpus();\r\nfor_each_online_cpu(cpu) {\r\nbp = perf_event_create_kernel_counter(attr, cpu, NULL,\r\ntriggered, context);\r\nif (IS_ERR(bp)) {\r\nerr = PTR_ERR(bp);\r\nbreak;\r\n}\r\nper_cpu(*cpu_events, cpu) = bp;\r\n}\r\nput_online_cpus();\r\nif (likely(!err))\r\nreturn cpu_events;\r\nunregister_wide_hw_breakpoint(cpu_events);\r\nreturn (void __percpu __force *)ERR_PTR(err);\r\n}\r\nvoid unregister_wide_hw_breakpoint(struct perf_event * __percpu *cpu_events)\r\n{\r\nint cpu;\r\nfor_each_possible_cpu(cpu)\r\nunregister_hw_breakpoint(per_cpu(*cpu_events, cpu));\r\nfree_percpu(cpu_events);\r\n}\r\nstatic void bp_perf_event_destroy(struct perf_event *event)\r\n{\r\nrelease_bp_slot(event);\r\n}\r\nstatic int hw_breakpoint_event_init(struct perf_event *bp)\r\n{\r\nint err;\r\nif (bp->attr.type != PERF_TYPE_BREAKPOINT)\r\nreturn -ENOENT;\r\nif (has_branch_stack(bp))\r\nreturn -EOPNOTSUPP;\r\nerr = register_perf_hw_breakpoint(bp);\r\nif (err)\r\nreturn err;\r\nbp->destroy = bp_perf_event_destroy;\r\nreturn 0;\r\n}\r\nstatic int hw_breakpoint_add(struct perf_event *bp, int flags)\r\n{\r\nif (!(flags & PERF_EF_START))\r\nbp->hw.state = PERF_HES_STOPPED;\r\nif (is_sampling_event(bp)) {\r\nbp->hw.last_period = bp->hw.sample_period;\r\nperf_swevent_set_period(bp);\r\n}\r\nreturn arch_install_hw_breakpoint(bp);\r\n}\r\nstatic void hw_breakpoint_del(struct perf_event *bp, int flags)\r\n{\r\narch_uninstall_hw_breakpoint(bp);\r\n}\r\nstatic void hw_breakpoint_start(struct perf_event *bp, int flags)\r\n{\r\nbp->hw.state = 0;\r\n}\r\nstatic void hw_breakpoint_stop(struct perf_event *bp, int flags)\r\n{\r\nbp->hw.state = PERF_HES_STOPPED;\r\n}\r\nint __init init_hw_breakpoint(void)\r\n{\r\nint cpu, err_cpu;\r\nint i;\r\nfor (i = 0; i < TYPE_MAX; i++)\r\nnr_slots[i] = hw_breakpoint_slots(i);\r\nfor_each_possible_cpu(cpu) {\r\nfor (i = 0; i < TYPE_MAX; i++) {\r\nstruct bp_cpuinfo *info = get_bp_info(cpu, i);\r\ninfo->tsk_pinned = kcalloc(nr_slots[i], sizeof(int),\r\nGFP_KERNEL);\r\nif (!info->tsk_pinned)\r\ngoto err_alloc;\r\n}\r\n}\r\nconstraints_initialized = 1;\r\nperf_pmu_register(&perf_breakpoint, "breakpoint", PERF_TYPE_BREAKPOINT);\r\nreturn register_die_notifier(&hw_breakpoint_exceptions_nb);\r\nerr_alloc:\r\nfor_each_possible_cpu(err_cpu) {\r\nfor (i = 0; i < TYPE_MAX; i++)\r\nkfree(get_bp_info(err_cpu, i)->tsk_pinned);\r\nif (err_cpu == cpu)\r\nbreak;\r\n}\r\nreturn -ENOMEM;\r\n}
