static void stex_gettime(__le64 *time)\r\n{\r\nstruct timeval tv;\r\ndo_gettimeofday(&tv);\r\n*time = cpu_to_le64(tv.tv_sec);\r\n}\r\nstatic struct status_msg *stex_get_status(struct st_hba *hba)\r\n{\r\nstruct status_msg *status = hba->status_buffer + hba->status_tail;\r\n++hba->status_tail;\r\nhba->status_tail %= hba->sts_count+1;\r\nreturn status;\r\n}\r\nstatic void stex_invalid_field(struct scsi_cmnd *cmd,\r\nvoid (*done)(struct scsi_cmnd *))\r\n{\r\ncmd->result = (DRIVER_SENSE << 24) | SAM_STAT_CHECK_CONDITION;\r\nscsi_build_sense_buffer(0, cmd->sense_buffer, ILLEGAL_REQUEST, 0x24,\r\n0x0);\r\ndone(cmd);\r\n}\r\nstatic struct req_msg *stex_alloc_req(struct st_hba *hba)\r\n{\r\nstruct req_msg *req = hba->dma_mem + hba->req_head * hba->rq_size;\r\n++hba->req_head;\r\nhba->req_head %= hba->rq_count+1;\r\nreturn req;\r\n}\r\nstatic struct req_msg *stex_ss_alloc_req(struct st_hba *hba)\r\n{\r\nreturn (struct req_msg *)(hba->dma_mem +\r\nhba->req_head * hba->rq_size + sizeof(struct st_msg_header));\r\n}\r\nstatic int stex_map_sg(struct st_hba *hba,\r\nstruct req_msg *req, struct st_ccb *ccb)\r\n{\r\nstruct scsi_cmnd *cmd;\r\nstruct scatterlist *sg;\r\nstruct st_sgtable *dst;\r\nstruct st_sgitem *table;\r\nint i, nseg;\r\ncmd = ccb->cmd;\r\nnseg = scsi_dma_map(cmd);\r\nBUG_ON(nseg < 0);\r\nif (nseg) {\r\ndst = (struct st_sgtable *)req->variable;\r\nccb->sg_count = nseg;\r\ndst->sg_count = cpu_to_le16((u16)nseg);\r\ndst->max_sg_count = cpu_to_le16(hba->host->sg_tablesize);\r\ndst->sz_in_byte = cpu_to_le32(scsi_bufflen(cmd));\r\ntable = (struct st_sgitem *)(dst + 1);\r\nscsi_for_each_sg(cmd, sg, nseg, i) {\r\ntable[i].count = cpu_to_le32((u32)sg_dma_len(sg));\r\ntable[i].addr = cpu_to_le64(sg_dma_address(sg));\r\ntable[i].ctrl = SG_CF_64B | SG_CF_HOST;\r\n}\r\ntable[--i].ctrl |= SG_CF_EOT;\r\n}\r\nreturn nseg;\r\n}\r\nstatic int stex_ss_map_sg(struct st_hba *hba,\r\nstruct req_msg *req, struct st_ccb *ccb)\r\n{\r\nstruct scsi_cmnd *cmd;\r\nstruct scatterlist *sg;\r\nstruct st_sgtable *dst;\r\nstruct st_ss_sgitem *table;\r\nint i, nseg;\r\ncmd = ccb->cmd;\r\nnseg = scsi_dma_map(cmd);\r\nBUG_ON(nseg < 0);\r\nif (nseg) {\r\ndst = (struct st_sgtable *)req->variable;\r\nccb->sg_count = nseg;\r\ndst->sg_count = cpu_to_le16((u16)nseg);\r\ndst->max_sg_count = cpu_to_le16(hba->host->sg_tablesize);\r\ndst->sz_in_byte = cpu_to_le32(scsi_bufflen(cmd));\r\ntable = (struct st_ss_sgitem *)(dst + 1);\r\nscsi_for_each_sg(cmd, sg, nseg, i) {\r\ntable[i].count = cpu_to_le32((u32)sg_dma_len(sg));\r\ntable[i].addr =\r\ncpu_to_le32(sg_dma_address(sg) & 0xffffffff);\r\ntable[i].addr_hi =\r\ncpu_to_le32((sg_dma_address(sg) >> 16) >> 16);\r\n}\r\n}\r\nreturn nseg;\r\n}\r\nstatic void stex_controller_info(struct st_hba *hba, struct st_ccb *ccb)\r\n{\r\nstruct st_frame *p;\r\nsize_t count = sizeof(struct st_frame);\r\np = hba->copy_buffer;\r\nscsi_sg_copy_to_buffer(ccb->cmd, p, count);\r\nmemset(p->base, 0, sizeof(u32)*6);\r\n*(unsigned long *)(p->base) = pci_resource_start(hba->pdev, 0);\r\np->rom_addr = 0;\r\np->drv_ver.major = ST_VER_MAJOR;\r\np->drv_ver.minor = ST_VER_MINOR;\r\np->drv_ver.oem = ST_OEM;\r\np->drv_ver.build = ST_BUILD_VER;\r\np->bus = hba->pdev->bus->number;\r\np->slot = hba->pdev->devfn;\r\np->irq_level = 0;\r\np->irq_vec = hba->pdev->irq;\r\np->id = hba->pdev->vendor << 16 | hba->pdev->device;\r\np->subid =\r\nhba->pdev->subsystem_vendor << 16 | hba->pdev->subsystem_device;\r\nscsi_sg_copy_from_buffer(ccb->cmd, p, count);\r\n}\r\nstatic void\r\nstex_send_cmd(struct st_hba *hba, struct req_msg *req, u16 tag)\r\n{\r\nreq->tag = cpu_to_le16(tag);\r\nhba->ccb[tag].req = req;\r\nhba->out_req_cnt++;\r\nwritel(hba->req_head, hba->mmio_base + IMR0);\r\nwritel(MU_INBOUND_DOORBELL_REQHEADCHANGED, hba->mmio_base + IDBL);\r\nreadl(hba->mmio_base + IDBL);\r\n}\r\nstatic void\r\nstex_ss_send_cmd(struct st_hba *hba, struct req_msg *req, u16 tag)\r\n{\r\nstruct scsi_cmnd *cmd;\r\nstruct st_msg_header *msg_h;\r\ndma_addr_t addr;\r\nreq->tag = cpu_to_le16(tag);\r\nhba->ccb[tag].req = req;\r\nhba->out_req_cnt++;\r\ncmd = hba->ccb[tag].cmd;\r\nmsg_h = (struct st_msg_header *)req - 1;\r\nif (likely(cmd)) {\r\nmsg_h->channel = (u8)cmd->device->channel;\r\nmsg_h->timeout = cpu_to_le16(cmd->request->timeout/HZ);\r\n}\r\naddr = hba->dma_handle + hba->req_head * hba->rq_size;\r\naddr += (hba->ccb[tag].sg_count+4)/11;\r\nmsg_h->handle = cpu_to_le64(addr);\r\n++hba->req_head;\r\nhba->req_head %= hba->rq_count+1;\r\nwritel((addr >> 16) >> 16, hba->mmio_base + YH2I_REQ_HI);\r\nreadl(hba->mmio_base + YH2I_REQ_HI);\r\nwritel(addr, hba->mmio_base + YH2I_REQ);\r\nreadl(hba->mmio_base + YH2I_REQ);\r\n}\r\nstatic int\r\nstex_slave_config(struct scsi_device *sdev)\r\n{\r\nsdev->use_10_for_rw = 1;\r\nsdev->use_10_for_ms = 1;\r\nblk_queue_rq_timeout(sdev->request_queue, 60 * HZ);\r\nreturn 0;\r\n}\r\nstatic int\r\nstex_queuecommand_lck(struct scsi_cmnd *cmd, void (*done)(struct scsi_cmnd *))\r\n{\r\nstruct st_hba *hba;\r\nstruct Scsi_Host *host;\r\nunsigned int id, lun;\r\nstruct req_msg *req;\r\nu16 tag;\r\nhost = cmd->device->host;\r\nid = cmd->device->id;\r\nlun = cmd->device->lun;\r\nhba = (struct st_hba *) &host->hostdata[0];\r\nif (unlikely(hba->mu_status == MU_STATE_RESETTING))\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\nswitch (cmd->cmnd[0]) {\r\ncase MODE_SENSE_10:\r\n{\r\nstatic char ms10_caching_page[12] =\r\n{ 0, 0x12, 0, 0, 0, 0, 0, 0, 0x8, 0xa, 0x4, 0 };\r\nunsigned char page;\r\npage = cmd->cmnd[2] & 0x3f;\r\nif (page == 0x8 || page == 0x3f) {\r\nscsi_sg_copy_from_buffer(cmd, ms10_caching_page,\r\nsizeof(ms10_caching_page));\r\ncmd->result = DID_OK << 16 | COMMAND_COMPLETE << 8;\r\ndone(cmd);\r\n} else\r\nstex_invalid_field(cmd, done);\r\nreturn 0;\r\n}\r\ncase REPORT_LUNS:\r\nif (hba->cardtype == st_shasta || id == host->max_id - 1) {\r\nstex_invalid_field(cmd, done);\r\nreturn 0;\r\n}\r\nbreak;\r\ncase TEST_UNIT_READY:\r\nif (id == host->max_id - 1) {\r\ncmd->result = DID_OK << 16 | COMMAND_COMPLETE << 8;\r\ndone(cmd);\r\nreturn 0;\r\n}\r\nbreak;\r\ncase INQUIRY:\r\nif (lun >= host->max_lun) {\r\ncmd->result = DID_NO_CONNECT << 16;\r\ndone(cmd);\r\nreturn 0;\r\n}\r\nif (id != host->max_id - 1)\r\nbreak;\r\nif (!lun && !cmd->device->channel &&\r\n(cmd->cmnd[1] & INQUIRY_EVPD) == 0) {\r\nscsi_sg_copy_from_buffer(cmd, (void *)console_inq_page,\r\nsizeof(console_inq_page));\r\ncmd->result = DID_OK << 16 | COMMAND_COMPLETE << 8;\r\ndone(cmd);\r\n} else\r\nstex_invalid_field(cmd, done);\r\nreturn 0;\r\ncase PASSTHRU_CMD:\r\nif (cmd->cmnd[1] == PASSTHRU_GET_DRVVER) {\r\nstruct st_drvver ver;\r\nsize_t cp_len = sizeof(ver);\r\nver.major = ST_VER_MAJOR;\r\nver.minor = ST_VER_MINOR;\r\nver.oem = ST_OEM;\r\nver.build = ST_BUILD_VER;\r\nver.signature[0] = PASSTHRU_SIGNATURE;\r\nver.console_id = host->max_id - 1;\r\nver.host_no = hba->host->host_no;\r\ncp_len = scsi_sg_copy_from_buffer(cmd, &ver, cp_len);\r\ncmd->result = sizeof(ver) == cp_len ?\r\nDID_OK << 16 | COMMAND_COMPLETE << 8 :\r\nDID_ERROR << 16 | COMMAND_COMPLETE << 8;\r\ndone(cmd);\r\nreturn 0;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\ncmd->scsi_done = done;\r\ntag = cmd->request->tag;\r\nif (unlikely(tag >= host->can_queue))\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\nreq = hba->alloc_rq(hba);\r\nreq->lun = lun;\r\nreq->target = id;\r\nmemcpy(req->cdb, cmd->cmnd, STEX_CDB_LENGTH);\r\nif (cmd->sc_data_direction == DMA_FROM_DEVICE)\r\nreq->data_dir = MSG_DATA_DIR_IN;\r\nelse if (cmd->sc_data_direction == DMA_TO_DEVICE)\r\nreq->data_dir = MSG_DATA_DIR_OUT;\r\nelse\r\nreq->data_dir = MSG_DATA_DIR_ND;\r\nhba->ccb[tag].cmd = cmd;\r\nhba->ccb[tag].sense_bufflen = SCSI_SENSE_BUFFERSIZE;\r\nhba->ccb[tag].sense_buffer = cmd->sense_buffer;\r\nif (!hba->map_sg(hba, req, &hba->ccb[tag])) {\r\nhba->ccb[tag].sg_count = 0;\r\nmemset(&req->variable[0], 0, 8);\r\n}\r\nhba->send(hba, req, tag);\r\nreturn 0;\r\n}\r\nvoid stex_copy_data(struct st_ccb *ccb,\r\nstruct status_msg *resp, unsigned int variable)\r\n{\r\nif (resp->scsi_status != SAM_STAT_GOOD) {\r\nif (ccb->sense_buffer != NULL)\r\nmemcpy(ccb->sense_buffer, resp->variable,\r\nmin(variable, ccb->sense_bufflen));\r\nreturn;\r\n}\r\nif (ccb->cmd == NULL)\r\nreturn;\r\nscsi_sg_copy_from_buffer(ccb->cmd, resp->variable, variable);\r\n}\r\nstatic void stex_check_cmd(struct st_hba *hba,\r\nstruct st_ccb *ccb, struct status_msg *resp)\r\n{\r\nif (ccb->cmd->cmnd[0] == MGT_CMD &&\r\nresp->scsi_status != SAM_STAT_CHECK_CONDITION)\r\nscsi_set_resid(ccb->cmd, scsi_bufflen(ccb->cmd) -\r\nle32_to_cpu(*(__le32 *)&resp->variable[0]));\r\n}\r\nstatic void stex_mu_intr(struct st_hba *hba, u32 doorbell)\r\n{\r\nvoid __iomem *base = hba->mmio_base;\r\nstruct status_msg *resp;\r\nstruct st_ccb *ccb;\r\nunsigned int size;\r\nu16 tag;\r\nif (unlikely(!(doorbell & MU_OUTBOUND_DOORBELL_STATUSHEADCHANGED)))\r\nreturn;\r\nhba->status_head = readl(base + OMR1);\r\nif (unlikely(hba->status_head > hba->sts_count)) {\r\nprintk(KERN_WARNING DRV_NAME "(%s): invalid status head\n",\r\npci_name(hba->pdev));\r\nreturn;\r\n}\r\nif (unlikely(hba->out_req_cnt <= 0 ||\r\n(hba->mu_status == MU_STATE_RESETTING &&\r\nhba->cardtype != st_yosemite))) {\r\nhba->status_tail = hba->status_head;\r\ngoto update_status;\r\n}\r\nwhile (hba->status_tail != hba->status_head) {\r\nresp = stex_get_status(hba);\r\ntag = le16_to_cpu(resp->tag);\r\nif (unlikely(tag >= hba->host->can_queue)) {\r\nprintk(KERN_WARNING DRV_NAME\r\n"(%s): invalid tag\n", pci_name(hba->pdev));\r\ncontinue;\r\n}\r\nhba->out_req_cnt--;\r\nccb = &hba->ccb[tag];\r\nif (unlikely(hba->wait_ccb == ccb))\r\nhba->wait_ccb = NULL;\r\nif (unlikely(ccb->req == NULL)) {\r\nprintk(KERN_WARNING DRV_NAME\r\n"(%s): lagging req\n", pci_name(hba->pdev));\r\ncontinue;\r\n}\r\nsize = resp->payload_sz * sizeof(u32);\r\nif (unlikely(size < sizeof(*resp) - STATUS_VAR_LEN ||\r\nsize > sizeof(*resp))) {\r\nprintk(KERN_WARNING DRV_NAME "(%s): bad status size\n",\r\npci_name(hba->pdev));\r\n} else {\r\nsize -= sizeof(*resp) - STATUS_VAR_LEN;\r\nif (size)\r\nstex_copy_data(ccb, resp, size);\r\n}\r\nccb->req = NULL;\r\nccb->srb_status = resp->srb_status;\r\nccb->scsi_status = resp->scsi_status;\r\nif (likely(ccb->cmd != NULL)) {\r\nif (hba->cardtype == st_yosemite)\r\nstex_check_cmd(hba, ccb, resp);\r\nif (unlikely(ccb->cmd->cmnd[0] == PASSTHRU_CMD &&\r\nccb->cmd->cmnd[1] == PASSTHRU_GET_ADAPTER))\r\nstex_controller_info(hba, ccb);\r\nscsi_dma_unmap(ccb->cmd);\r\nstex_scsi_done(ccb);\r\n} else\r\nccb->req_type = 0;\r\n}\r\nupdate_status:\r\nwritel(hba->status_head, base + IMR1);\r\nreadl(base + IMR1);\r\n}\r\nstatic irqreturn_t stex_intr(int irq, void *__hba)\r\n{\r\nstruct st_hba *hba = __hba;\r\nvoid __iomem *base = hba->mmio_base;\r\nu32 data;\r\nunsigned long flags;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\ndata = readl(base + ODBL);\r\nif (data && data != 0xffffffff) {\r\nwritel(data, base + ODBL);\r\nreadl(base + ODBL);\r\nstex_mu_intr(hba, data);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (unlikely(data & MU_OUTBOUND_DOORBELL_REQUEST_RESET &&\r\nhba->cardtype == st_shasta))\r\nqueue_work(hba->work_q, &hba->reset_work);\r\nreturn IRQ_HANDLED;\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn IRQ_NONE;\r\n}\r\nstatic void stex_ss_mu_intr(struct st_hba *hba)\r\n{\r\nstruct status_msg *resp;\r\nstruct st_ccb *ccb;\r\n__le32 *scratch;\r\nunsigned int size;\r\nint count = 0;\r\nu32 value;\r\nu16 tag;\r\nif (unlikely(hba->out_req_cnt <= 0 ||\r\nhba->mu_status == MU_STATE_RESETTING))\r\nreturn;\r\nwhile (count < hba->sts_count) {\r\nscratch = hba->scratch + hba->status_tail;\r\nvalue = le32_to_cpu(*scratch);\r\nif (unlikely(!(value & SS_STS_NORMAL)))\r\nreturn;\r\nresp = hba->status_buffer + hba->status_tail;\r\n*scratch = 0;\r\n++count;\r\n++hba->status_tail;\r\nhba->status_tail %= hba->sts_count+1;\r\ntag = (u16)value;\r\nif (unlikely(tag >= hba->host->can_queue)) {\r\nprintk(KERN_WARNING DRV_NAME\r\n"(%s): invalid tag\n", pci_name(hba->pdev));\r\ncontinue;\r\n}\r\nhba->out_req_cnt--;\r\nccb = &hba->ccb[tag];\r\nif (unlikely(hba->wait_ccb == ccb))\r\nhba->wait_ccb = NULL;\r\nif (unlikely(ccb->req == NULL)) {\r\nprintk(KERN_WARNING DRV_NAME\r\n"(%s): lagging req\n", pci_name(hba->pdev));\r\ncontinue;\r\n}\r\nccb->req = NULL;\r\nif (likely(value & SS_STS_DONE)) {\r\nccb->srb_status = SRB_STATUS_SUCCESS;\r\nccb->scsi_status = SAM_STAT_GOOD;\r\n} else {\r\nccb->srb_status = resp->srb_status;\r\nccb->scsi_status = resp->scsi_status;\r\nsize = resp->payload_sz * sizeof(u32);\r\nif (unlikely(size < sizeof(*resp) - STATUS_VAR_LEN ||\r\nsize > sizeof(*resp))) {\r\nprintk(KERN_WARNING DRV_NAME\r\n"(%s): bad status size\n",\r\npci_name(hba->pdev));\r\n} else {\r\nsize -= sizeof(*resp) - STATUS_VAR_LEN;\r\nif (size)\r\nstex_copy_data(ccb, resp, size);\r\n}\r\nif (likely(ccb->cmd != NULL))\r\nstex_check_cmd(hba, ccb, resp);\r\n}\r\nif (likely(ccb->cmd != NULL)) {\r\nscsi_dma_unmap(ccb->cmd);\r\nstex_scsi_done(ccb);\r\n} else\r\nccb->req_type = 0;\r\n}\r\n}\r\nstatic irqreturn_t stex_ss_intr(int irq, void *__hba)\r\n{\r\nstruct st_hba *hba = __hba;\r\nvoid __iomem *base = hba->mmio_base;\r\nu32 data;\r\nunsigned long flags;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\ndata = readl(base + YI2H_INT);\r\nif (data && data != 0xffffffff) {\r\nwritel(data, base + YI2H_INT_C);\r\nstex_ss_mu_intr(hba);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (unlikely(data & SS_I2H_REQUEST_RESET))\r\nqueue_work(hba->work_q, &hba->reset_work);\r\nreturn IRQ_HANDLED;\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn IRQ_NONE;\r\n}\r\nstatic int stex_common_handshake(struct st_hba *hba)\r\n{\r\nvoid __iomem *base = hba->mmio_base;\r\nstruct handshake_frame *h;\r\ndma_addr_t status_phys;\r\nu32 data;\r\nunsigned long before;\r\nif (readl(base + OMR0) != MU_HANDSHAKE_SIGNATURE) {\r\nwritel(MU_INBOUND_DOORBELL_HANDSHAKE, base + IDBL);\r\nreadl(base + IDBL);\r\nbefore = jiffies;\r\nwhile (readl(base + OMR0) != MU_HANDSHAKE_SIGNATURE) {\r\nif (time_after(jiffies, before + MU_MAX_DELAY * HZ)) {\r\nprintk(KERN_ERR DRV_NAME\r\n"(%s): no handshake signature\n",\r\npci_name(hba->pdev));\r\nreturn -1;\r\n}\r\nrmb();\r\nmsleep(1);\r\n}\r\n}\r\nudelay(10);\r\ndata = readl(base + OMR1);\r\nif ((data & 0xffff0000) == MU_HANDSHAKE_SIGNATURE_HALF) {\r\ndata &= 0x0000ffff;\r\nif (hba->host->can_queue > data) {\r\nhba->host->can_queue = data;\r\nhba->host->cmd_per_lun = data;\r\n}\r\n}\r\nh = (struct handshake_frame *)hba->status_buffer;\r\nh->rb_phy = cpu_to_le64(hba->dma_handle);\r\nh->req_sz = cpu_to_le16(hba->rq_size);\r\nh->req_cnt = cpu_to_le16(hba->rq_count+1);\r\nh->status_sz = cpu_to_le16(sizeof(struct status_msg));\r\nh->status_cnt = cpu_to_le16(hba->sts_count+1);\r\nstex_gettime(&h->hosttime);\r\nh->partner_type = HMU_PARTNER_TYPE;\r\nif (hba->extra_offset) {\r\nh->extra_offset = cpu_to_le32(hba->extra_offset);\r\nh->extra_size = cpu_to_le32(hba->dma_size - hba->extra_offset);\r\n} else\r\nh->extra_offset = h->extra_size = 0;\r\nstatus_phys = hba->dma_handle + (hba->rq_count+1) * hba->rq_size;\r\nwritel(status_phys, base + IMR0);\r\nreadl(base + IMR0);\r\nwritel((status_phys >> 16) >> 16, base + IMR1);\r\nreadl(base + IMR1);\r\nwritel((status_phys >> 16) >> 16, base + OMR0);\r\nreadl(base + OMR0);\r\nwritel(MU_INBOUND_DOORBELL_HANDSHAKE, base + IDBL);\r\nreadl(base + IDBL);\r\nudelay(10);\r\nbefore = jiffies;\r\nwhile (readl(base + OMR0) != MU_HANDSHAKE_SIGNATURE) {\r\nif (time_after(jiffies, before + MU_MAX_DELAY * HZ)) {\r\nprintk(KERN_ERR DRV_NAME\r\n"(%s): no signature after handshake frame\n",\r\npci_name(hba->pdev));\r\nreturn -1;\r\n}\r\nrmb();\r\nmsleep(1);\r\n}\r\nwritel(0, base + IMR0);\r\nreadl(base + IMR0);\r\nwritel(0, base + OMR0);\r\nreadl(base + OMR0);\r\nwritel(0, base + IMR1);\r\nreadl(base + IMR1);\r\nwritel(0, base + OMR1);\r\nreadl(base + OMR1);\r\nreturn 0;\r\n}\r\nstatic int stex_ss_handshake(struct st_hba *hba)\r\n{\r\nvoid __iomem *base = hba->mmio_base;\r\nstruct st_msg_header *msg_h;\r\nstruct handshake_frame *h;\r\n__le32 *scratch;\r\nu32 data, scratch_size;\r\nunsigned long before;\r\nint ret = 0;\r\nbefore = jiffies;\r\nwhile ((readl(base + YIOA_STATUS) & SS_MU_OPERATIONAL) == 0) {\r\nif (time_after(jiffies, before + MU_MAX_DELAY * HZ)) {\r\nprintk(KERN_ERR DRV_NAME\r\n"(%s): firmware not operational\n",\r\npci_name(hba->pdev));\r\nreturn -1;\r\n}\r\nmsleep(1);\r\n}\r\nmsg_h = (struct st_msg_header *)hba->dma_mem;\r\nmsg_h->handle = cpu_to_le64(hba->dma_handle);\r\nmsg_h->flag = SS_HEAD_HANDSHAKE;\r\nh = (struct handshake_frame *)(msg_h + 1);\r\nh->rb_phy = cpu_to_le64(hba->dma_handle);\r\nh->req_sz = cpu_to_le16(hba->rq_size);\r\nh->req_cnt = cpu_to_le16(hba->rq_count+1);\r\nh->status_sz = cpu_to_le16(sizeof(struct status_msg));\r\nh->status_cnt = cpu_to_le16(hba->sts_count+1);\r\nstex_gettime(&h->hosttime);\r\nh->partner_type = HMU_PARTNER_TYPE;\r\nh->extra_offset = h->extra_size = 0;\r\nscratch_size = (hba->sts_count+1)*sizeof(u32);\r\nh->scratch_size = cpu_to_le32(scratch_size);\r\ndata = readl(base + YINT_EN);\r\ndata &= ~4;\r\nwritel(data, base + YINT_EN);\r\nwritel((hba->dma_handle >> 16) >> 16, base + YH2I_REQ_HI);\r\nreadl(base + YH2I_REQ_HI);\r\nwritel(hba->dma_handle, base + YH2I_REQ);\r\nreadl(base + YH2I_REQ);\r\nscratch = hba->scratch;\r\nbefore = jiffies;\r\nwhile (!(le32_to_cpu(*scratch) & SS_STS_HANDSHAKE)) {\r\nif (time_after(jiffies, before + MU_MAX_DELAY * HZ)) {\r\nprintk(KERN_ERR DRV_NAME\r\n"(%s): no signature after handshake frame\n",\r\npci_name(hba->pdev));\r\nret = -1;\r\nbreak;\r\n}\r\nrmb();\r\nmsleep(1);\r\n}\r\nmemset(scratch, 0, scratch_size);\r\nmsg_h->flag = 0;\r\nreturn ret;\r\n}\r\nstatic int stex_handshake(struct st_hba *hba)\r\n{\r\nint err;\r\nunsigned long flags;\r\nunsigned int mu_status;\r\nerr = (hba->cardtype == st_yel) ?\r\nstex_ss_handshake(hba) : stex_common_handshake(hba);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nmu_status = hba->mu_status;\r\nif (err == 0) {\r\nhba->req_head = 0;\r\nhba->req_tail = 0;\r\nhba->status_head = 0;\r\nhba->status_tail = 0;\r\nhba->out_req_cnt = 0;\r\nhba->mu_status = MU_STATE_STARTED;\r\n} else\r\nhba->mu_status = MU_STATE_FAILED;\r\nif (mu_status == MU_STATE_RESETTING)\r\nwake_up_all(&hba->reset_waitq);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn err;\r\n}\r\nstatic int stex_abort(struct scsi_cmnd *cmd)\r\n{\r\nstruct Scsi_Host *host = cmd->device->host;\r\nstruct st_hba *hba = (struct st_hba *)host->hostdata;\r\nu16 tag = cmd->request->tag;\r\nvoid __iomem *base;\r\nu32 data;\r\nint result = SUCCESS;\r\nunsigned long flags;\r\nscmd_printk(KERN_INFO, cmd, "aborting command\n");\r\nbase = hba->mmio_base;\r\nspin_lock_irqsave(host->host_lock, flags);\r\nif (tag < host->can_queue &&\r\nhba->ccb[tag].req && hba->ccb[tag].cmd == cmd)\r\nhba->wait_ccb = &hba->ccb[tag];\r\nelse\r\ngoto out;\r\nif (hba->cardtype == st_yel) {\r\ndata = readl(base + YI2H_INT);\r\nif (data == 0 || data == 0xffffffff)\r\ngoto fail_out;\r\nwritel(data, base + YI2H_INT_C);\r\nstex_ss_mu_intr(hba);\r\n} else {\r\ndata = readl(base + ODBL);\r\nif (data == 0 || data == 0xffffffff)\r\ngoto fail_out;\r\nwritel(data, base + ODBL);\r\nreadl(base + ODBL);\r\nstex_mu_intr(hba, data);\r\n}\r\nif (hba->wait_ccb == NULL) {\r\nprintk(KERN_WARNING DRV_NAME\r\n"(%s): lost interrupt\n", pci_name(hba->pdev));\r\ngoto out;\r\n}\r\nfail_out:\r\nscsi_dma_unmap(cmd);\r\nhba->wait_ccb->req = NULL;\r\nhba->wait_ccb = NULL;\r\nresult = FAILED;\r\nout:\r\nspin_unlock_irqrestore(host->host_lock, flags);\r\nreturn result;\r\n}\r\nstatic void stex_hard_reset(struct st_hba *hba)\r\n{\r\nstruct pci_bus *bus;\r\nint i;\r\nu16 pci_cmd;\r\nu8 pci_bctl;\r\nfor (i = 0; i < 16; i++)\r\npci_read_config_dword(hba->pdev, i * 4,\r\n&hba->pdev->saved_config_space[i]);\r\nbus = hba->pdev->bus;\r\npci_read_config_byte(bus->self, PCI_BRIDGE_CONTROL, &pci_bctl);\r\npci_bctl |= PCI_BRIDGE_CTL_BUS_RESET;\r\npci_write_config_byte(bus->self, PCI_BRIDGE_CONTROL, pci_bctl);\r\nmsleep(100);\r\npci_bctl &= ~PCI_BRIDGE_CTL_BUS_RESET;\r\npci_write_config_byte(bus->self, PCI_BRIDGE_CONTROL, pci_bctl);\r\nfor (i = 0; i < MU_HARD_RESET_WAIT; i++) {\r\npci_read_config_word(hba->pdev, PCI_COMMAND, &pci_cmd);\r\nif (pci_cmd != 0xffff && (pci_cmd & PCI_COMMAND_MASTER))\r\nbreak;\r\nmsleep(1);\r\n}\r\nssleep(5);\r\nfor (i = 0; i < 16; i++)\r\npci_write_config_dword(hba->pdev, i * 4,\r\nhba->pdev->saved_config_space[i]);\r\n}\r\nstatic int stex_yos_reset(struct st_hba *hba)\r\n{\r\nvoid __iomem *base;\r\nunsigned long flags, before;\r\nint ret = 0;\r\nbase = hba->mmio_base;\r\nwritel(MU_INBOUND_DOORBELL_RESET, base + IDBL);\r\nreadl(base + IDBL);\r\nbefore = jiffies;\r\nwhile (hba->out_req_cnt > 0) {\r\nif (time_after(jiffies, before + ST_INTERNAL_TIMEOUT * HZ)) {\r\nprintk(KERN_WARNING DRV_NAME\r\n"(%s): reset timeout\n", pci_name(hba->pdev));\r\nret = -1;\r\nbreak;\r\n}\r\nmsleep(1);\r\n}\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (ret == -1)\r\nhba->mu_status = MU_STATE_FAILED;\r\nelse\r\nhba->mu_status = MU_STATE_STARTED;\r\nwake_up_all(&hba->reset_waitq);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void stex_ss_reset(struct st_hba *hba)\r\n{\r\nwritel(SS_H2I_INT_RESET, hba->mmio_base + YH2I_INT);\r\nreadl(hba->mmio_base + YH2I_INT);\r\nssleep(5);\r\n}\r\nstatic int stex_do_reset(struct st_hba *hba)\r\n{\r\nstruct st_ccb *ccb;\r\nunsigned long flags;\r\nunsigned int mu_status = MU_STATE_RESETTING;\r\nu16 tag;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nif (hba->mu_status == MU_STATE_STARTING) {\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nprintk(KERN_INFO DRV_NAME "(%s): request reset during init\n",\r\npci_name(hba->pdev));\r\nreturn 0;\r\n}\r\nwhile (hba->mu_status == MU_STATE_RESETTING) {\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nwait_event_timeout(hba->reset_waitq,\r\nhba->mu_status != MU_STATE_RESETTING,\r\nMU_MAX_DELAY * HZ);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nmu_status = hba->mu_status;\r\n}\r\nif (mu_status != MU_STATE_RESETTING) {\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nreturn (mu_status == MU_STATE_STARTED) ? 0 : -1;\r\n}\r\nhba->mu_status = MU_STATE_RESETTING;\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (hba->cardtype == st_yosemite)\r\nreturn stex_yos_reset(hba);\r\nif (hba->cardtype == st_shasta)\r\nstex_hard_reset(hba);\r\nelse if (hba->cardtype == st_yel)\r\nstex_ss_reset(hba);\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nfor (tag = 0; tag < hba->host->can_queue; tag++) {\r\nccb = &hba->ccb[tag];\r\nif (ccb->req == NULL)\r\ncontinue;\r\nccb->req = NULL;\r\nif (ccb->cmd) {\r\nscsi_dma_unmap(ccb->cmd);\r\nccb->cmd->result = DID_RESET << 16;\r\nccb->cmd->scsi_done(ccb->cmd);\r\nccb->cmd = NULL;\r\n}\r\n}\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nif (stex_handshake(hba) == 0)\r\nreturn 0;\r\nprintk(KERN_WARNING DRV_NAME "(%s): resetting: handshake failed\n",\r\npci_name(hba->pdev));\r\nreturn -1;\r\n}\r\nstatic int stex_reset(struct scsi_cmnd *cmd)\r\n{\r\nstruct st_hba *hba;\r\nhba = (struct st_hba *) &cmd->device->host->hostdata[0];\r\nshost_printk(KERN_INFO, cmd->device->host,\r\n"resetting host\n");\r\nreturn stex_do_reset(hba) ? FAILED : SUCCESS;\r\n}\r\nstatic void stex_reset_work(struct work_struct *work)\r\n{\r\nstruct st_hba *hba = container_of(work, struct st_hba, reset_work);\r\nstex_do_reset(hba);\r\n}\r\nstatic int stex_biosparam(struct scsi_device *sdev,\r\nstruct block_device *bdev, sector_t capacity, int geom[])\r\n{\r\nint heads = 255, sectors = 63;\r\nif (capacity < 0x200000) {\r\nheads = 64;\r\nsectors = 32;\r\n}\r\nsector_div(capacity, heads * sectors);\r\ngeom[0] = heads;\r\ngeom[1] = sectors;\r\ngeom[2] = capacity;\r\nreturn 0;\r\n}\r\nstatic int stex_set_dma_mask(struct pci_dev * pdev)\r\n{\r\nint ret;\r\nif (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))\r\n&& !pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64)))\r\nreturn 0;\r\nret = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (!ret)\r\nret = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\nreturn ret;\r\n}\r\nstatic int stex_request_irq(struct st_hba *hba)\r\n{\r\nstruct pci_dev *pdev = hba->pdev;\r\nint status;\r\nif (msi) {\r\nstatus = pci_enable_msi(pdev);\r\nif (status != 0)\r\nprintk(KERN_ERR DRV_NAME\r\n"(%s): error %d setting up MSI\n",\r\npci_name(pdev), status);\r\nelse\r\nhba->msi_enabled = 1;\r\n} else\r\nhba->msi_enabled = 0;\r\nstatus = request_irq(pdev->irq, hba->cardtype == st_yel ?\r\nstex_ss_intr : stex_intr, IRQF_SHARED, DRV_NAME, hba);\r\nif (status != 0) {\r\nif (hba->msi_enabled)\r\npci_disable_msi(pdev);\r\n}\r\nreturn status;\r\n}\r\nstatic void stex_free_irq(struct st_hba *hba)\r\n{\r\nstruct pci_dev *pdev = hba->pdev;\r\nfree_irq(pdev->irq, hba);\r\nif (hba->msi_enabled)\r\npci_disable_msi(pdev);\r\n}\r\nstatic int stex_probe(struct pci_dev *pdev, const struct pci_device_id *id)\r\n{\r\nstruct st_hba *hba;\r\nstruct Scsi_Host *host;\r\nconst struct st_card_info *ci = NULL;\r\nu32 sts_offset, cp_offset, scratch_offset;\r\nint err;\r\nerr = pci_enable_device(pdev);\r\nif (err)\r\nreturn err;\r\npci_set_master(pdev);\r\nhost = scsi_host_alloc(&driver_template, sizeof(struct st_hba));\r\nif (!host) {\r\nprintk(KERN_ERR DRV_NAME "(%s): scsi_host_alloc failed\n",\r\npci_name(pdev));\r\nerr = -ENOMEM;\r\ngoto out_disable;\r\n}\r\nhba = (struct st_hba *)host->hostdata;\r\nmemset(hba, 0, sizeof(struct st_hba));\r\nerr = pci_request_regions(pdev, DRV_NAME);\r\nif (err < 0) {\r\nprintk(KERN_ERR DRV_NAME "(%s): request regions failed\n",\r\npci_name(pdev));\r\ngoto out_scsi_host_put;\r\n}\r\nhba->mmio_base = pci_ioremap_bar(pdev, 0);\r\nif ( !hba->mmio_base) {\r\nprintk(KERN_ERR DRV_NAME "(%s): memory map failed\n",\r\npci_name(pdev));\r\nerr = -ENOMEM;\r\ngoto out_release_regions;\r\n}\r\nerr = stex_set_dma_mask(pdev);\r\nif (err) {\r\nprintk(KERN_ERR DRV_NAME "(%s): set dma mask failed\n",\r\npci_name(pdev));\r\ngoto out_iounmap;\r\n}\r\nhba->cardtype = (unsigned int) id->driver_data;\r\nci = &stex_card_info[hba->cardtype];\r\nsts_offset = scratch_offset = (ci->rq_count+1) * ci->rq_size;\r\nif (hba->cardtype == st_yel)\r\nsts_offset += (ci->sts_count+1) * sizeof(u32);\r\ncp_offset = sts_offset + (ci->sts_count+1) * sizeof(struct status_msg);\r\nhba->dma_size = cp_offset + sizeof(struct st_frame);\r\nif (hba->cardtype == st_seq ||\r\n(hba->cardtype == st_vsc && (pdev->subsystem_device & 1))) {\r\nhba->extra_offset = hba->dma_size;\r\nhba->dma_size += ST_ADDITIONAL_MEM;\r\n}\r\nhba->dma_mem = dma_alloc_coherent(&pdev->dev,\r\nhba->dma_size, &hba->dma_handle, GFP_KERNEL);\r\nif (!hba->dma_mem) {\r\nif (hba->cardtype == st_seq ||\r\n(hba->cardtype == st_vsc && (pdev->subsystem_device & 1))) {\r\nprintk(KERN_WARNING DRV_NAME\r\n"(%s): allocating min buffer for controller\n",\r\npci_name(pdev));\r\nhba->dma_size = hba->extra_offset\r\n+ ST_ADDITIONAL_MEM_MIN;\r\nhba->dma_mem = dma_alloc_coherent(&pdev->dev,\r\nhba->dma_size, &hba->dma_handle, GFP_KERNEL);\r\n}\r\nif (!hba->dma_mem) {\r\nerr = -ENOMEM;\r\nprintk(KERN_ERR DRV_NAME "(%s): dma mem alloc failed\n",\r\npci_name(pdev));\r\ngoto out_iounmap;\r\n}\r\n}\r\nhba->ccb = kcalloc(ci->rq_count, sizeof(struct st_ccb), GFP_KERNEL);\r\nif (!hba->ccb) {\r\nerr = -ENOMEM;\r\nprintk(KERN_ERR DRV_NAME "(%s): ccb alloc failed\n",\r\npci_name(pdev));\r\ngoto out_pci_free;\r\n}\r\nif (hba->cardtype == st_yel)\r\nhba->scratch = (__le32 *)(hba->dma_mem + scratch_offset);\r\nhba->status_buffer = (struct status_msg *)(hba->dma_mem + sts_offset);\r\nhba->copy_buffer = hba->dma_mem + cp_offset;\r\nhba->rq_count = ci->rq_count;\r\nhba->rq_size = ci->rq_size;\r\nhba->sts_count = ci->sts_count;\r\nhba->alloc_rq = ci->alloc_rq;\r\nhba->map_sg = ci->map_sg;\r\nhba->send = ci->send;\r\nhba->mu_status = MU_STATE_STARTING;\r\nif (hba->cardtype == st_yel)\r\nhost->sg_tablesize = 38;\r\nelse\r\nhost->sg_tablesize = 32;\r\nhost->can_queue = ci->rq_count;\r\nhost->cmd_per_lun = ci->rq_count;\r\nhost->max_id = ci->max_id;\r\nhost->max_lun = ci->max_lun;\r\nhost->max_channel = ci->max_channel;\r\nhost->unique_id = host->host_no;\r\nhost->max_cmd_len = STEX_CDB_LENGTH;\r\nhba->host = host;\r\nhba->pdev = pdev;\r\ninit_waitqueue_head(&hba->reset_waitq);\r\nsnprintf(hba->work_q_name, sizeof(hba->work_q_name),\r\n"stex_wq_%d", host->host_no);\r\nhba->work_q = create_singlethread_workqueue(hba->work_q_name);\r\nif (!hba->work_q) {\r\nprintk(KERN_ERR DRV_NAME "(%s): create workqueue failed\n",\r\npci_name(pdev));\r\nerr = -ENOMEM;\r\ngoto out_ccb_free;\r\n}\r\nINIT_WORK(&hba->reset_work, stex_reset_work);\r\nerr = stex_request_irq(hba);\r\nif (err) {\r\nprintk(KERN_ERR DRV_NAME "(%s): request irq failed\n",\r\npci_name(pdev));\r\ngoto out_free_wq;\r\n}\r\nerr = stex_handshake(hba);\r\nif (err)\r\ngoto out_free_irq;\r\nerr = scsi_init_shared_tag_map(host, host->can_queue);\r\nif (err) {\r\nprintk(KERN_ERR DRV_NAME "(%s): init shared queue failed\n",\r\npci_name(pdev));\r\ngoto out_free_irq;\r\n}\r\npci_set_drvdata(pdev, hba);\r\nerr = scsi_add_host(host, &pdev->dev);\r\nif (err) {\r\nprintk(KERN_ERR DRV_NAME "(%s): scsi_add_host failed\n",\r\npci_name(pdev));\r\ngoto out_free_irq;\r\n}\r\nscsi_scan_host(host);\r\nreturn 0;\r\nout_free_irq:\r\nstex_free_irq(hba);\r\nout_free_wq:\r\ndestroy_workqueue(hba->work_q);\r\nout_ccb_free:\r\nkfree(hba->ccb);\r\nout_pci_free:\r\ndma_free_coherent(&pdev->dev, hba->dma_size,\r\nhba->dma_mem, hba->dma_handle);\r\nout_iounmap:\r\niounmap(hba->mmio_base);\r\nout_release_regions:\r\npci_release_regions(pdev);\r\nout_scsi_host_put:\r\nscsi_host_put(host);\r\nout_disable:\r\npci_disable_device(pdev);\r\nreturn err;\r\n}\r\nstatic void stex_hba_stop(struct st_hba *hba)\r\n{\r\nstruct req_msg *req;\r\nstruct st_msg_header *msg_h;\r\nunsigned long flags;\r\nunsigned long before;\r\nu16 tag = 0;\r\nspin_lock_irqsave(hba->host->host_lock, flags);\r\nreq = hba->alloc_rq(hba);\r\nif (hba->cardtype == st_yel) {\r\nmsg_h = (struct st_msg_header *)req - 1;\r\nmemset(msg_h, 0, hba->rq_size);\r\n} else\r\nmemset(req, 0, hba->rq_size);\r\nif (hba->cardtype == st_yosemite || hba->cardtype == st_yel) {\r\nreq->cdb[0] = MGT_CMD;\r\nreq->cdb[1] = MGT_CMD_SIGNATURE;\r\nreq->cdb[2] = CTLR_CONFIG_CMD;\r\nreq->cdb[3] = CTLR_SHUTDOWN;\r\n} else {\r\nreq->cdb[0] = CONTROLLER_CMD;\r\nreq->cdb[1] = CTLR_POWER_STATE_CHANGE;\r\nreq->cdb[2] = CTLR_POWER_SAVING;\r\n}\r\nhba->ccb[tag].cmd = NULL;\r\nhba->ccb[tag].sg_count = 0;\r\nhba->ccb[tag].sense_bufflen = 0;\r\nhba->ccb[tag].sense_buffer = NULL;\r\nhba->ccb[tag].req_type = PASSTHRU_REQ_TYPE;\r\nhba->send(hba, req, tag);\r\nspin_unlock_irqrestore(hba->host->host_lock, flags);\r\nbefore = jiffies;\r\nwhile (hba->ccb[tag].req_type & PASSTHRU_REQ_TYPE) {\r\nif (time_after(jiffies, before + ST_INTERNAL_TIMEOUT * HZ)) {\r\nhba->ccb[tag].req_type = 0;\r\nreturn;\r\n}\r\nmsleep(1);\r\n}\r\n}\r\nstatic void stex_hba_free(struct st_hba *hba)\r\n{\r\nstex_free_irq(hba);\r\ndestroy_workqueue(hba->work_q);\r\niounmap(hba->mmio_base);\r\npci_release_regions(hba->pdev);\r\nkfree(hba->ccb);\r\ndma_free_coherent(&hba->pdev->dev, hba->dma_size,\r\nhba->dma_mem, hba->dma_handle);\r\n}\r\nstatic void stex_remove(struct pci_dev *pdev)\r\n{\r\nstruct st_hba *hba = pci_get_drvdata(pdev);\r\nscsi_remove_host(hba->host);\r\nstex_hba_stop(hba);\r\nstex_hba_free(hba);\r\nscsi_host_put(hba->host);\r\npci_disable_device(pdev);\r\n}\r\nstatic void stex_shutdown(struct pci_dev *pdev)\r\n{\r\nstruct st_hba *hba = pci_get_drvdata(pdev);\r\nstex_hba_stop(hba);\r\n}\r\nstatic int __init stex_init(void)\r\n{\r\nprintk(KERN_INFO DRV_NAME\r\n": Promise SuperTrak EX Driver version: %s\n",\r\nST_DRIVER_VERSION);\r\nreturn pci_register_driver(&stex_pci_driver);\r\n}\r\nstatic void __exit stex_exit(void)\r\n{\r\npci_unregister_driver(&stex_pci_driver);\r\n}
