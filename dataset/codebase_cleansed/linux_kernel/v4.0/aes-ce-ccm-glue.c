static int num_rounds(struct crypto_aes_ctx *ctx)\r\n{\r\nreturn 6 + ctx->key_length / 4;\r\n}\r\nstatic int ccm_setkey(struct crypto_aead *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_aead_ctx(tfm);\r\nint ret;\r\nret = ce_aes_expandkey(ctx, in_key, key_len);\r\nif (!ret)\r\nreturn 0;\r\ntfm->base.crt_flags |= CRYPTO_TFM_RES_BAD_KEY_LEN;\r\nreturn -EINVAL;\r\n}\r\nstatic int ccm_setauthsize(struct crypto_aead *tfm, unsigned int authsize)\r\n{\r\nif ((authsize & 1) || authsize < 4)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int ccm_init_mac(struct aead_request *req, u8 maciv[], u32 msglen)\r\n{\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\n__be32 *n = (__be32 *)&maciv[AES_BLOCK_SIZE - 8];\r\nu32 l = req->iv[0] + 1;\r\nif (l < 2 || l > 8)\r\nreturn -EINVAL;\r\nif (l < 4 && msglen >> (8 * l))\r\nreturn -EOVERFLOW;\r\nn[0] = 0;\r\nn[1] = cpu_to_be32(msglen);\r\nmemcpy(maciv, req->iv, AES_BLOCK_SIZE - l);\r\nmaciv[0] |= (crypto_aead_authsize(aead) - 2) << 2;\r\nif (req->assoclen)\r\nmaciv[0] |= 0x40;\r\nmemset(&req->iv[AES_BLOCK_SIZE - l], 0, l);\r\nreturn 0;\r\n}\r\nstatic void ccm_calculate_auth_mac(struct aead_request *req, u8 mac[])\r\n{\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_aead_ctx(aead);\r\nstruct __packed { __be16 l; __be32 h; u16 len; } ltag;\r\nstruct scatter_walk walk;\r\nu32 len = req->assoclen;\r\nu32 macp = 0;\r\nif (len < 0xff00) {\r\nltag.l = cpu_to_be16(len);\r\nltag.len = 2;\r\n} else {\r\nltag.l = cpu_to_be16(0xfffe);\r\nput_unaligned_be32(len, &ltag.h);\r\nltag.len = 6;\r\n}\r\nce_aes_ccm_auth_data(mac, (u8 *)&ltag, ltag.len, &macp, ctx->key_enc,\r\nnum_rounds(ctx));\r\nscatterwalk_start(&walk, req->assoc);\r\ndo {\r\nu32 n = scatterwalk_clamp(&walk, len);\r\nu8 *p;\r\nif (!n) {\r\nscatterwalk_start(&walk, sg_next(walk.sg));\r\nn = scatterwalk_clamp(&walk, len);\r\n}\r\np = scatterwalk_map(&walk);\r\nce_aes_ccm_auth_data(mac, p, n, &macp, ctx->key_enc,\r\nnum_rounds(ctx));\r\nlen -= n;\r\nscatterwalk_unmap(p);\r\nscatterwalk_advance(&walk, n);\r\nscatterwalk_done(&walk, 0, len);\r\n} while (len);\r\n}\r\nstatic int ccm_encrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_aead_ctx(aead);\r\nstruct blkcipher_desc desc = { .info = req->iv };\r\nstruct blkcipher_walk walk;\r\nu8 __aligned(8) mac[AES_BLOCK_SIZE];\r\nu8 buf[AES_BLOCK_SIZE];\r\nu32 len = req->cryptlen;\r\nint err;\r\nerr = ccm_init_mac(req, mac, len);\r\nif (err)\r\nreturn err;\r\nkernel_neon_begin_partial(6);\r\nif (req->assoclen)\r\nccm_calculate_auth_mac(req, mac);\r\nmemcpy(buf, req->iv, AES_BLOCK_SIZE);\r\nblkcipher_walk_init(&walk, req->dst, req->src, len);\r\nerr = blkcipher_aead_walk_virt_block(&desc, &walk, aead,\r\nAES_BLOCK_SIZE);\r\nwhile (walk.nbytes) {\r\nu32 tail = walk.nbytes % AES_BLOCK_SIZE;\r\nif (walk.nbytes == len)\r\ntail = 0;\r\nce_aes_ccm_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\nwalk.nbytes - tail, ctx->key_enc,\r\nnum_rounds(ctx), mac, walk.iv);\r\nlen -= walk.nbytes - tail;\r\nerr = blkcipher_walk_done(&desc, &walk, tail);\r\n}\r\nif (!err)\r\nce_aes_ccm_final(mac, buf, ctx->key_enc, num_rounds(ctx));\r\nkernel_neon_end();\r\nif (err)\r\nreturn err;\r\nscatterwalk_map_and_copy(mac, req->dst, req->cryptlen,\r\ncrypto_aead_authsize(aead), 1);\r\nreturn 0;\r\n}\r\nstatic int ccm_decrypt(struct aead_request *req)\r\n{\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_aead_ctx(aead);\r\nunsigned int authsize = crypto_aead_authsize(aead);\r\nstruct blkcipher_desc desc = { .info = req->iv };\r\nstruct blkcipher_walk walk;\r\nu8 __aligned(8) mac[AES_BLOCK_SIZE];\r\nu8 buf[AES_BLOCK_SIZE];\r\nu32 len = req->cryptlen - authsize;\r\nint err;\r\nerr = ccm_init_mac(req, mac, len);\r\nif (err)\r\nreturn err;\r\nkernel_neon_begin_partial(6);\r\nif (req->assoclen)\r\nccm_calculate_auth_mac(req, mac);\r\nmemcpy(buf, req->iv, AES_BLOCK_SIZE);\r\nblkcipher_walk_init(&walk, req->dst, req->src, len);\r\nerr = blkcipher_aead_walk_virt_block(&desc, &walk, aead,\r\nAES_BLOCK_SIZE);\r\nwhile (walk.nbytes) {\r\nu32 tail = walk.nbytes % AES_BLOCK_SIZE;\r\nif (walk.nbytes == len)\r\ntail = 0;\r\nce_aes_ccm_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\nwalk.nbytes - tail, ctx->key_enc,\r\nnum_rounds(ctx), mac, walk.iv);\r\nlen -= walk.nbytes - tail;\r\nerr = blkcipher_walk_done(&desc, &walk, tail);\r\n}\r\nif (!err)\r\nce_aes_ccm_final(mac, buf, ctx->key_enc, num_rounds(ctx));\r\nkernel_neon_end();\r\nif (err)\r\nreturn err;\r\nscatterwalk_map_and_copy(buf, req->src, req->cryptlen - authsize,\r\nauthsize, 0);\r\nif (memcmp(mac, buf, authsize))\r\nreturn -EBADMSG;\r\nreturn 0;\r\n}\r\nstatic int __init aes_mod_init(void)\r\n{\r\nif (!(elf_hwcap & HWCAP_AES))\r\nreturn -ENODEV;\r\nreturn crypto_register_alg(&ccm_aes_alg);\r\n}\r\nstatic void __exit aes_mod_exit(void)\r\n{\r\ncrypto_unregister_alg(&ccm_aes_alg);\r\n}
