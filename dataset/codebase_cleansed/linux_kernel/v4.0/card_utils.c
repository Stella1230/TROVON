int __genwqe_writeq(struct genwqe_dev *cd, u64 byte_offs, u64 val)\r\n{\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nif (cd->err_inject & GENWQE_INJECT_HARDWARE_FAILURE)\r\nreturn -EIO;\r\nif (cd->mmio == NULL)\r\nreturn -EIO;\r\nif (pci_channel_offline(pci_dev))\r\nreturn -EIO;\r\n__raw_writeq((__force u64)cpu_to_be64(val), cd->mmio + byte_offs);\r\nreturn 0;\r\n}\r\nu64 __genwqe_readq(struct genwqe_dev *cd, u64 byte_offs)\r\n{\r\nif (cd->err_inject & GENWQE_INJECT_HARDWARE_FAILURE)\r\nreturn 0xffffffffffffffffull;\r\nif ((cd->err_inject & GENWQE_INJECT_GFIR_FATAL) &&\r\n(byte_offs == IO_SLC_CFGREG_GFIR))\r\nreturn 0x000000000000ffffull;\r\nif ((cd->err_inject & GENWQE_INJECT_GFIR_INFO) &&\r\n(byte_offs == IO_SLC_CFGREG_GFIR))\r\nreturn 0x00000000ffff0000ull;\r\nif (cd->mmio == NULL)\r\nreturn 0xffffffffffffffffull;\r\nreturn be64_to_cpu((__force __be64)__raw_readq(cd->mmio + byte_offs));\r\n}\r\nint __genwqe_writel(struct genwqe_dev *cd, u64 byte_offs, u32 val)\r\n{\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nif (cd->err_inject & GENWQE_INJECT_HARDWARE_FAILURE)\r\nreturn -EIO;\r\nif (cd->mmio == NULL)\r\nreturn -EIO;\r\nif (pci_channel_offline(pci_dev))\r\nreturn -EIO;\r\n__raw_writel((__force u32)cpu_to_be32(val), cd->mmio + byte_offs);\r\nreturn 0;\r\n}\r\nu32 __genwqe_readl(struct genwqe_dev *cd, u64 byte_offs)\r\n{\r\nif (cd->err_inject & GENWQE_INJECT_HARDWARE_FAILURE)\r\nreturn 0xffffffff;\r\nif (cd->mmio == NULL)\r\nreturn 0xffffffff;\r\nreturn be32_to_cpu((__force __be32)__raw_readl(cd->mmio + byte_offs));\r\n}\r\nint genwqe_read_app_id(struct genwqe_dev *cd, char *app_name, int len)\r\n{\r\nint i, j;\r\nu32 app_id = (u32)cd->app_unitcfg;\r\nmemset(app_name, 0, len);\r\nfor (i = 0, j = 0; j < min(len, 4); j++) {\r\nchar ch = (char)((app_id >> (24 - j*8)) & 0xff);\r\nif (ch == ' ')\r\ncontinue;\r\napp_name[i++] = isprint(ch) ? ch : 'X';\r\n}\r\nreturn i;\r\n}\r\nvoid genwqe_init_crc32(void)\r\n{\r\nint i, j;\r\nu32 crc;\r\nfor (i = 0; i < 256; i++) {\r\ncrc = i << 24;\r\nfor (j = 0; j < 8; j++) {\r\nif (crc & 0x80000000)\r\ncrc = (crc << 1) ^ CRC32_POLYNOMIAL;\r\nelse\r\ncrc = (crc << 1);\r\n}\r\ncrc32_tab[i] = crc;\r\n}\r\n}\r\nu32 genwqe_crc32(u8 *buff, size_t len, u32 init)\r\n{\r\nint i;\r\nu32 crc;\r\ncrc = init;\r\nwhile (len--) {\r\ni = ((crc >> 24) ^ *buff++) & 0xFF;\r\ncrc = (crc << 8) ^ crc32_tab[i];\r\n}\r\nreturn crc;\r\n}\r\nvoid *__genwqe_alloc_consistent(struct genwqe_dev *cd, size_t size,\r\ndma_addr_t *dma_handle)\r\n{\r\nif (get_order(size) > MAX_ORDER)\r\nreturn NULL;\r\nreturn pci_alloc_consistent(cd->pci_dev, size, dma_handle);\r\n}\r\nvoid __genwqe_free_consistent(struct genwqe_dev *cd, size_t size,\r\nvoid *vaddr, dma_addr_t dma_handle)\r\n{\r\nif (vaddr == NULL)\r\nreturn;\r\npci_free_consistent(cd->pci_dev, size, vaddr, dma_handle);\r\n}\r\nstatic void genwqe_unmap_pages(struct genwqe_dev *cd, dma_addr_t *dma_list,\r\nint num_pages)\r\n{\r\nint i;\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nfor (i = 0; (i < num_pages) && (dma_list[i] != 0x0); i++) {\r\npci_unmap_page(pci_dev, dma_list[i],\r\nPAGE_SIZE, PCI_DMA_BIDIRECTIONAL);\r\ndma_list[i] = 0x0;\r\n}\r\n}\r\nstatic int genwqe_map_pages(struct genwqe_dev *cd,\r\nstruct page **page_list, int num_pages,\r\ndma_addr_t *dma_list)\r\n{\r\nint i;\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nfor (i = 0; i < num_pages; i++) {\r\ndma_addr_t daddr;\r\ndma_list[i] = 0x0;\r\ndaddr = pci_map_page(pci_dev, page_list[i],\r\n0,\r\nPAGE_SIZE,\r\nPCI_DMA_BIDIRECTIONAL);\r\nif (pci_dma_mapping_error(pci_dev, daddr)) {\r\ndev_err(&pci_dev->dev,\r\n"[%s] err: no dma addr daddr=%016llx!\n",\r\n__func__, (long long)daddr);\r\ngoto err;\r\n}\r\ndma_list[i] = daddr;\r\n}\r\nreturn 0;\r\nerr:\r\ngenwqe_unmap_pages(cd, dma_list, num_pages);\r\nreturn -EIO;\r\n}\r\nstatic int genwqe_sgl_size(int num_pages)\r\n{\r\nint len, num_tlb = num_pages / 7;\r\nlen = sizeof(struct sg_entry) * (num_pages+num_tlb + 1);\r\nreturn roundup(len, PAGE_SIZE);\r\n}\r\nint genwqe_alloc_sync_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl,\r\nvoid __user *user_addr, size_t user_size)\r\n{\r\nint rc;\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nsgl->fpage_offs = offset_in_page((unsigned long)user_addr);\r\nsgl->fpage_size = min_t(size_t, PAGE_SIZE-sgl->fpage_offs, user_size);\r\nsgl->nr_pages = DIV_ROUND_UP(sgl->fpage_offs + user_size, PAGE_SIZE);\r\nsgl->lpage_size = (user_size - sgl->fpage_size) % PAGE_SIZE;\r\ndev_dbg(&pci_dev->dev, "[%s] uaddr=%p usize=%8ld nr_pages=%ld fpage_offs=%lx fpage_size=%ld lpage_size=%ld\n",\r\n__func__, user_addr, user_size, sgl->nr_pages,\r\nsgl->fpage_offs, sgl->fpage_size, sgl->lpage_size);\r\nsgl->user_addr = user_addr;\r\nsgl->user_size = user_size;\r\nsgl->sgl_size = genwqe_sgl_size(sgl->nr_pages);\r\nif (get_order(sgl->sgl_size) > MAX_ORDER) {\r\ndev_err(&pci_dev->dev,\r\n"[%s] err: too much memory requested!\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nsgl->sgl = __genwqe_alloc_consistent(cd, sgl->sgl_size,\r\n&sgl->sgl_dma_addr);\r\nif (sgl->sgl == NULL) {\r\ndev_err(&pci_dev->dev,\r\n"[%s] err: no memory available!\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nif ((sgl->fpage_size != 0) && (sgl->fpage_size != PAGE_SIZE)) {\r\nsgl->fpage = __genwqe_alloc_consistent(cd, PAGE_SIZE,\r\n&sgl->fpage_dma_addr);\r\nif (sgl->fpage == NULL)\r\ngoto err_out;\r\nif (copy_from_user(sgl->fpage + sgl->fpage_offs,\r\nuser_addr, sgl->fpage_size)) {\r\nrc = -EFAULT;\r\ngoto err_out;\r\n}\r\n}\r\nif (sgl->lpage_size != 0) {\r\nsgl->lpage = __genwqe_alloc_consistent(cd, PAGE_SIZE,\r\n&sgl->lpage_dma_addr);\r\nif (sgl->lpage == NULL)\r\ngoto err_out1;\r\nif (copy_from_user(sgl->lpage, user_addr + user_size -\r\nsgl->lpage_size, sgl->lpage_size)) {\r\nrc = -EFAULT;\r\ngoto err_out1;\r\n}\r\n}\r\nreturn 0;\r\nerr_out1:\r\n__genwqe_free_consistent(cd, PAGE_SIZE, sgl->fpage,\r\nsgl->fpage_dma_addr);\r\nerr_out:\r\n__genwqe_free_consistent(cd, sgl->sgl_size, sgl->sgl,\r\nsgl->sgl_dma_addr);\r\nreturn -ENOMEM;\r\n}\r\nint genwqe_setup_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl,\r\ndma_addr_t *dma_list)\r\n{\r\nint i = 0, j = 0, p;\r\nunsigned long dma_offs, map_offs;\r\ndma_addr_t prev_daddr = 0;\r\nstruct sg_entry *s, *last_s = NULL;\r\nsize_t size = sgl->user_size;\r\ndma_offs = 128;\r\nmap_offs = sgl->fpage_offs;\r\ns = &sgl->sgl[0];\r\np = 0;\r\nwhile (p < sgl->nr_pages) {\r\ndma_addr_t daddr;\r\nunsigned int size_to_map;\r\nj = 0;\r\ns[j].target_addr = cpu_to_be64(sgl->sgl_dma_addr + dma_offs);\r\ns[j].len = cpu_to_be32(128);\r\ns[j].flags = cpu_to_be32(SG_CHAINED);\r\nj++;\r\nwhile (j < 8) {\r\nsize_to_map = min(size, PAGE_SIZE - map_offs);\r\nif ((p == 0) && (sgl->fpage != NULL)) {\r\ndaddr = sgl->fpage_dma_addr + map_offs;\r\n} else if ((p == sgl->nr_pages - 1) &&\r\n(sgl->lpage != NULL)) {\r\ndaddr = sgl->lpage_dma_addr;\r\n} else {\r\ndaddr = dma_list[p] + map_offs;\r\n}\r\nsize -= size_to_map;\r\nmap_offs = 0;\r\nif (prev_daddr == daddr) {\r\nu32 prev_len = be32_to_cpu(last_s->len);\r\nlast_s->len = cpu_to_be32(prev_len +\r\nsize_to_map);\r\np++;\r\nif (p == sgl->nr_pages)\r\ngoto fixup;\r\nprev_daddr = daddr + size_to_map;\r\ncontinue;\r\n}\r\ns[j].target_addr = cpu_to_be64(daddr);\r\ns[j].len = cpu_to_be32(size_to_map);\r\ns[j].flags = cpu_to_be32(SG_DATA);\r\nprev_daddr = daddr + size_to_map;\r\nlast_s = &s[j];\r\nj++;\r\np++;\r\nif (p == sgl->nr_pages)\r\ngoto fixup;\r\n}\r\ndma_offs += 128;\r\ns += 8;\r\n}\r\nfixup:\r\nif (j == 1) {\r\ns -= 8;\r\nj = 7;\r\n}\r\nfor (i = 0; i < j; i++)\r\ns[i] = s[i + 1];\r\ns[i].target_addr = cpu_to_be64(0);\r\ns[i].len = cpu_to_be32(0);\r\ns[i].flags = cpu_to_be32(SG_END_LIST);\r\nreturn 0;\r\n}\r\nint genwqe_free_sync_sgl(struct genwqe_dev *cd, struct genwqe_sgl *sgl)\r\n{\r\nint rc = 0;\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nif (sgl->fpage) {\r\nif (copy_to_user(sgl->user_addr, sgl->fpage + sgl->fpage_offs,\r\nsgl->fpage_size)) {\r\ndev_err(&pci_dev->dev, "[%s] err: copying fpage!\n",\r\n__func__);\r\nrc = -EFAULT;\r\n}\r\n__genwqe_free_consistent(cd, PAGE_SIZE, sgl->fpage,\r\nsgl->fpage_dma_addr);\r\nsgl->fpage = NULL;\r\nsgl->fpage_dma_addr = 0;\r\n}\r\nif (sgl->lpage) {\r\nif (copy_to_user(sgl->user_addr + sgl->user_size -\r\nsgl->lpage_size, sgl->lpage,\r\nsgl->lpage_size)) {\r\ndev_err(&pci_dev->dev, "[%s] err: copying lpage!\n",\r\n__func__);\r\nrc = -EFAULT;\r\n}\r\n__genwqe_free_consistent(cd, PAGE_SIZE, sgl->lpage,\r\nsgl->lpage_dma_addr);\r\nsgl->lpage = NULL;\r\nsgl->lpage_dma_addr = 0;\r\n}\r\n__genwqe_free_consistent(cd, sgl->sgl_size, sgl->sgl,\r\nsgl->sgl_dma_addr);\r\nsgl->sgl = NULL;\r\nsgl->sgl_dma_addr = 0x0;\r\nsgl->sgl_size = 0;\r\nreturn rc;\r\n}\r\nstatic int free_user_pages(struct page **page_list, unsigned int nr_pages,\r\nint dirty)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < nr_pages; i++) {\r\nif (page_list[i] != NULL) {\r\nif (dirty)\r\nset_page_dirty_lock(page_list[i]);\r\nput_page(page_list[i]);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint genwqe_user_vmap(struct genwqe_dev *cd, struct dma_mapping *m, void *uaddr,\r\nunsigned long size, struct ddcb_requ *req)\r\n{\r\nint rc = -EINVAL;\r\nunsigned long data, offs;\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nif ((uaddr == NULL) || (size == 0)) {\r\nm->size = 0;\r\nreturn -EINVAL;\r\n}\r\nm->u_vaddr = uaddr;\r\nm->size = size;\r\ndata = (unsigned long)uaddr;\r\noffs = offset_in_page(data);\r\nm->nr_pages = DIV_ROUND_UP(offs + size, PAGE_SIZE);\r\nm->page_list = kcalloc(m->nr_pages,\r\nsizeof(struct page *) + sizeof(dma_addr_t),\r\nGFP_KERNEL);\r\nif (!m->page_list) {\r\ndev_err(&pci_dev->dev, "err: alloc page_list failed\n");\r\nm->nr_pages = 0;\r\nm->u_vaddr = NULL;\r\nm->size = 0;\r\nreturn -ENOMEM;\r\n}\r\nm->dma_list = (dma_addr_t *)(m->page_list + m->nr_pages);\r\nrc = get_user_pages_fast(data & PAGE_MASK,\r\nm->nr_pages,\r\n1,\r\nm->page_list);\r\nif (rc < 0)\r\ngoto fail_get_user_pages;\r\nif (rc < m->nr_pages) {\r\nfree_user_pages(m->page_list, rc, 0);\r\nrc = -EFAULT;\r\ngoto fail_get_user_pages;\r\n}\r\nrc = genwqe_map_pages(cd, m->page_list, m->nr_pages, m->dma_list);\r\nif (rc != 0)\r\ngoto fail_free_user_pages;\r\nreturn 0;\r\nfail_free_user_pages:\r\nfree_user_pages(m->page_list, m->nr_pages, 0);\r\nfail_get_user_pages:\r\nkfree(m->page_list);\r\nm->page_list = NULL;\r\nm->dma_list = NULL;\r\nm->nr_pages = 0;\r\nm->u_vaddr = NULL;\r\nm->size = 0;\r\nreturn rc;\r\n}\r\nint genwqe_user_vunmap(struct genwqe_dev *cd, struct dma_mapping *m,\r\nstruct ddcb_requ *req)\r\n{\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nif (!dma_mapping_used(m)) {\r\ndev_err(&pci_dev->dev, "[%s] err: mapping %p not used!\n",\r\n__func__, m);\r\nreturn -EINVAL;\r\n}\r\nif (m->dma_list)\r\ngenwqe_unmap_pages(cd, m->dma_list, m->nr_pages);\r\nif (m->page_list) {\r\nfree_user_pages(m->page_list, m->nr_pages, 1);\r\nkfree(m->page_list);\r\nm->page_list = NULL;\r\nm->dma_list = NULL;\r\nm->nr_pages = 0;\r\n}\r\nm->u_vaddr = NULL;\r\nm->size = 0;\r\nreturn 0;\r\n}\r\nu8 genwqe_card_type(struct genwqe_dev *cd)\r\n{\r\nu64 card_type = cd->slu_unitcfg;\r\nreturn (u8)((card_type & IO_SLU_UNITCFG_TYPE_MASK) >> 20);\r\n}\r\nint genwqe_card_reset(struct genwqe_dev *cd)\r\n{\r\nu64 softrst;\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nif (!genwqe_is_privileged(cd))\r\nreturn -ENODEV;\r\n__genwqe_writeq(cd, IO_SLC_CFGREG_SOFTRESET, 0x1ull);\r\nmsleep(1000);\r\n__genwqe_readq(cd, IO_HSU_FIR_CLR);\r\n__genwqe_readq(cd, IO_APP_FIR_CLR);\r\n__genwqe_readq(cd, IO_SLU_FIR_CLR);\r\nsoftrst = __genwqe_readq(cd, IO_SLC_CFGREG_SOFTRESET) & 0x3cull;\r\n__genwqe_writeq(cd, IO_SLC_CFGREG_SOFTRESET, softrst | 0x2ull);\r\nmsleep(50);\r\nif (genwqe_need_err_masking(cd)) {\r\ndev_info(&pci_dev->dev,\r\n"[%s] masking errors for old bitstreams\n", __func__);\r\n__genwqe_writeq(cd, IO_SLC_MISC_DEBUG, 0x0aull);\r\n}\r\nreturn 0;\r\n}\r\nint genwqe_read_softreset(struct genwqe_dev *cd)\r\n{\r\nu64 bitstream;\r\nif (!genwqe_is_privileged(cd))\r\nreturn -ENODEV;\r\nbitstream = __genwqe_readq(cd, IO_SLU_BITSTREAM) & 0x1;\r\ncd->softreset = (bitstream == 0) ? 0x8ull : 0xcull;\r\nreturn 0;\r\n}\r\nint genwqe_set_interrupt_capability(struct genwqe_dev *cd, int count)\r\n{\r\nint rc;\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nrc = pci_enable_msi_range(pci_dev, 1, count);\r\nif (rc < 0)\r\nreturn rc;\r\ncd->flags |= GENWQE_FLAG_MSI_ENABLED;\r\nreturn 0;\r\n}\r\nvoid genwqe_reset_interrupt_capability(struct genwqe_dev *cd)\r\n{\r\nstruct pci_dev *pci_dev = cd->pci_dev;\r\nif (cd->flags & GENWQE_FLAG_MSI_ENABLED) {\r\npci_disable_msi(pci_dev);\r\ncd->flags &= ~GENWQE_FLAG_MSI_ENABLED;\r\n}\r\n}\r\nstatic int set_reg_idx(struct genwqe_dev *cd, struct genwqe_reg *r,\r\nunsigned int *i, unsigned int m, u32 addr, u32 idx,\r\nu64 val)\r\n{\r\nif (WARN_ON_ONCE(*i >= m))\r\nreturn -EFAULT;\r\nr[*i].addr = addr;\r\nr[*i].idx = idx;\r\nr[*i].val = val;\r\n++*i;\r\nreturn 0;\r\n}\r\nstatic int set_reg(struct genwqe_dev *cd, struct genwqe_reg *r,\r\nunsigned int *i, unsigned int m, u32 addr, u64 val)\r\n{\r\nreturn set_reg_idx(cd, r, i, m, addr, 0, val);\r\n}\r\nint genwqe_read_ffdc_regs(struct genwqe_dev *cd, struct genwqe_reg *regs,\r\nunsigned int max_regs, int all)\r\n{\r\nunsigned int i, j, idx = 0;\r\nu32 ufir_addr, ufec_addr, sfir_addr, sfec_addr;\r\nu64 gfir, sluid, appid, ufir, ufec, sfir, sfec;\r\ngfir = __genwqe_readq(cd, IO_SLC_CFGREG_GFIR);\r\nset_reg(cd, regs, &idx, max_regs, IO_SLC_CFGREG_GFIR, gfir);\r\nsluid = __genwqe_readq(cd, IO_SLU_UNITCFG);\r\nset_reg(cd, regs, &idx, max_regs, IO_SLU_UNITCFG, sluid);\r\nappid = __genwqe_readq(cd, IO_APP_UNITCFG);\r\nset_reg(cd, regs, &idx, max_regs, IO_APP_UNITCFG, appid);\r\nfor (i = 0; i < GENWQE_MAX_UNITS; i++) {\r\nufir_addr = (i << 24) | 0x008;\r\nufir = __genwqe_readq(cd, ufir_addr);\r\nset_reg(cd, regs, &idx, max_regs, ufir_addr, ufir);\r\nufec_addr = (i << 24) | 0x018;\r\nufec = __genwqe_readq(cd, ufec_addr);\r\nset_reg(cd, regs, &idx, max_regs, ufec_addr, ufec);\r\nfor (j = 0; j < 64; j++) {\r\nif (!all && (!(ufir & (1ull << j))))\r\ncontinue;\r\nsfir_addr = (i << 24) | (0x100 + 8 * j);\r\nsfir = __genwqe_readq(cd, sfir_addr);\r\nset_reg(cd, regs, &idx, max_regs, sfir_addr, sfir);\r\nsfec_addr = (i << 24) | (0x300 + 8 * j);\r\nsfec = __genwqe_readq(cd, sfec_addr);\r\nset_reg(cd, regs, &idx, max_regs, sfec_addr, sfec);\r\n}\r\n}\r\nfor (i = idx; i < max_regs; i++) {\r\nregs[i].addr = 0xffffffff;\r\nregs[i].val = 0xffffffffffffffffull;\r\n}\r\nreturn idx;\r\n}\r\nint genwqe_ffdc_buff_size(struct genwqe_dev *cd, int uid)\r\n{\r\nint entries = 0, ring, traps, traces, trace_entries;\r\nu32 eevptr_addr, l_addr, d_len, d_type;\r\nu64 eevptr, val, addr;\r\neevptr_addr = GENWQE_UID_OFFS(uid) | IO_EXTENDED_ERROR_POINTER;\r\neevptr = __genwqe_readq(cd, eevptr_addr);\r\nif ((eevptr != 0x0) && (eevptr != -1ull)) {\r\nl_addr = GENWQE_UID_OFFS(uid) | eevptr;\r\nwhile (1) {\r\nval = __genwqe_readq(cd, l_addr);\r\nif ((val == 0x0) || (val == -1ull))\r\nbreak;\r\nd_len = (val & 0x0000007fff000000ull) >> 24;\r\nd_type = (val & 0x0000008000000000ull) >> 36;\r\nif (d_type) {\r\nentries += d_len;\r\n} else {\r\nentries += d_len >> 3;\r\n}\r\nl_addr += 8;\r\n}\r\n}\r\nfor (ring = 0; ring < 8; ring++) {\r\naddr = GENWQE_UID_OFFS(uid) | IO_EXTENDED_DIAG_MAP(ring);\r\nval = __genwqe_readq(cd, addr);\r\nif ((val == 0x0ull) || (val == -1ull))\r\ncontinue;\r\ntraps = (val >> 24) & 0xff;\r\ntraces = (val >> 16) & 0xff;\r\ntrace_entries = val & 0xffff;\r\nentries += traps + (traces * trace_entries);\r\n}\r\nreturn entries;\r\n}\r\nint genwqe_ffdc_buff_read(struct genwqe_dev *cd, int uid,\r\nstruct genwqe_reg *regs, unsigned int max_regs)\r\n{\r\nint i, traps, traces, trace, trace_entries, trace_entry, ring;\r\nunsigned int idx = 0;\r\nu32 eevptr_addr, l_addr, d_addr, d_len, d_type;\r\nu64 eevptr, e, val, addr;\r\neevptr_addr = GENWQE_UID_OFFS(uid) | IO_EXTENDED_ERROR_POINTER;\r\neevptr = __genwqe_readq(cd, eevptr_addr);\r\nif ((eevptr != 0x0) && (eevptr != 0xffffffffffffffffull)) {\r\nl_addr = GENWQE_UID_OFFS(uid) | eevptr;\r\nwhile (1) {\r\ne = __genwqe_readq(cd, l_addr);\r\nif ((e == 0x0) || (e == 0xffffffffffffffffull))\r\nbreak;\r\nd_addr = (e & 0x0000000000ffffffull);\r\nd_len = (e & 0x0000007fff000000ull) >> 24;\r\nd_type = (e & 0x0000008000000000ull) >> 36;\r\nd_addr |= GENWQE_UID_OFFS(uid);\r\nif (d_type) {\r\nfor (i = 0; i < (int)d_len; i++) {\r\nval = __genwqe_readq(cd, d_addr);\r\nset_reg_idx(cd, regs, &idx, max_regs,\r\nd_addr, i, val);\r\n}\r\n} else {\r\nd_len >>= 3;\r\nfor (i = 0; i < (int)d_len; i++, d_addr += 8) {\r\nval = __genwqe_readq(cd, d_addr);\r\nset_reg_idx(cd, regs, &idx, max_regs,\r\nd_addr, 0, val);\r\n}\r\n}\r\nl_addr += 8;\r\n}\r\n}\r\nfor (ring = 0; ring < 8; ring++) {\r\naddr = GENWQE_UID_OFFS(uid) | IO_EXTENDED_DIAG_MAP(ring);\r\nval = __genwqe_readq(cd, addr);\r\nif ((val == 0x0ull) || (val == -1ull))\r\ncontinue;\r\ntraps = (val >> 24) & 0xff;\r\ntraces = (val >> 16) & 0xff;\r\ntrace_entries = val & 0xffff;\r\nfor (trace = 0; trace <= traces; trace++) {\r\nu32 diag_sel =\r\nGENWQE_EXTENDED_DIAG_SELECTOR(ring, trace);\r\naddr = (GENWQE_UID_OFFS(uid) |\r\nIO_EXTENDED_DIAG_SELECTOR);\r\n__genwqe_writeq(cd, addr, diag_sel);\r\nfor (trace_entry = 0;\r\ntrace_entry < (trace ? trace_entries : traps);\r\ntrace_entry++) {\r\naddr = (GENWQE_UID_OFFS(uid) |\r\nIO_EXTENDED_DIAG_READ_MBX);\r\nval = __genwqe_readq(cd, addr);\r\nset_reg_idx(cd, regs, &idx, max_regs, addr,\r\n(diag_sel<<16) | trace_entry, val);\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint genwqe_write_vreg(struct genwqe_dev *cd, u32 reg, u64 val, int func)\r\n{\r\n__genwqe_writeq(cd, IO_PF_SLC_VIRTUAL_WINDOW, func & 0xf);\r\n__genwqe_writeq(cd, reg, val);\r\nreturn 0;\r\n}\r\nu64 genwqe_read_vreg(struct genwqe_dev *cd, u32 reg, int func)\r\n{\r\n__genwqe_writeq(cd, IO_PF_SLC_VIRTUAL_WINDOW, func & 0xf);\r\nreturn __genwqe_readq(cd, reg);\r\n}\r\nint genwqe_base_clock_frequency(struct genwqe_dev *cd)\r\n{\r\nu16 speed;\r\nstatic const int speed_grade[] = { 250, 200, 166, 175 };\r\nspeed = (u16)((cd->slu_unitcfg >> 28) & 0x0full);\r\nif (speed >= ARRAY_SIZE(speed_grade))\r\nreturn 0;\r\nreturn speed_grade[speed];\r\n}\r\nvoid genwqe_stop_traps(struct genwqe_dev *cd)\r\n{\r\n__genwqe_writeq(cd, IO_SLC_MISC_DEBUG_SET, 0xcull);\r\n}\r\nvoid genwqe_start_traps(struct genwqe_dev *cd)\r\n{\r\n__genwqe_writeq(cd, IO_SLC_MISC_DEBUG_CLR, 0xcull);\r\nif (genwqe_need_err_masking(cd))\r\n__genwqe_writeq(cd, IO_SLC_MISC_DEBUG, 0x0aull);\r\n}
