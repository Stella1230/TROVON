static inline void smmu_writel(struct tegra_smmu *smmu, u32 value,\r\nunsigned long offset)\r\n{\r\nwritel(value, smmu->regs + offset);\r\n}\r\nstatic inline u32 smmu_readl(struct tegra_smmu *smmu, unsigned long offset)\r\n{\r\nreturn readl(smmu->regs + offset);\r\n}\r\nstatic inline void smmu_flush_ptc(struct tegra_smmu *smmu, struct page *page,\r\nunsigned long offset)\r\n{\r\nphys_addr_t phys = page ? page_to_phys(page) : 0;\r\nu32 value;\r\nif (page) {\r\noffset &= ~(smmu->mc->soc->atom_size - 1);\r\nif (smmu->mc->soc->num_address_bits > 32) {\r\n#ifdef CONFIG_PHYS_ADDR_T_64BIT\r\nvalue = (phys >> 32) & SMMU_PTC_FLUSH_HI_MASK;\r\n#else\r\nvalue = 0;\r\n#endif\r\nsmmu_writel(smmu, value, SMMU_PTC_FLUSH_HI);\r\n}\r\nvalue = (phys + offset) | SMMU_PTC_FLUSH_TYPE_ADR;\r\n} else {\r\nvalue = SMMU_PTC_FLUSH_TYPE_ALL;\r\n}\r\nsmmu_writel(smmu, value, SMMU_PTC_FLUSH);\r\n}\r\nstatic inline void smmu_flush_tlb(struct tegra_smmu *smmu)\r\n{\r\nsmmu_writel(smmu, SMMU_TLB_FLUSH_VA_MATCH_ALL, SMMU_TLB_FLUSH);\r\n}\r\nstatic inline void smmu_flush_tlb_asid(struct tegra_smmu *smmu,\r\nunsigned long asid)\r\n{\r\nu32 value;\r\nvalue = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |\r\nSMMU_TLB_FLUSH_VA_MATCH_ALL;\r\nsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\r\n}\r\nstatic inline void smmu_flush_tlb_section(struct tegra_smmu *smmu,\r\nunsigned long asid,\r\nunsigned long iova)\r\n{\r\nu32 value;\r\nvalue = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |\r\nSMMU_TLB_FLUSH_VA_SECTION(iova);\r\nsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\r\n}\r\nstatic inline void smmu_flush_tlb_group(struct tegra_smmu *smmu,\r\nunsigned long asid,\r\nunsigned long iova)\r\n{\r\nu32 value;\r\nvalue = SMMU_TLB_FLUSH_ASID_MATCH | SMMU_TLB_FLUSH_ASID(asid) |\r\nSMMU_TLB_FLUSH_VA_GROUP(iova);\r\nsmmu_writel(smmu, value, SMMU_TLB_FLUSH);\r\n}\r\nstatic inline void smmu_flush(struct tegra_smmu *smmu)\r\n{\r\nsmmu_readl(smmu, SMMU_CONFIG);\r\n}\r\nstatic int tegra_smmu_alloc_asid(struct tegra_smmu *smmu, unsigned int *idp)\r\n{\r\nunsigned long id;\r\nmutex_lock(&smmu->lock);\r\nid = find_first_zero_bit(smmu->asids, smmu->soc->num_asids);\r\nif (id >= smmu->soc->num_asids) {\r\nmutex_unlock(&smmu->lock);\r\nreturn -ENOSPC;\r\n}\r\nset_bit(id, smmu->asids);\r\n*idp = id;\r\nmutex_unlock(&smmu->lock);\r\nreturn 0;\r\n}\r\nstatic void tegra_smmu_free_asid(struct tegra_smmu *smmu, unsigned int id)\r\n{\r\nmutex_lock(&smmu->lock);\r\nclear_bit(id, smmu->asids);\r\nmutex_unlock(&smmu->lock);\r\n}\r\nstatic bool tegra_smmu_capable(enum iommu_cap cap)\r\n{\r\nreturn false;\r\n}\r\nstatic int tegra_smmu_domain_init(struct iommu_domain *domain)\r\n{\r\nstruct tegra_smmu_as *as;\r\nunsigned int i;\r\nuint32_t *pd;\r\nas = kzalloc(sizeof(*as), GFP_KERNEL);\r\nif (!as)\r\nreturn -ENOMEM;\r\nas->attr = SMMU_PD_READABLE | SMMU_PD_WRITABLE | SMMU_PD_NONSECURE;\r\nas->domain = domain;\r\nas->pd = alloc_page(GFP_KERNEL | __GFP_DMA);\r\nif (!as->pd) {\r\nkfree(as);\r\nreturn -ENOMEM;\r\n}\r\nas->count = alloc_page(GFP_KERNEL);\r\nif (!as->count) {\r\n__free_page(as->pd);\r\nkfree(as);\r\nreturn -ENOMEM;\r\n}\r\npd = page_address(as->pd);\r\nSetPageReserved(as->pd);\r\nfor (i = 0; i < SMMU_NUM_PDE; i++)\r\npd[i] = 0;\r\npd = page_address(as->count);\r\nSetPageReserved(as->count);\r\nfor (i = 0; i < SMMU_NUM_PDE; i++)\r\npd[i] = 0;\r\ndomain->priv = as;\r\nreturn 0;\r\n}\r\nstatic void tegra_smmu_domain_destroy(struct iommu_domain *domain)\r\n{\r\nstruct tegra_smmu_as *as = domain->priv;\r\nClearPageReserved(as->pd);\r\nkfree(as);\r\n}\r\nstatic const struct tegra_smmu_swgroup *\r\ntegra_smmu_find_swgroup(struct tegra_smmu *smmu, unsigned int swgroup)\r\n{\r\nconst struct tegra_smmu_swgroup *group = NULL;\r\nunsigned int i;\r\nfor (i = 0; i < smmu->soc->num_swgroups; i++) {\r\nif (smmu->soc->swgroups[i].swgroup == swgroup) {\r\ngroup = &smmu->soc->swgroups[i];\r\nbreak;\r\n}\r\n}\r\nreturn group;\r\n}\r\nstatic void tegra_smmu_enable(struct tegra_smmu *smmu, unsigned int swgroup,\r\nunsigned int asid)\r\n{\r\nconst struct tegra_smmu_swgroup *group;\r\nunsigned int i;\r\nu32 value;\r\nfor (i = 0; i < smmu->soc->num_clients; i++) {\r\nconst struct tegra_mc_client *client = &smmu->soc->clients[i];\r\nif (client->swgroup != swgroup)\r\ncontinue;\r\nvalue = smmu_readl(smmu, client->smmu.reg);\r\nvalue |= BIT(client->smmu.bit);\r\nsmmu_writel(smmu, value, client->smmu.reg);\r\n}\r\ngroup = tegra_smmu_find_swgroup(smmu, swgroup);\r\nif (group) {\r\nvalue = smmu_readl(smmu, group->reg);\r\nvalue &= ~SMMU_ASID_MASK;\r\nvalue |= SMMU_ASID_VALUE(asid);\r\nvalue |= SMMU_ASID_ENABLE;\r\nsmmu_writel(smmu, value, group->reg);\r\n}\r\n}\r\nstatic void tegra_smmu_disable(struct tegra_smmu *smmu, unsigned int swgroup,\r\nunsigned int asid)\r\n{\r\nconst struct tegra_smmu_swgroup *group;\r\nunsigned int i;\r\nu32 value;\r\ngroup = tegra_smmu_find_swgroup(smmu, swgroup);\r\nif (group) {\r\nvalue = smmu_readl(smmu, group->reg);\r\nvalue &= ~SMMU_ASID_MASK;\r\nvalue |= SMMU_ASID_VALUE(asid);\r\nvalue &= ~SMMU_ASID_ENABLE;\r\nsmmu_writel(smmu, value, group->reg);\r\n}\r\nfor (i = 0; i < smmu->soc->num_clients; i++) {\r\nconst struct tegra_mc_client *client = &smmu->soc->clients[i];\r\nif (client->swgroup != swgroup)\r\ncontinue;\r\nvalue = smmu_readl(smmu, client->smmu.reg);\r\nvalue &= ~BIT(client->smmu.bit);\r\nsmmu_writel(smmu, value, client->smmu.reg);\r\n}\r\n}\r\nstatic int tegra_smmu_as_prepare(struct tegra_smmu *smmu,\r\nstruct tegra_smmu_as *as)\r\n{\r\nu32 value;\r\nint err;\r\nif (as->use_count > 0) {\r\nas->use_count++;\r\nreturn 0;\r\n}\r\nerr = tegra_smmu_alloc_asid(smmu, &as->id);\r\nif (err < 0)\r\nreturn err;\r\nsmmu->soc->ops->flush_dcache(as->pd, 0, SMMU_SIZE_PD);\r\nsmmu_flush_ptc(smmu, as->pd, 0);\r\nsmmu_flush_tlb_asid(smmu, as->id);\r\nsmmu_writel(smmu, as->id & 0x7f, SMMU_PTB_ASID);\r\nvalue = SMMU_PTB_DATA_VALUE(as->pd, as->attr);\r\nsmmu_writel(smmu, value, SMMU_PTB_DATA);\r\nsmmu_flush(smmu);\r\nas->smmu = smmu;\r\nas->use_count++;\r\nreturn 0;\r\n}\r\nstatic void tegra_smmu_as_unprepare(struct tegra_smmu *smmu,\r\nstruct tegra_smmu_as *as)\r\n{\r\nif (--as->use_count > 0)\r\nreturn;\r\ntegra_smmu_free_asid(smmu, as->id);\r\nas->smmu = NULL;\r\n}\r\nstatic int tegra_smmu_attach_dev(struct iommu_domain *domain,\r\nstruct device *dev)\r\n{\r\nstruct tegra_smmu *smmu = dev->archdata.iommu;\r\nstruct tegra_smmu_as *as = domain->priv;\r\nstruct device_node *np = dev->of_node;\r\nstruct of_phandle_args args;\r\nunsigned int index = 0;\r\nint err = 0;\r\nwhile (!of_parse_phandle_with_args(np, "iommus", "#iommu-cells", index,\r\n&args)) {\r\nunsigned int swgroup = args.args[0];\r\nif (args.np != smmu->dev->of_node) {\r\nof_node_put(args.np);\r\ncontinue;\r\n}\r\nof_node_put(args.np);\r\nerr = tegra_smmu_as_prepare(smmu, as);\r\nif (err < 0)\r\nreturn err;\r\ntegra_smmu_enable(smmu, swgroup, as->id);\r\nindex++;\r\n}\r\nif (index == 0)\r\nreturn -ENODEV;\r\nreturn 0;\r\n}\r\nstatic void tegra_smmu_detach_dev(struct iommu_domain *domain, struct device *dev)\r\n{\r\nstruct tegra_smmu_as *as = domain->priv;\r\nstruct device_node *np = dev->of_node;\r\nstruct tegra_smmu *smmu = as->smmu;\r\nstruct of_phandle_args args;\r\nunsigned int index = 0;\r\nwhile (!of_parse_phandle_with_args(np, "iommus", "#iommu-cells", index,\r\n&args)) {\r\nunsigned int swgroup = args.args[0];\r\nif (args.np != smmu->dev->of_node) {\r\nof_node_put(args.np);\r\ncontinue;\r\n}\r\nof_node_put(args.np);\r\ntegra_smmu_disable(smmu, swgroup, as->id);\r\ntegra_smmu_as_unprepare(smmu, as);\r\nindex++;\r\n}\r\n}\r\nstatic u32 *as_get_pte(struct tegra_smmu_as *as, dma_addr_t iova,\r\nstruct page **pagep)\r\n{\r\nu32 *pd = page_address(as->pd), *pt, *count;\r\nu32 pde = (iova >> SMMU_PDE_SHIFT) & 0x3ff;\r\nu32 pte = (iova >> SMMU_PTE_SHIFT) & 0x3ff;\r\nstruct tegra_smmu *smmu = as->smmu;\r\nstruct page *page;\r\nunsigned int i;\r\nif (pd[pde] == 0) {\r\npage = alloc_page(GFP_KERNEL | __GFP_DMA);\r\nif (!page)\r\nreturn NULL;\r\npt = page_address(page);\r\nSetPageReserved(page);\r\nfor (i = 0; i < SMMU_NUM_PTE; i++)\r\npt[i] = 0;\r\nsmmu->soc->ops->flush_dcache(page, 0, SMMU_SIZE_PT);\r\npd[pde] = SMMU_MK_PDE(page, SMMU_PDE_ATTR | SMMU_PDE_NEXT);\r\nsmmu->soc->ops->flush_dcache(as->pd, pde << 2, 4);\r\nsmmu_flush_ptc(smmu, as->pd, pde << 2);\r\nsmmu_flush_tlb_section(smmu, as->id, iova);\r\nsmmu_flush(smmu);\r\n} else {\r\npage = pfn_to_page(pd[pde] & SMMU_PFN_MASK);\r\npt = page_address(page);\r\n}\r\n*pagep = page;\r\ncount = page_address(as->count);\r\nif (pt[pte] == 0)\r\ncount[pde]++;\r\nreturn &pt[pte];\r\n}\r\nstatic void as_put_pte(struct tegra_smmu_as *as, dma_addr_t iova)\r\n{\r\nu32 pde = (iova >> SMMU_PDE_SHIFT) & 0x3ff;\r\nu32 pte = (iova >> SMMU_PTE_SHIFT) & 0x3ff;\r\nu32 *count = page_address(as->count);\r\nu32 *pd = page_address(as->pd), *pt;\r\nstruct page *page;\r\npage = pfn_to_page(pd[pde] & SMMU_PFN_MASK);\r\npt = page_address(page);\r\nif (pt[pte] != 0) {\r\nif (--count[pde] == 0) {\r\nClearPageReserved(page);\r\n__free_page(page);\r\npd[pde] = 0;\r\n}\r\npt[pte] = 0;\r\n}\r\n}\r\nstatic int tegra_smmu_map(struct iommu_domain *domain, unsigned long iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct tegra_smmu_as *as = domain->priv;\r\nstruct tegra_smmu *smmu = as->smmu;\r\nunsigned long offset;\r\nstruct page *page;\r\nu32 *pte;\r\npte = as_get_pte(as, iova, &page);\r\nif (!pte)\r\nreturn -ENOMEM;\r\n*pte = __phys_to_pfn(paddr) | SMMU_PTE_ATTR;\r\noffset = offset_in_page(pte);\r\nsmmu->soc->ops->flush_dcache(page, offset, 4);\r\nsmmu_flush_ptc(smmu, page, offset);\r\nsmmu_flush_tlb_group(smmu, as->id, iova);\r\nsmmu_flush(smmu);\r\nreturn 0;\r\n}\r\nstatic size_t tegra_smmu_unmap(struct iommu_domain *domain, unsigned long iova,\r\nsize_t size)\r\n{\r\nstruct tegra_smmu_as *as = domain->priv;\r\nstruct tegra_smmu *smmu = as->smmu;\r\nunsigned long offset;\r\nstruct page *page;\r\nu32 *pte;\r\npte = as_get_pte(as, iova, &page);\r\nif (!pte)\r\nreturn 0;\r\noffset = offset_in_page(pte);\r\nas_put_pte(as, iova);\r\nsmmu->soc->ops->flush_dcache(page, offset, 4);\r\nsmmu_flush_ptc(smmu, page, offset);\r\nsmmu_flush_tlb_group(smmu, as->id, iova);\r\nsmmu_flush(smmu);\r\nreturn size;\r\n}\r\nstatic phys_addr_t tegra_smmu_iova_to_phys(struct iommu_domain *domain,\r\ndma_addr_t iova)\r\n{\r\nstruct tegra_smmu_as *as = domain->priv;\r\nstruct page *page;\r\nunsigned long pfn;\r\nu32 *pte;\r\npte = as_get_pte(as, iova, &page);\r\npfn = *pte & SMMU_PFN_MASK;\r\nreturn PFN_PHYS(pfn);\r\n}\r\nstatic struct tegra_smmu *tegra_smmu_find(struct device_node *np)\r\n{\r\nstruct platform_device *pdev;\r\nstruct tegra_mc *mc;\r\npdev = of_find_device_by_node(np);\r\nif (!pdev)\r\nreturn NULL;\r\nmc = platform_get_drvdata(pdev);\r\nif (!mc)\r\nreturn NULL;\r\nreturn mc->smmu;\r\n}\r\nstatic int tegra_smmu_add_device(struct device *dev)\r\n{\r\nstruct device_node *np = dev->of_node;\r\nstruct of_phandle_args args;\r\nunsigned int index = 0;\r\nwhile (of_parse_phandle_with_args(np, "iommus", "#iommu-cells", index,\r\n&args) == 0) {\r\nstruct tegra_smmu *smmu;\r\nsmmu = tegra_smmu_find(args.np);\r\nif (smmu) {\r\ndev->archdata.iommu = smmu;\r\nbreak;\r\n}\r\nindex++;\r\n}\r\nreturn 0;\r\n}\r\nstatic void tegra_smmu_remove_device(struct device *dev)\r\n{\r\ndev->archdata.iommu = NULL;\r\n}\r\nstatic void tegra_smmu_ahb_enable(void)\r\n{\r\nstatic const struct of_device_id ahb_match[] = {\r\n{ .compatible = "nvidia,tegra30-ahb", },\r\n{ }\r\n};\r\nstruct device_node *ahb;\r\nahb = of_find_matching_node(NULL, ahb_match);\r\nif (ahb) {\r\ntegra_ahb_enable_smmu(ahb);\r\nof_node_put(ahb);\r\n}\r\n}\r\nstruct tegra_smmu *tegra_smmu_probe(struct device *dev,\r\nconst struct tegra_smmu_soc *soc,\r\nstruct tegra_mc *mc)\r\n{\r\nstruct tegra_smmu *smmu;\r\nsize_t size;\r\nu32 value;\r\nint err;\r\nif (!soc)\r\nreturn NULL;\r\nsmmu = devm_kzalloc(dev, sizeof(*smmu), GFP_KERNEL);\r\nif (!smmu)\r\nreturn ERR_PTR(-ENOMEM);\r\nmc->smmu = smmu;\r\nsize = BITS_TO_LONGS(soc->num_asids) * sizeof(long);\r\nsmmu->asids = devm_kzalloc(dev, size, GFP_KERNEL);\r\nif (!smmu->asids)\r\nreturn ERR_PTR(-ENOMEM);\r\nmutex_init(&smmu->lock);\r\nsmmu->regs = mc->regs;\r\nsmmu->soc = soc;\r\nsmmu->dev = dev;\r\nsmmu->mc = mc;\r\nvalue = SMMU_PTC_CONFIG_ENABLE | SMMU_PTC_CONFIG_INDEX_MAP(0x3f);\r\nif (soc->supports_request_limit)\r\nvalue |= SMMU_PTC_CONFIG_REQ_LIMIT(8);\r\nsmmu_writel(smmu, value, SMMU_PTC_CONFIG);\r\nvalue = SMMU_TLB_CONFIG_HIT_UNDER_MISS |\r\nSMMU_TLB_CONFIG_ACTIVE_LINES(0x20);\r\nif (soc->supports_round_robin_arbitration)\r\nvalue |= SMMU_TLB_CONFIG_ROUND_ROBIN_ARBITRATION;\r\nsmmu_writel(smmu, value, SMMU_TLB_CONFIG);\r\nsmmu_flush_ptc(smmu, NULL, 0);\r\nsmmu_flush_tlb(smmu);\r\nsmmu_writel(smmu, SMMU_CONFIG_ENABLE, SMMU_CONFIG);\r\nsmmu_flush(smmu);\r\ntegra_smmu_ahb_enable();\r\nerr = bus_set_iommu(&platform_bus_type, &tegra_smmu_ops);\r\nif (err < 0)\r\nreturn ERR_PTR(err);\r\nreturn smmu;\r\n}
