static bool __check_shared_memory(size_t alloc_size)\r\n{\r\nsize_t a;\r\na = shared_memory_amount + alloc_size;\r\nif (a < shared_memory_amount)\r\nreturn false;\r\nif (a >> PAGE_SHIFT > totalram_pages / DM_STATS_MEMORY_FACTOR)\r\nreturn false;\r\n#ifdef CONFIG_MMU\r\nif (a > (VMALLOC_END - VMALLOC_START) / DM_STATS_VMALLOC_FACTOR)\r\nreturn false;\r\n#endif\r\nreturn true;\r\n}\r\nstatic bool check_shared_memory(size_t alloc_size)\r\n{\r\nbool ret;\r\nspin_lock_irq(&shared_memory_lock);\r\nret = __check_shared_memory(alloc_size);\r\nspin_unlock_irq(&shared_memory_lock);\r\nreturn ret;\r\n}\r\nstatic bool claim_shared_memory(size_t alloc_size)\r\n{\r\nspin_lock_irq(&shared_memory_lock);\r\nif (!__check_shared_memory(alloc_size)) {\r\nspin_unlock_irq(&shared_memory_lock);\r\nreturn false;\r\n}\r\nshared_memory_amount += alloc_size;\r\nspin_unlock_irq(&shared_memory_lock);\r\nreturn true;\r\n}\r\nstatic void free_shared_memory(size_t alloc_size)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&shared_memory_lock, flags);\r\nif (WARN_ON_ONCE(shared_memory_amount < alloc_size)) {\r\nspin_unlock_irqrestore(&shared_memory_lock, flags);\r\nDMCRIT("Memory usage accounting bug.");\r\nreturn;\r\n}\r\nshared_memory_amount -= alloc_size;\r\nspin_unlock_irqrestore(&shared_memory_lock, flags);\r\n}\r\nstatic void *dm_kvzalloc(size_t alloc_size, int node)\r\n{\r\nvoid *p;\r\nif (!claim_shared_memory(alloc_size))\r\nreturn NULL;\r\nif (alloc_size <= KMALLOC_MAX_SIZE) {\r\np = kzalloc_node(alloc_size, GFP_KERNEL | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN, node);\r\nif (p)\r\nreturn p;\r\n}\r\np = vzalloc_node(alloc_size, node);\r\nif (p)\r\nreturn p;\r\nfree_shared_memory(alloc_size);\r\nreturn NULL;\r\n}\r\nstatic void dm_kvfree(void *ptr, size_t alloc_size)\r\n{\r\nif (!ptr)\r\nreturn;\r\nfree_shared_memory(alloc_size);\r\nif (is_vmalloc_addr(ptr))\r\nvfree(ptr);\r\nelse\r\nkfree(ptr);\r\n}\r\nstatic void dm_stat_free(struct rcu_head *head)\r\n{\r\nint cpu;\r\nstruct dm_stat *s = container_of(head, struct dm_stat, rcu_head);\r\nkfree(s->program_id);\r\nkfree(s->aux_data);\r\nfor_each_possible_cpu(cpu)\r\ndm_kvfree(s->stat_percpu[cpu], s->percpu_alloc_size);\r\ndm_kvfree(s, s->shared_alloc_size);\r\n}\r\nstatic int dm_stat_in_flight(struct dm_stat_shared *shared)\r\n{\r\nreturn atomic_read(&shared->in_flight[READ]) +\r\natomic_read(&shared->in_flight[WRITE]);\r\n}\r\nvoid dm_stats_init(struct dm_stats *stats)\r\n{\r\nint cpu;\r\nstruct dm_stats_last_position *last;\r\nmutex_init(&stats->mutex);\r\nINIT_LIST_HEAD(&stats->list);\r\nstats->last = alloc_percpu(struct dm_stats_last_position);\r\nfor_each_possible_cpu(cpu) {\r\nlast = per_cpu_ptr(stats->last, cpu);\r\nlast->last_sector = (sector_t)ULLONG_MAX;\r\nlast->last_rw = UINT_MAX;\r\n}\r\n}\r\nvoid dm_stats_cleanup(struct dm_stats *stats)\r\n{\r\nsize_t ni;\r\nstruct dm_stat *s;\r\nstruct dm_stat_shared *shared;\r\nwhile (!list_empty(&stats->list)) {\r\ns = container_of(stats->list.next, struct dm_stat, list_entry);\r\nlist_del(&s->list_entry);\r\nfor (ni = 0; ni < s->n_entries; ni++) {\r\nshared = &s->stat_shared[ni];\r\nif (WARN_ON(dm_stat_in_flight(shared))) {\r\nDMCRIT("leaked in-flight counter at index %lu "\r\n"(start %llu, end %llu, step %llu): reads %d, writes %d",\r\n(unsigned long)ni,\r\n(unsigned long long)s->start,\r\n(unsigned long long)s->end,\r\n(unsigned long long)s->step,\r\natomic_read(&shared->in_flight[READ]),\r\natomic_read(&shared->in_flight[WRITE]));\r\n}\r\n}\r\ndm_stat_free(&s->rcu_head);\r\n}\r\nfree_percpu(stats->last);\r\n}\r\nstatic int dm_stats_create(struct dm_stats *stats, sector_t start, sector_t end,\r\nsector_t step, const char *program_id, const char *aux_data,\r\nvoid (*suspend_callback)(struct mapped_device *),\r\nvoid (*resume_callback)(struct mapped_device *),\r\nstruct mapped_device *md)\r\n{\r\nstruct list_head *l;\r\nstruct dm_stat *s, *tmp_s;\r\nsector_t n_entries;\r\nsize_t ni;\r\nsize_t shared_alloc_size;\r\nsize_t percpu_alloc_size;\r\nstruct dm_stat_percpu *p;\r\nint cpu;\r\nint ret_id;\r\nint r;\r\nif (end < start || !step)\r\nreturn -EINVAL;\r\nn_entries = end - start;\r\nif (dm_sector_div64(n_entries, step))\r\nn_entries++;\r\nif (n_entries != (size_t)n_entries || !(size_t)(n_entries + 1))\r\nreturn -EOVERFLOW;\r\nshared_alloc_size = sizeof(struct dm_stat) + (size_t)n_entries * sizeof(struct dm_stat_shared);\r\nif ((shared_alloc_size - sizeof(struct dm_stat)) / sizeof(struct dm_stat_shared) != n_entries)\r\nreturn -EOVERFLOW;\r\npercpu_alloc_size = (size_t)n_entries * sizeof(struct dm_stat_percpu);\r\nif (percpu_alloc_size / sizeof(struct dm_stat_percpu) != n_entries)\r\nreturn -EOVERFLOW;\r\nif (!check_shared_memory(shared_alloc_size + num_possible_cpus() * percpu_alloc_size))\r\nreturn -ENOMEM;\r\ns = dm_kvzalloc(shared_alloc_size, NUMA_NO_NODE);\r\nif (!s)\r\nreturn -ENOMEM;\r\ns->n_entries = n_entries;\r\ns->start = start;\r\ns->end = end;\r\ns->step = step;\r\ns->shared_alloc_size = shared_alloc_size;\r\ns->percpu_alloc_size = percpu_alloc_size;\r\ns->program_id = kstrdup(program_id, GFP_KERNEL);\r\nif (!s->program_id) {\r\nr = -ENOMEM;\r\ngoto out;\r\n}\r\ns->aux_data = kstrdup(aux_data, GFP_KERNEL);\r\nif (!s->aux_data) {\r\nr = -ENOMEM;\r\ngoto out;\r\n}\r\nfor (ni = 0; ni < n_entries; ni++) {\r\natomic_set(&s->stat_shared[ni].in_flight[READ], 0);\r\natomic_set(&s->stat_shared[ni].in_flight[WRITE], 0);\r\n}\r\nfor_each_possible_cpu(cpu) {\r\np = dm_kvzalloc(percpu_alloc_size, cpu_to_node(cpu));\r\nif (!p) {\r\nr = -ENOMEM;\r\ngoto out;\r\n}\r\ns->stat_percpu[cpu] = p;\r\n}\r\nsuspend_callback(md);\r\nmutex_lock(&stats->mutex);\r\ns->id = 0;\r\nlist_for_each(l, &stats->list) {\r\ntmp_s = container_of(l, struct dm_stat, list_entry);\r\nif (WARN_ON(tmp_s->id < s->id)) {\r\nr = -EINVAL;\r\ngoto out_unlock_resume;\r\n}\r\nif (tmp_s->id > s->id)\r\nbreak;\r\nif (unlikely(s->id == INT_MAX)) {\r\nr = -ENFILE;\r\ngoto out_unlock_resume;\r\n}\r\ns->id++;\r\n}\r\nret_id = s->id;\r\nlist_add_tail_rcu(&s->list_entry, l);\r\nmutex_unlock(&stats->mutex);\r\nresume_callback(md);\r\nreturn ret_id;\r\nout_unlock_resume:\r\nmutex_unlock(&stats->mutex);\r\nresume_callback(md);\r\nout:\r\ndm_stat_free(&s->rcu_head);\r\nreturn r;\r\n}\r\nstatic struct dm_stat *__dm_stats_find(struct dm_stats *stats, int id)\r\n{\r\nstruct dm_stat *s;\r\nlist_for_each_entry(s, &stats->list, list_entry) {\r\nif (s->id > id)\r\nbreak;\r\nif (s->id == id)\r\nreturn s;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int dm_stats_delete(struct dm_stats *stats, int id)\r\n{\r\nstruct dm_stat *s;\r\nint cpu;\r\nmutex_lock(&stats->mutex);\r\ns = __dm_stats_find(stats, id);\r\nif (!s) {\r\nmutex_unlock(&stats->mutex);\r\nreturn -ENOENT;\r\n}\r\nlist_del_rcu(&s->list_entry);\r\nmutex_unlock(&stats->mutex);\r\nfor_each_possible_cpu(cpu)\r\nif (is_vmalloc_addr(s->stat_percpu))\r\ngoto do_sync_free;\r\nif (is_vmalloc_addr(s)) {\r\ndo_sync_free:\r\nsynchronize_rcu_expedited();\r\ndm_stat_free(&s->rcu_head);\r\n} else {\r\nACCESS_ONCE(dm_stat_need_rcu_barrier) = 1;\r\ncall_rcu(&s->rcu_head, dm_stat_free);\r\n}\r\nreturn 0;\r\n}\r\nstatic int dm_stats_list(struct dm_stats *stats, const char *program,\r\nchar *result, unsigned maxlen)\r\n{\r\nstruct dm_stat *s;\r\nsector_t len;\r\nunsigned sz = 0;\r\nmutex_lock(&stats->mutex);\r\nlist_for_each_entry(s, &stats->list, list_entry) {\r\nif (!program || !strcmp(program, s->program_id)) {\r\nlen = s->end - s->start;\r\nDMEMIT("%d: %llu+%llu %llu %s %s\n", s->id,\r\n(unsigned long long)s->start,\r\n(unsigned long long)len,\r\n(unsigned long long)s->step,\r\ns->program_id,\r\ns->aux_data);\r\n}\r\n}\r\nmutex_unlock(&stats->mutex);\r\nreturn 1;\r\n}\r\nstatic void dm_stat_round(struct dm_stat_shared *shared, struct dm_stat_percpu *p)\r\n{\r\nunsigned long now = jiffies;\r\nunsigned in_flight_read;\r\nunsigned in_flight_write;\r\nunsigned long difference = now - shared->stamp;\r\nif (!difference)\r\nreturn;\r\nin_flight_read = (unsigned)atomic_read(&shared->in_flight[READ]);\r\nin_flight_write = (unsigned)atomic_read(&shared->in_flight[WRITE]);\r\nif (in_flight_read)\r\np->io_ticks[READ] += difference;\r\nif (in_flight_write)\r\np->io_ticks[WRITE] += difference;\r\nif (in_flight_read + in_flight_write) {\r\np->io_ticks_total += difference;\r\np->time_in_queue += (in_flight_read + in_flight_write) * difference;\r\n}\r\nshared->stamp = now;\r\n}\r\nstatic void dm_stat_for_entry(struct dm_stat *s, size_t entry,\r\nunsigned long bi_rw, sector_t len, bool merged,\r\nbool end, unsigned long duration)\r\n{\r\nunsigned long idx = bi_rw & REQ_WRITE;\r\nstruct dm_stat_shared *shared = &s->stat_shared[entry];\r\nstruct dm_stat_percpu *p;\r\n#if BITS_PER_LONG == 32\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\n#else\r\npreempt_disable();\r\n#endif\r\np = &s->stat_percpu[smp_processor_id()][entry];\r\nif (!end) {\r\ndm_stat_round(shared, p);\r\natomic_inc(&shared->in_flight[idx]);\r\n} else {\r\ndm_stat_round(shared, p);\r\natomic_dec(&shared->in_flight[idx]);\r\np->sectors[idx] += len;\r\np->ios[idx] += 1;\r\np->merges[idx] += merged;\r\np->ticks[idx] += duration;\r\n}\r\n#if BITS_PER_LONG == 32\r\nlocal_irq_restore(flags);\r\n#else\r\npreempt_enable();\r\n#endif\r\n}\r\nstatic void __dm_stat_bio(struct dm_stat *s, unsigned long bi_rw,\r\nsector_t bi_sector, sector_t end_sector,\r\nbool end, unsigned long duration,\r\nstruct dm_stats_aux *stats_aux)\r\n{\r\nsector_t rel_sector, offset, todo, fragment_len;\r\nsize_t entry;\r\nif (end_sector <= s->start || bi_sector >= s->end)\r\nreturn;\r\nif (unlikely(bi_sector < s->start)) {\r\nrel_sector = 0;\r\ntodo = end_sector - s->start;\r\n} else {\r\nrel_sector = bi_sector - s->start;\r\ntodo = end_sector - bi_sector;\r\n}\r\nif (unlikely(end_sector > s->end))\r\ntodo -= (end_sector - s->end);\r\noffset = dm_sector_div64(rel_sector, s->step);\r\nentry = rel_sector;\r\ndo {\r\nif (WARN_ON_ONCE(entry >= s->n_entries)) {\r\nDMCRIT("Invalid area access in region id %d", s->id);\r\nreturn;\r\n}\r\nfragment_len = todo;\r\nif (fragment_len > s->step - offset)\r\nfragment_len = s->step - offset;\r\ndm_stat_for_entry(s, entry, bi_rw, fragment_len,\r\nstats_aux->merged, end, duration);\r\ntodo -= fragment_len;\r\nentry++;\r\noffset = 0;\r\n} while (unlikely(todo != 0));\r\n}\r\nvoid dm_stats_account_io(struct dm_stats *stats, unsigned long bi_rw,\r\nsector_t bi_sector, unsigned bi_sectors, bool end,\r\nunsigned long duration, struct dm_stats_aux *stats_aux)\r\n{\r\nstruct dm_stat *s;\r\nsector_t end_sector;\r\nstruct dm_stats_last_position *last;\r\nif (unlikely(!bi_sectors))\r\nreturn;\r\nend_sector = bi_sector + bi_sectors;\r\nif (!end) {\r\nlast = raw_cpu_ptr(stats->last);\r\nstats_aux->merged =\r\n(bi_sector == (ACCESS_ONCE(last->last_sector) &&\r\n((bi_rw & (REQ_WRITE | REQ_DISCARD)) ==\r\n(ACCESS_ONCE(last->last_rw) & (REQ_WRITE | REQ_DISCARD)))\r\n));\r\nACCESS_ONCE(last->last_sector) = end_sector;\r\nACCESS_ONCE(last->last_rw) = bi_rw;\r\n}\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(s, &stats->list, list_entry)\r\n__dm_stat_bio(s, bi_rw, bi_sector, end_sector, end, duration, stats_aux);\r\nrcu_read_unlock();\r\n}\r\nstatic void __dm_stat_init_temporary_percpu_totals(struct dm_stat_shared *shared,\r\nstruct dm_stat *s, size_t x)\r\n{\r\nint cpu;\r\nstruct dm_stat_percpu *p;\r\nlocal_irq_disable();\r\np = &s->stat_percpu[smp_processor_id()][x];\r\ndm_stat_round(shared, p);\r\nlocal_irq_enable();\r\nmemset(&shared->tmp, 0, sizeof(shared->tmp));\r\nfor_each_possible_cpu(cpu) {\r\np = &s->stat_percpu[cpu][x];\r\nshared->tmp.sectors[READ] += ACCESS_ONCE(p->sectors[READ]);\r\nshared->tmp.sectors[WRITE] += ACCESS_ONCE(p->sectors[WRITE]);\r\nshared->tmp.ios[READ] += ACCESS_ONCE(p->ios[READ]);\r\nshared->tmp.ios[WRITE] += ACCESS_ONCE(p->ios[WRITE]);\r\nshared->tmp.merges[READ] += ACCESS_ONCE(p->merges[READ]);\r\nshared->tmp.merges[WRITE] += ACCESS_ONCE(p->merges[WRITE]);\r\nshared->tmp.ticks[READ] += ACCESS_ONCE(p->ticks[READ]);\r\nshared->tmp.ticks[WRITE] += ACCESS_ONCE(p->ticks[WRITE]);\r\nshared->tmp.io_ticks[READ] += ACCESS_ONCE(p->io_ticks[READ]);\r\nshared->tmp.io_ticks[WRITE] += ACCESS_ONCE(p->io_ticks[WRITE]);\r\nshared->tmp.io_ticks_total += ACCESS_ONCE(p->io_ticks_total);\r\nshared->tmp.time_in_queue += ACCESS_ONCE(p->time_in_queue);\r\n}\r\n}\r\nstatic void __dm_stat_clear(struct dm_stat *s, size_t idx_start, size_t idx_end,\r\nbool init_tmp_percpu_totals)\r\n{\r\nsize_t x;\r\nstruct dm_stat_shared *shared;\r\nstruct dm_stat_percpu *p;\r\nfor (x = idx_start; x < idx_end; x++) {\r\nshared = &s->stat_shared[x];\r\nif (init_tmp_percpu_totals)\r\n__dm_stat_init_temporary_percpu_totals(shared, s, x);\r\nlocal_irq_disable();\r\np = &s->stat_percpu[smp_processor_id()][x];\r\np->sectors[READ] -= shared->tmp.sectors[READ];\r\np->sectors[WRITE] -= shared->tmp.sectors[WRITE];\r\np->ios[READ] -= shared->tmp.ios[READ];\r\np->ios[WRITE] -= shared->tmp.ios[WRITE];\r\np->merges[READ] -= shared->tmp.merges[READ];\r\np->merges[WRITE] -= shared->tmp.merges[WRITE];\r\np->ticks[READ] -= shared->tmp.ticks[READ];\r\np->ticks[WRITE] -= shared->tmp.ticks[WRITE];\r\np->io_ticks[READ] -= shared->tmp.io_ticks[READ];\r\np->io_ticks[WRITE] -= shared->tmp.io_ticks[WRITE];\r\np->io_ticks_total -= shared->tmp.io_ticks_total;\r\np->time_in_queue -= shared->tmp.time_in_queue;\r\nlocal_irq_enable();\r\n}\r\n}\r\nstatic int dm_stats_clear(struct dm_stats *stats, int id)\r\n{\r\nstruct dm_stat *s;\r\nmutex_lock(&stats->mutex);\r\ns = __dm_stats_find(stats, id);\r\nif (!s) {\r\nmutex_unlock(&stats->mutex);\r\nreturn -ENOENT;\r\n}\r\n__dm_stat_clear(s, 0, s->n_entries, true);\r\nmutex_unlock(&stats->mutex);\r\nreturn 1;\r\n}\r\nstatic unsigned long long dm_jiffies_to_msec64(unsigned long long j)\r\n{\r\nunsigned long long result = 0;\r\nunsigned mult;\r\nif (j)\r\nresult = jiffies_to_msecs(j & 0x3fffff);\r\nif (j >= 1 << 22) {\r\nmult = jiffies_to_msecs(1 << 22);\r\nresult += (unsigned long long)mult * (unsigned long long)jiffies_to_msecs((j >> 22) & 0x3fffff);\r\n}\r\nif (j >= 1ULL << 44)\r\nresult += (unsigned long long)mult * (unsigned long long)mult * (unsigned long long)jiffies_to_msecs(j >> 44);\r\nreturn result;\r\n}\r\nstatic int dm_stats_print(struct dm_stats *stats, int id,\r\nsize_t idx_start, size_t idx_len,\r\nbool clear, char *result, unsigned maxlen)\r\n{\r\nunsigned sz = 0;\r\nstruct dm_stat *s;\r\nsize_t x;\r\nsector_t start, end, step;\r\nsize_t idx_end;\r\nstruct dm_stat_shared *shared;\r\nmutex_lock(&stats->mutex);\r\ns = __dm_stats_find(stats, id);\r\nif (!s) {\r\nmutex_unlock(&stats->mutex);\r\nreturn -ENOENT;\r\n}\r\nidx_end = idx_start + idx_len;\r\nif (idx_end < idx_start ||\r\nidx_end > s->n_entries)\r\nidx_end = s->n_entries;\r\nif (idx_start > idx_end)\r\nidx_start = idx_end;\r\nstep = s->step;\r\nstart = s->start + (step * idx_start);\r\nfor (x = idx_start; x < idx_end; x++, start = end) {\r\nshared = &s->stat_shared[x];\r\nend = start + step;\r\nif (unlikely(end > s->end))\r\nend = s->end;\r\n__dm_stat_init_temporary_percpu_totals(shared, s, x);\r\nDMEMIT("%llu+%llu %llu %llu %llu %llu %llu %llu %llu %llu %d %llu %llu %llu %llu\n",\r\n(unsigned long long)start,\r\n(unsigned long long)step,\r\nshared->tmp.ios[READ],\r\nshared->tmp.merges[READ],\r\nshared->tmp.sectors[READ],\r\ndm_jiffies_to_msec64(shared->tmp.ticks[READ]),\r\nshared->tmp.ios[WRITE],\r\nshared->tmp.merges[WRITE],\r\nshared->tmp.sectors[WRITE],\r\ndm_jiffies_to_msec64(shared->tmp.ticks[WRITE]),\r\ndm_stat_in_flight(shared),\r\ndm_jiffies_to_msec64(shared->tmp.io_ticks_total),\r\ndm_jiffies_to_msec64(shared->tmp.time_in_queue),\r\ndm_jiffies_to_msec64(shared->tmp.io_ticks[READ]),\r\ndm_jiffies_to_msec64(shared->tmp.io_ticks[WRITE]));\r\nif (unlikely(sz + 1 >= maxlen))\r\ngoto buffer_overflow;\r\n}\r\nif (clear)\r\n__dm_stat_clear(s, idx_start, idx_end, false);\r\nbuffer_overflow:\r\nmutex_unlock(&stats->mutex);\r\nreturn 1;\r\n}\r\nstatic int dm_stats_set_aux(struct dm_stats *stats, int id, const char *aux_data)\r\n{\r\nstruct dm_stat *s;\r\nconst char *new_aux_data;\r\nmutex_lock(&stats->mutex);\r\ns = __dm_stats_find(stats, id);\r\nif (!s) {\r\nmutex_unlock(&stats->mutex);\r\nreturn -ENOENT;\r\n}\r\nnew_aux_data = kstrdup(aux_data, GFP_KERNEL);\r\nif (!new_aux_data) {\r\nmutex_unlock(&stats->mutex);\r\nreturn -ENOMEM;\r\n}\r\nkfree(s->aux_data);\r\ns->aux_data = new_aux_data;\r\nmutex_unlock(&stats->mutex);\r\nreturn 0;\r\n}\r\nstatic int message_stats_create(struct mapped_device *md,\r\nunsigned argc, char **argv,\r\nchar *result, unsigned maxlen)\r\n{\r\nint id;\r\nchar dummy;\r\nunsigned long long start, end, len, step;\r\nunsigned divisor;\r\nconst char *program_id, *aux_data;\r\nif (argc < 3 || argc > 5)\r\nreturn -EINVAL;\r\nif (!strcmp(argv[1], "-")) {\r\nstart = 0;\r\nlen = dm_get_size(md);\r\nif (!len)\r\nlen = 1;\r\n} else if (sscanf(argv[1], "%llu+%llu%c", &start, &len, &dummy) != 2 ||\r\nstart != (sector_t)start || len != (sector_t)len)\r\nreturn -EINVAL;\r\nend = start + len;\r\nif (start >= end)\r\nreturn -EINVAL;\r\nif (sscanf(argv[2], "/%u%c", &divisor, &dummy) == 1) {\r\nstep = end - start;\r\nif (do_div(step, divisor))\r\nstep++;\r\nif (!step)\r\nstep = 1;\r\n} else if (sscanf(argv[2], "%llu%c", &step, &dummy) != 1 ||\r\nstep != (sector_t)step || !step)\r\nreturn -EINVAL;\r\nprogram_id = "-";\r\naux_data = "-";\r\nif (argc > 3)\r\nprogram_id = argv[3];\r\nif (argc > 4)\r\naux_data = argv[4];\r\nsnprintf(result, maxlen, "%d", INT_MAX);\r\nif (dm_message_test_buffer_overflow(result, maxlen))\r\nreturn 1;\r\nid = dm_stats_create(dm_get_stats(md), start, end, step, program_id, aux_data,\r\ndm_internal_suspend_fast, dm_internal_resume_fast, md);\r\nif (id < 0)\r\nreturn id;\r\nsnprintf(result, maxlen, "%d", id);\r\nreturn 1;\r\n}\r\nstatic int message_stats_delete(struct mapped_device *md,\r\nunsigned argc, char **argv)\r\n{\r\nint id;\r\nchar dummy;\r\nif (argc != 2)\r\nreturn -EINVAL;\r\nif (sscanf(argv[1], "%d%c", &id, &dummy) != 1 || id < 0)\r\nreturn -EINVAL;\r\nreturn dm_stats_delete(dm_get_stats(md), id);\r\n}\r\nstatic int message_stats_clear(struct mapped_device *md,\r\nunsigned argc, char **argv)\r\n{\r\nint id;\r\nchar dummy;\r\nif (argc != 2)\r\nreturn -EINVAL;\r\nif (sscanf(argv[1], "%d%c", &id, &dummy) != 1 || id < 0)\r\nreturn -EINVAL;\r\nreturn dm_stats_clear(dm_get_stats(md), id);\r\n}\r\nstatic int message_stats_list(struct mapped_device *md,\r\nunsigned argc, char **argv,\r\nchar *result, unsigned maxlen)\r\n{\r\nint r;\r\nconst char *program = NULL;\r\nif (argc < 1 || argc > 2)\r\nreturn -EINVAL;\r\nif (argc > 1) {\r\nprogram = kstrdup(argv[1], GFP_KERNEL);\r\nif (!program)\r\nreturn -ENOMEM;\r\n}\r\nr = dm_stats_list(dm_get_stats(md), program, result, maxlen);\r\nkfree(program);\r\nreturn r;\r\n}\r\nstatic int message_stats_print(struct mapped_device *md,\r\nunsigned argc, char **argv, bool clear,\r\nchar *result, unsigned maxlen)\r\n{\r\nint id;\r\nchar dummy;\r\nunsigned long idx_start = 0, idx_len = ULONG_MAX;\r\nif (argc != 2 && argc != 4)\r\nreturn -EINVAL;\r\nif (sscanf(argv[1], "%d%c", &id, &dummy) != 1 || id < 0)\r\nreturn -EINVAL;\r\nif (argc > 3) {\r\nif (strcmp(argv[2], "-") &&\r\nsscanf(argv[2], "%lu%c", &idx_start, &dummy) != 1)\r\nreturn -EINVAL;\r\nif (strcmp(argv[3], "-") &&\r\nsscanf(argv[3], "%lu%c", &idx_len, &dummy) != 1)\r\nreturn -EINVAL;\r\n}\r\nreturn dm_stats_print(dm_get_stats(md), id, idx_start, idx_len, clear,\r\nresult, maxlen);\r\n}\r\nstatic int message_stats_set_aux(struct mapped_device *md,\r\nunsigned argc, char **argv)\r\n{\r\nint id;\r\nchar dummy;\r\nif (argc != 3)\r\nreturn -EINVAL;\r\nif (sscanf(argv[1], "%d%c", &id, &dummy) != 1 || id < 0)\r\nreturn -EINVAL;\r\nreturn dm_stats_set_aux(dm_get_stats(md), id, argv[2]);\r\n}\r\nint dm_stats_message(struct mapped_device *md, unsigned argc, char **argv,\r\nchar *result, unsigned maxlen)\r\n{\r\nint r;\r\nif (dm_request_based(md)) {\r\nDMWARN("Statistics are only supported for bio-based devices");\r\nreturn -EOPNOTSUPP;\r\n}\r\nif (!strcasecmp(argv[0], "@stats_create"))\r\nr = message_stats_create(md, argc, argv, result, maxlen);\r\nelse if (!strcasecmp(argv[0], "@stats_delete"))\r\nr = message_stats_delete(md, argc, argv);\r\nelse if (!strcasecmp(argv[0], "@stats_clear"))\r\nr = message_stats_clear(md, argc, argv);\r\nelse if (!strcasecmp(argv[0], "@stats_list"))\r\nr = message_stats_list(md, argc, argv, result, maxlen);\r\nelse if (!strcasecmp(argv[0], "@stats_print"))\r\nr = message_stats_print(md, argc, argv, false, result, maxlen);\r\nelse if (!strcasecmp(argv[0], "@stats_print_clear"))\r\nr = message_stats_print(md, argc, argv, true, result, maxlen);\r\nelse if (!strcasecmp(argv[0], "@stats_set_aux"))\r\nr = message_stats_set_aux(md, argc, argv);\r\nelse\r\nreturn 2;\r\nif (r == -EINVAL)\r\nDMWARN("Invalid parameters for message %s", argv[0]);\r\nreturn r;\r\n}\r\nint __init dm_statistics_init(void)\r\n{\r\nshared_memory_amount = 0;\r\ndm_stat_need_rcu_barrier = 0;\r\nreturn 0;\r\n}\r\nvoid dm_statistics_exit(void)\r\n{\r\nif (dm_stat_need_rcu_barrier)\r\nrcu_barrier();\r\nif (WARN_ON(shared_memory_amount))\r\nDMCRIT("shared_memory_amount leaked: %lu", shared_memory_amount);\r\n}
