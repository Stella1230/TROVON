static unsigned int rds_pages_in_vec(struct rds_iovec *vec)\r\n{\r\nif ((vec->addr + vec->bytes <= vec->addr) ||\r\n(vec->bytes > (u64)UINT_MAX))\r\nreturn 0;\r\nreturn ((vec->addr + vec->bytes + PAGE_SIZE - 1) >> PAGE_SHIFT) -\r\n(vec->addr >> PAGE_SHIFT);\r\n}\r\nstatic struct rds_mr *rds_mr_tree_walk(struct rb_root *root, u64 key,\r\nstruct rds_mr *insert)\r\n{\r\nstruct rb_node **p = &root->rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct rds_mr *mr;\r\nwhile (*p) {\r\nparent = *p;\r\nmr = rb_entry(parent, struct rds_mr, r_rb_node);\r\nif (key < mr->r_key)\r\np = &(*p)->rb_left;\r\nelse if (key > mr->r_key)\r\np = &(*p)->rb_right;\r\nelse\r\nreturn mr;\r\n}\r\nif (insert) {\r\nrb_link_node(&insert->r_rb_node, parent, p);\r\nrb_insert_color(&insert->r_rb_node, root);\r\natomic_inc(&insert->r_refcount);\r\n}\r\nreturn NULL;\r\n}\r\nstatic void rds_destroy_mr(struct rds_mr *mr)\r\n{\r\nstruct rds_sock *rs = mr->r_sock;\r\nvoid *trans_private = NULL;\r\nunsigned long flags;\r\nrdsdebug("RDS: destroy mr key is %x refcnt %u\n",\r\nmr->r_key, atomic_read(&mr->r_refcount));\r\nif (test_and_set_bit(RDS_MR_DEAD, &mr->r_state))\r\nreturn;\r\nspin_lock_irqsave(&rs->rs_rdma_lock, flags);\r\nif (!RB_EMPTY_NODE(&mr->r_rb_node))\r\nrb_erase(&mr->r_rb_node, &rs->rs_rdma_keys);\r\ntrans_private = mr->r_trans_private;\r\nmr->r_trans_private = NULL;\r\nspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\r\nif (trans_private)\r\nmr->r_trans->free_mr(trans_private, mr->r_invalidate);\r\n}\r\nvoid __rds_put_mr_final(struct rds_mr *mr)\r\n{\r\nrds_destroy_mr(mr);\r\nkfree(mr);\r\n}\r\nvoid rds_rdma_drop_keys(struct rds_sock *rs)\r\n{\r\nstruct rds_mr *mr;\r\nstruct rb_node *node;\r\nunsigned long flags;\r\nspin_lock_irqsave(&rs->rs_rdma_lock, flags);\r\nwhile ((node = rb_first(&rs->rs_rdma_keys))) {\r\nmr = container_of(node, struct rds_mr, r_rb_node);\r\nif (mr->r_trans == rs->rs_transport)\r\nmr->r_invalidate = 0;\r\nrb_erase(&mr->r_rb_node, &rs->rs_rdma_keys);\r\nRB_CLEAR_NODE(&mr->r_rb_node);\r\nspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\r\nrds_destroy_mr(mr);\r\nrds_mr_put(mr);\r\nspin_lock_irqsave(&rs->rs_rdma_lock, flags);\r\n}\r\nspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\r\nif (rs->rs_transport && rs->rs_transport->flush_mrs)\r\nrs->rs_transport->flush_mrs();\r\n}\r\nstatic int rds_pin_pages(unsigned long user_addr, unsigned int nr_pages,\r\nstruct page **pages, int write)\r\n{\r\nint ret;\r\nret = get_user_pages_fast(user_addr, nr_pages, write, pages);\r\nif (ret >= 0 && ret < nr_pages) {\r\nwhile (ret--)\r\nput_page(pages[ret]);\r\nret = -EFAULT;\r\n}\r\nreturn ret;\r\n}\r\nstatic int __rds_rdma_map(struct rds_sock *rs, struct rds_get_mr_args *args,\r\nu64 *cookie_ret, struct rds_mr **mr_ret)\r\n{\r\nstruct rds_mr *mr = NULL, *found;\r\nunsigned int nr_pages;\r\nstruct page **pages = NULL;\r\nstruct scatterlist *sg;\r\nvoid *trans_private;\r\nunsigned long flags;\r\nrds_rdma_cookie_t cookie;\r\nunsigned int nents;\r\nlong i;\r\nint ret;\r\nif (rs->rs_bound_addr == 0) {\r\nret = -ENOTCONN;\r\ngoto out;\r\n}\r\nif (!rs->rs_transport->get_mr) {\r\nret = -EOPNOTSUPP;\r\ngoto out;\r\n}\r\nnr_pages = rds_pages_in_vec(&args->vec);\r\nif (nr_pages == 0) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nrdsdebug("RDS: get_mr addr %llx len %llu nr_pages %u\n",\r\nargs->vec.addr, args->vec.bytes, nr_pages);\r\npages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\r\nif (!pages) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nmr = kzalloc(sizeof(struct rds_mr), GFP_KERNEL);\r\nif (!mr) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\natomic_set(&mr->r_refcount, 1);\r\nRB_CLEAR_NODE(&mr->r_rb_node);\r\nmr->r_trans = rs->rs_transport;\r\nmr->r_sock = rs;\r\nif (args->flags & RDS_RDMA_USE_ONCE)\r\nmr->r_use_once = 1;\r\nif (args->flags & RDS_RDMA_INVALIDATE)\r\nmr->r_invalidate = 1;\r\nif (args->flags & RDS_RDMA_READWRITE)\r\nmr->r_write = 1;\r\nret = rds_pin_pages(args->vec.addr, nr_pages, pages, 1);\r\nif (ret < 0)\r\ngoto out;\r\nnents = ret;\r\nsg = kcalloc(nents, sizeof(*sg), GFP_KERNEL);\r\nif (!sg) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nWARN_ON(!nents);\r\nsg_init_table(sg, nents);\r\nfor (i = 0 ; i < nents; i++)\r\nsg_set_page(&sg[i], pages[i], PAGE_SIZE, 0);\r\nrdsdebug("RDS: trans_private nents is %u\n", nents);\r\ntrans_private = rs->rs_transport->get_mr(sg, nents, rs,\r\n&mr->r_key);\r\nif (IS_ERR(trans_private)) {\r\nfor (i = 0 ; i < nents; i++)\r\nput_page(sg_page(&sg[i]));\r\nkfree(sg);\r\nret = PTR_ERR(trans_private);\r\ngoto out;\r\n}\r\nmr->r_trans_private = trans_private;\r\nrdsdebug("RDS: get_mr put_user key is %x cookie_addr %p\n",\r\nmr->r_key, (void *)(unsigned long) args->cookie_addr);\r\ncookie = rds_rdma_make_cookie(mr->r_key, args->vec.addr & ~PAGE_MASK);\r\nif (cookie_ret)\r\n*cookie_ret = cookie;\r\nif (args->cookie_addr && put_user(cookie, (u64 __user *)(unsigned long) args->cookie_addr)) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\nspin_lock_irqsave(&rs->rs_rdma_lock, flags);\r\nfound = rds_mr_tree_walk(&rs->rs_rdma_keys, mr->r_key, mr);\r\nspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\r\nBUG_ON(found && found != mr);\r\nrdsdebug("RDS: get_mr key is %x\n", mr->r_key);\r\nif (mr_ret) {\r\natomic_inc(&mr->r_refcount);\r\n*mr_ret = mr;\r\n}\r\nret = 0;\r\nout:\r\nkfree(pages);\r\nif (mr)\r\nrds_mr_put(mr);\r\nreturn ret;\r\n}\r\nint rds_get_mr(struct rds_sock *rs, char __user *optval, int optlen)\r\n{\r\nstruct rds_get_mr_args args;\r\nif (optlen != sizeof(struct rds_get_mr_args))\r\nreturn -EINVAL;\r\nif (copy_from_user(&args, (struct rds_get_mr_args __user *)optval,\r\nsizeof(struct rds_get_mr_args)))\r\nreturn -EFAULT;\r\nreturn __rds_rdma_map(rs, &args, NULL, NULL);\r\n}\r\nint rds_get_mr_for_dest(struct rds_sock *rs, char __user *optval, int optlen)\r\n{\r\nstruct rds_get_mr_for_dest_args args;\r\nstruct rds_get_mr_args new_args;\r\nif (optlen != sizeof(struct rds_get_mr_for_dest_args))\r\nreturn -EINVAL;\r\nif (copy_from_user(&args, (struct rds_get_mr_for_dest_args __user *)optval,\r\nsizeof(struct rds_get_mr_for_dest_args)))\r\nreturn -EFAULT;\r\nnew_args.vec = args.vec;\r\nnew_args.cookie_addr = args.cookie_addr;\r\nnew_args.flags = args.flags;\r\nreturn __rds_rdma_map(rs, &new_args, NULL, NULL);\r\n}\r\nint rds_free_mr(struct rds_sock *rs, char __user *optval, int optlen)\r\n{\r\nstruct rds_free_mr_args args;\r\nstruct rds_mr *mr;\r\nunsigned long flags;\r\nif (optlen != sizeof(struct rds_free_mr_args))\r\nreturn -EINVAL;\r\nif (copy_from_user(&args, (struct rds_free_mr_args __user *)optval,\r\nsizeof(struct rds_free_mr_args)))\r\nreturn -EFAULT;\r\nif (args.cookie == 0) {\r\nif (!rs->rs_transport || !rs->rs_transport->flush_mrs)\r\nreturn -EINVAL;\r\nrs->rs_transport->flush_mrs();\r\nreturn 0;\r\n}\r\nspin_lock_irqsave(&rs->rs_rdma_lock, flags);\r\nmr = rds_mr_tree_walk(&rs->rs_rdma_keys, rds_rdma_cookie_key(args.cookie), NULL);\r\nif (mr) {\r\nrb_erase(&mr->r_rb_node, &rs->rs_rdma_keys);\r\nRB_CLEAR_NODE(&mr->r_rb_node);\r\nif (args.flags & RDS_RDMA_INVALIDATE)\r\nmr->r_invalidate = 1;\r\n}\r\nspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\r\nif (!mr)\r\nreturn -EINVAL;\r\nrds_destroy_mr(mr);\r\nrds_mr_put(mr);\r\nreturn 0;\r\n}\r\nvoid rds_rdma_unuse(struct rds_sock *rs, u32 r_key, int force)\r\n{\r\nstruct rds_mr *mr;\r\nunsigned long flags;\r\nint zot_me = 0;\r\nspin_lock_irqsave(&rs->rs_rdma_lock, flags);\r\nmr = rds_mr_tree_walk(&rs->rs_rdma_keys, r_key, NULL);\r\nif (!mr) {\r\nprintk(KERN_ERR "rds: trying to unuse MR with unknown r_key %u!\n", r_key);\r\nspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\r\nreturn;\r\n}\r\nif (mr->r_use_once || force) {\r\nrb_erase(&mr->r_rb_node, &rs->rs_rdma_keys);\r\nRB_CLEAR_NODE(&mr->r_rb_node);\r\nzot_me = 1;\r\n}\r\nspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\r\nif (mr->r_trans->sync_mr)\r\nmr->r_trans->sync_mr(mr->r_trans_private, DMA_FROM_DEVICE);\r\nif (zot_me)\r\nrds_destroy_mr(mr);\r\nrds_mr_put(mr);\r\n}\r\nvoid rds_rdma_free_op(struct rm_rdma_op *ro)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < ro->op_nents; i++) {\r\nstruct page *page = sg_page(&ro->op_sg[i]);\r\nif (!ro->op_write) {\r\nBUG_ON(irqs_disabled());\r\nset_page_dirty(page);\r\n}\r\nput_page(page);\r\n}\r\nkfree(ro->op_notifier);\r\nro->op_notifier = NULL;\r\nro->op_active = 0;\r\n}\r\nvoid rds_atomic_free_op(struct rm_atomic_op *ao)\r\n{\r\nstruct page *page = sg_page(ao->op_sg);\r\nset_page_dirty(page);\r\nput_page(page);\r\nkfree(ao->op_notifier);\r\nao->op_notifier = NULL;\r\nao->op_active = 0;\r\n}\r\nstatic int rds_rdma_pages(struct rds_iovec iov[], int nr_iovecs)\r\n{\r\nint tot_pages = 0;\r\nunsigned int nr_pages;\r\nunsigned int i;\r\nfor (i = 0; i < nr_iovecs; i++) {\r\nnr_pages = rds_pages_in_vec(&iov[i]);\r\nif (nr_pages == 0)\r\nreturn -EINVAL;\r\ntot_pages += nr_pages;\r\nif (tot_pages < 0)\r\nreturn -EINVAL;\r\n}\r\nreturn tot_pages;\r\n}\r\nint rds_rdma_extra_size(struct rds_rdma_args *args)\r\n{\r\nstruct rds_iovec vec;\r\nstruct rds_iovec __user *local_vec;\r\nint tot_pages = 0;\r\nunsigned int nr_pages;\r\nunsigned int i;\r\nlocal_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;\r\nfor (i = 0; i < args->nr_local; i++) {\r\nif (copy_from_user(&vec, &local_vec[i],\r\nsizeof(struct rds_iovec)))\r\nreturn -EFAULT;\r\nnr_pages = rds_pages_in_vec(&vec);\r\nif (nr_pages == 0)\r\nreturn -EINVAL;\r\ntot_pages += nr_pages;\r\nif (tot_pages < 0)\r\nreturn -EINVAL;\r\n}\r\nreturn tot_pages * sizeof(struct scatterlist);\r\n}\r\nint rds_cmsg_rdma_args(struct rds_sock *rs, struct rds_message *rm,\r\nstruct cmsghdr *cmsg)\r\n{\r\nstruct rds_rdma_args *args;\r\nstruct rm_rdma_op *op = &rm->rdma;\r\nint nr_pages;\r\nunsigned int nr_bytes;\r\nstruct page **pages = NULL;\r\nstruct rds_iovec iovstack[UIO_FASTIOV], *iovs = iovstack;\r\nint iov_size;\r\nunsigned int i, j;\r\nint ret = 0;\r\nif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct rds_rdma_args))\r\n|| rm->rdma.op_active)\r\nreturn -EINVAL;\r\nargs = CMSG_DATA(cmsg);\r\nif (rs->rs_bound_addr == 0) {\r\nret = -ENOTCONN;\r\ngoto out_ret;\r\n}\r\nif (args->nr_local > UIO_MAXIOV) {\r\nret = -EMSGSIZE;\r\ngoto out_ret;\r\n}\r\niov_size = args->nr_local * sizeof(struct rds_iovec);\r\nif (args->nr_local > UIO_FASTIOV) {\r\niovs = sock_kmalloc(rds_rs_to_sk(rs), iov_size, GFP_KERNEL);\r\nif (!iovs) {\r\nret = -ENOMEM;\r\ngoto out_ret;\r\n}\r\n}\r\nif (copy_from_user(iovs, (struct rds_iovec __user *)(unsigned long) args->local_vec_addr, iov_size)) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\nnr_pages = rds_rdma_pages(iovs, args->nr_local);\r\nif (nr_pages < 0) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\npages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\r\nif (!pages) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nop->op_write = !!(args->flags & RDS_RDMA_READWRITE);\r\nop->op_fence = !!(args->flags & RDS_RDMA_FENCE);\r\nop->op_notify = !!(args->flags & RDS_RDMA_NOTIFY_ME);\r\nop->op_silent = !!(args->flags & RDS_RDMA_SILENT);\r\nop->op_active = 1;\r\nop->op_recverr = rs->rs_recverr;\r\nWARN_ON(!nr_pages);\r\nop->op_sg = rds_message_alloc_sgs(rm, nr_pages);\r\nif (!op->op_sg) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (op->op_notify || op->op_recverr) {\r\nop->op_notifier = kmalloc(sizeof(struct rds_notifier), GFP_KERNEL);\r\nif (!op->op_notifier) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nop->op_notifier->n_user_token = args->user_token;\r\nop->op_notifier->n_status = RDS_RDMA_SUCCESS;\r\n}\r\nop->op_rkey = rds_rdma_cookie_key(args->cookie);\r\nop->op_remote_addr = args->remote_vec.addr + rds_rdma_cookie_offset(args->cookie);\r\nnr_bytes = 0;\r\nrdsdebug("RDS: rdma prepare nr_local %llu rva %llx rkey %x\n",\r\n(unsigned long long)args->nr_local,\r\n(unsigned long long)args->remote_vec.addr,\r\nop->op_rkey);\r\nfor (i = 0; i < args->nr_local; i++) {\r\nstruct rds_iovec *iov = &iovs[i];\r\nunsigned int nr = rds_pages_in_vec(iov);\r\nrs->rs_user_addr = iov->addr;\r\nrs->rs_user_bytes = iov->bytes;\r\nret = rds_pin_pages(iov->addr, nr, pages, !op->op_write);\r\nif (ret < 0)\r\ngoto out;\r\nrdsdebug("RDS: nr_bytes %u nr %u iov->bytes %llu iov->addr %llx\n",\r\nnr_bytes, nr, iov->bytes, iov->addr);\r\nnr_bytes += iov->bytes;\r\nfor (j = 0; j < nr; j++) {\r\nunsigned int offset = iov->addr & ~PAGE_MASK;\r\nstruct scatterlist *sg;\r\nsg = &op->op_sg[op->op_nents + j];\r\nsg_set_page(sg, pages[j],\r\nmin_t(unsigned int, iov->bytes, PAGE_SIZE - offset),\r\noffset);\r\nrdsdebug("RDS: sg->offset %x sg->len %x iov->addr %llx iov->bytes %llu\n",\r\nsg->offset, sg->length, iov->addr, iov->bytes);\r\niov->addr += sg->length;\r\niov->bytes -= sg->length;\r\n}\r\nop->op_nents += nr;\r\n}\r\nif (nr_bytes > args->remote_vec.bytes) {\r\nrdsdebug("RDS nr_bytes %u remote_bytes %u do not match\n",\r\nnr_bytes,\r\n(unsigned int) args->remote_vec.bytes);\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nop->op_bytes = nr_bytes;\r\nout:\r\nif (iovs != iovstack)\r\nsock_kfree_s(rds_rs_to_sk(rs), iovs, iov_size);\r\nkfree(pages);\r\nout_ret:\r\nif (ret)\r\nrds_rdma_free_op(op);\r\nelse\r\nrds_stats_inc(s_send_rdma);\r\nreturn ret;\r\n}\r\nint rds_cmsg_rdma_dest(struct rds_sock *rs, struct rds_message *rm,\r\nstruct cmsghdr *cmsg)\r\n{\r\nunsigned long flags;\r\nstruct rds_mr *mr;\r\nu32 r_key;\r\nint err = 0;\r\nif (cmsg->cmsg_len < CMSG_LEN(sizeof(rds_rdma_cookie_t)) ||\r\nrm->m_rdma_cookie != 0)\r\nreturn -EINVAL;\r\nmemcpy(&rm->m_rdma_cookie, CMSG_DATA(cmsg), sizeof(rm->m_rdma_cookie));\r\nr_key = rds_rdma_cookie_key(rm->m_rdma_cookie);\r\nspin_lock_irqsave(&rs->rs_rdma_lock, flags);\r\nmr = rds_mr_tree_walk(&rs->rs_rdma_keys, r_key, NULL);\r\nif (!mr)\r\nerr = -EINVAL;\r\nelse\r\natomic_inc(&mr->r_refcount);\r\nspin_unlock_irqrestore(&rs->rs_rdma_lock, flags);\r\nif (mr) {\r\nmr->r_trans->sync_mr(mr->r_trans_private, DMA_TO_DEVICE);\r\nrm->rdma.op_rdma_mr = mr;\r\n}\r\nreturn err;\r\n}\r\nint rds_cmsg_rdma_map(struct rds_sock *rs, struct rds_message *rm,\r\nstruct cmsghdr *cmsg)\r\n{\r\nif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct rds_get_mr_args)) ||\r\nrm->m_rdma_cookie != 0)\r\nreturn -EINVAL;\r\nreturn __rds_rdma_map(rs, CMSG_DATA(cmsg), &rm->m_rdma_cookie, &rm->rdma.op_rdma_mr);\r\n}\r\nint rds_cmsg_atomic(struct rds_sock *rs, struct rds_message *rm,\r\nstruct cmsghdr *cmsg)\r\n{\r\nstruct page *page = NULL;\r\nstruct rds_atomic_args *args;\r\nint ret = 0;\r\nif (cmsg->cmsg_len < CMSG_LEN(sizeof(struct rds_atomic_args))\r\n|| rm->atomic.op_active)\r\nreturn -EINVAL;\r\nargs = CMSG_DATA(cmsg);\r\nswitch (cmsg->cmsg_type) {\r\ncase RDS_CMSG_ATOMIC_FADD:\r\nrm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\r\nrm->atomic.op_m_fadd.add = args->fadd.add;\r\nrm->atomic.op_m_fadd.nocarry_mask = 0;\r\nbreak;\r\ncase RDS_CMSG_MASKED_ATOMIC_FADD:\r\nrm->atomic.op_type = RDS_ATOMIC_TYPE_FADD;\r\nrm->atomic.op_m_fadd.add = args->m_fadd.add;\r\nrm->atomic.op_m_fadd.nocarry_mask = args->m_fadd.nocarry_mask;\r\nbreak;\r\ncase RDS_CMSG_ATOMIC_CSWP:\r\nrm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\r\nrm->atomic.op_m_cswp.compare = args->cswp.compare;\r\nrm->atomic.op_m_cswp.swap = args->cswp.swap;\r\nrm->atomic.op_m_cswp.compare_mask = ~0;\r\nrm->atomic.op_m_cswp.swap_mask = ~0;\r\nbreak;\r\ncase RDS_CMSG_MASKED_ATOMIC_CSWP:\r\nrm->atomic.op_type = RDS_ATOMIC_TYPE_CSWP;\r\nrm->atomic.op_m_cswp.compare = args->m_cswp.compare;\r\nrm->atomic.op_m_cswp.swap = args->m_cswp.swap;\r\nrm->atomic.op_m_cswp.compare_mask = args->m_cswp.compare_mask;\r\nrm->atomic.op_m_cswp.swap_mask = args->m_cswp.swap_mask;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nrm->atomic.op_notify = !!(args->flags & RDS_RDMA_NOTIFY_ME);\r\nrm->atomic.op_silent = !!(args->flags & RDS_RDMA_SILENT);\r\nrm->atomic.op_active = 1;\r\nrm->atomic.op_recverr = rs->rs_recverr;\r\nrm->atomic.op_sg = rds_message_alloc_sgs(rm, 1);\r\nif (!rm->atomic.op_sg) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nif (args->local_addr & 0x7) {\r\nret = -EFAULT;\r\ngoto err;\r\n}\r\nret = rds_pin_pages(args->local_addr, 1, &page, 1);\r\nif (ret != 1)\r\ngoto err;\r\nret = 0;\r\nsg_set_page(rm->atomic.op_sg, page, 8, offset_in_page(args->local_addr));\r\nif (rm->atomic.op_notify || rm->atomic.op_recverr) {\r\nrm->atomic.op_notifier = kmalloc(sizeof(*rm->atomic.op_notifier), GFP_KERNEL);\r\nif (!rm->atomic.op_notifier) {\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\nrm->atomic.op_notifier->n_user_token = args->user_token;\r\nrm->atomic.op_notifier->n_status = RDS_RDMA_SUCCESS;\r\n}\r\nrm->atomic.op_rkey = rds_rdma_cookie_key(args->cookie);\r\nrm->atomic.op_remote_addr = args->remote_addr + rds_rdma_cookie_offset(args->cookie);\r\nreturn ret;\r\nerr:\r\nif (page)\r\nput_page(page);\r\nkfree(rm->atomic.op_notifier);\r\nreturn ret;\r\n}
