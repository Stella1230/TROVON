static int udl_attach_dma_buf(struct dma_buf *dmabuf,\r\nstruct device *dev,\r\nstruct dma_buf_attachment *attach)\r\n{\r\nstruct udl_drm_dmabuf_attachment *udl_attach;\r\nDRM_DEBUG_PRIME("[DEV:%s] size:%zd\n", dev_name(attach->dev),\r\nattach->dmabuf->size);\r\nudl_attach = kzalloc(sizeof(*udl_attach), GFP_KERNEL);\r\nif (!udl_attach)\r\nreturn -ENOMEM;\r\nudl_attach->dir = DMA_NONE;\r\nattach->priv = udl_attach;\r\nreturn 0;\r\n}\r\nstatic void udl_detach_dma_buf(struct dma_buf *dmabuf,\r\nstruct dma_buf_attachment *attach)\r\n{\r\nstruct udl_drm_dmabuf_attachment *udl_attach = attach->priv;\r\nstruct sg_table *sgt;\r\nif (!udl_attach)\r\nreturn;\r\nDRM_DEBUG_PRIME("[DEV:%s] size:%zd\n", dev_name(attach->dev),\r\nattach->dmabuf->size);\r\nsgt = &udl_attach->sgt;\r\nif (udl_attach->dir != DMA_NONE)\r\ndma_unmap_sg(attach->dev, sgt->sgl, sgt->nents,\r\nudl_attach->dir);\r\nsg_free_table(sgt);\r\nkfree(udl_attach);\r\nattach->priv = NULL;\r\n}\r\nstatic struct sg_table *udl_map_dma_buf(struct dma_buf_attachment *attach,\r\nenum dma_data_direction dir)\r\n{\r\nstruct udl_drm_dmabuf_attachment *udl_attach = attach->priv;\r\nstruct udl_gem_object *obj = to_udl_bo(attach->dmabuf->priv);\r\nstruct drm_device *dev = obj->base.dev;\r\nstruct scatterlist *rd, *wr;\r\nstruct sg_table *sgt = NULL;\r\nunsigned int i;\r\nint page_count;\r\nint nents, ret;\r\nDRM_DEBUG_PRIME("[DEV:%s] size:%zd dir=%d\n", dev_name(attach->dev),\r\nattach->dmabuf->size, dir);\r\nif (udl_attach->dir == dir && udl_attach->is_mapped)\r\nreturn &udl_attach->sgt;\r\nif (!obj->pages) {\r\nret = udl_gem_get_pages(obj);\r\nif (ret) {\r\nDRM_ERROR("failed to map pages.\n");\r\nreturn ERR_PTR(ret);\r\n}\r\n}\r\npage_count = obj->base.size / PAGE_SIZE;\r\nobj->sg = drm_prime_pages_to_sg(obj->pages, page_count);\r\nif (IS_ERR(obj->sg)) {\r\nDRM_ERROR("failed to allocate sgt.\n");\r\nreturn ERR_CAST(obj->sg);\r\n}\r\nsgt = &udl_attach->sgt;\r\nret = sg_alloc_table(sgt, obj->sg->orig_nents, GFP_KERNEL);\r\nif (ret) {\r\nDRM_ERROR("failed to alloc sgt.\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nmutex_lock(&dev->struct_mutex);\r\nrd = obj->sg->sgl;\r\nwr = sgt->sgl;\r\nfor (i = 0; i < sgt->orig_nents; ++i) {\r\nsg_set_page(wr, sg_page(rd), rd->length, rd->offset);\r\nrd = sg_next(rd);\r\nwr = sg_next(wr);\r\n}\r\nif (dir != DMA_NONE) {\r\nnents = dma_map_sg(attach->dev, sgt->sgl, sgt->orig_nents, dir);\r\nif (!nents) {\r\nDRM_ERROR("failed to map sgl with iommu.\n");\r\nsg_free_table(sgt);\r\nsgt = ERR_PTR(-EIO);\r\ngoto err_unlock;\r\n}\r\n}\r\nudl_attach->is_mapped = true;\r\nudl_attach->dir = dir;\r\nattach->priv = udl_attach;\r\nerr_unlock:\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn sgt;\r\n}\r\nstatic void udl_unmap_dma_buf(struct dma_buf_attachment *attach,\r\nstruct sg_table *sgt,\r\nenum dma_data_direction dir)\r\n{\r\nDRM_DEBUG_PRIME("[DEV:%s] size:%zd dir:%d\n", dev_name(attach->dev),\r\nattach->dmabuf->size, dir);\r\n}\r\nstatic void *udl_dmabuf_kmap(struct dma_buf *dma_buf, unsigned long page_num)\r\n{\r\nreturn NULL;\r\n}\r\nstatic void *udl_dmabuf_kmap_atomic(struct dma_buf *dma_buf,\r\nunsigned long page_num)\r\n{\r\nreturn NULL;\r\n}\r\nstatic void udl_dmabuf_kunmap(struct dma_buf *dma_buf,\r\nunsigned long page_num, void *addr)\r\n{\r\n}\r\nstatic void udl_dmabuf_kunmap_atomic(struct dma_buf *dma_buf,\r\nunsigned long page_num,\r\nvoid *addr)\r\n{\r\n}\r\nstatic int udl_dmabuf_mmap(struct dma_buf *dma_buf,\r\nstruct vm_area_struct *vma)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstruct dma_buf *udl_gem_prime_export(struct drm_device *dev,\r\nstruct drm_gem_object *obj, int flags)\r\n{\r\nreturn dma_buf_export(obj, &udl_dmabuf_ops, obj->size, flags, NULL);\r\n}\r\nstatic int udl_prime_create(struct drm_device *dev,\r\nsize_t size,\r\nstruct sg_table *sg,\r\nstruct udl_gem_object **obj_p)\r\n{\r\nstruct udl_gem_object *obj;\r\nint npages;\r\nnpages = size / PAGE_SIZE;\r\n*obj_p = NULL;\r\nobj = udl_gem_alloc_object(dev, npages * PAGE_SIZE);\r\nif (!obj)\r\nreturn -ENOMEM;\r\nobj->sg = sg;\r\nobj->pages = drm_malloc_ab(npages, sizeof(struct page *));\r\nif (obj->pages == NULL) {\r\nDRM_ERROR("obj pages is NULL %d\n", npages);\r\nreturn -ENOMEM;\r\n}\r\ndrm_prime_sg_to_page_addr_arrays(sg, obj->pages, NULL, npages);\r\n*obj_p = obj;\r\nreturn 0;\r\n}\r\nstruct drm_gem_object *udl_gem_prime_import(struct drm_device *dev,\r\nstruct dma_buf *dma_buf)\r\n{\r\nstruct dma_buf_attachment *attach;\r\nstruct sg_table *sg;\r\nstruct udl_gem_object *uobj;\r\nint ret;\r\nget_device(dev->dev);\r\nattach = dma_buf_attach(dma_buf, dev->dev);\r\nif (IS_ERR(attach)) {\r\nput_device(dev->dev);\r\nreturn ERR_CAST(attach);\r\n}\r\nget_dma_buf(dma_buf);\r\nsg = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);\r\nif (IS_ERR(sg)) {\r\nret = PTR_ERR(sg);\r\ngoto fail_detach;\r\n}\r\nret = udl_prime_create(dev, dma_buf->size, sg, &uobj);\r\nif (ret)\r\ngoto fail_unmap;\r\nuobj->base.import_attach = attach;\r\nuobj->flags = UDL_BO_WC;\r\nreturn &uobj->base;\r\nfail_unmap:\r\ndma_buf_unmap_attachment(attach, sg, DMA_BIDIRECTIONAL);\r\nfail_detach:\r\ndma_buf_detach(dma_buf, attach);\r\ndma_buf_put(dma_buf);\r\nput_device(dev->dev);\r\nreturn ERR_PTR(ret);\r\n}
