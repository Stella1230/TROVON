static int efx_check_disabled(struct efx_nic *efx)\r\n{\r\nif (efx->state == STATE_DISABLED || efx->state == STATE_RECOVERY) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"device is disabled due to earlier errors\n");\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int efx_process_channel(struct efx_channel *channel, int budget)\r\n{\r\nint spent;\r\nif (unlikely(!channel->enabled))\r\nreturn 0;\r\nspent = efx_nic_process_eventq(channel, budget);\r\nif (spent && efx_channel_has_rx_queue(channel)) {\r\nstruct efx_rx_queue *rx_queue =\r\nefx_channel_get_rx_queue(channel);\r\nefx_rx_flush_packet(channel);\r\nefx_fast_push_rx_descriptors(rx_queue, true);\r\n}\r\nreturn spent;\r\n}\r\nstatic int efx_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct efx_channel *channel =\r\ncontainer_of(napi, struct efx_channel, napi_str);\r\nstruct efx_nic *efx = channel->efx;\r\nint spent;\r\nif (!efx_channel_lock_napi(channel))\r\nreturn budget;\r\nnetif_vdbg(efx, intr, efx->net_dev,\r\n"channel %d NAPI poll executing on CPU %d\n",\r\nchannel->channel, raw_smp_processor_id());\r\nspent = efx_process_channel(channel, budget);\r\nif (spent < budget) {\r\nif (efx_channel_has_rx_queue(channel) &&\r\nefx->irq_rx_adaptive &&\r\nunlikely(++channel->irq_count == 1000)) {\r\nif (unlikely(channel->irq_mod_score <\r\nirq_adapt_low_thresh)) {\r\nif (channel->irq_moderation > 1) {\r\nchannel->irq_moderation -= 1;\r\nefx->type->push_irq_moderation(channel);\r\n}\r\n} else if (unlikely(channel->irq_mod_score >\r\nirq_adapt_high_thresh)) {\r\nif (channel->irq_moderation <\r\nefx->irq_rx_moderation) {\r\nchannel->irq_moderation += 1;\r\nefx->type->push_irq_moderation(channel);\r\n}\r\n}\r\nchannel->irq_count = 0;\r\nchannel->irq_mod_score = 0;\r\n}\r\nefx_filter_rfs_expire(channel);\r\nnapi_complete(napi);\r\nefx_nic_eventq_read_ack(channel);\r\n}\r\nefx_channel_unlock_napi(channel);\r\nreturn spent;\r\n}\r\nstatic int efx_probe_eventq(struct efx_channel *channel)\r\n{\r\nstruct efx_nic *efx = channel->efx;\r\nunsigned long entries;\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"chan %d create event queue\n", channel->channel);\r\nentries = roundup_pow_of_two(efx->rxq_entries + efx->txq_entries + 128);\r\nEFX_BUG_ON_PARANOID(entries > EFX_MAX_EVQ_SIZE);\r\nchannel->eventq_mask = max(entries, EFX_MIN_EVQ_SIZE) - 1;\r\nreturn efx_nic_probe_eventq(channel);\r\n}\r\nstatic int efx_init_eventq(struct efx_channel *channel)\r\n{\r\nstruct efx_nic *efx = channel->efx;\r\nint rc;\r\nEFX_WARN_ON_PARANOID(channel->eventq_init);\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"chan %d init event queue\n", channel->channel);\r\nrc = efx_nic_init_eventq(channel);\r\nif (rc == 0) {\r\nefx->type->push_irq_moderation(channel);\r\nchannel->eventq_read_ptr = 0;\r\nchannel->eventq_init = true;\r\n}\r\nreturn rc;\r\n}\r\nvoid efx_start_eventq(struct efx_channel *channel)\r\n{\r\nnetif_dbg(channel->efx, ifup, channel->efx->net_dev,\r\n"chan %d start event queue\n", channel->channel);\r\nchannel->enabled = true;\r\nsmp_wmb();\r\nefx_channel_enable(channel);\r\nnapi_enable(&channel->napi_str);\r\nefx_nic_eventq_read_ack(channel);\r\n}\r\nvoid efx_stop_eventq(struct efx_channel *channel)\r\n{\r\nif (!channel->enabled)\r\nreturn;\r\nnapi_disable(&channel->napi_str);\r\nwhile (!efx_channel_disable(channel))\r\nusleep_range(1000, 20000);\r\nchannel->enabled = false;\r\n}\r\nstatic void efx_fini_eventq(struct efx_channel *channel)\r\n{\r\nif (!channel->eventq_init)\r\nreturn;\r\nnetif_dbg(channel->efx, drv, channel->efx->net_dev,\r\n"chan %d fini event queue\n", channel->channel);\r\nefx_nic_fini_eventq(channel);\r\nchannel->eventq_init = false;\r\n}\r\nstatic void efx_remove_eventq(struct efx_channel *channel)\r\n{\r\nnetif_dbg(channel->efx, drv, channel->efx->net_dev,\r\n"chan %d remove event queue\n", channel->channel);\r\nefx_nic_remove_eventq(channel);\r\n}\r\nstatic struct efx_channel *\r\nefx_alloc_channel(struct efx_nic *efx, int i, struct efx_channel *old_channel)\r\n{\r\nstruct efx_channel *channel;\r\nstruct efx_rx_queue *rx_queue;\r\nstruct efx_tx_queue *tx_queue;\r\nint j;\r\nchannel = kzalloc(sizeof(*channel), GFP_KERNEL);\r\nif (!channel)\r\nreturn NULL;\r\nchannel->efx = efx;\r\nchannel->channel = i;\r\nchannel->type = &efx_default_channel_type;\r\nfor (j = 0; j < EFX_TXQ_TYPES; j++) {\r\ntx_queue = &channel->tx_queue[j];\r\ntx_queue->efx = efx;\r\ntx_queue->queue = i * EFX_TXQ_TYPES + j;\r\ntx_queue->channel = channel;\r\n}\r\nrx_queue = &channel->rx_queue;\r\nrx_queue->efx = efx;\r\nsetup_timer(&rx_queue->slow_fill, efx_rx_slow_fill,\r\n(unsigned long)rx_queue);\r\nreturn channel;\r\n}\r\nstatic struct efx_channel *\r\nefx_copy_channel(const struct efx_channel *old_channel)\r\n{\r\nstruct efx_channel *channel;\r\nstruct efx_rx_queue *rx_queue;\r\nstruct efx_tx_queue *tx_queue;\r\nint j;\r\nchannel = kmalloc(sizeof(*channel), GFP_KERNEL);\r\nif (!channel)\r\nreturn NULL;\r\n*channel = *old_channel;\r\nchannel->napi_dev = NULL;\r\nmemset(&channel->eventq, 0, sizeof(channel->eventq));\r\nfor (j = 0; j < EFX_TXQ_TYPES; j++) {\r\ntx_queue = &channel->tx_queue[j];\r\nif (tx_queue->channel)\r\ntx_queue->channel = channel;\r\ntx_queue->buffer = NULL;\r\nmemset(&tx_queue->txd, 0, sizeof(tx_queue->txd));\r\n}\r\nrx_queue = &channel->rx_queue;\r\nrx_queue->buffer = NULL;\r\nmemset(&rx_queue->rxd, 0, sizeof(rx_queue->rxd));\r\nsetup_timer(&rx_queue->slow_fill, efx_rx_slow_fill,\r\n(unsigned long)rx_queue);\r\nreturn channel;\r\n}\r\nstatic int efx_probe_channel(struct efx_channel *channel)\r\n{\r\nstruct efx_tx_queue *tx_queue;\r\nstruct efx_rx_queue *rx_queue;\r\nint rc;\r\nnetif_dbg(channel->efx, probe, channel->efx->net_dev,\r\n"creating channel %d\n", channel->channel);\r\nrc = channel->type->pre_probe(channel);\r\nif (rc)\r\ngoto fail;\r\nrc = efx_probe_eventq(channel);\r\nif (rc)\r\ngoto fail;\r\nefx_for_each_channel_tx_queue(tx_queue, channel) {\r\nrc = efx_probe_tx_queue(tx_queue);\r\nif (rc)\r\ngoto fail;\r\n}\r\nefx_for_each_channel_rx_queue(rx_queue, channel) {\r\nrc = efx_probe_rx_queue(rx_queue);\r\nif (rc)\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nefx_remove_channel(channel);\r\nreturn rc;\r\n}\r\nstatic void\r\nefx_get_channel_name(struct efx_channel *channel, char *buf, size_t len)\r\n{\r\nstruct efx_nic *efx = channel->efx;\r\nconst char *type;\r\nint number;\r\nnumber = channel->channel;\r\nif (efx->tx_channel_offset == 0) {\r\ntype = "";\r\n} else if (channel->channel < efx->tx_channel_offset) {\r\ntype = "-rx";\r\n} else {\r\ntype = "-tx";\r\nnumber -= efx->tx_channel_offset;\r\n}\r\nsnprintf(buf, len, "%s%s-%d", efx->name, type, number);\r\n}\r\nstatic void efx_set_channel_names(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nefx_for_each_channel(channel, efx)\r\nchannel->type->get_name(channel,\r\nefx->msi_context[channel->channel].name,\r\nsizeof(efx->msi_context[0].name));\r\n}\r\nstatic int efx_probe_channels(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nint rc;\r\nefx->next_buffer_table = 0;\r\nefx_for_each_channel_rev(channel, efx) {\r\nrc = efx_probe_channel(channel);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to create channel %d\n",\r\nchannel->channel);\r\ngoto fail;\r\n}\r\n}\r\nefx_set_channel_names(efx);\r\nreturn 0;\r\nfail:\r\nefx_remove_channels(efx);\r\nreturn rc;\r\n}\r\nstatic void efx_start_datapath(struct efx_nic *efx)\r\n{\r\nbool old_rx_scatter = efx->rx_scatter;\r\nstruct efx_tx_queue *tx_queue;\r\nstruct efx_rx_queue *rx_queue;\r\nstruct efx_channel *channel;\r\nsize_t rx_buf_len;\r\nefx->rx_dma_len = (efx->rx_prefix_size +\r\nEFX_MAX_FRAME_LEN(efx->net_dev->mtu) +\r\nefx->type->rx_buffer_padding);\r\nrx_buf_len = (sizeof(struct efx_rx_page_state) +\r\nefx->rx_ip_align + efx->rx_dma_len);\r\nif (rx_buf_len <= PAGE_SIZE) {\r\nefx->rx_scatter = efx->type->always_rx_scatter;\r\nefx->rx_buffer_order = 0;\r\n} else if (efx->type->can_rx_scatter) {\r\nBUILD_BUG_ON(EFX_RX_USR_BUF_SIZE % L1_CACHE_BYTES);\r\nBUILD_BUG_ON(sizeof(struct efx_rx_page_state) +\r\n2 * ALIGN(NET_IP_ALIGN + EFX_RX_USR_BUF_SIZE,\r\nEFX_RX_BUF_ALIGNMENT) >\r\nPAGE_SIZE);\r\nefx->rx_scatter = true;\r\nefx->rx_dma_len = EFX_RX_USR_BUF_SIZE;\r\nefx->rx_buffer_order = 0;\r\n} else {\r\nefx->rx_scatter = false;\r\nefx->rx_buffer_order = get_order(rx_buf_len);\r\n}\r\nefx_rx_config_page_split(efx);\r\nif (efx->rx_buffer_order)\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"RX buf len=%u; page order=%u batch=%u\n",\r\nefx->rx_dma_len, efx->rx_buffer_order,\r\nefx->rx_pages_per_batch);\r\nelse\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"RX buf len=%u step=%u bpp=%u; page batch=%u\n",\r\nefx->rx_dma_len, efx->rx_page_buf_step,\r\nefx->rx_bufs_per_page, efx->rx_pages_per_batch);\r\nif (efx->rx_scatter != old_rx_scatter)\r\nefx->type->filter_update_rx_scatter(efx);\r\nefx->txq_stop_thresh = efx->txq_entries - efx_tx_max_skb_descs(efx);\r\nefx->txq_wake_thresh = efx->txq_stop_thresh / 2;\r\nefx_for_each_channel(channel, efx) {\r\nefx_for_each_channel_tx_queue(tx_queue, channel) {\r\nefx_init_tx_queue(tx_queue);\r\natomic_inc(&efx->active_queues);\r\n}\r\nefx_for_each_channel_rx_queue(rx_queue, channel) {\r\nefx_init_rx_queue(rx_queue);\r\natomic_inc(&efx->active_queues);\r\nefx_stop_eventq(channel);\r\nefx_fast_push_rx_descriptors(rx_queue, false);\r\nefx_start_eventq(channel);\r\n}\r\nWARN_ON(channel->rx_pkt_n_frags);\r\n}\r\nefx_ptp_start_datapath(efx);\r\nif (netif_device_present(efx->net_dev))\r\nnetif_tx_wake_all_queues(efx->net_dev);\r\n}\r\nstatic void efx_stop_datapath(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nstruct efx_tx_queue *tx_queue;\r\nstruct efx_rx_queue *rx_queue;\r\nint rc;\r\nEFX_ASSERT_RESET_SERIALISED(efx);\r\nBUG_ON(efx->port_enabled);\r\nefx_ptp_stop_datapath(efx);\r\nefx_for_each_channel(channel, efx) {\r\nefx_for_each_channel_rx_queue(rx_queue, channel)\r\nrx_queue->refill_enabled = false;\r\n}\r\nefx_for_each_channel(channel, efx) {\r\nif (efx_channel_has_rx_queue(channel)) {\r\nefx_stop_eventq(channel);\r\nefx_start_eventq(channel);\r\n}\r\n}\r\nrc = efx->type->fini_dmaq(efx);\r\nif (rc && EFX_WORKAROUND_7803(efx)) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"Resetting to recover from flush failure\n");\r\nefx_schedule_reset(efx, RESET_TYPE_ALL);\r\n} else if (rc) {\r\nnetif_err(efx, drv, efx->net_dev, "failed to flush queues\n");\r\n} else {\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"successfully flushed all queues\n");\r\n}\r\nefx_for_each_channel(channel, efx) {\r\nefx_for_each_channel_rx_queue(rx_queue, channel)\r\nefx_fini_rx_queue(rx_queue);\r\nefx_for_each_possible_channel_tx_queue(tx_queue, channel)\r\nefx_fini_tx_queue(tx_queue);\r\n}\r\n}\r\nstatic void efx_remove_channel(struct efx_channel *channel)\r\n{\r\nstruct efx_tx_queue *tx_queue;\r\nstruct efx_rx_queue *rx_queue;\r\nnetif_dbg(channel->efx, drv, channel->efx->net_dev,\r\n"destroy chan %d\n", channel->channel);\r\nefx_for_each_channel_rx_queue(rx_queue, channel)\r\nefx_remove_rx_queue(rx_queue);\r\nefx_for_each_possible_channel_tx_queue(tx_queue, channel)\r\nefx_remove_tx_queue(tx_queue);\r\nefx_remove_eventq(channel);\r\nchannel->type->post_remove(channel);\r\n}\r\nstatic void efx_remove_channels(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nefx_for_each_channel(channel, efx)\r\nefx_remove_channel(channel);\r\n}\r\nint\r\nefx_realloc_channels(struct efx_nic *efx, u32 rxq_entries, u32 txq_entries)\r\n{\r\nstruct efx_channel *other_channel[EFX_MAX_CHANNELS], *channel;\r\nu32 old_rxq_entries, old_txq_entries;\r\nunsigned i, next_buffer_table = 0;\r\nint rc, rc2;\r\nrc = efx_check_disabled(efx);\r\nif (rc)\r\nreturn rc;\r\nefx_for_each_channel(channel, efx) {\r\nstruct efx_rx_queue *rx_queue;\r\nstruct efx_tx_queue *tx_queue;\r\nif (channel->type->copy)\r\ncontinue;\r\nnext_buffer_table = max(next_buffer_table,\r\nchannel->eventq.index +\r\nchannel->eventq.entries);\r\nefx_for_each_channel_rx_queue(rx_queue, channel)\r\nnext_buffer_table = max(next_buffer_table,\r\nrx_queue->rxd.index +\r\nrx_queue->rxd.entries);\r\nefx_for_each_channel_tx_queue(tx_queue, channel)\r\nnext_buffer_table = max(next_buffer_table,\r\ntx_queue->txd.index +\r\ntx_queue->txd.entries);\r\n}\r\nefx_device_detach_sync(efx);\r\nefx_stop_all(efx);\r\nefx_soft_disable_interrupts(efx);\r\nmemset(other_channel, 0, sizeof(other_channel));\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = efx->channel[i];\r\nif (channel->type->copy)\r\nchannel = channel->type->copy(channel);\r\nif (!channel) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nother_channel[i] = channel;\r\n}\r\nold_rxq_entries = efx->rxq_entries;\r\nold_txq_entries = efx->txq_entries;\r\nefx->rxq_entries = rxq_entries;\r\nefx->txq_entries = txq_entries;\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = efx->channel[i];\r\nefx->channel[i] = other_channel[i];\r\nother_channel[i] = channel;\r\n}\r\nefx->next_buffer_table = next_buffer_table;\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = efx->channel[i];\r\nif (!channel->type->copy)\r\ncontinue;\r\nrc = efx_probe_channel(channel);\r\nif (rc)\r\ngoto rollback;\r\nefx_init_napi_channel(efx->channel[i]);\r\n}\r\nout:\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = other_channel[i];\r\nif (channel && channel->type->copy) {\r\nefx_fini_napi_channel(channel);\r\nefx_remove_channel(channel);\r\nkfree(channel);\r\n}\r\n}\r\nrc2 = efx_soft_enable_interrupts(efx);\r\nif (rc2) {\r\nrc = rc ? rc : rc2;\r\nnetif_err(efx, drv, efx->net_dev,\r\n"unable to restart interrupts on channel reallocation\n");\r\nefx_schedule_reset(efx, RESET_TYPE_DISABLE);\r\n} else {\r\nefx_start_all(efx);\r\nnetif_device_attach(efx->net_dev);\r\n}\r\nreturn rc;\r\nrollback:\r\nefx->rxq_entries = old_rxq_entries;\r\nefx->txq_entries = old_txq_entries;\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = efx->channel[i];\r\nefx->channel[i] = other_channel[i];\r\nother_channel[i] = channel;\r\n}\r\ngoto out;\r\n}\r\nvoid efx_schedule_slow_fill(struct efx_rx_queue *rx_queue)\r\n{\r\nmod_timer(&rx_queue->slow_fill, jiffies + msecs_to_jiffies(100));\r\n}\r\nint efx_channel_dummy_op_int(struct efx_channel *channel)\r\n{\r\nreturn 0;\r\n}\r\nvoid efx_channel_dummy_op_void(struct efx_channel *channel)\r\n{\r\n}\r\nvoid efx_link_status_changed(struct efx_nic *efx)\r\n{\r\nstruct efx_link_state *link_state = &efx->link_state;\r\nif (!netif_running(efx->net_dev))\r\nreturn;\r\nif (link_state->up != netif_carrier_ok(efx->net_dev)) {\r\nefx->n_link_state_changes++;\r\nif (link_state->up)\r\nnetif_carrier_on(efx->net_dev);\r\nelse\r\nnetif_carrier_off(efx->net_dev);\r\n}\r\nif (link_state->up)\r\nnetif_info(efx, link, efx->net_dev,\r\n"link up at %uMbps %s-duplex (MTU %d)\n",\r\nlink_state->speed, link_state->fd ? "full" : "half",\r\nefx->net_dev->mtu);\r\nelse\r\nnetif_info(efx, link, efx->net_dev, "link down\n");\r\n}\r\nvoid efx_link_set_advertising(struct efx_nic *efx, u32 advertising)\r\n{\r\nefx->link_advertising = advertising;\r\nif (advertising) {\r\nif (advertising & ADVERTISED_Pause)\r\nefx->wanted_fc |= (EFX_FC_TX | EFX_FC_RX);\r\nelse\r\nefx->wanted_fc &= ~(EFX_FC_TX | EFX_FC_RX);\r\nif (advertising & ADVERTISED_Asym_Pause)\r\nefx->wanted_fc ^= EFX_FC_TX;\r\n}\r\n}\r\nvoid efx_link_set_wanted_fc(struct efx_nic *efx, u8 wanted_fc)\r\n{\r\nefx->wanted_fc = wanted_fc;\r\nif (efx->link_advertising) {\r\nif (wanted_fc & EFX_FC_RX)\r\nefx->link_advertising |= (ADVERTISED_Pause |\r\nADVERTISED_Asym_Pause);\r\nelse\r\nefx->link_advertising &= ~(ADVERTISED_Pause |\r\nADVERTISED_Asym_Pause);\r\nif (wanted_fc & EFX_FC_TX)\r\nefx->link_advertising ^= ADVERTISED_Asym_Pause;\r\n}\r\n}\r\nint __efx_reconfigure_port(struct efx_nic *efx)\r\n{\r\nenum efx_phy_mode phy_mode;\r\nint rc;\r\nWARN_ON(!mutex_is_locked(&efx->mac_lock));\r\nphy_mode = efx->phy_mode;\r\nif (LOOPBACK_INTERNAL(efx))\r\nefx->phy_mode |= PHY_MODE_TX_DISABLED;\r\nelse\r\nefx->phy_mode &= ~PHY_MODE_TX_DISABLED;\r\nrc = efx->type->reconfigure_port(efx);\r\nif (rc)\r\nefx->phy_mode = phy_mode;\r\nreturn rc;\r\n}\r\nint efx_reconfigure_port(struct efx_nic *efx)\r\n{\r\nint rc;\r\nEFX_ASSERT_RESET_SERIALISED(efx);\r\nmutex_lock(&efx->mac_lock);\r\nrc = __efx_reconfigure_port(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nreturn rc;\r\n}\r\nstatic void efx_mac_work(struct work_struct *data)\r\n{\r\nstruct efx_nic *efx = container_of(data, struct efx_nic, mac_work);\r\nmutex_lock(&efx->mac_lock);\r\nif (efx->port_enabled)\r\nefx->type->reconfigure_mac(efx);\r\nmutex_unlock(&efx->mac_lock);\r\n}\r\nstatic int efx_probe_port(struct efx_nic *efx)\r\n{\r\nint rc;\r\nnetif_dbg(efx, probe, efx->net_dev, "create port\n");\r\nif (phy_flash_cfg)\r\nefx->phy_mode = PHY_MODE_SPECIAL;\r\nrc = efx->type->probe_port(efx);\r\nif (rc)\r\nreturn rc;\r\nether_addr_copy(efx->net_dev->dev_addr, efx->net_dev->perm_addr);\r\nreturn 0;\r\n}\r\nstatic int efx_init_port(struct efx_nic *efx)\r\n{\r\nint rc;\r\nnetif_dbg(efx, drv, efx->net_dev, "init port\n");\r\nmutex_lock(&efx->mac_lock);\r\nrc = efx->phy_op->init(efx);\r\nif (rc)\r\ngoto fail1;\r\nefx->port_initialized = true;\r\nefx->type->reconfigure_mac(efx);\r\nrc = efx->phy_op->reconfigure(efx);\r\nif (rc)\r\ngoto fail2;\r\nmutex_unlock(&efx->mac_lock);\r\nreturn 0;\r\nfail2:\r\nefx->phy_op->fini(efx);\r\nfail1:\r\nmutex_unlock(&efx->mac_lock);\r\nreturn rc;\r\n}\r\nstatic void efx_start_port(struct efx_nic *efx)\r\n{\r\nnetif_dbg(efx, ifup, efx->net_dev, "start port\n");\r\nBUG_ON(efx->port_enabled);\r\nmutex_lock(&efx->mac_lock);\r\nefx->port_enabled = true;\r\nefx->type->reconfigure_mac(efx);\r\nmutex_unlock(&efx->mac_lock);\r\n}\r\nstatic void efx_stop_port(struct efx_nic *efx)\r\n{\r\nnetif_dbg(efx, ifdown, efx->net_dev, "stop port\n");\r\nEFX_ASSERT_RESET_SERIALISED(efx);\r\nmutex_lock(&efx->mac_lock);\r\nefx->port_enabled = false;\r\nmutex_unlock(&efx->mac_lock);\r\nnetif_addr_lock_bh(efx->net_dev);\r\nnetif_addr_unlock_bh(efx->net_dev);\r\ncancel_delayed_work_sync(&efx->monitor_work);\r\nefx_selftest_async_cancel(efx);\r\ncancel_work_sync(&efx->mac_work);\r\n}\r\nstatic void efx_fini_port(struct efx_nic *efx)\r\n{\r\nnetif_dbg(efx, drv, efx->net_dev, "shut down port\n");\r\nif (!efx->port_initialized)\r\nreturn;\r\nefx->phy_op->fini(efx);\r\nefx->port_initialized = false;\r\nefx->link_state.up = false;\r\nefx_link_status_changed(efx);\r\n}\r\nstatic void efx_remove_port(struct efx_nic *efx)\r\n{\r\nnetif_dbg(efx, drv, efx->net_dev, "destroying port\n");\r\nefx->type->remove_port(efx);\r\n}\r\nstatic bool efx_same_controller(struct efx_nic *left, struct efx_nic *right)\r\n{\r\nreturn left->type == right->type &&\r\nleft->vpd_sn && right->vpd_sn &&\r\n!strcmp(left->vpd_sn, right->vpd_sn);\r\n}\r\nstatic void efx_associate(struct efx_nic *efx)\r\n{\r\nstruct efx_nic *other, *next;\r\nif (efx->primary == efx) {\r\nnetif_dbg(efx, probe, efx->net_dev, "adding to primary list\n");\r\nlist_add_tail(&efx->node, &efx_primary_list);\r\nlist_for_each_entry_safe(other, next, &efx_unassociated_list,\r\nnode) {\r\nif (efx_same_controller(efx, other)) {\r\nlist_del(&other->node);\r\nnetif_dbg(other, probe, other->net_dev,\r\n"moving to secondary list of %s %s\n",\r\npci_name(efx->pci_dev),\r\nefx->net_dev->name);\r\nlist_add_tail(&other->node,\r\n&efx->secondary_list);\r\nother->primary = efx;\r\n}\r\n}\r\n} else {\r\nlist_for_each_entry(other, &efx_primary_list, node) {\r\nif (efx_same_controller(efx, other)) {\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"adding to secondary list of %s %s\n",\r\npci_name(other->pci_dev),\r\nother->net_dev->name);\r\nlist_add_tail(&efx->node,\r\n&other->secondary_list);\r\nefx->primary = other;\r\nreturn;\r\n}\r\n}\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"adding to unassociated list\n");\r\nlist_add_tail(&efx->node, &efx_unassociated_list);\r\n}\r\n}\r\nstatic void efx_dissociate(struct efx_nic *efx)\r\n{\r\nstruct efx_nic *other, *next;\r\nlist_del(&efx->node);\r\nefx->primary = NULL;\r\nlist_for_each_entry_safe(other, next, &efx->secondary_list, node) {\r\nlist_del(&other->node);\r\nnetif_dbg(other, probe, other->net_dev,\r\n"moving to unassociated list\n");\r\nlist_add_tail(&other->node, &efx_unassociated_list);\r\nother->primary = NULL;\r\n}\r\n}\r\nstatic int efx_init_io(struct efx_nic *efx)\r\n{\r\nstruct pci_dev *pci_dev = efx->pci_dev;\r\ndma_addr_t dma_mask = efx->type->max_dma_mask;\r\nunsigned int mem_map_size = efx->type->mem_map_size(efx);\r\nint rc;\r\nnetif_dbg(efx, probe, efx->net_dev, "initialising I/O\n");\r\nrc = pci_enable_device(pci_dev);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to enable PCI device\n");\r\ngoto fail1;\r\n}\r\npci_set_master(pci_dev);\r\nwhile (dma_mask > 0x7fffffffUL) {\r\nif (dma_supported(&pci_dev->dev, dma_mask)) {\r\nrc = dma_set_mask_and_coherent(&pci_dev->dev, dma_mask);\r\nif (rc == 0)\r\nbreak;\r\n}\r\ndma_mask >>= 1;\r\n}\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"could not find a suitable DMA mask\n");\r\ngoto fail2;\r\n}\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"using DMA mask %llx\n", (unsigned long long) dma_mask);\r\nefx->membase_phys = pci_resource_start(efx->pci_dev, EFX_MEM_BAR);\r\nrc = pci_request_region(pci_dev, EFX_MEM_BAR, "sfc");\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"request for memory BAR failed\n");\r\nrc = -EIO;\r\ngoto fail3;\r\n}\r\nefx->membase = ioremap_nocache(efx->membase_phys, mem_map_size);\r\nif (!efx->membase) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"could not map memory BAR at %llx+%x\n",\r\n(unsigned long long)efx->membase_phys, mem_map_size);\r\nrc = -ENOMEM;\r\ngoto fail4;\r\n}\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"memory BAR at %llx+%x (virtual %p)\n",\r\n(unsigned long long)efx->membase_phys, mem_map_size,\r\nefx->membase);\r\nreturn 0;\r\nfail4:\r\npci_release_region(efx->pci_dev, EFX_MEM_BAR);\r\nfail3:\r\nefx->membase_phys = 0;\r\nfail2:\r\npci_disable_device(efx->pci_dev);\r\nfail1:\r\nreturn rc;\r\n}\r\nstatic void efx_fini_io(struct efx_nic *efx)\r\n{\r\nnetif_dbg(efx, drv, efx->net_dev, "shutting down I/O\n");\r\nif (efx->membase) {\r\niounmap(efx->membase);\r\nefx->membase = NULL;\r\n}\r\nif (efx->membase_phys) {\r\npci_release_region(efx->pci_dev, EFX_MEM_BAR);\r\nefx->membase_phys = 0;\r\n}\r\npci_disable_device(efx->pci_dev);\r\n}\r\nstatic unsigned int efx_wanted_parallelism(struct efx_nic *efx)\r\n{\r\ncpumask_var_t thread_mask;\r\nunsigned int count;\r\nint cpu;\r\nif (rss_cpus) {\r\ncount = rss_cpus;\r\n} else {\r\nif (unlikely(!zalloc_cpumask_var(&thread_mask, GFP_KERNEL))) {\r\nnetif_warn(efx, probe, efx->net_dev,\r\n"RSS disabled due to allocation failure\n");\r\nreturn 1;\r\n}\r\ncount = 0;\r\nfor_each_online_cpu(cpu) {\r\nif (!cpumask_test_cpu(cpu, thread_mask)) {\r\n++count;\r\ncpumask_or(thread_mask, thread_mask,\r\ntopology_thread_cpumask(cpu));\r\n}\r\n}\r\nfree_cpumask_var(thread_mask);\r\n}\r\nif (efx->type->sriov_wanted(efx) && efx_vf_size(efx) > 1 &&\r\ncount > efx_vf_size(efx)) {\r\nnetif_warn(efx, probe, efx->net_dev,\r\n"Reducing number of RSS channels from %u to %u for "\r\n"VF support. Increase vf-msix-limit to use more "\r\n"channels on the PF.\n",\r\ncount, efx_vf_size(efx));\r\ncount = efx_vf_size(efx);\r\n}\r\nreturn count;\r\n}\r\nstatic int efx_probe_interrupts(struct efx_nic *efx)\r\n{\r\nunsigned int extra_channels = 0;\r\nunsigned int i, j;\r\nint rc;\r\nfor (i = 0; i < EFX_MAX_EXTRA_CHANNELS; i++)\r\nif (efx->extra_channel_type[i])\r\n++extra_channels;\r\nif (efx->interrupt_mode == EFX_INT_MODE_MSIX) {\r\nstruct msix_entry xentries[EFX_MAX_CHANNELS];\r\nunsigned int n_channels;\r\nn_channels = efx_wanted_parallelism(efx);\r\nif (separate_tx_channels)\r\nn_channels *= 2;\r\nn_channels += extra_channels;\r\nn_channels = min(n_channels, efx->max_channels);\r\nfor (i = 0; i < n_channels; i++)\r\nxentries[i].entry = i;\r\nrc = pci_enable_msix_range(efx->pci_dev,\r\nxentries, 1, n_channels);\r\nif (rc < 0) {\r\nefx->interrupt_mode = EFX_INT_MODE_MSI;\r\nnetif_err(efx, drv, efx->net_dev,\r\n"could not enable MSI-X\n");\r\n} else if (rc < n_channels) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"WARNING: Insufficient MSI-X vectors"\r\n" available (%d < %u).\n", rc, n_channels);\r\nnetif_err(efx, drv, efx->net_dev,\r\n"WARNING: Performance may be reduced.\n");\r\nn_channels = rc;\r\n}\r\nif (rc > 0) {\r\nefx->n_channels = n_channels;\r\nif (n_channels > extra_channels)\r\nn_channels -= extra_channels;\r\nif (separate_tx_channels) {\r\nefx->n_tx_channels = max(n_channels / 2, 1U);\r\nefx->n_rx_channels = max(n_channels -\r\nefx->n_tx_channels,\r\n1U);\r\n} else {\r\nefx->n_tx_channels = n_channels;\r\nefx->n_rx_channels = n_channels;\r\n}\r\nfor (i = 0; i < efx->n_channels; i++)\r\nefx_get_channel(efx, i)->irq =\r\nxentries[i].vector;\r\n}\r\n}\r\nif (efx->interrupt_mode == EFX_INT_MODE_MSI) {\r\nefx->n_channels = 1;\r\nefx->n_rx_channels = 1;\r\nefx->n_tx_channels = 1;\r\nrc = pci_enable_msi(efx->pci_dev);\r\nif (rc == 0) {\r\nefx_get_channel(efx, 0)->irq = efx->pci_dev->irq;\r\n} else {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"could not enable MSI\n");\r\nefx->interrupt_mode = EFX_INT_MODE_LEGACY;\r\n}\r\n}\r\nif (efx->interrupt_mode == EFX_INT_MODE_LEGACY) {\r\nefx->n_channels = 1 + (separate_tx_channels ? 1 : 0);\r\nefx->n_rx_channels = 1;\r\nefx->n_tx_channels = 1;\r\nefx->legacy_irq = efx->pci_dev->irq;\r\n}\r\nj = efx->n_channels;\r\nfor (i = 0; i < EFX_MAX_EXTRA_CHANNELS; i++) {\r\nif (!efx->extra_channel_type[i])\r\ncontinue;\r\nif (efx->interrupt_mode != EFX_INT_MODE_MSIX ||\r\nefx->n_channels <= extra_channels) {\r\nefx->extra_channel_type[i]->handle_no_channel(efx);\r\n} else {\r\n--j;\r\nefx_get_channel(efx, j)->type =\r\nefx->extra_channel_type[i];\r\n}\r\n}\r\nefx->rss_spread = ((efx->n_rx_channels > 1 ||\r\n!efx->type->sriov_wanted(efx)) ?\r\nefx->n_rx_channels : efx_vf_size(efx));\r\nreturn 0;\r\n}\r\nstatic int efx_soft_enable_interrupts(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel, *end_channel;\r\nint rc;\r\nBUG_ON(efx->state == STATE_DISABLED);\r\nefx->irq_soft_enabled = true;\r\nsmp_wmb();\r\nefx_for_each_channel(channel, efx) {\r\nif (!channel->type->keep_eventq) {\r\nrc = efx_init_eventq(channel);\r\nif (rc)\r\ngoto fail;\r\n}\r\nefx_start_eventq(channel);\r\n}\r\nefx_mcdi_mode_event(efx);\r\nreturn 0;\r\nfail:\r\nend_channel = channel;\r\nefx_for_each_channel(channel, efx) {\r\nif (channel == end_channel)\r\nbreak;\r\nefx_stop_eventq(channel);\r\nif (!channel->type->keep_eventq)\r\nefx_fini_eventq(channel);\r\n}\r\nreturn rc;\r\n}\r\nstatic void efx_soft_disable_interrupts(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nif (efx->state == STATE_DISABLED)\r\nreturn;\r\nefx_mcdi_mode_poll(efx);\r\nefx->irq_soft_enabled = false;\r\nsmp_wmb();\r\nif (efx->legacy_irq)\r\nsynchronize_irq(efx->legacy_irq);\r\nefx_for_each_channel(channel, efx) {\r\nif (channel->irq)\r\nsynchronize_irq(channel->irq);\r\nefx_stop_eventq(channel);\r\nif (!channel->type->keep_eventq)\r\nefx_fini_eventq(channel);\r\n}\r\nefx_mcdi_flush_async(efx);\r\n}\r\nstatic int efx_enable_interrupts(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel, *end_channel;\r\nint rc;\r\nBUG_ON(efx->state == STATE_DISABLED);\r\nif (efx->eeh_disabled_legacy_irq) {\r\nenable_irq(efx->legacy_irq);\r\nefx->eeh_disabled_legacy_irq = false;\r\n}\r\nefx->type->irq_enable_master(efx);\r\nefx_for_each_channel(channel, efx) {\r\nif (channel->type->keep_eventq) {\r\nrc = efx_init_eventq(channel);\r\nif (rc)\r\ngoto fail;\r\n}\r\n}\r\nrc = efx_soft_enable_interrupts(efx);\r\nif (rc)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\nend_channel = channel;\r\nefx_for_each_channel(channel, efx) {\r\nif (channel == end_channel)\r\nbreak;\r\nif (channel->type->keep_eventq)\r\nefx_fini_eventq(channel);\r\n}\r\nefx->type->irq_disable_non_ev(efx);\r\nreturn rc;\r\n}\r\nstatic void efx_disable_interrupts(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nefx_soft_disable_interrupts(efx);\r\nefx_for_each_channel(channel, efx) {\r\nif (channel->type->keep_eventq)\r\nefx_fini_eventq(channel);\r\n}\r\nefx->type->irq_disable_non_ev(efx);\r\n}\r\nstatic void efx_remove_interrupts(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nefx_for_each_channel(channel, efx)\r\nchannel->irq = 0;\r\npci_disable_msi(efx->pci_dev);\r\npci_disable_msix(efx->pci_dev);\r\nefx->legacy_irq = 0;\r\n}\r\nstatic void efx_set_channels(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nstruct efx_tx_queue *tx_queue;\r\nefx->tx_channel_offset =\r\nseparate_tx_channels ? efx->n_channels - efx->n_tx_channels : 0;\r\nefx_for_each_channel(channel, efx) {\r\nif (channel->channel < efx->n_rx_channels)\r\nchannel->rx_queue.core_index = channel->channel;\r\nelse\r\nchannel->rx_queue.core_index = -1;\r\nefx_for_each_channel_tx_queue(tx_queue, channel)\r\ntx_queue->queue -= (efx->tx_channel_offset *\r\nEFX_TXQ_TYPES);\r\n}\r\n}\r\nstatic int efx_probe_nic(struct efx_nic *efx)\r\n{\r\nsize_t i;\r\nint rc;\r\nnetif_dbg(efx, probe, efx->net_dev, "creating NIC\n");\r\nrc = efx->type->probe(efx);\r\nif (rc)\r\nreturn rc;\r\nrc = efx_probe_interrupts(efx);\r\nif (rc)\r\ngoto fail1;\r\nefx_set_channels(efx);\r\nrc = efx->type->dimension_resources(efx);\r\nif (rc)\r\ngoto fail2;\r\nif (efx->n_channels > 1)\r\nnetdev_rss_key_fill(&efx->rx_hash_key, sizeof(efx->rx_hash_key));\r\nfor (i = 0; i < ARRAY_SIZE(efx->rx_indir_table); i++)\r\nefx->rx_indir_table[i] =\r\nethtool_rxfh_indir_default(i, efx->rss_spread);\r\nnetif_set_real_num_tx_queues(efx->net_dev, efx->n_tx_channels);\r\nnetif_set_real_num_rx_queues(efx->net_dev, efx->n_rx_channels);\r\nefx_init_irq_moderation(efx, tx_irq_mod_usec, rx_irq_mod_usec, true,\r\ntrue);\r\nreturn 0;\r\nfail2:\r\nefx_remove_interrupts(efx);\r\nfail1:\r\nefx->type->remove(efx);\r\nreturn rc;\r\n}\r\nstatic void efx_remove_nic(struct efx_nic *efx)\r\n{\r\nnetif_dbg(efx, drv, efx->net_dev, "destroying NIC\n");\r\nefx_remove_interrupts(efx);\r\nefx->type->remove(efx);\r\n}\r\nstatic int efx_probe_filters(struct efx_nic *efx)\r\n{\r\nint rc;\r\nspin_lock_init(&efx->filter_lock);\r\nrc = efx->type->filter_table_probe(efx);\r\nif (rc)\r\nreturn rc;\r\n#ifdef CONFIG_RFS_ACCEL\r\nif (efx->type->offload_features & NETIF_F_NTUPLE) {\r\nefx->rps_flow_id = kcalloc(efx->type->max_rx_ip_filters,\r\nsizeof(*efx->rps_flow_id),\r\nGFP_KERNEL);\r\nif (!efx->rps_flow_id) {\r\nefx->type->filter_table_remove(efx);\r\nreturn -ENOMEM;\r\n}\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void efx_remove_filters(struct efx_nic *efx)\r\n{\r\n#ifdef CONFIG_RFS_ACCEL\r\nkfree(efx->rps_flow_id);\r\n#endif\r\nefx->type->filter_table_remove(efx);\r\n}\r\nstatic void efx_restore_filters(struct efx_nic *efx)\r\n{\r\nefx->type->filter_table_restore(efx);\r\n}\r\nstatic int efx_probe_all(struct efx_nic *efx)\r\n{\r\nint rc;\r\nrc = efx_probe_nic(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev, "failed to create NIC\n");\r\ngoto fail1;\r\n}\r\nrc = efx_probe_port(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev, "failed to create port\n");\r\ngoto fail2;\r\n}\r\nBUILD_BUG_ON(EFX_DEFAULT_DMAQ_SIZE < EFX_RXQ_MIN_ENT);\r\nif (WARN_ON(EFX_DEFAULT_DMAQ_SIZE < EFX_TXQ_MIN_ENT(efx))) {\r\nrc = -EINVAL;\r\ngoto fail3;\r\n}\r\nefx->rxq_entries = efx->txq_entries = EFX_DEFAULT_DMAQ_SIZE;\r\nrc = efx_probe_filters(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to create filter tables\n");\r\ngoto fail3;\r\n}\r\nrc = efx_probe_channels(efx);\r\nif (rc)\r\ngoto fail4;\r\nreturn 0;\r\nfail4:\r\nefx_remove_filters(efx);\r\nfail3:\r\nefx_remove_port(efx);\r\nfail2:\r\nefx_remove_nic(efx);\r\nfail1:\r\nreturn rc;\r\n}\r\nstatic void efx_start_all(struct efx_nic *efx)\r\n{\r\nEFX_ASSERT_RESET_SERIALISED(efx);\r\nBUG_ON(efx->state == STATE_DISABLED);\r\nif (efx->port_enabled || !netif_running(efx->net_dev) ||\r\nefx->reset_pending)\r\nreturn;\r\nefx_start_port(efx);\r\nefx_start_datapath(efx);\r\nif (efx->type->monitor != NULL)\r\nqueue_delayed_work(efx->workqueue, &efx->monitor_work,\r\nefx_monitor_interval);\r\nif (efx_nic_rev(efx) >= EFX_REV_SIENA_A0) {\r\nmutex_lock(&efx->mac_lock);\r\nif (efx->phy_op->poll(efx))\r\nefx_link_status_changed(efx);\r\nmutex_unlock(&efx->mac_lock);\r\n}\r\nefx->type->start_stats(efx);\r\nefx->type->pull_stats(efx);\r\nspin_lock_bh(&efx->stats_lock);\r\nefx->type->update_stats(efx, NULL, NULL);\r\nspin_unlock_bh(&efx->stats_lock);\r\n}\r\nstatic void efx_stop_all(struct efx_nic *efx)\r\n{\r\nEFX_ASSERT_RESET_SERIALISED(efx);\r\nif (!efx->port_enabled)\r\nreturn;\r\nefx->type->pull_stats(efx);\r\nspin_lock_bh(&efx->stats_lock);\r\nefx->type->update_stats(efx, NULL, NULL);\r\nspin_unlock_bh(&efx->stats_lock);\r\nefx->type->stop_stats(efx);\r\nefx_stop_port(efx);\r\nWARN_ON(netif_running(efx->net_dev) &&\r\nnetif_device_present(efx->net_dev));\r\nnetif_tx_disable(efx->net_dev);\r\nefx_stop_datapath(efx);\r\n}\r\nstatic void efx_remove_all(struct efx_nic *efx)\r\n{\r\nefx_remove_channels(efx);\r\nefx_remove_filters(efx);\r\nefx_remove_port(efx);\r\nefx_remove_nic(efx);\r\n}\r\nstatic unsigned int irq_mod_ticks(unsigned int usecs, unsigned int quantum_ns)\r\n{\r\nif (usecs == 0)\r\nreturn 0;\r\nif (usecs * 1000 < quantum_ns)\r\nreturn 1;\r\nreturn usecs * 1000 / quantum_ns;\r\n}\r\nint efx_init_irq_moderation(struct efx_nic *efx, unsigned int tx_usecs,\r\nunsigned int rx_usecs, bool rx_adaptive,\r\nbool rx_may_override_tx)\r\n{\r\nstruct efx_channel *channel;\r\nunsigned int irq_mod_max = DIV_ROUND_UP(efx->type->timer_period_max *\r\nefx->timer_quantum_ns,\r\n1000);\r\nunsigned int tx_ticks;\r\nunsigned int rx_ticks;\r\nEFX_ASSERT_RESET_SERIALISED(efx);\r\nif (tx_usecs > irq_mod_max || rx_usecs > irq_mod_max)\r\nreturn -EINVAL;\r\ntx_ticks = irq_mod_ticks(tx_usecs, efx->timer_quantum_ns);\r\nrx_ticks = irq_mod_ticks(rx_usecs, efx->timer_quantum_ns);\r\nif (tx_ticks != rx_ticks && efx->tx_channel_offset == 0 &&\r\n!rx_may_override_tx) {\r\nnetif_err(efx, drv, efx->net_dev, "Channels are shared. "\r\n"RX and TX IRQ moderation must be equal\n");\r\nreturn -EINVAL;\r\n}\r\nefx->irq_rx_adaptive = rx_adaptive;\r\nefx->irq_rx_moderation = rx_ticks;\r\nefx_for_each_channel(channel, efx) {\r\nif (efx_channel_has_rx_queue(channel))\r\nchannel->irq_moderation = rx_ticks;\r\nelse if (efx_channel_has_tx_queues(channel))\r\nchannel->irq_moderation = tx_ticks;\r\n}\r\nreturn 0;\r\n}\r\nvoid efx_get_irq_moderation(struct efx_nic *efx, unsigned int *tx_usecs,\r\nunsigned int *rx_usecs, bool *rx_adaptive)\r\n{\r\n*rx_adaptive = efx->irq_rx_adaptive;\r\n*rx_usecs = DIV_ROUND_UP(efx->irq_rx_moderation *\r\nefx->timer_quantum_ns,\r\n1000);\r\nif (efx->tx_channel_offset == 0)\r\n*tx_usecs = *rx_usecs;\r\nelse\r\n*tx_usecs = DIV_ROUND_UP(\r\nefx->channel[efx->tx_channel_offset]->irq_moderation *\r\nefx->timer_quantum_ns,\r\n1000);\r\n}\r\nstatic void efx_monitor(struct work_struct *data)\r\n{\r\nstruct efx_nic *efx = container_of(data, struct efx_nic,\r\nmonitor_work.work);\r\nnetif_vdbg(efx, timer, efx->net_dev,\r\n"hardware monitor executing on CPU %d\n",\r\nraw_smp_processor_id());\r\nBUG_ON(efx->type->monitor == NULL);\r\nif (mutex_trylock(&efx->mac_lock)) {\r\nif (efx->port_enabled)\r\nefx->type->monitor(efx);\r\nmutex_unlock(&efx->mac_lock);\r\n}\r\nqueue_delayed_work(efx->workqueue, &efx->monitor_work,\r\nefx_monitor_interval);\r\n}\r\nstatic int efx_ioctl(struct net_device *net_dev, struct ifreq *ifr, int cmd)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nstruct mii_ioctl_data *data = if_mii(ifr);\r\nif (cmd == SIOCSHWTSTAMP)\r\nreturn efx_ptp_set_ts_config(efx, ifr);\r\nif (cmd == SIOCGHWTSTAMP)\r\nreturn efx_ptp_get_ts_config(efx, ifr);\r\nif ((cmd == SIOCGMIIREG || cmd == SIOCSMIIREG) &&\r\n(data->phy_id & 0xfc00) == 0x0400)\r\ndata->phy_id ^= MDIO_PHY_ID_C45 | 0x0400;\r\nreturn mdio_mii_ioctl(&efx->mdio, data, cmd);\r\n}\r\nstatic void efx_init_napi_channel(struct efx_channel *channel)\r\n{\r\nstruct efx_nic *efx = channel->efx;\r\nchannel->napi_dev = efx->net_dev;\r\nnetif_napi_add(channel->napi_dev, &channel->napi_str,\r\nefx_poll, napi_weight);\r\nnapi_hash_add(&channel->napi_str);\r\nefx_channel_init_lock(channel);\r\n}\r\nstatic void efx_init_napi(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nefx_for_each_channel(channel, efx)\r\nefx_init_napi_channel(channel);\r\n}\r\nstatic void efx_fini_napi_channel(struct efx_channel *channel)\r\n{\r\nif (channel->napi_dev) {\r\nnetif_napi_del(&channel->napi_str);\r\nnapi_hash_del(&channel->napi_str);\r\n}\r\nchannel->napi_dev = NULL;\r\n}\r\nstatic void efx_fini_napi(struct efx_nic *efx)\r\n{\r\nstruct efx_channel *channel;\r\nefx_for_each_channel(channel, efx)\r\nefx_fini_napi_channel(channel);\r\n}\r\nstatic void efx_netpoll(struct net_device *net_dev)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nstruct efx_channel *channel;\r\nefx_for_each_channel(channel, efx)\r\nefx_schedule_channel(channel);\r\n}\r\nstatic int efx_busy_poll(struct napi_struct *napi)\r\n{\r\nstruct efx_channel *channel =\r\ncontainer_of(napi, struct efx_channel, napi_str);\r\nstruct efx_nic *efx = channel->efx;\r\nint budget = 4;\r\nint old_rx_packets, rx_packets;\r\nif (!netif_running(efx->net_dev))\r\nreturn LL_FLUSH_FAILED;\r\nif (!efx_channel_lock_poll(channel))\r\nreturn LL_FLUSH_BUSY;\r\nold_rx_packets = channel->rx_queue.rx_packets;\r\nefx_process_channel(channel, budget);\r\nrx_packets = channel->rx_queue.rx_packets - old_rx_packets;\r\nefx_channel_unlock_poll(channel);\r\nreturn rx_packets;\r\n}\r\nstatic int efx_net_open(struct net_device *net_dev)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nint rc;\r\nnetif_dbg(efx, ifup, efx->net_dev, "opening device on CPU %d\n",\r\nraw_smp_processor_id());\r\nrc = efx_check_disabled(efx);\r\nif (rc)\r\nreturn rc;\r\nif (efx->phy_mode & PHY_MODE_SPECIAL)\r\nreturn -EBUSY;\r\nif (efx_mcdi_poll_reboot(efx) && efx_reset(efx, RESET_TYPE_ALL))\r\nreturn -EIO;\r\nefx_link_status_changed(efx);\r\nefx_start_all(efx);\r\nefx_selftest_async_start(efx);\r\nreturn 0;\r\n}\r\nstatic int efx_net_stop(struct net_device *net_dev)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nnetif_dbg(efx, ifdown, efx->net_dev, "closing on CPU %d\n",\r\nraw_smp_processor_id());\r\nefx_stop_all(efx);\r\nreturn 0;\r\n}\r\nstatic struct rtnl_link_stats64 *efx_net_stats(struct net_device *net_dev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nspin_lock_bh(&efx->stats_lock);\r\nefx->type->update_stats(efx, NULL, stats);\r\nspin_unlock_bh(&efx->stats_lock);\r\nreturn stats;\r\n}\r\nstatic void efx_watchdog(struct net_device *net_dev)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nnetif_err(efx, tx_err, efx->net_dev,\r\n"TX stuck with port_enabled=%d: resetting channels\n",\r\nefx->port_enabled);\r\nefx_schedule_reset(efx, RESET_TYPE_TX_WATCHDOG);\r\n}\r\nstatic int efx_change_mtu(struct net_device *net_dev, int new_mtu)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nint rc;\r\nrc = efx_check_disabled(efx);\r\nif (rc)\r\nreturn rc;\r\nif (new_mtu > EFX_MAX_MTU)\r\nreturn -EINVAL;\r\nnetif_dbg(efx, drv, efx->net_dev, "changing MTU to %d\n", new_mtu);\r\nefx_device_detach_sync(efx);\r\nefx_stop_all(efx);\r\nmutex_lock(&efx->mac_lock);\r\nnet_dev->mtu = new_mtu;\r\nefx->type->reconfigure_mac(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nefx_start_all(efx);\r\nnetif_device_attach(efx->net_dev);\r\nreturn 0;\r\n}\r\nstatic int efx_set_mac_address(struct net_device *net_dev, void *data)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nstruct sockaddr *addr = data;\r\nu8 *new_addr = addr->sa_data;\r\nif (!is_valid_ether_addr(new_addr)) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"invalid ethernet MAC address requested: %pM\n",\r\nnew_addr);\r\nreturn -EADDRNOTAVAIL;\r\n}\r\nether_addr_copy(net_dev->dev_addr, new_addr);\r\nefx->type->sriov_mac_address_changed(efx);\r\nmutex_lock(&efx->mac_lock);\r\nefx->type->reconfigure_mac(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nreturn 0;\r\n}\r\nstatic void efx_set_rx_mode(struct net_device *net_dev)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nif (efx->port_enabled)\r\nqueue_work(efx->workqueue, &efx->mac_work);\r\n}\r\nstatic int efx_set_features(struct net_device *net_dev, netdev_features_t data)\r\n{\r\nstruct efx_nic *efx = netdev_priv(net_dev);\r\nif (net_dev->features & ~data & NETIF_F_NTUPLE)\r\nreturn efx->type->filter_clear_rx(efx, EFX_FILTER_PRI_MANUAL);\r\nreturn 0;\r\n}\r\nstatic void efx_update_name(struct efx_nic *efx)\r\n{\r\nstrcpy(efx->name, efx->net_dev->name);\r\nefx_mtd_rename(efx);\r\nefx_set_channel_names(efx);\r\n}\r\nstatic int efx_netdev_event(struct notifier_block *this,\r\nunsigned long event, void *ptr)\r\n{\r\nstruct net_device *net_dev = netdev_notifier_info_to_dev(ptr);\r\nif ((net_dev->netdev_ops == &efx_farch_netdev_ops ||\r\nnet_dev->netdev_ops == &efx_ef10_netdev_ops) &&\r\nevent == NETDEV_CHANGENAME)\r\nefx_update_name(netdev_priv(net_dev));\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic ssize_t\r\nshow_phy_type(struct device *dev, struct device_attribute *attr, char *buf)\r\n{\r\nstruct efx_nic *efx = pci_get_drvdata(to_pci_dev(dev));\r\nreturn sprintf(buf, "%d\n", efx->phy_type);\r\n}\r\nstatic int efx_register_netdev(struct efx_nic *efx)\r\n{\r\nstruct net_device *net_dev = efx->net_dev;\r\nstruct efx_channel *channel;\r\nint rc;\r\nnet_dev->watchdog_timeo = 5 * HZ;\r\nnet_dev->irq = efx->pci_dev->irq;\r\nif (efx_nic_rev(efx) >= EFX_REV_HUNT_A0) {\r\nnet_dev->netdev_ops = &efx_ef10_netdev_ops;\r\nnet_dev->priv_flags |= IFF_UNICAST_FLT;\r\n} else {\r\nnet_dev->netdev_ops = &efx_farch_netdev_ops;\r\n}\r\nnet_dev->ethtool_ops = &efx_ethtool_ops;\r\nnet_dev->gso_max_segs = EFX_TSO_MAX_SEGS;\r\nrtnl_lock();\r\nefx->state = STATE_READY;\r\nsmp_mb();\r\nif (efx->reset_pending) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"aborting probe due to scheduled reset\n");\r\nrc = -EIO;\r\ngoto fail_locked;\r\n}\r\nrc = dev_alloc_name(net_dev, net_dev->name);\r\nif (rc < 0)\r\ngoto fail_locked;\r\nefx_update_name(efx);\r\nnetif_carrier_off(net_dev);\r\nrc = register_netdevice(net_dev);\r\nif (rc)\r\ngoto fail_locked;\r\nefx_for_each_channel(channel, efx) {\r\nstruct efx_tx_queue *tx_queue;\r\nefx_for_each_channel_tx_queue(tx_queue, channel)\r\nefx_init_tx_queue_core_txq(tx_queue);\r\n}\r\nefx_associate(efx);\r\nrtnl_unlock();\r\nrc = device_create_file(&efx->pci_dev->dev, &dev_attr_phy_type);\r\nif (rc) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"failed to init net dev attributes\n");\r\ngoto fail_registered;\r\n}\r\nreturn 0;\r\nfail_registered:\r\nrtnl_lock();\r\nefx_dissociate(efx);\r\nunregister_netdevice(net_dev);\r\nfail_locked:\r\nefx->state = STATE_UNINIT;\r\nrtnl_unlock();\r\nnetif_err(efx, drv, efx->net_dev, "could not register net dev\n");\r\nreturn rc;\r\n}\r\nstatic void efx_unregister_netdev(struct efx_nic *efx)\r\n{\r\nif (!efx->net_dev)\r\nreturn;\r\nBUG_ON(netdev_priv(efx->net_dev) != efx);\r\nstrlcpy(efx->name, pci_name(efx->pci_dev), sizeof(efx->name));\r\ndevice_remove_file(&efx->pci_dev->dev, &dev_attr_phy_type);\r\nrtnl_lock();\r\nunregister_netdevice(efx->net_dev);\r\nefx->state = STATE_UNINIT;\r\nrtnl_unlock();\r\n}\r\nvoid efx_reset_down(struct efx_nic *efx, enum reset_type method)\r\n{\r\nEFX_ASSERT_RESET_SERIALISED(efx);\r\nif (method == RESET_TYPE_MCDI_TIMEOUT)\r\nefx->type->prepare_flr(efx);\r\nefx_stop_all(efx);\r\nefx_disable_interrupts(efx);\r\nmutex_lock(&efx->mac_lock);\r\nif (efx->port_initialized && method != RESET_TYPE_INVISIBLE)\r\nefx->phy_op->fini(efx);\r\nefx->type->fini(efx);\r\n}\r\nint efx_reset_up(struct efx_nic *efx, enum reset_type method, bool ok)\r\n{\r\nint rc;\r\nEFX_ASSERT_RESET_SERIALISED(efx);\r\nif (method == RESET_TYPE_MCDI_TIMEOUT)\r\nefx->type->finish_flr(efx);\r\nrc = efx->type->init(efx);\r\nif (rc) {\r\nnetif_err(efx, drv, efx->net_dev, "failed to initialise NIC\n");\r\ngoto fail;\r\n}\r\nif (!ok)\r\ngoto fail;\r\nif (efx->port_initialized && method != RESET_TYPE_INVISIBLE) {\r\nrc = efx->phy_op->init(efx);\r\nif (rc)\r\ngoto fail;\r\nif (efx->phy_op->reconfigure(efx))\r\nnetif_err(efx, drv, efx->net_dev,\r\n"could not restore PHY settings\n");\r\n}\r\nrc = efx_enable_interrupts(efx);\r\nif (rc)\r\ngoto fail;\r\nefx_restore_filters(efx);\r\nefx->type->sriov_reset(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nefx_start_all(efx);\r\nreturn 0;\r\nfail:\r\nefx->port_initialized = false;\r\nmutex_unlock(&efx->mac_lock);\r\nreturn rc;\r\n}\r\nint efx_reset(struct efx_nic *efx, enum reset_type method)\r\n{\r\nint rc, rc2;\r\nbool disabled;\r\nnetif_info(efx, drv, efx->net_dev, "resetting (%s)\n",\r\nRESET_TYPE(method));\r\nefx_device_detach_sync(efx);\r\nefx_reset_down(efx, method);\r\nrc = efx->type->reset(efx, method);\r\nif (rc) {\r\nnetif_err(efx, drv, efx->net_dev, "failed to reset hardware\n");\r\ngoto out;\r\n}\r\nif (method < RESET_TYPE_MAX_METHOD)\r\nefx->reset_pending &= -(1 << (method + 1));\r\nelse\r\n__clear_bit(method, &efx->reset_pending);\r\npci_set_master(efx->pci_dev);\r\nout:\r\ndisabled = rc ||\r\nmethod == RESET_TYPE_DISABLE ||\r\nmethod == RESET_TYPE_RECOVER_OR_DISABLE;\r\nrc2 = efx_reset_up(efx, method, !disabled);\r\nif (rc2) {\r\ndisabled = true;\r\nif (!rc)\r\nrc = rc2;\r\n}\r\nif (disabled) {\r\ndev_close(efx->net_dev);\r\nnetif_err(efx, drv, efx->net_dev, "has been disabled\n");\r\nefx->state = STATE_DISABLED;\r\n} else {\r\nnetif_dbg(efx, drv, efx->net_dev, "reset complete\n");\r\nnetif_device_attach(efx->net_dev);\r\n}\r\nreturn rc;\r\n}\r\nint efx_try_recovery(struct efx_nic *efx)\r\n{\r\n#ifdef CONFIG_EEH\r\nstruct eeh_dev *eehdev =\r\nof_node_to_eeh_dev(pci_device_to_OF_node(efx->pci_dev));\r\nif (eeh_dev_check_failure(eehdev)) {\r\nreturn 1;\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void efx_wait_for_bist_end(struct efx_nic *efx)\r\n{\r\nint i;\r\nfor (i = 0; i < BIST_WAIT_DELAY_COUNT; ++i) {\r\nif (efx_mcdi_poll_reboot(efx))\r\ngoto out;\r\nmsleep(BIST_WAIT_DELAY_MS);\r\n}\r\nnetif_err(efx, drv, efx->net_dev, "Warning: No MC reboot after BIST mode\n");\r\nout:\r\nefx->mc_bist_for_other_fn = false;\r\n}\r\nstatic void efx_reset_work(struct work_struct *data)\r\n{\r\nstruct efx_nic *efx = container_of(data, struct efx_nic, reset_work);\r\nunsigned long pending;\r\nenum reset_type method;\r\npending = ACCESS_ONCE(efx->reset_pending);\r\nmethod = fls(pending) - 1;\r\nif (method == RESET_TYPE_MC_BIST)\r\nefx_wait_for_bist_end(efx);\r\nif ((method == RESET_TYPE_RECOVER_OR_DISABLE ||\r\nmethod == RESET_TYPE_RECOVER_OR_ALL) &&\r\nefx_try_recovery(efx))\r\nreturn;\r\nif (!pending)\r\nreturn;\r\nrtnl_lock();\r\nif (efx->state == STATE_READY)\r\n(void)efx_reset(efx, method);\r\nrtnl_unlock();\r\n}\r\nvoid efx_schedule_reset(struct efx_nic *efx, enum reset_type type)\r\n{\r\nenum reset_type method;\r\nif (efx->state == STATE_RECOVERY) {\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"recovering: skip scheduling %s reset\n",\r\nRESET_TYPE(type));\r\nreturn;\r\n}\r\nswitch (type) {\r\ncase RESET_TYPE_INVISIBLE:\r\ncase RESET_TYPE_ALL:\r\ncase RESET_TYPE_RECOVER_OR_ALL:\r\ncase RESET_TYPE_WORLD:\r\ncase RESET_TYPE_DISABLE:\r\ncase RESET_TYPE_RECOVER_OR_DISABLE:\r\ncase RESET_TYPE_MC_BIST:\r\ncase RESET_TYPE_MCDI_TIMEOUT:\r\nmethod = type;\r\nnetif_dbg(efx, drv, efx->net_dev, "scheduling %s reset\n",\r\nRESET_TYPE(method));\r\nbreak;\r\ndefault:\r\nmethod = efx->type->map_reset_reason(type);\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"scheduling %s reset for %s\n",\r\nRESET_TYPE(method), RESET_TYPE(type));\r\nbreak;\r\n}\r\nset_bit(method, &efx->reset_pending);\r\nsmp_mb();\r\nif (ACCESS_ONCE(efx->state) != STATE_READY)\r\nreturn;\r\nefx_mcdi_mode_poll(efx);\r\nqueue_work(reset_workqueue, &efx->reset_work);\r\n}\r\nint efx_port_dummy_op_int(struct efx_nic *efx)\r\n{\r\nreturn 0;\r\n}\r\nvoid efx_port_dummy_op_void(struct efx_nic *efx) {}\r\nstatic bool efx_port_dummy_op_poll(struct efx_nic *efx)\r\n{\r\nreturn false;\r\n}\r\nstatic int efx_init_struct(struct efx_nic *efx,\r\nstruct pci_dev *pci_dev, struct net_device *net_dev)\r\n{\r\nint i;\r\nINIT_LIST_HEAD(&efx->node);\r\nINIT_LIST_HEAD(&efx->secondary_list);\r\nspin_lock_init(&efx->biu_lock);\r\n#ifdef CONFIG_SFC_MTD\r\nINIT_LIST_HEAD(&efx->mtd_list);\r\n#endif\r\nINIT_WORK(&efx->reset_work, efx_reset_work);\r\nINIT_DELAYED_WORK(&efx->monitor_work, efx_monitor);\r\nINIT_DELAYED_WORK(&efx->selftest_work, efx_selftest_async_work);\r\nefx->pci_dev = pci_dev;\r\nefx->msg_enable = debug;\r\nefx->state = STATE_UNINIT;\r\nstrlcpy(efx->name, pci_name(pci_dev), sizeof(efx->name));\r\nefx->net_dev = net_dev;\r\nefx->rx_prefix_size = efx->type->rx_prefix_size;\r\nefx->rx_ip_align =\r\nNET_IP_ALIGN ? (efx->rx_prefix_size + NET_IP_ALIGN) % 4 : 0;\r\nefx->rx_packet_hash_offset =\r\nefx->type->rx_hash_offset - efx->type->rx_prefix_size;\r\nefx->rx_packet_ts_offset =\r\nefx->type->rx_ts_offset - efx->type->rx_prefix_size;\r\nspin_lock_init(&efx->stats_lock);\r\nmutex_init(&efx->mac_lock);\r\nefx->phy_op = &efx_dummy_phy_operations;\r\nefx->mdio.dev = net_dev;\r\nINIT_WORK(&efx->mac_work, efx_mac_work);\r\ninit_waitqueue_head(&efx->flush_wq);\r\nfor (i = 0; i < EFX_MAX_CHANNELS; i++) {\r\nefx->channel[i] = efx_alloc_channel(efx, i, NULL);\r\nif (!efx->channel[i])\r\ngoto fail;\r\nefx->msi_context[i].efx = efx;\r\nefx->msi_context[i].index = i;\r\n}\r\nefx->interrupt_mode = max(efx->type->max_interrupt_mode,\r\ninterrupt_mode);\r\nsnprintf(efx->workqueue_name, sizeof(efx->workqueue_name), "sfc%s",\r\npci_name(pci_dev));\r\nefx->workqueue = create_singlethread_workqueue(efx->workqueue_name);\r\nif (!efx->workqueue)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\nefx_fini_struct(efx);\r\nreturn -ENOMEM;\r\n}\r\nstatic void efx_fini_struct(struct efx_nic *efx)\r\n{\r\nint i;\r\nfor (i = 0; i < EFX_MAX_CHANNELS; i++)\r\nkfree(efx->channel[i]);\r\nkfree(efx->vpd_sn);\r\nif (efx->workqueue) {\r\ndestroy_workqueue(efx->workqueue);\r\nefx->workqueue = NULL;\r\n}\r\n}\r\nvoid efx_update_sw_stats(struct efx_nic *efx, u64 *stats)\r\n{\r\nu64 n_rx_nodesc_trunc = 0;\r\nstruct efx_channel *channel;\r\nefx_for_each_channel(channel, efx)\r\nn_rx_nodesc_trunc += channel->n_rx_nodesc_trunc;\r\nstats[GENERIC_STAT_rx_nodesc_trunc] = n_rx_nodesc_trunc;\r\nstats[GENERIC_STAT_rx_noskb_drops] = atomic_read(&efx->n_rx_noskb_drops);\r\n}\r\nstatic void efx_pci_remove_main(struct efx_nic *efx)\r\n{\r\nBUG_ON(efx->state == STATE_READY);\r\ncancel_work_sync(&efx->reset_work);\r\nefx_disable_interrupts(efx);\r\nefx_nic_fini_interrupt(efx);\r\nefx_fini_port(efx);\r\nefx->type->fini(efx);\r\nefx_fini_napi(efx);\r\nefx_remove_all(efx);\r\n}\r\nstatic void efx_pci_remove(struct pci_dev *pci_dev)\r\n{\r\nstruct efx_nic *efx;\r\nefx = pci_get_drvdata(pci_dev);\r\nif (!efx)\r\nreturn;\r\nrtnl_lock();\r\nefx_dissociate(efx);\r\ndev_close(efx->net_dev);\r\nefx_disable_interrupts(efx);\r\nrtnl_unlock();\r\nefx->type->sriov_fini(efx);\r\nefx_unregister_netdev(efx);\r\nefx_mtd_remove(efx);\r\nefx_pci_remove_main(efx);\r\nefx_fini_io(efx);\r\nnetif_dbg(efx, drv, efx->net_dev, "shutdown successful\n");\r\nefx_fini_struct(efx);\r\nfree_netdev(efx->net_dev);\r\npci_disable_pcie_error_reporting(pci_dev);\r\n}\r\nstatic void efx_probe_vpd_strings(struct efx_nic *efx)\r\n{\r\nstruct pci_dev *dev = efx->pci_dev;\r\nchar vpd_data[SFC_VPD_LEN];\r\nssize_t vpd_size;\r\nint ro_start, ro_size, i, j;\r\nvpd_size = pci_read_vpd(dev, 0, sizeof(vpd_data), vpd_data);\r\nif (vpd_size <= 0) {\r\nnetif_err(efx, drv, efx->net_dev, "Unable to read VPD\n");\r\nreturn;\r\n}\r\nro_start = pci_vpd_find_tag(vpd_data, 0, vpd_size, PCI_VPD_LRDT_RO_DATA);\r\nif (ro_start < 0) {\r\nnetif_err(efx, drv, efx->net_dev, "VPD Read-only not found\n");\r\nreturn;\r\n}\r\nro_size = pci_vpd_lrdt_size(&vpd_data[ro_start]);\r\nj = ro_size;\r\ni = ro_start + PCI_VPD_LRDT_TAG_SIZE;\r\nif (i + j > vpd_size)\r\nj = vpd_size - i;\r\ni = pci_vpd_find_info_keyword(vpd_data, i, j, "PN");\r\nif (i < 0) {\r\nnetif_err(efx, drv, efx->net_dev, "Part number not found\n");\r\nreturn;\r\n}\r\nj = pci_vpd_info_field_size(&vpd_data[i]);\r\ni += PCI_VPD_INFO_FLD_HDR_SIZE;\r\nif (i + j > vpd_size) {\r\nnetif_err(efx, drv, efx->net_dev, "Incomplete part number\n");\r\nreturn;\r\n}\r\nnetif_info(efx, drv, efx->net_dev,\r\n"Part Number : %.*s\n", j, &vpd_data[i]);\r\ni = ro_start + PCI_VPD_LRDT_TAG_SIZE;\r\nj = ro_size;\r\ni = pci_vpd_find_info_keyword(vpd_data, i, j, "SN");\r\nif (i < 0) {\r\nnetif_err(efx, drv, efx->net_dev, "Serial number not found\n");\r\nreturn;\r\n}\r\nj = pci_vpd_info_field_size(&vpd_data[i]);\r\ni += PCI_VPD_INFO_FLD_HDR_SIZE;\r\nif (i + j > vpd_size) {\r\nnetif_err(efx, drv, efx->net_dev, "Incomplete serial number\n");\r\nreturn;\r\n}\r\nefx->vpd_sn = kmalloc(j + 1, GFP_KERNEL);\r\nif (!efx->vpd_sn)\r\nreturn;\r\nsnprintf(efx->vpd_sn, j + 1, "%s", &vpd_data[i]);\r\n}\r\nstatic int efx_pci_probe_main(struct efx_nic *efx)\r\n{\r\nint rc;\r\nrc = efx_probe_all(efx);\r\nif (rc)\r\ngoto fail1;\r\nefx_init_napi(efx);\r\nrc = efx->type->init(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to initialise NIC\n");\r\ngoto fail3;\r\n}\r\nrc = efx_init_port(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to initialise port\n");\r\ngoto fail4;\r\n}\r\nrc = efx_nic_init_interrupt(efx);\r\nif (rc)\r\ngoto fail5;\r\nrc = efx_enable_interrupts(efx);\r\nif (rc)\r\ngoto fail6;\r\nreturn 0;\r\nfail6:\r\nefx_nic_fini_interrupt(efx);\r\nfail5:\r\nefx_fini_port(efx);\r\nfail4:\r\nefx->type->fini(efx);\r\nfail3:\r\nefx_fini_napi(efx);\r\nefx_remove_all(efx);\r\nfail1:\r\nreturn rc;\r\n}\r\nstatic int efx_pci_probe(struct pci_dev *pci_dev,\r\nconst struct pci_device_id *entry)\r\n{\r\nstruct net_device *net_dev;\r\nstruct efx_nic *efx;\r\nint rc;\r\nnet_dev = alloc_etherdev_mqs(sizeof(*efx), EFX_MAX_CORE_TX_QUEUES,\r\nEFX_MAX_RX_QUEUES);\r\nif (!net_dev)\r\nreturn -ENOMEM;\r\nefx = netdev_priv(net_dev);\r\nefx->type = (const struct efx_nic_type *) entry->driver_data;\r\nnet_dev->features |= (efx->type->offload_features | NETIF_F_SG |\r\nNETIF_F_HIGHDMA | NETIF_F_TSO |\r\nNETIF_F_RXCSUM);\r\nif (efx->type->offload_features & NETIF_F_V6_CSUM)\r\nnet_dev->features |= NETIF_F_TSO6;\r\nnet_dev->vlan_features |= (NETIF_F_ALL_CSUM | NETIF_F_SG |\r\nNETIF_F_HIGHDMA | NETIF_F_ALL_TSO |\r\nNETIF_F_RXCSUM);\r\nnet_dev->hw_features = net_dev->features & ~NETIF_F_HIGHDMA;\r\npci_set_drvdata(pci_dev, efx);\r\nSET_NETDEV_DEV(net_dev, &pci_dev->dev);\r\nrc = efx_init_struct(efx, pci_dev, net_dev);\r\nif (rc)\r\ngoto fail1;\r\nnetif_info(efx, probe, efx->net_dev,\r\n"Solarflare NIC detected\n");\r\nefx_probe_vpd_strings(efx);\r\nrc = efx_init_io(efx);\r\nif (rc)\r\ngoto fail2;\r\nrc = efx_pci_probe_main(efx);\r\nif (rc)\r\ngoto fail3;\r\nrc = efx_register_netdev(efx);\r\nif (rc)\r\ngoto fail4;\r\nrc = efx->type->sriov_init(efx);\r\nif (rc)\r\nnetif_err(efx, probe, efx->net_dev,\r\n"SR-IOV can't be enabled rc %d\n", rc);\r\nnetif_dbg(efx, probe, efx->net_dev, "initialisation successful\n");\r\nrtnl_lock();\r\nrc = efx_mtd_probe(efx);\r\nrtnl_unlock();\r\nif (rc)\r\nnetif_warn(efx, probe, efx->net_dev,\r\n"failed to create MTDs (%d)\n", rc);\r\nrc = pci_enable_pcie_error_reporting(pci_dev);\r\nif (rc && rc != -EINVAL)\r\nnetif_warn(efx, probe, efx->net_dev,\r\n"pci_enable_pcie_error_reporting failed (%d)\n", rc);\r\nreturn 0;\r\nfail4:\r\nefx_pci_remove_main(efx);\r\nfail3:\r\nefx_fini_io(efx);\r\nfail2:\r\nefx_fini_struct(efx);\r\nfail1:\r\nWARN_ON(rc > 0);\r\nnetif_dbg(efx, drv, efx->net_dev, "initialisation failed. rc=%d\n", rc);\r\nfree_netdev(net_dev);\r\nreturn rc;\r\n}\r\nstatic int efx_pm_freeze(struct device *dev)\r\n{\r\nstruct efx_nic *efx = pci_get_drvdata(to_pci_dev(dev));\r\nrtnl_lock();\r\nif (efx->state != STATE_DISABLED) {\r\nefx->state = STATE_UNINIT;\r\nefx_device_detach_sync(efx);\r\nefx_stop_all(efx);\r\nefx_disable_interrupts(efx);\r\n}\r\nrtnl_unlock();\r\nreturn 0;\r\n}\r\nstatic int efx_pm_thaw(struct device *dev)\r\n{\r\nint rc;\r\nstruct efx_nic *efx = pci_get_drvdata(to_pci_dev(dev));\r\nrtnl_lock();\r\nif (efx->state != STATE_DISABLED) {\r\nrc = efx_enable_interrupts(efx);\r\nif (rc)\r\ngoto fail;\r\nmutex_lock(&efx->mac_lock);\r\nefx->phy_op->reconfigure(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nefx_start_all(efx);\r\nnetif_device_attach(efx->net_dev);\r\nefx->state = STATE_READY;\r\nefx->type->resume_wol(efx);\r\n}\r\nrtnl_unlock();\r\nqueue_work(reset_workqueue, &efx->reset_work);\r\nreturn 0;\r\nfail:\r\nrtnl_unlock();\r\nreturn rc;\r\n}\r\nstatic int efx_pm_poweroff(struct device *dev)\r\n{\r\nstruct pci_dev *pci_dev = to_pci_dev(dev);\r\nstruct efx_nic *efx = pci_get_drvdata(pci_dev);\r\nefx->type->fini(efx);\r\nefx->reset_pending = 0;\r\npci_save_state(pci_dev);\r\nreturn pci_set_power_state(pci_dev, PCI_D3hot);\r\n}\r\nstatic int efx_pm_resume(struct device *dev)\r\n{\r\nstruct pci_dev *pci_dev = to_pci_dev(dev);\r\nstruct efx_nic *efx = pci_get_drvdata(pci_dev);\r\nint rc;\r\nrc = pci_set_power_state(pci_dev, PCI_D0);\r\nif (rc)\r\nreturn rc;\r\npci_restore_state(pci_dev);\r\nrc = pci_enable_device(pci_dev);\r\nif (rc)\r\nreturn rc;\r\npci_set_master(efx->pci_dev);\r\nrc = efx->type->reset(efx, RESET_TYPE_ALL);\r\nif (rc)\r\nreturn rc;\r\nrc = efx->type->init(efx);\r\nif (rc)\r\nreturn rc;\r\nrc = efx_pm_thaw(dev);\r\nreturn rc;\r\n}\r\nstatic int efx_pm_suspend(struct device *dev)\r\n{\r\nint rc;\r\nefx_pm_freeze(dev);\r\nrc = efx_pm_poweroff(dev);\r\nif (rc)\r\nefx_pm_resume(dev);\r\nreturn rc;\r\n}\r\nstatic pci_ers_result_t efx_io_error_detected(struct pci_dev *pdev,\r\nenum pci_channel_state state)\r\n{\r\npci_ers_result_t status = PCI_ERS_RESULT_RECOVERED;\r\nstruct efx_nic *efx = pci_get_drvdata(pdev);\r\nif (state == pci_channel_io_perm_failure)\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\nrtnl_lock();\r\nif (efx->state != STATE_DISABLED) {\r\nefx->state = STATE_RECOVERY;\r\nefx->reset_pending = 0;\r\nefx_device_detach_sync(efx);\r\nefx_stop_all(efx);\r\nefx_disable_interrupts(efx);\r\nstatus = PCI_ERS_RESULT_NEED_RESET;\r\n} else {\r\nstatus = PCI_ERS_RESULT_RECOVERED;\r\n}\r\nrtnl_unlock();\r\npci_disable_device(pdev);\r\nreturn status;\r\n}\r\nstatic pci_ers_result_t efx_io_slot_reset(struct pci_dev *pdev)\r\n{\r\nstruct efx_nic *efx = pci_get_drvdata(pdev);\r\npci_ers_result_t status = PCI_ERS_RESULT_RECOVERED;\r\nint rc;\r\nif (pci_enable_device(pdev)) {\r\nnetif_err(efx, hw, efx->net_dev,\r\n"Cannot re-enable PCI device after reset.\n");\r\nstatus = PCI_ERS_RESULT_DISCONNECT;\r\n}\r\nrc = pci_cleanup_aer_uncorrect_error_status(pdev);\r\nif (rc) {\r\nnetif_err(efx, hw, efx->net_dev,\r\n"pci_cleanup_aer_uncorrect_error_status failed (%d)\n", rc);\r\n}\r\nreturn status;\r\n}\r\nstatic void efx_io_resume(struct pci_dev *pdev)\r\n{\r\nstruct efx_nic *efx = pci_get_drvdata(pdev);\r\nint rc;\r\nrtnl_lock();\r\nif (efx->state == STATE_DISABLED)\r\ngoto out;\r\nrc = efx_reset(efx, RESET_TYPE_ALL);\r\nif (rc) {\r\nnetif_err(efx, hw, efx->net_dev,\r\n"efx_reset failed after PCI error (%d)\n", rc);\r\n} else {\r\nefx->state = STATE_READY;\r\nnetif_dbg(efx, hw, efx->net_dev,\r\n"Done resetting and resuming IO after PCI error.\n");\r\n}\r\nout:\r\nrtnl_unlock();\r\n}\r\nstatic int __init efx_init_module(void)\r\n{\r\nint rc;\r\nprintk(KERN_INFO "Solarflare NET driver v" EFX_DRIVER_VERSION "\n");\r\nrc = register_netdevice_notifier(&efx_netdev_notifier);\r\nif (rc)\r\ngoto err_notifier;\r\nrc = efx_init_sriov();\r\nif (rc)\r\ngoto err_sriov;\r\nreset_workqueue = create_singlethread_workqueue("sfc_reset");\r\nif (!reset_workqueue) {\r\nrc = -ENOMEM;\r\ngoto err_reset;\r\n}\r\nrc = pci_register_driver(&efx_pci_driver);\r\nif (rc < 0)\r\ngoto err_pci;\r\nreturn 0;\r\nerr_pci:\r\ndestroy_workqueue(reset_workqueue);\r\nerr_reset:\r\nefx_fini_sriov();\r\nerr_sriov:\r\nunregister_netdevice_notifier(&efx_netdev_notifier);\r\nerr_notifier:\r\nreturn rc;\r\n}\r\nstatic void __exit efx_exit_module(void)\r\n{\r\nprintk(KERN_INFO "Solarflare NET driver unloading\n");\r\npci_unregister_driver(&efx_pci_driver);\r\ndestroy_workqueue(reset_workqueue);\r\nefx_fini_sriov();\r\nunregister_netdevice_notifier(&efx_netdev_notifier);\r\n}
