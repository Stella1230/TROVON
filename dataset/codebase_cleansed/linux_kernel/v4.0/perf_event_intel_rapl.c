static inline u64 rapl_read_counter(struct perf_event *event)\r\n{\r\nu64 raw;\r\nrdmsrl(event->hw.event_base, raw);\r\nreturn raw;\r\n}\r\nstatic inline u64 rapl_scale(u64 v)\r\n{\r\nreturn v << (32 - __this_cpu_read(rapl_pmu)->hw_unit);\r\n}\r\nstatic u64 rapl_event_update(struct perf_event *event)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nu64 prev_raw_count, new_raw_count;\r\ns64 delta, sdelta;\r\nint shift = RAPL_CNTR_WIDTH;\r\nagain:\r\nprev_raw_count = local64_read(&hwc->prev_count);\r\nrdmsrl(event->hw.event_base, new_raw_count);\r\nif (local64_cmpxchg(&hwc->prev_count, prev_raw_count,\r\nnew_raw_count) != prev_raw_count) {\r\ncpu_relax();\r\ngoto again;\r\n}\r\ndelta = (new_raw_count << shift) - (prev_raw_count << shift);\r\ndelta >>= shift;\r\nsdelta = rapl_scale(delta);\r\nlocal64_add(sdelta, &event->count);\r\nreturn new_raw_count;\r\n}\r\nstatic void rapl_start_hrtimer(struct rapl_pmu *pmu)\r\n{\r\n__hrtimer_start_range_ns(&pmu->hrtimer,\r\npmu->timer_interval, 0,\r\nHRTIMER_MODE_REL_PINNED, 0);\r\n}\r\nstatic void rapl_stop_hrtimer(struct rapl_pmu *pmu)\r\n{\r\nhrtimer_cancel(&pmu->hrtimer);\r\n}\r\nstatic enum hrtimer_restart rapl_hrtimer_handle(struct hrtimer *hrtimer)\r\n{\r\nstruct rapl_pmu *pmu = __this_cpu_read(rapl_pmu);\r\nstruct perf_event *event;\r\nunsigned long flags;\r\nif (!pmu->n_active)\r\nreturn HRTIMER_NORESTART;\r\nspin_lock_irqsave(&pmu->lock, flags);\r\nlist_for_each_entry(event, &pmu->active_list, active_entry) {\r\nrapl_event_update(event);\r\n}\r\nspin_unlock_irqrestore(&pmu->lock, flags);\r\nhrtimer_forward_now(hrtimer, pmu->timer_interval);\r\nreturn HRTIMER_RESTART;\r\n}\r\nstatic void rapl_hrtimer_init(struct rapl_pmu *pmu)\r\n{\r\nstruct hrtimer *hr = &pmu->hrtimer;\r\nhrtimer_init(hr, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nhr->function = rapl_hrtimer_handle;\r\n}\r\nstatic void __rapl_pmu_event_start(struct rapl_pmu *pmu,\r\nstruct perf_event *event)\r\n{\r\nif (WARN_ON_ONCE(!(event->hw.state & PERF_HES_STOPPED)))\r\nreturn;\r\nevent->hw.state = 0;\r\nlist_add_tail(&event->active_entry, &pmu->active_list);\r\nlocal64_set(&event->hw.prev_count, rapl_read_counter(event));\r\npmu->n_active++;\r\nif (pmu->n_active == 1)\r\nrapl_start_hrtimer(pmu);\r\n}\r\nstatic void rapl_pmu_event_start(struct perf_event *event, int mode)\r\n{\r\nstruct rapl_pmu *pmu = __this_cpu_read(rapl_pmu);\r\nunsigned long flags;\r\nspin_lock_irqsave(&pmu->lock, flags);\r\n__rapl_pmu_event_start(pmu, event);\r\nspin_unlock_irqrestore(&pmu->lock, flags);\r\n}\r\nstatic void rapl_pmu_event_stop(struct perf_event *event, int mode)\r\n{\r\nstruct rapl_pmu *pmu = __this_cpu_read(rapl_pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pmu->lock, flags);\r\nif (!(hwc->state & PERF_HES_STOPPED)) {\r\nWARN_ON_ONCE(pmu->n_active <= 0);\r\npmu->n_active--;\r\nif (pmu->n_active == 0)\r\nrapl_stop_hrtimer(pmu);\r\nlist_del(&event->active_entry);\r\nWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\r\nhwc->state |= PERF_HES_STOPPED;\r\n}\r\nif ((mode & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\r\nrapl_event_update(event);\r\nhwc->state |= PERF_HES_UPTODATE;\r\n}\r\nspin_unlock_irqrestore(&pmu->lock, flags);\r\n}\r\nstatic int rapl_pmu_event_add(struct perf_event *event, int mode)\r\n{\r\nstruct rapl_pmu *pmu = __this_cpu_read(rapl_pmu);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pmu->lock, flags);\r\nhwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\r\nif (mode & PERF_EF_START)\r\n__rapl_pmu_event_start(pmu, event);\r\nspin_unlock_irqrestore(&pmu->lock, flags);\r\nreturn 0;\r\n}\r\nstatic void rapl_pmu_event_del(struct perf_event *event, int flags)\r\n{\r\nrapl_pmu_event_stop(event, PERF_EF_UPDATE);\r\n}\r\nstatic int rapl_pmu_event_init(struct perf_event *event)\r\n{\r\nu64 cfg = event->attr.config & RAPL_EVENT_MASK;\r\nint bit, msr, ret = 0;\r\nif (event->attr.type != rapl_pmu_class.type)\r\nreturn -ENOENT;\r\nif (event->attr.config & ~RAPL_EVENT_MASK)\r\nreturn -EINVAL;\r\nswitch (cfg) {\r\ncase INTEL_RAPL_PP0:\r\nbit = RAPL_IDX_PP0_NRG_STAT;\r\nmsr = MSR_PP0_ENERGY_STATUS;\r\nbreak;\r\ncase INTEL_RAPL_PKG:\r\nbit = RAPL_IDX_PKG_NRG_STAT;\r\nmsr = MSR_PKG_ENERGY_STATUS;\r\nbreak;\r\ncase INTEL_RAPL_RAM:\r\nbit = RAPL_IDX_RAM_NRG_STAT;\r\nmsr = MSR_DRAM_ENERGY_STATUS;\r\nbreak;\r\ncase INTEL_RAPL_PP1:\r\nbit = RAPL_IDX_PP1_NRG_STAT;\r\nmsr = MSR_PP1_ENERGY_STATUS;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (!(rapl_cntr_mask & (1 << bit)))\r\nreturn -EINVAL;\r\nif (event->attr.exclude_user ||\r\nevent->attr.exclude_kernel ||\r\nevent->attr.exclude_hv ||\r\nevent->attr.exclude_idle ||\r\nevent->attr.exclude_host ||\r\nevent->attr.exclude_guest ||\r\nevent->attr.sample_period)\r\nreturn -EINVAL;\r\nevent->hw.event_base = msr;\r\nevent->hw.config = cfg;\r\nevent->hw.idx = bit;\r\nreturn ret;\r\n}\r\nstatic void rapl_pmu_event_read(struct perf_event *event)\r\n{\r\nrapl_event_update(event);\r\n}\r\nstatic ssize_t rapl_get_attr_cpumask(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nreturn cpumap_print_to_pagebuf(true, buf, &rapl_cpu_mask);\r\n}\r\nstatic ssize_t rapl_sysfs_show(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *page)\r\n{\r\nstruct perf_pmu_events_attr *pmu_attr = \\r\ncontainer_of(attr, struct perf_pmu_events_attr, attr);\r\nif (pmu_attr->event_str)\r\nreturn sprintf(page, "%s", pmu_attr->event_str);\r\nreturn 0;\r\n}\r\nstatic void rapl_cpu_exit(int cpu)\r\n{\r\nstruct rapl_pmu *pmu = per_cpu(rapl_pmu, cpu);\r\nint i, phys_id = topology_physical_package_id(cpu);\r\nint target = -1;\r\nfor_each_online_cpu(i) {\r\nif (i == cpu)\r\ncontinue;\r\nif (phys_id == topology_physical_package_id(i)) {\r\ntarget = i;\r\nbreak;\r\n}\r\n}\r\nif (cpumask_test_and_clear_cpu(cpu, &rapl_cpu_mask) && target >= 0)\r\ncpumask_set_cpu(target, &rapl_cpu_mask);\r\nWARN_ON(cpumask_empty(&rapl_cpu_mask));\r\nif (target >= 0)\r\nperf_pmu_migrate_context(pmu->pmu, cpu, target);\r\nrapl_stop_hrtimer(pmu);\r\n}\r\nstatic void rapl_cpu_init(int cpu)\r\n{\r\nint i, phys_id = topology_physical_package_id(cpu);\r\nfor_each_cpu(i, &rapl_cpu_mask) {\r\nif (phys_id == topology_physical_package_id(i))\r\nreturn;\r\n}\r\ncpumask_set_cpu(cpu, &rapl_cpu_mask);\r\n}\r\nstatic int rapl_cpu_prepare(int cpu)\r\n{\r\nstruct rapl_pmu *pmu = per_cpu(rapl_pmu, cpu);\r\nint phys_id = topology_physical_package_id(cpu);\r\nu64 ms;\r\nu64 msr_rapl_power_unit_bits;\r\nif (pmu)\r\nreturn 0;\r\nif (phys_id < 0)\r\nreturn -1;\r\nif (rdmsrl_safe(MSR_RAPL_POWER_UNIT, &msr_rapl_power_unit_bits))\r\nreturn -1;\r\npmu = kzalloc_node(sizeof(*pmu), GFP_KERNEL, cpu_to_node(cpu));\r\nif (!pmu)\r\nreturn -1;\r\nspin_lock_init(&pmu->lock);\r\nINIT_LIST_HEAD(&pmu->active_list);\r\npmu->hw_unit = (msr_rapl_power_unit_bits >> 8) & 0x1FULL;\r\npmu->pmu = &rapl_pmu_class;\r\nif (pmu->hw_unit < 32)\r\nms = (1000 / (2 * 100)) * (1ULL << (32 - pmu->hw_unit - 1));\r\nelse\r\nms = 2;\r\npmu->timer_interval = ms_to_ktime(ms);\r\nrapl_hrtimer_init(pmu);\r\nper_cpu(rapl_pmu, cpu) = pmu;\r\nper_cpu(rapl_pmu_to_free, cpu) = NULL;\r\nreturn 0;\r\n}\r\nstatic void rapl_cpu_kfree(int cpu)\r\n{\r\nstruct rapl_pmu *pmu = per_cpu(rapl_pmu_to_free, cpu);\r\nkfree(pmu);\r\nper_cpu(rapl_pmu_to_free, cpu) = NULL;\r\n}\r\nstatic int rapl_cpu_dying(int cpu)\r\n{\r\nstruct rapl_pmu *pmu = per_cpu(rapl_pmu, cpu);\r\nif (!pmu)\r\nreturn 0;\r\nper_cpu(rapl_pmu, cpu) = NULL;\r\nper_cpu(rapl_pmu_to_free, cpu) = pmu;\r\nreturn 0;\r\n}\r\nstatic int rapl_cpu_notifier(struct notifier_block *self,\r\nunsigned long action, void *hcpu)\r\n{\r\nunsigned int cpu = (long)hcpu;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_UP_PREPARE:\r\nrapl_cpu_prepare(cpu);\r\nbreak;\r\ncase CPU_STARTING:\r\nrapl_cpu_init(cpu);\r\nbreak;\r\ncase CPU_UP_CANCELED:\r\ncase CPU_DYING:\r\nrapl_cpu_dying(cpu);\r\nbreak;\r\ncase CPU_ONLINE:\r\ncase CPU_DEAD:\r\nrapl_cpu_kfree(cpu);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\nrapl_cpu_exit(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic int __init rapl_pmu_init(void)\r\n{\r\nstruct rapl_pmu *pmu;\r\nint cpu, ret;\r\nif (!x86_match_cpu(rapl_cpu_match))\r\nreturn 0;\r\nswitch (boot_cpu_data.x86_model) {\r\ncase 42:\r\ncase 58:\r\nrapl_cntr_mask = RAPL_IDX_CLN;\r\nrapl_pmu_events_group.attrs = rapl_events_cln_attr;\r\nbreak;\r\ncase 60:\r\ncase 69:\r\nrapl_cntr_mask = RAPL_IDX_HSW;\r\nrapl_pmu_events_group.attrs = rapl_events_hsw_attr;\r\nbreak;\r\ncase 45:\r\ncase 62:\r\nrapl_cntr_mask = RAPL_IDX_SRV;\r\nrapl_pmu_events_group.attrs = rapl_events_srv_attr;\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\ncpu_notifier_register_begin();\r\nfor_each_online_cpu(cpu) {\r\nret = rapl_cpu_prepare(cpu);\r\nif (ret)\r\ngoto out;\r\nrapl_cpu_init(cpu);\r\n}\r\n__perf_cpu_notifier(rapl_cpu_notifier);\r\nret = perf_pmu_register(&rapl_pmu_class, "power", -1);\r\nif (WARN_ON(ret)) {\r\npr_info("RAPL PMU detected, registration failed (%d), RAPL PMU disabled\n", ret);\r\ncpu_notifier_register_done();\r\nreturn -1;\r\n}\r\npmu = __this_cpu_read(rapl_pmu);\r\npr_info("RAPL PMU detected, hw unit 2^-%d Joules,"\r\n" API unit is 2^-32 Joules,"\r\n" %d fixed counters"\r\n" %llu ms ovfl timer\n",\r\npmu->hw_unit,\r\nhweight32(rapl_cntr_mask),\r\nktime_to_ms(pmu->timer_interval));\r\nout:\r\ncpu_notifier_register_done();\r\nreturn 0;\r\n}
