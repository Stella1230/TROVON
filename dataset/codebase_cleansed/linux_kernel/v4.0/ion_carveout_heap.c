ion_phys_addr_t ion_carveout_allocate(struct ion_heap *heap,\r\nunsigned long size,\r\nunsigned long align)\r\n{\r\nstruct ion_carveout_heap *carveout_heap =\r\ncontainer_of(heap, struct ion_carveout_heap, heap);\r\nunsigned long offset = gen_pool_alloc(carveout_heap->pool, size);\r\nif (!offset)\r\nreturn ION_CARVEOUT_ALLOCATE_FAIL;\r\nreturn offset;\r\n}\r\nvoid ion_carveout_free(struct ion_heap *heap, ion_phys_addr_t addr,\r\nunsigned long size)\r\n{\r\nstruct ion_carveout_heap *carveout_heap =\r\ncontainer_of(heap, struct ion_carveout_heap, heap);\r\nif (addr == ION_CARVEOUT_ALLOCATE_FAIL)\r\nreturn;\r\ngen_pool_free(carveout_heap->pool, addr, size);\r\n}\r\nstatic int ion_carveout_heap_phys(struct ion_heap *heap,\r\nstruct ion_buffer *buffer,\r\nion_phys_addr_t *addr, size_t *len)\r\n{\r\nstruct sg_table *table = buffer->priv_virt;\r\nstruct page *page = sg_page(table->sgl);\r\nion_phys_addr_t paddr = PFN_PHYS(page_to_pfn(page));\r\n*addr = paddr;\r\n*len = buffer->size;\r\nreturn 0;\r\n}\r\nstatic int ion_carveout_heap_allocate(struct ion_heap *heap,\r\nstruct ion_buffer *buffer,\r\nunsigned long size, unsigned long align,\r\nunsigned long flags)\r\n{\r\nstruct sg_table *table;\r\nion_phys_addr_t paddr;\r\nint ret;\r\nif (align > PAGE_SIZE)\r\nreturn -EINVAL;\r\ntable = kmalloc(sizeof(struct sg_table), GFP_KERNEL);\r\nif (!table)\r\nreturn -ENOMEM;\r\nret = sg_alloc_table(table, 1, GFP_KERNEL);\r\nif (ret)\r\ngoto err_free;\r\npaddr = ion_carveout_allocate(heap, size, align);\r\nif (paddr == ION_CARVEOUT_ALLOCATE_FAIL) {\r\nret = -ENOMEM;\r\ngoto err_free_table;\r\n}\r\nsg_set_page(table->sgl, pfn_to_page(PFN_DOWN(paddr)), size, 0);\r\nbuffer->priv_virt = table;\r\nreturn 0;\r\nerr_free_table:\r\nsg_free_table(table);\r\nerr_free:\r\nkfree(table);\r\nreturn ret;\r\n}\r\nstatic void ion_carveout_heap_free(struct ion_buffer *buffer)\r\n{\r\nstruct ion_heap *heap = buffer->heap;\r\nstruct sg_table *table = buffer->priv_virt;\r\nstruct page *page = sg_page(table->sgl);\r\nion_phys_addr_t paddr = PFN_PHYS(page_to_pfn(page));\r\nion_heap_buffer_zero(buffer);\r\nif (ion_buffer_cached(buffer))\r\ndma_sync_sg_for_device(NULL, table->sgl, table->nents,\r\nDMA_BIDIRECTIONAL);\r\nion_carveout_free(heap, paddr, buffer->size);\r\nsg_free_table(table);\r\nkfree(table);\r\n}\r\nstatic struct sg_table *ion_carveout_heap_map_dma(struct ion_heap *heap,\r\nstruct ion_buffer *buffer)\r\n{\r\nreturn buffer->priv_virt;\r\n}\r\nstatic void ion_carveout_heap_unmap_dma(struct ion_heap *heap,\r\nstruct ion_buffer *buffer)\r\n{\r\n}\r\nstruct ion_heap *ion_carveout_heap_create(struct ion_platform_heap *heap_data)\r\n{\r\nstruct ion_carveout_heap *carveout_heap;\r\nint ret;\r\nstruct page *page;\r\nsize_t size;\r\npage = pfn_to_page(PFN_DOWN(heap_data->base));\r\nsize = heap_data->size;\r\nion_pages_sync_for_device(NULL, page, size, DMA_BIDIRECTIONAL);\r\nret = ion_heap_pages_zero(page, size, pgprot_writecombine(PAGE_KERNEL));\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\ncarveout_heap = kzalloc(sizeof(struct ion_carveout_heap), GFP_KERNEL);\r\nif (!carveout_heap)\r\nreturn ERR_PTR(-ENOMEM);\r\ncarveout_heap->pool = gen_pool_create(12, -1);\r\nif (!carveout_heap->pool) {\r\nkfree(carveout_heap);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\ncarveout_heap->base = heap_data->base;\r\ngen_pool_add(carveout_heap->pool, carveout_heap->base, heap_data->size,\r\n-1);\r\ncarveout_heap->heap.ops = &carveout_heap_ops;\r\ncarveout_heap->heap.type = ION_HEAP_TYPE_CARVEOUT;\r\ncarveout_heap->heap.flags = ION_HEAP_FLAG_DEFER_FREE;\r\nreturn &carveout_heap->heap;\r\n}\r\nvoid ion_carveout_heap_destroy(struct ion_heap *heap)\r\n{\r\nstruct ion_carveout_heap *carveout_heap =\r\ncontainer_of(heap, struct ion_carveout_heap, heap);\r\ngen_pool_destroy(carveout_heap->pool);\r\nkfree(carveout_heap);\r\ncarveout_heap = NULL;\r\n}
