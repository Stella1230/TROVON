static struct sg_table *omap_gem_map_dma_buf(\r\nstruct dma_buf_attachment *attachment,\r\nenum dma_data_direction dir)\r\n{\r\nstruct drm_gem_object *obj = attachment->dmabuf->priv;\r\nstruct sg_table *sg;\r\ndma_addr_t paddr;\r\nint ret;\r\nsg = kzalloc(sizeof(*sg), GFP_KERNEL);\r\nif (!sg)\r\nreturn ERR_PTR(-ENOMEM);\r\nret = omap_gem_get_paddr(obj, &paddr, true);\r\nif (ret)\r\ngoto out;\r\nret = sg_alloc_table(sg, 1, GFP_KERNEL);\r\nif (ret)\r\ngoto out;\r\nsg_init_table(sg->sgl, 1);\r\nsg_dma_len(sg->sgl) = obj->size;\r\nsg_set_page(sg->sgl, pfn_to_page(PFN_DOWN(paddr)), obj->size, 0);\r\nsg_dma_address(sg->sgl) = paddr;\r\nomap_gem_dma_sync(obj, dir);\r\nreturn sg;\r\nout:\r\nkfree(sg);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void omap_gem_unmap_dma_buf(struct dma_buf_attachment *attachment,\r\nstruct sg_table *sg, enum dma_data_direction dir)\r\n{\r\nstruct drm_gem_object *obj = attachment->dmabuf->priv;\r\nomap_gem_put_paddr(obj);\r\nsg_free_table(sg);\r\nkfree(sg);\r\n}\r\nstatic void omap_gem_dmabuf_release(struct dma_buf *buffer)\r\n{\r\nstruct drm_gem_object *obj = buffer->priv;\r\ndrm_gem_object_unreference_unlocked(obj);\r\n}\r\nstatic int omap_gem_dmabuf_begin_cpu_access(struct dma_buf *buffer,\r\nsize_t start, size_t len, enum dma_data_direction dir)\r\n{\r\nstruct drm_gem_object *obj = buffer->priv;\r\nstruct page **pages;\r\nif (omap_gem_flags(obj) & OMAP_BO_TILED) {\r\nreturn -ENOMEM;\r\n}\r\nreturn omap_gem_get_pages(obj, &pages, true);\r\n}\r\nstatic void omap_gem_dmabuf_end_cpu_access(struct dma_buf *buffer,\r\nsize_t start, size_t len, enum dma_data_direction dir)\r\n{\r\nstruct drm_gem_object *obj = buffer->priv;\r\nomap_gem_put_pages(obj);\r\n}\r\nstatic void *omap_gem_dmabuf_kmap_atomic(struct dma_buf *buffer,\r\nunsigned long page_num)\r\n{\r\nstruct drm_gem_object *obj = buffer->priv;\r\nstruct page **pages;\r\nomap_gem_get_pages(obj, &pages, false);\r\nomap_gem_cpu_sync(obj, page_num);\r\nreturn kmap_atomic(pages[page_num]);\r\n}\r\nstatic void omap_gem_dmabuf_kunmap_atomic(struct dma_buf *buffer,\r\nunsigned long page_num, void *addr)\r\n{\r\nkunmap_atomic(addr);\r\n}\r\nstatic void *omap_gem_dmabuf_kmap(struct dma_buf *buffer,\r\nunsigned long page_num)\r\n{\r\nstruct drm_gem_object *obj = buffer->priv;\r\nstruct page **pages;\r\nomap_gem_get_pages(obj, &pages, false);\r\nomap_gem_cpu_sync(obj, page_num);\r\nreturn kmap(pages[page_num]);\r\n}\r\nstatic void omap_gem_dmabuf_kunmap(struct dma_buf *buffer,\r\nunsigned long page_num, void *addr)\r\n{\r\nstruct drm_gem_object *obj = buffer->priv;\r\nstruct page **pages;\r\nomap_gem_get_pages(obj, &pages, false);\r\nkunmap(pages[page_num]);\r\n}\r\nstatic int omap_gem_dmabuf_mmap(struct dma_buf *buffer,\r\nstruct vm_area_struct *vma)\r\n{\r\nstruct drm_gem_object *obj = buffer->priv;\r\nstruct drm_device *dev = obj->dev;\r\nint ret = 0;\r\nif (WARN_ON(!obj->filp))\r\nreturn -EINVAL;\r\nmutex_lock(&dev->struct_mutex);\r\nret = drm_gem_mmap_obj(obj, omap_gem_mmap_size(obj), vma);\r\nmutex_unlock(&dev->struct_mutex);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn omap_gem_mmap_obj(obj, vma);\r\n}\r\nstruct dma_buf *omap_gem_prime_export(struct drm_device *dev,\r\nstruct drm_gem_object *obj, int flags)\r\n{\r\nreturn dma_buf_export(obj, &omap_dmabuf_ops, obj->size, flags, NULL);\r\n}\r\nstruct drm_gem_object *omap_gem_prime_import(struct drm_device *dev,\r\nstruct dma_buf *buffer)\r\n{\r\nstruct drm_gem_object *obj;\r\nif (buffer->ops == &omap_dmabuf_ops) {\r\nobj = buffer->priv;\r\nif (obj->dev == dev) {\r\ndrm_gem_object_reference(obj);\r\nreturn obj;\r\n}\r\n}\r\nreturn ERR_PTR(-EINVAL);\r\n}
