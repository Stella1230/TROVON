ssize_t uncore_event_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nstruct uncore_event_desc *event =\r\ncontainer_of(attr, struct uncore_event_desc, attr);\r\nreturn sprintf(buf, "%s", event->config);\r\n}\r\nstruct intel_uncore_pmu *uncore_event_to_pmu(struct perf_event *event)\r\n{\r\nreturn container_of(event->pmu, struct intel_uncore_pmu, pmu);\r\n}\r\nstruct intel_uncore_box *uncore_pmu_to_box(struct intel_uncore_pmu *pmu, int cpu)\r\n{\r\nstruct intel_uncore_box *box;\r\nbox = *per_cpu_ptr(pmu->box, cpu);\r\nif (box)\r\nreturn box;\r\nraw_spin_lock(&uncore_box_lock);\r\nif (*per_cpu_ptr(pmu->box, cpu))\r\ngoto out;\r\nlist_for_each_entry(box, &pmu->box_list, list) {\r\nif (box->phys_id == topology_physical_package_id(cpu)) {\r\natomic_inc(&box->refcnt);\r\n*per_cpu_ptr(pmu->box, cpu) = box;\r\nbreak;\r\n}\r\n}\r\nout:\r\nraw_spin_unlock(&uncore_box_lock);\r\nreturn *per_cpu_ptr(pmu->box, cpu);\r\n}\r\nstruct intel_uncore_box *uncore_event_to_box(struct perf_event *event)\r\n{\r\nreturn uncore_pmu_to_box(uncore_event_to_pmu(event), smp_processor_id());\r\n}\r\nu64 uncore_msr_read_counter(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nu64 count;\r\nrdmsrl(event->hw.event_base, count);\r\nreturn count;\r\n}\r\nstruct event_constraint *\r\nuncore_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nstruct hw_perf_event_extra *reg2 = &event->hw.branch_reg;\r\nunsigned long flags;\r\nbool ok = false;\r\nif (reg1->idx == EXTRA_REG_NONE ||\r\n(!uncore_box_is_fake(box) && reg1->alloc))\r\nreturn NULL;\r\ner = &box->shared_regs[reg1->idx];\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nif (!atomic_read(&er->ref) ||\r\n(er->config1 == reg1->config && er->config2 == reg2->config)) {\r\natomic_inc(&er->ref);\r\ner->config1 = reg1->config;\r\ner->config2 = reg2->config;\r\nok = true;\r\n}\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nif (ok) {\r\nif (!uncore_box_is_fake(box))\r\nreg1->alloc = 1;\r\nreturn NULL;\r\n}\r\nreturn &uncore_constraint_empty;\r\n}\r\nvoid uncore_put_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\r\nif (uncore_box_is_fake(box) || !reg1->alloc)\r\nreturn;\r\ner = &box->shared_regs[reg1->idx];\r\natomic_dec(&er->ref);\r\nreg1->alloc = 0;\r\n}\r\nu64 uncore_shared_reg_config(struct intel_uncore_box *box, int idx)\r\n{\r\nstruct intel_uncore_extra_reg *er;\r\nunsigned long flags;\r\nu64 config;\r\ner = &box->shared_regs[idx];\r\nraw_spin_lock_irqsave(&er->lock, flags);\r\nconfig = er->config;\r\nraw_spin_unlock_irqrestore(&er->lock, flags);\r\nreturn config;\r\n}\r\nstatic void uncore_assign_hw_event(struct intel_uncore_box *box, struct perf_event *event, int idx)\r\n{\r\nstruct hw_perf_event *hwc = &event->hw;\r\nhwc->idx = idx;\r\nhwc->last_tag = ++box->tags[idx];\r\nif (hwc->idx == UNCORE_PMC_IDX_FIXED) {\r\nhwc->event_base = uncore_fixed_ctr(box);\r\nhwc->config_base = uncore_fixed_ctl(box);\r\nreturn;\r\n}\r\nhwc->config_base = uncore_event_ctl(box, hwc->idx);\r\nhwc->event_base = uncore_perf_ctr(box, hwc->idx);\r\n}\r\nvoid uncore_perf_event_update(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nu64 prev_count, new_count, delta;\r\nint shift;\r\nif (event->hw.idx >= UNCORE_PMC_IDX_FIXED)\r\nshift = 64 - uncore_fixed_ctr_bits(box);\r\nelse\r\nshift = 64 - uncore_perf_ctr_bits(box);\r\nagain:\r\nprev_count = local64_read(&event->hw.prev_count);\r\nnew_count = uncore_read_counter(box, event);\r\nif (local64_xchg(&event->hw.prev_count, new_count) != prev_count)\r\ngoto again;\r\ndelta = (new_count << shift) - (prev_count << shift);\r\ndelta >>= shift;\r\nlocal64_add(delta, &event->count);\r\n}\r\nstatic enum hrtimer_restart uncore_pmu_hrtimer(struct hrtimer *hrtimer)\r\n{\r\nstruct intel_uncore_box *box;\r\nstruct perf_event *event;\r\nunsigned long flags;\r\nint bit;\r\nbox = container_of(hrtimer, struct intel_uncore_box, hrtimer);\r\nif (!box->n_active || box->cpu != smp_processor_id())\r\nreturn HRTIMER_NORESTART;\r\nlocal_irq_save(flags);\r\nlist_for_each_entry(event, &box->active_list, active_entry) {\r\nuncore_perf_event_update(box, event);\r\n}\r\nfor_each_set_bit(bit, box->active_mask, UNCORE_PMC_IDX_MAX)\r\nuncore_perf_event_update(box, box->events[bit]);\r\nlocal_irq_restore(flags);\r\nhrtimer_forward_now(hrtimer, ns_to_ktime(box->hrtimer_duration));\r\nreturn HRTIMER_RESTART;\r\n}\r\nvoid uncore_pmu_start_hrtimer(struct intel_uncore_box *box)\r\n{\r\n__hrtimer_start_range_ns(&box->hrtimer,\r\nns_to_ktime(box->hrtimer_duration), 0,\r\nHRTIMER_MODE_REL_PINNED, 0);\r\n}\r\nvoid uncore_pmu_cancel_hrtimer(struct intel_uncore_box *box)\r\n{\r\nhrtimer_cancel(&box->hrtimer);\r\n}\r\nstatic void uncore_pmu_init_hrtimer(struct intel_uncore_box *box)\r\n{\r\nhrtimer_init(&box->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nbox->hrtimer.function = uncore_pmu_hrtimer;\r\n}\r\nstatic struct intel_uncore_box *uncore_alloc_box(struct intel_uncore_type *type, int node)\r\n{\r\nstruct intel_uncore_box *box;\r\nint i, size;\r\nsize = sizeof(*box) + type->num_shared_regs * sizeof(struct intel_uncore_extra_reg);\r\nbox = kzalloc_node(size, GFP_KERNEL, node);\r\nif (!box)\r\nreturn NULL;\r\nfor (i = 0; i < type->num_shared_regs; i++)\r\nraw_spin_lock_init(&box->shared_regs[i].lock);\r\nuncore_pmu_init_hrtimer(box);\r\natomic_set(&box->refcnt, 1);\r\nbox->cpu = -1;\r\nbox->phys_id = -1;\r\nbox->hrtimer_duration = UNCORE_PMU_HRTIMER_INTERVAL;\r\nINIT_LIST_HEAD(&box->active_list);\r\nreturn box;\r\n}\r\nstatic bool is_uncore_event(struct perf_event *event)\r\n{\r\nreturn event->pmu->event_init == uncore_pmu_event_init;\r\n}\r\nstatic int\r\nuncore_collect_events(struct intel_uncore_box *box, struct perf_event *leader, bool dogrp)\r\n{\r\nstruct perf_event *event;\r\nint n, max_count;\r\nmax_count = box->pmu->type->num_counters;\r\nif (box->pmu->type->fixed_ctl)\r\nmax_count++;\r\nif (box->n_events >= max_count)\r\nreturn -EINVAL;\r\nn = box->n_events;\r\nif (is_uncore_event(leader)) {\r\nbox->event_list[n] = leader;\r\nn++;\r\n}\r\nif (!dogrp)\r\nreturn n;\r\nlist_for_each_entry(event, &leader->sibling_list, group_entry) {\r\nif (!is_uncore_event(event) ||\r\nevent->state <= PERF_EVENT_STATE_OFF)\r\ncontinue;\r\nif (n >= max_count)\r\nreturn -EINVAL;\r\nbox->event_list[n] = event;\r\nn++;\r\n}\r\nreturn n;\r\n}\r\nstatic struct event_constraint *\r\nuncore_get_event_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nstruct intel_uncore_type *type = box->pmu->type;\r\nstruct event_constraint *c;\r\nif (type->ops->get_constraint) {\r\nc = type->ops->get_constraint(box, event);\r\nif (c)\r\nreturn c;\r\n}\r\nif (event->attr.config == UNCORE_FIXED_EVENT)\r\nreturn &uncore_constraint_fixed;\r\nif (type->constraints) {\r\nfor_each_event_constraint(c, type->constraints) {\r\nif ((event->hw.config & c->cmask) == c->code)\r\nreturn c;\r\n}\r\n}\r\nreturn &type->unconstrainted;\r\n}\r\nstatic void uncore_put_event_constraint(struct intel_uncore_box *box, struct perf_event *event)\r\n{\r\nif (box->pmu->type->ops->put_constraint)\r\nbox->pmu->type->ops->put_constraint(box, event);\r\n}\r\nstatic int uncore_assign_events(struct intel_uncore_box *box, int assign[], int n)\r\n{\r\nunsigned long used_mask[BITS_TO_LONGS(UNCORE_PMC_IDX_MAX)];\r\nstruct event_constraint *c;\r\nint i, wmin, wmax, ret = 0;\r\nstruct hw_perf_event *hwc;\r\nbitmap_zero(used_mask, UNCORE_PMC_IDX_MAX);\r\nfor (i = 0, wmin = UNCORE_PMC_IDX_MAX, wmax = 0; i < n; i++) {\r\nhwc = &box->event_list[i]->hw;\r\nc = uncore_get_event_constraint(box, box->event_list[i]);\r\nhwc->constraint = c;\r\nwmin = min(wmin, c->weight);\r\nwmax = max(wmax, c->weight);\r\n}\r\nfor (i = 0; i < n; i++) {\r\nhwc = &box->event_list[i]->hw;\r\nc = hwc->constraint;\r\nif (hwc->idx == -1)\r\nbreak;\r\nif (!test_bit(hwc->idx, c->idxmsk))\r\nbreak;\r\nif (test_bit(hwc->idx, used_mask))\r\nbreak;\r\n__set_bit(hwc->idx, used_mask);\r\nif (assign)\r\nassign[i] = hwc->idx;\r\n}\r\nif (i != n)\r\nret = perf_assign_events(box->event_list, n,\r\nwmin, wmax, assign);\r\nif (!assign || ret) {\r\nfor (i = 0; i < n; i++)\r\nuncore_put_event_constraint(box, box->event_list[i]);\r\n}\r\nreturn ret ? -EINVAL : 0;\r\n}\r\nstatic void uncore_pmu_event_start(struct perf_event *event, int flags)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nint idx = event->hw.idx;\r\nif (WARN_ON_ONCE(!(event->hw.state & PERF_HES_STOPPED)))\r\nreturn;\r\nif (WARN_ON_ONCE(idx == -1 || idx >= UNCORE_PMC_IDX_MAX))\r\nreturn;\r\nevent->hw.state = 0;\r\nbox->events[idx] = event;\r\nbox->n_active++;\r\n__set_bit(idx, box->active_mask);\r\nlocal64_set(&event->hw.prev_count, uncore_read_counter(box, event));\r\nuncore_enable_event(box, event);\r\nif (box->n_active == 1) {\r\nuncore_enable_box(box);\r\nuncore_pmu_start_hrtimer(box);\r\n}\r\n}\r\nstatic void uncore_pmu_event_stop(struct perf_event *event, int flags)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nif (__test_and_clear_bit(hwc->idx, box->active_mask)) {\r\nuncore_disable_event(box, event);\r\nbox->n_active--;\r\nbox->events[hwc->idx] = NULL;\r\nWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\r\nhwc->state |= PERF_HES_STOPPED;\r\nif (box->n_active == 0) {\r\nuncore_disable_box(box);\r\nuncore_pmu_cancel_hrtimer(box);\r\n}\r\n}\r\nif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\r\nuncore_perf_event_update(box, event);\r\nhwc->state |= PERF_HES_UPTODATE;\r\n}\r\n}\r\nstatic int uncore_pmu_event_add(struct perf_event *event, int flags)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint assign[UNCORE_PMC_IDX_MAX];\r\nint i, n, ret;\r\nif (!box)\r\nreturn -ENODEV;\r\nret = n = uncore_collect_events(box, event, false);\r\nif (ret < 0)\r\nreturn ret;\r\nhwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\r\nif (!(flags & PERF_EF_START))\r\nhwc->state |= PERF_HES_ARCH;\r\nret = uncore_assign_events(box, assign, n);\r\nif (ret)\r\nreturn ret;\r\nfor (i = 0; i < box->n_events; i++) {\r\nevent = box->event_list[i];\r\nhwc = &event->hw;\r\nif (hwc->idx == assign[i] &&\r\nhwc->last_tag == box->tags[assign[i]])\r\ncontinue;\r\nif (hwc->state & PERF_HES_STOPPED)\r\nhwc->state |= PERF_HES_ARCH;\r\nuncore_pmu_event_stop(event, PERF_EF_UPDATE);\r\n}\r\nfor (i = 0; i < n; i++) {\r\nevent = box->event_list[i];\r\nhwc = &event->hw;\r\nif (hwc->idx != assign[i] ||\r\nhwc->last_tag != box->tags[assign[i]])\r\nuncore_assign_hw_event(box, event, assign[i]);\r\nelse if (i < box->n_events)\r\ncontinue;\r\nif (hwc->state & PERF_HES_ARCH)\r\ncontinue;\r\nuncore_pmu_event_start(event, 0);\r\n}\r\nbox->n_events = n;\r\nreturn 0;\r\n}\r\nstatic void uncore_pmu_event_del(struct perf_event *event, int flags)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nint i;\r\nuncore_pmu_event_stop(event, PERF_EF_UPDATE);\r\nfor (i = 0; i < box->n_events; i++) {\r\nif (event == box->event_list[i]) {\r\nuncore_put_event_constraint(box, event);\r\nwhile (++i < box->n_events)\r\nbox->event_list[i - 1] = box->event_list[i];\r\n--box->n_events;\r\nbreak;\r\n}\r\n}\r\nevent->hw.idx = -1;\r\nevent->hw.last_tag = ~0ULL;\r\n}\r\nvoid uncore_pmu_event_read(struct perf_event *event)\r\n{\r\nstruct intel_uncore_box *box = uncore_event_to_box(event);\r\nuncore_perf_event_update(box, event);\r\n}\r\nstatic int uncore_validate_group(struct intel_uncore_pmu *pmu,\r\nstruct perf_event *event)\r\n{\r\nstruct perf_event *leader = event->group_leader;\r\nstruct intel_uncore_box *fake_box;\r\nint ret = -EINVAL, n;\r\nfake_box = uncore_alloc_box(pmu->type, NUMA_NO_NODE);\r\nif (!fake_box)\r\nreturn -ENOMEM;\r\nfake_box->pmu = pmu;\r\nn = uncore_collect_events(fake_box, leader, true);\r\nif (n < 0)\r\ngoto out;\r\nfake_box->n_events = n;\r\nn = uncore_collect_events(fake_box, event, false);\r\nif (n < 0)\r\ngoto out;\r\nfake_box->n_events = n;\r\nret = uncore_assign_events(fake_box, NULL, n);\r\nout:\r\nkfree(fake_box);\r\nreturn ret;\r\n}\r\nstatic int uncore_pmu_event_init(struct perf_event *event)\r\n{\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nstruct hw_perf_event *hwc = &event->hw;\r\nint ret;\r\nif (event->attr.type != event->pmu->type)\r\nreturn -ENOENT;\r\npmu = uncore_event_to_pmu(event);\r\nif (pmu->func_id < 0)\r\nreturn -ENOENT;\r\nif (event->attr.exclude_user || event->attr.exclude_kernel ||\r\nevent->attr.exclude_hv || event->attr.exclude_idle)\r\nreturn -EINVAL;\r\nif (hwc->sample_period)\r\nreturn -EINVAL;\r\nif (event->cpu < 0)\r\nreturn -EINVAL;\r\nbox = uncore_pmu_to_box(pmu, event->cpu);\r\nif (!box || box->cpu < 0)\r\nreturn -EINVAL;\r\nevent->cpu = box->cpu;\r\nevent->hw.idx = -1;\r\nevent->hw.last_tag = ~0ULL;\r\nevent->hw.extra_reg.idx = EXTRA_REG_NONE;\r\nevent->hw.branch_reg.idx = EXTRA_REG_NONE;\r\nif (event->attr.config == UNCORE_FIXED_EVENT) {\r\nif (!pmu->type->fixed_ctl)\r\nreturn -EINVAL;\r\nif (pmu->type->single_fixed && pmu->pmu_idx > 0)\r\nreturn -EINVAL;\r\nhwc->config = 0ULL;\r\n} else {\r\nhwc->config = event->attr.config & pmu->type->event_mask;\r\nif (pmu->type->ops->hw_config) {\r\nret = pmu->type->ops->hw_config(box, event);\r\nif (ret)\r\nreturn ret;\r\n}\r\n}\r\nif (event->group_leader != event)\r\nret = uncore_validate_group(pmu, event);\r\nelse\r\nret = 0;\r\nreturn ret;\r\n}\r\nstatic ssize_t uncore_get_attr_cpumask(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nreturn cpumap_print_to_pagebuf(true, buf, &uncore_cpu_mask);\r\n}\r\nstatic int uncore_pmu_register(struct intel_uncore_pmu *pmu)\r\n{\r\nint ret;\r\nif (!pmu->type->pmu) {\r\npmu->pmu = (struct pmu) {\r\n.attr_groups = pmu->type->attr_groups,\r\n.task_ctx_nr = perf_invalid_context,\r\n.event_init = uncore_pmu_event_init,\r\n.add = uncore_pmu_event_add,\r\n.del = uncore_pmu_event_del,\r\n.start = uncore_pmu_event_start,\r\n.stop = uncore_pmu_event_stop,\r\n.read = uncore_pmu_event_read,\r\n};\r\n} else {\r\npmu->pmu = *pmu->type->pmu;\r\npmu->pmu.attr_groups = pmu->type->attr_groups;\r\n}\r\nif (pmu->type->num_boxes == 1) {\r\nif (strlen(pmu->type->name) > 0)\r\nsprintf(pmu->name, "uncore_%s", pmu->type->name);\r\nelse\r\nsprintf(pmu->name, "uncore");\r\n} else {\r\nsprintf(pmu->name, "uncore_%s_%d", pmu->type->name,\r\npmu->pmu_idx);\r\n}\r\nret = perf_pmu_register(&pmu->pmu, pmu->name, -1);\r\nreturn ret;\r\n}\r\nstatic void __init uncore_type_exit(struct intel_uncore_type *type)\r\n{\r\nint i;\r\nfor (i = 0; i < type->num_boxes; i++)\r\nfree_percpu(type->pmus[i].box);\r\nkfree(type->pmus);\r\ntype->pmus = NULL;\r\nkfree(type->events_group);\r\ntype->events_group = NULL;\r\n}\r\nstatic void __init uncore_types_exit(struct intel_uncore_type **types)\r\n{\r\nint i;\r\nfor (i = 0; types[i]; i++)\r\nuncore_type_exit(types[i]);\r\n}\r\nstatic int __init uncore_type_init(struct intel_uncore_type *type)\r\n{\r\nstruct intel_uncore_pmu *pmus;\r\nstruct attribute_group *attr_group;\r\nstruct attribute **attrs;\r\nint i, j;\r\npmus = kzalloc(sizeof(*pmus) * type->num_boxes, GFP_KERNEL);\r\nif (!pmus)\r\nreturn -ENOMEM;\r\ntype->pmus = pmus;\r\ntype->unconstrainted = (struct event_constraint)\r\n__EVENT_CONSTRAINT(0, (1ULL << type->num_counters) - 1,\r\n0, type->num_counters, 0, 0);\r\nfor (i = 0; i < type->num_boxes; i++) {\r\npmus[i].func_id = -1;\r\npmus[i].pmu_idx = i;\r\npmus[i].type = type;\r\nINIT_LIST_HEAD(&pmus[i].box_list);\r\npmus[i].box = alloc_percpu(struct intel_uncore_box *);\r\nif (!pmus[i].box)\r\ngoto fail;\r\n}\r\nif (type->event_descs) {\r\ni = 0;\r\nwhile (type->event_descs[i].attr.attr.name)\r\ni++;\r\nattr_group = kzalloc(sizeof(struct attribute *) * (i + 1) +\r\nsizeof(*attr_group), GFP_KERNEL);\r\nif (!attr_group)\r\ngoto fail;\r\nattrs = (struct attribute **)(attr_group + 1);\r\nattr_group->name = "events";\r\nattr_group->attrs = attrs;\r\nfor (j = 0; j < i; j++)\r\nattrs[j] = &type->event_descs[j].attr.attr;\r\ntype->events_group = attr_group;\r\n}\r\ntype->pmu_group = &uncore_pmu_attr_group;\r\nreturn 0;\r\nfail:\r\nuncore_type_exit(type);\r\nreturn -ENOMEM;\r\n}\r\nstatic int __init uncore_types_init(struct intel_uncore_type **types)\r\n{\r\nint i, ret;\r\nfor (i = 0; types[i]; i++) {\r\nret = uncore_type_init(types[i]);\r\nif (ret)\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nwhile (--i >= 0)\r\nuncore_type_exit(types[i]);\r\nreturn ret;\r\n}\r\nstatic int uncore_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\r\n{\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nstruct intel_uncore_type *type;\r\nint phys_id;\r\nbool first_box = false;\r\nphys_id = uncore_pcibus_to_physid[pdev->bus->number];\r\nif (phys_id < 0)\r\nreturn -ENODEV;\r\nif (UNCORE_PCI_DEV_TYPE(id->driver_data) == UNCORE_EXTRA_PCI_DEV) {\r\nint idx = UNCORE_PCI_DEV_IDX(id->driver_data);\r\nuncore_extra_pci_dev[phys_id][idx] = pdev;\r\npci_set_drvdata(pdev, NULL);\r\nreturn 0;\r\n}\r\ntype = uncore_pci_uncores[UNCORE_PCI_DEV_TYPE(id->driver_data)];\r\nbox = uncore_alloc_box(type, NUMA_NO_NODE);\r\nif (!box)\r\nreturn -ENOMEM;\r\npmu = &type->pmus[UNCORE_PCI_DEV_IDX(id->driver_data)];\r\nif (pmu->func_id < 0)\r\npmu->func_id = pdev->devfn;\r\nelse\r\nWARN_ON_ONCE(pmu->func_id != pdev->devfn);\r\nbox->phys_id = phys_id;\r\nbox->pci_dev = pdev;\r\nbox->pmu = pmu;\r\npci_set_drvdata(pdev, box);\r\nraw_spin_lock(&uncore_box_lock);\r\nif (list_empty(&pmu->box_list))\r\nfirst_box = true;\r\nlist_add_tail(&box->list, &pmu->box_list);\r\nraw_spin_unlock(&uncore_box_lock);\r\nif (first_box)\r\nuncore_pmu_register(pmu);\r\nreturn 0;\r\n}\r\nstatic void uncore_pci_remove(struct pci_dev *pdev)\r\n{\r\nstruct intel_uncore_box *box = pci_get_drvdata(pdev);\r\nstruct intel_uncore_pmu *pmu;\r\nint i, cpu, phys_id = uncore_pcibus_to_physid[pdev->bus->number];\r\nbool last_box = false;\r\nbox = pci_get_drvdata(pdev);\r\nif (!box) {\r\nfor (i = 0; i < UNCORE_EXTRA_PCI_DEV_MAX; i++) {\r\nif (uncore_extra_pci_dev[phys_id][i] == pdev) {\r\nuncore_extra_pci_dev[phys_id][i] = NULL;\r\nbreak;\r\n}\r\n}\r\nWARN_ON_ONCE(i >= UNCORE_EXTRA_PCI_DEV_MAX);\r\nreturn;\r\n}\r\npmu = box->pmu;\r\nif (WARN_ON_ONCE(phys_id != box->phys_id))\r\nreturn;\r\npci_set_drvdata(pdev, NULL);\r\nraw_spin_lock(&uncore_box_lock);\r\nlist_del(&box->list);\r\nif (list_empty(&pmu->box_list))\r\nlast_box = true;\r\nraw_spin_unlock(&uncore_box_lock);\r\nfor_each_possible_cpu(cpu) {\r\nif (*per_cpu_ptr(pmu->box, cpu) == box) {\r\n*per_cpu_ptr(pmu->box, cpu) = NULL;\r\natomic_dec(&box->refcnt);\r\n}\r\n}\r\nWARN_ON_ONCE(atomic_read(&box->refcnt) != 1);\r\nkfree(box);\r\nif (last_box)\r\nperf_pmu_unregister(&pmu->pmu);\r\n}\r\nstatic int __init uncore_pci_init(void)\r\n{\r\nint ret;\r\nswitch (boot_cpu_data.x86_model) {\r\ncase 45:\r\nret = snbep_uncore_pci_init();\r\nbreak;\r\ncase 62:\r\nret = ivbep_uncore_pci_init();\r\nbreak;\r\ncase 63:\r\nret = hswep_uncore_pci_init();\r\nbreak;\r\ncase 42:\r\nret = snb_uncore_pci_init();\r\nbreak;\r\ncase 58:\r\nret = ivb_uncore_pci_init();\r\nbreak;\r\ncase 60:\r\ncase 69:\r\nret = hsw_uncore_pci_init();\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\nif (ret)\r\nreturn ret;\r\nret = uncore_types_init(uncore_pci_uncores);\r\nif (ret)\r\nreturn ret;\r\nuncore_pci_driver->probe = uncore_pci_probe;\r\nuncore_pci_driver->remove = uncore_pci_remove;\r\nret = pci_register_driver(uncore_pci_driver);\r\nif (ret == 0)\r\npcidrv_registered = true;\r\nelse\r\nuncore_types_exit(uncore_pci_uncores);\r\nreturn ret;\r\n}\r\nstatic void __init uncore_pci_exit(void)\r\n{\r\nif (pcidrv_registered) {\r\npcidrv_registered = false;\r\npci_unregister_driver(uncore_pci_driver);\r\nuncore_types_exit(uncore_pci_uncores);\r\n}\r\n}\r\nstatic void uncore_kfree_boxes(void)\r\n{\r\nstruct intel_uncore_box *box;\r\nwhile (!list_empty(&boxes_to_free)) {\r\nbox = list_entry(boxes_to_free.next,\r\nstruct intel_uncore_box, list);\r\nlist_del(&box->list);\r\nkfree(box);\r\n}\r\n}\r\nstatic void uncore_cpu_dying(int cpu)\r\n{\r\nstruct intel_uncore_type *type;\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nint i, j;\r\nfor (i = 0; uncore_msr_uncores[i]; i++) {\r\ntype = uncore_msr_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nbox = *per_cpu_ptr(pmu->box, cpu);\r\n*per_cpu_ptr(pmu->box, cpu) = NULL;\r\nif (box && atomic_dec_and_test(&box->refcnt))\r\nlist_add(&box->list, &boxes_to_free);\r\n}\r\n}\r\n}\r\nstatic int uncore_cpu_starting(int cpu)\r\n{\r\nstruct intel_uncore_type *type;\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box, *exist;\r\nint i, j, k, phys_id;\r\nphys_id = topology_physical_package_id(cpu);\r\nfor (i = 0; uncore_msr_uncores[i]; i++) {\r\ntype = uncore_msr_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nbox = *per_cpu_ptr(pmu->box, cpu);\r\nif (box && box->phys_id >= 0)\r\ncontinue;\r\nfor_each_online_cpu(k) {\r\nexist = *per_cpu_ptr(pmu->box, k);\r\nif (exist && exist->phys_id == phys_id) {\r\natomic_inc(&exist->refcnt);\r\n*per_cpu_ptr(pmu->box, cpu) = exist;\r\nif (box) {\r\nlist_add(&box->list,\r\n&boxes_to_free);\r\nbox = NULL;\r\n}\r\nbreak;\r\n}\r\n}\r\nif (box)\r\nbox->phys_id = phys_id;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int uncore_cpu_prepare(int cpu, int phys_id)\r\n{\r\nstruct intel_uncore_type *type;\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nint i, j;\r\nfor (i = 0; uncore_msr_uncores[i]; i++) {\r\ntype = uncore_msr_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nif (pmu->func_id < 0)\r\npmu->func_id = j;\r\nbox = uncore_alloc_box(type, cpu_to_node(cpu));\r\nif (!box)\r\nreturn -ENOMEM;\r\nbox->pmu = pmu;\r\nbox->phys_id = phys_id;\r\n*per_cpu_ptr(pmu->box, cpu) = box;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\nuncore_change_context(struct intel_uncore_type **uncores, int old_cpu, int new_cpu)\r\n{\r\nstruct intel_uncore_type *type;\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_box *box;\r\nint i, j;\r\nfor (i = 0; uncores[i]; i++) {\r\ntype = uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nif (old_cpu < 0)\r\nbox = uncore_pmu_to_box(pmu, new_cpu);\r\nelse\r\nbox = uncore_pmu_to_box(pmu, old_cpu);\r\nif (!box)\r\ncontinue;\r\nif (old_cpu < 0) {\r\nWARN_ON_ONCE(box->cpu != -1);\r\nbox->cpu = new_cpu;\r\ncontinue;\r\n}\r\nWARN_ON_ONCE(box->cpu != old_cpu);\r\nif (new_cpu >= 0) {\r\nuncore_pmu_cancel_hrtimer(box);\r\nperf_pmu_migrate_context(&pmu->pmu,\r\nold_cpu, new_cpu);\r\nbox->cpu = new_cpu;\r\n} else {\r\nbox->cpu = -1;\r\n}\r\n}\r\n}\r\n}\r\nstatic void uncore_event_exit_cpu(int cpu)\r\n{\r\nint i, phys_id, target;\r\nif (!cpumask_test_and_clear_cpu(cpu, &uncore_cpu_mask))\r\nreturn;\r\nphys_id = topology_physical_package_id(cpu);\r\ntarget = -1;\r\nfor_each_online_cpu(i) {\r\nif (i == cpu)\r\ncontinue;\r\nif (phys_id == topology_physical_package_id(i)) {\r\ntarget = i;\r\nbreak;\r\n}\r\n}\r\nif (target >= 0)\r\ncpumask_set_cpu(target, &uncore_cpu_mask);\r\nuncore_change_context(uncore_msr_uncores, cpu, target);\r\nuncore_change_context(uncore_pci_uncores, cpu, target);\r\n}\r\nstatic void uncore_event_init_cpu(int cpu)\r\n{\r\nint i, phys_id;\r\nphys_id = topology_physical_package_id(cpu);\r\nfor_each_cpu(i, &uncore_cpu_mask) {\r\nif (phys_id == topology_physical_package_id(i))\r\nreturn;\r\n}\r\ncpumask_set_cpu(cpu, &uncore_cpu_mask);\r\nuncore_change_context(uncore_msr_uncores, -1, cpu);\r\nuncore_change_context(uncore_pci_uncores, -1, cpu);\r\n}\r\nstatic int uncore_cpu_notifier(struct notifier_block *self,\r\nunsigned long action, void *hcpu)\r\n{\r\nunsigned int cpu = (long)hcpu;\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_UP_PREPARE:\r\nuncore_cpu_prepare(cpu, -1);\r\nbreak;\r\ncase CPU_STARTING:\r\nuncore_cpu_starting(cpu);\r\nbreak;\r\ncase CPU_UP_CANCELED:\r\ncase CPU_DYING:\r\nuncore_cpu_dying(cpu);\r\nbreak;\r\ncase CPU_ONLINE:\r\ncase CPU_DEAD:\r\nuncore_kfree_boxes();\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nswitch (action & ~CPU_TASKS_FROZEN) {\r\ncase CPU_DOWN_FAILED:\r\ncase CPU_STARTING:\r\nuncore_event_init_cpu(cpu);\r\nbreak;\r\ncase CPU_DOWN_PREPARE:\r\nuncore_event_exit_cpu(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void __init uncore_cpu_setup(void *dummy)\r\n{\r\nuncore_cpu_starting(smp_processor_id());\r\n}\r\nstatic int __init uncore_cpu_init(void)\r\n{\r\nint ret;\r\nswitch (boot_cpu_data.x86_model) {\r\ncase 26:\r\ncase 30:\r\ncase 37:\r\ncase 44:\r\nnhm_uncore_cpu_init();\r\nbreak;\r\ncase 42:\r\ncase 58:\r\nsnb_uncore_cpu_init();\r\nbreak;\r\ncase 45:\r\nsnbep_uncore_cpu_init();\r\nbreak;\r\ncase 46:\r\ncase 47:\r\nnhmex_uncore_cpu_init();\r\nbreak;\r\ncase 62:\r\nivbep_uncore_cpu_init();\r\nbreak;\r\ncase 63:\r\nhswep_uncore_cpu_init();\r\nbreak;\r\ndefault:\r\nreturn 0;\r\n}\r\nret = uncore_types_init(uncore_msr_uncores);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nstatic int __init uncore_pmus_register(void)\r\n{\r\nstruct intel_uncore_pmu *pmu;\r\nstruct intel_uncore_type *type;\r\nint i, j;\r\nfor (i = 0; uncore_msr_uncores[i]; i++) {\r\ntype = uncore_msr_uncores[i];\r\nfor (j = 0; j < type->num_boxes; j++) {\r\npmu = &type->pmus[j];\r\nuncore_pmu_register(pmu);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void __init uncore_cpumask_init(void)\r\n{\r\nint cpu;\r\nif (!cpumask_empty(&uncore_cpu_mask))\r\nreturn;\r\ncpu_notifier_register_begin();\r\nfor_each_online_cpu(cpu) {\r\nint i, phys_id = topology_physical_package_id(cpu);\r\nfor_each_cpu(i, &uncore_cpu_mask) {\r\nif (phys_id == topology_physical_package_id(i)) {\r\nphys_id = -1;\r\nbreak;\r\n}\r\n}\r\nif (phys_id < 0)\r\ncontinue;\r\nuncore_cpu_prepare(cpu, phys_id);\r\nuncore_event_init_cpu(cpu);\r\n}\r\non_each_cpu(uncore_cpu_setup, NULL, 1);\r\n__register_cpu_notifier(&uncore_cpu_nb);\r\ncpu_notifier_register_done();\r\n}\r\nstatic int __init intel_uncore_init(void)\r\n{\r\nint ret;\r\nif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)\r\nreturn -ENODEV;\r\nif (cpu_has_hypervisor)\r\nreturn -ENODEV;\r\nret = uncore_pci_init();\r\nif (ret)\r\ngoto fail;\r\nret = uncore_cpu_init();\r\nif (ret) {\r\nuncore_pci_exit();\r\ngoto fail;\r\n}\r\nuncore_cpumask_init();\r\nuncore_pmus_register();\r\nreturn 0;\r\nfail:\r\nreturn ret;\r\n}
