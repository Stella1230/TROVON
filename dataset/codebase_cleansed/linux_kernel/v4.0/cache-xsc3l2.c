static inline int xsc3_l2_present(void)\r\n{\r\nunsigned long l2ctype;\r\n__asm__("mrc p15, 1, %0, c0, c0, 1" : "=r" (l2ctype));\r\nreturn !!(l2ctype & 0xf8);\r\n}\r\nstatic inline void xsc3_l2_clean_mva(unsigned long addr)\r\n{\r\n__asm__("mcr p15, 1, %0, c7, c11, 1" : : "r" (addr));\r\n}\r\nstatic inline void xsc3_l2_inv_mva(unsigned long addr)\r\n{\r\n__asm__("mcr p15, 1, %0, c7, c7, 1" : : "r" (addr));\r\n}\r\nstatic inline void xsc3_l2_inv_all(void)\r\n{\r\nunsigned long l2ctype, set_way;\r\nint set, way;\r\n__asm__("mrc p15, 1, %0, c0, c0, 1" : "=r" (l2ctype));\r\nfor (set = 0; set < CACHE_SET_SIZE(l2ctype); set++) {\r\nfor (way = 0; way < CACHE_WAY_PER_SET; way++) {\r\nset_way = (way << 29) | (set << 5);\r\n__asm__("mcr p15, 1, %0, c7, c11, 2" : : "r"(set_way));\r\n}\r\n}\r\ndsb();\r\n}\r\nstatic inline void l2_unmap_va(unsigned long va)\r\n{\r\n#ifdef CONFIG_HIGHMEM\r\nif (va != -1)\r\nkunmap_atomic((void *)va);\r\n#endif\r\n}\r\nstatic inline unsigned long l2_map_va(unsigned long pa, unsigned long prev_va)\r\n{\r\n#ifdef CONFIG_HIGHMEM\r\nunsigned long va = prev_va & PAGE_MASK;\r\nunsigned long pa_offset = pa << (32 - PAGE_SHIFT);\r\nif (unlikely(pa_offset < (prev_va << (32 - PAGE_SHIFT)))) {\r\nl2_unmap_va(prev_va);\r\nva = (unsigned long)kmap_atomic_pfn(pa >> PAGE_SHIFT);\r\n}\r\nreturn va + (pa_offset >> (32 - PAGE_SHIFT));\r\n#else\r\nreturn __phys_to_virt(pa);\r\n#endif\r\n}\r\nstatic void xsc3_l2_inv_range(unsigned long start, unsigned long end)\r\n{\r\nunsigned long vaddr;\r\nif (start == 0 && end == -1ul) {\r\nxsc3_l2_inv_all();\r\nreturn;\r\n}\r\nvaddr = -1;\r\nif (start & (CACHE_LINE_SIZE - 1)) {\r\nvaddr = l2_map_va(start & ~(CACHE_LINE_SIZE - 1), vaddr);\r\nxsc3_l2_clean_mva(vaddr);\r\nxsc3_l2_inv_mva(vaddr);\r\nstart = (start | (CACHE_LINE_SIZE - 1)) + 1;\r\n}\r\nwhile (start < (end & ~(CACHE_LINE_SIZE - 1))) {\r\nvaddr = l2_map_va(start, vaddr);\r\nxsc3_l2_inv_mva(vaddr);\r\nstart += CACHE_LINE_SIZE;\r\n}\r\nif (start < end) {\r\nvaddr = l2_map_va(start, vaddr);\r\nxsc3_l2_clean_mva(vaddr);\r\nxsc3_l2_inv_mva(vaddr);\r\n}\r\nl2_unmap_va(vaddr);\r\ndsb();\r\n}\r\nstatic void xsc3_l2_clean_range(unsigned long start, unsigned long end)\r\n{\r\nunsigned long vaddr;\r\nvaddr = -1;\r\nstart &= ~(CACHE_LINE_SIZE - 1);\r\nwhile (start < end) {\r\nvaddr = l2_map_va(start, vaddr);\r\nxsc3_l2_clean_mva(vaddr);\r\nstart += CACHE_LINE_SIZE;\r\n}\r\nl2_unmap_va(vaddr);\r\ndsb();\r\n}\r\nstatic inline void xsc3_l2_flush_all(void)\r\n{\r\nunsigned long l2ctype, set_way;\r\nint set, way;\r\n__asm__("mrc p15, 1, %0, c0, c0, 1" : "=r" (l2ctype));\r\nfor (set = 0; set < CACHE_SET_SIZE(l2ctype); set++) {\r\nfor (way = 0; way < CACHE_WAY_PER_SET; way++) {\r\nset_way = (way << 29) | (set << 5);\r\n__asm__("mcr p15, 1, %0, c7, c15, 2" : : "r"(set_way));\r\n}\r\n}\r\ndsb();\r\n}\r\nstatic void xsc3_l2_flush_range(unsigned long start, unsigned long end)\r\n{\r\nunsigned long vaddr;\r\nif (start == 0 && end == -1ul) {\r\nxsc3_l2_flush_all();\r\nreturn;\r\n}\r\nvaddr = -1;\r\nstart &= ~(CACHE_LINE_SIZE - 1);\r\nwhile (start < end) {\r\nvaddr = l2_map_va(start, vaddr);\r\nxsc3_l2_clean_mva(vaddr);\r\nxsc3_l2_inv_mva(vaddr);\r\nstart += CACHE_LINE_SIZE;\r\n}\r\nl2_unmap_va(vaddr);\r\ndsb();\r\n}\r\nstatic int __init xsc3_l2_init(void)\r\n{\r\nif (!cpu_is_xsc3() || !xsc3_l2_present())\r\nreturn 0;\r\nif (get_cr() & CR_L2) {\r\npr_info("XScale3 L2 cache enabled.\n");\r\nxsc3_l2_inv_all();\r\nouter_cache.inv_range = xsc3_l2_inv_range;\r\nouter_cache.clean_range = xsc3_l2_clean_range;\r\nouter_cache.flush_range = xsc3_l2_flush_range;\r\n}\r\nreturn 0;\r\n}
