static ssize_t\r\nqueue_var_show(unsigned long var, char *page)\r\n{\r\nreturn sprintf(page, "%lu\n", var);\r\n}\r\nstatic ssize_t\r\nqueue_var_store(unsigned long *var, const char *page, size_t count)\r\n{\r\nint err;\r\nunsigned long v;\r\nerr = kstrtoul(page, 10, &v);\r\nif (err || v > UINT_MAX)\r\nreturn -EINVAL;\r\n*var = v;\r\nreturn count;\r\n}\r\nstatic ssize_t queue_requests_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(q->nr_requests, (page));\r\n}\r\nstatic ssize_t\r\nqueue_requests_store(struct request_queue *q, const char *page, size_t count)\r\n{\r\nunsigned long nr;\r\nint ret, err;\r\nif (!q->request_fn && !q->mq_ops)\r\nreturn -EINVAL;\r\nret = queue_var_store(&nr, page, count);\r\nif (ret < 0)\r\nreturn ret;\r\nif (nr < BLKDEV_MIN_RQ)\r\nnr = BLKDEV_MIN_RQ;\r\nif (q->request_fn)\r\nerr = blk_update_nr_requests(q, nr);\r\nelse\r\nerr = blk_mq_update_nr_requests(q, nr);\r\nif (err)\r\nreturn err;\r\nreturn ret;\r\n}\r\nstatic ssize_t queue_ra_show(struct request_queue *q, char *page)\r\n{\r\nunsigned long ra_kb = q->backing_dev_info.ra_pages <<\r\n(PAGE_CACHE_SHIFT - 10);\r\nreturn queue_var_show(ra_kb, (page));\r\n}\r\nstatic ssize_t\r\nqueue_ra_store(struct request_queue *q, const char *page, size_t count)\r\n{\r\nunsigned long ra_kb;\r\nssize_t ret = queue_var_store(&ra_kb, page, count);\r\nif (ret < 0)\r\nreturn ret;\r\nq->backing_dev_info.ra_pages = ra_kb >> (PAGE_CACHE_SHIFT - 10);\r\nreturn ret;\r\n}\r\nstatic ssize_t queue_max_sectors_show(struct request_queue *q, char *page)\r\n{\r\nint max_sectors_kb = queue_max_sectors(q) >> 1;\r\nreturn queue_var_show(max_sectors_kb, (page));\r\n}\r\nstatic ssize_t queue_max_segments_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(queue_max_segments(q), (page));\r\n}\r\nstatic ssize_t queue_max_integrity_segments_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(q->limits.max_integrity_segments, (page));\r\n}\r\nstatic ssize_t queue_max_segment_size_show(struct request_queue *q, char *page)\r\n{\r\nif (blk_queue_cluster(q))\r\nreturn queue_var_show(queue_max_segment_size(q), (page));\r\nreturn queue_var_show(PAGE_CACHE_SIZE, (page));\r\n}\r\nstatic ssize_t queue_logical_block_size_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(queue_logical_block_size(q), page);\r\n}\r\nstatic ssize_t queue_physical_block_size_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(queue_physical_block_size(q), page);\r\n}\r\nstatic ssize_t queue_io_min_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(queue_io_min(q), page);\r\n}\r\nstatic ssize_t queue_io_opt_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(queue_io_opt(q), page);\r\n}\r\nstatic ssize_t queue_discard_granularity_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(q->limits.discard_granularity, page);\r\n}\r\nstatic ssize_t queue_discard_max_show(struct request_queue *q, char *page)\r\n{\r\nreturn sprintf(page, "%llu\n",\r\n(unsigned long long)q->limits.max_discard_sectors << 9);\r\n}\r\nstatic ssize_t queue_discard_zeroes_data_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show(queue_discard_zeroes_data(q), page);\r\n}\r\nstatic ssize_t queue_write_same_max_show(struct request_queue *q, char *page)\r\n{\r\nreturn sprintf(page, "%llu\n",\r\n(unsigned long long)q->limits.max_write_same_sectors << 9);\r\n}\r\nstatic ssize_t\r\nqueue_max_sectors_store(struct request_queue *q, const char *page, size_t count)\r\n{\r\nunsigned long max_sectors_kb,\r\nmax_hw_sectors_kb = queue_max_hw_sectors(q) >> 1,\r\npage_kb = 1 << (PAGE_CACHE_SHIFT - 10);\r\nssize_t ret = queue_var_store(&max_sectors_kb, page, count);\r\nif (ret < 0)\r\nreturn ret;\r\nif (max_sectors_kb > max_hw_sectors_kb || max_sectors_kb < page_kb)\r\nreturn -EINVAL;\r\nspin_lock_irq(q->queue_lock);\r\nq->limits.max_sectors = max_sectors_kb << 1;\r\nspin_unlock_irq(q->queue_lock);\r\nreturn ret;\r\n}\r\nstatic ssize_t queue_max_hw_sectors_show(struct request_queue *q, char *page)\r\n{\r\nint max_hw_sectors_kb = queue_max_hw_sectors(q) >> 1;\r\nreturn queue_var_show(max_hw_sectors_kb, (page));\r\n}\r\nstatic ssize_t queue_nomerges_show(struct request_queue *q, char *page)\r\n{\r\nreturn queue_var_show((blk_queue_nomerges(q) << 1) |\r\nblk_queue_noxmerges(q), page);\r\n}\r\nstatic ssize_t queue_nomerges_store(struct request_queue *q, const char *page,\r\nsize_t count)\r\n{\r\nunsigned long nm;\r\nssize_t ret = queue_var_store(&nm, page, count);\r\nif (ret < 0)\r\nreturn ret;\r\nspin_lock_irq(q->queue_lock);\r\nqueue_flag_clear(QUEUE_FLAG_NOMERGES, q);\r\nqueue_flag_clear(QUEUE_FLAG_NOXMERGES, q);\r\nif (nm == 2)\r\nqueue_flag_set(QUEUE_FLAG_NOMERGES, q);\r\nelse if (nm)\r\nqueue_flag_set(QUEUE_FLAG_NOXMERGES, q);\r\nspin_unlock_irq(q->queue_lock);\r\nreturn ret;\r\n}\r\nstatic ssize_t queue_rq_affinity_show(struct request_queue *q, char *page)\r\n{\r\nbool set = test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags);\r\nbool force = test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags);\r\nreturn queue_var_show(set << force, page);\r\n}\r\nstatic ssize_t\r\nqueue_rq_affinity_store(struct request_queue *q, const char *page, size_t count)\r\n{\r\nssize_t ret = -EINVAL;\r\n#ifdef CONFIG_SMP\r\nunsigned long val;\r\nret = queue_var_store(&val, page, count);\r\nif (ret < 0)\r\nreturn ret;\r\nspin_lock_irq(q->queue_lock);\r\nif (val == 2) {\r\nqueue_flag_set(QUEUE_FLAG_SAME_COMP, q);\r\nqueue_flag_set(QUEUE_FLAG_SAME_FORCE, q);\r\n} else if (val == 1) {\r\nqueue_flag_set(QUEUE_FLAG_SAME_COMP, q);\r\nqueue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);\r\n} else if (val == 0) {\r\nqueue_flag_clear(QUEUE_FLAG_SAME_COMP, q);\r\nqueue_flag_clear(QUEUE_FLAG_SAME_FORCE, q);\r\n}\r\nspin_unlock_irq(q->queue_lock);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic ssize_t\r\nqueue_attr_show(struct kobject *kobj, struct attribute *attr, char *page)\r\n{\r\nstruct queue_sysfs_entry *entry = to_queue(attr);\r\nstruct request_queue *q =\r\ncontainer_of(kobj, struct request_queue, kobj);\r\nssize_t res;\r\nif (!entry->show)\r\nreturn -EIO;\r\nmutex_lock(&q->sysfs_lock);\r\nif (blk_queue_dying(q)) {\r\nmutex_unlock(&q->sysfs_lock);\r\nreturn -ENOENT;\r\n}\r\nres = entry->show(q, page);\r\nmutex_unlock(&q->sysfs_lock);\r\nreturn res;\r\n}\r\nstatic ssize_t\r\nqueue_attr_store(struct kobject *kobj, struct attribute *attr,\r\nconst char *page, size_t length)\r\n{\r\nstruct queue_sysfs_entry *entry = to_queue(attr);\r\nstruct request_queue *q;\r\nssize_t res;\r\nif (!entry->store)\r\nreturn -EIO;\r\nq = container_of(kobj, struct request_queue, kobj);\r\nmutex_lock(&q->sysfs_lock);\r\nif (blk_queue_dying(q)) {\r\nmutex_unlock(&q->sysfs_lock);\r\nreturn -ENOENT;\r\n}\r\nres = entry->store(q, page, length);\r\nmutex_unlock(&q->sysfs_lock);\r\nreturn res;\r\n}\r\nstatic void blk_free_queue_rcu(struct rcu_head *rcu_head)\r\n{\r\nstruct request_queue *q = container_of(rcu_head, struct request_queue,\r\nrcu_head);\r\nkmem_cache_free(blk_requestq_cachep, q);\r\n}\r\nstatic void blk_release_queue(struct kobject *kobj)\r\n{\r\nstruct request_queue *q =\r\ncontainer_of(kobj, struct request_queue, kobj);\r\nblkcg_exit_queue(q);\r\nif (q->elevator) {\r\nspin_lock_irq(q->queue_lock);\r\nioc_clear_queue(q);\r\nspin_unlock_irq(q->queue_lock);\r\nelevator_exit(q->elevator);\r\n}\r\nblk_exit_rl(&q->root_rl);\r\nif (q->queue_tags)\r\n__blk_queue_free_tags(q);\r\nif (!q->mq_ops)\r\nblk_free_flush_queue(q->fq);\r\nelse\r\nblk_mq_release(q);\r\nblk_trace_shutdown(q);\r\nbdi_destroy(&q->backing_dev_info);\r\nida_simple_remove(&blk_queue_ida, q->id);\r\ncall_rcu(&q->rcu_head, blk_free_queue_rcu);\r\n}\r\nint blk_register_queue(struct gendisk *disk)\r\n{\r\nint ret;\r\nstruct device *dev = disk_to_dev(disk);\r\nstruct request_queue *q = disk->queue;\r\nif (WARN_ON(!q))\r\nreturn -ENXIO;\r\nif (!blk_queue_init_done(q)) {\r\nqueue_flag_set_unlocked(QUEUE_FLAG_INIT_DONE, q);\r\nblk_queue_bypass_end(q);\r\nif (q->mq_ops)\r\nblk_mq_finish_init(q);\r\n}\r\nret = blk_trace_init_sysfs(dev);\r\nif (ret)\r\nreturn ret;\r\nret = kobject_add(&q->kobj, kobject_get(&dev->kobj), "%s", "queue");\r\nif (ret < 0) {\r\nblk_trace_remove_sysfs(dev);\r\nreturn ret;\r\n}\r\nkobject_uevent(&q->kobj, KOBJ_ADD);\r\nif (q->mq_ops)\r\nblk_mq_register_disk(disk);\r\nif (!q->request_fn)\r\nreturn 0;\r\nret = elv_register_queue(q);\r\nif (ret) {\r\nkobject_uevent(&q->kobj, KOBJ_REMOVE);\r\nkobject_del(&q->kobj);\r\nblk_trace_remove_sysfs(dev);\r\nkobject_put(&dev->kobj);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nvoid blk_unregister_queue(struct gendisk *disk)\r\n{\r\nstruct request_queue *q = disk->queue;\r\nif (WARN_ON(!q))\r\nreturn;\r\nif (q->mq_ops)\r\nblk_mq_unregister_disk(disk);\r\nif (q->request_fn)\r\nelv_unregister_queue(q);\r\nkobject_uevent(&q->kobj, KOBJ_REMOVE);\r\nkobject_del(&q->kobj);\r\nblk_trace_remove_sysfs(disk_to_dev(disk));\r\nkobject_put(&disk_to_dev(disk)->kobj);\r\n}
