static inline void gfs2_setbit(const struct gfs2_rbm *rbm, bool do_clone,\r\nunsigned char new_state)\r\n{\r\nunsigned char *byte1, *byte2, *end, cur_state;\r\nstruct gfs2_bitmap *bi = rbm_bi(rbm);\r\nunsigned int buflen = bi->bi_len;\r\nconst unsigned int bit = (rbm->offset % GFS2_NBBY) * GFS2_BIT_SIZE;\r\nbyte1 = bi->bi_bh->b_data + bi->bi_offset + (rbm->offset / GFS2_NBBY);\r\nend = bi->bi_bh->b_data + bi->bi_offset + buflen;\r\nBUG_ON(byte1 >= end);\r\ncur_state = (*byte1 >> bit) & GFS2_BIT_MASK;\r\nif (unlikely(!valid_change[new_state * 4 + cur_state])) {\r\npr_warn("buf_blk = 0x%x old_state=%d, new_state=%d\n",\r\nrbm->offset, cur_state, new_state);\r\npr_warn("rgrp=0x%llx bi_start=0x%x\n",\r\n(unsigned long long)rbm->rgd->rd_addr, bi->bi_start);\r\npr_warn("bi_offset=0x%x bi_len=0x%x\n",\r\nbi->bi_offset, bi->bi_len);\r\ndump_stack();\r\ngfs2_consist_rgrpd(rbm->rgd);\r\nreturn;\r\n}\r\n*byte1 ^= (cur_state ^ new_state) << bit;\r\nif (do_clone && bi->bi_clone) {\r\nbyte2 = bi->bi_clone + bi->bi_offset + (rbm->offset / GFS2_NBBY);\r\ncur_state = (*byte2 >> bit) & GFS2_BIT_MASK;\r\n*byte2 ^= (cur_state ^ new_state) << bit;\r\n}\r\n}\r\nstatic inline u8 gfs2_testbit(const struct gfs2_rbm *rbm)\r\n{\r\nstruct gfs2_bitmap *bi = rbm_bi(rbm);\r\nconst u8 *buffer = bi->bi_bh->b_data + bi->bi_offset;\r\nconst u8 *byte;\r\nunsigned int bit;\r\nbyte = buffer + (rbm->offset / GFS2_NBBY);\r\nbit = (rbm->offset % GFS2_NBBY) * GFS2_BIT_SIZE;\r\nreturn (*byte >> bit) & GFS2_BIT_MASK;\r\n}\r\nstatic inline u64 gfs2_bit_search(const __le64 *ptr, u64 mask, u8 state)\r\n{\r\nu64 tmp;\r\nstatic const u64 search[] = {\r\n[0] = 0xffffffffffffffffULL,\r\n[1] = 0xaaaaaaaaaaaaaaaaULL,\r\n[2] = 0x5555555555555555ULL,\r\n[3] = 0x0000000000000000ULL,\r\n};\r\ntmp = le64_to_cpu(*ptr) ^ search[state];\r\ntmp &= (tmp >> 1);\r\ntmp &= mask;\r\nreturn tmp;\r\n}\r\nstatic inline int rs_cmp(u64 blk, u32 len, struct gfs2_blkreserv *rs)\r\n{\r\nu64 startblk = gfs2_rbm_to_block(&rs->rs_rbm);\r\nif (blk >= startblk + rs->rs_free)\r\nreturn 1;\r\nif (blk + len - 1 < startblk)\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic u32 gfs2_bitfit(const u8 *buf, const unsigned int len,\r\nu32 goal, u8 state)\r\n{\r\nu32 spoint = (goal << 1) & ((8*sizeof(u64)) - 1);\r\nconst __le64 *ptr = ((__le64 *)buf) + (goal >> 5);\r\nconst __le64 *end = (__le64 *)(buf + ALIGN(len, sizeof(u64)));\r\nu64 tmp;\r\nu64 mask = 0x5555555555555555ULL;\r\nu32 bit;\r\nmask <<= spoint;\r\ntmp = gfs2_bit_search(ptr, mask, state);\r\nptr++;\r\nwhile(tmp == 0 && ptr < end) {\r\ntmp = gfs2_bit_search(ptr, 0x5555555555555555ULL, state);\r\nptr++;\r\n}\r\nif (ptr == end && (len & (sizeof(u64) - 1)))\r\ntmp &= (((u64)~0) >> (64 - 8*(len & (sizeof(u64) - 1))));\r\nif (tmp == 0)\r\nreturn BFITNOENT;\r\nptr--;\r\nbit = __ffs64(tmp);\r\nbit /= 2;\r\nreturn (((const unsigned char *)ptr - buf) * GFS2_NBBY) + bit;\r\n}\r\nstatic int gfs2_rbm_from_block(struct gfs2_rbm *rbm, u64 block)\r\n{\r\nu64 rblock = block - rbm->rgd->rd_data0;\r\nif (WARN_ON_ONCE(rblock > UINT_MAX))\r\nreturn -EINVAL;\r\nif (block >= rbm->rgd->rd_data0 + rbm->rgd->rd_data)\r\nreturn -E2BIG;\r\nrbm->bii = 0;\r\nrbm->offset = (u32)(rblock);\r\nif (rbm->offset < rbm_bi(rbm)->bi_blocks)\r\nreturn 0;\r\nrbm->offset += (sizeof(struct gfs2_rgrp) -\r\nsizeof(struct gfs2_meta_header)) * GFS2_NBBY;\r\nrbm->bii = rbm->offset / rbm->rgd->rd_sbd->sd_blocks_per_bitmap;\r\nrbm->offset -= rbm->bii * rbm->rgd->rd_sbd->sd_blocks_per_bitmap;\r\nreturn 0;\r\n}\r\nstatic bool gfs2_rbm_incr(struct gfs2_rbm *rbm)\r\n{\r\nif (rbm->offset + 1 < rbm_bi(rbm)->bi_blocks) {\r\nrbm->offset++;\r\nreturn false;\r\n}\r\nif (rbm->bii == rbm->rgd->rd_length - 1)\r\nreturn true;\r\nrbm->offset = 0;\r\nrbm->bii++;\r\nreturn false;\r\n}\r\nstatic bool gfs2_unaligned_extlen(struct gfs2_rbm *rbm, u32 n_unaligned, u32 *len)\r\n{\r\nu32 n;\r\nu8 res;\r\nfor (n = 0; n < n_unaligned; n++) {\r\nres = gfs2_testbit(rbm);\r\nif (res != GFS2_BLKST_FREE)\r\nreturn true;\r\n(*len)--;\r\nif (*len == 0)\r\nreturn true;\r\nif (gfs2_rbm_incr(rbm))\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic u32 gfs2_free_extlen(const struct gfs2_rbm *rrbm, u32 len)\r\n{\r\nstruct gfs2_rbm rbm = *rrbm;\r\nu32 n_unaligned = rbm.offset & 3;\r\nu32 size = len;\r\nu32 bytes;\r\nu32 chunk_size;\r\nu8 *ptr, *start, *end;\r\nu64 block;\r\nstruct gfs2_bitmap *bi;\r\nif (n_unaligned &&\r\ngfs2_unaligned_extlen(&rbm, 4 - n_unaligned, &len))\r\ngoto out;\r\nn_unaligned = len & 3;\r\nwhile (len > 3) {\r\nbi = rbm_bi(&rbm);\r\nstart = bi->bi_bh->b_data;\r\nif (bi->bi_clone)\r\nstart = bi->bi_clone;\r\nend = start + bi->bi_bh->b_size;\r\nstart += bi->bi_offset;\r\nBUG_ON(rbm.offset & 3);\r\nstart += (rbm.offset / GFS2_NBBY);\r\nbytes = min_t(u32, len / GFS2_NBBY, (end - start));\r\nptr = memchr_inv(start, 0, bytes);\r\nchunk_size = ((ptr == NULL) ? bytes : (ptr - start));\r\nchunk_size *= GFS2_NBBY;\r\nBUG_ON(len < chunk_size);\r\nlen -= chunk_size;\r\nblock = gfs2_rbm_to_block(&rbm);\r\nif (gfs2_rbm_from_block(&rbm, block + chunk_size)) {\r\nn_unaligned = 0;\r\nbreak;\r\n}\r\nif (ptr) {\r\nn_unaligned = 3;\r\nbreak;\r\n}\r\nn_unaligned = len & 3;\r\n}\r\nif (n_unaligned)\r\ngfs2_unaligned_extlen(&rbm, n_unaligned, &len);\r\nout:\r\nreturn size - len;\r\n}\r\nstatic u32 gfs2_bitcount(struct gfs2_rgrpd *rgd, const u8 *buffer,\r\nunsigned int buflen, u8 state)\r\n{\r\nconst u8 *byte = buffer;\r\nconst u8 *end = buffer + buflen;\r\nconst u8 state1 = state << 2;\r\nconst u8 state2 = state << 4;\r\nconst u8 state3 = state << 6;\r\nu32 count = 0;\r\nfor (; byte < end; byte++) {\r\nif (((*byte) & 0x03) == state)\r\ncount++;\r\nif (((*byte) & 0x0C) == state1)\r\ncount++;\r\nif (((*byte) & 0x30) == state2)\r\ncount++;\r\nif (((*byte) & 0xC0) == state3)\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nvoid gfs2_rgrp_verify(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nstruct gfs2_bitmap *bi = NULL;\r\nu32 length = rgd->rd_length;\r\nu32 count[4], tmp;\r\nint buf, x;\r\nmemset(count, 0, 4 * sizeof(u32));\r\nfor (buf = 0; buf < length; buf++) {\r\nbi = rgd->rd_bits + buf;\r\nfor (x = 0; x < 4; x++)\r\ncount[x] += gfs2_bitcount(rgd,\r\nbi->bi_bh->b_data +\r\nbi->bi_offset,\r\nbi->bi_len, x);\r\n}\r\nif (count[0] != rgd->rd_free) {\r\nif (gfs2_consist_rgrpd(rgd))\r\nfs_err(sdp, "free data mismatch: %u != %u\n",\r\ncount[0], rgd->rd_free);\r\nreturn;\r\n}\r\ntmp = rgd->rd_data - rgd->rd_free - rgd->rd_dinodes;\r\nif (count[1] != tmp) {\r\nif (gfs2_consist_rgrpd(rgd))\r\nfs_err(sdp, "used data mismatch: %u != %u\n",\r\ncount[1], tmp);\r\nreturn;\r\n}\r\nif (count[2] + count[3] != rgd->rd_dinodes) {\r\nif (gfs2_consist_rgrpd(rgd))\r\nfs_err(sdp, "used metadata mismatch: %u != %u\n",\r\ncount[2] + count[3], rgd->rd_dinodes);\r\nreturn;\r\n}\r\n}\r\nstatic inline int rgrp_contains_block(struct gfs2_rgrpd *rgd, u64 block)\r\n{\r\nu64 first = rgd->rd_data0;\r\nu64 last = first + rgd->rd_data;\r\nreturn first <= block && block < last;\r\n}\r\nstruct gfs2_rgrpd *gfs2_blk2rgrpd(struct gfs2_sbd *sdp, u64 blk, bool exact)\r\n{\r\nstruct rb_node *n, *next;\r\nstruct gfs2_rgrpd *cur;\r\nspin_lock(&sdp->sd_rindex_spin);\r\nn = sdp->sd_rindex_tree.rb_node;\r\nwhile (n) {\r\ncur = rb_entry(n, struct gfs2_rgrpd, rd_node);\r\nnext = NULL;\r\nif (blk < cur->rd_addr)\r\nnext = n->rb_left;\r\nelse if (blk >= cur->rd_data0 + cur->rd_data)\r\nnext = n->rb_right;\r\nif (next == NULL) {\r\nspin_unlock(&sdp->sd_rindex_spin);\r\nif (exact) {\r\nif (blk < cur->rd_addr)\r\nreturn NULL;\r\nif (blk >= cur->rd_data0 + cur->rd_data)\r\nreturn NULL;\r\n}\r\nreturn cur;\r\n}\r\nn = next;\r\n}\r\nspin_unlock(&sdp->sd_rindex_spin);\r\nreturn NULL;\r\n}\r\nstruct gfs2_rgrpd *gfs2_rgrpd_get_first(struct gfs2_sbd *sdp)\r\n{\r\nconst struct rb_node *n;\r\nstruct gfs2_rgrpd *rgd;\r\nspin_lock(&sdp->sd_rindex_spin);\r\nn = rb_first(&sdp->sd_rindex_tree);\r\nrgd = rb_entry(n, struct gfs2_rgrpd, rd_node);\r\nspin_unlock(&sdp->sd_rindex_spin);\r\nreturn rgd;\r\n}\r\nstruct gfs2_rgrpd *gfs2_rgrpd_get_next(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nconst struct rb_node *n;\r\nspin_lock(&sdp->sd_rindex_spin);\r\nn = rb_next(&rgd->rd_node);\r\nif (n == NULL)\r\nn = rb_first(&sdp->sd_rindex_tree);\r\nif (unlikely(&rgd->rd_node == n)) {\r\nspin_unlock(&sdp->sd_rindex_spin);\r\nreturn NULL;\r\n}\r\nrgd = rb_entry(n, struct gfs2_rgrpd, rd_node);\r\nspin_unlock(&sdp->sd_rindex_spin);\r\nreturn rgd;\r\n}\r\nvoid check_and_update_goal(struct gfs2_inode *ip)\r\n{\r\nstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\nif (!ip->i_goal || gfs2_blk2rgrpd(sdp, ip->i_goal, 1) == NULL)\r\nip->i_goal = ip->i_no_addr;\r\n}\r\nvoid gfs2_free_clones(struct gfs2_rgrpd *rgd)\r\n{\r\nint x;\r\nfor (x = 0; x < rgd->rd_length; x++) {\r\nstruct gfs2_bitmap *bi = rgd->rd_bits + x;\r\nkfree(bi->bi_clone);\r\nbi->bi_clone = NULL;\r\n}\r\n}\r\nint gfs2_rs_alloc(struct gfs2_inode *ip)\r\n{\r\nint error = 0;\r\ndown_write(&ip->i_rw_mutex);\r\nif (ip->i_res)\r\ngoto out;\r\nip->i_res = kmem_cache_zalloc(gfs2_rsrv_cachep, GFP_NOFS);\r\nif (!ip->i_res) {\r\nerror = -ENOMEM;\r\ngoto out;\r\n}\r\nRB_CLEAR_NODE(&ip->i_res->rs_node);\r\nout:\r\nup_write(&ip->i_rw_mutex);\r\nreturn error;\r\n}\r\nstatic void dump_rs(struct seq_file *seq, const struct gfs2_blkreserv *rs)\r\n{\r\ngfs2_print_dbg(seq, " B: n:%llu s:%llu b:%u f:%u\n",\r\n(unsigned long long)rs->rs_inum,\r\n(unsigned long long)gfs2_rbm_to_block(&rs->rs_rbm),\r\nrs->rs_rbm.offset, rs->rs_free);\r\n}\r\nstatic void __rs_deltree(struct gfs2_blkreserv *rs)\r\n{\r\nstruct gfs2_rgrpd *rgd;\r\nif (!gfs2_rs_active(rs))\r\nreturn;\r\nrgd = rs->rs_rbm.rgd;\r\ntrace_gfs2_rs(rs, TRACE_RS_TREEDEL);\r\nrb_erase(&rs->rs_node, &rgd->rd_rstree);\r\nRB_CLEAR_NODE(&rs->rs_node);\r\nif (rs->rs_free) {\r\nstruct gfs2_bitmap *bi = rbm_bi(&rs->rs_rbm);\r\nBUG_ON(rs->rs_rbm.rgd->rd_reserved < rs->rs_free);\r\nrs->rs_rbm.rgd->rd_reserved -= rs->rs_free;\r\nrgd->rd_extfail_pt += rs->rs_free;\r\nrs->rs_free = 0;\r\nclear_bit(GBF_FULL, &bi->bi_flags);\r\n}\r\n}\r\nvoid gfs2_rs_deltree(struct gfs2_blkreserv *rs)\r\n{\r\nstruct gfs2_rgrpd *rgd;\r\nrgd = rs->rs_rbm.rgd;\r\nif (rgd) {\r\nspin_lock(&rgd->rd_rsspin);\r\n__rs_deltree(rs);\r\nspin_unlock(&rgd->rd_rsspin);\r\n}\r\n}\r\nvoid gfs2_rs_delete(struct gfs2_inode *ip, atomic_t *wcount)\r\n{\r\ndown_write(&ip->i_rw_mutex);\r\nif (ip->i_res && ((wcount == NULL) || (atomic_read(wcount) <= 1))) {\r\ngfs2_rs_deltree(ip->i_res);\r\nBUG_ON(ip->i_res->rs_free);\r\nkmem_cache_free(gfs2_rsrv_cachep, ip->i_res);\r\nip->i_res = NULL;\r\n}\r\nup_write(&ip->i_rw_mutex);\r\n}\r\nstatic void return_all_reservations(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct rb_node *n;\r\nstruct gfs2_blkreserv *rs;\r\nspin_lock(&rgd->rd_rsspin);\r\nwhile ((n = rb_first(&rgd->rd_rstree))) {\r\nrs = rb_entry(n, struct gfs2_blkreserv, rs_node);\r\n__rs_deltree(rs);\r\n}\r\nspin_unlock(&rgd->rd_rsspin);\r\n}\r\nvoid gfs2_clear_rgrpd(struct gfs2_sbd *sdp)\r\n{\r\nstruct rb_node *n;\r\nstruct gfs2_rgrpd *rgd;\r\nstruct gfs2_glock *gl;\r\nwhile ((n = rb_first(&sdp->sd_rindex_tree))) {\r\nrgd = rb_entry(n, struct gfs2_rgrpd, rd_node);\r\ngl = rgd->rd_gl;\r\nrb_erase(n, &sdp->sd_rindex_tree);\r\nif (gl) {\r\nspin_lock(&gl->gl_spin);\r\ngl->gl_object = NULL;\r\nspin_unlock(&gl->gl_spin);\r\ngfs2_glock_add_to_lru(gl);\r\ngfs2_glock_put(gl);\r\n}\r\ngfs2_free_clones(rgd);\r\nkfree(rgd->rd_bits);\r\nreturn_all_reservations(rgd);\r\nkmem_cache_free(gfs2_rgrpd_cachep, rgd);\r\n}\r\n}\r\nstatic void gfs2_rindex_print(const struct gfs2_rgrpd *rgd)\r\n{\r\npr_info("ri_addr = %llu\n", (unsigned long long)rgd->rd_addr);\r\npr_info("ri_length = %u\n", rgd->rd_length);\r\npr_info("ri_data0 = %llu\n", (unsigned long long)rgd->rd_data0);\r\npr_info("ri_data = %u\n", rgd->rd_data);\r\npr_info("ri_bitbytes = %u\n", rgd->rd_bitbytes);\r\n}\r\nstatic int compute_bitstructs(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nstruct gfs2_bitmap *bi;\r\nu32 length = rgd->rd_length;\r\nu32 bytes_left, bytes;\r\nint x;\r\nif (!length)\r\nreturn -EINVAL;\r\nrgd->rd_bits = kcalloc(length, sizeof(struct gfs2_bitmap), GFP_NOFS);\r\nif (!rgd->rd_bits)\r\nreturn -ENOMEM;\r\nbytes_left = rgd->rd_bitbytes;\r\nfor (x = 0; x < length; x++) {\r\nbi = rgd->rd_bits + x;\r\nbi->bi_flags = 0;\r\nif (length == 1) {\r\nbytes = bytes_left;\r\nbi->bi_offset = sizeof(struct gfs2_rgrp);\r\nbi->bi_start = 0;\r\nbi->bi_len = bytes;\r\nbi->bi_blocks = bytes * GFS2_NBBY;\r\n} else if (x == 0) {\r\nbytes = sdp->sd_sb.sb_bsize - sizeof(struct gfs2_rgrp);\r\nbi->bi_offset = sizeof(struct gfs2_rgrp);\r\nbi->bi_start = 0;\r\nbi->bi_len = bytes;\r\nbi->bi_blocks = bytes * GFS2_NBBY;\r\n} else if (x + 1 == length) {\r\nbytes = bytes_left;\r\nbi->bi_offset = sizeof(struct gfs2_meta_header);\r\nbi->bi_start = rgd->rd_bitbytes - bytes_left;\r\nbi->bi_len = bytes;\r\nbi->bi_blocks = bytes * GFS2_NBBY;\r\n} else {\r\nbytes = sdp->sd_sb.sb_bsize -\r\nsizeof(struct gfs2_meta_header);\r\nbi->bi_offset = sizeof(struct gfs2_meta_header);\r\nbi->bi_start = rgd->rd_bitbytes - bytes_left;\r\nbi->bi_len = bytes;\r\nbi->bi_blocks = bytes * GFS2_NBBY;\r\n}\r\nbytes_left -= bytes;\r\n}\r\nif (bytes_left) {\r\ngfs2_consist_rgrpd(rgd);\r\nreturn -EIO;\r\n}\r\nbi = rgd->rd_bits + (length - 1);\r\nif ((bi->bi_start + bi->bi_len) * GFS2_NBBY != rgd->rd_data) {\r\nif (gfs2_consist_rgrpd(rgd)) {\r\ngfs2_rindex_print(rgd);\r\nfs_err(sdp, "start=%u len=%u offset=%u\n",\r\nbi->bi_start, bi->bi_len, bi->bi_offset);\r\n}\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nu64 gfs2_ri_total(struct gfs2_sbd *sdp)\r\n{\r\nu64 total_data = 0;\r\nstruct inode *inode = sdp->sd_rindex;\r\nstruct gfs2_inode *ip = GFS2_I(inode);\r\nchar buf[sizeof(struct gfs2_rindex)];\r\nint error, rgrps;\r\nfor (rgrps = 0;; rgrps++) {\r\nloff_t pos = rgrps * sizeof(struct gfs2_rindex);\r\nif (pos + sizeof(struct gfs2_rindex) > i_size_read(inode))\r\nbreak;\r\nerror = gfs2_internal_read(ip, buf, &pos,\r\nsizeof(struct gfs2_rindex));\r\nif (error != sizeof(struct gfs2_rindex))\r\nbreak;\r\ntotal_data += be32_to_cpu(((struct gfs2_rindex *)buf)->ri_data);\r\n}\r\nreturn total_data;\r\n}\r\nstatic int rgd_insert(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nstruct rb_node **newn = &sdp->sd_rindex_tree.rb_node, *parent = NULL;\r\nwhile (*newn) {\r\nstruct gfs2_rgrpd *cur = rb_entry(*newn, struct gfs2_rgrpd,\r\nrd_node);\r\nparent = *newn;\r\nif (rgd->rd_addr < cur->rd_addr)\r\nnewn = &((*newn)->rb_left);\r\nelse if (rgd->rd_addr > cur->rd_addr)\r\nnewn = &((*newn)->rb_right);\r\nelse\r\nreturn -EEXIST;\r\n}\r\nrb_link_node(&rgd->rd_node, parent, newn);\r\nrb_insert_color(&rgd->rd_node, &sdp->sd_rindex_tree);\r\nsdp->sd_rgrps++;\r\nreturn 0;\r\n}\r\nstatic int read_rindex_entry(struct gfs2_inode *ip)\r\n{\r\nstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\nconst unsigned bsize = sdp->sd_sb.sb_bsize;\r\nloff_t pos = sdp->sd_rgrps * sizeof(struct gfs2_rindex);\r\nstruct gfs2_rindex buf;\r\nint error;\r\nstruct gfs2_rgrpd *rgd;\r\nif (pos >= i_size_read(&ip->i_inode))\r\nreturn 1;\r\nerror = gfs2_internal_read(ip, (char *)&buf, &pos,\r\nsizeof(struct gfs2_rindex));\r\nif (error != sizeof(struct gfs2_rindex))\r\nreturn (error == 0) ? 1 : error;\r\nrgd = kmem_cache_zalloc(gfs2_rgrpd_cachep, GFP_NOFS);\r\nerror = -ENOMEM;\r\nif (!rgd)\r\nreturn error;\r\nrgd->rd_sbd = sdp;\r\nrgd->rd_addr = be64_to_cpu(buf.ri_addr);\r\nrgd->rd_length = be32_to_cpu(buf.ri_length);\r\nrgd->rd_data0 = be64_to_cpu(buf.ri_data0);\r\nrgd->rd_data = be32_to_cpu(buf.ri_data);\r\nrgd->rd_bitbytes = be32_to_cpu(buf.ri_bitbytes);\r\nspin_lock_init(&rgd->rd_rsspin);\r\nerror = compute_bitstructs(rgd);\r\nif (error)\r\ngoto fail;\r\nerror = gfs2_glock_get(sdp, rgd->rd_addr,\r\n&gfs2_rgrp_glops, CREATE, &rgd->rd_gl);\r\nif (error)\r\ngoto fail;\r\nrgd->rd_gl->gl_object = rgd;\r\nrgd->rd_gl->gl_vm.start = rgd->rd_addr * bsize;\r\nrgd->rd_gl->gl_vm.end = rgd->rd_gl->gl_vm.start + (rgd->rd_length * bsize) - 1;\r\nrgd->rd_rgl = (struct gfs2_rgrp_lvb *)rgd->rd_gl->gl_lksb.sb_lvbptr;\r\nrgd->rd_flags &= ~(GFS2_RDF_UPTODATE | GFS2_RDF_PREFERRED);\r\nif (rgd->rd_data > sdp->sd_max_rg_data)\r\nsdp->sd_max_rg_data = rgd->rd_data;\r\nspin_lock(&sdp->sd_rindex_spin);\r\nerror = rgd_insert(rgd);\r\nspin_unlock(&sdp->sd_rindex_spin);\r\nif (!error)\r\nreturn 0;\r\nerror = 0;\r\ngfs2_glock_put(rgd->rd_gl);\r\nfail:\r\nkfree(rgd->rd_bits);\r\nkmem_cache_free(gfs2_rgrpd_cachep, rgd);\r\nreturn error;\r\n}\r\nstatic void set_rgrp_preferences(struct gfs2_sbd *sdp)\r\n{\r\nstruct gfs2_rgrpd *rgd, *first;\r\nint i;\r\nrgd = gfs2_rgrpd_get_first(sdp);\r\nfor (i = 0; i < sdp->sd_lockstruct.ls_jid; i++)\r\nrgd = gfs2_rgrpd_get_next(rgd);\r\nfirst = rgd;\r\ndo {\r\nrgd->rd_flags |= GFS2_RDF_PREFERRED;\r\nfor (i = 0; i < sdp->sd_journals; i++) {\r\nrgd = gfs2_rgrpd_get_next(rgd);\r\nif (rgd == first)\r\nbreak;\r\n}\r\n} while (rgd != first);\r\n}\r\nstatic int gfs2_ri_update(struct gfs2_inode *ip)\r\n{\r\nstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\nint error;\r\ndo {\r\nerror = read_rindex_entry(ip);\r\n} while (error == 0);\r\nif (error < 0)\r\nreturn error;\r\nset_rgrp_preferences(sdp);\r\nsdp->sd_rindex_uptodate = 1;\r\nreturn 0;\r\n}\r\nint gfs2_rindex_update(struct gfs2_sbd *sdp)\r\n{\r\nstruct gfs2_inode *ip = GFS2_I(sdp->sd_rindex);\r\nstruct gfs2_glock *gl = ip->i_gl;\r\nstruct gfs2_holder ri_gh;\r\nint error = 0;\r\nint unlock_required = 0;\r\nif (!sdp->sd_rindex_uptodate) {\r\nif (!gfs2_glock_is_locked_by_me(gl)) {\r\nerror = gfs2_glock_nq_init(gl, LM_ST_SHARED, 0, &ri_gh);\r\nif (error)\r\nreturn error;\r\nunlock_required = 1;\r\n}\r\nif (!sdp->sd_rindex_uptodate)\r\nerror = gfs2_ri_update(ip);\r\nif (unlock_required)\r\ngfs2_glock_dq_uninit(&ri_gh);\r\n}\r\nreturn error;\r\n}\r\nstatic void gfs2_rgrp_in(struct gfs2_rgrpd *rgd, const void *buf)\r\n{\r\nconst struct gfs2_rgrp *str = buf;\r\nu32 rg_flags;\r\nrg_flags = be32_to_cpu(str->rg_flags);\r\nrg_flags &= ~GFS2_RDF_MASK;\r\nrgd->rd_flags &= GFS2_RDF_MASK;\r\nrgd->rd_flags |= rg_flags;\r\nrgd->rd_free = be32_to_cpu(str->rg_free);\r\nrgd->rd_dinodes = be32_to_cpu(str->rg_dinodes);\r\nrgd->rd_igeneration = be64_to_cpu(str->rg_igeneration);\r\n}\r\nstatic void gfs2_rgrp_out(struct gfs2_rgrpd *rgd, void *buf)\r\n{\r\nstruct gfs2_rgrp *str = buf;\r\nstr->rg_flags = cpu_to_be32(rgd->rd_flags & ~GFS2_RDF_MASK);\r\nstr->rg_free = cpu_to_be32(rgd->rd_free);\r\nstr->rg_dinodes = cpu_to_be32(rgd->rd_dinodes);\r\nstr->__pad = cpu_to_be32(0);\r\nstr->rg_igeneration = cpu_to_be64(rgd->rd_igeneration);\r\nmemset(&str->rg_reserved, 0, sizeof(str->rg_reserved));\r\n}\r\nstatic int gfs2_rgrp_lvb_valid(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_rgrp_lvb *rgl = rgd->rd_rgl;\r\nstruct gfs2_rgrp *str = (struct gfs2_rgrp *)rgd->rd_bits[0].bi_bh->b_data;\r\nif (rgl->rl_flags != str->rg_flags || rgl->rl_free != str->rg_free ||\r\nrgl->rl_dinodes != str->rg_dinodes ||\r\nrgl->rl_igeneration != str->rg_igeneration)\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic void gfs2_rgrp_ondisk2lvb(struct gfs2_rgrp_lvb *rgl, const void *buf)\r\n{\r\nconst struct gfs2_rgrp *str = buf;\r\nrgl->rl_magic = cpu_to_be32(GFS2_MAGIC);\r\nrgl->rl_flags = str->rg_flags;\r\nrgl->rl_free = str->rg_free;\r\nrgl->rl_dinodes = str->rg_dinodes;\r\nrgl->rl_igeneration = str->rg_igeneration;\r\nrgl->__pad = 0UL;\r\n}\r\nstatic void update_rgrp_lvb_unlinked(struct gfs2_rgrpd *rgd, u32 change)\r\n{\r\nstruct gfs2_rgrp_lvb *rgl = rgd->rd_rgl;\r\nu32 unlinked = be32_to_cpu(rgl->rl_unlinked) + change;\r\nrgl->rl_unlinked = cpu_to_be32(unlinked);\r\n}\r\nstatic u32 count_unlinked(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_bitmap *bi;\r\nconst u32 length = rgd->rd_length;\r\nconst u8 *buffer = NULL;\r\nu32 i, goal, count = 0;\r\nfor (i = 0, bi = rgd->rd_bits; i < length; i++, bi++) {\r\ngoal = 0;\r\nbuffer = bi->bi_bh->b_data + bi->bi_offset;\r\nWARN_ON(!buffer_uptodate(bi->bi_bh));\r\nwhile (goal < bi->bi_len * GFS2_NBBY) {\r\ngoal = gfs2_bitfit(buffer, bi->bi_len, goal,\r\nGFS2_BLKST_UNLINKED);\r\nif (goal == BFITNOENT)\r\nbreak;\r\ncount++;\r\ngoal++;\r\n}\r\n}\r\nreturn count;\r\n}\r\nstatic int gfs2_rgrp_bh_get(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nstruct gfs2_glock *gl = rgd->rd_gl;\r\nunsigned int length = rgd->rd_length;\r\nstruct gfs2_bitmap *bi;\r\nunsigned int x, y;\r\nint error;\r\nif (rgd->rd_bits[0].bi_bh != NULL)\r\nreturn 0;\r\nfor (x = 0; x < length; x++) {\r\nbi = rgd->rd_bits + x;\r\nerror = gfs2_meta_read(gl, rgd->rd_addr + x, 0, &bi->bi_bh);\r\nif (error)\r\ngoto fail;\r\n}\r\nfor (y = length; y--;) {\r\nbi = rgd->rd_bits + y;\r\nerror = gfs2_meta_wait(sdp, bi->bi_bh);\r\nif (error)\r\ngoto fail;\r\nif (gfs2_metatype_check(sdp, bi->bi_bh, y ? GFS2_METATYPE_RB :\r\nGFS2_METATYPE_RG)) {\r\nerror = -EIO;\r\ngoto fail;\r\n}\r\n}\r\nif (!(rgd->rd_flags & GFS2_RDF_UPTODATE)) {\r\nfor (x = 0; x < length; x++)\r\nclear_bit(GBF_FULL, &rgd->rd_bits[x].bi_flags);\r\ngfs2_rgrp_in(rgd, (rgd->rd_bits[0].bi_bh)->b_data);\r\nrgd->rd_flags |= (GFS2_RDF_UPTODATE | GFS2_RDF_CHECK);\r\nrgd->rd_free_clone = rgd->rd_free;\r\nrgd->rd_extfail_pt = rgd->rd_free;\r\n}\r\nif (cpu_to_be32(GFS2_MAGIC) != rgd->rd_rgl->rl_magic) {\r\nrgd->rd_rgl->rl_unlinked = cpu_to_be32(count_unlinked(rgd));\r\ngfs2_rgrp_ondisk2lvb(rgd->rd_rgl,\r\nrgd->rd_bits[0].bi_bh->b_data);\r\n}\r\nelse if (sdp->sd_args.ar_rgrplvb) {\r\nif (!gfs2_rgrp_lvb_valid(rgd)){\r\ngfs2_consist_rgrpd(rgd);\r\nerror = -EIO;\r\ngoto fail;\r\n}\r\nif (rgd->rd_rgl->rl_unlinked == 0)\r\nrgd->rd_flags &= ~GFS2_RDF_CHECK;\r\n}\r\nreturn 0;\r\nfail:\r\nwhile (x--) {\r\nbi = rgd->rd_bits + x;\r\nbrelse(bi->bi_bh);\r\nbi->bi_bh = NULL;\r\ngfs2_assert_warn(sdp, !bi->bi_clone);\r\n}\r\nreturn error;\r\n}\r\nstatic int update_rgrp_lvb(struct gfs2_rgrpd *rgd)\r\n{\r\nu32 rl_flags;\r\nif (rgd->rd_flags & GFS2_RDF_UPTODATE)\r\nreturn 0;\r\nif (cpu_to_be32(GFS2_MAGIC) != rgd->rd_rgl->rl_magic)\r\nreturn gfs2_rgrp_bh_get(rgd);\r\nrl_flags = be32_to_cpu(rgd->rd_rgl->rl_flags);\r\nrl_flags &= ~GFS2_RDF_MASK;\r\nrgd->rd_flags &= GFS2_RDF_MASK;\r\nrgd->rd_flags |= (rl_flags | GFS2_RDF_UPTODATE | GFS2_RDF_CHECK);\r\nif (rgd->rd_rgl->rl_unlinked == 0)\r\nrgd->rd_flags &= ~GFS2_RDF_CHECK;\r\nrgd->rd_free = be32_to_cpu(rgd->rd_rgl->rl_free);\r\nrgd->rd_free_clone = rgd->rd_free;\r\nrgd->rd_dinodes = be32_to_cpu(rgd->rd_rgl->rl_dinodes);\r\nrgd->rd_igeneration = be64_to_cpu(rgd->rd_rgl->rl_igeneration);\r\nreturn 0;\r\n}\r\nint gfs2_rgrp_go_lock(struct gfs2_holder *gh)\r\n{\r\nstruct gfs2_rgrpd *rgd = gh->gh_gl->gl_object;\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nif (gh->gh_flags & GL_SKIP && sdp->sd_args.ar_rgrplvb)\r\nreturn 0;\r\nreturn gfs2_rgrp_bh_get(rgd);\r\n}\r\nvoid gfs2_rgrp_go_unlock(struct gfs2_holder *gh)\r\n{\r\nstruct gfs2_rgrpd *rgd = gh->gh_gl->gl_object;\r\nint x, length = rgd->rd_length;\r\nfor (x = 0; x < length; x++) {\r\nstruct gfs2_bitmap *bi = rgd->rd_bits + x;\r\nif (bi->bi_bh) {\r\nbrelse(bi->bi_bh);\r\nbi->bi_bh = NULL;\r\n}\r\n}\r\n}\r\nint gfs2_rgrp_send_discards(struct gfs2_sbd *sdp, u64 offset,\r\nstruct buffer_head *bh,\r\nconst struct gfs2_bitmap *bi, unsigned minlen, u64 *ptrimmed)\r\n{\r\nstruct super_block *sb = sdp->sd_vfs;\r\nu64 blk;\r\nsector_t start = 0;\r\nsector_t nr_blks = 0;\r\nint rv;\r\nunsigned int x;\r\nu32 trimmed = 0;\r\nu8 diff;\r\nfor (x = 0; x < bi->bi_len; x++) {\r\nconst u8 *clone = bi->bi_clone ? bi->bi_clone : bi->bi_bh->b_data;\r\nclone += bi->bi_offset;\r\nclone += x;\r\nif (bh) {\r\nconst u8 *orig = bh->b_data + bi->bi_offset + x;\r\ndiff = ~(*orig | (*orig >> 1)) & (*clone | (*clone >> 1));\r\n} else {\r\ndiff = ~(*clone | (*clone >> 1));\r\n}\r\ndiff &= 0x55;\r\nif (diff == 0)\r\ncontinue;\r\nblk = offset + ((bi->bi_start + x) * GFS2_NBBY);\r\nwhile(diff) {\r\nif (diff & 1) {\r\nif (nr_blks == 0)\r\ngoto start_new_extent;\r\nif ((start + nr_blks) != blk) {\r\nif (nr_blks >= minlen) {\r\nrv = sb_issue_discard(sb,\r\nstart, nr_blks,\r\nGFP_NOFS, 0);\r\nif (rv)\r\ngoto fail;\r\ntrimmed += nr_blks;\r\n}\r\nnr_blks = 0;\r\nstart_new_extent:\r\nstart = blk;\r\n}\r\nnr_blks++;\r\n}\r\ndiff >>= 2;\r\nblk++;\r\n}\r\n}\r\nif (nr_blks >= minlen) {\r\nrv = sb_issue_discard(sb, start, nr_blks, GFP_NOFS, 0);\r\nif (rv)\r\ngoto fail;\r\ntrimmed += nr_blks;\r\n}\r\nif (ptrimmed)\r\n*ptrimmed = trimmed;\r\nreturn 0;\r\nfail:\r\nif (sdp->sd_args.ar_discard)\r\nfs_warn(sdp, "error %d on discard request, turning discards off for this filesystem", rv);\r\nsdp->sd_args.ar_discard = 0;\r\nreturn -EIO;\r\n}\r\nint gfs2_fitrim(struct file *filp, void __user *argp)\r\n{\r\nstruct inode *inode = file_inode(filp);\r\nstruct gfs2_sbd *sdp = GFS2_SB(inode);\r\nstruct request_queue *q = bdev_get_queue(sdp->sd_vfs->s_bdev);\r\nstruct buffer_head *bh;\r\nstruct gfs2_rgrpd *rgd;\r\nstruct gfs2_rgrpd *rgd_end;\r\nstruct gfs2_holder gh;\r\nstruct fstrim_range r;\r\nint ret = 0;\r\nu64 amt;\r\nu64 trimmed = 0;\r\nu64 start, end, minlen;\r\nunsigned int x;\r\nunsigned bs_shift = sdp->sd_sb.sb_bsize_shift;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nif (!blk_queue_discard(q))\r\nreturn -EOPNOTSUPP;\r\nif (copy_from_user(&r, argp, sizeof(r)))\r\nreturn -EFAULT;\r\nret = gfs2_rindex_update(sdp);\r\nif (ret)\r\nreturn ret;\r\nstart = r.start >> bs_shift;\r\nend = start + (r.len >> bs_shift);\r\nminlen = max_t(u64, r.minlen,\r\nq->limits.discard_granularity) >> bs_shift;\r\nif (end <= start || minlen > sdp->sd_max_rg_data)\r\nreturn -EINVAL;\r\nrgd = gfs2_blk2rgrpd(sdp, start, 0);\r\nrgd_end = gfs2_blk2rgrpd(sdp, end, 0);\r\nif ((gfs2_rgrpd_get_first(sdp) == gfs2_rgrpd_get_next(rgd_end))\r\n&& (start > rgd_end->rd_data0 + rgd_end->rd_data))\r\nreturn -EINVAL;\r\nwhile (1) {\r\nret = gfs2_glock_nq_init(rgd->rd_gl, LM_ST_EXCLUSIVE, 0, &gh);\r\nif (ret)\r\ngoto out;\r\nif (!(rgd->rd_flags & GFS2_RGF_TRIMMED)) {\r\nfor (x = 0; x < rgd->rd_length; x++) {\r\nstruct gfs2_bitmap *bi = rgd->rd_bits + x;\r\nret = gfs2_rgrp_send_discards(sdp,\r\nrgd->rd_data0, NULL, bi, minlen,\r\n&amt);\r\nif (ret) {\r\ngfs2_glock_dq_uninit(&gh);\r\ngoto out;\r\n}\r\ntrimmed += amt;\r\n}\r\nret = gfs2_trans_begin(sdp, RES_RG_HDR, 0);\r\nif (ret == 0) {\r\nbh = rgd->rd_bits[0].bi_bh;\r\nrgd->rd_flags |= GFS2_RGF_TRIMMED;\r\ngfs2_trans_add_meta(rgd->rd_gl, bh);\r\ngfs2_rgrp_out(rgd, bh->b_data);\r\ngfs2_rgrp_ondisk2lvb(rgd->rd_rgl, bh->b_data);\r\ngfs2_trans_end(sdp);\r\n}\r\n}\r\ngfs2_glock_dq_uninit(&gh);\r\nif (rgd == rgd_end)\r\nbreak;\r\nrgd = gfs2_rgrpd_get_next(rgd);\r\n}\r\nout:\r\nr.len = trimmed << bs_shift;\r\nif (copy_to_user(argp, &r, sizeof(r)))\r\nreturn -EFAULT;\r\nreturn ret;\r\n}\r\nstatic void rs_insert(struct gfs2_inode *ip)\r\n{\r\nstruct rb_node **newn, *parent = NULL;\r\nint rc;\r\nstruct gfs2_blkreserv *rs = ip->i_res;\r\nstruct gfs2_rgrpd *rgd = rs->rs_rbm.rgd;\r\nu64 fsblock = gfs2_rbm_to_block(&rs->rs_rbm);\r\nBUG_ON(gfs2_rs_active(rs));\r\nspin_lock(&rgd->rd_rsspin);\r\nnewn = &rgd->rd_rstree.rb_node;\r\nwhile (*newn) {\r\nstruct gfs2_blkreserv *cur =\r\nrb_entry(*newn, struct gfs2_blkreserv, rs_node);\r\nparent = *newn;\r\nrc = rs_cmp(fsblock, rs->rs_free, cur);\r\nif (rc > 0)\r\nnewn = &((*newn)->rb_right);\r\nelse if (rc < 0)\r\nnewn = &((*newn)->rb_left);\r\nelse {\r\nspin_unlock(&rgd->rd_rsspin);\r\nWARN_ON(1);\r\nreturn;\r\n}\r\n}\r\nrb_link_node(&rs->rs_node, parent, newn);\r\nrb_insert_color(&rs->rs_node, &rgd->rd_rstree);\r\nrgd->rd_reserved += rs->rs_free;\r\nspin_unlock(&rgd->rd_rsspin);\r\ntrace_gfs2_rs(rs, TRACE_RS_INSERT);\r\n}\r\nstatic void rg_mblk_search(struct gfs2_rgrpd *rgd, struct gfs2_inode *ip,\r\nconst struct gfs2_alloc_parms *ap)\r\n{\r\nstruct gfs2_rbm rbm = { .rgd = rgd, };\r\nu64 goal;\r\nstruct gfs2_blkreserv *rs = ip->i_res;\r\nu32 extlen;\r\nu32 free_blocks = rgd->rd_free_clone - rgd->rd_reserved;\r\nint ret;\r\nstruct inode *inode = &ip->i_inode;\r\nif (S_ISDIR(inode->i_mode))\r\nextlen = 1;\r\nelse {\r\nextlen = max_t(u32, atomic_read(&rs->rs_sizehint), ap->target);\r\nextlen = clamp(extlen, RGRP_RSRV_MINBLKS, free_blocks);\r\n}\r\nif ((rgd->rd_free_clone < rgd->rd_reserved) || (free_blocks < extlen))\r\nreturn;\r\nif (rgrp_contains_block(rgd, ip->i_goal))\r\ngoal = ip->i_goal;\r\nelse\r\ngoal = rgd->rd_last_alloc + rgd->rd_data0;\r\nif (WARN_ON(gfs2_rbm_from_block(&rbm, goal)))\r\nreturn;\r\nret = gfs2_rbm_find(&rbm, GFS2_BLKST_FREE, &extlen, ip, true, ap);\r\nif (ret == 0) {\r\nrs->rs_rbm = rbm;\r\nrs->rs_free = extlen;\r\nrs->rs_inum = ip->i_no_addr;\r\nrs_insert(ip);\r\n} else {\r\nif (goal == rgd->rd_last_alloc + rgd->rd_data0)\r\nrgd->rd_last_alloc = 0;\r\n}\r\n}\r\nstatic u64 gfs2_next_unreserved_block(struct gfs2_rgrpd *rgd, u64 block,\r\nu32 length,\r\nconst struct gfs2_inode *ip)\r\n{\r\nstruct gfs2_blkreserv *rs;\r\nstruct rb_node *n;\r\nint rc;\r\nspin_lock(&rgd->rd_rsspin);\r\nn = rgd->rd_rstree.rb_node;\r\nwhile (n) {\r\nrs = rb_entry(n, struct gfs2_blkreserv, rs_node);\r\nrc = rs_cmp(block, length, rs);\r\nif (rc < 0)\r\nn = n->rb_left;\r\nelse if (rc > 0)\r\nn = n->rb_right;\r\nelse\r\nbreak;\r\n}\r\nif (n) {\r\nwhile ((rs_cmp(block, length, rs) == 0) && (ip->i_res != rs)) {\r\nblock = gfs2_rbm_to_block(&rs->rs_rbm) + rs->rs_free;\r\nn = n->rb_right;\r\nif (n == NULL)\r\nbreak;\r\nrs = rb_entry(n, struct gfs2_blkreserv, rs_node);\r\n}\r\n}\r\nspin_unlock(&rgd->rd_rsspin);\r\nreturn block;\r\n}\r\nstatic int gfs2_reservation_check_and_update(struct gfs2_rbm *rbm,\r\nconst struct gfs2_inode *ip,\r\nu32 minext,\r\nstruct gfs2_extent *maxext)\r\n{\r\nu64 block = gfs2_rbm_to_block(rbm);\r\nu32 extlen = 1;\r\nu64 nblock;\r\nint ret;\r\nif (minext) {\r\nextlen = gfs2_free_extlen(rbm, minext);\r\nif (extlen <= maxext->len)\r\ngoto fail;\r\n}\r\nnblock = gfs2_next_unreserved_block(rbm->rgd, block, extlen, ip);\r\nif (nblock == block) {\r\nif (!minext || extlen >= minext)\r\nreturn 0;\r\nif (extlen > maxext->len) {\r\nmaxext->len = extlen;\r\nmaxext->rbm = *rbm;\r\n}\r\nfail:\r\nnblock = block + extlen;\r\n}\r\nret = gfs2_rbm_from_block(rbm, nblock);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn 1;\r\n}\r\nstatic int gfs2_rbm_find(struct gfs2_rbm *rbm, u8 state, u32 *minext,\r\nconst struct gfs2_inode *ip, bool nowrap,\r\nconst struct gfs2_alloc_parms *ap)\r\n{\r\nstruct buffer_head *bh;\r\nint initial_bii;\r\nu32 initial_offset;\r\nint first_bii = rbm->bii;\r\nu32 first_offset = rbm->offset;\r\nu32 offset;\r\nu8 *buffer;\r\nint n = 0;\r\nint iters = rbm->rgd->rd_length;\r\nint ret;\r\nstruct gfs2_bitmap *bi;\r\nstruct gfs2_extent maxext = { .rbm.rgd = rbm->rgd, };\r\nif (rbm->offset != 0)\r\niters++;\r\nwhile(1) {\r\nbi = rbm_bi(rbm);\r\nif (test_bit(GBF_FULL, &bi->bi_flags) &&\r\n(state == GFS2_BLKST_FREE))\r\ngoto next_bitmap;\r\nbh = bi->bi_bh;\r\nbuffer = bh->b_data + bi->bi_offset;\r\nWARN_ON(!buffer_uptodate(bh));\r\nif (state != GFS2_BLKST_UNLINKED && bi->bi_clone)\r\nbuffer = bi->bi_clone + bi->bi_offset;\r\ninitial_offset = rbm->offset;\r\noffset = gfs2_bitfit(buffer, bi->bi_len, rbm->offset, state);\r\nif (offset == BFITNOENT)\r\ngoto bitmap_full;\r\nrbm->offset = offset;\r\nif (ip == NULL)\r\nreturn 0;\r\ninitial_bii = rbm->bii;\r\nret = gfs2_reservation_check_and_update(rbm, ip,\r\nminext ? *minext : 0,\r\n&maxext);\r\nif (ret == 0)\r\nreturn 0;\r\nif (ret > 0) {\r\nn += (rbm->bii - initial_bii);\r\ngoto next_iter;\r\n}\r\nif (ret == -E2BIG) {\r\nrbm->bii = 0;\r\nrbm->offset = 0;\r\nn += (rbm->bii - initial_bii);\r\ngoto res_covered_end_of_rgrp;\r\n}\r\nreturn ret;\r\nbitmap_full:\r\nif ((state == GFS2_BLKST_FREE) && initial_offset == 0) {\r\nstruct gfs2_bitmap *bi = rbm_bi(rbm);\r\nset_bit(GBF_FULL, &bi->bi_flags);\r\n}\r\nnext_bitmap:\r\nrbm->offset = 0;\r\nrbm->bii++;\r\nif (rbm->bii == rbm->rgd->rd_length)\r\nrbm->bii = 0;\r\nres_covered_end_of_rgrp:\r\nif ((rbm->bii == 0) && nowrap)\r\nbreak;\r\nn++;\r\nnext_iter:\r\nif (n >= iters)\r\nbreak;\r\n}\r\nif (minext == NULL || state != GFS2_BLKST_FREE)\r\nreturn -ENOSPC;\r\nif ((first_offset == 0) && (first_bii == 0) &&\r\n(*minext < rbm->rgd->rd_extfail_pt))\r\nrbm->rgd->rd_extfail_pt = *minext;\r\nif (maxext.len) {\r\n*rbm = maxext.rbm;\r\n*minext = maxext.len;\r\nreturn 0;\r\n}\r\nreturn -ENOSPC;\r\n}\r\nstatic void try_rgrp_unlink(struct gfs2_rgrpd *rgd, u64 *last_unlinked, u64 skip)\r\n{\r\nu64 block;\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nstruct gfs2_glock *gl;\r\nstruct gfs2_inode *ip;\r\nint error;\r\nint found = 0;\r\nstruct gfs2_rbm rbm = { .rgd = rgd, .bii = 0, .offset = 0 };\r\nwhile (1) {\r\ndown_write(&sdp->sd_log_flush_lock);\r\nerror = gfs2_rbm_find(&rbm, GFS2_BLKST_UNLINKED, NULL, NULL,\r\ntrue, NULL);\r\nup_write(&sdp->sd_log_flush_lock);\r\nif (error == -ENOSPC)\r\nbreak;\r\nif (WARN_ON_ONCE(error))\r\nbreak;\r\nblock = gfs2_rbm_to_block(&rbm);\r\nif (gfs2_rbm_from_block(&rbm, block + 1))\r\nbreak;\r\nif (*last_unlinked != NO_BLOCK && block <= *last_unlinked)\r\ncontinue;\r\nif (block == skip)\r\ncontinue;\r\n*last_unlinked = block;\r\nerror = gfs2_glock_get(sdp, block, &gfs2_inode_glops, CREATE, &gl);\r\nif (error)\r\ncontinue;\r\nip = gl->gl_object;\r\nif (ip || queue_work(gfs2_delete_workqueue, &gl->gl_delete) == 0)\r\ngfs2_glock_put(gl);\r\nelse\r\nfound++;\r\nif (found > NR_CPUS)\r\nreturn;\r\n}\r\nrgd->rd_flags &= ~GFS2_RDF_CHECK;\r\nreturn;\r\n}\r\nstatic bool gfs2_rgrp_congested(const struct gfs2_rgrpd *rgd, int loops)\r\n{\r\nconst struct gfs2_glock *gl = rgd->rd_gl;\r\nconst struct gfs2_sbd *sdp = gl->gl_sbd;\r\nstruct gfs2_lkstats *st;\r\ns64 r_dcount, l_dcount;\r\ns64 r_srttb, l_srttb;\r\ns64 srttb_diff;\r\ns64 sqr_diff;\r\ns64 var;\r\npreempt_disable();\r\nst = &this_cpu_ptr(sdp->sd_lkstats)->lkstats[LM_TYPE_RGRP];\r\nr_srttb = st->stats[GFS2_LKS_SRTTB];\r\nr_dcount = st->stats[GFS2_LKS_DCOUNT];\r\nvar = st->stats[GFS2_LKS_SRTTVARB] +\r\ngl->gl_stats.stats[GFS2_LKS_SRTTVARB];\r\npreempt_enable();\r\nl_srttb = gl->gl_stats.stats[GFS2_LKS_SRTTB];\r\nl_dcount = gl->gl_stats.stats[GFS2_LKS_DCOUNT];\r\nif ((l_dcount < 1) || (r_dcount < 1) || (r_srttb == 0))\r\nreturn false;\r\nsrttb_diff = r_srttb - l_srttb;\r\nsqr_diff = srttb_diff * srttb_diff;\r\nvar *= 2;\r\nif (l_dcount < 8 || r_dcount < 8)\r\nvar *= 2;\r\nif (loops == 1)\r\nvar *= 2;\r\nreturn ((srttb_diff < 0) && (sqr_diff > var));\r\n}\r\nstatic bool gfs2_rgrp_used_recently(const struct gfs2_blkreserv *rs,\r\nu64 msecs)\r\n{\r\nu64 tdiff;\r\ntdiff = ktime_to_ns(ktime_sub(ktime_get_real(),\r\nrs->rs_rbm.rgd->rd_gl->gl_dstamp));\r\nreturn tdiff > (msecs * 1000 * 1000);\r\n}\r\nstatic u32 gfs2_orlov_skip(const struct gfs2_inode *ip)\r\n{\r\nconst struct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\nu32 skip;\r\nget_random_bytes(&skip, sizeof(skip));\r\nreturn skip % sdp->sd_rgrps;\r\n}\r\nstatic bool gfs2_select_rgrp(struct gfs2_rgrpd **pos, const struct gfs2_rgrpd *begin)\r\n{\r\nstruct gfs2_rgrpd *rgd = *pos;\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nrgd = gfs2_rgrpd_get_next(rgd);\r\nif (rgd == NULL)\r\nrgd = gfs2_rgrpd_get_first(sdp);\r\n*pos = rgd;\r\nif (rgd != begin)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline int fast_to_acquire(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_glock *gl = rgd->rd_gl;\r\nif (gl->gl_state != LM_ST_UNLOCKED && list_empty(&gl->gl_holders) &&\r\n!test_bit(GLF_DEMOTE_IN_PROGRESS, &gl->gl_flags) &&\r\n!test_bit(GLF_DEMOTE, &gl->gl_flags))\r\nreturn 1;\r\nif (rgd->rd_flags & GFS2_RDF_PREFERRED)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nint gfs2_inplace_reserve(struct gfs2_inode *ip, const struct gfs2_alloc_parms *ap)\r\n{\r\nstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\nstruct gfs2_rgrpd *begin = NULL;\r\nstruct gfs2_blkreserv *rs = ip->i_res;\r\nint error = 0, rg_locked, flags = 0;\r\nu64 last_unlinked = NO_BLOCK;\r\nint loops = 0;\r\nu32 skip = 0;\r\nif (sdp->sd_args.ar_rgrplvb)\r\nflags |= GL_SKIP;\r\nif (gfs2_assert_warn(sdp, ap->target))\r\nreturn -EINVAL;\r\nif (gfs2_rs_active(rs)) {\r\nbegin = rs->rs_rbm.rgd;\r\n} else if (ip->i_rgd && rgrp_contains_block(ip->i_rgd, ip->i_goal)) {\r\nrs->rs_rbm.rgd = begin = ip->i_rgd;\r\n} else {\r\ncheck_and_update_goal(ip);\r\nrs->rs_rbm.rgd = begin = gfs2_blk2rgrpd(sdp, ip->i_goal, 1);\r\n}\r\nif (S_ISDIR(ip->i_inode.i_mode) && (ap->aflags & GFS2_AF_ORLOV))\r\nskip = gfs2_orlov_skip(ip);\r\nif (rs->rs_rbm.rgd == NULL)\r\nreturn -EBADSLT;\r\nwhile (loops < 3) {\r\nrg_locked = 1;\r\nif (!gfs2_glock_is_locked_by_me(rs->rs_rbm.rgd->rd_gl)) {\r\nrg_locked = 0;\r\nif (skip && skip--)\r\ngoto next_rgrp;\r\nif (!gfs2_rs_active(rs)) {\r\nif (loops == 0 &&\r\n!fast_to_acquire(rs->rs_rbm.rgd))\r\ngoto next_rgrp;\r\nif ((loops < 2) &&\r\ngfs2_rgrp_used_recently(rs, 1000) &&\r\ngfs2_rgrp_congested(rs->rs_rbm.rgd, loops))\r\ngoto next_rgrp;\r\n}\r\nerror = gfs2_glock_nq_init(rs->rs_rbm.rgd->rd_gl,\r\nLM_ST_EXCLUSIVE, flags,\r\n&rs->rs_rgd_gh);\r\nif (unlikely(error))\r\nreturn error;\r\nif (!gfs2_rs_active(rs) && (loops < 2) &&\r\ngfs2_rgrp_congested(rs->rs_rbm.rgd, loops))\r\ngoto skip_rgrp;\r\nif (sdp->sd_args.ar_rgrplvb) {\r\nerror = update_rgrp_lvb(rs->rs_rbm.rgd);\r\nif (unlikely(error)) {\r\ngfs2_glock_dq_uninit(&rs->rs_rgd_gh);\r\nreturn error;\r\n}\r\n}\r\n}\r\nif ((rs->rs_rbm.rgd->rd_flags & (GFS2_RGF_NOALLOC |\r\nGFS2_RDF_ERROR)) ||\r\n(ap->target > rs->rs_rbm.rgd->rd_extfail_pt))\r\ngoto skip_rgrp;\r\nif (sdp->sd_args.ar_rgrplvb)\r\ngfs2_rgrp_bh_get(rs->rs_rbm.rgd);\r\nif (!gfs2_rs_active(rs))\r\nrg_mblk_search(rs->rs_rbm.rgd, ip, ap);\r\nif (!gfs2_rs_active(rs) && (loops < 1))\r\ngoto check_rgrp;\r\nif (rs->rs_rbm.rgd->rd_free_clone >= ap->target) {\r\nip->i_rgd = rs->rs_rbm.rgd;\r\nreturn 0;\r\n}\r\ncheck_rgrp:\r\nif (rs->rs_rbm.rgd->rd_flags & GFS2_RDF_CHECK)\r\ntry_rgrp_unlink(rs->rs_rbm.rgd, &last_unlinked,\r\nip->i_no_addr);\r\nskip_rgrp:\r\nif (gfs2_rs_active(rs))\r\ngfs2_rs_deltree(rs);\r\nif (!rg_locked)\r\ngfs2_glock_dq_uninit(&rs->rs_rgd_gh);\r\nnext_rgrp:\r\nif (gfs2_select_rgrp(&rs->rs_rbm.rgd, begin))\r\ncontinue;\r\nif (skip)\r\ncontinue;\r\nloops++;\r\nif (ip == GFS2_I(sdp->sd_rindex) && !sdp->sd_rindex_uptodate) {\r\nerror = gfs2_ri_update(ip);\r\nif (error)\r\nreturn error;\r\n}\r\nif (loops == 2)\r\ngfs2_log_flush(sdp, NULL, NORMAL_FLUSH);\r\n}\r\nreturn -ENOSPC;\r\n}\r\nvoid gfs2_inplace_release(struct gfs2_inode *ip)\r\n{\r\nstruct gfs2_blkreserv *rs = ip->i_res;\r\nif (rs->rs_rgd_gh.gh_gl)\r\ngfs2_glock_dq_uninit(&rs->rs_rgd_gh);\r\n}\r\nstatic unsigned char gfs2_get_block_type(struct gfs2_rgrpd *rgd, u64 block)\r\n{\r\nstruct gfs2_rbm rbm = { .rgd = rgd, };\r\nint ret;\r\nret = gfs2_rbm_from_block(&rbm, block);\r\nWARN_ON_ONCE(ret != 0);\r\nreturn gfs2_testbit(&rbm);\r\n}\r\nstatic void gfs2_alloc_extent(const struct gfs2_rbm *rbm, bool dinode,\r\nunsigned int *n)\r\n{\r\nstruct gfs2_rbm pos = { .rgd = rbm->rgd, };\r\nconst unsigned int elen = *n;\r\nu64 block;\r\nint ret;\r\n*n = 1;\r\nblock = gfs2_rbm_to_block(rbm);\r\ngfs2_trans_add_meta(rbm->rgd->rd_gl, rbm_bi(rbm)->bi_bh);\r\ngfs2_setbit(rbm, true, dinode ? GFS2_BLKST_DINODE : GFS2_BLKST_USED);\r\nblock++;\r\nwhile (*n < elen) {\r\nret = gfs2_rbm_from_block(&pos, block);\r\nif (ret || gfs2_testbit(&pos) != GFS2_BLKST_FREE)\r\nbreak;\r\ngfs2_trans_add_meta(pos.rgd->rd_gl, rbm_bi(&pos)->bi_bh);\r\ngfs2_setbit(&pos, true, GFS2_BLKST_USED);\r\n(*n)++;\r\nblock++;\r\n}\r\n}\r\nstatic struct gfs2_rgrpd *rgblk_free(struct gfs2_sbd *sdp, u64 bstart,\r\nu32 blen, unsigned char new_state)\r\n{\r\nstruct gfs2_rbm rbm;\r\nstruct gfs2_bitmap *bi, *bi_prev = NULL;\r\nrbm.rgd = gfs2_blk2rgrpd(sdp, bstart, 1);\r\nif (!rbm.rgd) {\r\nif (gfs2_consist(sdp))\r\nfs_err(sdp, "block = %llu\n", (unsigned long long)bstart);\r\nreturn NULL;\r\n}\r\ngfs2_rbm_from_block(&rbm, bstart);\r\nwhile (blen--) {\r\nbi = rbm_bi(&rbm);\r\nif (bi != bi_prev) {\r\nif (!bi->bi_clone) {\r\nbi->bi_clone = kmalloc(bi->bi_bh->b_size,\r\nGFP_NOFS | __GFP_NOFAIL);\r\nmemcpy(bi->bi_clone + bi->bi_offset,\r\nbi->bi_bh->b_data + bi->bi_offset,\r\nbi->bi_len);\r\n}\r\ngfs2_trans_add_meta(rbm.rgd->rd_gl, bi->bi_bh);\r\nbi_prev = bi;\r\n}\r\ngfs2_setbit(&rbm, false, new_state);\r\ngfs2_rbm_incr(&rbm);\r\n}\r\nreturn rbm.rgd;\r\n}\r\nvoid gfs2_rgrp_dump(struct seq_file *seq, const struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_rgrpd *rgd = gl->gl_object;\r\nstruct gfs2_blkreserv *trs;\r\nconst struct rb_node *n;\r\nif (rgd == NULL)\r\nreturn;\r\ngfs2_print_dbg(seq, " R: n:%llu f:%02x b:%u/%u i:%u r:%u e:%u\n",\r\n(unsigned long long)rgd->rd_addr, rgd->rd_flags,\r\nrgd->rd_free, rgd->rd_free_clone, rgd->rd_dinodes,\r\nrgd->rd_reserved, rgd->rd_extfail_pt);\r\nspin_lock(&rgd->rd_rsspin);\r\nfor (n = rb_first(&rgd->rd_rstree); n; n = rb_next(&trs->rs_node)) {\r\ntrs = rb_entry(n, struct gfs2_blkreserv, rs_node);\r\ndump_rs(seq, trs);\r\n}\r\nspin_unlock(&rgd->rd_rsspin);\r\n}\r\nstatic void gfs2_rgrp_error(struct gfs2_rgrpd *rgd)\r\n{\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nfs_warn(sdp, "rgrp %llu has an error, marking it readonly until umount\n",\r\n(unsigned long long)rgd->rd_addr);\r\nfs_warn(sdp, "umount on all nodes and run fsck.gfs2 to fix the error\n");\r\ngfs2_rgrp_dump(NULL, rgd->rd_gl);\r\nrgd->rd_flags |= GFS2_RDF_ERROR;\r\n}\r\nstatic void gfs2_adjust_reservation(struct gfs2_inode *ip,\r\nconst struct gfs2_rbm *rbm, unsigned len)\r\n{\r\nstruct gfs2_blkreserv *rs = ip->i_res;\r\nstruct gfs2_rgrpd *rgd = rbm->rgd;\r\nunsigned rlen;\r\nu64 block;\r\nint ret;\r\nspin_lock(&rgd->rd_rsspin);\r\nif (gfs2_rs_active(rs)) {\r\nif (gfs2_rbm_eq(&rs->rs_rbm, rbm)) {\r\nblock = gfs2_rbm_to_block(rbm);\r\nret = gfs2_rbm_from_block(&rs->rs_rbm, block + len);\r\nrlen = min(rs->rs_free, len);\r\nrs->rs_free -= rlen;\r\nrgd->rd_reserved -= rlen;\r\ntrace_gfs2_rs(rs, TRACE_RS_CLAIM);\r\nif (rs->rs_free && !ret)\r\ngoto out;\r\natomic_add(RGRP_RSRV_ADDBLKS, &rs->rs_sizehint);\r\n}\r\n__rs_deltree(rs);\r\n}\r\nout:\r\nspin_unlock(&rgd->rd_rsspin);\r\n}\r\nstatic void gfs2_set_alloc_start(struct gfs2_rbm *rbm,\r\nconst struct gfs2_inode *ip, bool dinode)\r\n{\r\nu64 goal;\r\nif (gfs2_rs_active(ip->i_res)) {\r\n*rbm = ip->i_res->rs_rbm;\r\nreturn;\r\n}\r\nif (!dinode && rgrp_contains_block(rbm->rgd, ip->i_goal))\r\ngoal = ip->i_goal;\r\nelse\r\ngoal = rbm->rgd->rd_last_alloc + rbm->rgd->rd_data0;\r\ngfs2_rbm_from_block(rbm, goal);\r\n}\r\nint gfs2_alloc_blocks(struct gfs2_inode *ip, u64 *bn, unsigned int *nblocks,\r\nbool dinode, u64 *generation)\r\n{\r\nstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\nstruct buffer_head *dibh;\r\nstruct gfs2_rbm rbm = { .rgd = ip->i_rgd, };\r\nunsigned int ndata;\r\nu64 block;\r\nint error;\r\ngfs2_set_alloc_start(&rbm, ip, dinode);\r\nerror = gfs2_rbm_find(&rbm, GFS2_BLKST_FREE, NULL, ip, false, NULL);\r\nif (error == -ENOSPC) {\r\ngfs2_set_alloc_start(&rbm, ip, dinode);\r\nerror = gfs2_rbm_find(&rbm, GFS2_BLKST_FREE, NULL, NULL, false,\r\nNULL);\r\n}\r\nif (error) {\r\nfs_warn(sdp, "inum=%llu error=%d, nblocks=%u, full=%d fail_pt=%d\n",\r\n(unsigned long long)ip->i_no_addr, error, *nblocks,\r\ntest_bit(GBF_FULL, &rbm.rgd->rd_bits->bi_flags),\r\nrbm.rgd->rd_extfail_pt);\r\ngoto rgrp_error;\r\n}\r\ngfs2_alloc_extent(&rbm, dinode, nblocks);\r\nblock = gfs2_rbm_to_block(&rbm);\r\nrbm.rgd->rd_last_alloc = block - rbm.rgd->rd_data0;\r\nif (gfs2_rs_active(ip->i_res))\r\ngfs2_adjust_reservation(ip, &rbm, *nblocks);\r\nndata = *nblocks;\r\nif (dinode)\r\nndata--;\r\nif (!dinode) {\r\nip->i_goal = block + ndata - 1;\r\nerror = gfs2_meta_inode_buffer(ip, &dibh);\r\nif (error == 0) {\r\nstruct gfs2_dinode *di =\r\n(struct gfs2_dinode *)dibh->b_data;\r\ngfs2_trans_add_meta(ip->i_gl, dibh);\r\ndi->di_goal_meta = di->di_goal_data =\r\ncpu_to_be64(ip->i_goal);\r\nbrelse(dibh);\r\n}\r\n}\r\nif (rbm.rgd->rd_free < *nblocks) {\r\npr_warn("nblocks=%u\n", *nblocks);\r\ngoto rgrp_error;\r\n}\r\nrbm.rgd->rd_free -= *nblocks;\r\nif (dinode) {\r\nrbm.rgd->rd_dinodes++;\r\n*generation = rbm.rgd->rd_igeneration++;\r\nif (*generation == 0)\r\n*generation = rbm.rgd->rd_igeneration++;\r\n}\r\ngfs2_trans_add_meta(rbm.rgd->rd_gl, rbm.rgd->rd_bits[0].bi_bh);\r\ngfs2_rgrp_out(rbm.rgd, rbm.rgd->rd_bits[0].bi_bh->b_data);\r\ngfs2_rgrp_ondisk2lvb(rbm.rgd->rd_rgl, rbm.rgd->rd_bits[0].bi_bh->b_data);\r\ngfs2_statfs_change(sdp, 0, -(s64)*nblocks, dinode ? 1 : 0);\r\nif (dinode)\r\ngfs2_trans_add_unrevoke(sdp, block, *nblocks);\r\ngfs2_quota_change(ip, *nblocks, ip->i_inode.i_uid, ip->i_inode.i_gid);\r\nrbm.rgd->rd_free_clone -= *nblocks;\r\ntrace_gfs2_block_alloc(ip, rbm.rgd, block, *nblocks,\r\ndinode ? GFS2_BLKST_DINODE : GFS2_BLKST_USED);\r\n*bn = block;\r\nreturn 0;\r\nrgrp_error:\r\ngfs2_rgrp_error(rbm.rgd);\r\nreturn -EIO;\r\n}\r\nvoid __gfs2_free_blocks(struct gfs2_inode *ip, u64 bstart, u32 blen, int meta)\r\n{\r\nstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\nstruct gfs2_rgrpd *rgd;\r\nrgd = rgblk_free(sdp, bstart, blen, GFS2_BLKST_FREE);\r\nif (!rgd)\r\nreturn;\r\ntrace_gfs2_block_alloc(ip, rgd, bstart, blen, GFS2_BLKST_FREE);\r\nrgd->rd_free += blen;\r\nrgd->rd_flags &= ~GFS2_RGF_TRIMMED;\r\ngfs2_trans_add_meta(rgd->rd_gl, rgd->rd_bits[0].bi_bh);\r\ngfs2_rgrp_out(rgd, rgd->rd_bits[0].bi_bh->b_data);\r\ngfs2_rgrp_ondisk2lvb(rgd->rd_rgl, rgd->rd_bits[0].bi_bh->b_data);\r\nif (meta || ip->i_depth)\r\ngfs2_meta_wipe(ip, bstart, blen);\r\n}\r\nvoid gfs2_free_meta(struct gfs2_inode *ip, u64 bstart, u32 blen)\r\n{\r\nstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\n__gfs2_free_blocks(ip, bstart, blen, 1);\r\ngfs2_statfs_change(sdp, 0, +blen, 0);\r\ngfs2_quota_change(ip, -(s64)blen, ip->i_inode.i_uid, ip->i_inode.i_gid);\r\n}\r\nvoid gfs2_unlink_di(struct inode *inode)\r\n{\r\nstruct gfs2_inode *ip = GFS2_I(inode);\r\nstruct gfs2_sbd *sdp = GFS2_SB(inode);\r\nstruct gfs2_rgrpd *rgd;\r\nu64 blkno = ip->i_no_addr;\r\nrgd = rgblk_free(sdp, blkno, 1, GFS2_BLKST_UNLINKED);\r\nif (!rgd)\r\nreturn;\r\ntrace_gfs2_block_alloc(ip, rgd, blkno, 1, GFS2_BLKST_UNLINKED);\r\ngfs2_trans_add_meta(rgd->rd_gl, rgd->rd_bits[0].bi_bh);\r\ngfs2_rgrp_out(rgd, rgd->rd_bits[0].bi_bh->b_data);\r\ngfs2_rgrp_ondisk2lvb(rgd->rd_rgl, rgd->rd_bits[0].bi_bh->b_data);\r\nupdate_rgrp_lvb_unlinked(rgd, 1);\r\n}\r\nstatic void gfs2_free_uninit_di(struct gfs2_rgrpd *rgd, u64 blkno)\r\n{\r\nstruct gfs2_sbd *sdp = rgd->rd_sbd;\r\nstruct gfs2_rgrpd *tmp_rgd;\r\ntmp_rgd = rgblk_free(sdp, blkno, 1, GFS2_BLKST_FREE);\r\nif (!tmp_rgd)\r\nreturn;\r\ngfs2_assert_withdraw(sdp, rgd == tmp_rgd);\r\nif (!rgd->rd_dinodes)\r\ngfs2_consist_rgrpd(rgd);\r\nrgd->rd_dinodes--;\r\nrgd->rd_free++;\r\ngfs2_trans_add_meta(rgd->rd_gl, rgd->rd_bits[0].bi_bh);\r\ngfs2_rgrp_out(rgd, rgd->rd_bits[0].bi_bh->b_data);\r\ngfs2_rgrp_ondisk2lvb(rgd->rd_rgl, rgd->rd_bits[0].bi_bh->b_data);\r\nupdate_rgrp_lvb_unlinked(rgd, -1);\r\ngfs2_statfs_change(sdp, 0, +1, -1);\r\n}\r\nvoid gfs2_free_di(struct gfs2_rgrpd *rgd, struct gfs2_inode *ip)\r\n{\r\ngfs2_free_uninit_di(rgd, ip->i_no_addr);\r\ntrace_gfs2_block_alloc(ip, rgd, ip->i_no_addr, 1, GFS2_BLKST_FREE);\r\ngfs2_quota_change(ip, -1, ip->i_inode.i_uid, ip->i_inode.i_gid);\r\ngfs2_meta_wipe(ip, ip->i_no_addr, 1);\r\n}\r\nint gfs2_check_blk_type(struct gfs2_sbd *sdp, u64 no_addr, unsigned int type)\r\n{\r\nstruct gfs2_rgrpd *rgd;\r\nstruct gfs2_holder rgd_gh;\r\nint error = -EINVAL;\r\nrgd = gfs2_blk2rgrpd(sdp, no_addr, 1);\r\nif (!rgd)\r\ngoto fail;\r\nerror = gfs2_glock_nq_init(rgd->rd_gl, LM_ST_SHARED, 0, &rgd_gh);\r\nif (error)\r\ngoto fail;\r\nif (gfs2_get_block_type(rgd, no_addr) != type)\r\nerror = -ESTALE;\r\ngfs2_glock_dq_uninit(&rgd_gh);\r\nfail:\r\nreturn error;\r\n}\r\nvoid gfs2_rlist_add(struct gfs2_inode *ip, struct gfs2_rgrp_list *rlist,\r\nu64 block)\r\n{\r\nstruct gfs2_sbd *sdp = GFS2_SB(&ip->i_inode);\r\nstruct gfs2_rgrpd *rgd;\r\nstruct gfs2_rgrpd **tmp;\r\nunsigned int new_space;\r\nunsigned int x;\r\nif (gfs2_assert_warn(sdp, !rlist->rl_ghs))\r\nreturn;\r\nif (ip->i_rgd && rgrp_contains_block(ip->i_rgd, block))\r\nrgd = ip->i_rgd;\r\nelse\r\nrgd = gfs2_blk2rgrpd(sdp, block, 1);\r\nif (!rgd) {\r\nfs_err(sdp, "rlist_add: no rgrp for block %llu\n", (unsigned long long)block);\r\nreturn;\r\n}\r\nip->i_rgd = rgd;\r\nfor (x = 0; x < rlist->rl_rgrps; x++)\r\nif (rlist->rl_rgd[x] == rgd)\r\nreturn;\r\nif (rlist->rl_rgrps == rlist->rl_space) {\r\nnew_space = rlist->rl_space + 10;\r\ntmp = kcalloc(new_space, sizeof(struct gfs2_rgrpd *),\r\nGFP_NOFS | __GFP_NOFAIL);\r\nif (rlist->rl_rgd) {\r\nmemcpy(tmp, rlist->rl_rgd,\r\nrlist->rl_space * sizeof(struct gfs2_rgrpd *));\r\nkfree(rlist->rl_rgd);\r\n}\r\nrlist->rl_space = new_space;\r\nrlist->rl_rgd = tmp;\r\n}\r\nrlist->rl_rgd[rlist->rl_rgrps++] = rgd;\r\n}\r\nvoid gfs2_rlist_alloc(struct gfs2_rgrp_list *rlist, unsigned int state)\r\n{\r\nunsigned int x;\r\nrlist->rl_ghs = kcalloc(rlist->rl_rgrps, sizeof(struct gfs2_holder),\r\nGFP_NOFS | __GFP_NOFAIL);\r\nfor (x = 0; x < rlist->rl_rgrps; x++)\r\ngfs2_holder_init(rlist->rl_rgd[x]->rd_gl,\r\nstate, 0,\r\n&rlist->rl_ghs[x]);\r\n}\r\nvoid gfs2_rlist_free(struct gfs2_rgrp_list *rlist)\r\n{\r\nunsigned int x;\r\nkfree(rlist->rl_rgd);\r\nif (rlist->rl_ghs) {\r\nfor (x = 0; x < rlist->rl_rgrps; x++)\r\ngfs2_holder_uninit(&rlist->rl_ghs[x]);\r\nkfree(rlist->rl_ghs);\r\nrlist->rl_ghs = NULL;\r\n}\r\n}
