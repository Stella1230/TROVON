static u8 *emit_code(u8 *ptr, u32 bytes, unsigned int len)\r\n{\r\nif (len == 1)\r\n*ptr = bytes;\r\nelse if (len == 2)\r\n*(u16 *)ptr = bytes;\r\nelse {\r\n*(u32 *)ptr = bytes;\r\nbarrier();\r\n}\r\nreturn ptr + len;\r\n}\r\nstatic bool is_imm8(int value)\r\n{\r\nreturn value <= 127 && value >= -128;\r\n}\r\nstatic bool is_simm32(s64 value)\r\n{\r\nreturn value == (s64) (s32) value;\r\n}\r\nstatic int bpf_size_to_x86_bytes(int bpf_size)\r\n{\r\nif (bpf_size == BPF_W)\r\nreturn 4;\r\nelse if (bpf_size == BPF_H)\r\nreturn 2;\r\nelse if (bpf_size == BPF_B)\r\nreturn 1;\r\nelse if (bpf_size == BPF_DW)\r\nreturn 4;\r\nelse\r\nreturn 0;\r\n}\r\nstatic void bpf_flush_icache(void *start, void *end)\r\n{\r\nmm_segment_t old_fs = get_fs();\r\nset_fs(KERNEL_DS);\r\nsmp_wmb();\r\nflush_icache_range((unsigned long)start, (unsigned long)end);\r\nset_fs(old_fs);\r\n}\r\nstatic bool is_ereg(u32 reg)\r\n{\r\nreturn (1 << reg) & (BIT(BPF_REG_5) |\r\nBIT(AUX_REG) |\r\nBIT(BPF_REG_7) |\r\nBIT(BPF_REG_8) |\r\nBIT(BPF_REG_9));\r\n}\r\nstatic u8 add_1mod(u8 byte, u32 reg)\r\n{\r\nif (is_ereg(reg))\r\nbyte |= 1;\r\nreturn byte;\r\n}\r\nstatic u8 add_2mod(u8 byte, u32 r1, u32 r2)\r\n{\r\nif (is_ereg(r1))\r\nbyte |= 1;\r\nif (is_ereg(r2))\r\nbyte |= 4;\r\nreturn byte;\r\n}\r\nstatic u8 add_1reg(u8 byte, u32 dst_reg)\r\n{\r\nreturn byte + reg2hex[dst_reg];\r\n}\r\nstatic u8 add_2reg(u8 byte, u32 dst_reg, u32 src_reg)\r\n{\r\nreturn byte + reg2hex[dst_reg] + (reg2hex[src_reg] << 3);\r\n}\r\nstatic void jit_fill_hole(void *area, unsigned int size)\r\n{\r\nmemset(area, 0xcc, size);\r\n}\r\nstatic int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image,\r\nint oldproglen, struct jit_context *ctx)\r\n{\r\nstruct bpf_insn *insn = bpf_prog->insnsi;\r\nint insn_cnt = bpf_prog->len;\r\nbool seen_ld_abs = ctx->seen_ld_abs | (oldproglen == 0);\r\nbool seen_exit = false;\r\nu8 temp[BPF_MAX_INSN_SIZE + BPF_INSN_SAFETY];\r\nint i;\r\nint proglen = 0;\r\nu8 *prog = temp;\r\nint stacksize = MAX_BPF_STACK +\r\n32 +\r\n8 ;\r\nEMIT1(0x55);\r\nEMIT3(0x48, 0x89, 0xE5);\r\nEMIT3_off32(0x48, 0x81, 0xEC, stacksize);\r\nEMIT3_off32(0x48, 0x89, 0x9D, -stacksize);\r\nEMIT3_off32(0x4C, 0x89, 0xAD, -stacksize + 8);\r\nEMIT3_off32(0x4C, 0x89, 0xB5, -stacksize + 16);\r\nEMIT3_off32(0x4C, 0x89, 0xBD, -stacksize + 24);\r\nEMIT2(0x31, 0xc0);\r\nEMIT3(0x4D, 0x31, 0xED);\r\nif (seen_ld_abs) {\r\nif (is_imm8(offsetof(struct sk_buff, len)))\r\nEMIT4(0x44, 0x8b, 0x4f,\r\noffsetof(struct sk_buff, len));\r\nelse\r\nEMIT3_off32(0x44, 0x8b, 0x8f,\r\noffsetof(struct sk_buff, len));\r\nif (is_imm8(offsetof(struct sk_buff, data_len)))\r\nEMIT4(0x44, 0x2b, 0x4f,\r\noffsetof(struct sk_buff, data_len));\r\nelse\r\nEMIT3_off32(0x44, 0x2b, 0x8f,\r\noffsetof(struct sk_buff, data_len));\r\nif (is_imm8(offsetof(struct sk_buff, data)))\r\nEMIT4(0x4c, 0x8b, 0x57,\r\noffsetof(struct sk_buff, data));\r\nelse\r\nEMIT3_off32(0x4c, 0x8b, 0x97,\r\noffsetof(struct sk_buff, data));\r\n}\r\nfor (i = 0; i < insn_cnt; i++, insn++) {\r\nconst s32 imm32 = insn->imm;\r\nu32 dst_reg = insn->dst_reg;\r\nu32 src_reg = insn->src_reg;\r\nu8 b1 = 0, b2 = 0, b3 = 0;\r\ns64 jmp_offset;\r\nu8 jmp_cond;\r\nint ilen;\r\nu8 *func;\r\nswitch (insn->code) {\r\ncase BPF_ALU | BPF_ADD | BPF_X:\r\ncase BPF_ALU | BPF_SUB | BPF_X:\r\ncase BPF_ALU | BPF_AND | BPF_X:\r\ncase BPF_ALU | BPF_OR | BPF_X:\r\ncase BPF_ALU | BPF_XOR | BPF_X:\r\ncase BPF_ALU64 | BPF_ADD | BPF_X:\r\ncase BPF_ALU64 | BPF_SUB | BPF_X:\r\ncase BPF_ALU64 | BPF_AND | BPF_X:\r\ncase BPF_ALU64 | BPF_OR | BPF_X:\r\ncase BPF_ALU64 | BPF_XOR | BPF_X:\r\nswitch (BPF_OP(insn->code)) {\r\ncase BPF_ADD: b2 = 0x01; break;\r\ncase BPF_SUB: b2 = 0x29; break;\r\ncase BPF_AND: b2 = 0x21; break;\r\ncase BPF_OR: b2 = 0x09; break;\r\ncase BPF_XOR: b2 = 0x31; break;\r\n}\r\nif (BPF_CLASS(insn->code) == BPF_ALU64)\r\nEMIT1(add_2mod(0x48, dst_reg, src_reg));\r\nelse if (is_ereg(dst_reg) || is_ereg(src_reg))\r\nEMIT1(add_2mod(0x40, dst_reg, src_reg));\r\nEMIT2(b2, add_2reg(0xC0, dst_reg, src_reg));\r\nbreak;\r\ncase BPF_ALU64 | BPF_MOV | BPF_X:\r\nEMIT_mov(dst_reg, src_reg);\r\nbreak;\r\ncase BPF_ALU | BPF_MOV | BPF_X:\r\nif (is_ereg(dst_reg) || is_ereg(src_reg))\r\nEMIT1(add_2mod(0x40, dst_reg, src_reg));\r\nEMIT2(0x89, add_2reg(0xC0, dst_reg, src_reg));\r\nbreak;\r\ncase BPF_ALU | BPF_NEG:\r\ncase BPF_ALU64 | BPF_NEG:\r\nif (BPF_CLASS(insn->code) == BPF_ALU64)\r\nEMIT1(add_1mod(0x48, dst_reg));\r\nelse if (is_ereg(dst_reg))\r\nEMIT1(add_1mod(0x40, dst_reg));\r\nEMIT2(0xF7, add_1reg(0xD8, dst_reg));\r\nbreak;\r\ncase BPF_ALU | BPF_ADD | BPF_K:\r\ncase BPF_ALU | BPF_SUB | BPF_K:\r\ncase BPF_ALU | BPF_AND | BPF_K:\r\ncase BPF_ALU | BPF_OR | BPF_K:\r\ncase BPF_ALU | BPF_XOR | BPF_K:\r\ncase BPF_ALU64 | BPF_ADD | BPF_K:\r\ncase BPF_ALU64 | BPF_SUB | BPF_K:\r\ncase BPF_ALU64 | BPF_AND | BPF_K:\r\ncase BPF_ALU64 | BPF_OR | BPF_K:\r\ncase BPF_ALU64 | BPF_XOR | BPF_K:\r\nif (BPF_CLASS(insn->code) == BPF_ALU64)\r\nEMIT1(add_1mod(0x48, dst_reg));\r\nelse if (is_ereg(dst_reg))\r\nEMIT1(add_1mod(0x40, dst_reg));\r\nswitch (BPF_OP(insn->code)) {\r\ncase BPF_ADD: b3 = 0xC0; break;\r\ncase BPF_SUB: b3 = 0xE8; break;\r\ncase BPF_AND: b3 = 0xE0; break;\r\ncase BPF_OR: b3 = 0xC8; break;\r\ncase BPF_XOR: b3 = 0xF0; break;\r\n}\r\nif (is_imm8(imm32))\r\nEMIT3(0x83, add_1reg(b3, dst_reg), imm32);\r\nelse\r\nEMIT2_off32(0x81, add_1reg(b3, dst_reg), imm32);\r\nbreak;\r\ncase BPF_ALU64 | BPF_MOV | BPF_K:\r\nif (imm32 < 0) {\r\nb1 = add_1mod(0x48, dst_reg);\r\nb2 = 0xC7;\r\nb3 = 0xC0;\r\nEMIT3_off32(b1, b2, add_1reg(b3, dst_reg), imm32);\r\nbreak;\r\n}\r\ncase BPF_ALU | BPF_MOV | BPF_K:\r\nif (is_ereg(dst_reg))\r\nEMIT1(add_1mod(0x40, dst_reg));\r\nEMIT1_off32(add_1reg(0xB8, dst_reg), imm32);\r\nbreak;\r\ncase BPF_LD | BPF_IMM | BPF_DW:\r\nif (insn[1].code != 0 || insn[1].src_reg != 0 ||\r\ninsn[1].dst_reg != 0 || insn[1].off != 0) {\r\npr_err("invalid BPF_LD_IMM64 insn\n");\r\nreturn -EINVAL;\r\n}\r\nEMIT2(add_1mod(0x48, dst_reg), add_1reg(0xB8, dst_reg));\r\nEMIT(insn[0].imm, 4);\r\nEMIT(insn[1].imm, 4);\r\ninsn++;\r\ni++;\r\nbreak;\r\ncase BPF_ALU | BPF_MOD | BPF_X:\r\ncase BPF_ALU | BPF_DIV | BPF_X:\r\ncase BPF_ALU | BPF_MOD | BPF_K:\r\ncase BPF_ALU | BPF_DIV | BPF_K:\r\ncase BPF_ALU64 | BPF_MOD | BPF_X:\r\ncase BPF_ALU64 | BPF_DIV | BPF_X:\r\ncase BPF_ALU64 | BPF_MOD | BPF_K:\r\ncase BPF_ALU64 | BPF_DIV | BPF_K:\r\nEMIT1(0x50);\r\nEMIT1(0x52);\r\nif (BPF_SRC(insn->code) == BPF_X)\r\nEMIT_mov(AUX_REG, src_reg);\r\nelse\r\nEMIT3_off32(0x49, 0xC7, 0xC3, imm32);\r\nEMIT_mov(BPF_REG_0, dst_reg);\r\nEMIT2(0x31, 0xd2);\r\nif (BPF_SRC(insn->code) == BPF_X) {\r\nEMIT4(0x49, 0x83, 0xFB, 0x00);\r\nEMIT2(X86_JNE, 1 + 1 + 2 + 5);\r\nEMIT1(0x5A);\r\nEMIT1(0x58);\r\nEMIT2(0x31, 0xc0);\r\njmp_offset = ctx->cleanup_addr - (addrs[i] - 11);\r\nEMIT1_off32(0xE9, jmp_offset);\r\n}\r\nif (BPF_CLASS(insn->code) == BPF_ALU64)\r\nEMIT3(0x49, 0xF7, 0xF3);\r\nelse\r\nEMIT3(0x41, 0xF7, 0xF3);\r\nif (BPF_OP(insn->code) == BPF_MOD)\r\nEMIT3(0x49, 0x89, 0xD3);\r\nelse\r\nEMIT3(0x49, 0x89, 0xC3);\r\nEMIT1(0x5A);\r\nEMIT1(0x58);\r\nEMIT_mov(dst_reg, AUX_REG);\r\nbreak;\r\ncase BPF_ALU | BPF_MUL | BPF_K:\r\ncase BPF_ALU | BPF_MUL | BPF_X:\r\ncase BPF_ALU64 | BPF_MUL | BPF_K:\r\ncase BPF_ALU64 | BPF_MUL | BPF_X:\r\nEMIT1(0x50);\r\nEMIT1(0x52);\r\nEMIT_mov(AUX_REG, dst_reg);\r\nif (BPF_SRC(insn->code) == BPF_X)\r\nEMIT_mov(BPF_REG_0, src_reg);\r\nelse\r\nEMIT3_off32(0x48, 0xC7, 0xC0, imm32);\r\nif (BPF_CLASS(insn->code) == BPF_ALU64)\r\nEMIT1(add_1mod(0x48, AUX_REG));\r\nelse if (is_ereg(AUX_REG))\r\nEMIT1(add_1mod(0x40, AUX_REG));\r\nEMIT2(0xF7, add_1reg(0xE0, AUX_REG));\r\nEMIT_mov(AUX_REG, BPF_REG_0);\r\nEMIT1(0x5A);\r\nEMIT1(0x58);\r\nEMIT_mov(dst_reg, AUX_REG);\r\nbreak;\r\ncase BPF_ALU | BPF_LSH | BPF_K:\r\ncase BPF_ALU | BPF_RSH | BPF_K:\r\ncase BPF_ALU | BPF_ARSH | BPF_K:\r\ncase BPF_ALU64 | BPF_LSH | BPF_K:\r\ncase BPF_ALU64 | BPF_RSH | BPF_K:\r\ncase BPF_ALU64 | BPF_ARSH | BPF_K:\r\nif (BPF_CLASS(insn->code) == BPF_ALU64)\r\nEMIT1(add_1mod(0x48, dst_reg));\r\nelse if (is_ereg(dst_reg))\r\nEMIT1(add_1mod(0x40, dst_reg));\r\nswitch (BPF_OP(insn->code)) {\r\ncase BPF_LSH: b3 = 0xE0; break;\r\ncase BPF_RSH: b3 = 0xE8; break;\r\ncase BPF_ARSH: b3 = 0xF8; break;\r\n}\r\nEMIT3(0xC1, add_1reg(b3, dst_reg), imm32);\r\nbreak;\r\ncase BPF_ALU | BPF_LSH | BPF_X:\r\ncase BPF_ALU | BPF_RSH | BPF_X:\r\ncase BPF_ALU | BPF_ARSH | BPF_X:\r\ncase BPF_ALU64 | BPF_LSH | BPF_X:\r\ncase BPF_ALU64 | BPF_RSH | BPF_X:\r\ncase BPF_ALU64 | BPF_ARSH | BPF_X:\r\nif (dst_reg == BPF_REG_4) {\r\nEMIT_mov(AUX_REG, dst_reg);\r\ndst_reg = AUX_REG;\r\n}\r\nif (src_reg != BPF_REG_4) {\r\nEMIT1(0x51);\r\nEMIT_mov(BPF_REG_4, src_reg);\r\n}\r\nif (BPF_CLASS(insn->code) == BPF_ALU64)\r\nEMIT1(add_1mod(0x48, dst_reg));\r\nelse if (is_ereg(dst_reg))\r\nEMIT1(add_1mod(0x40, dst_reg));\r\nswitch (BPF_OP(insn->code)) {\r\ncase BPF_LSH: b3 = 0xE0; break;\r\ncase BPF_RSH: b3 = 0xE8; break;\r\ncase BPF_ARSH: b3 = 0xF8; break;\r\n}\r\nEMIT2(0xD3, add_1reg(b3, dst_reg));\r\nif (src_reg != BPF_REG_4)\r\nEMIT1(0x59);\r\nif (insn->dst_reg == BPF_REG_4)\r\nEMIT_mov(insn->dst_reg, AUX_REG);\r\nbreak;\r\ncase BPF_ALU | BPF_END | BPF_FROM_BE:\r\nswitch (imm32) {\r\ncase 16:\r\nEMIT1(0x66);\r\nif (is_ereg(dst_reg))\r\nEMIT1(0x41);\r\nEMIT3(0xC1, add_1reg(0xC8, dst_reg), 8);\r\nbreak;\r\ncase 32:\r\nif (is_ereg(dst_reg))\r\nEMIT2(0x41, 0x0F);\r\nelse\r\nEMIT1(0x0F);\r\nEMIT1(add_1reg(0xC8, dst_reg));\r\nbreak;\r\ncase 64:\r\nEMIT3(add_1mod(0x48, dst_reg), 0x0F,\r\nadd_1reg(0xC8, dst_reg));\r\nbreak;\r\n}\r\nbreak;\r\ncase BPF_ALU | BPF_END | BPF_FROM_LE:\r\nbreak;\r\ncase BPF_ST | BPF_MEM | BPF_B:\r\nif (is_ereg(dst_reg))\r\nEMIT2(0x41, 0xC6);\r\nelse\r\nEMIT1(0xC6);\r\ngoto st;\r\ncase BPF_ST | BPF_MEM | BPF_H:\r\nif (is_ereg(dst_reg))\r\nEMIT3(0x66, 0x41, 0xC7);\r\nelse\r\nEMIT2(0x66, 0xC7);\r\ngoto st;\r\ncase BPF_ST | BPF_MEM | BPF_W:\r\nif (is_ereg(dst_reg))\r\nEMIT2(0x41, 0xC7);\r\nelse\r\nEMIT1(0xC7);\r\ngoto st;\r\ncase BPF_ST | BPF_MEM | BPF_DW:\r\nEMIT2(add_1mod(0x48, dst_reg), 0xC7);\r\nst: if (is_imm8(insn->off))\r\nEMIT2(add_1reg(0x40, dst_reg), insn->off);\r\nelse\r\nEMIT1_off32(add_1reg(0x80, dst_reg), insn->off);\r\nEMIT(imm32, bpf_size_to_x86_bytes(BPF_SIZE(insn->code)));\r\nbreak;\r\ncase BPF_STX | BPF_MEM | BPF_B:\r\nif (is_ereg(dst_reg) || is_ereg(src_reg) ||\r\nsrc_reg == BPF_REG_1 || src_reg == BPF_REG_2)\r\nEMIT2(add_2mod(0x40, dst_reg, src_reg), 0x88);\r\nelse\r\nEMIT1(0x88);\r\ngoto stx;\r\ncase BPF_STX | BPF_MEM | BPF_H:\r\nif (is_ereg(dst_reg) || is_ereg(src_reg))\r\nEMIT3(0x66, add_2mod(0x40, dst_reg, src_reg), 0x89);\r\nelse\r\nEMIT2(0x66, 0x89);\r\ngoto stx;\r\ncase BPF_STX | BPF_MEM | BPF_W:\r\nif (is_ereg(dst_reg) || is_ereg(src_reg))\r\nEMIT2(add_2mod(0x40, dst_reg, src_reg), 0x89);\r\nelse\r\nEMIT1(0x89);\r\ngoto stx;\r\ncase BPF_STX | BPF_MEM | BPF_DW:\r\nEMIT2(add_2mod(0x48, dst_reg, src_reg), 0x89);\r\nstx: if (is_imm8(insn->off))\r\nEMIT2(add_2reg(0x40, dst_reg, src_reg), insn->off);\r\nelse\r\nEMIT1_off32(add_2reg(0x80, dst_reg, src_reg),\r\ninsn->off);\r\nbreak;\r\ncase BPF_LDX | BPF_MEM | BPF_B:\r\nEMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xB6);\r\ngoto ldx;\r\ncase BPF_LDX | BPF_MEM | BPF_H:\r\nEMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xB7);\r\ngoto ldx;\r\ncase BPF_LDX | BPF_MEM | BPF_W:\r\nif (is_ereg(dst_reg) || is_ereg(src_reg))\r\nEMIT2(add_2mod(0x40, src_reg, dst_reg), 0x8B);\r\nelse\r\nEMIT1(0x8B);\r\ngoto ldx;\r\ncase BPF_LDX | BPF_MEM | BPF_DW:\r\nEMIT2(add_2mod(0x48, src_reg, dst_reg), 0x8B);\r\nldx:\r\nif (is_imm8(insn->off))\r\nEMIT2(add_2reg(0x40, src_reg, dst_reg), insn->off);\r\nelse\r\nEMIT1_off32(add_2reg(0x80, src_reg, dst_reg),\r\ninsn->off);\r\nbreak;\r\ncase BPF_STX | BPF_XADD | BPF_W:\r\nif (is_ereg(dst_reg) || is_ereg(src_reg))\r\nEMIT3(0xF0, add_2mod(0x40, dst_reg, src_reg), 0x01);\r\nelse\r\nEMIT2(0xF0, 0x01);\r\ngoto xadd;\r\ncase BPF_STX | BPF_XADD | BPF_DW:\r\nEMIT3(0xF0, add_2mod(0x48, dst_reg, src_reg), 0x01);\r\nxadd: if (is_imm8(insn->off))\r\nEMIT2(add_2reg(0x40, dst_reg, src_reg), insn->off);\r\nelse\r\nEMIT1_off32(add_2reg(0x80, dst_reg, src_reg),\r\ninsn->off);\r\nbreak;\r\ncase BPF_JMP | BPF_CALL:\r\nfunc = (u8 *) __bpf_call_base + imm32;\r\njmp_offset = func - (image + addrs[i]);\r\nif (seen_ld_abs) {\r\nEMIT2(0x41, 0x52);\r\nEMIT2(0x41, 0x51);\r\njmp_offset += 4;\r\n}\r\nif (!imm32 || !is_simm32(jmp_offset)) {\r\npr_err("unsupported bpf func %d addr %p image %p\n",\r\nimm32, func, image);\r\nreturn -EINVAL;\r\n}\r\nEMIT1_off32(0xE8, jmp_offset);\r\nif (seen_ld_abs) {\r\nEMIT2(0x41, 0x59);\r\nEMIT2(0x41, 0x5A);\r\n}\r\nbreak;\r\ncase BPF_JMP | BPF_JEQ | BPF_X:\r\ncase BPF_JMP | BPF_JNE | BPF_X:\r\ncase BPF_JMP | BPF_JGT | BPF_X:\r\ncase BPF_JMP | BPF_JGE | BPF_X:\r\ncase BPF_JMP | BPF_JSGT | BPF_X:\r\ncase BPF_JMP | BPF_JSGE | BPF_X:\r\nEMIT3(add_2mod(0x48, dst_reg, src_reg), 0x39,\r\nadd_2reg(0xC0, dst_reg, src_reg));\r\ngoto emit_cond_jmp;\r\ncase BPF_JMP | BPF_JSET | BPF_X:\r\nEMIT3(add_2mod(0x48, dst_reg, src_reg), 0x85,\r\nadd_2reg(0xC0, dst_reg, src_reg));\r\ngoto emit_cond_jmp;\r\ncase BPF_JMP | BPF_JSET | BPF_K:\r\nEMIT1(add_1mod(0x48, dst_reg));\r\nEMIT2_off32(0xF7, add_1reg(0xC0, dst_reg), imm32);\r\ngoto emit_cond_jmp;\r\ncase BPF_JMP | BPF_JEQ | BPF_K:\r\ncase BPF_JMP | BPF_JNE | BPF_K:\r\ncase BPF_JMP | BPF_JGT | BPF_K:\r\ncase BPF_JMP | BPF_JGE | BPF_K:\r\ncase BPF_JMP | BPF_JSGT | BPF_K:\r\ncase BPF_JMP | BPF_JSGE | BPF_K:\r\nEMIT1(add_1mod(0x48, dst_reg));\r\nif (is_imm8(imm32))\r\nEMIT3(0x83, add_1reg(0xF8, dst_reg), imm32);\r\nelse\r\nEMIT2_off32(0x81, add_1reg(0xF8, dst_reg), imm32);\r\nemit_cond_jmp:\r\nswitch (BPF_OP(insn->code)) {\r\ncase BPF_JEQ:\r\njmp_cond = X86_JE;\r\nbreak;\r\ncase BPF_JSET:\r\ncase BPF_JNE:\r\njmp_cond = X86_JNE;\r\nbreak;\r\ncase BPF_JGT:\r\njmp_cond = X86_JA;\r\nbreak;\r\ncase BPF_JGE:\r\njmp_cond = X86_JAE;\r\nbreak;\r\ncase BPF_JSGT:\r\njmp_cond = X86_JG;\r\nbreak;\r\ncase BPF_JSGE:\r\njmp_cond = X86_JGE;\r\nbreak;\r\ndefault:\r\nreturn -EFAULT;\r\n}\r\njmp_offset = addrs[i + insn->off] - addrs[i];\r\nif (is_imm8(jmp_offset)) {\r\nEMIT2(jmp_cond, jmp_offset);\r\n} else if (is_simm32(jmp_offset)) {\r\nEMIT2_off32(0x0F, jmp_cond + 0x10, jmp_offset);\r\n} else {\r\npr_err("cond_jmp gen bug %llx\n", jmp_offset);\r\nreturn -EFAULT;\r\n}\r\nbreak;\r\ncase BPF_JMP | BPF_JA:\r\njmp_offset = addrs[i + insn->off] - addrs[i];\r\nif (!jmp_offset)\r\nbreak;\r\nemit_jmp:\r\nif (is_imm8(jmp_offset)) {\r\nEMIT2(0xEB, jmp_offset);\r\n} else if (is_simm32(jmp_offset)) {\r\nEMIT1_off32(0xE9, jmp_offset);\r\n} else {\r\npr_err("jmp gen bug %llx\n", jmp_offset);\r\nreturn -EFAULT;\r\n}\r\nbreak;\r\ncase BPF_LD | BPF_IND | BPF_W:\r\nfunc = sk_load_word;\r\ngoto common_load;\r\ncase BPF_LD | BPF_ABS | BPF_W:\r\nfunc = CHOOSE_LOAD_FUNC(imm32, sk_load_word);\r\ncommon_load:\r\nctx->seen_ld_abs = seen_ld_abs = true;\r\njmp_offset = func - (image + addrs[i]);\r\nif (!func || !is_simm32(jmp_offset)) {\r\npr_err("unsupported bpf func %d addr %p image %p\n",\r\nimm32, func, image);\r\nreturn -EINVAL;\r\n}\r\nif (BPF_MODE(insn->code) == BPF_ABS) {\r\nEMIT1_off32(0xBE, imm32);\r\n} else {\r\nEMIT_mov(BPF_REG_2, src_reg);\r\nif (imm32) {\r\nif (is_imm8(imm32))\r\nEMIT3(0x83, 0xC6, imm32);\r\nelse\r\nEMIT2_off32(0x81, 0xC6, imm32);\r\n}\r\n}\r\nEMIT1_off32(0xE8, jmp_offset);\r\nbreak;\r\ncase BPF_LD | BPF_IND | BPF_H:\r\nfunc = sk_load_half;\r\ngoto common_load;\r\ncase BPF_LD | BPF_ABS | BPF_H:\r\nfunc = CHOOSE_LOAD_FUNC(imm32, sk_load_half);\r\ngoto common_load;\r\ncase BPF_LD | BPF_IND | BPF_B:\r\nfunc = sk_load_byte;\r\ngoto common_load;\r\ncase BPF_LD | BPF_ABS | BPF_B:\r\nfunc = CHOOSE_LOAD_FUNC(imm32, sk_load_byte);\r\ngoto common_load;\r\ncase BPF_JMP | BPF_EXIT:\r\nif (seen_exit) {\r\njmp_offset = ctx->cleanup_addr - addrs[i];\r\ngoto emit_jmp;\r\n}\r\nseen_exit = true;\r\nctx->cleanup_addr = proglen;\r\nEMIT3_off32(0x48, 0x8B, 0x9D, -stacksize);\r\nEMIT3_off32(0x4C, 0x8B, 0xAD, -stacksize + 8);\r\nEMIT3_off32(0x4C, 0x8B, 0xB5, -stacksize + 16);\r\nEMIT3_off32(0x4C, 0x8B, 0xBD, -stacksize + 24);\r\nEMIT1(0xC9);\r\nEMIT1(0xC3);\r\nbreak;\r\ndefault:\r\npr_err("bpf_jit: unknown opcode %02x\n", insn->code);\r\nreturn -EINVAL;\r\n}\r\nilen = prog - temp;\r\nif (ilen > BPF_MAX_INSN_SIZE) {\r\npr_err("bpf_jit_compile fatal insn size error\n");\r\nreturn -EFAULT;\r\n}\r\nif (image) {\r\nif (unlikely(proglen + ilen > oldproglen)) {\r\npr_err("bpf_jit_compile fatal error\n");\r\nreturn -EFAULT;\r\n}\r\nmemcpy(image + proglen, temp, ilen);\r\n}\r\nproglen += ilen;\r\naddrs[i] = proglen;\r\nprog = temp;\r\n}\r\nreturn proglen;\r\n}\r\nvoid bpf_jit_compile(struct bpf_prog *prog)\r\n{\r\n}\r\nvoid bpf_int_jit_compile(struct bpf_prog *prog)\r\n{\r\nstruct bpf_binary_header *header = NULL;\r\nint proglen, oldproglen = 0;\r\nstruct jit_context ctx = {};\r\nu8 *image = NULL;\r\nint *addrs;\r\nint pass;\r\nint i;\r\nif (!bpf_jit_enable)\r\nreturn;\r\nif (!prog || !prog->len)\r\nreturn;\r\naddrs = kmalloc(prog->len * sizeof(*addrs), GFP_KERNEL);\r\nif (!addrs)\r\nreturn;\r\nfor (proglen = 0, i = 0; i < prog->len; i++) {\r\nproglen += 64;\r\naddrs[i] = proglen;\r\n}\r\nctx.cleanup_addr = proglen;\r\nfor (pass = 0; pass < 10; pass++) {\r\nproglen = do_jit(prog, addrs, image, oldproglen, &ctx);\r\nif (proglen <= 0) {\r\nimage = NULL;\r\nif (header)\r\nbpf_jit_binary_free(header);\r\ngoto out;\r\n}\r\nif (image) {\r\nif (proglen != oldproglen) {\r\npr_err("bpf_jit: proglen=%d != oldproglen=%d\n",\r\nproglen, oldproglen);\r\ngoto out;\r\n}\r\nbreak;\r\n}\r\nif (proglen == oldproglen) {\r\nheader = bpf_jit_binary_alloc(proglen, &image,\r\n1, jit_fill_hole);\r\nif (!header)\r\ngoto out;\r\n}\r\noldproglen = proglen;\r\n}\r\nif (bpf_jit_enable > 1)\r\nbpf_jit_dump(prog->len, proglen, 0, image);\r\nif (image) {\r\nbpf_flush_icache(header, image + proglen);\r\nset_memory_ro((unsigned long)header, header->pages);\r\nprog->bpf_func = (void *)image;\r\nprog->jited = true;\r\n}\r\nout:\r\nkfree(addrs);\r\n}\r\nvoid bpf_jit_free(struct bpf_prog *fp)\r\n{\r\nunsigned long addr = (unsigned long)fp->bpf_func & PAGE_MASK;\r\nstruct bpf_binary_header *header = (void *)addr;\r\nif (!fp->jited)\r\ngoto free_filter;\r\nset_memory_rw(addr, header->pages);\r\nbpf_jit_binary_free(header);\r\nfree_filter:\r\nbpf_prog_unlock_free(fp);\r\n}
