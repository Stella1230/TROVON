static inline struct sfq_head *sfq_dep_head(struct sfq_sched_data *q, sfq_index val)\r\n{\r\nif (val < SFQ_SLOTS)\r\nreturn &q->slots[val].dep;\r\nreturn &q->dep[val - SFQ_SLOTS];\r\n}\r\nstatic unsigned int sfq_fold_hash(struct sfq_sched_data *q, u32 h, u32 h1)\r\n{\r\nreturn jhash_2words(h, h1, q->perturbation) & (q->divisor - 1);\r\n}\r\nstatic unsigned int sfq_hash(struct sfq_sched_data *q, struct sk_buff *skb)\r\n{\r\nu32 h, h2;\r\nswitch (skb->protocol) {\r\ncase htons(ETH_P_IP):\r\n{\r\nconst struct iphdr *iph;\r\nint poff;\r\nif (!pskb_network_may_pull(skb, sizeof(*iph)))\r\ngoto err;\r\niph = ip_hdr(skb);\r\nh = (__force u32)iph->daddr;\r\nh2 = (__force u32)iph->saddr ^ iph->protocol;\r\nif (ip_is_fragment(iph))\r\nbreak;\r\npoff = proto_ports_offset(iph->protocol);\r\nif (poff >= 0 &&\r\npskb_network_may_pull(skb, iph->ihl * 4 + 4 + poff)) {\r\niph = ip_hdr(skb);\r\nh2 ^= *(u32 *)((void *)iph + iph->ihl * 4 + poff);\r\n}\r\nbreak;\r\n}\r\ncase htons(ETH_P_IPV6):\r\n{\r\nconst struct ipv6hdr *iph;\r\nint poff;\r\nif (!pskb_network_may_pull(skb, sizeof(*iph)))\r\ngoto err;\r\niph = ipv6_hdr(skb);\r\nh = (__force u32)iph->daddr.s6_addr32[3];\r\nh2 = (__force u32)iph->saddr.s6_addr32[3] ^ iph->nexthdr;\r\npoff = proto_ports_offset(iph->nexthdr);\r\nif (poff >= 0 &&\r\npskb_network_may_pull(skb, sizeof(*iph) + 4 + poff)) {\r\niph = ipv6_hdr(skb);\r\nh2 ^= *(u32 *)((void *)iph + sizeof(*iph) + poff);\r\n}\r\nbreak;\r\n}\r\ndefault:\r\nerr:\r\nh = (unsigned long)skb_dst(skb) ^ (__force u32)skb->protocol;\r\nh2 = (unsigned long)skb->sk;\r\n}\r\nreturn sfq_fold_hash(q, h, h2);\r\n}\r\nstatic unsigned int sfq_classify(struct sk_buff *skb, struct Qdisc *sch,\r\nint *qerr)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nstruct tcf_result res;\r\nint result;\r\nif (TC_H_MAJ(skb->priority) == sch->handle &&\r\nTC_H_MIN(skb->priority) > 0 &&\r\nTC_H_MIN(skb->priority) <= q->divisor)\r\nreturn TC_H_MIN(skb->priority);\r\nif (!q->filter_list)\r\nreturn sfq_hash(q, skb) + 1;\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_BYPASS;\r\nresult = tc_classify(skb, q->filter_list, &res);\r\nif (result >= 0) {\r\n#ifdef CONFIG_NET_CLS_ACT\r\nswitch (result) {\r\ncase TC_ACT_STOLEN:\r\ncase TC_ACT_QUEUED:\r\n*qerr = NET_XMIT_SUCCESS | __NET_XMIT_STOLEN;\r\ncase TC_ACT_SHOT:\r\nreturn 0;\r\n}\r\n#endif\r\nif (TC_H_MIN(res.classid) <= q->divisor)\r\nreturn TC_H_MIN(res.classid);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void sfq_link(struct sfq_sched_data *q, sfq_index x)\r\n{\r\nsfq_index p, n;\r\nint qlen = q->slots[x].qlen;\r\np = qlen + SFQ_SLOTS;\r\nn = q->dep[qlen].next;\r\nq->slots[x].dep.next = n;\r\nq->slots[x].dep.prev = p;\r\nq->dep[qlen].next = x;\r\nsfq_dep_head(q, n)->prev = x;\r\n}\r\nstatic inline void sfq_dec(struct sfq_sched_data *q, sfq_index x)\r\n{\r\nsfq_index p, n;\r\nint d;\r\nsfq_unlink(q, x, n, p);\r\nd = q->slots[x].qlen--;\r\nif (n == p && q->cur_depth == d)\r\nq->cur_depth--;\r\nsfq_link(q, x);\r\n}\r\nstatic inline void sfq_inc(struct sfq_sched_data *q, sfq_index x)\r\n{\r\nsfq_index p, n;\r\nint d;\r\nsfq_unlink(q, x, n, p);\r\nd = ++q->slots[x].qlen;\r\nif (q->cur_depth < d)\r\nq->cur_depth = d;\r\nsfq_link(q, x);\r\n}\r\nstatic inline struct sk_buff *slot_dequeue_tail(struct sfq_slot *slot)\r\n{\r\nstruct sk_buff *skb = slot->skblist_prev;\r\nslot->skblist_prev = skb->prev;\r\nskb->prev->next = (struct sk_buff *)slot;\r\nskb->next = skb->prev = NULL;\r\nreturn skb;\r\n}\r\nstatic inline struct sk_buff *slot_dequeue_head(struct sfq_slot *slot)\r\n{\r\nstruct sk_buff *skb = slot->skblist_next;\r\nslot->skblist_next = skb->next;\r\nskb->next->prev = (struct sk_buff *)slot;\r\nskb->next = skb->prev = NULL;\r\nreturn skb;\r\n}\r\nstatic inline void slot_queue_init(struct sfq_slot *slot)\r\n{\r\nslot->skblist_prev = slot->skblist_next = (struct sk_buff *)slot;\r\n}\r\nstatic inline void slot_queue_add(struct sfq_slot *slot, struct sk_buff *skb)\r\n{\r\nskb->prev = slot->skblist_prev;\r\nskb->next = (struct sk_buff *)slot;\r\nslot->skblist_prev->next = skb;\r\nslot->skblist_prev = skb;\r\n}\r\nstatic unsigned int sfq_drop(struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nsfq_index x, d = q->cur_depth;\r\nstruct sk_buff *skb;\r\nunsigned int len;\r\nstruct sfq_slot *slot;\r\nif (d > 1) {\r\nx = q->dep[d].next;\r\nslot = &q->slots[x];\r\ndrop:\r\nskb = slot_dequeue_tail(slot);\r\nlen = qdisc_pkt_len(skb);\r\nsfq_dec(q, x);\r\nkfree_skb(skb);\r\nsch->q.qlen--;\r\nsch->qstats.drops++;\r\nsch->qstats.backlog -= len;\r\nreturn len;\r\n}\r\nif (d == 1) {\r\nx = q->tail->next;\r\nslot = &q->slots[x];\r\nq->tail->next = slot->next;\r\nq->ht[slot->hash] = SFQ_EMPTY_SLOT;\r\ngoto drop;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nsfq_enqueue(struct sk_buff *skb, struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nunsigned int hash;\r\nsfq_index x, qlen;\r\nstruct sfq_slot *slot;\r\nint uninitialized_var(ret);\r\nhash = sfq_classify(skb, sch, &ret);\r\nif (hash == 0) {\r\nif (ret & __NET_XMIT_BYPASS)\r\nsch->qstats.drops++;\r\nkfree_skb(skb);\r\nreturn ret;\r\n}\r\nhash--;\r\nx = q->ht[hash];\r\nslot = &q->slots[x];\r\nif (x == SFQ_EMPTY_SLOT) {\r\nx = q->dep[0].next;\r\nq->ht[hash] = x;\r\nslot = &q->slots[x];\r\nslot->hash = hash;\r\n}\r\nif (slot->qlen >= q->limit)\r\nreturn qdisc_drop(skb, sch);\r\nsch->qstats.backlog += qdisc_pkt_len(skb);\r\nslot_queue_add(slot, skb);\r\nsfq_inc(q, x);\r\nif (slot->qlen == 1) {\r\nif (q->tail == NULL) {\r\nslot->next = x;\r\n} else {\r\nslot->next = q->tail->next;\r\nq->tail->next = x;\r\n}\r\nq->tail = slot;\r\nslot->allot = q->scaled_quantum;\r\n}\r\nif (++sch->q.qlen <= q->limit)\r\nreturn NET_XMIT_SUCCESS;\r\nqlen = slot->qlen;\r\nsfq_drop(sch);\r\nif (qlen != slot->qlen)\r\nreturn NET_XMIT_CN;\r\nqdisc_tree_decrease_qlen(sch, 1);\r\nreturn NET_XMIT_SUCCESS;\r\n}\r\nstatic struct sk_buff *\r\nsfq_dequeue(struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nstruct sk_buff *skb;\r\nsfq_index a, next_a;\r\nstruct sfq_slot *slot;\r\nif (q->tail == NULL)\r\nreturn NULL;\r\nnext_slot:\r\na = q->tail->next;\r\nslot = &q->slots[a];\r\nif (slot->allot <= 0) {\r\nq->tail = slot;\r\nslot->allot += q->scaled_quantum;\r\ngoto next_slot;\r\n}\r\nskb = slot_dequeue_head(slot);\r\nsfq_dec(q, a);\r\nqdisc_bstats_update(sch, skb);\r\nsch->q.qlen--;\r\nsch->qstats.backlog -= qdisc_pkt_len(skb);\r\nif (slot->qlen == 0) {\r\nq->ht[slot->hash] = SFQ_EMPTY_SLOT;\r\nnext_a = slot->next;\r\nif (a == next_a) {\r\nq->tail = NULL;\r\nreturn skb;\r\n}\r\nq->tail->next = next_a;\r\n} else {\r\nslot->allot -= SFQ_ALLOT_SIZE(qdisc_pkt_len(skb));\r\n}\r\nreturn skb;\r\n}\r\nstatic void\r\nsfq_reset(struct Qdisc *sch)\r\n{\r\nstruct sk_buff *skb;\r\nwhile ((skb = sfq_dequeue(sch)) != NULL)\r\nkfree_skb(skb);\r\n}\r\nstatic void sfq_perturbation(unsigned long arg)\r\n{\r\nstruct Qdisc *sch = (struct Qdisc *)arg;\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nq->perturbation = net_random();\r\nif (q->perturb_period)\r\nmod_timer(&q->perturb_timer, jiffies + q->perturb_period);\r\n}\r\nstatic int sfq_change(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nstruct tc_sfq_qopt *ctl = nla_data(opt);\r\nunsigned int qlen;\r\nif (opt->nla_len < nla_attr_size(sizeof(*ctl)))\r\nreturn -EINVAL;\r\nif (ctl->divisor &&\r\n(!is_power_of_2(ctl->divisor) || ctl->divisor > 65536))\r\nreturn -EINVAL;\r\nsch_tree_lock(sch);\r\nq->quantum = ctl->quantum ? : psched_mtu(qdisc_dev(sch));\r\nq->scaled_quantum = SFQ_ALLOT_SIZE(q->quantum);\r\nq->perturb_period = ctl->perturb_period * HZ;\r\nif (ctl->limit)\r\nq->limit = min_t(u32, ctl->limit, SFQ_DEPTH - 1);\r\nif (ctl->divisor)\r\nq->divisor = ctl->divisor;\r\nqlen = sch->q.qlen;\r\nwhile (sch->q.qlen > q->limit)\r\nsfq_drop(sch);\r\nqdisc_tree_decrease_qlen(sch, qlen - sch->q.qlen);\r\ndel_timer(&q->perturb_timer);\r\nif (q->perturb_period) {\r\nmod_timer(&q->perturb_timer, jiffies + q->perturb_period);\r\nq->perturbation = net_random();\r\n}\r\nsch_tree_unlock(sch);\r\nreturn 0;\r\n}\r\nstatic int sfq_init(struct Qdisc *sch, struct nlattr *opt)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nsize_t sz;\r\nint i;\r\nq->perturb_timer.function = sfq_perturbation;\r\nq->perturb_timer.data = (unsigned long)sch;\r\ninit_timer_deferrable(&q->perturb_timer);\r\nfor (i = 0; i < SFQ_DEPTH; i++) {\r\nq->dep[i].next = i + SFQ_SLOTS;\r\nq->dep[i].prev = i + SFQ_SLOTS;\r\n}\r\nq->limit = SFQ_DEPTH - 1;\r\nq->cur_depth = 0;\r\nq->tail = NULL;\r\nq->divisor = SFQ_DEFAULT_HASH_DIVISOR;\r\nif (opt == NULL) {\r\nq->quantum = psched_mtu(qdisc_dev(sch));\r\nq->scaled_quantum = SFQ_ALLOT_SIZE(q->quantum);\r\nq->perturb_period = 0;\r\nq->perturbation = net_random();\r\n} else {\r\nint err = sfq_change(sch, opt);\r\nif (err)\r\nreturn err;\r\n}\r\nsz = sizeof(q->ht[0]) * q->divisor;\r\nq->ht = kmalloc(sz, GFP_KERNEL);\r\nif (!q->ht && sz > PAGE_SIZE)\r\nq->ht = vmalloc(sz);\r\nif (!q->ht)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < q->divisor; i++)\r\nq->ht[i] = SFQ_EMPTY_SLOT;\r\nfor (i = 0; i < SFQ_SLOTS; i++) {\r\nslot_queue_init(&q->slots[i]);\r\nsfq_link(q, i);\r\n}\r\nif (q->limit >= 1)\r\nsch->flags |= TCQ_F_CAN_BYPASS;\r\nelse\r\nsch->flags &= ~TCQ_F_CAN_BYPASS;\r\nreturn 0;\r\n}\r\nstatic void sfq_destroy(struct Qdisc *sch)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\ntcf_destroy_chain(&q->filter_list);\r\nq->perturb_period = 0;\r\ndel_timer_sync(&q->perturb_timer);\r\nif (is_vmalloc_addr(q->ht))\r\nvfree(q->ht);\r\nelse\r\nkfree(q->ht);\r\n}\r\nstatic int sfq_dump(struct Qdisc *sch, struct sk_buff *skb)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nunsigned char *b = skb_tail_pointer(skb);\r\nstruct tc_sfq_qopt opt;\r\nopt.quantum = q->quantum;\r\nopt.perturb_period = q->perturb_period / HZ;\r\nopt.limit = q->limit;\r\nopt.divisor = q->divisor;\r\nopt.flows = q->limit;\r\nNLA_PUT(skb, TCA_OPTIONS, sizeof(opt), &opt);\r\nreturn skb->len;\r\nnla_put_failure:\r\nnlmsg_trim(skb, b);\r\nreturn -1;\r\n}\r\nstatic struct Qdisc *sfq_leaf(struct Qdisc *sch, unsigned long arg)\r\n{\r\nreturn NULL;\r\n}\r\nstatic unsigned long sfq_get(struct Qdisc *sch, u32 classid)\r\n{\r\nreturn 0;\r\n}\r\nstatic unsigned long sfq_bind(struct Qdisc *sch, unsigned long parent,\r\nu32 classid)\r\n{\r\nsch->flags &= ~TCQ_F_CAN_BYPASS;\r\nreturn 0;\r\n}\r\nstatic void sfq_put(struct Qdisc *q, unsigned long cl)\r\n{\r\n}\r\nstatic struct tcf_proto **sfq_find_tcf(struct Qdisc *sch, unsigned long cl)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nif (cl)\r\nreturn NULL;\r\nreturn &q->filter_list;\r\n}\r\nstatic int sfq_dump_class(struct Qdisc *sch, unsigned long cl,\r\nstruct sk_buff *skb, struct tcmsg *tcm)\r\n{\r\ntcm->tcm_handle |= TC_H_MIN(cl);\r\nreturn 0;\r\n}\r\nstatic int sfq_dump_class_stats(struct Qdisc *sch, unsigned long cl,\r\nstruct gnet_dump *d)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nsfq_index idx = q->ht[cl - 1];\r\nstruct gnet_stats_queue qs = { 0 };\r\nstruct tc_sfq_xstats xstats = { 0 };\r\nstruct sk_buff *skb;\r\nif (idx != SFQ_EMPTY_SLOT) {\r\nconst struct sfq_slot *slot = &q->slots[idx];\r\nxstats.allot = slot->allot << SFQ_ALLOT_SHIFT;\r\nqs.qlen = slot->qlen;\r\nslot_queue_walk(slot, skb)\r\nqs.backlog += qdisc_pkt_len(skb);\r\n}\r\nif (gnet_stats_copy_queue(d, &qs) < 0)\r\nreturn -1;\r\nreturn gnet_stats_copy_app(d, &xstats, sizeof(xstats));\r\n}\r\nstatic void sfq_walk(struct Qdisc *sch, struct qdisc_walker *arg)\r\n{\r\nstruct sfq_sched_data *q = qdisc_priv(sch);\r\nunsigned int i;\r\nif (arg->stop)\r\nreturn;\r\nfor (i = 0; i < q->divisor; i++) {\r\nif (q->ht[i] == SFQ_EMPTY_SLOT ||\r\narg->count < arg->skip) {\r\narg->count++;\r\ncontinue;\r\n}\r\nif (arg->fn(sch, i + 1, arg) < 0) {\r\narg->stop = 1;\r\nbreak;\r\n}\r\narg->count++;\r\n}\r\n}\r\nstatic int __init sfq_module_init(void)\r\n{\r\nreturn register_qdisc(&sfq_qdisc_ops);\r\n}\r\nstatic void __exit sfq_module_exit(void)\r\n{\r\nunregister_qdisc(&sfq_qdisc_ops);\r\n}
