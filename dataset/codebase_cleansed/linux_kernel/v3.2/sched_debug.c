static long long nsec_high(unsigned long long nsec)\r\n{\r\nif ((long long)nsec < 0) {\r\nnsec = -nsec;\r\ndo_div(nsec, 1000000);\r\nreturn -nsec;\r\n}\r\ndo_div(nsec, 1000000);\r\nreturn nsec;\r\n}\r\nstatic unsigned long nsec_low(unsigned long long nsec)\r\n{\r\nif ((long long)nsec < 0)\r\nnsec = -nsec;\r\nreturn do_div(nsec, 1000000);\r\n}\r\nstatic void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group *tg)\r\n{\r\nstruct sched_entity *se = tg->se[cpu];\r\nif (!se)\r\nreturn;\r\n#define P(F) \\r\nSEQ_printf(m, " .%-30s: %lld\n", #F, (long long)F)\r\n#define PN(F) \\r\nSEQ_printf(m, " .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)F))\r\nPN(se->exec_start);\r\nPN(se->vruntime);\r\nPN(se->sum_exec_runtime);\r\n#ifdef CONFIG_SCHEDSTATS\r\nPN(se->statistics.wait_start);\r\nPN(se->statistics.sleep_start);\r\nPN(se->statistics.block_start);\r\nPN(se->statistics.sleep_max);\r\nPN(se->statistics.block_max);\r\nPN(se->statistics.exec_max);\r\nPN(se->statistics.slice_max);\r\nPN(se->statistics.wait_max);\r\nPN(se->statistics.wait_sum);\r\nP(se->statistics.wait_count);\r\n#endif\r\nP(se->load.weight);\r\n#undef PN\r\n#undef P\r\n}\r\nstatic char *task_group_path(struct task_group *tg)\r\n{\r\nif (autogroup_path(tg, group_path, PATH_MAX))\r\nreturn group_path;\r\nif (!tg->css.cgroup) {\r\ngroup_path[0] = '\0';\r\nreturn group_path;\r\n}\r\ncgroup_path(tg->css.cgroup, group_path, PATH_MAX);\r\nreturn group_path;\r\n}\r\nstatic void\r\nprint_task(struct seq_file *m, struct rq *rq, struct task_struct *p)\r\n{\r\nif (rq->curr == p)\r\nSEQ_printf(m, "R");\r\nelse\r\nSEQ_printf(m, " ");\r\nSEQ_printf(m, "%15s %5d %9Ld.%06ld %9Ld %5d ",\r\np->comm, p->pid,\r\nSPLIT_NS(p->se.vruntime),\r\n(long long)(p->nvcsw + p->nivcsw),\r\np->prio);\r\n#ifdef CONFIG_SCHEDSTATS\r\nSEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",\r\nSPLIT_NS(p->se.vruntime),\r\nSPLIT_NS(p->se.sum_exec_runtime),\r\nSPLIT_NS(p->se.statistics.sum_sleep_runtime));\r\n#else\r\nSEQ_printf(m, "%15Ld %15Ld %15Ld.%06ld %15Ld.%06ld %15Ld.%06ld",\r\n0LL, 0LL, 0LL, 0L, 0LL, 0L, 0LL, 0L);\r\n#endif\r\n#ifdef CONFIG_CGROUP_SCHED\r\nSEQ_printf(m, " %s", task_group_path(task_group(p)));\r\n#endif\r\nSEQ_printf(m, "\n");\r\n}\r\nstatic void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)\r\n{\r\nstruct task_struct *g, *p;\r\nunsigned long flags;\r\nSEQ_printf(m,\r\n"\nrunnable tasks:\n"\r\n" task PID tree-key switches prio"\r\n" exec-runtime sum-exec sum-sleep\n"\r\n"------------------------------------------------------"\r\n"----------------------------------------------------\n");\r\nread_lock_irqsave(&tasklist_lock, flags);\r\ndo_each_thread(g, p) {\r\nif (!p->on_rq || task_cpu(p) != rq_cpu)\r\ncontinue;\r\nprint_task(m, rq, p);\r\n} while_each_thread(g, p);\r\nread_unlock_irqrestore(&tasklist_lock, flags);\r\n}\r\nvoid print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)\r\n{\r\ns64 MIN_vruntime = -1, min_vruntime, max_vruntime = -1,\r\nspread, rq0_min_vruntime, spread0;\r\nstruct rq *rq = cpu_rq(cpu);\r\nstruct sched_entity *last;\r\nunsigned long flags;\r\n#ifdef CONFIG_FAIR_GROUP_SCHED\r\nSEQ_printf(m, "\ncfs_rq[%d]:%s\n", cpu, task_group_path(cfs_rq->tg));\r\n#else\r\nSEQ_printf(m, "\ncfs_rq[%d]:\n", cpu);\r\n#endif\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", "exec_clock",\r\nSPLIT_NS(cfs_rq->exec_clock));\r\nraw_spin_lock_irqsave(&rq->lock, flags);\r\nif (cfs_rq->rb_leftmost)\r\nMIN_vruntime = (__pick_first_entity(cfs_rq))->vruntime;\r\nlast = __pick_last_entity(cfs_rq);\r\nif (last)\r\nmax_vruntime = last->vruntime;\r\nmin_vruntime = cfs_rq->min_vruntime;\r\nrq0_min_vruntime = cpu_rq(0)->cfs.min_vruntime;\r\nraw_spin_unlock_irqrestore(&rq->lock, flags);\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", "MIN_vruntime",\r\nSPLIT_NS(MIN_vruntime));\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", "min_vruntime",\r\nSPLIT_NS(min_vruntime));\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", "max_vruntime",\r\nSPLIT_NS(max_vruntime));\r\nspread = max_vruntime - MIN_vruntime;\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", "spread",\r\nSPLIT_NS(spread));\r\nspread0 = min_vruntime - rq0_min_vruntime;\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", "spread0",\r\nSPLIT_NS(spread0));\r\nSEQ_printf(m, " .%-30s: %d\n", "nr_spread_over",\r\ncfs_rq->nr_spread_over);\r\nSEQ_printf(m, " .%-30s: %ld\n", "nr_running", cfs_rq->nr_running);\r\nSEQ_printf(m, " .%-30s: %ld\n", "load", cfs_rq->load.weight);\r\n#ifdef CONFIG_FAIR_GROUP_SCHED\r\n#ifdef CONFIG_SMP\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", "load_avg",\r\nSPLIT_NS(cfs_rq->load_avg));\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", "load_period",\r\nSPLIT_NS(cfs_rq->load_period));\r\nSEQ_printf(m, " .%-30s: %ld\n", "load_contrib",\r\ncfs_rq->load_contribution);\r\nSEQ_printf(m, " .%-30s: %d\n", "load_tg",\r\natomic_read(&cfs_rq->tg->load_weight));\r\n#endif\r\nprint_cfs_group_stats(m, cpu, cfs_rq->tg);\r\n#endif\r\n}\r\nvoid print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)\r\n{\r\n#ifdef CONFIG_RT_GROUP_SCHED\r\nSEQ_printf(m, "\nrt_rq[%d]:%s\n", cpu, task_group_path(rt_rq->tg));\r\n#else\r\nSEQ_printf(m, "\nrt_rq[%d]:\n", cpu);\r\n#endif\r\n#define P(x) \\r\nSEQ_printf(m, " .%-30s: %Ld\n", #x, (long long)(rt_rq->x))\r\n#define PN(x) \\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(rt_rq->x))\r\nP(rt_nr_running);\r\nP(rt_throttled);\r\nPN(rt_time);\r\nPN(rt_runtime);\r\n#undef PN\r\n#undef P\r\n}\r\nstatic void print_cpu(struct seq_file *m, int cpu)\r\n{\r\nstruct rq *rq = cpu_rq(cpu);\r\nunsigned long flags;\r\n#ifdef CONFIG_X86\r\n{\r\nunsigned int freq = cpu_khz ? : 1;\r\nSEQ_printf(m, "\ncpu#%d, %u.%03u MHz\n",\r\ncpu, freq / 1000, (freq % 1000));\r\n}\r\n#else\r\nSEQ_printf(m, "\ncpu#%d\n", cpu);\r\n#endif\r\n#define P(x) \\r\nSEQ_printf(m, " .%-30s: %Ld\n", #x, (long long)(rq->x))\r\n#define PN(x) \\r\nSEQ_printf(m, " .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(rq->x))\r\nP(nr_running);\r\nSEQ_printf(m, " .%-30s: %lu\n", "load",\r\nrq->load.weight);\r\nP(nr_switches);\r\nP(nr_load_updates);\r\nP(nr_uninterruptible);\r\nPN(next_balance);\r\nP(curr->pid);\r\nPN(clock);\r\nP(cpu_load[0]);\r\nP(cpu_load[1]);\r\nP(cpu_load[2]);\r\nP(cpu_load[3]);\r\nP(cpu_load[4]);\r\n#undef P\r\n#undef PN\r\n#ifdef CONFIG_SCHEDSTATS\r\n#define P(n) SEQ_printf(m, " .%-30s: %d\n", #n, rq->n);\r\n#define P64(n) SEQ_printf(m, " .%-30s: %Ld\n", #n, rq->n);\r\nP(yld_count);\r\nP(sched_switch);\r\nP(sched_count);\r\nP(sched_goidle);\r\n#ifdef CONFIG_SMP\r\nP64(avg_idle);\r\n#endif\r\nP(ttwu_count);\r\nP(ttwu_local);\r\n#undef P\r\n#undef P64\r\n#endif\r\nspin_lock_irqsave(&sched_debug_lock, flags);\r\nprint_cfs_stats(m, cpu);\r\nprint_rt_stats(m, cpu);\r\nrcu_read_lock();\r\nprint_rq(m, rq, cpu);\r\nrcu_read_unlock();\r\nspin_unlock_irqrestore(&sched_debug_lock, flags);\r\n}\r\nstatic int sched_debug_show(struct seq_file *m, void *v)\r\n{\r\nu64 ktime, sched_clk, cpu_clk;\r\nunsigned long flags;\r\nint cpu;\r\nlocal_irq_save(flags);\r\nktime = ktime_to_ns(ktime_get());\r\nsched_clk = sched_clock();\r\ncpu_clk = local_clock();\r\nlocal_irq_restore(flags);\r\nSEQ_printf(m, "Sched Debug Version: v0.10, %s %.*s\n",\r\ninit_utsname()->release,\r\n(int)strcspn(init_utsname()->version, " "),\r\ninit_utsname()->version);\r\n#define P(x) \\r\nSEQ_printf(m, "%-40s: %Ld\n", #x, (long long)(x))\r\n#define PN(x) \\r\nSEQ_printf(m, "%-40s: %Ld.%06ld\n", #x, SPLIT_NS(x))\r\nPN(ktime);\r\nPN(sched_clk);\r\nPN(cpu_clk);\r\nP(jiffies);\r\n#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK\r\nP(sched_clock_stable);\r\n#endif\r\n#undef PN\r\n#undef P\r\nSEQ_printf(m, "\n");\r\nSEQ_printf(m, "sysctl_sched\n");\r\n#define P(x) \\r\nSEQ_printf(m, " .%-40s: %Ld\n", #x, (long long)(x))\r\n#define PN(x) \\r\nSEQ_printf(m, " .%-40s: %Ld.%06ld\n", #x, SPLIT_NS(x))\r\nPN(sysctl_sched_latency);\r\nPN(sysctl_sched_min_granularity);\r\nPN(sysctl_sched_wakeup_granularity);\r\nP(sysctl_sched_child_runs_first);\r\nP(sysctl_sched_features);\r\n#undef PN\r\n#undef P\r\nSEQ_printf(m, " .%-40s: %d (%s)\n", "sysctl_sched_tunable_scaling",\r\nsysctl_sched_tunable_scaling,\r\nsched_tunable_scaling_names[sysctl_sched_tunable_scaling]);\r\nfor_each_online_cpu(cpu)\r\nprint_cpu(m, cpu);\r\nSEQ_printf(m, "\n");\r\nreturn 0;\r\n}\r\nstatic void sysrq_sched_debug_show(void)\r\n{\r\nsched_debug_show(NULL, NULL);\r\n}\r\nstatic int sched_debug_open(struct inode *inode, struct file *filp)\r\n{\r\nreturn single_open(filp, sched_debug_show, NULL);\r\n}\r\nstatic int __init init_sched_debug_procfs(void)\r\n{\r\nstruct proc_dir_entry *pe;\r\npe = proc_create("sched_debug", 0444, NULL, &sched_debug_fops);\r\nif (!pe)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid proc_sched_show_task(struct task_struct *p, struct seq_file *m)\r\n{\r\nunsigned long nr_switches;\r\nSEQ_printf(m, "%s (%d, #threads: %d)\n", p->comm, p->pid,\r\nget_nr_threads(p));\r\nSEQ_printf(m,\r\n"---------------------------------------------------------\n");\r\n#define __P(F) \\r\nSEQ_printf(m, "%-35s:%21Ld\n", #F, (long long)F)\r\n#define P(F) \\r\nSEQ_printf(m, "%-35s:%21Ld\n", #F, (long long)p->F)\r\n#define __PN(F) \\r\nSEQ_printf(m, "%-35s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))\r\n#define PN(F) \\r\nSEQ_printf(m, "%-35s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))\r\nPN(se.exec_start);\r\nPN(se.vruntime);\r\nPN(se.sum_exec_runtime);\r\nnr_switches = p->nvcsw + p->nivcsw;\r\n#ifdef CONFIG_SCHEDSTATS\r\nPN(se.statistics.wait_start);\r\nPN(se.statistics.sleep_start);\r\nPN(se.statistics.block_start);\r\nPN(se.statistics.sleep_max);\r\nPN(se.statistics.block_max);\r\nPN(se.statistics.exec_max);\r\nPN(se.statistics.slice_max);\r\nPN(se.statistics.wait_max);\r\nPN(se.statistics.wait_sum);\r\nP(se.statistics.wait_count);\r\nPN(se.statistics.iowait_sum);\r\nP(se.statistics.iowait_count);\r\nP(se.nr_migrations);\r\nP(se.statistics.nr_migrations_cold);\r\nP(se.statistics.nr_failed_migrations_affine);\r\nP(se.statistics.nr_failed_migrations_running);\r\nP(se.statistics.nr_failed_migrations_hot);\r\nP(se.statistics.nr_forced_migrations);\r\nP(se.statistics.nr_wakeups);\r\nP(se.statistics.nr_wakeups_sync);\r\nP(se.statistics.nr_wakeups_migrate);\r\nP(se.statistics.nr_wakeups_local);\r\nP(se.statistics.nr_wakeups_remote);\r\nP(se.statistics.nr_wakeups_affine);\r\nP(se.statistics.nr_wakeups_affine_attempts);\r\nP(se.statistics.nr_wakeups_passive);\r\nP(se.statistics.nr_wakeups_idle);\r\n{\r\nu64 avg_atom, avg_per_cpu;\r\navg_atom = p->se.sum_exec_runtime;\r\nif (nr_switches)\r\ndo_div(avg_atom, nr_switches);\r\nelse\r\navg_atom = -1LL;\r\navg_per_cpu = p->se.sum_exec_runtime;\r\nif (p->se.nr_migrations) {\r\navg_per_cpu = div64_u64(avg_per_cpu,\r\np->se.nr_migrations);\r\n} else {\r\navg_per_cpu = -1LL;\r\n}\r\n__PN(avg_atom);\r\n__PN(avg_per_cpu);\r\n}\r\n#endif\r\n__P(nr_switches);\r\nSEQ_printf(m, "%-35s:%21Ld\n",\r\n"nr_voluntary_switches", (long long)p->nvcsw);\r\nSEQ_printf(m, "%-35s:%21Ld\n",\r\n"nr_involuntary_switches", (long long)p->nivcsw);\r\nP(se.load.weight);\r\nP(policy);\r\nP(prio);\r\n#undef PN\r\n#undef __PN\r\n#undef P\r\n#undef __P\r\n{\r\nunsigned int this_cpu = raw_smp_processor_id();\r\nu64 t0, t1;\r\nt0 = cpu_clock(this_cpu);\r\nt1 = cpu_clock(this_cpu);\r\nSEQ_printf(m, "%-35s:%21Ld\n",\r\n"clock-delta", (long long)(t1-t0));\r\n}\r\n}\r\nvoid proc_sched_set_task(struct task_struct *p)\r\n{\r\n#ifdef CONFIG_SCHEDSTATS\r\nmemset(&p->se.statistics, 0, sizeof(p->se.statistics));\r\n#endif\r\n}
