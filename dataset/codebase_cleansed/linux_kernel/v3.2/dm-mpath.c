static struct pgpath *alloc_pgpath(void)\r\n{\r\nstruct pgpath *pgpath = kzalloc(sizeof(*pgpath), GFP_KERNEL);\r\nif (pgpath) {\r\npgpath->is_active = 1;\r\nINIT_DELAYED_WORK(&pgpath->activate_path, activate_path);\r\n}\r\nreturn pgpath;\r\n}\r\nstatic void free_pgpath(struct pgpath *pgpath)\r\n{\r\nkfree(pgpath);\r\n}\r\nstatic struct priority_group *alloc_priority_group(void)\r\n{\r\nstruct priority_group *pg;\r\npg = kzalloc(sizeof(*pg), GFP_KERNEL);\r\nif (pg)\r\nINIT_LIST_HEAD(&pg->pgpaths);\r\nreturn pg;\r\n}\r\nstatic void free_pgpaths(struct list_head *pgpaths, struct dm_target *ti)\r\n{\r\nstruct pgpath *pgpath, *tmp;\r\nstruct multipath *m = ti->private;\r\nlist_for_each_entry_safe(pgpath, tmp, pgpaths, list) {\r\nlist_del(&pgpath->list);\r\nif (m->hw_handler_name)\r\nscsi_dh_detach(bdev_get_queue(pgpath->path.dev->bdev));\r\ndm_put_device(ti, pgpath->path.dev);\r\nfree_pgpath(pgpath);\r\n}\r\n}\r\nstatic void free_priority_group(struct priority_group *pg,\r\nstruct dm_target *ti)\r\n{\r\nstruct path_selector *ps = &pg->ps;\r\nif (ps->type) {\r\nps->type->destroy(ps);\r\ndm_put_path_selector(ps->type);\r\n}\r\nfree_pgpaths(&pg->pgpaths, ti);\r\nkfree(pg);\r\n}\r\nstatic struct multipath *alloc_multipath(struct dm_target *ti)\r\n{\r\nstruct multipath *m;\r\nm = kzalloc(sizeof(*m), GFP_KERNEL);\r\nif (m) {\r\nINIT_LIST_HEAD(&m->priority_groups);\r\nINIT_LIST_HEAD(&m->queued_ios);\r\nspin_lock_init(&m->lock);\r\nm->queue_io = 1;\r\nm->pg_init_delay_msecs = DM_PG_INIT_DELAY_DEFAULT;\r\nINIT_WORK(&m->process_queued_ios, process_queued_ios);\r\nINIT_WORK(&m->trigger_event, trigger_event);\r\ninit_waitqueue_head(&m->pg_init_wait);\r\nmutex_init(&m->work_mutex);\r\nm->mpio_pool = mempool_create_slab_pool(MIN_IOS, _mpio_cache);\r\nif (!m->mpio_pool) {\r\nkfree(m);\r\nreturn NULL;\r\n}\r\nm->ti = ti;\r\nti->private = m;\r\n}\r\nreturn m;\r\n}\r\nstatic void free_multipath(struct multipath *m)\r\n{\r\nstruct priority_group *pg, *tmp;\r\nlist_for_each_entry_safe(pg, tmp, &m->priority_groups, list) {\r\nlist_del(&pg->list);\r\nfree_priority_group(pg, m->ti);\r\n}\r\nkfree(m->hw_handler_name);\r\nkfree(m->hw_handler_params);\r\nmempool_destroy(m->mpio_pool);\r\nkfree(m);\r\n}\r\nstatic void __pg_init_all_paths(struct multipath *m)\r\n{\r\nstruct pgpath *pgpath;\r\nunsigned long pg_init_delay = 0;\r\nm->pg_init_count++;\r\nm->pg_init_required = 0;\r\nif (m->pg_init_delay_retry)\r\npg_init_delay = msecs_to_jiffies(m->pg_init_delay_msecs != DM_PG_INIT_DELAY_DEFAULT ?\r\nm->pg_init_delay_msecs : DM_PG_INIT_DELAY_MSECS);\r\nlist_for_each_entry(pgpath, &m->current_pg->pgpaths, list) {\r\nif (!pgpath->is_active)\r\ncontinue;\r\nif (queue_delayed_work(kmpath_handlerd, &pgpath->activate_path,\r\npg_init_delay))\r\nm->pg_init_in_progress++;\r\n}\r\n}\r\nstatic void __switch_pg(struct multipath *m, struct pgpath *pgpath)\r\n{\r\nm->current_pg = pgpath->pg;\r\nif (m->hw_handler_name) {\r\nm->pg_init_required = 1;\r\nm->queue_io = 1;\r\n} else {\r\nm->pg_init_required = 0;\r\nm->queue_io = 0;\r\n}\r\nm->pg_init_count = 0;\r\n}\r\nstatic int __choose_path_in_pg(struct multipath *m, struct priority_group *pg,\r\nsize_t nr_bytes)\r\n{\r\nstruct dm_path *path;\r\npath = pg->ps.type->select_path(&pg->ps, &m->repeat_count, nr_bytes);\r\nif (!path)\r\nreturn -ENXIO;\r\nm->current_pgpath = path_to_pgpath(path);\r\nif (m->current_pg != pg)\r\n__switch_pg(m, m->current_pgpath);\r\nreturn 0;\r\n}\r\nstatic void __choose_pgpath(struct multipath *m, size_t nr_bytes)\r\n{\r\nstruct priority_group *pg;\r\nunsigned bypassed = 1;\r\nif (!m->nr_valid_paths)\r\ngoto failed;\r\nif (m->next_pg) {\r\npg = m->next_pg;\r\nm->next_pg = NULL;\r\nif (!__choose_path_in_pg(m, pg, nr_bytes))\r\nreturn;\r\n}\r\nif (m->current_pg && !__choose_path_in_pg(m, m->current_pg, nr_bytes))\r\nreturn;\r\ndo {\r\nlist_for_each_entry(pg, &m->priority_groups, list) {\r\nif (pg->bypassed == bypassed)\r\ncontinue;\r\nif (!__choose_path_in_pg(m, pg, nr_bytes))\r\nreturn;\r\n}\r\n} while (bypassed--);\r\nfailed:\r\nm->current_pgpath = NULL;\r\nm->current_pg = NULL;\r\n}\r\nstatic int __must_push_back(struct multipath *m)\r\n{\r\nreturn (m->queue_if_no_path != m->saved_queue_if_no_path &&\r\ndm_noflush_suspending(m->ti));\r\n}\r\nstatic int map_io(struct multipath *m, struct request *clone,\r\nstruct dm_mpath_io *mpio, unsigned was_queued)\r\n{\r\nint r = DM_MAPIO_REMAPPED;\r\nsize_t nr_bytes = blk_rq_bytes(clone);\r\nunsigned long flags;\r\nstruct pgpath *pgpath;\r\nstruct block_device *bdev;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (!m->current_pgpath ||\r\n(!m->queue_io && (m->repeat_count && --m->repeat_count == 0)))\r\n__choose_pgpath(m, nr_bytes);\r\npgpath = m->current_pgpath;\r\nif (was_queued)\r\nm->queue_size--;\r\nif ((pgpath && m->queue_io) ||\r\n(!pgpath && m->queue_if_no_path)) {\r\nlist_add_tail(&clone->queuelist, &m->queued_ios);\r\nm->queue_size++;\r\nif ((m->pg_init_required && !m->pg_init_in_progress) ||\r\n!m->queue_io)\r\nqueue_work(kmultipathd, &m->process_queued_ios);\r\npgpath = NULL;\r\nr = DM_MAPIO_SUBMITTED;\r\n} else if (pgpath) {\r\nbdev = pgpath->path.dev->bdev;\r\nclone->q = bdev_get_queue(bdev);\r\nclone->rq_disk = bdev->bd_disk;\r\n} else if (__must_push_back(m))\r\nr = DM_MAPIO_REQUEUE;\r\nelse\r\nr = -EIO;\r\nmpio->pgpath = pgpath;\r\nmpio->nr_bytes = nr_bytes;\r\nif (r == DM_MAPIO_REMAPPED && pgpath->pg->ps.type->start_io)\r\npgpath->pg->ps.type->start_io(&pgpath->pg->ps, &pgpath->path,\r\nnr_bytes);\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn r;\r\n}\r\nstatic int queue_if_no_path(struct multipath *m, unsigned queue_if_no_path,\r\nunsigned save_old_value)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (save_old_value)\r\nm->saved_queue_if_no_path = m->queue_if_no_path;\r\nelse\r\nm->saved_queue_if_no_path = queue_if_no_path;\r\nm->queue_if_no_path = queue_if_no_path;\r\nif (!m->queue_if_no_path && m->queue_size)\r\nqueue_work(kmultipathd, &m->process_queued_ios);\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn 0;\r\n}\r\nstatic void dispatch_queued_ios(struct multipath *m)\r\n{\r\nint r;\r\nunsigned long flags;\r\nstruct dm_mpath_io *mpio;\r\nunion map_info *info;\r\nstruct request *clone, *n;\r\nLIST_HEAD(cl);\r\nspin_lock_irqsave(&m->lock, flags);\r\nlist_splice_init(&m->queued_ios, &cl);\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nlist_for_each_entry_safe(clone, n, &cl, queuelist) {\r\nlist_del_init(&clone->queuelist);\r\ninfo = dm_get_rq_mapinfo(clone);\r\nmpio = info->ptr;\r\nr = map_io(m, clone, mpio, 1);\r\nif (r < 0) {\r\nmempool_free(mpio, m->mpio_pool);\r\ndm_kill_unmapped_request(clone, r);\r\n} else if (r == DM_MAPIO_REMAPPED)\r\ndm_dispatch_request(clone);\r\nelse if (r == DM_MAPIO_REQUEUE) {\r\nmempool_free(mpio, m->mpio_pool);\r\ndm_requeue_unmapped_request(clone);\r\n}\r\n}\r\n}\r\nstatic void process_queued_ios(struct work_struct *work)\r\n{\r\nstruct multipath *m =\r\ncontainer_of(work, struct multipath, process_queued_ios);\r\nstruct pgpath *pgpath = NULL;\r\nunsigned must_queue = 1;\r\nunsigned long flags;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (!m->queue_size)\r\ngoto out;\r\nif (!m->current_pgpath)\r\n__choose_pgpath(m, 0);\r\npgpath = m->current_pgpath;\r\nif ((pgpath && !m->queue_io) ||\r\n(!pgpath && !m->queue_if_no_path))\r\nmust_queue = 0;\r\nif (m->pg_init_required && !m->pg_init_in_progress && pgpath)\r\n__pg_init_all_paths(m);\r\nout:\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nif (!must_queue)\r\ndispatch_queued_ios(m);\r\n}\r\nstatic void trigger_event(struct work_struct *work)\r\n{\r\nstruct multipath *m =\r\ncontainer_of(work, struct multipath, trigger_event);\r\ndm_table_event(m->ti->table);\r\n}\r\nstatic int parse_path_selector(struct dm_arg_set *as, struct priority_group *pg,\r\nstruct dm_target *ti)\r\n{\r\nint r;\r\nstruct path_selector_type *pst;\r\nunsigned ps_argc;\r\nstatic struct dm_arg _args[] = {\r\n{0, 1024, "invalid number of path selector args"},\r\n};\r\npst = dm_get_path_selector(dm_shift_arg(as));\r\nif (!pst) {\r\nti->error = "unknown path selector type";\r\nreturn -EINVAL;\r\n}\r\nr = dm_read_arg_group(_args, as, &ps_argc, &ti->error);\r\nif (r) {\r\ndm_put_path_selector(pst);\r\nreturn -EINVAL;\r\n}\r\nr = pst->create(&pg->ps, ps_argc, as->argv);\r\nif (r) {\r\ndm_put_path_selector(pst);\r\nti->error = "path selector constructor failed";\r\nreturn r;\r\n}\r\npg->ps.type = pst;\r\ndm_consume_args(as, ps_argc);\r\nreturn 0;\r\n}\r\nstatic struct pgpath *parse_path(struct dm_arg_set *as, struct path_selector *ps,\r\nstruct dm_target *ti)\r\n{\r\nint r;\r\nstruct pgpath *p;\r\nstruct multipath *m = ti->private;\r\nif (as->argc < 1) {\r\nti->error = "no device given";\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\np = alloc_pgpath();\r\nif (!p)\r\nreturn ERR_PTR(-ENOMEM);\r\nr = dm_get_device(ti, dm_shift_arg(as), dm_table_get_mode(ti->table),\r\n&p->path.dev);\r\nif (r) {\r\nti->error = "error getting device";\r\ngoto bad;\r\n}\r\nif (m->hw_handler_name) {\r\nstruct request_queue *q = bdev_get_queue(p->path.dev->bdev);\r\nr = scsi_dh_attach(q, m->hw_handler_name);\r\nif (r == -EBUSY) {\r\nscsi_dh_detach(q);\r\nr = scsi_dh_attach(q, m->hw_handler_name);\r\n}\r\nif (r < 0) {\r\nti->error = "error attaching hardware handler";\r\ndm_put_device(ti, p->path.dev);\r\ngoto bad;\r\n}\r\nif (m->hw_handler_params) {\r\nr = scsi_dh_set_params(q, m->hw_handler_params);\r\nif (r < 0) {\r\nti->error = "unable to set hardware "\r\n"handler parameters";\r\nscsi_dh_detach(q);\r\ndm_put_device(ti, p->path.dev);\r\ngoto bad;\r\n}\r\n}\r\n}\r\nr = ps->type->add_path(ps, &p->path, as->argc, as->argv, &ti->error);\r\nif (r) {\r\ndm_put_device(ti, p->path.dev);\r\ngoto bad;\r\n}\r\nreturn p;\r\nbad:\r\nfree_pgpath(p);\r\nreturn ERR_PTR(r);\r\n}\r\nstatic struct priority_group *parse_priority_group(struct dm_arg_set *as,\r\nstruct multipath *m)\r\n{\r\nstatic struct dm_arg _args[] = {\r\n{1, 1024, "invalid number of paths"},\r\n{0, 1024, "invalid number of selector args"}\r\n};\r\nint r;\r\nunsigned i, nr_selector_args, nr_args;\r\nstruct priority_group *pg;\r\nstruct dm_target *ti = m->ti;\r\nif (as->argc < 2) {\r\nas->argc = 0;\r\nti->error = "not enough priority group arguments";\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\npg = alloc_priority_group();\r\nif (!pg) {\r\nti->error = "couldn't allocate priority group";\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\npg->m = m;\r\nr = parse_path_selector(as, pg, ti);\r\nif (r)\r\ngoto bad;\r\nr = dm_read_arg(_args, as, &pg->nr_pgpaths, &ti->error);\r\nif (r)\r\ngoto bad;\r\nr = dm_read_arg(_args + 1, as, &nr_selector_args, &ti->error);\r\nif (r)\r\ngoto bad;\r\nnr_args = 1 + nr_selector_args;\r\nfor (i = 0; i < pg->nr_pgpaths; i++) {\r\nstruct pgpath *pgpath;\r\nstruct dm_arg_set path_args;\r\nif (as->argc < nr_args) {\r\nti->error = "not enough path parameters";\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\npath_args.argc = nr_args;\r\npath_args.argv = as->argv;\r\npgpath = parse_path(&path_args, &pg->ps, ti);\r\nif (IS_ERR(pgpath)) {\r\nr = PTR_ERR(pgpath);\r\ngoto bad;\r\n}\r\npgpath->pg = pg;\r\nlist_add_tail(&pgpath->list, &pg->pgpaths);\r\ndm_consume_args(as, nr_args);\r\n}\r\nreturn pg;\r\nbad:\r\nfree_priority_group(pg, ti);\r\nreturn ERR_PTR(r);\r\n}\r\nstatic int parse_hw_handler(struct dm_arg_set *as, struct multipath *m)\r\n{\r\nunsigned hw_argc;\r\nint ret;\r\nstruct dm_target *ti = m->ti;\r\nstatic struct dm_arg _args[] = {\r\n{0, 1024, "invalid number of hardware handler args"},\r\n};\r\nif (dm_read_arg_group(_args, as, &hw_argc, &ti->error))\r\nreturn -EINVAL;\r\nif (!hw_argc)\r\nreturn 0;\r\nm->hw_handler_name = kstrdup(dm_shift_arg(as), GFP_KERNEL);\r\nrequest_module("scsi_dh_%s", m->hw_handler_name);\r\nif (scsi_dh_handler_exist(m->hw_handler_name) == 0) {\r\nti->error = "unknown hardware handler type";\r\nret = -EINVAL;\r\ngoto fail;\r\n}\r\nif (hw_argc > 1) {\r\nchar *p;\r\nint i, j, len = 4;\r\nfor (i = 0; i <= hw_argc - 2; i++)\r\nlen += strlen(as->argv[i]) + 1;\r\np = m->hw_handler_params = kzalloc(len, GFP_KERNEL);\r\nif (!p) {\r\nti->error = "memory allocation failed";\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\nj = sprintf(p, "%d", hw_argc - 1);\r\nfor (i = 0, p+=j+1; i <= hw_argc - 2; i++, p+=j+1)\r\nj = sprintf(p, "%s", as->argv[i]);\r\n}\r\ndm_consume_args(as, hw_argc - 1);\r\nreturn 0;\r\nfail:\r\nkfree(m->hw_handler_name);\r\nm->hw_handler_name = NULL;\r\nreturn ret;\r\n}\r\nstatic int parse_features(struct dm_arg_set *as, struct multipath *m)\r\n{\r\nint r;\r\nunsigned argc;\r\nstruct dm_target *ti = m->ti;\r\nconst char *arg_name;\r\nstatic struct dm_arg _args[] = {\r\n{0, 5, "invalid number of feature args"},\r\n{1, 50, "pg_init_retries must be between 1 and 50"},\r\n{0, 60000, "pg_init_delay_msecs must be between 0 and 60000"},\r\n};\r\nr = dm_read_arg_group(_args, as, &argc, &ti->error);\r\nif (r)\r\nreturn -EINVAL;\r\nif (!argc)\r\nreturn 0;\r\ndo {\r\narg_name = dm_shift_arg(as);\r\nargc--;\r\nif (!strcasecmp(arg_name, "queue_if_no_path")) {\r\nr = queue_if_no_path(m, 1, 0);\r\ncontinue;\r\n}\r\nif (!strcasecmp(arg_name, "pg_init_retries") &&\r\n(argc >= 1)) {\r\nr = dm_read_arg(_args + 1, as, &m->pg_init_retries, &ti->error);\r\nargc--;\r\ncontinue;\r\n}\r\nif (!strcasecmp(arg_name, "pg_init_delay_msecs") &&\r\n(argc >= 1)) {\r\nr = dm_read_arg(_args + 2, as, &m->pg_init_delay_msecs, &ti->error);\r\nargc--;\r\ncontinue;\r\n}\r\nti->error = "Unrecognised multipath feature request";\r\nr = -EINVAL;\r\n} while (argc && !r);\r\nreturn r;\r\n}\r\nstatic int multipath_ctr(struct dm_target *ti, unsigned int argc,\r\nchar **argv)\r\n{\r\nstatic struct dm_arg _args[] = {\r\n{0, 1024, "invalid number of priority groups"},\r\n{0, 1024, "invalid initial priority group number"},\r\n};\r\nint r;\r\nstruct multipath *m;\r\nstruct dm_arg_set as;\r\nunsigned pg_count = 0;\r\nunsigned next_pg_num;\r\nas.argc = argc;\r\nas.argv = argv;\r\nm = alloc_multipath(ti);\r\nif (!m) {\r\nti->error = "can't allocate multipath";\r\nreturn -EINVAL;\r\n}\r\nr = parse_features(&as, m);\r\nif (r)\r\ngoto bad;\r\nr = parse_hw_handler(&as, m);\r\nif (r)\r\ngoto bad;\r\nr = dm_read_arg(_args, &as, &m->nr_priority_groups, &ti->error);\r\nif (r)\r\ngoto bad;\r\nr = dm_read_arg(_args + 1, &as, &next_pg_num, &ti->error);\r\nif (r)\r\ngoto bad;\r\nif ((!m->nr_priority_groups && next_pg_num) ||\r\n(m->nr_priority_groups && !next_pg_num)) {\r\nti->error = "invalid initial priority group";\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\nwhile (as.argc) {\r\nstruct priority_group *pg;\r\npg = parse_priority_group(&as, m);\r\nif (IS_ERR(pg)) {\r\nr = PTR_ERR(pg);\r\ngoto bad;\r\n}\r\nm->nr_valid_paths += pg->nr_pgpaths;\r\nlist_add_tail(&pg->list, &m->priority_groups);\r\npg_count++;\r\npg->pg_num = pg_count;\r\nif (!--next_pg_num)\r\nm->next_pg = pg;\r\n}\r\nif (pg_count != m->nr_priority_groups) {\r\nti->error = "priority group count mismatch";\r\nr = -EINVAL;\r\ngoto bad;\r\n}\r\nti->num_flush_requests = 1;\r\nti->num_discard_requests = 1;\r\nreturn 0;\r\nbad:\r\nfree_multipath(m);\r\nreturn r;\r\n}\r\nstatic void multipath_wait_for_pg_init_completion(struct multipath *m)\r\n{\r\nDECLARE_WAITQUEUE(wait, current);\r\nunsigned long flags;\r\nadd_wait_queue(&m->pg_init_wait, &wait);\r\nwhile (1) {\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (!m->pg_init_in_progress) {\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nio_schedule();\r\n}\r\nset_current_state(TASK_RUNNING);\r\nremove_wait_queue(&m->pg_init_wait, &wait);\r\n}\r\nstatic void flush_multipath_work(struct multipath *m)\r\n{\r\nflush_workqueue(kmpath_handlerd);\r\nmultipath_wait_for_pg_init_completion(m);\r\nflush_workqueue(kmultipathd);\r\nflush_work_sync(&m->trigger_event);\r\n}\r\nstatic void multipath_dtr(struct dm_target *ti)\r\n{\r\nstruct multipath *m = ti->private;\r\nflush_multipath_work(m);\r\nfree_multipath(m);\r\n}\r\nstatic int multipath_map(struct dm_target *ti, struct request *clone,\r\nunion map_info *map_context)\r\n{\r\nint r;\r\nstruct dm_mpath_io *mpio;\r\nstruct multipath *m = (struct multipath *) ti->private;\r\nmpio = mempool_alloc(m->mpio_pool, GFP_ATOMIC);\r\nif (!mpio)\r\nreturn DM_MAPIO_REQUEUE;\r\nmemset(mpio, 0, sizeof(*mpio));\r\nmap_context->ptr = mpio;\r\nclone->cmd_flags |= REQ_FAILFAST_TRANSPORT;\r\nr = map_io(m, clone, mpio, 0);\r\nif (r < 0 || r == DM_MAPIO_REQUEUE)\r\nmempool_free(mpio, m->mpio_pool);\r\nreturn r;\r\n}\r\nstatic int fail_path(struct pgpath *pgpath)\r\n{\r\nunsigned long flags;\r\nstruct multipath *m = pgpath->pg->m;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (!pgpath->is_active)\r\ngoto out;\r\nDMWARN("Failing path %s.", pgpath->path.dev->name);\r\npgpath->pg->ps.type->fail_path(&pgpath->pg->ps, &pgpath->path);\r\npgpath->is_active = 0;\r\npgpath->fail_count++;\r\nm->nr_valid_paths--;\r\nif (pgpath == m->current_pgpath)\r\nm->current_pgpath = NULL;\r\ndm_path_uevent(DM_UEVENT_PATH_FAILED, m->ti,\r\npgpath->path.dev->name, m->nr_valid_paths);\r\nschedule_work(&m->trigger_event);\r\nout:\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int reinstate_path(struct pgpath *pgpath)\r\n{\r\nint r = 0;\r\nunsigned long flags;\r\nstruct multipath *m = pgpath->pg->m;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (pgpath->is_active)\r\ngoto out;\r\nif (!pgpath->pg->ps.type->reinstate_path) {\r\nDMWARN("Reinstate path not supported by path selector %s",\r\npgpath->pg->ps.type->name);\r\nr = -EINVAL;\r\ngoto out;\r\n}\r\nr = pgpath->pg->ps.type->reinstate_path(&pgpath->pg->ps, &pgpath->path);\r\nif (r)\r\ngoto out;\r\npgpath->is_active = 1;\r\nif (!m->nr_valid_paths++ && m->queue_size) {\r\nm->current_pgpath = NULL;\r\nqueue_work(kmultipathd, &m->process_queued_ios);\r\n} else if (m->hw_handler_name && (m->current_pg == pgpath->pg)) {\r\nif (queue_work(kmpath_handlerd, &pgpath->activate_path.work))\r\nm->pg_init_in_progress++;\r\n}\r\ndm_path_uevent(DM_UEVENT_PATH_REINSTATED, m->ti,\r\npgpath->path.dev->name, m->nr_valid_paths);\r\nschedule_work(&m->trigger_event);\r\nout:\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn r;\r\n}\r\nstatic int action_dev(struct multipath *m, struct dm_dev *dev,\r\naction_fn action)\r\n{\r\nint r = -EINVAL;\r\nstruct pgpath *pgpath;\r\nstruct priority_group *pg;\r\nlist_for_each_entry(pg, &m->priority_groups, list) {\r\nlist_for_each_entry(pgpath, &pg->pgpaths, list) {\r\nif (pgpath->path.dev == dev)\r\nr = action(pgpath);\r\n}\r\n}\r\nreturn r;\r\n}\r\nstatic void bypass_pg(struct multipath *m, struct priority_group *pg,\r\nint bypassed)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&m->lock, flags);\r\npg->bypassed = bypassed;\r\nm->current_pgpath = NULL;\r\nm->current_pg = NULL;\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nschedule_work(&m->trigger_event);\r\n}\r\nstatic int switch_pg_num(struct multipath *m, const char *pgstr)\r\n{\r\nstruct priority_group *pg;\r\nunsigned pgnum;\r\nunsigned long flags;\r\nif (!pgstr || (sscanf(pgstr, "%u", &pgnum) != 1) || !pgnum ||\r\n(pgnum > m->nr_priority_groups)) {\r\nDMWARN("invalid PG number supplied to switch_pg_num");\r\nreturn -EINVAL;\r\n}\r\nspin_lock_irqsave(&m->lock, flags);\r\nlist_for_each_entry(pg, &m->priority_groups, list) {\r\npg->bypassed = 0;\r\nif (--pgnum)\r\ncontinue;\r\nm->current_pgpath = NULL;\r\nm->current_pg = NULL;\r\nm->next_pg = pg;\r\n}\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nschedule_work(&m->trigger_event);\r\nreturn 0;\r\n}\r\nstatic int bypass_pg_num(struct multipath *m, const char *pgstr, int bypassed)\r\n{\r\nstruct priority_group *pg;\r\nunsigned pgnum;\r\nif (!pgstr || (sscanf(pgstr, "%u", &pgnum) != 1) || !pgnum ||\r\n(pgnum > m->nr_priority_groups)) {\r\nDMWARN("invalid PG number supplied to bypass_pg");\r\nreturn -EINVAL;\r\n}\r\nlist_for_each_entry(pg, &m->priority_groups, list) {\r\nif (!--pgnum)\r\nbreak;\r\n}\r\nbypass_pg(m, pg, bypassed);\r\nreturn 0;\r\n}\r\nstatic int pg_init_limit_reached(struct multipath *m, struct pgpath *pgpath)\r\n{\r\nunsigned long flags;\r\nint limit_reached = 0;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (m->pg_init_count <= m->pg_init_retries)\r\nm->pg_init_required = 1;\r\nelse\r\nlimit_reached = 1;\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn limit_reached;\r\n}\r\nstatic void pg_init_done(void *data, int errors)\r\n{\r\nstruct pgpath *pgpath = data;\r\nstruct priority_group *pg = pgpath->pg;\r\nstruct multipath *m = pg->m;\r\nunsigned long flags;\r\nunsigned delay_retry = 0;\r\nswitch (errors) {\r\ncase SCSI_DH_OK:\r\nbreak;\r\ncase SCSI_DH_NOSYS:\r\nif (!m->hw_handler_name) {\r\nerrors = 0;\r\nbreak;\r\n}\r\nDMERR("Could not failover the device: Handler scsi_dh_%s "\r\n"Error %d.", m->hw_handler_name, errors);\r\nfail_path(pgpath);\r\nbreak;\r\ncase SCSI_DH_DEV_TEMP_BUSY:\r\nbypass_pg(m, pg, 1);\r\nbreak;\r\ncase SCSI_DH_RETRY:\r\ndelay_retry = 1;\r\ncase SCSI_DH_IMM_RETRY:\r\ncase SCSI_DH_RES_TEMP_UNAVAIL:\r\nif (pg_init_limit_reached(m, pgpath))\r\nfail_path(pgpath);\r\nerrors = 0;\r\nbreak;\r\ndefault:\r\nfail_path(pgpath);\r\n}\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (errors) {\r\nif (pgpath == m->current_pgpath) {\r\nDMERR("Could not failover device. Error %d.", errors);\r\nm->current_pgpath = NULL;\r\nm->current_pg = NULL;\r\n}\r\n} else if (!m->pg_init_required)\r\npg->bypassed = 0;\r\nif (--m->pg_init_in_progress)\r\ngoto out;\r\nif (!m->pg_init_required)\r\nm->queue_io = 0;\r\nm->pg_init_delay_retry = delay_retry;\r\nqueue_work(kmultipathd, &m->process_queued_ios);\r\nwake_up(&m->pg_init_wait);\r\nout:\r\nspin_unlock_irqrestore(&m->lock, flags);\r\n}\r\nstatic void activate_path(struct work_struct *work)\r\n{\r\nstruct pgpath *pgpath =\r\ncontainer_of(work, struct pgpath, activate_path.work);\r\nscsi_dh_activate(bdev_get_queue(pgpath->path.dev->bdev),\r\npg_init_done, pgpath);\r\n}\r\nstatic int do_end_io(struct multipath *m, struct request *clone,\r\nint error, struct dm_mpath_io *mpio)\r\n{\r\nint r = DM_ENDIO_REQUEUE;\r\nunsigned long flags;\r\nif (!error && !clone->errors)\r\nreturn 0;\r\nif (error == -EOPNOTSUPP || error == -EREMOTEIO || error == -EILSEQ)\r\nreturn error;\r\nif (mpio->pgpath)\r\nfail_path(mpio->pgpath);\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (!m->nr_valid_paths) {\r\nif (!m->queue_if_no_path) {\r\nif (!__must_push_back(m))\r\nr = -EIO;\r\n} else {\r\nif (error == -EBADE)\r\nr = error;\r\n}\r\n}\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn r;\r\n}\r\nstatic int multipath_end_io(struct dm_target *ti, struct request *clone,\r\nint error, union map_info *map_context)\r\n{\r\nstruct multipath *m = ti->private;\r\nstruct dm_mpath_io *mpio = map_context->ptr;\r\nstruct pgpath *pgpath = mpio->pgpath;\r\nstruct path_selector *ps;\r\nint r;\r\nr = do_end_io(m, clone, error, mpio);\r\nif (pgpath) {\r\nps = &pgpath->pg->ps;\r\nif (ps->type->end_io)\r\nps->type->end_io(ps, &pgpath->path, mpio->nr_bytes);\r\n}\r\nmempool_free(mpio, m->mpio_pool);\r\nreturn r;\r\n}\r\nstatic void multipath_presuspend(struct dm_target *ti)\r\n{\r\nstruct multipath *m = (struct multipath *) ti->private;\r\nqueue_if_no_path(m, 0, 1);\r\n}\r\nstatic void multipath_postsuspend(struct dm_target *ti)\r\n{\r\nstruct multipath *m = ti->private;\r\nmutex_lock(&m->work_mutex);\r\nflush_multipath_work(m);\r\nmutex_unlock(&m->work_mutex);\r\n}\r\nstatic void multipath_resume(struct dm_target *ti)\r\n{\r\nstruct multipath *m = (struct multipath *) ti->private;\r\nunsigned long flags;\r\nspin_lock_irqsave(&m->lock, flags);\r\nm->queue_if_no_path = m->saved_queue_if_no_path;\r\nspin_unlock_irqrestore(&m->lock, flags);\r\n}\r\nstatic int multipath_status(struct dm_target *ti, status_type_t type,\r\nchar *result, unsigned int maxlen)\r\n{\r\nint sz = 0;\r\nunsigned long flags;\r\nstruct multipath *m = (struct multipath *) ti->private;\r\nstruct priority_group *pg;\r\nstruct pgpath *p;\r\nunsigned pg_num;\r\nchar state;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (type == STATUSTYPE_INFO)\r\nDMEMIT("2 %u %u ", m->queue_size, m->pg_init_count);\r\nelse {\r\nDMEMIT("%u ", m->queue_if_no_path +\r\n(m->pg_init_retries > 0) * 2 +\r\n(m->pg_init_delay_msecs != DM_PG_INIT_DELAY_DEFAULT) * 2);\r\nif (m->queue_if_no_path)\r\nDMEMIT("queue_if_no_path ");\r\nif (m->pg_init_retries)\r\nDMEMIT("pg_init_retries %u ", m->pg_init_retries);\r\nif (m->pg_init_delay_msecs != DM_PG_INIT_DELAY_DEFAULT)\r\nDMEMIT("pg_init_delay_msecs %u ", m->pg_init_delay_msecs);\r\n}\r\nif (!m->hw_handler_name || type == STATUSTYPE_INFO)\r\nDMEMIT("0 ");\r\nelse\r\nDMEMIT("1 %s ", m->hw_handler_name);\r\nDMEMIT("%u ", m->nr_priority_groups);\r\nif (m->next_pg)\r\npg_num = m->next_pg->pg_num;\r\nelse if (m->current_pg)\r\npg_num = m->current_pg->pg_num;\r\nelse\r\npg_num = (m->nr_priority_groups ? 1 : 0);\r\nDMEMIT("%u ", pg_num);\r\nswitch (type) {\r\ncase STATUSTYPE_INFO:\r\nlist_for_each_entry(pg, &m->priority_groups, list) {\r\nif (pg->bypassed)\r\nstate = 'D';\r\nelse if (pg == m->current_pg)\r\nstate = 'A';\r\nelse\r\nstate = 'E';\r\nDMEMIT("%c ", state);\r\nif (pg->ps.type->status)\r\nsz += pg->ps.type->status(&pg->ps, NULL, type,\r\nresult + sz,\r\nmaxlen - sz);\r\nelse\r\nDMEMIT("0 ");\r\nDMEMIT("%u %u ", pg->nr_pgpaths,\r\npg->ps.type->info_args);\r\nlist_for_each_entry(p, &pg->pgpaths, list) {\r\nDMEMIT("%s %s %u ", p->path.dev->name,\r\np->is_active ? "A" : "F",\r\np->fail_count);\r\nif (pg->ps.type->status)\r\nsz += pg->ps.type->status(&pg->ps,\r\n&p->path, type, result + sz,\r\nmaxlen - sz);\r\n}\r\n}\r\nbreak;\r\ncase STATUSTYPE_TABLE:\r\nlist_for_each_entry(pg, &m->priority_groups, list) {\r\nDMEMIT("%s ", pg->ps.type->name);\r\nif (pg->ps.type->status)\r\nsz += pg->ps.type->status(&pg->ps, NULL, type,\r\nresult + sz,\r\nmaxlen - sz);\r\nelse\r\nDMEMIT("0 ");\r\nDMEMIT("%u %u ", pg->nr_pgpaths,\r\npg->ps.type->table_args);\r\nlist_for_each_entry(p, &pg->pgpaths, list) {\r\nDMEMIT("%s ", p->path.dev->name);\r\nif (pg->ps.type->status)\r\nsz += pg->ps.type->status(&pg->ps,\r\n&p->path, type, result + sz,\r\nmaxlen - sz);\r\n}\r\n}\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn 0;\r\n}\r\nstatic int multipath_message(struct dm_target *ti, unsigned argc, char **argv)\r\n{\r\nint r = -EINVAL;\r\nstruct dm_dev *dev;\r\nstruct multipath *m = (struct multipath *) ti->private;\r\naction_fn action;\r\nmutex_lock(&m->work_mutex);\r\nif (dm_suspended(ti)) {\r\nr = -EBUSY;\r\ngoto out;\r\n}\r\nif (argc == 1) {\r\nif (!strcasecmp(argv[0], "queue_if_no_path")) {\r\nr = queue_if_no_path(m, 1, 0);\r\ngoto out;\r\n} else if (!strcasecmp(argv[0], "fail_if_no_path")) {\r\nr = queue_if_no_path(m, 0, 0);\r\ngoto out;\r\n}\r\n}\r\nif (argc != 2) {\r\nDMWARN("Unrecognised multipath message received.");\r\ngoto out;\r\n}\r\nif (!strcasecmp(argv[0], "disable_group")) {\r\nr = bypass_pg_num(m, argv[1], 1);\r\ngoto out;\r\n} else if (!strcasecmp(argv[0], "enable_group")) {\r\nr = bypass_pg_num(m, argv[1], 0);\r\ngoto out;\r\n} else if (!strcasecmp(argv[0], "switch_group")) {\r\nr = switch_pg_num(m, argv[1]);\r\ngoto out;\r\n} else if (!strcasecmp(argv[0], "reinstate_path"))\r\naction = reinstate_path;\r\nelse if (!strcasecmp(argv[0], "fail_path"))\r\naction = fail_path;\r\nelse {\r\nDMWARN("Unrecognised multipath message received.");\r\ngoto out;\r\n}\r\nr = dm_get_device(ti, argv[1], dm_table_get_mode(ti->table), &dev);\r\nif (r) {\r\nDMWARN("message: error getting device %s",\r\nargv[1]);\r\ngoto out;\r\n}\r\nr = action_dev(m, dev, action);\r\ndm_put_device(ti, dev);\r\nout:\r\nmutex_unlock(&m->work_mutex);\r\nreturn r;\r\n}\r\nstatic int multipath_ioctl(struct dm_target *ti, unsigned int cmd,\r\nunsigned long arg)\r\n{\r\nstruct multipath *m = (struct multipath *) ti->private;\r\nstruct block_device *bdev = NULL;\r\nfmode_t mode = 0;\r\nunsigned long flags;\r\nint r = 0;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (!m->current_pgpath)\r\n__choose_pgpath(m, 0);\r\nif (m->current_pgpath) {\r\nbdev = m->current_pgpath->path.dev->bdev;\r\nmode = m->current_pgpath->path.dev->mode;\r\n}\r\nif (m->queue_io)\r\nr = -EAGAIN;\r\nelse if (!bdev)\r\nr = -EIO;\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn r ? : __blkdev_driver_ioctl(bdev, mode, cmd, arg);\r\n}\r\nstatic int multipath_iterate_devices(struct dm_target *ti,\r\niterate_devices_callout_fn fn, void *data)\r\n{\r\nstruct multipath *m = ti->private;\r\nstruct priority_group *pg;\r\nstruct pgpath *p;\r\nint ret = 0;\r\nlist_for_each_entry(pg, &m->priority_groups, list) {\r\nlist_for_each_entry(p, &pg->pgpaths, list) {\r\nret = fn(ti, p->path.dev, ti->begin, ti->len, data);\r\nif (ret)\r\ngoto out;\r\n}\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int __pgpath_busy(struct pgpath *pgpath)\r\n{\r\nstruct request_queue *q = bdev_get_queue(pgpath->path.dev->bdev);\r\nreturn dm_underlying_device_busy(q);\r\n}\r\nstatic int multipath_busy(struct dm_target *ti)\r\n{\r\nint busy = 0, has_active = 0;\r\nstruct multipath *m = ti->private;\r\nstruct priority_group *pg;\r\nstruct pgpath *pgpath;\r\nunsigned long flags;\r\nspin_lock_irqsave(&m->lock, flags);\r\nif (unlikely(!m->current_pgpath && m->next_pg))\r\npg = m->next_pg;\r\nelse if (likely(m->current_pg))\r\npg = m->current_pg;\r\nelse\r\ngoto out;\r\nbusy = 1;\r\nlist_for_each_entry(pgpath, &pg->pgpaths, list)\r\nif (pgpath->is_active) {\r\nhas_active = 1;\r\nif (!__pgpath_busy(pgpath)) {\r\nbusy = 0;\r\nbreak;\r\n}\r\n}\r\nif (!has_active)\r\nbusy = 0;\r\nout:\r\nspin_unlock_irqrestore(&m->lock, flags);\r\nreturn busy;\r\n}\r\nstatic int __init dm_multipath_init(void)\r\n{\r\nint r;\r\n_mpio_cache = KMEM_CACHE(dm_mpath_io, 0);\r\nif (!_mpio_cache)\r\nreturn -ENOMEM;\r\nr = dm_register_target(&multipath_target);\r\nif (r < 0) {\r\nDMERR("register failed %d", r);\r\nkmem_cache_destroy(_mpio_cache);\r\nreturn -EINVAL;\r\n}\r\nkmultipathd = alloc_workqueue("kmpathd", WQ_MEM_RECLAIM, 0);\r\nif (!kmultipathd) {\r\nDMERR("failed to create workqueue kmpathd");\r\ndm_unregister_target(&multipath_target);\r\nkmem_cache_destroy(_mpio_cache);\r\nreturn -ENOMEM;\r\n}\r\nkmpath_handlerd = alloc_ordered_workqueue("kmpath_handlerd",\r\nWQ_MEM_RECLAIM);\r\nif (!kmpath_handlerd) {\r\nDMERR("failed to create workqueue kmpath_handlerd");\r\ndestroy_workqueue(kmultipathd);\r\ndm_unregister_target(&multipath_target);\r\nkmem_cache_destroy(_mpio_cache);\r\nreturn -ENOMEM;\r\n}\r\nDMINFO("version %u.%u.%u loaded",\r\nmultipath_target.version[0], multipath_target.version[1],\r\nmultipath_target.version[2]);\r\nreturn r;\r\n}\r\nstatic void __exit dm_multipath_exit(void)\r\n{\r\ndestroy_workqueue(kmpath_handlerd);\r\ndestroy_workqueue(kmultipathd);\r\ndm_unregister_target(&multipath_target);\r\nkmem_cache_destroy(_mpio_cache);\r\n}
