int _get_ldev_if_state(struct drbd_conf *mdev, enum drbd_disk_state mins)\r\n{\r\nint io_allowed;\r\natomic_inc(&mdev->local_cnt);\r\nio_allowed = (mdev->state.disk >= mins);\r\nif (!io_allowed) {\r\nif (atomic_dec_and_test(&mdev->local_cnt))\r\nwake_up(&mdev->misc_wait);\r\n}\r\nreturn io_allowed;\r\n}\r\nstatic int tl_init(struct drbd_conf *mdev)\r\n{\r\nstruct drbd_tl_epoch *b;\r\nb = kmalloc(sizeof(struct drbd_tl_epoch), GFP_KERNEL);\r\nif (!b)\r\nreturn 0;\r\nINIT_LIST_HEAD(&b->requests);\r\nINIT_LIST_HEAD(&b->w.list);\r\nb->next = NULL;\r\nb->br_number = 4711;\r\nb->n_writes = 0;\r\nb->w.cb = NULL;\r\nmdev->oldest_tle = b;\r\nmdev->newest_tle = b;\r\nINIT_LIST_HEAD(&mdev->out_of_sequence_requests);\r\nmdev->tl_hash = NULL;\r\nmdev->tl_hash_s = 0;\r\nreturn 1;\r\n}\r\nstatic void tl_cleanup(struct drbd_conf *mdev)\r\n{\r\nD_ASSERT(mdev->oldest_tle == mdev->newest_tle);\r\nD_ASSERT(list_empty(&mdev->out_of_sequence_requests));\r\nkfree(mdev->oldest_tle);\r\nmdev->oldest_tle = NULL;\r\nkfree(mdev->unused_spare_tle);\r\nmdev->unused_spare_tle = NULL;\r\nkfree(mdev->tl_hash);\r\nmdev->tl_hash = NULL;\r\nmdev->tl_hash_s = 0;\r\n}\r\nvoid _tl_add_barrier(struct drbd_conf *mdev, struct drbd_tl_epoch *new)\r\n{\r\nstruct drbd_tl_epoch *newest_before;\r\nINIT_LIST_HEAD(&new->requests);\r\nINIT_LIST_HEAD(&new->w.list);\r\nnew->w.cb = NULL;\r\nnew->next = NULL;\r\nnew->n_writes = 0;\r\nnewest_before = mdev->newest_tle;\r\nnew->br_number = (newest_before->br_number+1) ?: 1;\r\nif (mdev->newest_tle != new) {\r\nmdev->newest_tle->next = new;\r\nmdev->newest_tle = new;\r\n}\r\n}\r\nvoid tl_release(struct drbd_conf *mdev, unsigned int barrier_nr,\r\nunsigned int set_size)\r\n{\r\nstruct drbd_tl_epoch *b, *nob;\r\nstruct list_head *le, *tle;\r\nstruct drbd_request *r;\r\nspin_lock_irq(&mdev->req_lock);\r\nb = mdev->oldest_tle;\r\nif (b == NULL) {\r\ndev_err(DEV, "BAD! BarrierAck #%u received, but no epoch in tl!?\n",\r\nbarrier_nr);\r\ngoto bail;\r\n}\r\nif (b->br_number != barrier_nr) {\r\ndev_err(DEV, "BAD! BarrierAck #%u received, expected #%u!\n",\r\nbarrier_nr, b->br_number);\r\ngoto bail;\r\n}\r\nif (b->n_writes != set_size) {\r\ndev_err(DEV, "BAD! BarrierAck #%u received with n_writes=%u, expected n_writes=%u!\n",\r\nbarrier_nr, set_size, b->n_writes);\r\ngoto bail;\r\n}\r\nlist_for_each_safe(le, tle, &b->requests) {\r\nr = list_entry(le, struct drbd_request, tl_requests);\r\n_req_mod(r, barrier_acked);\r\n}\r\nlist_del_init(&b->requests);\r\nnob = b->next;\r\nif (test_and_clear_bit(CREATE_BARRIER, &mdev->flags)) {\r\n_tl_add_barrier(mdev, b);\r\nif (nob)\r\nmdev->oldest_tle = nob;\r\n} else {\r\nD_ASSERT(nob != NULL);\r\nmdev->oldest_tle = nob;\r\nkfree(b);\r\n}\r\nspin_unlock_irq(&mdev->req_lock);\r\ndec_ap_pending(mdev);\r\nreturn;\r\nbail:\r\nspin_unlock_irq(&mdev->req_lock);\r\ndrbd_force_state(mdev, NS(conn, C_PROTOCOL_ERROR));\r\n}\r\nstatic void _tl_restart(struct drbd_conf *mdev, enum drbd_req_event what)\r\n{\r\nstruct drbd_tl_epoch *b, *tmp, **pn;\r\nstruct list_head *le, *tle, carry_reads;\r\nstruct drbd_request *req;\r\nint rv, n_writes, n_reads;\r\nb = mdev->oldest_tle;\r\npn = &mdev->oldest_tle;\r\nwhile (b) {\r\nn_writes = 0;\r\nn_reads = 0;\r\nINIT_LIST_HEAD(&carry_reads);\r\nlist_for_each_safe(le, tle, &b->requests) {\r\nreq = list_entry(le, struct drbd_request, tl_requests);\r\nrv = _req_mod(req, what);\r\nn_writes += (rv & MR_WRITE) >> MR_WRITE_SHIFT;\r\nn_reads += (rv & MR_READ) >> MR_READ_SHIFT;\r\n}\r\ntmp = b->next;\r\nif (n_writes) {\r\nif (what == resend) {\r\nb->n_writes = n_writes;\r\nif (b->w.cb == NULL) {\r\nb->w.cb = w_send_barrier;\r\ninc_ap_pending(mdev);\r\nset_bit(CREATE_BARRIER, &mdev->flags);\r\n}\r\ndrbd_queue_work(&mdev->data.work, &b->w);\r\n}\r\npn = &b->next;\r\n} else {\r\nif (n_reads)\r\nlist_add(&carry_reads, &b->requests);\r\nlist_del(&b->requests);\r\nif (b->w.cb != NULL)\r\ndec_ap_pending(mdev);\r\nif (b == mdev->newest_tle) {\r\nD_ASSERT(tmp == NULL);\r\nINIT_LIST_HEAD(&b->requests);\r\nlist_splice(&carry_reads, &b->requests);\r\nINIT_LIST_HEAD(&b->w.list);\r\nb->w.cb = NULL;\r\nb->br_number = net_random();\r\nb->n_writes = 0;\r\n*pn = b;\r\nbreak;\r\n}\r\n*pn = tmp;\r\nkfree(b);\r\n}\r\nb = tmp;\r\nlist_splice(&carry_reads, &b->requests);\r\n}\r\n}\r\nvoid tl_clear(struct drbd_conf *mdev)\r\n{\r\nstruct list_head *le, *tle;\r\nstruct drbd_request *r;\r\nspin_lock_irq(&mdev->req_lock);\r\n_tl_restart(mdev, connection_lost_while_pending);\r\nD_ASSERT(list_empty(&mdev->out_of_sequence_requests));\r\nlist_for_each_safe(le, tle, &mdev->out_of_sequence_requests) {\r\nr = list_entry(le, struct drbd_request, tl_requests);\r\n_req_mod(r, connection_lost_while_pending);\r\n}\r\nclear_bit(CREATE_BARRIER, &mdev->flags);\r\nmemset(mdev->app_reads_hash, 0, APP_R_HSIZE*sizeof(void *));\r\nspin_unlock_irq(&mdev->req_lock);\r\n}\r\nvoid tl_restart(struct drbd_conf *mdev, enum drbd_req_event what)\r\n{\r\nspin_lock_irq(&mdev->req_lock);\r\n_tl_restart(mdev, what);\r\nspin_unlock_irq(&mdev->req_lock);\r\n}\r\nstatic int cl_wide_st_chg(struct drbd_conf *mdev,\r\nunion drbd_state os, union drbd_state ns)\r\n{\r\nreturn (os.conn >= C_CONNECTED && ns.conn >= C_CONNECTED &&\r\n((os.role != R_PRIMARY && ns.role == R_PRIMARY) ||\r\n(os.conn != C_STARTING_SYNC_T && ns.conn == C_STARTING_SYNC_T) ||\r\n(os.conn != C_STARTING_SYNC_S && ns.conn == C_STARTING_SYNC_S) ||\r\n(os.disk != D_DISKLESS && ns.disk == D_DISKLESS))) ||\r\n(os.conn >= C_CONNECTED && ns.conn == C_DISCONNECTING) ||\r\n(os.conn == C_CONNECTED && ns.conn == C_VERIFY_S);\r\n}\r\nenum drbd_state_rv\r\ndrbd_change_state(struct drbd_conf *mdev, enum chg_state_flags f,\r\nunion drbd_state mask, union drbd_state val)\r\n{\r\nunsigned long flags;\r\nunion drbd_state os, ns;\r\nenum drbd_state_rv rv;\r\nspin_lock_irqsave(&mdev->req_lock, flags);\r\nos = mdev->state;\r\nns.i = (os.i & ~mask.i) | val.i;\r\nrv = _drbd_set_state(mdev, ns, f, NULL);\r\nns = mdev->state;\r\nspin_unlock_irqrestore(&mdev->req_lock, flags);\r\nreturn rv;\r\n}\r\nvoid drbd_force_state(struct drbd_conf *mdev,\r\nunion drbd_state mask, union drbd_state val)\r\n{\r\ndrbd_change_state(mdev, CS_HARD, mask, val);\r\n}\r\nstatic enum drbd_state_rv\r\n_req_st_cond(struct drbd_conf *mdev, union drbd_state mask,\r\nunion drbd_state val)\r\n{\r\nunion drbd_state os, ns;\r\nunsigned long flags;\r\nenum drbd_state_rv rv;\r\nif (test_and_clear_bit(CL_ST_CHG_SUCCESS, &mdev->flags))\r\nreturn SS_CW_SUCCESS;\r\nif (test_and_clear_bit(CL_ST_CHG_FAIL, &mdev->flags))\r\nreturn SS_CW_FAILED_BY_PEER;\r\nrv = 0;\r\nspin_lock_irqsave(&mdev->req_lock, flags);\r\nos = mdev->state;\r\nns.i = (os.i & ~mask.i) | val.i;\r\nns = sanitize_state(mdev, os, ns, NULL);\r\nif (!cl_wide_st_chg(mdev, os, ns))\r\nrv = SS_CW_NO_NEED;\r\nif (!rv) {\r\nrv = is_valid_state(mdev, ns);\r\nif (rv == SS_SUCCESS) {\r\nrv = is_valid_state_transition(mdev, ns, os);\r\nif (rv == SS_SUCCESS)\r\nrv = SS_UNKNOWN_ERROR;\r\n}\r\n}\r\nspin_unlock_irqrestore(&mdev->req_lock, flags);\r\nreturn rv;\r\n}\r\nstatic enum drbd_state_rv\r\ndrbd_req_state(struct drbd_conf *mdev, union drbd_state mask,\r\nunion drbd_state val, enum chg_state_flags f)\r\n{\r\nstruct completion done;\r\nunsigned long flags;\r\nunion drbd_state os, ns;\r\nenum drbd_state_rv rv;\r\ninit_completion(&done);\r\nif (f & CS_SERIALIZE)\r\nmutex_lock(&mdev->state_mutex);\r\nspin_lock_irqsave(&mdev->req_lock, flags);\r\nos = mdev->state;\r\nns.i = (os.i & ~mask.i) | val.i;\r\nns = sanitize_state(mdev, os, ns, NULL);\r\nif (cl_wide_st_chg(mdev, os, ns)) {\r\nrv = is_valid_state(mdev, ns);\r\nif (rv == SS_SUCCESS)\r\nrv = is_valid_state_transition(mdev, ns, os);\r\nspin_unlock_irqrestore(&mdev->req_lock, flags);\r\nif (rv < SS_SUCCESS) {\r\nif (f & CS_VERBOSE)\r\nprint_st_err(mdev, os, ns, rv);\r\ngoto abort;\r\n}\r\ndrbd_state_lock(mdev);\r\nif (!drbd_send_state_req(mdev, mask, val)) {\r\ndrbd_state_unlock(mdev);\r\nrv = SS_CW_FAILED_BY_PEER;\r\nif (f & CS_VERBOSE)\r\nprint_st_err(mdev, os, ns, rv);\r\ngoto abort;\r\n}\r\nwait_event(mdev->state_wait,\r\n(rv = _req_st_cond(mdev, mask, val)));\r\nif (rv < SS_SUCCESS) {\r\ndrbd_state_unlock(mdev);\r\nif (f & CS_VERBOSE)\r\nprint_st_err(mdev, os, ns, rv);\r\ngoto abort;\r\n}\r\nspin_lock_irqsave(&mdev->req_lock, flags);\r\nos = mdev->state;\r\nns.i = (os.i & ~mask.i) | val.i;\r\nrv = _drbd_set_state(mdev, ns, f, &done);\r\ndrbd_state_unlock(mdev);\r\n} else {\r\nrv = _drbd_set_state(mdev, ns, f, &done);\r\n}\r\nspin_unlock_irqrestore(&mdev->req_lock, flags);\r\nif (f & CS_WAIT_COMPLETE && rv == SS_SUCCESS) {\r\nD_ASSERT(current != mdev->worker.task);\r\nwait_for_completion(&done);\r\n}\r\nabort:\r\nif (f & CS_SERIALIZE)\r\nmutex_unlock(&mdev->state_mutex);\r\nreturn rv;\r\n}\r\nenum drbd_state_rv\r\n_drbd_request_state(struct drbd_conf *mdev, union drbd_state mask,\r\nunion drbd_state val, enum chg_state_flags f)\r\n{\r\nenum drbd_state_rv rv;\r\nwait_event(mdev->state_wait,\r\n(rv = drbd_req_state(mdev, mask, val, f)) != SS_IN_TRANSIENT_STATE);\r\nreturn rv;\r\n}\r\nstatic void print_st(struct drbd_conf *mdev, char *name, union drbd_state ns)\r\n{\r\ndev_err(DEV, " %s = { cs:%s ro:%s/%s ds:%s/%s %c%c%c%c }\n",\r\nname,\r\ndrbd_conn_str(ns.conn),\r\ndrbd_role_str(ns.role),\r\ndrbd_role_str(ns.peer),\r\ndrbd_disk_str(ns.disk),\r\ndrbd_disk_str(ns.pdsk),\r\nis_susp(ns) ? 's' : 'r',\r\nns.aftr_isp ? 'a' : '-',\r\nns.peer_isp ? 'p' : '-',\r\nns.user_isp ? 'u' : '-'\r\n);\r\n}\r\nvoid print_st_err(struct drbd_conf *mdev, union drbd_state os,\r\nunion drbd_state ns, enum drbd_state_rv err)\r\n{\r\nif (err == SS_IN_TRANSIENT_STATE)\r\nreturn;\r\ndev_err(DEV, "State change failed: %s\n", drbd_set_st_err_str(err));\r\nprint_st(mdev, " state", os);\r\nprint_st(mdev, "wanted", ns);\r\n}\r\nstatic enum drbd_state_rv\r\nis_valid_state(struct drbd_conf *mdev, union drbd_state ns)\r\n{\r\nenum drbd_fencing_p fp;\r\nenum drbd_state_rv rv = SS_SUCCESS;\r\nfp = FP_DONT_CARE;\r\nif (get_ldev(mdev)) {\r\nfp = mdev->ldev->dc.fencing;\r\nput_ldev(mdev);\r\n}\r\nif (get_net_conf(mdev)) {\r\nif (!mdev->net_conf->two_primaries &&\r\nns.role == R_PRIMARY && ns.peer == R_PRIMARY)\r\nrv = SS_TWO_PRIMARIES;\r\nput_net_conf(mdev);\r\n}\r\nif (rv <= 0)\r\n;\r\nelse if (ns.role == R_SECONDARY && mdev->open_cnt)\r\nrv = SS_DEVICE_IN_USE;\r\nelse if (ns.role == R_PRIMARY && ns.conn < C_CONNECTED && ns.disk < D_UP_TO_DATE)\r\nrv = SS_NO_UP_TO_DATE_DISK;\r\nelse if (fp >= FP_RESOURCE &&\r\nns.role == R_PRIMARY && ns.conn < C_CONNECTED && ns.pdsk >= D_UNKNOWN)\r\nrv = SS_PRIMARY_NOP;\r\nelse if (ns.role == R_PRIMARY && ns.disk <= D_INCONSISTENT && ns.pdsk <= D_INCONSISTENT)\r\nrv = SS_NO_UP_TO_DATE_DISK;\r\nelse if (ns.conn > C_CONNECTED && ns.disk < D_INCONSISTENT)\r\nrv = SS_NO_LOCAL_DISK;\r\nelse if (ns.conn > C_CONNECTED && ns.pdsk < D_INCONSISTENT)\r\nrv = SS_NO_REMOTE_DISK;\r\nelse if (ns.conn > C_CONNECTED && ns.disk < D_UP_TO_DATE && ns.pdsk < D_UP_TO_DATE)\r\nrv = SS_NO_UP_TO_DATE_DISK;\r\nelse if ((ns.conn == C_CONNECTED ||\r\nns.conn == C_WF_BITMAP_S ||\r\nns.conn == C_SYNC_SOURCE ||\r\nns.conn == C_PAUSED_SYNC_S) &&\r\nns.disk == D_OUTDATED)\r\nrv = SS_CONNECTED_OUTDATES;\r\nelse if ((ns.conn == C_VERIFY_S || ns.conn == C_VERIFY_T) &&\r\n(mdev->sync_conf.verify_alg[0] == 0))\r\nrv = SS_NO_VERIFY_ALG;\r\nelse if ((ns.conn == C_VERIFY_S || ns.conn == C_VERIFY_T) &&\r\nmdev->agreed_pro_version < 88)\r\nrv = SS_NOT_SUPPORTED;\r\nelse if (ns.conn >= C_CONNECTED && ns.pdsk == D_UNKNOWN)\r\nrv = SS_CONNECTED_OUTDATES;\r\nreturn rv;\r\n}\r\nstatic enum drbd_state_rv\r\nis_valid_state_transition(struct drbd_conf *mdev, union drbd_state ns,\r\nunion drbd_state os)\r\n{\r\nenum drbd_state_rv rv = SS_SUCCESS;\r\nif ((ns.conn == C_STARTING_SYNC_T || ns.conn == C_STARTING_SYNC_S) &&\r\nos.conn > C_CONNECTED)\r\nrv = SS_RESYNC_RUNNING;\r\nif (ns.conn == C_DISCONNECTING && os.conn == C_STANDALONE)\r\nrv = SS_ALREADY_STANDALONE;\r\nif (ns.disk > D_ATTACHING && os.disk == D_DISKLESS)\r\nrv = SS_IS_DISKLESS;\r\nif (ns.conn == C_WF_CONNECTION && os.conn < C_UNCONNECTED)\r\nrv = SS_NO_NET_CONFIG;\r\nif (ns.disk == D_OUTDATED && os.disk < D_OUTDATED && os.disk != D_ATTACHING)\r\nrv = SS_LOWER_THAN_OUTDATED;\r\nif (ns.conn == C_DISCONNECTING && os.conn == C_UNCONNECTED)\r\nrv = SS_IN_TRANSIENT_STATE;\r\nif (ns.conn == os.conn && ns.conn == C_WF_REPORT_PARAMS)\r\nrv = SS_IN_TRANSIENT_STATE;\r\nif ((ns.conn == C_VERIFY_S || ns.conn == C_VERIFY_T) && os.conn < C_CONNECTED)\r\nrv = SS_NEED_CONNECTION;\r\nif ((ns.conn == C_VERIFY_S || ns.conn == C_VERIFY_T) &&\r\nns.conn != os.conn && os.conn > C_CONNECTED)\r\nrv = SS_RESYNC_RUNNING;\r\nif ((ns.conn == C_STARTING_SYNC_S || ns.conn == C_STARTING_SYNC_T) &&\r\nos.conn < C_CONNECTED)\r\nrv = SS_NEED_CONNECTION;\r\nif ((ns.conn == C_SYNC_TARGET || ns.conn == C_SYNC_SOURCE)\r\n&& os.conn < C_WF_REPORT_PARAMS)\r\nrv = SS_NEED_CONNECTION;\r\nreturn rv;\r\n}\r\nstatic union drbd_state sanitize_state(struct drbd_conf *mdev, union drbd_state os,\r\nunion drbd_state ns, const char **warn_sync_abort)\r\n{\r\nenum drbd_fencing_p fp;\r\nenum drbd_disk_state disk_min, disk_max, pdsk_min, pdsk_max;\r\nfp = FP_DONT_CARE;\r\nif (get_ldev(mdev)) {\r\nfp = mdev->ldev->dc.fencing;\r\nput_ldev(mdev);\r\n}\r\nif ((ns.conn >= C_TIMEOUT && ns.conn <= C_TEAR_DOWN) &&\r\nos.conn <= C_DISCONNECTING)\r\nns.conn = os.conn;\r\nif (os.conn >= C_TIMEOUT && os.conn <= C_TEAR_DOWN &&\r\nns.conn != C_UNCONNECTED && ns.conn != C_DISCONNECTING && ns.conn <= C_TEAR_DOWN)\r\nns.conn = os.conn;\r\nif (ns.disk == D_FAILED && os.disk == D_DISKLESS)\r\nns.disk = D_DISKLESS;\r\nif (ns.disk == D_FAILED && os.disk == D_ATTACHING)\r\nns.disk = D_DISKLESS;\r\nif (os.conn == C_DISCONNECTING && ns.conn != C_STANDALONE)\r\nns.conn = os.conn;\r\nif (ns.conn < C_CONNECTED) {\r\nns.peer_isp = 0;\r\nns.peer = R_UNKNOWN;\r\nif (ns.pdsk > D_UNKNOWN || ns.pdsk < D_INCONSISTENT)\r\nns.pdsk = D_UNKNOWN;\r\n}\r\nif (ns.conn == C_STANDALONE && ns.disk == D_DISKLESS && ns.role == R_SECONDARY)\r\nns.aftr_isp = 0;\r\nif (os.conn > C_CONNECTED && ns.conn > C_CONNECTED &&\r\n(ns.disk <= D_FAILED || ns.pdsk <= D_FAILED)) {\r\nif (warn_sync_abort)\r\n*warn_sync_abort =\r\nos.conn == C_VERIFY_S || os.conn == C_VERIFY_T ?\r\n"Online-verify" : "Resync";\r\nns.conn = C_CONNECTED;\r\n}\r\nif (ns.conn < C_CONNECTED && ns.disk == D_NEGOTIATING &&\r\nget_ldev_if_state(mdev, D_NEGOTIATING)) {\r\nif (mdev->ed_uuid == mdev->ldev->md.uuid[UI_CURRENT]) {\r\nns.disk = mdev->new_state_tmp.disk;\r\nns.pdsk = mdev->new_state_tmp.pdsk;\r\n} else {\r\ndev_alert(DEV, "Connection lost while negotiating, no data!\n");\r\nns.disk = D_DISKLESS;\r\nns.pdsk = D_UNKNOWN;\r\n}\r\nput_ldev(mdev);\r\n}\r\nif (ns.conn >= C_CONNECTED && ns.conn < C_AHEAD) {\r\nif (ns.disk == D_CONSISTENT || ns.disk == D_OUTDATED)\r\nns.disk = D_UP_TO_DATE;\r\nif (ns.pdsk == D_CONSISTENT || ns.pdsk == D_OUTDATED)\r\nns.pdsk = D_UP_TO_DATE;\r\n}\r\ndisk_min = D_DISKLESS;\r\ndisk_max = D_UP_TO_DATE;\r\npdsk_min = D_INCONSISTENT;\r\npdsk_max = D_UNKNOWN;\r\nswitch ((enum drbd_conns)ns.conn) {\r\ncase C_WF_BITMAP_T:\r\ncase C_PAUSED_SYNC_T:\r\ncase C_STARTING_SYNC_T:\r\ncase C_WF_SYNC_UUID:\r\ncase C_BEHIND:\r\ndisk_min = D_INCONSISTENT;\r\ndisk_max = D_OUTDATED;\r\npdsk_min = D_UP_TO_DATE;\r\npdsk_max = D_UP_TO_DATE;\r\nbreak;\r\ncase C_VERIFY_S:\r\ncase C_VERIFY_T:\r\ndisk_min = D_UP_TO_DATE;\r\ndisk_max = D_UP_TO_DATE;\r\npdsk_min = D_UP_TO_DATE;\r\npdsk_max = D_UP_TO_DATE;\r\nbreak;\r\ncase C_CONNECTED:\r\ndisk_min = D_DISKLESS;\r\ndisk_max = D_UP_TO_DATE;\r\npdsk_min = D_DISKLESS;\r\npdsk_max = D_UP_TO_DATE;\r\nbreak;\r\ncase C_WF_BITMAP_S:\r\ncase C_PAUSED_SYNC_S:\r\ncase C_STARTING_SYNC_S:\r\ncase C_AHEAD:\r\ndisk_min = D_UP_TO_DATE;\r\ndisk_max = D_UP_TO_DATE;\r\npdsk_min = D_INCONSISTENT;\r\npdsk_max = D_CONSISTENT;\r\nbreak;\r\ncase C_SYNC_TARGET:\r\ndisk_min = D_INCONSISTENT;\r\ndisk_max = D_INCONSISTENT;\r\npdsk_min = D_UP_TO_DATE;\r\npdsk_max = D_UP_TO_DATE;\r\nbreak;\r\ncase C_SYNC_SOURCE:\r\ndisk_min = D_UP_TO_DATE;\r\ndisk_max = D_UP_TO_DATE;\r\npdsk_min = D_INCONSISTENT;\r\npdsk_max = D_INCONSISTENT;\r\nbreak;\r\ncase C_STANDALONE:\r\ncase C_DISCONNECTING:\r\ncase C_UNCONNECTED:\r\ncase C_TIMEOUT:\r\ncase C_BROKEN_PIPE:\r\ncase C_NETWORK_FAILURE:\r\ncase C_PROTOCOL_ERROR:\r\ncase C_TEAR_DOWN:\r\ncase C_WF_CONNECTION:\r\ncase C_WF_REPORT_PARAMS:\r\ncase C_MASK:\r\nbreak;\r\n}\r\nif (ns.disk > disk_max)\r\nns.disk = disk_max;\r\nif (ns.disk < disk_min) {\r\ndev_warn(DEV, "Implicitly set disk from %s to %s\n",\r\ndrbd_disk_str(ns.disk), drbd_disk_str(disk_min));\r\nns.disk = disk_min;\r\n}\r\nif (ns.pdsk > pdsk_max)\r\nns.pdsk = pdsk_max;\r\nif (ns.pdsk < pdsk_min) {\r\ndev_warn(DEV, "Implicitly set pdsk from %s to %s\n",\r\ndrbd_disk_str(ns.pdsk), drbd_disk_str(pdsk_min));\r\nns.pdsk = pdsk_min;\r\n}\r\nif (fp == FP_STONITH &&\r\n(ns.role == R_PRIMARY && ns.conn < C_CONNECTED && ns.pdsk > D_OUTDATED) &&\r\n!(os.role == R_PRIMARY && os.conn < C_CONNECTED && os.pdsk > D_OUTDATED))\r\nns.susp_fen = 1;\r\nif (mdev->sync_conf.on_no_data == OND_SUSPEND_IO &&\r\n(ns.role == R_PRIMARY && ns.disk < D_UP_TO_DATE && ns.pdsk < D_UP_TO_DATE) &&\r\n!(os.role == R_PRIMARY && os.disk < D_UP_TO_DATE && os.pdsk < D_UP_TO_DATE))\r\nns.susp_nod = 1;\r\nif (ns.aftr_isp || ns.peer_isp || ns.user_isp) {\r\nif (ns.conn == C_SYNC_SOURCE)\r\nns.conn = C_PAUSED_SYNC_S;\r\nif (ns.conn == C_SYNC_TARGET)\r\nns.conn = C_PAUSED_SYNC_T;\r\n} else {\r\nif (ns.conn == C_PAUSED_SYNC_S)\r\nns.conn = C_SYNC_SOURCE;\r\nif (ns.conn == C_PAUSED_SYNC_T)\r\nns.conn = C_SYNC_TARGET;\r\n}\r\nreturn ns;\r\n}\r\nstatic void set_ov_position(struct drbd_conf *mdev, enum drbd_conns cs)\r\n{\r\nif (mdev->agreed_pro_version < 90)\r\nmdev->ov_start_sector = 0;\r\nmdev->rs_total = drbd_bm_bits(mdev);\r\nmdev->ov_position = 0;\r\nif (cs == C_VERIFY_T) {\r\nmdev->ov_start_sector = ~(sector_t)0;\r\n} else {\r\nunsigned long bit = BM_SECT_TO_BIT(mdev->ov_start_sector);\r\nif (bit >= mdev->rs_total) {\r\nmdev->ov_start_sector =\r\nBM_BIT_TO_SECT(mdev->rs_total - 1);\r\nmdev->rs_total = 1;\r\n} else\r\nmdev->rs_total -= bit;\r\nmdev->ov_position = mdev->ov_start_sector;\r\n}\r\nmdev->ov_left = mdev->rs_total;\r\n}\r\nstatic void drbd_resume_al(struct drbd_conf *mdev)\r\n{\r\nif (test_and_clear_bit(AL_SUSPENDED, &mdev->flags))\r\ndev_info(DEV, "Resumed AL updates\n");\r\n}\r\nenum drbd_state_rv\r\n__drbd_set_state(struct drbd_conf *mdev, union drbd_state ns,\r\nenum chg_state_flags flags, struct completion *done)\r\n{\r\nunion drbd_state os;\r\nenum drbd_state_rv rv = SS_SUCCESS;\r\nconst char *warn_sync_abort = NULL;\r\nstruct after_state_chg_work *ascw;\r\nos = mdev->state;\r\nns = sanitize_state(mdev, os, ns, &warn_sync_abort);\r\nif (ns.i == os.i)\r\nreturn SS_NOTHING_TO_DO;\r\nif (!(flags & CS_HARD)) {\r\nrv = is_valid_state(mdev, ns);\r\nif (rv < SS_SUCCESS) {\r\nif (is_valid_state(mdev, os) == rv)\r\nrv = is_valid_state_transition(mdev, ns, os);\r\n} else\r\nrv = is_valid_state_transition(mdev, ns, os);\r\n}\r\nif (rv < SS_SUCCESS) {\r\nif (flags & CS_VERBOSE)\r\nprint_st_err(mdev, os, ns, rv);\r\nreturn rv;\r\n}\r\nif (warn_sync_abort)\r\ndev_warn(DEV, "%s aborted.\n", warn_sync_abort);\r\n{\r\nchar *pbp, pb[300];\r\npbp = pb;\r\n*pbp = 0;\r\nif (ns.role != os.role)\r\npbp += sprintf(pbp, "role( %s -> %s ) ",\r\ndrbd_role_str(os.role),\r\ndrbd_role_str(ns.role));\r\nif (ns.peer != os.peer)\r\npbp += sprintf(pbp, "peer( %s -> %s ) ",\r\ndrbd_role_str(os.peer),\r\ndrbd_role_str(ns.peer));\r\nif (ns.conn != os.conn)\r\npbp += sprintf(pbp, "conn( %s -> %s ) ",\r\ndrbd_conn_str(os.conn),\r\ndrbd_conn_str(ns.conn));\r\nif (ns.disk != os.disk)\r\npbp += sprintf(pbp, "disk( %s -> %s ) ",\r\ndrbd_disk_str(os.disk),\r\ndrbd_disk_str(ns.disk));\r\nif (ns.pdsk != os.pdsk)\r\npbp += sprintf(pbp, "pdsk( %s -> %s ) ",\r\ndrbd_disk_str(os.pdsk),\r\ndrbd_disk_str(ns.pdsk));\r\nif (is_susp(ns) != is_susp(os))\r\npbp += sprintf(pbp, "susp( %d -> %d ) ",\r\nis_susp(os),\r\nis_susp(ns));\r\nif (ns.aftr_isp != os.aftr_isp)\r\npbp += sprintf(pbp, "aftr_isp( %d -> %d ) ",\r\nos.aftr_isp,\r\nns.aftr_isp);\r\nif (ns.peer_isp != os.peer_isp)\r\npbp += sprintf(pbp, "peer_isp( %d -> %d ) ",\r\nos.peer_isp,\r\nns.peer_isp);\r\nif (ns.user_isp != os.user_isp)\r\npbp += sprintf(pbp, "user_isp( %d -> %d ) ",\r\nos.user_isp,\r\nns.user_isp);\r\ndev_info(DEV, "%s\n", pb);\r\n}\r\nif (ns.disk == D_DISKLESS &&\r\nns.conn == C_STANDALONE &&\r\nns.role == R_SECONDARY &&\r\n!test_and_set_bit(CONFIG_PENDING, &mdev->flags))\r\nset_bit(DEVICE_DYING, &mdev->flags);\r\nif ((os.disk != D_FAILED && ns.disk == D_FAILED) ||\r\n(os.disk != D_DISKLESS && ns.disk == D_DISKLESS))\r\natomic_inc(&mdev->local_cnt);\r\nmdev->state = ns;\r\nif (os.disk == D_ATTACHING && ns.disk >= D_NEGOTIATING)\r\ndrbd_print_uuids(mdev, "attached to UUIDs");\r\nwake_up(&mdev->misc_wait);\r\nwake_up(&mdev->state_wait);\r\nif ((os.conn == C_VERIFY_S || os.conn == C_VERIFY_T) &&\r\nns.conn < C_CONNECTED) {\r\nmdev->ov_start_sector =\r\nBM_BIT_TO_SECT(drbd_bm_bits(mdev) - mdev->ov_left);\r\ndev_info(DEV, "Online Verify reached sector %llu\n",\r\n(unsigned long long)mdev->ov_start_sector);\r\n}\r\nif ((os.conn == C_PAUSED_SYNC_T || os.conn == C_PAUSED_SYNC_S) &&\r\n(ns.conn == C_SYNC_TARGET || ns.conn == C_SYNC_SOURCE)) {\r\ndev_info(DEV, "Syncer continues.\n");\r\nmdev->rs_paused += (long)jiffies\r\n-(long)mdev->rs_mark_time[mdev->rs_last_mark];\r\nif (ns.conn == C_SYNC_TARGET)\r\nmod_timer(&mdev->resync_timer, jiffies);\r\n}\r\nif ((os.conn == C_SYNC_TARGET || os.conn == C_SYNC_SOURCE) &&\r\n(ns.conn == C_PAUSED_SYNC_T || ns.conn == C_PAUSED_SYNC_S)) {\r\ndev_info(DEV, "Resync suspended\n");\r\nmdev->rs_mark_time[mdev->rs_last_mark] = jiffies;\r\n}\r\nif (os.conn == C_CONNECTED &&\r\n(ns.conn == C_VERIFY_S || ns.conn == C_VERIFY_T)) {\r\nunsigned long now = jiffies;\r\nint i;\r\nset_ov_position(mdev, ns.conn);\r\nmdev->rs_start = now;\r\nmdev->rs_last_events = 0;\r\nmdev->rs_last_sect_ev = 0;\r\nmdev->ov_last_oos_size = 0;\r\nmdev->ov_last_oos_start = 0;\r\nfor (i = 0; i < DRBD_SYNC_MARKS; i++) {\r\nmdev->rs_mark_left[i] = mdev->ov_left;\r\nmdev->rs_mark_time[i] = now;\r\n}\r\ndrbd_rs_controller_reset(mdev);\r\nif (ns.conn == C_VERIFY_S) {\r\ndev_info(DEV, "Starting Online Verify from sector %llu\n",\r\n(unsigned long long)mdev->ov_position);\r\nmod_timer(&mdev->resync_timer, jiffies);\r\n}\r\n}\r\nif (get_ldev(mdev)) {\r\nu32 mdf = mdev->ldev->md.flags & ~(MDF_CONSISTENT|MDF_PRIMARY_IND|\r\nMDF_CONNECTED_IND|MDF_WAS_UP_TO_DATE|\r\nMDF_PEER_OUT_DATED|MDF_CRASHED_PRIMARY);\r\nif (test_bit(CRASHED_PRIMARY, &mdev->flags))\r\nmdf |= MDF_CRASHED_PRIMARY;\r\nif (mdev->state.role == R_PRIMARY ||\r\n(mdev->state.pdsk < D_INCONSISTENT && mdev->state.peer == R_PRIMARY))\r\nmdf |= MDF_PRIMARY_IND;\r\nif (mdev->state.conn > C_WF_REPORT_PARAMS)\r\nmdf |= MDF_CONNECTED_IND;\r\nif (mdev->state.disk > D_INCONSISTENT)\r\nmdf |= MDF_CONSISTENT;\r\nif (mdev->state.disk > D_OUTDATED)\r\nmdf |= MDF_WAS_UP_TO_DATE;\r\nif (mdev->state.pdsk <= D_OUTDATED && mdev->state.pdsk >= D_INCONSISTENT)\r\nmdf |= MDF_PEER_OUT_DATED;\r\nif (mdf != mdev->ldev->md.flags) {\r\nmdev->ldev->md.flags = mdf;\r\ndrbd_md_mark_dirty(mdev);\r\n}\r\nif (os.disk < D_CONSISTENT && ns.disk >= D_CONSISTENT)\r\ndrbd_set_ed_uuid(mdev, mdev->ldev->md.uuid[UI_CURRENT]);\r\nput_ldev(mdev);\r\n}\r\nif (os.disk == D_INCONSISTENT && os.pdsk == D_INCONSISTENT &&\r\nos.peer == R_SECONDARY && ns.peer == R_PRIMARY)\r\nset_bit(CONSIDER_RESYNC, &mdev->flags);\r\nif (os.conn != C_DISCONNECTING && ns.conn == C_DISCONNECTING)\r\ndrbd_thread_stop_nowait(&mdev->receiver);\r\nif (os.conn != C_STANDALONE && ns.conn == C_STANDALONE)\r\ndrbd_thread_stop_nowait(&mdev->receiver);\r\nif (os.conn > C_TEAR_DOWN &&\r\nns.conn <= C_TEAR_DOWN && ns.conn >= C_TIMEOUT)\r\ndrbd_thread_restart_nowait(&mdev->receiver);\r\nif (os.conn < C_CONNECTED && ns.conn >= C_CONNECTED)\r\ndrbd_resume_al(mdev);\r\nascw = kmalloc(sizeof(*ascw), GFP_ATOMIC);\r\nif (ascw) {\r\nascw->os = os;\r\nascw->ns = ns;\r\nascw->flags = flags;\r\nascw->w.cb = w_after_state_ch;\r\nascw->done = done;\r\ndrbd_queue_work(&mdev->data.work, &ascw->w);\r\n} else {\r\ndev_warn(DEV, "Could not kmalloc an ascw\n");\r\n}\r\nreturn rv;\r\n}\r\nstatic int w_after_state_ch(struct drbd_conf *mdev, struct drbd_work *w, int unused)\r\n{\r\nstruct after_state_chg_work *ascw =\r\ncontainer_of(w, struct after_state_chg_work, w);\r\nafter_state_ch(mdev, ascw->os, ascw->ns, ascw->flags);\r\nif (ascw->flags & CS_WAIT_COMPLETE) {\r\nD_ASSERT(ascw->done != NULL);\r\ncomplete(ascw->done);\r\n}\r\nkfree(ascw);\r\nreturn 1;\r\n}\r\nstatic void abw_start_sync(struct drbd_conf *mdev, int rv)\r\n{\r\nif (rv) {\r\ndev_err(DEV, "Writing the bitmap failed not starting resync.\n");\r\n_drbd_request_state(mdev, NS(conn, C_CONNECTED), CS_VERBOSE);\r\nreturn;\r\n}\r\nswitch (mdev->state.conn) {\r\ncase C_STARTING_SYNC_T:\r\n_drbd_request_state(mdev, NS(conn, C_WF_SYNC_UUID), CS_VERBOSE);\r\nbreak;\r\ncase C_STARTING_SYNC_S:\r\ndrbd_start_resync(mdev, C_SYNC_SOURCE);\r\nbreak;\r\n}\r\n}\r\nint drbd_bitmap_io_from_worker(struct drbd_conf *mdev,\r\nint (*io_fn)(struct drbd_conf *),\r\nchar *why, enum bm_flag flags)\r\n{\r\nint rv;\r\nD_ASSERT(current == mdev->worker.task);\r\nset_bit(SUSPEND_IO, &mdev->flags);\r\ndrbd_bm_lock(mdev, why, flags);\r\nrv = io_fn(mdev);\r\ndrbd_bm_unlock(mdev);\r\ndrbd_resume_io(mdev);\r\nreturn rv;\r\n}\r\nstatic void after_state_ch(struct drbd_conf *mdev, union drbd_state os,\r\nunion drbd_state ns, enum chg_state_flags flags)\r\n{\r\nenum drbd_fencing_p fp;\r\nenum drbd_req_event what = nothing;\r\nunion drbd_state nsm = (union drbd_state){ .i = -1 };\r\nif (os.conn != C_CONNECTED && ns.conn == C_CONNECTED) {\r\nclear_bit(CRASHED_PRIMARY, &mdev->flags);\r\nif (mdev->p_uuid)\r\nmdev->p_uuid[UI_FLAGS] &= ~((u64)2);\r\n}\r\nfp = FP_DONT_CARE;\r\nif (get_ldev(mdev)) {\r\nfp = mdev->ldev->dc.fencing;\r\nput_ldev(mdev);\r\n}\r\ndrbd_bcast_state(mdev, ns);\r\nif (!(os.role == R_PRIMARY && os.disk < D_UP_TO_DATE && os.pdsk < D_UP_TO_DATE) &&\r\n(ns.role == R_PRIMARY && ns.disk < D_UP_TO_DATE && ns.pdsk < D_UP_TO_DATE))\r\ndrbd_khelper(mdev, "pri-on-incon-degr");\r\nnsm.i = -1;\r\nif (ns.susp_nod) {\r\nif (os.conn < C_CONNECTED && ns.conn >= C_CONNECTED)\r\nwhat = resend;\r\nif (os.disk == D_ATTACHING && ns.disk > D_ATTACHING)\r\nwhat = restart_frozen_disk_io;\r\nif (what != nothing)\r\nnsm.susp_nod = 0;\r\n}\r\nif (ns.susp_fen) {\r\nif (os.pdsk > D_OUTDATED && ns.pdsk <= D_OUTDATED) {\r\ntl_clear(mdev);\r\nif (test_bit(NEW_CUR_UUID, &mdev->flags)) {\r\ndrbd_uuid_new_current(mdev);\r\nclear_bit(NEW_CUR_UUID, &mdev->flags);\r\n}\r\nspin_lock_irq(&mdev->req_lock);\r\n_drbd_set_state(_NS(mdev, susp_fen, 0), CS_VERBOSE, NULL);\r\nspin_unlock_irq(&mdev->req_lock);\r\n}\r\nif (os.conn < C_CONNECTED && ns.conn >= C_CONNECTED) {\r\nclear_bit(NEW_CUR_UUID, &mdev->flags);\r\nwhat = resend;\r\nnsm.susp_fen = 0;\r\n}\r\n}\r\nif (what != nothing) {\r\nspin_lock_irq(&mdev->req_lock);\r\n_tl_restart(mdev, what);\r\nnsm.i &= mdev->state.i;\r\n_drbd_set_state(mdev, nsm, CS_VERBOSE, NULL);\r\nspin_unlock_irq(&mdev->req_lock);\r\n}\r\nif ((os.conn != C_SYNC_SOURCE && os.conn != C_PAUSED_SYNC_S) &&\r\n(ns.conn == C_SYNC_SOURCE || ns.conn == C_PAUSED_SYNC_S) &&\r\nmdev->agreed_pro_version >= 96 && get_ldev(mdev)) {\r\ndrbd_gen_and_send_sync_uuid(mdev);\r\nput_ldev(mdev);\r\n}\r\nif (os.pdsk == D_DISKLESS && ns.pdsk > D_DISKLESS) {\r\ndrbd_send_uuids(mdev);\r\ndrbd_send_state(mdev);\r\n}\r\nif (os.conn != C_WF_BITMAP_S && ns.conn == C_WF_BITMAP_S &&\r\nmdev->state.conn == C_WF_BITMAP_S)\r\ndrbd_queue_bitmap_io(mdev, &drbd_send_bitmap, NULL,\r\n"send_bitmap (WFBitMapS)",\r\nBM_LOCKED_TEST_ALLOWED);\r\nif ((os.pdsk >= D_INCONSISTENT &&\r\nos.pdsk != D_UNKNOWN &&\r\nos.pdsk != D_OUTDATED)\r\n&& (ns.pdsk < D_INCONSISTENT ||\r\nns.pdsk == D_UNKNOWN ||\r\nns.pdsk == D_OUTDATED)) {\r\nif (get_ldev(mdev)) {\r\nif ((ns.role == R_PRIMARY || ns.peer == R_PRIMARY) &&\r\nmdev->ldev->md.uuid[UI_BITMAP] == 0 && ns.disk >= D_UP_TO_DATE) {\r\nif (is_susp(mdev->state)) {\r\nset_bit(NEW_CUR_UUID, &mdev->flags);\r\n} else {\r\ndrbd_uuid_new_current(mdev);\r\ndrbd_send_uuids(mdev);\r\n}\r\n}\r\nput_ldev(mdev);\r\n}\r\n}\r\nif (ns.pdsk < D_INCONSISTENT && get_ldev(mdev)) {\r\nif (ns.peer == R_PRIMARY && mdev->ldev->md.uuid[UI_BITMAP] == 0) {\r\ndrbd_uuid_new_current(mdev);\r\ndrbd_send_uuids(mdev);\r\n}\r\nif (os.peer == R_PRIMARY && ns.peer == R_SECONDARY)\r\ndrbd_bitmap_io_from_worker(mdev, &drbd_bm_write,\r\n"demote diskless peer", BM_LOCKED_SET_ALLOWED);\r\nput_ldev(mdev);\r\n}\r\nif (os.role == R_PRIMARY && ns.role == R_SECONDARY &&\r\nmdev->state.conn <= C_CONNECTED && get_ldev(mdev)) {\r\ndrbd_bitmap_io_from_worker(mdev, &drbd_bm_write,\r\n"demote", BM_LOCKED_TEST_ALLOWED);\r\nput_ldev(mdev);\r\n}\r\nif (ns.conn >= C_CONNECTED &&\r\nos.disk == D_ATTACHING && ns.disk == D_NEGOTIATING) {\r\ndrbd_send_sizes(mdev, 0, 0);\r\ndrbd_send_uuids(mdev);\r\ndrbd_send_state(mdev);\r\n}\r\nif (ns.conn >= C_CONNECTED &&\r\n((os.aftr_isp != ns.aftr_isp) ||\r\n(os.user_isp != ns.user_isp)))\r\ndrbd_send_state(mdev);\r\nif ((!os.aftr_isp && !os.peer_isp && !os.user_isp) &&\r\n(ns.aftr_isp || ns.peer_isp || ns.user_isp))\r\nsuspend_other_sg(mdev);\r\nif (os.conn == C_WF_REPORT_PARAMS && ns.conn >= C_CONNECTED)\r\ndrbd_send_state(mdev);\r\nif (os.conn != C_AHEAD && ns.conn == C_AHEAD)\r\ndrbd_send_state(mdev);\r\nif ((os.conn != C_STARTING_SYNC_T && ns.conn == C_STARTING_SYNC_T) ||\r\n(os.conn != C_STARTING_SYNC_S && ns.conn == C_STARTING_SYNC_S))\r\ndrbd_queue_bitmap_io(mdev,\r\n&drbd_bmio_set_n_write, &abw_start_sync,\r\n"set_n_write from StartingSync", BM_LOCKED_TEST_ALLOWED);\r\nif (os.conn < C_CONNECTED && ns.conn < C_CONNECTED &&\r\nos.disk > D_INCONSISTENT && ns.disk == D_INCONSISTENT)\r\ndrbd_queue_bitmap_io(mdev, &drbd_bmio_set_n_write, NULL,\r\n"set_n_write from invalidate", BM_LOCKED_MASK);\r\nif (os.disk != D_FAILED && ns.disk == D_FAILED) {\r\nenum drbd_io_error_p eh;\r\nint was_io_error;\r\neh = mdev->ldev->dc.on_io_error;\r\nwas_io_error = test_and_clear_bit(WAS_IO_ERROR, &mdev->flags);\r\nif (mdev->state.disk != D_FAILED)\r\ndev_err(DEV,\r\n"ASSERT FAILED: disk is %s during detach\n",\r\ndrbd_disk_str(mdev->state.disk));\r\nif (drbd_send_state(mdev))\r\ndev_warn(DEV, "Notified peer that I am detaching my disk\n");\r\nelse\r\ndev_err(DEV, "Sending state for detaching disk failed\n");\r\ndrbd_rs_cancel_all(mdev);\r\ndrbd_md_sync(mdev);\r\nput_ldev(mdev);\r\nif (was_io_error && eh == EP_CALL_HELPER)\r\ndrbd_khelper(mdev, "local-io-error");\r\n}\r\nif (os.disk != D_DISKLESS && ns.disk == D_DISKLESS) {\r\nif (mdev->state.disk != D_DISKLESS)\r\ndev_err(DEV,\r\n"ASSERT FAILED: disk is %s while going diskless\n",\r\ndrbd_disk_str(mdev->state.disk));\r\nmdev->rs_total = 0;\r\nmdev->rs_failed = 0;\r\natomic_set(&mdev->rs_pending_cnt, 0);\r\nif (drbd_send_state(mdev))\r\ndev_warn(DEV, "Notified peer that I'm now diskless.\n");\r\nput_ldev(mdev);\r\n}\r\nif (os.disk == D_UP_TO_DATE && ns.disk == D_INCONSISTENT)\r\ndrbd_send_state(mdev);\r\nif (ns.disk > D_NEGOTIATING && ns.pdsk > D_NEGOTIATING &&\r\ntest_and_clear_bit(RESYNC_AFTER_NEG, &mdev->flags)) {\r\nif (ns.conn == C_CONNECTED)\r\nresync_after_online_grow(mdev);\r\n}\r\nif ((os.conn > C_CONNECTED && ns.conn <= C_CONNECTED) ||\r\n(os.peer_isp && !ns.peer_isp) ||\r\n(os.user_isp && !ns.user_isp))\r\nresume_next_sg(mdev);\r\nif (os.disk < D_UP_TO_DATE && os.conn >= C_SYNC_SOURCE && ns.conn == C_CONNECTED)\r\ndrbd_send_state(mdev);\r\nif (os.conn > C_CONNECTED && ns.conn <= C_CONNECTED && get_ldev(mdev)) {\r\ndrbd_queue_bitmap_io(mdev, &drbd_bm_write, NULL,\r\n"write from resync_finished", BM_LOCKED_SET_ALLOWED);\r\nput_ldev(mdev);\r\n}\r\nif (ns.conn == C_STANDALONE && !is_susp(ns) && mdev->tl_hash)\r\ndrbd_free_tl_hash(mdev);\r\nif (os.conn == C_STANDALONE && ns.conn == C_UNCONNECTED)\r\ndrbd_thread_start(&mdev->receiver);\r\nif (ns.disk == D_DISKLESS &&\r\nns.conn == C_STANDALONE &&\r\nns.role == R_SECONDARY) {\r\nif (os.aftr_isp != ns.aftr_isp)\r\nresume_next_sg(mdev);\r\nif (test_bit(DEVICE_DYING, &mdev->flags))\r\ndrbd_thread_stop_nowait(&mdev->worker);\r\n}\r\ndrbd_md_sync(mdev);\r\n}\r\nstatic int drbd_thread_setup(void *arg)\r\n{\r\nstruct drbd_thread *thi = (struct drbd_thread *) arg;\r\nstruct drbd_conf *mdev = thi->mdev;\r\nunsigned long flags;\r\nint retval;\r\nrestart:\r\nretval = thi->function(thi);\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nif (thi->t_state == Restarting) {\r\ndev_info(DEV, "Restarting %s\n", current->comm);\r\nthi->t_state = Running;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\ngoto restart;\r\n}\r\nthi->task = NULL;\r\nthi->t_state = None;\r\nsmp_mb();\r\ncomplete(&thi->stop);\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\ndev_info(DEV, "Terminating %s\n", current->comm);\r\nmodule_put(THIS_MODULE);\r\nreturn retval;\r\n}\r\nstatic void drbd_thread_init(struct drbd_conf *mdev, struct drbd_thread *thi,\r\nint (*func) (struct drbd_thread *))\r\n{\r\nspin_lock_init(&thi->t_lock);\r\nthi->task = NULL;\r\nthi->t_state = None;\r\nthi->function = func;\r\nthi->mdev = mdev;\r\n}\r\nint drbd_thread_start(struct drbd_thread *thi)\r\n{\r\nstruct drbd_conf *mdev = thi->mdev;\r\nstruct task_struct *nt;\r\nunsigned long flags;\r\nconst char *me =\r\nthi == &mdev->receiver ? "receiver" :\r\nthi == &mdev->asender ? "asender" :\r\nthi == &mdev->worker ? "worker" : "NONSENSE";\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nswitch (thi->t_state) {\r\ncase None:\r\ndev_info(DEV, "Starting %s thread (from %s [%d])\n",\r\nme, current->comm, current->pid);\r\nif (!try_module_get(THIS_MODULE)) {\r\ndev_err(DEV, "Failed to get module reference in drbd_thread_start\n");\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nreturn false;\r\n}\r\ninit_completion(&thi->stop);\r\nD_ASSERT(thi->task == NULL);\r\nthi->reset_cpu_mask = 1;\r\nthi->t_state = Running;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nflush_signals(current);\r\nnt = kthread_create(drbd_thread_setup, (void *) thi,\r\n"drbd%d_%s", mdev_to_minor(mdev), me);\r\nif (IS_ERR(nt)) {\r\ndev_err(DEV, "Couldn't start thread\n");\r\nmodule_put(THIS_MODULE);\r\nreturn false;\r\n}\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nthi->task = nt;\r\nthi->t_state = Running;\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nwake_up_process(nt);\r\nbreak;\r\ncase Exiting:\r\nthi->t_state = Restarting;\r\ndev_info(DEV, "Restarting %s thread (from %s [%d])\n",\r\nme, current->comm, current->pid);\r\ncase Running:\r\ncase Restarting:\r\ndefault:\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nbreak;\r\n}\r\nreturn true;\r\n}\r\nvoid _drbd_thread_stop(struct drbd_thread *thi, int restart, int wait)\r\n{\r\nunsigned long flags;\r\nenum drbd_thread_state ns = restart ? Restarting : Exiting;\r\nspin_lock_irqsave(&thi->t_lock, flags);\r\nif (thi->t_state == None) {\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nif (restart)\r\ndrbd_thread_start(thi);\r\nreturn;\r\n}\r\nif (thi->t_state != ns) {\r\nif (thi->task == NULL) {\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nreturn;\r\n}\r\nthi->t_state = ns;\r\nsmp_mb();\r\ninit_completion(&thi->stop);\r\nif (thi->task != current)\r\nforce_sig(DRBD_SIGKILL, thi->task);\r\n}\r\nspin_unlock_irqrestore(&thi->t_lock, flags);\r\nif (wait)\r\nwait_for_completion(&thi->stop);\r\n}\r\nvoid drbd_calc_cpu_mask(struct drbd_conf *mdev)\r\n{\r\nint ord, cpu;\r\nif (cpumask_weight(mdev->cpu_mask))\r\nreturn;\r\nord = mdev_to_minor(mdev) % cpumask_weight(cpu_online_mask);\r\nfor_each_online_cpu(cpu) {\r\nif (ord-- == 0) {\r\ncpumask_set_cpu(cpu, mdev->cpu_mask);\r\nreturn;\r\n}\r\n}\r\ncpumask_setall(mdev->cpu_mask);\r\n}\r\nvoid drbd_thread_current_set_cpu(struct drbd_conf *mdev)\r\n{\r\nstruct task_struct *p = current;\r\nstruct drbd_thread *thi =\r\np == mdev->asender.task ? &mdev->asender :\r\np == mdev->receiver.task ? &mdev->receiver :\r\np == mdev->worker.task ? &mdev->worker :\r\nNULL;\r\nERR_IF(thi == NULL)\r\nreturn;\r\nif (!thi->reset_cpu_mask)\r\nreturn;\r\nthi->reset_cpu_mask = 0;\r\nset_cpus_allowed_ptr(p, mdev->cpu_mask);\r\n}\r\nint _drbd_send_cmd(struct drbd_conf *mdev, struct socket *sock,\r\nenum drbd_packets cmd, struct p_header80 *h,\r\nsize_t size, unsigned msg_flags)\r\n{\r\nint sent, ok;\r\nERR_IF(!h) return false;\r\nERR_IF(!size) return false;\r\nh->magic = BE_DRBD_MAGIC;\r\nh->command = cpu_to_be16(cmd);\r\nh->length = cpu_to_be16(size-sizeof(struct p_header80));\r\nsent = drbd_send(mdev, sock, h, size, msg_flags);\r\nok = (sent == size);\r\nif (!ok && !signal_pending(current))\r\ndev_warn(DEV, "short sent %s size=%d sent=%d\n",\r\ncmdname(cmd), (int)size, sent);\r\nreturn ok;\r\n}\r\nint drbd_send_cmd(struct drbd_conf *mdev, int use_data_socket,\r\nenum drbd_packets cmd, struct p_header80 *h, size_t size)\r\n{\r\nint ok = 0;\r\nstruct socket *sock;\r\nif (use_data_socket) {\r\nmutex_lock(&mdev->data.mutex);\r\nsock = mdev->data.socket;\r\n} else {\r\nmutex_lock(&mdev->meta.mutex);\r\nsock = mdev->meta.socket;\r\n}\r\nif (likely(sock != NULL))\r\nok = _drbd_send_cmd(mdev, sock, cmd, h, size, 0);\r\nif (use_data_socket)\r\nmutex_unlock(&mdev->data.mutex);\r\nelse\r\nmutex_unlock(&mdev->meta.mutex);\r\nreturn ok;\r\n}\r\nint drbd_send_cmd2(struct drbd_conf *mdev, enum drbd_packets cmd, char *data,\r\nsize_t size)\r\n{\r\nstruct p_header80 h;\r\nint ok;\r\nh.magic = BE_DRBD_MAGIC;\r\nh.command = cpu_to_be16(cmd);\r\nh.length = cpu_to_be16(size);\r\nif (!drbd_get_data_sock(mdev))\r\nreturn 0;\r\nok = (sizeof(h) ==\r\ndrbd_send(mdev, mdev->data.socket, &h, sizeof(h), 0));\r\nok = ok && (size ==\r\ndrbd_send(mdev, mdev->data.socket, data, size, 0));\r\ndrbd_put_data_sock(mdev);\r\nreturn ok;\r\n}\r\nint drbd_send_sync_param(struct drbd_conf *mdev, struct syncer_conf *sc)\r\n{\r\nstruct p_rs_param_95 *p;\r\nstruct socket *sock;\r\nint size, rv;\r\nconst int apv = mdev->agreed_pro_version;\r\nsize = apv <= 87 ? sizeof(struct p_rs_param)\r\n: apv == 88 ? sizeof(struct p_rs_param)\r\n+ strlen(mdev->sync_conf.verify_alg) + 1\r\n: apv <= 94 ? sizeof(struct p_rs_param_89)\r\n: sizeof(struct p_rs_param_95);\r\nmutex_lock(&mdev->data.mutex);\r\nsock = mdev->data.socket;\r\nif (likely(sock != NULL)) {\r\nenum drbd_packets cmd = apv >= 89 ? P_SYNC_PARAM89 : P_SYNC_PARAM;\r\np = &mdev->data.sbuf.rs_param_95;\r\nmemset(p->verify_alg, 0, 2 * SHARED_SECRET_MAX);\r\np->rate = cpu_to_be32(sc->rate);\r\np->c_plan_ahead = cpu_to_be32(sc->c_plan_ahead);\r\np->c_delay_target = cpu_to_be32(sc->c_delay_target);\r\np->c_fill_target = cpu_to_be32(sc->c_fill_target);\r\np->c_max_rate = cpu_to_be32(sc->c_max_rate);\r\nif (apv >= 88)\r\nstrcpy(p->verify_alg, mdev->sync_conf.verify_alg);\r\nif (apv >= 89)\r\nstrcpy(p->csums_alg, mdev->sync_conf.csums_alg);\r\nrv = _drbd_send_cmd(mdev, sock, cmd, &p->head, size, 0);\r\n} else\r\nrv = 0;\r\nmutex_unlock(&mdev->data.mutex);\r\nreturn rv;\r\n}\r\nint drbd_send_protocol(struct drbd_conf *mdev)\r\n{\r\nstruct p_protocol *p;\r\nint size, cf, rv;\r\nsize = sizeof(struct p_protocol);\r\nif (mdev->agreed_pro_version >= 87)\r\nsize += strlen(mdev->net_conf->integrity_alg) + 1;\r\np = kmalloc(size, GFP_NOIO);\r\nif (p == NULL)\r\nreturn 0;\r\np->protocol = cpu_to_be32(mdev->net_conf->wire_protocol);\r\np->after_sb_0p = cpu_to_be32(mdev->net_conf->after_sb_0p);\r\np->after_sb_1p = cpu_to_be32(mdev->net_conf->after_sb_1p);\r\np->after_sb_2p = cpu_to_be32(mdev->net_conf->after_sb_2p);\r\np->two_primaries = cpu_to_be32(mdev->net_conf->two_primaries);\r\ncf = 0;\r\nif (mdev->net_conf->want_lose)\r\ncf |= CF_WANT_LOSE;\r\nif (mdev->net_conf->dry_run) {\r\nif (mdev->agreed_pro_version >= 92)\r\ncf |= CF_DRY_RUN;\r\nelse {\r\ndev_err(DEV, "--dry-run is not supported by peer");\r\nkfree(p);\r\nreturn -1;\r\n}\r\n}\r\np->conn_flags = cpu_to_be32(cf);\r\nif (mdev->agreed_pro_version >= 87)\r\nstrcpy(p->integrity_alg, mdev->net_conf->integrity_alg);\r\nrv = drbd_send_cmd(mdev, USE_DATA_SOCKET, P_PROTOCOL,\r\n(struct p_header80 *)p, size);\r\nkfree(p);\r\nreturn rv;\r\n}\r\nint _drbd_send_uuids(struct drbd_conf *mdev, u64 uuid_flags)\r\n{\r\nstruct p_uuids p;\r\nint i;\r\nif (!get_ldev_if_state(mdev, D_NEGOTIATING))\r\nreturn 1;\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\np.uuid[i] = mdev->ldev ? cpu_to_be64(mdev->ldev->md.uuid[i]) : 0;\r\nmdev->comm_bm_set = drbd_bm_total_weight(mdev);\r\np.uuid[UI_SIZE] = cpu_to_be64(mdev->comm_bm_set);\r\nuuid_flags |= mdev->net_conf->want_lose ? 1 : 0;\r\nuuid_flags |= test_bit(CRASHED_PRIMARY, &mdev->flags) ? 2 : 0;\r\nuuid_flags |= mdev->new_state_tmp.disk == D_INCONSISTENT ? 4 : 0;\r\np.uuid[UI_FLAGS] = cpu_to_be64(uuid_flags);\r\nput_ldev(mdev);\r\nreturn drbd_send_cmd(mdev, USE_DATA_SOCKET, P_UUIDS,\r\n(struct p_header80 *)&p, sizeof(p));\r\n}\r\nint drbd_send_uuids(struct drbd_conf *mdev)\r\n{\r\nreturn _drbd_send_uuids(mdev, 0);\r\n}\r\nint drbd_send_uuids_skip_initial_sync(struct drbd_conf *mdev)\r\n{\r\nreturn _drbd_send_uuids(mdev, 8);\r\n}\r\nvoid drbd_print_uuids(struct drbd_conf *mdev, const char *text)\r\n{\r\nif (get_ldev_if_state(mdev, D_NEGOTIATING)) {\r\nu64 *uuid = mdev->ldev->md.uuid;\r\ndev_info(DEV, "%s %016llX:%016llX:%016llX:%016llX\n",\r\ntext,\r\n(unsigned long long)uuid[UI_CURRENT],\r\n(unsigned long long)uuid[UI_BITMAP],\r\n(unsigned long long)uuid[UI_HISTORY_START],\r\n(unsigned long long)uuid[UI_HISTORY_END]);\r\nput_ldev(mdev);\r\n} else {\r\ndev_info(DEV, "%s effective data uuid: %016llX\n",\r\ntext,\r\n(unsigned long long)mdev->ed_uuid);\r\n}\r\n}\r\nint drbd_gen_and_send_sync_uuid(struct drbd_conf *mdev)\r\n{\r\nstruct p_rs_uuid p;\r\nu64 uuid;\r\nD_ASSERT(mdev->state.disk == D_UP_TO_DATE);\r\nuuid = mdev->ldev->md.uuid[UI_BITMAP] + UUID_NEW_BM_OFFSET;\r\ndrbd_uuid_set(mdev, UI_BITMAP, uuid);\r\ndrbd_print_uuids(mdev, "updated sync UUID");\r\ndrbd_md_sync(mdev);\r\np.uuid = cpu_to_be64(uuid);\r\nreturn drbd_send_cmd(mdev, USE_DATA_SOCKET, P_SYNC_UUID,\r\n(struct p_header80 *)&p, sizeof(p));\r\n}\r\nint drbd_send_sizes(struct drbd_conf *mdev, int trigger_reply, enum dds_flags flags)\r\n{\r\nstruct p_sizes p;\r\nsector_t d_size, u_size;\r\nint q_order_type, max_bio_size;\r\nint ok;\r\nif (get_ldev_if_state(mdev, D_NEGOTIATING)) {\r\nD_ASSERT(mdev->ldev->backing_bdev);\r\nd_size = drbd_get_max_capacity(mdev->ldev);\r\nu_size = mdev->ldev->dc.disk_size;\r\nq_order_type = drbd_queue_order_type(mdev);\r\nmax_bio_size = queue_max_hw_sectors(mdev->ldev->backing_bdev->bd_disk->queue) << 9;\r\nmax_bio_size = min_t(int, max_bio_size, DRBD_MAX_BIO_SIZE);\r\nput_ldev(mdev);\r\n} else {\r\nd_size = 0;\r\nu_size = 0;\r\nq_order_type = QUEUE_ORDERED_NONE;\r\nmax_bio_size = DRBD_MAX_BIO_SIZE;\r\n}\r\np.d_size = cpu_to_be64(d_size);\r\np.u_size = cpu_to_be64(u_size);\r\np.c_size = cpu_to_be64(trigger_reply ? 0 : drbd_get_capacity(mdev->this_bdev));\r\np.max_bio_size = cpu_to_be32(max_bio_size);\r\np.queue_order_type = cpu_to_be16(q_order_type);\r\np.dds_flags = cpu_to_be16(flags);\r\nok = drbd_send_cmd(mdev, USE_DATA_SOCKET, P_SIZES,\r\n(struct p_header80 *)&p, sizeof(p));\r\nreturn ok;\r\n}\r\nint drbd_send_state(struct drbd_conf *mdev)\r\n{\r\nstruct socket *sock;\r\nstruct p_state p;\r\nint ok = 0;\r\ndrbd_state_lock(mdev);\r\nmutex_lock(&mdev->data.mutex);\r\np.state = cpu_to_be32(mdev->state.i);\r\nsock = mdev->data.socket;\r\nif (likely(sock != NULL)) {\r\nok = _drbd_send_cmd(mdev, sock, P_STATE,\r\n(struct p_header80 *)&p, sizeof(p), 0);\r\n}\r\nmutex_unlock(&mdev->data.mutex);\r\ndrbd_state_unlock(mdev);\r\nreturn ok;\r\n}\r\nint drbd_send_state_req(struct drbd_conf *mdev,\r\nunion drbd_state mask, union drbd_state val)\r\n{\r\nstruct p_req_state p;\r\np.mask = cpu_to_be32(mask.i);\r\np.val = cpu_to_be32(val.i);\r\nreturn drbd_send_cmd(mdev, USE_DATA_SOCKET, P_STATE_CHG_REQ,\r\n(struct p_header80 *)&p, sizeof(p));\r\n}\r\nint drbd_send_sr_reply(struct drbd_conf *mdev, enum drbd_state_rv retcode)\r\n{\r\nstruct p_req_state_reply p;\r\np.retcode = cpu_to_be32(retcode);\r\nreturn drbd_send_cmd(mdev, USE_META_SOCKET, P_STATE_CHG_REPLY,\r\n(struct p_header80 *)&p, sizeof(p));\r\n}\r\nint fill_bitmap_rle_bits(struct drbd_conf *mdev,\r\nstruct p_compressed_bm *p,\r\nstruct bm_xfer_ctx *c)\r\n{\r\nstruct bitstream bs;\r\nunsigned long plain_bits;\r\nunsigned long tmp;\r\nunsigned long rl;\r\nunsigned len;\r\nunsigned toggle;\r\nint bits;\r\nif ((mdev->sync_conf.use_rle == 0) ||\r\n(mdev->agreed_pro_version < 90))\r\nreturn 0;\r\nif (c->bit_offset >= c->bm_bits)\r\nreturn 0;\r\nbitstream_init(&bs, p->code, BM_PACKET_VLI_BYTES_MAX, 0);\r\nmemset(p->code, 0, BM_PACKET_VLI_BYTES_MAX);\r\nplain_bits = 0;\r\ntoggle = 2;\r\ndo {\r\ntmp = (toggle == 0) ? _drbd_bm_find_next_zero(mdev, c->bit_offset)\r\n: _drbd_bm_find_next(mdev, c->bit_offset);\r\nif (tmp == -1UL)\r\ntmp = c->bm_bits;\r\nrl = tmp - c->bit_offset;\r\nif (toggle == 2) {\r\nif (rl == 0) {\r\nDCBP_set_start(p, 1);\r\ntoggle = !toggle;\r\ncontinue;\r\n}\r\nDCBP_set_start(p, 0);\r\n}\r\nif (rl == 0) {\r\ndev_err(DEV, "unexpected zero runlength while encoding bitmap "\r\n"t:%u bo:%lu\n", toggle, c->bit_offset);\r\nreturn -1;\r\n}\r\nbits = vli_encode_bits(&bs, rl);\r\nif (bits == -ENOBUFS)\r\nbreak;\r\nif (bits <= 0) {\r\ndev_err(DEV, "error while encoding bitmap: %d\n", bits);\r\nreturn 0;\r\n}\r\ntoggle = !toggle;\r\nplain_bits += rl;\r\nc->bit_offset = tmp;\r\n} while (c->bit_offset < c->bm_bits);\r\nlen = bs.cur.b - p->code + !!bs.cur.bit;\r\nif (plain_bits < (len << 3)) {\r\nc->bit_offset -= plain_bits;\r\nbm_xfer_ctx_bit_to_word_offset(c);\r\nc->bit_offset = c->word_offset * BITS_PER_LONG;\r\nreturn 0;\r\n}\r\nbm_xfer_ctx_bit_to_word_offset(c);\r\nDCBP_set_pad_bits(p, (8 - bs.cur.bit) & 0x7);\r\nreturn len;\r\n}\r\nstatic int\r\nsend_bitmap_rle_or_plain(struct drbd_conf *mdev,\r\nstruct p_header80 *h, struct bm_xfer_ctx *c)\r\n{\r\nstruct p_compressed_bm *p = (void*)h;\r\nunsigned long num_words;\r\nint len;\r\nint ok;\r\nlen = fill_bitmap_rle_bits(mdev, p, c);\r\nif (len < 0)\r\nreturn -EIO;\r\nif (len) {\r\nDCBP_set_code(p, RLE_VLI_Bits);\r\nok = _drbd_send_cmd(mdev, mdev->data.socket, P_COMPRESSED_BITMAP, h,\r\nsizeof(*p) + len, 0);\r\nc->packets[0]++;\r\nc->bytes[0] += sizeof(*p) + len;\r\nif (c->bit_offset >= c->bm_bits)\r\nlen = 0;\r\n} else {\r\nnum_words = min_t(size_t, BM_PACKET_WORDS, c->bm_words - c->word_offset);\r\nlen = num_words * sizeof(long);\r\nif (len)\r\ndrbd_bm_get_lel(mdev, c->word_offset, num_words, (unsigned long*)h->payload);\r\nok = _drbd_send_cmd(mdev, mdev->data.socket, P_BITMAP,\r\nh, sizeof(struct p_header80) + len, 0);\r\nc->word_offset += num_words;\r\nc->bit_offset = c->word_offset * BITS_PER_LONG;\r\nc->packets[1]++;\r\nc->bytes[1] += sizeof(struct p_header80) + len;\r\nif (c->bit_offset > c->bm_bits)\r\nc->bit_offset = c->bm_bits;\r\n}\r\nif (ok) {\r\nif (len == 0) {\r\nINFO_bm_xfer_stats(mdev, "send", c);\r\nreturn 0;\r\n} else\r\nreturn 1;\r\n}\r\nreturn -EIO;\r\n}\r\nint _drbd_send_bitmap(struct drbd_conf *mdev)\r\n{\r\nstruct bm_xfer_ctx c;\r\nstruct p_header80 *p;\r\nint err;\r\nERR_IF(!mdev->bitmap) return false;\r\np = (struct p_header80 *) __get_free_page(GFP_NOIO);\r\nif (!p) {\r\ndev_err(DEV, "failed to allocate one page buffer in %s\n", __func__);\r\nreturn false;\r\n}\r\nif (get_ldev(mdev)) {\r\nif (drbd_md_test_flag(mdev->ldev, MDF_FULL_SYNC)) {\r\ndev_info(DEV, "Writing the whole bitmap, MDF_FullSync was set.\n");\r\ndrbd_bm_set_all(mdev);\r\nif (drbd_bm_write(mdev)) {\r\ndev_err(DEV, "Failed to write bitmap to disk!\n");\r\n} else {\r\ndrbd_md_clear_flag(mdev, MDF_FULL_SYNC);\r\ndrbd_md_sync(mdev);\r\n}\r\n}\r\nput_ldev(mdev);\r\n}\r\nc = (struct bm_xfer_ctx) {\r\n.bm_bits = drbd_bm_bits(mdev),\r\n.bm_words = drbd_bm_words(mdev),\r\n};\r\ndo {\r\nerr = send_bitmap_rle_or_plain(mdev, p, &c);\r\n} while (err > 0);\r\nfree_page((unsigned long) p);\r\nreturn err == 0;\r\n}\r\nint drbd_send_bitmap(struct drbd_conf *mdev)\r\n{\r\nint err;\r\nif (!drbd_get_data_sock(mdev))\r\nreturn -1;\r\nerr = !_drbd_send_bitmap(mdev);\r\ndrbd_put_data_sock(mdev);\r\nreturn err;\r\n}\r\nint drbd_send_b_ack(struct drbd_conf *mdev, u32 barrier_nr, u32 set_size)\r\n{\r\nint ok;\r\nstruct p_barrier_ack p;\r\np.barrier = barrier_nr;\r\np.set_size = cpu_to_be32(set_size);\r\nif (mdev->state.conn < C_CONNECTED)\r\nreturn false;\r\nok = drbd_send_cmd(mdev, USE_META_SOCKET, P_BARRIER_ACK,\r\n(struct p_header80 *)&p, sizeof(p));\r\nreturn ok;\r\n}\r\nstatic int _drbd_send_ack(struct drbd_conf *mdev, enum drbd_packets cmd,\r\nu64 sector,\r\nu32 blksize,\r\nu64 block_id)\r\n{\r\nint ok;\r\nstruct p_block_ack p;\r\np.sector = sector;\r\np.block_id = block_id;\r\np.blksize = blksize;\r\np.seq_num = cpu_to_be32(atomic_add_return(1, &mdev->packet_seq));\r\nif (!mdev->meta.socket || mdev->state.conn < C_CONNECTED)\r\nreturn false;\r\nok = drbd_send_cmd(mdev, USE_META_SOCKET, cmd,\r\n(struct p_header80 *)&p, sizeof(p));\r\nreturn ok;\r\n}\r\nint drbd_send_ack_dp(struct drbd_conf *mdev, enum drbd_packets cmd,\r\nstruct p_data *dp, int data_size)\r\n{\r\ndata_size -= (mdev->agreed_pro_version >= 87 && mdev->integrity_r_tfm) ?\r\ncrypto_hash_digestsize(mdev->integrity_r_tfm) : 0;\r\nreturn _drbd_send_ack(mdev, cmd, dp->sector, cpu_to_be32(data_size),\r\ndp->block_id);\r\n}\r\nint drbd_send_ack_rp(struct drbd_conf *mdev, enum drbd_packets cmd,\r\nstruct p_block_req *rp)\r\n{\r\nreturn _drbd_send_ack(mdev, cmd, rp->sector, rp->blksize, rp->block_id);\r\n}\r\nint drbd_send_ack(struct drbd_conf *mdev,\r\nenum drbd_packets cmd, struct drbd_epoch_entry *e)\r\n{\r\nreturn _drbd_send_ack(mdev, cmd,\r\ncpu_to_be64(e->sector),\r\ncpu_to_be32(e->size),\r\ne->block_id);\r\n}\r\nint drbd_send_ack_ex(struct drbd_conf *mdev, enum drbd_packets cmd,\r\nsector_t sector, int blksize, u64 block_id)\r\n{\r\nreturn _drbd_send_ack(mdev, cmd,\r\ncpu_to_be64(sector),\r\ncpu_to_be32(blksize),\r\ncpu_to_be64(block_id));\r\n}\r\nint drbd_send_drequest(struct drbd_conf *mdev, int cmd,\r\nsector_t sector, int size, u64 block_id)\r\n{\r\nint ok;\r\nstruct p_block_req p;\r\np.sector = cpu_to_be64(sector);\r\np.block_id = block_id;\r\np.blksize = cpu_to_be32(size);\r\nok = drbd_send_cmd(mdev, USE_DATA_SOCKET, cmd,\r\n(struct p_header80 *)&p, sizeof(p));\r\nreturn ok;\r\n}\r\nint drbd_send_drequest_csum(struct drbd_conf *mdev,\r\nsector_t sector, int size,\r\nvoid *digest, int digest_size,\r\nenum drbd_packets cmd)\r\n{\r\nint ok;\r\nstruct p_block_req p;\r\np.sector = cpu_to_be64(sector);\r\np.block_id = BE_DRBD_MAGIC + 0xbeef;\r\np.blksize = cpu_to_be32(size);\r\np.head.magic = BE_DRBD_MAGIC;\r\np.head.command = cpu_to_be16(cmd);\r\np.head.length = cpu_to_be16(sizeof(p) - sizeof(struct p_header80) + digest_size);\r\nmutex_lock(&mdev->data.mutex);\r\nok = (sizeof(p) == drbd_send(mdev, mdev->data.socket, &p, sizeof(p), 0));\r\nok = ok && (digest_size == drbd_send(mdev, mdev->data.socket, digest, digest_size, 0));\r\nmutex_unlock(&mdev->data.mutex);\r\nreturn ok;\r\n}\r\nint drbd_send_ov_request(struct drbd_conf *mdev, sector_t sector, int size)\r\n{\r\nint ok;\r\nstruct p_block_req p;\r\np.sector = cpu_to_be64(sector);\r\np.block_id = BE_DRBD_MAGIC + 0xbabe;\r\np.blksize = cpu_to_be32(size);\r\nok = drbd_send_cmd(mdev, USE_DATA_SOCKET, P_OV_REQUEST,\r\n(struct p_header80 *)&p, sizeof(p));\r\nreturn ok;\r\n}\r\nstatic int we_should_drop_the_connection(struct drbd_conf *mdev, struct socket *sock)\r\n{\r\nint drop_it;\r\ndrop_it = mdev->meta.socket == sock\r\n|| !mdev->asender.task\r\n|| get_t_state(&mdev->asender) != Running\r\n|| mdev->state.conn < C_CONNECTED;\r\nif (drop_it)\r\nreturn true;\r\ndrop_it = !--mdev->ko_count;\r\nif (!drop_it) {\r\ndev_err(DEV, "[%s/%d] sock_sendmsg time expired, ko = %u\n",\r\ncurrent->comm, current->pid, mdev->ko_count);\r\nrequest_ping(mdev);\r\n}\r\nreturn drop_it; ;\r\n}\r\nstatic int _drbd_no_send_page(struct drbd_conf *mdev, struct page *page,\r\nint offset, size_t size, unsigned msg_flags)\r\n{\r\nint sent = drbd_send(mdev, mdev->data.socket, kmap(page) + offset, size, msg_flags);\r\nkunmap(page);\r\nif (sent == size)\r\nmdev->send_cnt += size>>9;\r\nreturn sent == size;\r\n}\r\nstatic int _drbd_send_page(struct drbd_conf *mdev, struct page *page,\r\nint offset, size_t size, unsigned msg_flags)\r\n{\r\nmm_segment_t oldfs = get_fs();\r\nint sent, ok;\r\nint len = size;\r\nif (disable_sendpage || (page_count(page) < 1) || PageSlab(page))\r\nreturn _drbd_no_send_page(mdev, page, offset, size, msg_flags);\r\nmsg_flags |= MSG_NOSIGNAL;\r\ndrbd_update_congested(mdev);\r\nset_fs(KERNEL_DS);\r\ndo {\r\nsent = mdev->data.socket->ops->sendpage(mdev->data.socket, page,\r\noffset, len,\r\nmsg_flags);\r\nif (sent == -EAGAIN) {\r\nif (we_should_drop_the_connection(mdev,\r\nmdev->data.socket))\r\nbreak;\r\nelse\r\ncontinue;\r\n}\r\nif (sent <= 0) {\r\ndev_warn(DEV, "%s: size=%d len=%d sent=%d\n",\r\n__func__, (int)size, len, sent);\r\nbreak;\r\n}\r\nlen -= sent;\r\noffset += sent;\r\n} while (len > 0 );\r\nset_fs(oldfs);\r\nclear_bit(NET_CONGESTED, &mdev->flags);\r\nok = (len == 0);\r\nif (likely(ok))\r\nmdev->send_cnt += size>>9;\r\nreturn ok;\r\n}\r\nstatic int _drbd_send_bio(struct drbd_conf *mdev, struct bio *bio)\r\n{\r\nstruct bio_vec *bvec;\r\nint i;\r\n__bio_for_each_segment(bvec, bio, i, 0) {\r\nif (!_drbd_no_send_page(mdev, bvec->bv_page,\r\nbvec->bv_offset, bvec->bv_len,\r\ni == bio->bi_vcnt -1 ? 0 : MSG_MORE))\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int _drbd_send_zc_bio(struct drbd_conf *mdev, struct bio *bio)\r\n{\r\nstruct bio_vec *bvec;\r\nint i;\r\n__bio_for_each_segment(bvec, bio, i, 0) {\r\nif (!_drbd_send_page(mdev, bvec->bv_page,\r\nbvec->bv_offset, bvec->bv_len,\r\ni == bio->bi_vcnt -1 ? 0 : MSG_MORE))\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int _drbd_send_zc_ee(struct drbd_conf *mdev, struct drbd_epoch_entry *e)\r\n{\r\nstruct page *page = e->pages;\r\nunsigned len = e->size;\r\npage_chain_for_each(page) {\r\nunsigned l = min_t(unsigned, len, PAGE_SIZE);\r\nif (!_drbd_send_page(mdev, page, 0, l,\r\npage_chain_next(page) ? MSG_MORE : 0))\r\nreturn 0;\r\nlen -= l;\r\n}\r\nreturn 1;\r\n}\r\nstatic u32 bio_flags_to_wire(struct drbd_conf *mdev, unsigned long bi_rw)\r\n{\r\nif (mdev->agreed_pro_version >= 95)\r\nreturn (bi_rw & REQ_SYNC ? DP_RW_SYNC : 0) |\r\n(bi_rw & REQ_FUA ? DP_FUA : 0) |\r\n(bi_rw & REQ_FLUSH ? DP_FLUSH : 0) |\r\n(bi_rw & REQ_DISCARD ? DP_DISCARD : 0);\r\nelse\r\nreturn bi_rw & REQ_SYNC ? DP_RW_SYNC : 0;\r\n}\r\nint drbd_send_dblock(struct drbd_conf *mdev, struct drbd_request *req)\r\n{\r\nint ok = 1;\r\nstruct p_data p;\r\nunsigned int dp_flags = 0;\r\nvoid *dgb;\r\nint dgs;\r\nif (!drbd_get_data_sock(mdev))\r\nreturn 0;\r\ndgs = (mdev->agreed_pro_version >= 87 && mdev->integrity_w_tfm) ?\r\ncrypto_hash_digestsize(mdev->integrity_w_tfm) : 0;\r\nif (req->size <= DRBD_MAX_SIZE_H80_PACKET) {\r\np.head.h80.magic = BE_DRBD_MAGIC;\r\np.head.h80.command = cpu_to_be16(P_DATA);\r\np.head.h80.length =\r\ncpu_to_be16(sizeof(p) - sizeof(union p_header) + dgs + req->size);\r\n} else {\r\np.head.h95.magic = BE_DRBD_MAGIC_BIG;\r\np.head.h95.command = cpu_to_be16(P_DATA);\r\np.head.h95.length =\r\ncpu_to_be32(sizeof(p) - sizeof(union p_header) + dgs + req->size);\r\n}\r\np.sector = cpu_to_be64(req->sector);\r\np.block_id = (unsigned long)req;\r\np.seq_num = cpu_to_be32(req->seq_num =\r\natomic_add_return(1, &mdev->packet_seq));\r\ndp_flags = bio_flags_to_wire(mdev, req->master_bio->bi_rw);\r\nif (mdev->state.conn >= C_SYNC_SOURCE &&\r\nmdev->state.conn <= C_PAUSED_SYNC_T)\r\ndp_flags |= DP_MAY_SET_IN_SYNC;\r\np.dp_flags = cpu_to_be32(dp_flags);\r\nset_bit(UNPLUG_REMOTE, &mdev->flags);\r\nok = (sizeof(p) ==\r\ndrbd_send(mdev, mdev->data.socket, &p, sizeof(p), dgs ? MSG_MORE : 0));\r\nif (ok && dgs) {\r\ndgb = mdev->int_dig_out;\r\ndrbd_csum_bio(mdev, mdev->integrity_w_tfm, req->master_bio, dgb);\r\nok = dgs == drbd_send(mdev, mdev->data.socket, dgb, dgs, 0);\r\n}\r\nif (ok) {\r\nif (mdev->net_conf->wire_protocol == DRBD_PROT_A || dgs)\r\nok = _drbd_send_bio(mdev, req->master_bio);\r\nelse\r\nok = _drbd_send_zc_bio(mdev, req->master_bio);\r\nif (dgs > 0 && dgs <= 64) {\r\nunsigned char digest[64];\r\ndrbd_csum_bio(mdev, mdev->integrity_w_tfm, req->master_bio, digest);\r\nif (memcmp(mdev->int_dig_out, digest, dgs)) {\r\ndev_warn(DEV,\r\n"Digest mismatch, buffer modified by upper layers during write: %llus +%u\n",\r\n(unsigned long long)req->sector, req->size);\r\n}\r\n}\r\n}\r\ndrbd_put_data_sock(mdev);\r\nreturn ok;\r\n}\r\nint drbd_send_block(struct drbd_conf *mdev, enum drbd_packets cmd,\r\nstruct drbd_epoch_entry *e)\r\n{\r\nint ok;\r\nstruct p_data p;\r\nvoid *dgb;\r\nint dgs;\r\ndgs = (mdev->agreed_pro_version >= 87 && mdev->integrity_w_tfm) ?\r\ncrypto_hash_digestsize(mdev->integrity_w_tfm) : 0;\r\nif (e->size <= DRBD_MAX_SIZE_H80_PACKET) {\r\np.head.h80.magic = BE_DRBD_MAGIC;\r\np.head.h80.command = cpu_to_be16(cmd);\r\np.head.h80.length =\r\ncpu_to_be16(sizeof(p) - sizeof(struct p_header80) + dgs + e->size);\r\n} else {\r\np.head.h95.magic = BE_DRBD_MAGIC_BIG;\r\np.head.h95.command = cpu_to_be16(cmd);\r\np.head.h95.length =\r\ncpu_to_be32(sizeof(p) - sizeof(struct p_header80) + dgs + e->size);\r\n}\r\np.sector = cpu_to_be64(e->sector);\r\np.block_id = e->block_id;\r\nif (!drbd_get_data_sock(mdev))\r\nreturn 0;\r\nok = sizeof(p) == drbd_send(mdev, mdev->data.socket, &p, sizeof(p), dgs ? MSG_MORE : 0);\r\nif (ok && dgs) {\r\ndgb = mdev->int_dig_out;\r\ndrbd_csum_ee(mdev, mdev->integrity_w_tfm, e, dgb);\r\nok = dgs == drbd_send(mdev, mdev->data.socket, dgb, dgs, 0);\r\n}\r\nif (ok)\r\nok = _drbd_send_zc_ee(mdev, e);\r\ndrbd_put_data_sock(mdev);\r\nreturn ok;\r\n}\r\nint drbd_send_oos(struct drbd_conf *mdev, struct drbd_request *req)\r\n{\r\nstruct p_block_desc p;\r\np.sector = cpu_to_be64(req->sector);\r\np.blksize = cpu_to_be32(req->size);\r\nreturn drbd_send_cmd(mdev, USE_DATA_SOCKET, P_OUT_OF_SYNC, &p.head, sizeof(p));\r\n}\r\nint drbd_send(struct drbd_conf *mdev, struct socket *sock,\r\nvoid *buf, size_t size, unsigned msg_flags)\r\n{\r\nstruct kvec iov;\r\nstruct msghdr msg;\r\nint rv, sent = 0;\r\nif (!sock)\r\nreturn -1000;\r\niov.iov_base = buf;\r\niov.iov_len = size;\r\nmsg.msg_name = NULL;\r\nmsg.msg_namelen = 0;\r\nmsg.msg_control = NULL;\r\nmsg.msg_controllen = 0;\r\nmsg.msg_flags = msg_flags | MSG_NOSIGNAL;\r\nif (sock == mdev->data.socket) {\r\nmdev->ko_count = mdev->net_conf->ko_count;\r\ndrbd_update_congested(mdev);\r\n}\r\ndo {\r\nrv = kernel_sendmsg(sock, &msg, &iov, 1, size);\r\nif (rv == -EAGAIN) {\r\nif (we_should_drop_the_connection(mdev, sock))\r\nbreak;\r\nelse\r\ncontinue;\r\n}\r\nD_ASSERT(rv != 0);\r\nif (rv == -EINTR) {\r\nflush_signals(current);\r\nrv = 0;\r\n}\r\nif (rv < 0)\r\nbreak;\r\nsent += rv;\r\niov.iov_base += rv;\r\niov.iov_len -= rv;\r\n} while (sent < size);\r\nif (sock == mdev->data.socket)\r\nclear_bit(NET_CONGESTED, &mdev->flags);\r\nif (rv <= 0) {\r\nif (rv != -EAGAIN) {\r\ndev_err(DEV, "%s_sendmsg returned %d\n",\r\nsock == mdev->meta.socket ? "msock" : "sock",\r\nrv);\r\ndrbd_force_state(mdev, NS(conn, C_BROKEN_PIPE));\r\n} else\r\ndrbd_force_state(mdev, NS(conn, C_TIMEOUT));\r\n}\r\nreturn sent;\r\n}\r\nstatic int drbd_open(struct block_device *bdev, fmode_t mode)\r\n{\r\nstruct drbd_conf *mdev = bdev->bd_disk->private_data;\r\nunsigned long flags;\r\nint rv = 0;\r\nmutex_lock(&drbd_main_mutex);\r\nspin_lock_irqsave(&mdev->req_lock, flags);\r\nif (mdev->state.role != R_PRIMARY) {\r\nif (mode & FMODE_WRITE)\r\nrv = -EROFS;\r\nelse if (!allow_oos)\r\nrv = -EMEDIUMTYPE;\r\n}\r\nif (!rv)\r\nmdev->open_cnt++;\r\nspin_unlock_irqrestore(&mdev->req_lock, flags);\r\nmutex_unlock(&drbd_main_mutex);\r\nreturn rv;\r\n}\r\nstatic int drbd_release(struct gendisk *gd, fmode_t mode)\r\n{\r\nstruct drbd_conf *mdev = gd->private_data;\r\nmutex_lock(&drbd_main_mutex);\r\nmdev->open_cnt--;\r\nmutex_unlock(&drbd_main_mutex);\r\nreturn 0;\r\n}\r\nstatic void drbd_set_defaults(struct drbd_conf *mdev)\r\n{\r\nmdev->sync_conf = (struct syncer_conf) {\r\nDRBD_RATE_DEF,\r\nDRBD_AFTER_DEF,\r\nDRBD_AL_EXTENTS_DEF,\r\n{}, 0,\r\n{}, 0,\r\n{}, 0,\r\n0,\r\nDRBD_ON_NO_DATA_DEF,\r\nDRBD_C_PLAN_AHEAD_DEF,\r\nDRBD_C_DELAY_TARGET_DEF,\r\nDRBD_C_FILL_TARGET_DEF,\r\nDRBD_C_MAX_RATE_DEF,\r\nDRBD_C_MIN_RATE_DEF\r\n};\r\nmdev->state = (union drbd_state) {\r\n{ .role = R_SECONDARY,\r\n.peer = R_UNKNOWN,\r\n.conn = C_STANDALONE,\r\n.disk = D_DISKLESS,\r\n.pdsk = D_UNKNOWN,\r\n.susp = 0,\r\n.susp_nod = 0,\r\n.susp_fen = 0\r\n} };\r\n}\r\nvoid drbd_init_set_defaults(struct drbd_conf *mdev)\r\n{\r\ndrbd_set_defaults(mdev);\r\natomic_set(&mdev->ap_bio_cnt, 0);\r\natomic_set(&mdev->ap_pending_cnt, 0);\r\natomic_set(&mdev->rs_pending_cnt, 0);\r\natomic_set(&mdev->unacked_cnt, 0);\r\natomic_set(&mdev->local_cnt, 0);\r\natomic_set(&mdev->net_cnt, 0);\r\natomic_set(&mdev->packet_seq, 0);\r\natomic_set(&mdev->pp_in_use, 0);\r\natomic_set(&mdev->pp_in_use_by_net, 0);\r\natomic_set(&mdev->rs_sect_in, 0);\r\natomic_set(&mdev->rs_sect_ev, 0);\r\natomic_set(&mdev->ap_in_flight, 0);\r\nmutex_init(&mdev->md_io_mutex);\r\nmutex_init(&mdev->data.mutex);\r\nmutex_init(&mdev->meta.mutex);\r\nsema_init(&mdev->data.work.s, 0);\r\nsema_init(&mdev->meta.work.s, 0);\r\nmutex_init(&mdev->state_mutex);\r\nspin_lock_init(&mdev->data.work.q_lock);\r\nspin_lock_init(&mdev->meta.work.q_lock);\r\nspin_lock_init(&mdev->al_lock);\r\nspin_lock_init(&mdev->req_lock);\r\nspin_lock_init(&mdev->peer_seq_lock);\r\nspin_lock_init(&mdev->epoch_lock);\r\nINIT_LIST_HEAD(&mdev->active_ee);\r\nINIT_LIST_HEAD(&mdev->sync_ee);\r\nINIT_LIST_HEAD(&mdev->done_ee);\r\nINIT_LIST_HEAD(&mdev->read_ee);\r\nINIT_LIST_HEAD(&mdev->net_ee);\r\nINIT_LIST_HEAD(&mdev->resync_reads);\r\nINIT_LIST_HEAD(&mdev->data.work.q);\r\nINIT_LIST_HEAD(&mdev->meta.work.q);\r\nINIT_LIST_HEAD(&mdev->resync_work.list);\r\nINIT_LIST_HEAD(&mdev->unplug_work.list);\r\nINIT_LIST_HEAD(&mdev->go_diskless.list);\r\nINIT_LIST_HEAD(&mdev->md_sync_work.list);\r\nINIT_LIST_HEAD(&mdev->start_resync_work.list);\r\nINIT_LIST_HEAD(&mdev->bm_io_work.w.list);\r\nmdev->resync_work.cb = w_resync_timer;\r\nmdev->unplug_work.cb = w_send_write_hint;\r\nmdev->go_diskless.cb = w_go_diskless;\r\nmdev->md_sync_work.cb = w_md_sync;\r\nmdev->bm_io_work.w.cb = w_bitmap_io;\r\nmdev->start_resync_work.cb = w_start_resync;\r\ninit_timer(&mdev->resync_timer);\r\ninit_timer(&mdev->md_sync_timer);\r\ninit_timer(&mdev->start_resync_timer);\r\ninit_timer(&mdev->request_timer);\r\nmdev->resync_timer.function = resync_timer_fn;\r\nmdev->resync_timer.data = (unsigned long) mdev;\r\nmdev->md_sync_timer.function = md_sync_timer_fn;\r\nmdev->md_sync_timer.data = (unsigned long) mdev;\r\nmdev->start_resync_timer.function = start_resync_timer_fn;\r\nmdev->start_resync_timer.data = (unsigned long) mdev;\r\nmdev->request_timer.function = request_timer_fn;\r\nmdev->request_timer.data = (unsigned long) mdev;\r\ninit_waitqueue_head(&mdev->misc_wait);\r\ninit_waitqueue_head(&mdev->state_wait);\r\ninit_waitqueue_head(&mdev->net_cnt_wait);\r\ninit_waitqueue_head(&mdev->ee_wait);\r\ninit_waitqueue_head(&mdev->al_wait);\r\ninit_waitqueue_head(&mdev->seq_wait);\r\ndrbd_thread_init(mdev, &mdev->receiver, drbdd_init);\r\ndrbd_thread_init(mdev, &mdev->worker, drbd_worker);\r\ndrbd_thread_init(mdev, &mdev->asender, drbd_asender);\r\nmdev->agreed_pro_version = PRO_VERSION_MAX;\r\nmdev->write_ordering = WO_bdev_flush;\r\nmdev->resync_wenr = LC_FREE;\r\nmdev->peer_max_bio_size = DRBD_MAX_BIO_SIZE_SAFE;\r\nmdev->local_max_bio_size = DRBD_MAX_BIO_SIZE_SAFE;\r\n}\r\nvoid drbd_mdev_cleanup(struct drbd_conf *mdev)\r\n{\r\nint i;\r\nif (mdev->receiver.t_state != None)\r\ndev_err(DEV, "ASSERT FAILED: receiver t_state == %d expected 0.\n",\r\nmdev->receiver.t_state);\r\nif (atomic_read(&mdev->current_epoch->epoch_size) != 0)\r\ndev_err(DEV, "epoch_size:%d\n", atomic_read(&mdev->current_epoch->epoch_size));\r\nmdev->al_writ_cnt =\r\nmdev->bm_writ_cnt =\r\nmdev->read_cnt =\r\nmdev->recv_cnt =\r\nmdev->send_cnt =\r\nmdev->writ_cnt =\r\nmdev->p_size =\r\nmdev->rs_start =\r\nmdev->rs_total =\r\nmdev->rs_failed = 0;\r\nmdev->rs_last_events = 0;\r\nmdev->rs_last_sect_ev = 0;\r\nfor (i = 0; i < DRBD_SYNC_MARKS; i++) {\r\nmdev->rs_mark_left[i] = 0;\r\nmdev->rs_mark_time[i] = 0;\r\n}\r\nD_ASSERT(mdev->net_conf == NULL);\r\ndrbd_set_my_capacity(mdev, 0);\r\nif (mdev->bitmap) {\r\ndrbd_bm_resize(mdev, 0, 1);\r\ndrbd_bm_cleanup(mdev);\r\n}\r\ndrbd_free_resources(mdev);\r\nclear_bit(AL_SUSPENDED, &mdev->flags);\r\nD_ASSERT(list_empty(&mdev->active_ee));\r\nD_ASSERT(list_empty(&mdev->sync_ee));\r\nD_ASSERT(list_empty(&mdev->done_ee));\r\nD_ASSERT(list_empty(&mdev->read_ee));\r\nD_ASSERT(list_empty(&mdev->net_ee));\r\nD_ASSERT(list_empty(&mdev->resync_reads));\r\nD_ASSERT(list_empty(&mdev->data.work.q));\r\nD_ASSERT(list_empty(&mdev->meta.work.q));\r\nD_ASSERT(list_empty(&mdev->resync_work.list));\r\nD_ASSERT(list_empty(&mdev->unplug_work.list));\r\nD_ASSERT(list_empty(&mdev->go_diskless.list));\r\ndrbd_set_defaults(mdev);\r\n}\r\nstatic void drbd_destroy_mempools(void)\r\n{\r\nstruct page *page;\r\nwhile (drbd_pp_pool) {\r\npage = drbd_pp_pool;\r\ndrbd_pp_pool = (struct page *)page_private(page);\r\n__free_page(page);\r\ndrbd_pp_vacant--;\r\n}\r\nif (drbd_ee_mempool)\r\nmempool_destroy(drbd_ee_mempool);\r\nif (drbd_request_mempool)\r\nmempool_destroy(drbd_request_mempool);\r\nif (drbd_ee_cache)\r\nkmem_cache_destroy(drbd_ee_cache);\r\nif (drbd_request_cache)\r\nkmem_cache_destroy(drbd_request_cache);\r\nif (drbd_bm_ext_cache)\r\nkmem_cache_destroy(drbd_bm_ext_cache);\r\nif (drbd_al_ext_cache)\r\nkmem_cache_destroy(drbd_al_ext_cache);\r\ndrbd_ee_mempool = NULL;\r\ndrbd_request_mempool = NULL;\r\ndrbd_ee_cache = NULL;\r\ndrbd_request_cache = NULL;\r\ndrbd_bm_ext_cache = NULL;\r\ndrbd_al_ext_cache = NULL;\r\nreturn;\r\n}\r\nstatic int drbd_create_mempools(void)\r\n{\r\nstruct page *page;\r\nconst int number = (DRBD_MAX_BIO_SIZE/PAGE_SIZE) * minor_count;\r\nint i;\r\ndrbd_request_mempool = NULL;\r\ndrbd_ee_cache = NULL;\r\ndrbd_request_cache = NULL;\r\ndrbd_bm_ext_cache = NULL;\r\ndrbd_al_ext_cache = NULL;\r\ndrbd_pp_pool = NULL;\r\ndrbd_request_cache = kmem_cache_create(\r\n"drbd_req", sizeof(struct drbd_request), 0, 0, NULL);\r\nif (drbd_request_cache == NULL)\r\ngoto Enomem;\r\ndrbd_ee_cache = kmem_cache_create(\r\n"drbd_ee", sizeof(struct drbd_epoch_entry), 0, 0, NULL);\r\nif (drbd_ee_cache == NULL)\r\ngoto Enomem;\r\ndrbd_bm_ext_cache = kmem_cache_create(\r\n"drbd_bm", sizeof(struct bm_extent), 0, 0, NULL);\r\nif (drbd_bm_ext_cache == NULL)\r\ngoto Enomem;\r\ndrbd_al_ext_cache = kmem_cache_create(\r\n"drbd_al", sizeof(struct lc_element), 0, 0, NULL);\r\nif (drbd_al_ext_cache == NULL)\r\ngoto Enomem;\r\ndrbd_request_mempool = mempool_create(number,\r\nmempool_alloc_slab, mempool_free_slab, drbd_request_cache);\r\nif (drbd_request_mempool == NULL)\r\ngoto Enomem;\r\ndrbd_ee_mempool = mempool_create(number,\r\nmempool_alloc_slab, mempool_free_slab, drbd_ee_cache);\r\nif (drbd_ee_mempool == NULL)\r\ngoto Enomem;\r\nspin_lock_init(&drbd_pp_lock);\r\nfor (i = 0; i < number; i++) {\r\npage = alloc_page(GFP_HIGHUSER);\r\nif (!page)\r\ngoto Enomem;\r\nset_page_private(page, (unsigned long)drbd_pp_pool);\r\ndrbd_pp_pool = page;\r\n}\r\ndrbd_pp_vacant = number;\r\nreturn 0;\r\nEnomem:\r\ndrbd_destroy_mempools();\r\nreturn -ENOMEM;\r\n}\r\nstatic int drbd_notify_sys(struct notifier_block *this, unsigned long code,\r\nvoid *unused)\r\n{\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic void drbd_release_ee_lists(struct drbd_conf *mdev)\r\n{\r\nint rr;\r\nrr = drbd_release_ee(mdev, &mdev->active_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in active list found!\n", rr);\r\nrr = drbd_release_ee(mdev, &mdev->sync_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in sync list found!\n", rr);\r\nrr = drbd_release_ee(mdev, &mdev->read_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in read list found!\n", rr);\r\nrr = drbd_release_ee(mdev, &mdev->done_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in done list found!\n", rr);\r\nrr = drbd_release_ee(mdev, &mdev->net_ee);\r\nif (rr)\r\ndev_err(DEV, "%d EEs in net list found!\n", rr);\r\n}\r\nstatic void drbd_delete_device(unsigned int minor)\r\n{\r\nstruct drbd_conf *mdev = minor_to_mdev(minor);\r\nif (!mdev)\r\nreturn;\r\nif (mdev->open_cnt != 0)\r\ndev_err(DEV, "open_cnt = %d in %s:%u", mdev->open_cnt,\r\n__FILE__ , __LINE__);\r\nERR_IF (!list_empty(&mdev->data.work.q)) {\r\nstruct list_head *lp;\r\nlist_for_each(lp, &mdev->data.work.q) {\r\ndev_err(DEV, "lp = %p\n", lp);\r\n}\r\n};\r\ndel_gendisk(mdev->vdisk);\r\nif (mdev->this_bdev)\r\nbdput(mdev->this_bdev);\r\ndrbd_free_resources(mdev);\r\ndrbd_release_ee_lists(mdev);\r\nkfree(mdev->ee_hash);\r\nlc_destroy(mdev->act_log);\r\nlc_destroy(mdev->resync);\r\nkfree(mdev->p_uuid);\r\nkfree(mdev->int_dig_out);\r\nkfree(mdev->int_dig_in);\r\nkfree(mdev->int_dig_vv);\r\ndrbd_free_mdev(mdev);\r\n}\r\nstatic void drbd_cleanup(void)\r\n{\r\nunsigned int i;\r\nunregister_reboot_notifier(&drbd_notifier);\r\nif (drbd_proc)\r\nremove_proc_entry("drbd", NULL);\r\ndrbd_nl_cleanup();\r\nif (minor_table) {\r\ni = minor_count;\r\nwhile (i--)\r\ndrbd_delete_device(i);\r\ndrbd_destroy_mempools();\r\n}\r\nkfree(minor_table);\r\nunregister_blkdev(DRBD_MAJOR, "drbd");\r\nprintk(KERN_INFO "drbd: module cleanup done.\n");\r\n}\r\nstatic int drbd_congested(void *congested_data, int bdi_bits)\r\n{\r\nstruct drbd_conf *mdev = congested_data;\r\nstruct request_queue *q;\r\nchar reason = '-';\r\nint r = 0;\r\nif (!may_inc_ap_bio(mdev)) {\r\nr = bdi_bits;\r\nreason = 'd';\r\ngoto out;\r\n}\r\nif (get_ldev(mdev)) {\r\nq = bdev_get_queue(mdev->ldev->backing_bdev);\r\nr = bdi_congested(&q->backing_dev_info, bdi_bits);\r\nput_ldev(mdev);\r\nif (r)\r\nreason = 'b';\r\n}\r\nif (bdi_bits & (1 << BDI_async_congested) && test_bit(NET_CONGESTED, &mdev->flags)) {\r\nr |= (1 << BDI_async_congested);\r\nreason = reason == 'b' ? 'a' : 'n';\r\n}\r\nout:\r\nmdev->congestion_reason = reason;\r\nreturn r;\r\n}\r\nstruct drbd_conf *drbd_new_device(unsigned int minor)\r\n{\r\nstruct drbd_conf *mdev;\r\nstruct gendisk *disk;\r\nstruct request_queue *q;\r\nmdev = kzalloc(sizeof(struct drbd_conf), GFP_KERNEL);\r\nif (!mdev)\r\nreturn NULL;\r\nif (!zalloc_cpumask_var(&mdev->cpu_mask, GFP_KERNEL))\r\ngoto out_no_cpumask;\r\nmdev->minor = minor;\r\ndrbd_init_set_defaults(mdev);\r\nq = blk_alloc_queue(GFP_KERNEL);\r\nif (!q)\r\ngoto out_no_q;\r\nmdev->rq_queue = q;\r\nq->queuedata = mdev;\r\ndisk = alloc_disk(1);\r\nif (!disk)\r\ngoto out_no_disk;\r\nmdev->vdisk = disk;\r\nset_disk_ro(disk, true);\r\ndisk->queue = q;\r\ndisk->major = DRBD_MAJOR;\r\ndisk->first_minor = minor;\r\ndisk->fops = &drbd_ops;\r\nsprintf(disk->disk_name, "drbd%d", minor);\r\ndisk->private_data = mdev;\r\nmdev->this_bdev = bdget(MKDEV(DRBD_MAJOR, minor));\r\nmdev->this_bdev->bd_contains = mdev->this_bdev;\r\nq->backing_dev_info.congested_fn = drbd_congested;\r\nq->backing_dev_info.congested_data = mdev;\r\nblk_queue_make_request(q, drbd_make_request);\r\nblk_queue_max_hw_sectors(q, DRBD_MAX_BIO_SIZE_SAFE >> 8);\r\nblk_queue_bounce_limit(q, BLK_BOUNCE_ANY);\r\nblk_queue_merge_bvec(q, drbd_merge_bvec);\r\nq->queue_lock = &mdev->req_lock;\r\nmdev->md_io_page = alloc_page(GFP_KERNEL);\r\nif (!mdev->md_io_page)\r\ngoto out_no_io_page;\r\nif (drbd_bm_init(mdev))\r\ngoto out_no_bitmap;\r\nif (!tl_init(mdev))\r\ngoto out_no_tl;\r\nmdev->app_reads_hash = kzalloc(APP_R_HSIZE*sizeof(void *), GFP_KERNEL);\r\nif (!mdev->app_reads_hash)\r\ngoto out_no_app_reads;\r\nmdev->current_epoch = kzalloc(sizeof(struct drbd_epoch), GFP_KERNEL);\r\nif (!mdev->current_epoch)\r\ngoto out_no_epoch;\r\nINIT_LIST_HEAD(&mdev->current_epoch->list);\r\nmdev->epochs = 1;\r\nreturn mdev;\r\nout_no_epoch:\r\nkfree(mdev->app_reads_hash);\r\nout_no_app_reads:\r\ntl_cleanup(mdev);\r\nout_no_tl:\r\ndrbd_bm_cleanup(mdev);\r\nout_no_bitmap:\r\n__free_page(mdev->md_io_page);\r\nout_no_io_page:\r\nput_disk(disk);\r\nout_no_disk:\r\nblk_cleanup_queue(q);\r\nout_no_q:\r\nfree_cpumask_var(mdev->cpu_mask);\r\nout_no_cpumask:\r\nkfree(mdev);\r\nreturn NULL;\r\n}\r\nvoid drbd_free_mdev(struct drbd_conf *mdev)\r\n{\r\nkfree(mdev->current_epoch);\r\nkfree(mdev->app_reads_hash);\r\ntl_cleanup(mdev);\r\nif (mdev->bitmap)\r\ndrbd_bm_cleanup(mdev);\r\n__free_page(mdev->md_io_page);\r\nput_disk(mdev->vdisk);\r\nblk_cleanup_queue(mdev->rq_queue);\r\nfree_cpumask_var(mdev->cpu_mask);\r\ndrbd_free_tl_hash(mdev);\r\nkfree(mdev);\r\n}\r\nint __init drbd_init(void)\r\n{\r\nint err;\r\nif (sizeof(struct p_handshake) != 80) {\r\nprintk(KERN_ERR\r\n"drbd: never change the size or layout "\r\n"of the HandShake packet.\n");\r\nreturn -EINVAL;\r\n}\r\nif (minor_count < DRBD_MINOR_COUNT_MIN || minor_count > DRBD_MINOR_COUNT_MAX) {\r\nprintk(KERN_ERR\r\n"drbd: invalid minor_count (%d)\n", minor_count);\r\n#ifdef MODULE\r\nreturn -EINVAL;\r\n#else\r\nminor_count = 8;\r\n#endif\r\n}\r\nerr = drbd_nl_init();\r\nif (err)\r\nreturn err;\r\nerr = register_blkdev(DRBD_MAJOR, "drbd");\r\nif (err) {\r\nprintk(KERN_ERR\r\n"drbd: unable to register block device major %d\n",\r\nDRBD_MAJOR);\r\nreturn err;\r\n}\r\nregister_reboot_notifier(&drbd_notifier);\r\nerr = -ENOMEM;\r\ninit_waitqueue_head(&drbd_pp_wait);\r\ndrbd_proc = NULL;\r\nminor_table = kzalloc(sizeof(struct drbd_conf *)*minor_count,\r\nGFP_KERNEL);\r\nif (!minor_table)\r\ngoto Enomem;\r\nerr = drbd_create_mempools();\r\nif (err)\r\ngoto Enomem;\r\ndrbd_proc = proc_create_data("drbd", S_IFREG | S_IRUGO , NULL, &drbd_proc_fops, NULL);\r\nif (!drbd_proc) {\r\nprintk(KERN_ERR "drbd: unable to register proc file\n");\r\ngoto Enomem;\r\n}\r\nrwlock_init(&global_state_lock);\r\nprintk(KERN_INFO "drbd: initialized. "\r\n"Version: " REL_VERSION " (api:%d/proto:%d-%d)\n",\r\nAPI_VERSION, PRO_VERSION_MIN, PRO_VERSION_MAX);\r\nprintk(KERN_INFO "drbd: %s\n", drbd_buildtag());\r\nprintk(KERN_INFO "drbd: registered as block device major %d\n",\r\nDRBD_MAJOR);\r\nprintk(KERN_INFO "drbd: minor_table @ 0x%p\n", minor_table);\r\nreturn 0;\r\nEnomem:\r\ndrbd_cleanup();\r\nif (err == -ENOMEM)\r\nprintk(KERN_ERR "drbd: ran out of memory\n");\r\nelse\r\nprintk(KERN_ERR "drbd: initialization failure\n");\r\nreturn err;\r\n}\r\nvoid drbd_free_bc(struct drbd_backing_dev *ldev)\r\n{\r\nif (ldev == NULL)\r\nreturn;\r\nblkdev_put(ldev->backing_bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);\r\nblkdev_put(ldev->md_bdev, FMODE_READ | FMODE_WRITE | FMODE_EXCL);\r\nkfree(ldev);\r\n}\r\nvoid drbd_free_sock(struct drbd_conf *mdev)\r\n{\r\nif (mdev->data.socket) {\r\nmutex_lock(&mdev->data.mutex);\r\nkernel_sock_shutdown(mdev->data.socket, SHUT_RDWR);\r\nsock_release(mdev->data.socket);\r\nmdev->data.socket = NULL;\r\nmutex_unlock(&mdev->data.mutex);\r\n}\r\nif (mdev->meta.socket) {\r\nmutex_lock(&mdev->meta.mutex);\r\nkernel_sock_shutdown(mdev->meta.socket, SHUT_RDWR);\r\nsock_release(mdev->meta.socket);\r\nmdev->meta.socket = NULL;\r\nmutex_unlock(&mdev->meta.mutex);\r\n}\r\n}\r\nvoid drbd_free_resources(struct drbd_conf *mdev)\r\n{\r\ncrypto_free_hash(mdev->csums_tfm);\r\nmdev->csums_tfm = NULL;\r\ncrypto_free_hash(mdev->verify_tfm);\r\nmdev->verify_tfm = NULL;\r\ncrypto_free_hash(mdev->cram_hmac_tfm);\r\nmdev->cram_hmac_tfm = NULL;\r\ncrypto_free_hash(mdev->integrity_w_tfm);\r\nmdev->integrity_w_tfm = NULL;\r\ncrypto_free_hash(mdev->integrity_r_tfm);\r\nmdev->integrity_r_tfm = NULL;\r\ndrbd_free_sock(mdev);\r\n__no_warn(local,\r\ndrbd_free_bc(mdev->ldev);\r\nmdev->ldev = NULL;);\r\n}\r\nvoid drbd_md_sync(struct drbd_conf *mdev)\r\n{\r\nstruct meta_data_on_disk *buffer;\r\nsector_t sector;\r\nint i;\r\ndel_timer(&mdev->md_sync_timer);\r\nif (!test_and_clear_bit(MD_DIRTY, &mdev->flags))\r\nreturn;\r\nif (!get_ldev_if_state(mdev, D_FAILED))\r\nreturn;\r\nmutex_lock(&mdev->md_io_mutex);\r\nbuffer = (struct meta_data_on_disk *)page_address(mdev->md_io_page);\r\nmemset(buffer, 0, 512);\r\nbuffer->la_size = cpu_to_be64(drbd_get_capacity(mdev->this_bdev));\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\nbuffer->uuid[i] = cpu_to_be64(mdev->ldev->md.uuid[i]);\r\nbuffer->flags = cpu_to_be32(mdev->ldev->md.flags);\r\nbuffer->magic = cpu_to_be32(DRBD_MD_MAGIC);\r\nbuffer->md_size_sect = cpu_to_be32(mdev->ldev->md.md_size_sect);\r\nbuffer->al_offset = cpu_to_be32(mdev->ldev->md.al_offset);\r\nbuffer->al_nr_extents = cpu_to_be32(mdev->act_log->nr_elements);\r\nbuffer->bm_bytes_per_bit = cpu_to_be32(BM_BLOCK_SIZE);\r\nbuffer->device_uuid = cpu_to_be64(mdev->ldev->md.device_uuid);\r\nbuffer->bm_offset = cpu_to_be32(mdev->ldev->md.bm_offset);\r\nbuffer->la_peer_max_bio_size = cpu_to_be32(mdev->peer_max_bio_size);\r\nD_ASSERT(drbd_md_ss__(mdev, mdev->ldev) == mdev->ldev->md.md_offset);\r\nsector = mdev->ldev->md.md_offset;\r\nif (!drbd_md_sync_page_io(mdev, mdev->ldev, sector, WRITE)) {\r\ndev_err(DEV, "meta data update failed!\n");\r\ndrbd_chk_io_error(mdev, 1, true);\r\n}\r\nmdev->ldev->md.la_size_sect = drbd_get_capacity(mdev->this_bdev);\r\nmutex_unlock(&mdev->md_io_mutex);\r\nput_ldev(mdev);\r\n}\r\nint drbd_md_read(struct drbd_conf *mdev, struct drbd_backing_dev *bdev)\r\n{\r\nstruct meta_data_on_disk *buffer;\r\nint i, rv = NO_ERROR;\r\nif (!get_ldev_if_state(mdev, D_ATTACHING))\r\nreturn ERR_IO_MD_DISK;\r\nmutex_lock(&mdev->md_io_mutex);\r\nbuffer = (struct meta_data_on_disk *)page_address(mdev->md_io_page);\r\nif (!drbd_md_sync_page_io(mdev, bdev, bdev->md.md_offset, READ)) {\r\ndev_err(DEV, "Error while reading metadata.\n");\r\nrv = ERR_IO_MD_DISK;\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->magic) != DRBD_MD_MAGIC) {\r\ndev_err(DEV, "Error while reading metadata, magic not found.\n");\r\nrv = ERR_MD_INVALID;\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->al_offset) != bdev->md.al_offset) {\r\ndev_err(DEV, "unexpected al_offset: %d (expected %d)\n",\r\nbe32_to_cpu(buffer->al_offset), bdev->md.al_offset);\r\nrv = ERR_MD_INVALID;\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->bm_offset) != bdev->md.bm_offset) {\r\ndev_err(DEV, "unexpected bm_offset: %d (expected %d)\n",\r\nbe32_to_cpu(buffer->bm_offset), bdev->md.bm_offset);\r\nrv = ERR_MD_INVALID;\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->md_size_sect) != bdev->md.md_size_sect) {\r\ndev_err(DEV, "unexpected md_size: %u (expected %u)\n",\r\nbe32_to_cpu(buffer->md_size_sect), bdev->md.md_size_sect);\r\nrv = ERR_MD_INVALID;\r\ngoto err;\r\n}\r\nif (be32_to_cpu(buffer->bm_bytes_per_bit) != BM_BLOCK_SIZE) {\r\ndev_err(DEV, "unexpected bm_bytes_per_bit: %u (expected %u)\n",\r\nbe32_to_cpu(buffer->bm_bytes_per_bit), BM_BLOCK_SIZE);\r\nrv = ERR_MD_INVALID;\r\ngoto err;\r\n}\r\nbdev->md.la_size_sect = be64_to_cpu(buffer->la_size);\r\nfor (i = UI_CURRENT; i < UI_SIZE; i++)\r\nbdev->md.uuid[i] = be64_to_cpu(buffer->uuid[i]);\r\nbdev->md.flags = be32_to_cpu(buffer->flags);\r\nmdev->sync_conf.al_extents = be32_to_cpu(buffer->al_nr_extents);\r\nbdev->md.device_uuid = be64_to_cpu(buffer->device_uuid);\r\nspin_lock_irq(&mdev->req_lock);\r\nif (mdev->state.conn < C_CONNECTED) {\r\nint peer;\r\npeer = be32_to_cpu(buffer->la_peer_max_bio_size);\r\npeer = max_t(int, peer, DRBD_MAX_BIO_SIZE_SAFE);\r\nmdev->peer_max_bio_size = peer;\r\n}\r\nspin_unlock_irq(&mdev->req_lock);\r\nif (mdev->sync_conf.al_extents < 7)\r\nmdev->sync_conf.al_extents = 127;\r\nerr:\r\nmutex_unlock(&mdev->md_io_mutex);\r\nput_ldev(mdev);\r\nreturn rv;\r\n}\r\nvoid drbd_md_mark_dirty_(struct drbd_conf *mdev, unsigned int line, const char *func)\r\n{\r\nif (!test_and_set_bit(MD_DIRTY, &mdev->flags)) {\r\nmod_timer(&mdev->md_sync_timer, jiffies + HZ);\r\nmdev->last_md_mark_dirty.line = line;\r\nmdev->last_md_mark_dirty.func = func;\r\n}\r\n}\r\nvoid drbd_md_mark_dirty(struct drbd_conf *mdev)\r\n{\r\nif (!test_and_set_bit(MD_DIRTY, &mdev->flags))\r\nmod_timer(&mdev->md_sync_timer, jiffies + 5*HZ);\r\n}\r\nstatic void drbd_uuid_move_history(struct drbd_conf *mdev) __must_hold(local)\r\n{\r\nint i;\r\nfor (i = UI_HISTORY_START; i < UI_HISTORY_END; i++)\r\nmdev->ldev->md.uuid[i+1] = mdev->ldev->md.uuid[i];\r\n}\r\nvoid _drbd_uuid_set(struct drbd_conf *mdev, int idx, u64 val) __must_hold(local)\r\n{\r\nif (idx == UI_CURRENT) {\r\nif (mdev->state.role == R_PRIMARY)\r\nval |= 1;\r\nelse\r\nval &= ~((u64)1);\r\ndrbd_set_ed_uuid(mdev, val);\r\n}\r\nmdev->ldev->md.uuid[idx] = val;\r\ndrbd_md_mark_dirty(mdev);\r\n}\r\nvoid drbd_uuid_set(struct drbd_conf *mdev, int idx, u64 val) __must_hold(local)\r\n{\r\nif (mdev->ldev->md.uuid[idx]) {\r\ndrbd_uuid_move_history(mdev);\r\nmdev->ldev->md.uuid[UI_HISTORY_START] = mdev->ldev->md.uuid[idx];\r\n}\r\n_drbd_uuid_set(mdev, idx, val);\r\n}\r\nvoid drbd_uuid_new_current(struct drbd_conf *mdev) __must_hold(local)\r\n{\r\nu64 val;\r\nunsigned long long bm_uuid = mdev->ldev->md.uuid[UI_BITMAP];\r\nif (bm_uuid)\r\ndev_warn(DEV, "bm UUID was already set: %llX\n", bm_uuid);\r\nmdev->ldev->md.uuid[UI_BITMAP] = mdev->ldev->md.uuid[UI_CURRENT];\r\nget_random_bytes(&val, sizeof(u64));\r\n_drbd_uuid_set(mdev, UI_CURRENT, val);\r\ndrbd_print_uuids(mdev, "new current UUID");\r\ndrbd_md_sync(mdev);\r\n}\r\nvoid drbd_uuid_set_bm(struct drbd_conf *mdev, u64 val) __must_hold(local)\r\n{\r\nif (mdev->ldev->md.uuid[UI_BITMAP] == 0 && val == 0)\r\nreturn;\r\nif (val == 0) {\r\ndrbd_uuid_move_history(mdev);\r\nmdev->ldev->md.uuid[UI_HISTORY_START] = mdev->ldev->md.uuid[UI_BITMAP];\r\nmdev->ldev->md.uuid[UI_BITMAP] = 0;\r\n} else {\r\nunsigned long long bm_uuid = mdev->ldev->md.uuid[UI_BITMAP];\r\nif (bm_uuid)\r\ndev_warn(DEV, "bm UUID was already set: %llX\n", bm_uuid);\r\nmdev->ldev->md.uuid[UI_BITMAP] = val & ~((u64)1);\r\n}\r\ndrbd_md_mark_dirty(mdev);\r\n}\r\nint drbd_bmio_set_n_write(struct drbd_conf *mdev)\r\n{\r\nint rv = -EIO;\r\nif (get_ldev_if_state(mdev, D_ATTACHING)) {\r\ndrbd_md_set_flag(mdev, MDF_FULL_SYNC);\r\ndrbd_md_sync(mdev);\r\ndrbd_bm_set_all(mdev);\r\nrv = drbd_bm_write(mdev);\r\nif (!rv) {\r\ndrbd_md_clear_flag(mdev, MDF_FULL_SYNC);\r\ndrbd_md_sync(mdev);\r\n}\r\nput_ldev(mdev);\r\n}\r\nreturn rv;\r\n}\r\nint drbd_bmio_clear_n_write(struct drbd_conf *mdev)\r\n{\r\nint rv = -EIO;\r\ndrbd_resume_al(mdev);\r\nif (get_ldev_if_state(mdev, D_ATTACHING)) {\r\ndrbd_bm_clear_all(mdev);\r\nrv = drbd_bm_write(mdev);\r\nput_ldev(mdev);\r\n}\r\nreturn rv;\r\n}\r\nstatic int w_bitmap_io(struct drbd_conf *mdev, struct drbd_work *w, int unused)\r\n{\r\nstruct bm_io_work *work = container_of(w, struct bm_io_work, w);\r\nint rv = -EIO;\r\nD_ASSERT(atomic_read(&mdev->ap_bio_cnt) == 0);\r\nif (get_ldev(mdev)) {\r\ndrbd_bm_lock(mdev, work->why, work->flags);\r\nrv = work->io_fn(mdev);\r\ndrbd_bm_unlock(mdev);\r\nput_ldev(mdev);\r\n}\r\nclear_bit(BITMAP_IO, &mdev->flags);\r\nsmp_mb__after_clear_bit();\r\nwake_up(&mdev->misc_wait);\r\nif (work->done)\r\nwork->done(mdev, rv);\r\nclear_bit(BITMAP_IO_QUEUED, &mdev->flags);\r\nwork->why = NULL;\r\nwork->flags = 0;\r\nreturn 1;\r\n}\r\nvoid drbd_ldev_destroy(struct drbd_conf *mdev)\r\n{\r\nlc_destroy(mdev->resync);\r\nmdev->resync = NULL;\r\nlc_destroy(mdev->act_log);\r\nmdev->act_log = NULL;\r\n__no_warn(local,\r\ndrbd_free_bc(mdev->ldev);\r\nmdev->ldev = NULL;);\r\nif (mdev->md_io_tmpp) {\r\n__free_page(mdev->md_io_tmpp);\r\nmdev->md_io_tmpp = NULL;\r\n}\r\nclear_bit(GO_DISKLESS, &mdev->flags);\r\n}\r\nstatic int w_go_diskless(struct drbd_conf *mdev, struct drbd_work *w, int unused)\r\n{\r\nD_ASSERT(mdev->state.disk == D_FAILED);\r\ndrbd_force_state(mdev, NS(disk, D_DISKLESS));\r\nreturn 1;\r\n}\r\nvoid drbd_go_diskless(struct drbd_conf *mdev)\r\n{\r\nD_ASSERT(mdev->state.disk == D_FAILED);\r\nif (!test_and_set_bit(GO_DISKLESS, &mdev->flags))\r\ndrbd_queue_work(&mdev->data.work, &mdev->go_diskless);\r\n}\r\nvoid drbd_queue_bitmap_io(struct drbd_conf *mdev,\r\nint (*io_fn)(struct drbd_conf *),\r\nvoid (*done)(struct drbd_conf *, int),\r\nchar *why, enum bm_flag flags)\r\n{\r\nD_ASSERT(current == mdev->worker.task);\r\nD_ASSERT(!test_bit(BITMAP_IO_QUEUED, &mdev->flags));\r\nD_ASSERT(!test_bit(BITMAP_IO, &mdev->flags));\r\nD_ASSERT(list_empty(&mdev->bm_io_work.w.list));\r\nif (mdev->bm_io_work.why)\r\ndev_err(DEV, "FIXME going to queue '%s' but '%s' still pending?\n",\r\nwhy, mdev->bm_io_work.why);\r\nmdev->bm_io_work.io_fn = io_fn;\r\nmdev->bm_io_work.done = done;\r\nmdev->bm_io_work.why = why;\r\nmdev->bm_io_work.flags = flags;\r\nspin_lock_irq(&mdev->req_lock);\r\nset_bit(BITMAP_IO, &mdev->flags);\r\nif (atomic_read(&mdev->ap_bio_cnt) == 0) {\r\nif (!test_and_set_bit(BITMAP_IO_QUEUED, &mdev->flags))\r\ndrbd_queue_work(&mdev->data.work, &mdev->bm_io_work.w);\r\n}\r\nspin_unlock_irq(&mdev->req_lock);\r\n}\r\nint drbd_bitmap_io(struct drbd_conf *mdev, int (*io_fn)(struct drbd_conf *),\r\nchar *why, enum bm_flag flags)\r\n{\r\nint rv;\r\nD_ASSERT(current != mdev->worker.task);\r\nif ((flags & BM_LOCKED_SET_ALLOWED) == 0)\r\ndrbd_suspend_io(mdev);\r\ndrbd_bm_lock(mdev, why, flags);\r\nrv = io_fn(mdev);\r\ndrbd_bm_unlock(mdev);\r\nif ((flags & BM_LOCKED_SET_ALLOWED) == 0)\r\ndrbd_resume_io(mdev);\r\nreturn rv;\r\n}\r\nvoid drbd_md_set_flag(struct drbd_conf *mdev, int flag) __must_hold(local)\r\n{\r\nif ((mdev->ldev->md.flags & flag) != flag) {\r\ndrbd_md_mark_dirty(mdev);\r\nmdev->ldev->md.flags |= flag;\r\n}\r\n}\r\nvoid drbd_md_clear_flag(struct drbd_conf *mdev, int flag) __must_hold(local)\r\n{\r\nif ((mdev->ldev->md.flags & flag) != 0) {\r\ndrbd_md_mark_dirty(mdev);\r\nmdev->ldev->md.flags &= ~flag;\r\n}\r\n}\r\nint drbd_md_test_flag(struct drbd_backing_dev *bdev, int flag)\r\n{\r\nreturn (bdev->md.flags & flag) != 0;\r\n}\r\nstatic void md_sync_timer_fn(unsigned long data)\r\n{\r\nstruct drbd_conf *mdev = (struct drbd_conf *) data;\r\ndrbd_queue_work_front(&mdev->data.work, &mdev->md_sync_work);\r\n}\r\nstatic int w_md_sync(struct drbd_conf *mdev, struct drbd_work *w, int unused)\r\n{\r\ndev_warn(DEV, "md_sync_timer expired! Worker calls drbd_md_sync().\n");\r\n#ifdef DEBUG\r\ndev_warn(DEV, "last md_mark_dirty: %s:%u\n",\r\nmdev->last_md_mark_dirty.func, mdev->last_md_mark_dirty.line);\r\n#endif\r\ndrbd_md_sync(mdev);\r\nreturn 1;\r\n}\r\nstatic unsigned long\r\n_drbd_fault_random(struct fault_random_state *rsp)\r\n{\r\nlong refresh;\r\nif (!rsp->count--) {\r\nget_random_bytes(&refresh, sizeof(refresh));\r\nrsp->state += refresh;\r\nrsp->count = FAULT_RANDOM_REFRESH;\r\n}\r\nrsp->state = rsp->state * FAULT_RANDOM_MULT + FAULT_RANDOM_ADD;\r\nreturn swahw32(rsp->state);\r\n}\r\nstatic char *\r\n_drbd_fault_str(unsigned int type) {\r\nstatic char *_faults[] = {\r\n[DRBD_FAULT_MD_WR] = "Meta-data write",\r\n[DRBD_FAULT_MD_RD] = "Meta-data read",\r\n[DRBD_FAULT_RS_WR] = "Resync write",\r\n[DRBD_FAULT_RS_RD] = "Resync read",\r\n[DRBD_FAULT_DT_WR] = "Data write",\r\n[DRBD_FAULT_DT_RD] = "Data read",\r\n[DRBD_FAULT_DT_RA] = "Data read ahead",\r\n[DRBD_FAULT_BM_ALLOC] = "BM allocation",\r\n[DRBD_FAULT_AL_EE] = "EE allocation",\r\n[DRBD_FAULT_RECEIVE] = "receive data corruption",\r\n};\r\nreturn (type < DRBD_FAULT_MAX) ? _faults[type] : "**Unknown**";\r\n}\r\nunsigned int\r\n_drbd_insert_fault(struct drbd_conf *mdev, unsigned int type)\r\n{\r\nstatic struct fault_random_state rrs = {0, 0};\r\nunsigned int ret = (\r\n(fault_devs == 0 ||\r\n((1 << mdev_to_minor(mdev)) & fault_devs) != 0) &&\r\n(((_drbd_fault_random(&rrs) % 100) + 1) <= fault_rate));\r\nif (ret) {\r\nfault_count++;\r\nif (__ratelimit(&drbd_ratelimit_state))\r\ndev_warn(DEV, "***Simulating %s failure\n",\r\n_drbd_fault_str(type));\r\n}\r\nreturn ret;\r\n}\r\nconst char *drbd_buildtag(void)\r\n{\r\nstatic char buildtag[38] = "\0uilt-in";\r\nif (buildtag[0] == 0) {\r\n#ifdef CONFIG_MODULES\r\nif (THIS_MODULE != NULL)\r\nsprintf(buildtag, "srcversion: %-24s", THIS_MODULE->srcversion);\r\nelse\r\n#endif\r\nbuildtag[0] = 'b';\r\n}\r\nreturn buildtag;\r\n}
