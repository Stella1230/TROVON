static int copy_vm86_regs_to_user(struct vm86_regs __user *user,\r\nconst struct kernel_vm86_regs *regs)\r\n{\r\nint ret = 0;\r\nret += copy_to_user(user, regs, offsetof(struct kernel_vm86_regs, pt.orig_ax));\r\nret += copy_to_user(&user->orig_eax, &regs->pt.orig_ax,\r\nsizeof(struct kernel_vm86_regs) -\r\noffsetof(struct kernel_vm86_regs, pt.orig_ax));\r\nreturn ret;\r\n}\r\nstatic int copy_vm86_regs_from_user(struct kernel_vm86_regs *regs,\r\nconst struct vm86_regs __user *user,\r\nunsigned extra)\r\n{\r\nint ret = 0;\r\nret += copy_from_user(regs, user, offsetof(struct kernel_vm86_regs, pt.orig_ax));\r\nret += copy_from_user(&regs->pt.orig_ax, &user->orig_eax,\r\nsizeof(struct kernel_vm86_regs) -\r\noffsetof(struct kernel_vm86_regs, pt.orig_ax) +\r\nextra);\r\nreturn ret;\r\n}\r\nstruct pt_regs *save_v86_state(struct kernel_vm86_regs *regs)\r\n{\r\nstruct tss_struct *tss;\r\nstruct pt_regs *ret;\r\nunsigned long tmp;\r\nlocal_irq_enable();\r\nif (!current->thread.vm86_info) {\r\nprintk("no vm86_info: BAD\n");\r\ndo_exit(SIGSEGV);\r\n}\r\nset_flags(regs->pt.flags, VEFLAGS, X86_EFLAGS_VIF | current->thread.v86mask);\r\ntmp = copy_vm86_regs_to_user(&current->thread.vm86_info->regs, regs);\r\ntmp += put_user(current->thread.screen_bitmap, &current->thread.vm86_info->screen_bitmap);\r\nif (tmp) {\r\nprintk("vm86: could not access userspace vm86_info\n");\r\ndo_exit(SIGSEGV);\r\n}\r\ntss = &per_cpu(init_tss, get_cpu());\r\ncurrent->thread.sp0 = current->thread.saved_sp0;\r\ncurrent->thread.sysenter_cs = __KERNEL_CS;\r\nload_sp0(tss, &current->thread);\r\ncurrent->thread.saved_sp0 = 0;\r\nput_cpu();\r\nret = KVM86->regs32;\r\nret->fs = current->thread.saved_fs;\r\nset_user_gs(ret, current->thread.saved_gs);\r\nreturn ret;\r\n}\r\nstatic void mark_screen_rdonly(struct mm_struct *mm)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *pte;\r\nspinlock_t *ptl;\r\nint i;\r\npgd = pgd_offset(mm, 0xA0000);\r\nif (pgd_none_or_clear_bad(pgd))\r\ngoto out;\r\npud = pud_offset(pgd, 0xA0000);\r\nif (pud_none_or_clear_bad(pud))\r\ngoto out;\r\npmd = pmd_offset(pud, 0xA0000);\r\nsplit_huge_page_pmd(mm, pmd);\r\nif (pmd_none_or_clear_bad(pmd))\r\ngoto out;\r\npte = pte_offset_map_lock(mm, pmd, 0xA0000, &ptl);\r\nfor (i = 0; i < 32; i++) {\r\nif (pte_present(*pte))\r\nset_pte(pte, pte_wrprotect(*pte));\r\npte++;\r\n}\r\npte_unmap_unlock(pte, ptl);\r\nout:\r\nflush_tlb();\r\n}\r\nint sys_vm86old(struct vm86_struct __user *v86, struct pt_regs *regs)\r\n{\r\nstruct kernel_vm86_struct info;\r\nstruct task_struct *tsk;\r\nint tmp, ret = -EPERM;\r\ntsk = current;\r\nif (tsk->thread.saved_sp0)\r\ngoto out;\r\ntmp = copy_vm86_regs_from_user(&info.regs, &v86->regs,\r\noffsetof(struct kernel_vm86_struct, vm86plus) -\r\nsizeof(info.regs));\r\nret = -EFAULT;\r\nif (tmp)\r\ngoto out;\r\nmemset(&info.vm86plus, 0, (int)&info.regs32 - (int)&info.vm86plus);\r\ninfo.regs32 = regs;\r\ntsk->thread.vm86_info = v86;\r\ndo_sys_vm86(&info, tsk);\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nint sys_vm86(unsigned long cmd, unsigned long arg, struct pt_regs *regs)\r\n{\r\nstruct kernel_vm86_struct info;\r\nstruct task_struct *tsk;\r\nint tmp, ret;\r\nstruct vm86plus_struct __user *v86;\r\ntsk = current;\r\nswitch (cmd) {\r\ncase VM86_REQUEST_IRQ:\r\ncase VM86_FREE_IRQ:\r\ncase VM86_GET_IRQ_BITS:\r\ncase VM86_GET_AND_RESET_IRQ:\r\nret = do_vm86_irq_handling(cmd, (int)arg);\r\ngoto out;\r\ncase VM86_PLUS_INSTALL_CHECK:\r\nret = 0;\r\ngoto out;\r\n}\r\nret = -EPERM;\r\nif (tsk->thread.saved_sp0)\r\ngoto out;\r\nv86 = (struct vm86plus_struct __user *)arg;\r\ntmp = copy_vm86_regs_from_user(&info.regs, &v86->regs,\r\noffsetof(struct kernel_vm86_struct, regs32) -\r\nsizeof(info.regs));\r\nret = -EFAULT;\r\nif (tmp)\r\ngoto out;\r\ninfo.regs32 = regs;\r\ninfo.vm86plus.is_vm86pus = 1;\r\ntsk->thread.vm86_info = (struct vm86_struct __user *)v86;\r\ndo_sys_vm86(&info, tsk);\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic void do_sys_vm86(struct kernel_vm86_struct *info, struct task_struct *tsk)\r\n{\r\nstruct tss_struct *tss;\r\ninfo->regs.pt.ds = 0;\r\ninfo->regs.pt.es = 0;\r\ninfo->regs.pt.fs = 0;\r\n#ifndef CONFIG_X86_32_LAZY_GS\r\ninfo->regs.pt.gs = 0;\r\n#endif\r\nVEFLAGS = info->regs.pt.flags;\r\ninfo->regs.pt.flags &= SAFE_MASK;\r\ninfo->regs.pt.flags |= info->regs32->flags & ~SAFE_MASK;\r\ninfo->regs.pt.flags |= X86_VM_MASK;\r\nswitch (info->cpu_type) {\r\ncase CPU_286:\r\ntsk->thread.v86mask = 0;\r\nbreak;\r\ncase CPU_386:\r\ntsk->thread.v86mask = X86_EFLAGS_NT | X86_EFLAGS_IOPL;\r\nbreak;\r\ncase CPU_486:\r\ntsk->thread.v86mask = X86_EFLAGS_AC | X86_EFLAGS_NT | X86_EFLAGS_IOPL;\r\nbreak;\r\ndefault:\r\ntsk->thread.v86mask = X86_EFLAGS_ID | X86_EFLAGS_AC | X86_EFLAGS_NT | X86_EFLAGS_IOPL;\r\nbreak;\r\n}\r\ninfo->regs32->ax = VM86_SIGNAL;\r\ntsk->thread.saved_sp0 = tsk->thread.sp0;\r\ntsk->thread.saved_fs = info->regs32->fs;\r\ntsk->thread.saved_gs = get_user_gs(info->regs32);\r\ntss = &per_cpu(init_tss, get_cpu());\r\ntsk->thread.sp0 = (unsigned long) &info->VM86_TSS_ESP0;\r\nif (cpu_has_sep)\r\ntsk->thread.sysenter_cs = 0;\r\nload_sp0(tss, &tsk->thread);\r\nput_cpu();\r\ntsk->thread.screen_bitmap = info->screen_bitmap;\r\nif (info->flags & VM86_SCREEN_BITMAP)\r\nmark_screen_rdonly(tsk->mm);\r\nif (unlikely(current->audit_context))\r\naudit_syscall_exit(AUDITSC_RESULT(0), 0);\r\n__asm__ __volatile__(\r\n"movl %0,%%esp\n\t"\r\n"movl %1,%%ebp\n\t"\r\n#ifdef CONFIG_X86_32_LAZY_GS\r\n"mov %2, %%gs\n\t"\r\n#endif\r\n"jmp resume_userspace"\r\n:\r\n:"r" (&info->regs), "r" (task_thread_info(tsk)), "r" (0));\r\n}\r\nstatic inline void return_to_32bit(struct kernel_vm86_regs *regs16, int retval)\r\n{\r\nstruct pt_regs *regs32;\r\nregs32 = save_v86_state(regs16);\r\nregs32->ax = retval;\r\n__asm__ __volatile__("movl %0,%%esp\n\t"\r\n"movl %1,%%ebp\n\t"\r\n"jmp resume_userspace"\r\n: : "r" (regs32), "r" (current_thread_info()));\r\n}\r\nstatic inline void set_IF(struct kernel_vm86_regs *regs)\r\n{\r\nVEFLAGS |= X86_EFLAGS_VIF;\r\nif (VEFLAGS & X86_EFLAGS_VIP)\r\nreturn_to_32bit(regs, VM86_STI);\r\n}\r\nstatic inline void clear_IF(struct kernel_vm86_regs *regs)\r\n{\r\nVEFLAGS &= ~X86_EFLAGS_VIF;\r\n}\r\nstatic inline void clear_TF(struct kernel_vm86_regs *regs)\r\n{\r\nregs->pt.flags &= ~X86_EFLAGS_TF;\r\n}\r\nstatic inline void clear_AC(struct kernel_vm86_regs *regs)\r\n{\r\nregs->pt.flags &= ~X86_EFLAGS_AC;\r\n}\r\nstatic inline void set_vflags_long(unsigned long flags, struct kernel_vm86_regs *regs)\r\n{\r\nset_flags(VEFLAGS, flags, current->thread.v86mask);\r\nset_flags(regs->pt.flags, flags, SAFE_MASK);\r\nif (flags & X86_EFLAGS_IF)\r\nset_IF(regs);\r\nelse\r\nclear_IF(regs);\r\n}\r\nstatic inline void set_vflags_short(unsigned short flags, struct kernel_vm86_regs *regs)\r\n{\r\nset_flags(VFLAGS, flags, current->thread.v86mask);\r\nset_flags(regs->pt.flags, flags, SAFE_MASK);\r\nif (flags & X86_EFLAGS_IF)\r\nset_IF(regs);\r\nelse\r\nclear_IF(regs);\r\n}\r\nstatic inline unsigned long get_vflags(struct kernel_vm86_regs *regs)\r\n{\r\nunsigned long flags = regs->pt.flags & RETURN_MASK;\r\nif (VEFLAGS & X86_EFLAGS_VIF)\r\nflags |= X86_EFLAGS_IF;\r\nflags |= X86_EFLAGS_IOPL;\r\nreturn flags | (VEFLAGS & current->thread.v86mask);\r\n}\r\nstatic inline int is_revectored(int nr, struct revectored_struct *bitmap)\r\n{\r\n__asm__ __volatile__("btl %2,%1\n\tsbbl %0,%0"\r\n:"=r" (nr)\r\n:"m" (*bitmap), "r" (nr));\r\nreturn nr;\r\n}\r\nstatic void do_int(struct kernel_vm86_regs *regs, int i,\r\nunsigned char __user *ssp, unsigned short sp)\r\n{\r\nunsigned long __user *intr_ptr;\r\nunsigned long segoffs;\r\nif (regs->pt.cs == BIOSSEG)\r\ngoto cannot_handle;\r\nif (is_revectored(i, &KVM86->int_revectored))\r\ngoto cannot_handle;\r\nif (i == 0x21 && is_revectored(AH(regs), &KVM86->int21_revectored))\r\ngoto cannot_handle;\r\nintr_ptr = (unsigned long __user *) (i << 2);\r\nif (get_user(segoffs, intr_ptr))\r\ngoto cannot_handle;\r\nif ((segoffs >> 16) == BIOSSEG)\r\ngoto cannot_handle;\r\npushw(ssp, sp, get_vflags(regs), cannot_handle);\r\npushw(ssp, sp, regs->pt.cs, cannot_handle);\r\npushw(ssp, sp, IP(regs), cannot_handle);\r\nregs->pt.cs = segoffs >> 16;\r\nSP(regs) -= 6;\r\nIP(regs) = segoffs & 0xffff;\r\nclear_TF(regs);\r\nclear_IF(regs);\r\nclear_AC(regs);\r\nreturn;\r\ncannot_handle:\r\nreturn_to_32bit(regs, VM86_INTx + (i << 8));\r\n}\r\nint handle_vm86_trap(struct kernel_vm86_regs *regs, long error_code, int trapno)\r\n{\r\nif (VMPI.is_vm86pus) {\r\nif ((trapno == 3) || (trapno == 1)) {\r\nKVM86->regs32->ax = VM86_TRAP + (trapno << 8);\r\nset_thread_flag(TIF_IRET);\r\nreturn 0;\r\n}\r\ndo_int(regs, trapno, (unsigned char __user *) (regs->pt.ss << 4), SP(regs));\r\nreturn 0;\r\n}\r\nif (trapno != 1)\r\nreturn 1;\r\ncurrent->thread.trap_no = trapno;\r\ncurrent->thread.error_code = error_code;\r\nforce_sig(SIGTRAP, current);\r\nreturn 0;\r\n}\r\nvoid handle_vm86_fault(struct kernel_vm86_regs *regs, long error_code)\r\n{\r\nunsigned char opcode;\r\nunsigned char __user *csp;\r\nunsigned char __user *ssp;\r\nunsigned short ip, sp, orig_flags;\r\nint data32, pref_done;\r\n#define CHECK_IF_IN_TRAP \\r\nif (VMPI.vm86dbg_active && VMPI.vm86dbg_TFpendig) \\r\nnewflags |= X86_EFLAGS_TF\r\n#define VM86_FAULT_RETURN do { \\r\nif (VMPI.force_return_for_pic && (VEFLAGS & (X86_EFLAGS_IF | X86_EFLAGS_VIF))) \\r\nreturn_to_32bit(regs, VM86_PICRETURN); \\r\nif (orig_flags & X86_EFLAGS_TF) \\r\nhandle_vm86_trap(regs, 0, 1); \\r\nreturn; } while (0)\r\norig_flags = *(unsigned short *)&regs->pt.flags;\r\ncsp = (unsigned char __user *) (regs->pt.cs << 4);\r\nssp = (unsigned char __user *) (regs->pt.ss << 4);\r\nsp = SP(regs);\r\nip = IP(regs);\r\ndata32 = 0;\r\npref_done = 0;\r\ndo {\r\nswitch (opcode = popb(csp, ip, simulate_sigsegv)) {\r\ncase 0x66: data32 = 1; break;\r\ncase 0x67: break;\r\ncase 0x2e: break;\r\ncase 0x3e: break;\r\ncase 0x26: break;\r\ncase 0x36: break;\r\ncase 0x65: break;\r\ncase 0x64: break;\r\ncase 0xf2: break;\r\ncase 0xf3: break;\r\ndefault: pref_done = 1;\r\n}\r\n} while (!pref_done);\r\nswitch (opcode) {\r\ncase 0x9c:\r\nif (data32) {\r\npushl(ssp, sp, get_vflags(regs), simulate_sigsegv);\r\nSP(regs) -= 4;\r\n} else {\r\npushw(ssp, sp, get_vflags(regs), simulate_sigsegv);\r\nSP(regs) -= 2;\r\n}\r\nIP(regs) = ip;\r\nVM86_FAULT_RETURN;\r\ncase 0x9d:\r\n{\r\nunsigned long newflags;\r\nif (data32) {\r\nnewflags = popl(ssp, sp, simulate_sigsegv);\r\nSP(regs) += 4;\r\n} else {\r\nnewflags = popw(ssp, sp, simulate_sigsegv);\r\nSP(regs) += 2;\r\n}\r\nIP(regs) = ip;\r\nCHECK_IF_IN_TRAP;\r\nif (data32)\r\nset_vflags_long(newflags, regs);\r\nelse\r\nset_vflags_short(newflags, regs);\r\nVM86_FAULT_RETURN;\r\n}\r\ncase 0xcd: {\r\nint intno = popb(csp, ip, simulate_sigsegv);\r\nIP(regs) = ip;\r\nif (VMPI.vm86dbg_active) {\r\nif ((1 << (intno & 7)) & VMPI.vm86dbg_intxxtab[intno >> 3])\r\nreturn_to_32bit(regs, VM86_INTx + (intno << 8));\r\n}\r\ndo_int(regs, intno, ssp, sp);\r\nreturn;\r\n}\r\ncase 0xcf:\r\n{\r\nunsigned long newip;\r\nunsigned long newcs;\r\nunsigned long newflags;\r\nif (data32) {\r\nnewip = popl(ssp, sp, simulate_sigsegv);\r\nnewcs = popl(ssp, sp, simulate_sigsegv);\r\nnewflags = popl(ssp, sp, simulate_sigsegv);\r\nSP(regs) += 12;\r\n} else {\r\nnewip = popw(ssp, sp, simulate_sigsegv);\r\nnewcs = popw(ssp, sp, simulate_sigsegv);\r\nnewflags = popw(ssp, sp, simulate_sigsegv);\r\nSP(regs) += 6;\r\n}\r\nIP(regs) = newip;\r\nregs->pt.cs = newcs;\r\nCHECK_IF_IN_TRAP;\r\nif (data32) {\r\nset_vflags_long(newflags, regs);\r\n} else {\r\nset_vflags_short(newflags, regs);\r\n}\r\nVM86_FAULT_RETURN;\r\n}\r\ncase 0xfa:\r\nIP(regs) = ip;\r\nclear_IF(regs);\r\nVM86_FAULT_RETURN;\r\ncase 0xfb:\r\nIP(regs) = ip;\r\nset_IF(regs);\r\nVM86_FAULT_RETURN;\r\ndefault:\r\nreturn_to_32bit(regs, VM86_UNKNOWN);\r\n}\r\nreturn;\r\nsimulate_sigsegv:\r\nreturn_to_32bit(regs, VM86_UNKNOWN);\r\n}\r\nstatic irqreturn_t irq_handler(int intno, void *dev_id)\r\n{\r\nint irq_bit;\r\nunsigned long flags;\r\nspin_lock_irqsave(&irqbits_lock, flags);\r\nirq_bit = 1 << intno;\r\nif ((irqbits & irq_bit) || !vm86_irqs[intno].tsk)\r\ngoto out;\r\nirqbits |= irq_bit;\r\nif (vm86_irqs[intno].sig)\r\nsend_sig(vm86_irqs[intno].sig, vm86_irqs[intno].tsk, 1);\r\ndisable_irq_nosync(intno);\r\nspin_unlock_irqrestore(&irqbits_lock, flags);\r\nreturn IRQ_HANDLED;\r\nout:\r\nspin_unlock_irqrestore(&irqbits_lock, flags);\r\nreturn IRQ_NONE;\r\n}\r\nstatic inline void free_vm86_irq(int irqnumber)\r\n{\r\nunsigned long flags;\r\nfree_irq(irqnumber, NULL);\r\nvm86_irqs[irqnumber].tsk = NULL;\r\nspin_lock_irqsave(&irqbits_lock, flags);\r\nirqbits &= ~(1 << irqnumber);\r\nspin_unlock_irqrestore(&irqbits_lock, flags);\r\n}\r\nvoid release_vm86_irqs(struct task_struct *task)\r\n{\r\nint i;\r\nfor (i = FIRST_VM86_IRQ ; i <= LAST_VM86_IRQ; i++)\r\nif (vm86_irqs[i].tsk == task)\r\nfree_vm86_irq(i);\r\n}\r\nstatic inline int get_and_reset_irq(int irqnumber)\r\n{\r\nint bit;\r\nunsigned long flags;\r\nint ret = 0;\r\nif (invalid_vm86_irq(irqnumber)) return 0;\r\nif (vm86_irqs[irqnumber].tsk != current) return 0;\r\nspin_lock_irqsave(&irqbits_lock, flags);\r\nbit = irqbits & (1 << irqnumber);\r\nirqbits &= ~bit;\r\nif (bit) {\r\nenable_irq(irqnumber);\r\nret = 1;\r\n}\r\nspin_unlock_irqrestore(&irqbits_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int do_vm86_irq_handling(int subfunction, int irqnumber)\r\n{\r\nint ret;\r\nswitch (subfunction) {\r\ncase VM86_GET_AND_RESET_IRQ: {\r\nreturn get_and_reset_irq(irqnumber);\r\n}\r\ncase VM86_GET_IRQ_BITS: {\r\nreturn irqbits;\r\n}\r\ncase VM86_REQUEST_IRQ: {\r\nint sig = irqnumber >> 8;\r\nint irq = irqnumber & 255;\r\nif (!capable(CAP_SYS_ADMIN)) return -EPERM;\r\nif (!((1 << sig) & ALLOWED_SIGS)) return -EPERM;\r\nif (invalid_vm86_irq(irq)) return -EPERM;\r\nif (vm86_irqs[irq].tsk) return -EPERM;\r\nret = request_irq(irq, &irq_handler, 0, VM86_IRQNAME, NULL);\r\nif (ret) return ret;\r\nvm86_irqs[irq].sig = sig;\r\nvm86_irqs[irq].tsk = current;\r\nreturn irq;\r\n}\r\ncase VM86_FREE_IRQ: {\r\nif (invalid_vm86_irq(irqnumber)) return -EPERM;\r\nif (!vm86_irqs[irqnumber].tsk) return 0;\r\nif (vm86_irqs[irqnumber].tsk != current) return -EPERM;\r\nfree_vm86_irq(irqnumber);\r\nreturn 0;\r\n}\r\n}\r\nreturn -EINVAL;\r\n}
