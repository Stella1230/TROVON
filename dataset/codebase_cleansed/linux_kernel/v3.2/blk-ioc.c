static void cfq_dtor(struct io_context *ioc)\r\n{\r\nif (!hlist_empty(&ioc->cic_list)) {\r\nstruct cfq_io_context *cic;\r\ncic = hlist_entry(ioc->cic_list.first, struct cfq_io_context,\r\ncic_list);\r\ncic->dtor(ioc);\r\n}\r\n}\r\nint put_io_context(struct io_context *ioc)\r\n{\r\nif (ioc == NULL)\r\nreturn 1;\r\nBUG_ON(atomic_long_read(&ioc->refcount) == 0);\r\nif (atomic_long_dec_and_test(&ioc->refcount)) {\r\nrcu_read_lock();\r\ncfq_dtor(ioc);\r\nrcu_read_unlock();\r\nkmem_cache_free(iocontext_cachep, ioc);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void cfq_exit(struct io_context *ioc)\r\n{\r\nrcu_read_lock();\r\nif (!hlist_empty(&ioc->cic_list)) {\r\nstruct cfq_io_context *cic;\r\ncic = hlist_entry(ioc->cic_list.first, struct cfq_io_context,\r\ncic_list);\r\ncic->exit(ioc);\r\n}\r\nrcu_read_unlock();\r\n}\r\nvoid exit_io_context(struct task_struct *task)\r\n{\r\nstruct io_context *ioc;\r\ntask_lock(task);\r\nioc = task->io_context;\r\ntask->io_context = NULL;\r\ntask_unlock(task);\r\nif (atomic_dec_and_test(&ioc->nr_tasks))\r\ncfq_exit(ioc);\r\nput_io_context(ioc);\r\n}\r\nstruct io_context *alloc_io_context(gfp_t gfp_flags, int node)\r\n{\r\nstruct io_context *ioc;\r\nioc = kmem_cache_alloc_node(iocontext_cachep, gfp_flags, node);\r\nif (ioc) {\r\natomic_long_set(&ioc->refcount, 1);\r\natomic_set(&ioc->nr_tasks, 1);\r\nspin_lock_init(&ioc->lock);\r\nioc->ioprio_changed = 0;\r\nioc->ioprio = 0;\r\nioc->last_waited = 0;\r\nioc->nr_batch_requests = 0;\r\nINIT_RADIX_TREE(&ioc->radix_root, GFP_ATOMIC | __GFP_HIGH);\r\nINIT_HLIST_HEAD(&ioc->cic_list);\r\nioc->ioc_data = NULL;\r\n#if defined(CONFIG_BLK_CGROUP) || defined(CONFIG_BLK_CGROUP_MODULE)\r\nioc->cgroup_changed = 0;\r\n#endif\r\n}\r\nreturn ioc;\r\n}\r\nstruct io_context *current_io_context(gfp_t gfp_flags, int node)\r\n{\r\nstruct task_struct *tsk = current;\r\nstruct io_context *ret;\r\nret = tsk->io_context;\r\nif (likely(ret))\r\nreturn ret;\r\nret = alloc_io_context(gfp_flags, node);\r\nif (ret) {\r\nsmp_wmb();\r\ntsk->io_context = ret;\r\n}\r\nreturn ret;\r\n}\r\nstruct io_context *get_io_context(gfp_t gfp_flags, int node)\r\n{\r\nstruct io_context *ioc = NULL;\r\ndo {\r\nioc = current_io_context(gfp_flags, node);\r\nif (unlikely(!ioc))\r\nbreak;\r\n} while (!atomic_long_inc_not_zero(&ioc->refcount));\r\nreturn ioc;\r\n}\r\nstatic int __init blk_ioc_init(void)\r\n{\r\niocontext_cachep = kmem_cache_create("blkdev_ioc",\r\nsizeof(struct io_context), 0, SLAB_PANIC, NULL);\r\nreturn 0;\r\n}
