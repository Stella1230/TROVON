void mlx4_en_fill_qp_context(struct mlx4_en_priv *priv, int size, int stride,\r\nint is_tx, int rss, int qpn, int cqn,\r\nstruct mlx4_qp_context *context)\r\n{\r\nstruct mlx4_en_dev *mdev = priv->mdev;\r\nmemset(context, 0, sizeof *context);\r\ncontext->flags = cpu_to_be32(7 << 16 | rss << 13);\r\ncontext->pd = cpu_to_be32(mdev->priv_pdn);\r\ncontext->mtu_msgmax = 0xff;\r\nif (!is_tx && !rss)\r\ncontext->rq_size_stride = ilog2(size) << 3 | (ilog2(stride) - 4);\r\nif (is_tx)\r\ncontext->sq_size_stride = ilog2(size) << 3 | (ilog2(stride) - 4);\r\nelse\r\ncontext->sq_size_stride = ilog2(TXBB_SIZE) - 4;\r\ncontext->usr_page = cpu_to_be32(mdev->priv_uar.index);\r\ncontext->local_qpn = cpu_to_be32(qpn);\r\ncontext->pri_path.ackto = 1 & 0x07;\r\ncontext->pri_path.sched_queue = 0x83 | (priv->port - 1) << 6;\r\ncontext->pri_path.counter_index = 0xff;\r\ncontext->cqn_send = cpu_to_be32(cqn);\r\ncontext->cqn_recv = cpu_to_be32(cqn);\r\ncontext->db_rec_addr = cpu_to_be64(priv->res.db.dma << 2);\r\n}\r\nint mlx4_en_map_buffer(struct mlx4_buf *buf)\r\n{\r\nstruct page **pages;\r\nint i;\r\nif (BITS_PER_LONG == 64 || buf->nbufs == 1)\r\nreturn 0;\r\npages = kmalloc(sizeof *pages * buf->nbufs, GFP_KERNEL);\r\nif (!pages)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < buf->nbufs; ++i)\r\npages[i] = virt_to_page(buf->page_list[i].buf);\r\nbuf->direct.buf = vmap(pages, buf->nbufs, VM_MAP, PAGE_KERNEL);\r\nkfree(pages);\r\nif (!buf->direct.buf)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid mlx4_en_unmap_buffer(struct mlx4_buf *buf)\r\n{\r\nif (BITS_PER_LONG == 64 || buf->nbufs == 1)\r\nreturn;\r\nvunmap(buf->direct.buf);\r\n}\r\nvoid mlx4_en_sqp_event(struct mlx4_qp *qp, enum mlx4_event event)\r\n{\r\nreturn;\r\n}
