static unsigned int gl_hash(const struct gfs2_sbd *sdp,\r\nconst struct lm_lockname *name)\r\n{\r\nunsigned int h;\r\nh = jhash(&name->ln_number, sizeof(u64), 0);\r\nh = jhash(&name->ln_type, sizeof(unsigned int), h);\r\nh = jhash(&sdp, sizeof(struct gfs2_sbd *), h);\r\nh &= GFS2_GL_HASH_MASK;\r\nreturn h;\r\n}\r\nstatic inline void spin_lock_bucket(unsigned int hash)\r\n{\r\nhlist_bl_lock(&gl_hash_table[hash]);\r\n}\r\nstatic inline void spin_unlock_bucket(unsigned int hash)\r\n{\r\nhlist_bl_unlock(&gl_hash_table[hash]);\r\n}\r\nstatic void gfs2_glock_dealloc(struct rcu_head *rcu)\r\n{\r\nstruct gfs2_glock *gl = container_of(rcu, struct gfs2_glock, gl_rcu);\r\nif (gl->gl_ops->go_flags & GLOF_ASPACE)\r\nkmem_cache_free(gfs2_glock_aspace_cachep, gl);\r\nelse\r\nkmem_cache_free(gfs2_glock_cachep, gl);\r\n}\r\nvoid gfs2_glock_free(struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\ncall_rcu(&gl->gl_rcu, gfs2_glock_dealloc);\r\nif (atomic_dec_and_test(&sdp->sd_glock_disposal))\r\nwake_up(&sdp->sd_glock_wait);\r\n}\r\nvoid gfs2_glock_hold(struct gfs2_glock *gl)\r\n{\r\nGLOCK_BUG_ON(gl, atomic_read(&gl->gl_ref) == 0);\r\natomic_inc(&gl->gl_ref);\r\n}\r\nstatic int demote_ok(const struct gfs2_glock *gl)\r\n{\r\nconst struct gfs2_glock_operations *glops = gl->gl_ops;\r\nif (gl->gl_state == LM_ST_UNLOCKED)\r\nreturn 0;\r\nif (!list_empty(&gl->gl_holders))\r\nreturn 0;\r\nif (glops->go_demote_ok)\r\nreturn glops->go_demote_ok(gl);\r\nreturn 1;\r\n}\r\nvoid gfs2_glock_add_to_lru(struct gfs2_glock *gl)\r\n{\r\nspin_lock(&lru_lock);\r\nif (!list_empty(&gl->gl_lru))\r\nlist_del_init(&gl->gl_lru);\r\nelse\r\natomic_inc(&lru_count);\r\nlist_add_tail(&gl->gl_lru, &lru_list);\r\nset_bit(GLF_LRU, &gl->gl_flags);\r\nspin_unlock(&lru_lock);\r\n}\r\nstatic void gfs2_glock_remove_from_lru(struct gfs2_glock *gl)\r\n{\r\nspin_lock(&lru_lock);\r\nif (!list_empty(&gl->gl_lru)) {\r\nlist_del_init(&gl->gl_lru);\r\natomic_dec(&lru_count);\r\nclear_bit(GLF_LRU, &gl->gl_flags);\r\n}\r\nspin_unlock(&lru_lock);\r\n}\r\nstatic void __gfs2_glock_schedule_for_reclaim(struct gfs2_glock *gl)\r\n{\r\nif (demote_ok(gl))\r\ngfs2_glock_add_to_lru(gl);\r\n}\r\nvoid gfs2_glock_put_nolock(struct gfs2_glock *gl)\r\n{\r\nif (atomic_dec_and_test(&gl->gl_ref))\r\nGLOCK_BUG_ON(gl, 1);\r\n}\r\nvoid gfs2_glock_put(struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nstruct address_space *mapping = gfs2_glock2aspace(gl);\r\nif (atomic_dec_and_test(&gl->gl_ref)) {\r\nspin_lock_bucket(gl->gl_hash);\r\nhlist_bl_del_rcu(&gl->gl_list);\r\nspin_unlock_bucket(gl->gl_hash);\r\ngfs2_glock_remove_from_lru(gl);\r\nGLOCK_BUG_ON(gl, !list_empty(&gl->gl_holders));\r\nGLOCK_BUG_ON(gl, mapping && mapping->nrpages);\r\ntrace_gfs2_glock_put(gl);\r\nsdp->sd_lockstruct.ls_ops->lm_put_lock(gl);\r\n}\r\n}\r\nstatic struct gfs2_glock *search_bucket(unsigned int hash,\r\nconst struct gfs2_sbd *sdp,\r\nconst struct lm_lockname *name)\r\n{\r\nstruct gfs2_glock *gl;\r\nstruct hlist_bl_node *h;\r\nhlist_bl_for_each_entry_rcu(gl, h, &gl_hash_table[hash], gl_list) {\r\nif (!lm_name_equal(&gl->gl_name, name))\r\ncontinue;\r\nif (gl->gl_sbd != sdp)\r\ncontinue;\r\nif (atomic_inc_not_zero(&gl->gl_ref))\r\nreturn gl;\r\n}\r\nreturn NULL;\r\n}\r\nstatic inline int may_grant(const struct gfs2_glock *gl, const struct gfs2_holder *gh)\r\n{\r\nconst struct gfs2_holder *gh_head = list_entry(gl->gl_holders.next, const struct gfs2_holder, gh_list);\r\nif ((gh->gh_state == LM_ST_EXCLUSIVE ||\r\ngh_head->gh_state == LM_ST_EXCLUSIVE) && gh != gh_head)\r\nreturn 0;\r\nif (gl->gl_state == gh->gh_state)\r\nreturn 1;\r\nif (gh->gh_flags & GL_EXACT)\r\nreturn 0;\r\nif (gl->gl_state == LM_ST_EXCLUSIVE) {\r\nif (gh->gh_state == LM_ST_SHARED && gh_head->gh_state == LM_ST_SHARED)\r\nreturn 1;\r\nif (gh->gh_state == LM_ST_DEFERRED && gh_head->gh_state == LM_ST_DEFERRED)\r\nreturn 1;\r\n}\r\nif (gl->gl_state != LM_ST_UNLOCKED && (gh->gh_flags & LM_FLAG_ANY))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void gfs2_holder_wake(struct gfs2_holder *gh)\r\n{\r\nclear_bit(HIF_WAIT, &gh->gh_iflags);\r\nsmp_mb__after_clear_bit();\r\nwake_up_bit(&gh->gh_iflags, HIF_WAIT);\r\n}\r\nstatic inline void do_error(struct gfs2_glock *gl, const int ret)\r\n{\r\nstruct gfs2_holder *gh, *tmp;\r\nlist_for_each_entry_safe(gh, tmp, &gl->gl_holders, gh_list) {\r\nif (test_bit(HIF_HOLDER, &gh->gh_iflags))\r\ncontinue;\r\nif (ret & LM_OUT_ERROR)\r\ngh->gh_error = -EIO;\r\nelse if (gh->gh_flags & (LM_FLAG_TRY | LM_FLAG_TRY_1CB))\r\ngh->gh_error = GLR_TRYFAILED;\r\nelse\r\ncontinue;\r\nlist_del_init(&gh->gh_list);\r\ntrace_gfs2_glock_queue(gh, 0);\r\ngfs2_holder_wake(gh);\r\n}\r\n}\r\nstatic int do_promote(struct gfs2_glock *gl)\r\n__releases(&gl->gl_spin\r\nstatic inline struct gfs2_holder *find_first_waiter(const struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_holder *gh;\r\nlist_for_each_entry(gh, &gl->gl_holders, gh_list) {\r\nif (!test_bit(HIF_HOLDER, &gh->gh_iflags))\r\nreturn gh;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void state_change(struct gfs2_glock *gl, unsigned int new_state)\r\n{\r\nint held1, held2;\r\nheld1 = (gl->gl_state != LM_ST_UNLOCKED);\r\nheld2 = (new_state != LM_ST_UNLOCKED);\r\nif (held1 != held2) {\r\nif (held2)\r\ngfs2_glock_hold(gl);\r\nelse\r\ngfs2_glock_put_nolock(gl);\r\n}\r\nif (held1 && held2 && list_empty(&gl->gl_holders))\r\nclear_bit(GLF_QUEUED, &gl->gl_flags);\r\nif (new_state != gl->gl_target)\r\ngl->gl_hold_time = max(gl->gl_hold_time - GL_GLOCK_HOLD_DECR,\r\nGL_GLOCK_MIN_HOLD);\r\ngl->gl_state = new_state;\r\ngl->gl_tchange = jiffies;\r\n}\r\nstatic void gfs2_demote_wake(struct gfs2_glock *gl)\r\n{\r\ngl->gl_demote_state = LM_ST_EXCLUSIVE;\r\nclear_bit(GLF_DEMOTE, &gl->gl_flags);\r\nsmp_mb__after_clear_bit();\r\nwake_up_bit(&gl->gl_flags, GLF_DEMOTE);\r\n}\r\nstatic void finish_xmote(struct gfs2_glock *gl, unsigned int ret)\r\n{\r\nconst struct gfs2_glock_operations *glops = gl->gl_ops;\r\nstruct gfs2_holder *gh;\r\nunsigned state = ret & LM_OUT_ST_MASK;\r\nint rv;\r\nspin_lock(&gl->gl_spin);\r\ntrace_gfs2_glock_state_change(gl, state);\r\nstate_change(gl, state);\r\ngh = find_first_waiter(gl);\r\nif (test_bit(GLF_DEMOTE_IN_PROGRESS, &gl->gl_flags) &&\r\nstate != LM_ST_UNLOCKED && gl->gl_demote_state == LM_ST_UNLOCKED)\r\ngl->gl_target = LM_ST_UNLOCKED;\r\nif (unlikely(state != gl->gl_target)) {\r\nif (gh && !test_bit(GLF_DEMOTE_IN_PROGRESS, &gl->gl_flags)) {\r\nif (ret & LM_OUT_CANCELED) {\r\nif ((gh->gh_flags & LM_FLAG_PRIORITY) == 0)\r\nlist_move_tail(&gh->gh_list, &gl->gl_holders);\r\ngh = find_first_waiter(gl);\r\ngl->gl_target = gh->gh_state;\r\ngoto retry;\r\n}\r\nif ((ret & LM_OUT_ERROR) ||\r\n(gh->gh_flags & (LM_FLAG_TRY | LM_FLAG_TRY_1CB))) {\r\ngl->gl_target = gl->gl_state;\r\ndo_error(gl, ret);\r\ngoto out;\r\n}\r\n}\r\nswitch(state) {\r\ncase LM_ST_UNLOCKED:\r\nretry:\r\ndo_xmote(gl, gh, gl->gl_target);\r\nbreak;\r\ncase LM_ST_SHARED:\r\ncase LM_ST_DEFERRED:\r\ndo_xmote(gl, gh, LM_ST_UNLOCKED);\r\nbreak;\r\ndefault:\r\nprintk(KERN_ERR "GFS2: wanted %u got %u\n", gl->gl_target, state);\r\nGLOCK_BUG_ON(gl, 1);\r\n}\r\nspin_unlock(&gl->gl_spin);\r\nreturn;\r\n}\r\nif (test_and_clear_bit(GLF_DEMOTE_IN_PROGRESS, &gl->gl_flags))\r\ngfs2_demote_wake(gl);\r\nif (state != LM_ST_UNLOCKED) {\r\nif (glops->go_xmote_bh) {\r\nspin_unlock(&gl->gl_spin);\r\nrv = glops->go_xmote_bh(gl, gh);\r\nspin_lock(&gl->gl_spin);\r\nif (rv) {\r\ndo_error(gl, rv);\r\ngoto out;\r\n}\r\n}\r\nrv = do_promote(gl);\r\nif (rv == 2)\r\ngoto out_locked;\r\n}\r\nout:\r\nclear_bit(GLF_LOCK, &gl->gl_flags);\r\nout_locked:\r\nspin_unlock(&gl->gl_spin);\r\n}\r\nstatic void do_xmote(struct gfs2_glock *gl, struct gfs2_holder *gh, unsigned int target)\r\n__releases(&gl->gl_spin\r\nstatic inline struct gfs2_holder *find_first_holder(const struct gfs2_glock *gl)\r\n{\r\nstruct gfs2_holder *gh;\r\nif (!list_empty(&gl->gl_holders)) {\r\ngh = list_entry(gl->gl_holders.next, struct gfs2_holder, gh_list);\r\nif (test_bit(HIF_HOLDER, &gh->gh_iflags))\r\nreturn gh;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void run_queue(struct gfs2_glock *gl, const int nonblock)\r\n__releases(&gl->gl_spin\r\nstatic void delete_work_func(struct work_struct *work)\r\n{\r\nstruct gfs2_glock *gl = container_of(work, struct gfs2_glock, gl_delete);\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nstruct gfs2_inode *ip;\r\nstruct inode *inode;\r\nu64 no_addr = gl->gl_name.ln_number;\r\nip = gl->gl_object;\r\nif (ip)\r\ninode = gfs2_ilookup(sdp->sd_vfs, no_addr, 1);\r\nelse\r\ninode = gfs2_lookup_by_inum(sdp, no_addr, NULL, GFS2_BLKST_UNLINKED);\r\nif (inode && !IS_ERR(inode)) {\r\nd_prune_aliases(inode);\r\niput(inode);\r\n}\r\ngfs2_glock_put(gl);\r\n}\r\nstatic void glock_work_func(struct work_struct *work)\r\n{\r\nunsigned long delay = 0;\r\nstruct gfs2_glock *gl = container_of(work, struct gfs2_glock, gl_work.work);\r\nint drop_ref = 0;\r\nif (test_and_clear_bit(GLF_REPLY_PENDING, &gl->gl_flags)) {\r\nfinish_xmote(gl, gl->gl_reply);\r\ndrop_ref = 1;\r\n}\r\nspin_lock(&gl->gl_spin);\r\nif (test_bit(GLF_PENDING_DEMOTE, &gl->gl_flags) &&\r\ngl->gl_state != LM_ST_UNLOCKED &&\r\ngl->gl_demote_state != LM_ST_EXCLUSIVE) {\r\nunsigned long holdtime, now = jiffies;\r\nholdtime = gl->gl_tchange + gl->gl_hold_time;\r\nif (time_before(now, holdtime))\r\ndelay = holdtime - now;\r\nif (!delay) {\r\nclear_bit(GLF_PENDING_DEMOTE, &gl->gl_flags);\r\nset_bit(GLF_DEMOTE, &gl->gl_flags);\r\n}\r\n}\r\nrun_queue(gl, 0);\r\nspin_unlock(&gl->gl_spin);\r\nif (!delay)\r\ngfs2_glock_put(gl);\r\nelse {\r\nif (gl->gl_name.ln_type != LM_TYPE_INODE)\r\ndelay = 0;\r\nif (queue_delayed_work(glock_workqueue, &gl->gl_work, delay) == 0)\r\ngfs2_glock_put(gl);\r\n}\r\nif (drop_ref)\r\ngfs2_glock_put(gl);\r\n}\r\nint gfs2_glock_get(struct gfs2_sbd *sdp, u64 number,\r\nconst struct gfs2_glock_operations *glops, int create,\r\nstruct gfs2_glock **glp)\r\n{\r\nstruct super_block *s = sdp->sd_vfs;\r\nstruct lm_lockname name = { .ln_number = number, .ln_type = glops->go_type };\r\nstruct gfs2_glock *gl, *tmp;\r\nunsigned int hash = gl_hash(sdp, &name);\r\nstruct address_space *mapping;\r\nstruct kmem_cache *cachep;\r\nrcu_read_lock();\r\ngl = search_bucket(hash, sdp, &name);\r\nrcu_read_unlock();\r\n*glp = gl;\r\nif (gl)\r\nreturn 0;\r\nif (!create)\r\nreturn -ENOENT;\r\nif (glops->go_flags & GLOF_ASPACE)\r\ncachep = gfs2_glock_aspace_cachep;\r\nelse\r\ncachep = gfs2_glock_cachep;\r\ngl = kmem_cache_alloc(cachep, GFP_KERNEL);\r\nif (!gl)\r\nreturn -ENOMEM;\r\natomic_inc(&sdp->sd_glock_disposal);\r\ngl->gl_flags = 0;\r\ngl->gl_name = name;\r\natomic_set(&gl->gl_ref, 1);\r\ngl->gl_state = LM_ST_UNLOCKED;\r\ngl->gl_target = LM_ST_UNLOCKED;\r\ngl->gl_demote_state = LM_ST_EXCLUSIVE;\r\ngl->gl_hash = hash;\r\ngl->gl_ops = glops;\r\nsnprintf(gl->gl_strname, GDLM_STRNAME_BYTES, "%8x%16llx", name.ln_type, (unsigned long long)number);\r\nmemset(&gl->gl_lksb, 0, sizeof(struct dlm_lksb));\r\ngl->gl_lksb.sb_lvbptr = gl->gl_lvb;\r\ngl->gl_tchange = jiffies;\r\ngl->gl_object = NULL;\r\ngl->gl_sbd = sdp;\r\ngl->gl_hold_time = GL_GLOCK_DFT_HOLD;\r\nINIT_DELAYED_WORK(&gl->gl_work, glock_work_func);\r\nINIT_WORK(&gl->gl_delete, delete_work_func);\r\nmapping = gfs2_glock2aspace(gl);\r\nif (mapping) {\r\nmapping->a_ops = &gfs2_meta_aops;\r\nmapping->host = s->s_bdev->bd_inode;\r\nmapping->flags = 0;\r\nmapping_set_gfp_mask(mapping, GFP_NOFS);\r\nmapping->assoc_mapping = NULL;\r\nmapping->backing_dev_info = s->s_bdi;\r\nmapping->writeback_index = 0;\r\n}\r\nspin_lock_bucket(hash);\r\ntmp = search_bucket(hash, sdp, &name);\r\nif (tmp) {\r\nspin_unlock_bucket(hash);\r\nkmem_cache_free(cachep, gl);\r\natomic_dec(&sdp->sd_glock_disposal);\r\ngl = tmp;\r\n} else {\r\nhlist_bl_add_head_rcu(&gl->gl_list, &gl_hash_table[hash]);\r\nspin_unlock_bucket(hash);\r\n}\r\n*glp = gl;\r\nreturn 0;\r\n}\r\nvoid gfs2_holder_init(struct gfs2_glock *gl, unsigned int state, unsigned flags,\r\nstruct gfs2_holder *gh)\r\n{\r\nINIT_LIST_HEAD(&gh->gh_list);\r\ngh->gh_gl = gl;\r\ngh->gh_ip = (unsigned long)__builtin_return_address(0);\r\ngh->gh_owner_pid = get_pid(task_pid(current));\r\ngh->gh_state = state;\r\ngh->gh_flags = flags;\r\ngh->gh_error = 0;\r\ngh->gh_iflags = 0;\r\ngfs2_glock_hold(gl);\r\n}\r\nvoid gfs2_holder_reinit(unsigned int state, unsigned flags, struct gfs2_holder *gh)\r\n{\r\ngh->gh_state = state;\r\ngh->gh_flags = flags;\r\ngh->gh_iflags = 0;\r\ngh->gh_ip = (unsigned long)__builtin_return_address(0);\r\nif (gh->gh_owner_pid)\r\nput_pid(gh->gh_owner_pid);\r\ngh->gh_owner_pid = get_pid(task_pid(current));\r\n}\r\nvoid gfs2_holder_uninit(struct gfs2_holder *gh)\r\n{\r\nput_pid(gh->gh_owner_pid);\r\ngfs2_glock_put(gh->gh_gl);\r\ngh->gh_gl = NULL;\r\ngh->gh_ip = 0;\r\n}\r\nstatic int gfs2_glock_holder_wait(void *word)\r\n{\r\nschedule();\r\nreturn 0;\r\n}\r\nstatic int gfs2_glock_demote_wait(void *word)\r\n{\r\nschedule();\r\nreturn 0;\r\n}\r\nstatic void wait_on_holder(struct gfs2_holder *gh)\r\n{\r\nunsigned long time1 = jiffies;\r\nmight_sleep();\r\nwait_on_bit(&gh->gh_iflags, HIF_WAIT, gfs2_glock_holder_wait, TASK_UNINTERRUPTIBLE);\r\nif (time_after(jiffies, time1 + HZ))\r\ngh->gh_gl->gl_hold_time = min(gh->gh_gl->gl_hold_time +\r\nGL_GLOCK_HOLD_INCR,\r\nGL_GLOCK_MAX_HOLD);\r\n}\r\nstatic void wait_on_demote(struct gfs2_glock *gl)\r\n{\r\nmight_sleep();\r\nwait_on_bit(&gl->gl_flags, GLF_DEMOTE, gfs2_glock_demote_wait, TASK_UNINTERRUPTIBLE);\r\n}\r\nstatic void handle_callback(struct gfs2_glock *gl, unsigned int state,\r\nunsigned long delay)\r\n{\r\nint bit = delay ? GLF_PENDING_DEMOTE : GLF_DEMOTE;\r\nset_bit(bit, &gl->gl_flags);\r\nif (gl->gl_demote_state == LM_ST_EXCLUSIVE) {\r\ngl->gl_demote_state = state;\r\ngl->gl_demote_time = jiffies;\r\n} else if (gl->gl_demote_state != LM_ST_UNLOCKED &&\r\ngl->gl_demote_state != state) {\r\ngl->gl_demote_state = LM_ST_UNLOCKED;\r\n}\r\nif (gl->gl_ops->go_callback)\r\ngl->gl_ops->go_callback(gl);\r\ntrace_gfs2_demote_rq(gl);\r\n}\r\nint gfs2_glock_wait(struct gfs2_holder *gh)\r\n{\r\nwait_on_holder(gh);\r\nreturn gh->gh_error;\r\n}\r\nvoid gfs2_print_dbg(struct seq_file *seq, const char *fmt, ...)\r\n{\r\nstruct va_format vaf;\r\nva_list args;\r\nva_start(args, fmt);\r\nif (seq) {\r\nstruct gfs2_glock_iter *gi = seq->private;\r\nvsprintf(gi->string, fmt, args);\r\nseq_printf(seq, gi->string);\r\n} else {\r\nvaf.fmt = fmt;\r\nvaf.va = &args;\r\nprintk(KERN_ERR " %pV", &vaf);\r\n}\r\nva_end(args);\r\n}\r\nstatic inline void add_to_queue(struct gfs2_holder *gh)\r\n__releases(&gl->gl_spin\r\nint gfs2_glock_nq(struct gfs2_holder *gh)\r\n{\r\nstruct gfs2_glock *gl = gh->gh_gl;\r\nstruct gfs2_sbd *sdp = gl->gl_sbd;\r\nint error = 0;\r\nif (unlikely(test_bit(SDF_SHUTDOWN, &sdp->sd_flags)))\r\nreturn -EIO;\r\nif (test_bit(GLF_LRU, &gl->gl_flags))\r\ngfs2_glock_remove_from_lru(gl);\r\nspin_lock(&gl->gl_spin);\r\nadd_to_queue(gh);\r\nif ((LM_FLAG_NOEXP & gh->gh_flags) &&\r\ntest_and_clear_bit(GLF_FROZEN, &gl->gl_flags))\r\nset_bit(GLF_REPLY_PENDING, &gl->gl_flags);\r\nrun_queue(gl, 1);\r\nspin_unlock(&gl->gl_spin);\r\nif (!(gh->gh_flags & GL_ASYNC))\r\nerror = gfs2_glock_wait(gh);\r\nreturn error;\r\n}\r\nint gfs2_glock_poll(struct gfs2_holder *gh)\r\n{\r\nreturn test_bit(HIF_WAIT, &gh->gh_iflags) ? 0 : 1;\r\n}\r\nvoid gfs2_glock_dq(struct gfs2_holder *gh)\r\n{\r\nstruct gfs2_glock *gl = gh->gh_gl;\r\nconst struct gfs2_glock_operations *glops = gl->gl_ops;\r\nunsigned delay = 0;\r\nint fast_path = 0;\r\nspin_lock(&gl->gl_spin);\r\nif (gh->gh_flags & GL_NOCACHE)\r\nhandle_callback(gl, LM_ST_UNLOCKED, 0);\r\nlist_del_init(&gh->gh_list);\r\nif (find_first_holder(gl) == NULL) {\r\nif (glops->go_unlock) {\r\nGLOCK_BUG_ON(gl, test_and_set_bit(GLF_LOCK, &gl->gl_flags));\r\nspin_unlock(&gl->gl_spin);\r\nglops->go_unlock(gh);\r\nspin_lock(&gl->gl_spin);\r\nclear_bit(GLF_LOCK, &gl->gl_flags);\r\n}\r\nif (list_empty(&gl->gl_holders) &&\r\n!test_bit(GLF_PENDING_DEMOTE, &gl->gl_flags) &&\r\n!test_bit(GLF_DEMOTE, &gl->gl_flags))\r\nfast_path = 1;\r\n}\r\nif (!test_bit(GLF_LFLUSH, &gl->gl_flags))\r\n__gfs2_glock_schedule_for_reclaim(gl);\r\ntrace_gfs2_glock_queue(gh, 0);\r\nspin_unlock(&gl->gl_spin);\r\nif (likely(fast_path))\r\nreturn;\r\ngfs2_glock_hold(gl);\r\nif (test_bit(GLF_PENDING_DEMOTE, &gl->gl_flags) &&\r\n!test_bit(GLF_DEMOTE, &gl->gl_flags) &&\r\ngl->gl_name.ln_type == LM_TYPE_INODE)\r\ndelay = gl->gl_hold_time;\r\nif (queue_delayed_work(glock_workqueue, &gl->gl_work, delay) == 0)\r\ngfs2_glock_put(gl);\r\n}\r\nvoid gfs2_glock_dq_wait(struct gfs2_holder *gh)\r\n{\r\nstruct gfs2_glock *gl = gh->gh_gl;\r\ngfs2_glock_dq(gh);\r\nwait_on_demote(gl);\r\n}\r\nvoid gfs2_glock_dq_uninit(struct gfs2_holder *gh)\r\n{\r\ngfs2_glock_dq(gh);\r\ngfs2_holder_uninit(gh);\r\n}\r\nint gfs2_glock_nq_num(struct gfs2_sbd *sdp, u64 number,\r\nconst struct gfs2_glock_operations *glops,\r\nunsigned int state, int flags, struct gfs2_holder *gh)\r\n{\r\nstruct gfs2_glock *gl;\r\nint error;\r\nerror = gfs2_glock_get(sdp, number, glops, CREATE, &gl);\r\nif (!error) {\r\nerror = gfs2_glock_nq_init(gl, state, flags, gh);\r\ngfs2_glock_put(gl);\r\n}\r\nreturn error;\r\n}\r\nstatic int glock_compare(const void *arg_a, const void *arg_b)\r\n{\r\nconst struct gfs2_holder *gh_a = *(const struct gfs2_holder **)arg_a;\r\nconst struct gfs2_holder *gh_b = *(const struct gfs2_holder **)arg_b;\r\nconst struct lm_lockname *a = &gh_a->gh_gl->gl_name;\r\nconst struct lm_lockname *b = &gh_b->gh_gl->gl_name;\r\nif (a->ln_number > b->ln_number)\r\nreturn 1;\r\nif (a->ln_number < b->ln_number)\r\nreturn -1;\r\nBUG_ON(gh_a->gh_gl->gl_ops->go_type == gh_b->gh_gl->gl_ops->go_type);\r\nreturn 0;\r\n}\r\nstatic int nq_m_sync(unsigned int num_gh, struct gfs2_holder *ghs,\r\nstruct gfs2_holder **p)\r\n{\r\nunsigned int x;\r\nint error = 0;\r\nfor (x = 0; x < num_gh; x++)\r\np[x] = &ghs[x];\r\nsort(p, num_gh, sizeof(struct gfs2_holder *), glock_compare, NULL);\r\nfor (x = 0; x < num_gh; x++) {\r\np[x]->gh_flags &= ~(LM_FLAG_TRY | GL_ASYNC);\r\nerror = gfs2_glock_nq(p[x]);\r\nif (error) {\r\nwhile (x--)\r\ngfs2_glock_dq(p[x]);\r\nbreak;\r\n}\r\n}\r\nreturn error;\r\n}\r\nint gfs2_glock_nq_m(unsigned int num_gh, struct gfs2_holder *ghs)\r\n{\r\nstruct gfs2_holder *tmp[4];\r\nstruct gfs2_holder **pph = tmp;\r\nint error = 0;\r\nswitch(num_gh) {\r\ncase 0:\r\nreturn 0;\r\ncase 1:\r\nghs->gh_flags &= ~(LM_FLAG_TRY | GL_ASYNC);\r\nreturn gfs2_glock_nq(ghs);\r\ndefault:\r\nif (num_gh <= 4)\r\nbreak;\r\npph = kmalloc(num_gh * sizeof(struct gfs2_holder *), GFP_NOFS);\r\nif (!pph)\r\nreturn -ENOMEM;\r\n}\r\nerror = nq_m_sync(num_gh, ghs, pph);\r\nif (pph != tmp)\r\nkfree(pph);\r\nreturn error;\r\n}\r\nvoid gfs2_glock_dq_m(unsigned int num_gh, struct gfs2_holder *ghs)\r\n{\r\nwhile (num_gh--)\r\ngfs2_glock_dq(&ghs[num_gh]);\r\n}\r\nvoid gfs2_glock_dq_uninit_m(unsigned int num_gh, struct gfs2_holder *ghs)\r\n{\r\nwhile (num_gh--)\r\ngfs2_glock_dq_uninit(&ghs[num_gh]);\r\n}\r\nvoid gfs2_glock_cb(struct gfs2_glock *gl, unsigned int state)\r\n{\r\nunsigned long delay = 0;\r\nunsigned long holdtime;\r\nunsigned long now = jiffies;\r\ngfs2_glock_hold(gl);\r\nholdtime = gl->gl_tchange + gl->gl_hold_time;\r\nif (test_bit(GLF_QUEUED, &gl->gl_flags) &&\r\ngl->gl_name.ln_type == LM_TYPE_INODE) {\r\nif (time_before(now, holdtime))\r\ndelay = holdtime - now;\r\nif (test_bit(GLF_REPLY_PENDING, &gl->gl_flags))\r\ndelay = gl->gl_hold_time;\r\n}\r\nspin_lock(&gl->gl_spin);\r\nhandle_callback(gl, state, delay);\r\nspin_unlock(&gl->gl_spin);\r\nif (queue_delayed_work(glock_workqueue, &gl->gl_work, delay) == 0)\r\ngfs2_glock_put(gl);\r\n}\r\nstatic int gfs2_should_freeze(const struct gfs2_glock *gl)\r\n{\r\nconst struct gfs2_holder *gh;\r\nif (gl->gl_reply & ~LM_OUT_ST_MASK)\r\nreturn 0;\r\nif (gl->gl_target == LM_ST_UNLOCKED)\r\nreturn 0;\r\nlist_for_each_entry(gh, &gl->gl_holders, gh_list) {\r\nif (test_bit(HIF_HOLDER, &gh->gh_iflags))\r\ncontinue;\r\nif (LM_FLAG_NOEXP & gh->gh_flags)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nvoid gfs2_glock_complete(struct gfs2_glock *gl, int ret)\r\n{\r\nstruct lm_lockstruct *ls = &gl->gl_sbd->sd_lockstruct;\r\nspin_lock(&gl->gl_spin);\r\ngl->gl_reply = ret;\r\nif (unlikely(test_bit(DFL_BLOCK_LOCKS, &ls->ls_flags))) {\r\nif (gfs2_should_freeze(gl)) {\r\nset_bit(GLF_FROZEN, &gl->gl_flags);\r\nspin_unlock(&gl->gl_spin);\r\nreturn;\r\n}\r\n}\r\nspin_unlock(&gl->gl_spin);\r\nset_bit(GLF_REPLY_PENDING, &gl->gl_flags);\r\nsmp_wmb();\r\ngfs2_glock_hold(gl);\r\nif (queue_delayed_work(glock_workqueue, &gl->gl_work, 0) == 0)\r\ngfs2_glock_put(gl);\r\n}\r\nstatic int gfs2_shrink_glock_memory(struct shrinker *shrink,\r\nstruct shrink_control *sc)\r\n{\r\nstruct gfs2_glock *gl;\r\nint may_demote;\r\nint nr_skipped = 0;\r\nint nr = sc->nr_to_scan;\r\ngfp_t gfp_mask = sc->gfp_mask;\r\nLIST_HEAD(skipped);\r\nif (nr == 0)\r\ngoto out;\r\nif (!(gfp_mask & __GFP_FS))\r\nreturn -1;\r\nspin_lock(&lru_lock);\r\nwhile(nr && !list_empty(&lru_list)) {\r\ngl = list_entry(lru_list.next, struct gfs2_glock, gl_lru);\r\nlist_del_init(&gl->gl_lru);\r\nclear_bit(GLF_LRU, &gl->gl_flags);\r\natomic_dec(&lru_count);\r\nif (!test_and_set_bit(GLF_LOCK, &gl->gl_flags)) {\r\ngfs2_glock_hold(gl);\r\nspin_unlock(&lru_lock);\r\nspin_lock(&gl->gl_spin);\r\nmay_demote = demote_ok(gl);\r\nif (may_demote) {\r\nhandle_callback(gl, LM_ST_UNLOCKED, 0);\r\nnr--;\r\n}\r\nclear_bit(GLF_LOCK, &gl->gl_flags);\r\nsmp_mb__after_clear_bit();\r\nif (queue_delayed_work(glock_workqueue, &gl->gl_work, 0) == 0)\r\ngfs2_glock_put_nolock(gl);\r\nspin_unlock(&gl->gl_spin);\r\nspin_lock(&lru_lock);\r\ncontinue;\r\n}\r\nnr_skipped++;\r\nlist_add(&gl->gl_lru, &skipped);\r\nset_bit(GLF_LRU, &gl->gl_flags);\r\n}\r\nlist_splice(&skipped, &lru_list);\r\natomic_add(nr_skipped, &lru_count);\r\nspin_unlock(&lru_lock);\r\nout:\r\nreturn (atomic_read(&lru_count) / 100) * sysctl_vfs_cache_pressure;\r\n}\r\nstatic void examine_bucket(glock_examiner examiner, const struct gfs2_sbd *sdp,\r\nunsigned int hash)\r\n{\r\nstruct gfs2_glock *gl;\r\nstruct hlist_bl_head *head = &gl_hash_table[hash];\r\nstruct hlist_bl_node *pos;\r\nrcu_read_lock();\r\nhlist_bl_for_each_entry_rcu(gl, pos, head, gl_list) {\r\nif ((gl->gl_sbd == sdp) && atomic_read(&gl->gl_ref))\r\nexaminer(gl);\r\n}\r\nrcu_read_unlock();\r\ncond_resched();\r\n}\r\nstatic void glock_hash_walk(glock_examiner examiner, const struct gfs2_sbd *sdp)\r\n{\r\nunsigned x;\r\nfor (x = 0; x < GFS2_GL_HASH_SIZE; x++)\r\nexamine_bucket(examiner, sdp, x);\r\n}\r\nstatic void thaw_glock(struct gfs2_glock *gl)\r\n{\r\nif (!test_and_clear_bit(GLF_FROZEN, &gl->gl_flags))\r\nreturn;\r\nset_bit(GLF_REPLY_PENDING, &gl->gl_flags);\r\ngfs2_glock_hold(gl);\r\nif (queue_delayed_work(glock_workqueue, &gl->gl_work, 0) == 0)\r\ngfs2_glock_put(gl);\r\n}\r\nstatic void clear_glock(struct gfs2_glock *gl)\r\n{\r\ngfs2_glock_remove_from_lru(gl);\r\nspin_lock(&gl->gl_spin);\r\nif (gl->gl_state != LM_ST_UNLOCKED)\r\nhandle_callback(gl, LM_ST_UNLOCKED, 0);\r\nspin_unlock(&gl->gl_spin);\r\ngfs2_glock_hold(gl);\r\nif (queue_delayed_work(glock_workqueue, &gl->gl_work, 0) == 0)\r\ngfs2_glock_put(gl);\r\n}\r\nvoid gfs2_glock_thaw(struct gfs2_sbd *sdp)\r\n{\r\nglock_hash_walk(thaw_glock, sdp);\r\n}\r\nstatic int dump_glock(struct seq_file *seq, struct gfs2_glock *gl)\r\n{\r\nint ret;\r\nspin_lock(&gl->gl_spin);\r\nret = __dump_glock(seq, gl);\r\nspin_unlock(&gl->gl_spin);\r\nreturn ret;\r\n}\r\nstatic void dump_glock_func(struct gfs2_glock *gl)\r\n{\r\ndump_glock(NULL, gl);\r\n}\r\nvoid gfs2_gl_hash_clear(struct gfs2_sbd *sdp)\r\n{\r\nglock_hash_walk(clear_glock, sdp);\r\nflush_workqueue(glock_workqueue);\r\nwait_event(sdp->sd_glock_wait, atomic_read(&sdp->sd_glock_disposal) == 0);\r\nglock_hash_walk(dump_glock_func, sdp);\r\n}\r\nvoid gfs2_glock_finish_truncate(struct gfs2_inode *ip)\r\n{\r\nstruct gfs2_glock *gl = ip->i_gl;\r\nint ret;\r\nret = gfs2_truncatei_resume(ip);\r\ngfs2_assert_withdraw(gl->gl_sbd, ret == 0);\r\nspin_lock(&gl->gl_spin);\r\nclear_bit(GLF_LOCK, &gl->gl_flags);\r\nrun_queue(gl, 1);\r\nspin_unlock(&gl->gl_spin);\r\n}\r\nstatic const char *state2str(unsigned state)\r\n{\r\nswitch(state) {\r\ncase LM_ST_UNLOCKED:\r\nreturn "UN";\r\ncase LM_ST_SHARED:\r\nreturn "SH";\r\ncase LM_ST_DEFERRED:\r\nreturn "DF";\r\ncase LM_ST_EXCLUSIVE:\r\nreturn "EX";\r\n}\r\nreturn "??";\r\n}\r\nstatic const char *hflags2str(char *buf, unsigned flags, unsigned long iflags)\r\n{\r\nchar *p = buf;\r\nif (flags & LM_FLAG_TRY)\r\n*p++ = 't';\r\nif (flags & LM_FLAG_TRY_1CB)\r\n*p++ = 'T';\r\nif (flags & LM_FLAG_NOEXP)\r\n*p++ = 'e';\r\nif (flags & LM_FLAG_ANY)\r\n*p++ = 'A';\r\nif (flags & LM_FLAG_PRIORITY)\r\n*p++ = 'p';\r\nif (flags & GL_ASYNC)\r\n*p++ = 'a';\r\nif (flags & GL_EXACT)\r\n*p++ = 'E';\r\nif (flags & GL_NOCACHE)\r\n*p++ = 'c';\r\nif (test_bit(HIF_HOLDER, &iflags))\r\n*p++ = 'H';\r\nif (test_bit(HIF_WAIT, &iflags))\r\n*p++ = 'W';\r\nif (test_bit(HIF_FIRST, &iflags))\r\n*p++ = 'F';\r\n*p = 0;\r\nreturn buf;\r\n}\r\nstatic int dump_holder(struct seq_file *seq, const struct gfs2_holder *gh)\r\n{\r\nstruct task_struct *gh_owner = NULL;\r\nchar flags_buf[32];\r\nif (gh->gh_owner_pid)\r\ngh_owner = pid_task(gh->gh_owner_pid, PIDTYPE_PID);\r\ngfs2_print_dbg(seq, " H: s:%s f:%s e:%d p:%ld [%s] %pS\n",\r\nstate2str(gh->gh_state),\r\nhflags2str(flags_buf, gh->gh_flags, gh->gh_iflags),\r\ngh->gh_error,\r\ngh->gh_owner_pid ? (long)pid_nr(gh->gh_owner_pid) : -1,\r\ngh_owner ? gh_owner->comm : "(ended)",\r\n(void *)gh->gh_ip);\r\nreturn 0;\r\n}\r\nstatic const char *gflags2str(char *buf, const struct gfs2_glock *gl)\r\n{\r\nconst unsigned long *gflags = &gl->gl_flags;\r\nchar *p = buf;\r\nif (test_bit(GLF_LOCK, gflags))\r\n*p++ = 'l';\r\nif (test_bit(GLF_DEMOTE, gflags))\r\n*p++ = 'D';\r\nif (test_bit(GLF_PENDING_DEMOTE, gflags))\r\n*p++ = 'd';\r\nif (test_bit(GLF_DEMOTE_IN_PROGRESS, gflags))\r\n*p++ = 'p';\r\nif (test_bit(GLF_DIRTY, gflags))\r\n*p++ = 'y';\r\nif (test_bit(GLF_LFLUSH, gflags))\r\n*p++ = 'f';\r\nif (test_bit(GLF_INVALIDATE_IN_PROGRESS, gflags))\r\n*p++ = 'i';\r\nif (test_bit(GLF_REPLY_PENDING, gflags))\r\n*p++ = 'r';\r\nif (test_bit(GLF_INITIAL, gflags))\r\n*p++ = 'I';\r\nif (test_bit(GLF_FROZEN, gflags))\r\n*p++ = 'F';\r\nif (test_bit(GLF_QUEUED, gflags))\r\n*p++ = 'q';\r\nif (test_bit(GLF_LRU, gflags))\r\n*p++ = 'L';\r\nif (gl->gl_object)\r\n*p++ = 'o';\r\n*p = 0;\r\nreturn buf;\r\n}\r\nstatic int __dump_glock(struct seq_file *seq, const struct gfs2_glock *gl)\r\n{\r\nconst struct gfs2_glock_operations *glops = gl->gl_ops;\r\nunsigned long long dtime;\r\nconst struct gfs2_holder *gh;\r\nchar gflags_buf[32];\r\nint error = 0;\r\ndtime = jiffies - gl->gl_demote_time;\r\ndtime *= 1000000/HZ;\r\nif (!test_bit(GLF_DEMOTE, &gl->gl_flags))\r\ndtime = 0;\r\ngfs2_print_dbg(seq, "G: s:%s n:%u/%llx f:%s t:%s d:%s/%llu a:%d v:%d r:%d m:%ld\n",\r\nstate2str(gl->gl_state),\r\ngl->gl_name.ln_type,\r\n(unsigned long long)gl->gl_name.ln_number,\r\ngflags2str(gflags_buf, gl),\r\nstate2str(gl->gl_target),\r\nstate2str(gl->gl_demote_state), dtime,\r\natomic_read(&gl->gl_ail_count),\r\natomic_read(&gl->gl_revokes),\r\natomic_read(&gl->gl_ref), gl->gl_hold_time);\r\nlist_for_each_entry(gh, &gl->gl_holders, gh_list) {\r\nerror = dump_holder(seq, gh);\r\nif (error)\r\ngoto out;\r\n}\r\nif (gl->gl_state != LM_ST_UNLOCKED && glops->go_dump)\r\nerror = glops->go_dump(seq, gl);\r\nout:\r\nreturn error;\r\n}\r\nint __init gfs2_glock_init(void)\r\n{\r\nunsigned i;\r\nfor(i = 0; i < GFS2_GL_HASH_SIZE; i++) {\r\nINIT_HLIST_BL_HEAD(&gl_hash_table[i]);\r\n}\r\nglock_workqueue = alloc_workqueue("glock_workqueue", WQ_MEM_RECLAIM |\r\nWQ_HIGHPRI | WQ_FREEZABLE, 0);\r\nif (IS_ERR(glock_workqueue))\r\nreturn PTR_ERR(glock_workqueue);\r\ngfs2_delete_workqueue = alloc_workqueue("delete_workqueue",\r\nWQ_MEM_RECLAIM | WQ_FREEZABLE,\r\n0);\r\nif (IS_ERR(gfs2_delete_workqueue)) {\r\ndestroy_workqueue(glock_workqueue);\r\nreturn PTR_ERR(gfs2_delete_workqueue);\r\n}\r\nregister_shrinker(&glock_shrinker);\r\nreturn 0;\r\n}\r\nvoid gfs2_glock_exit(void)\r\n{\r\nunregister_shrinker(&glock_shrinker);\r\ndestroy_workqueue(glock_workqueue);\r\ndestroy_workqueue(gfs2_delete_workqueue);\r\n}\r\nstatic inline struct gfs2_glock *glock_hash_chain(unsigned hash)\r\n{\r\nreturn hlist_bl_entry(hlist_bl_first_rcu(&gl_hash_table[hash]),\r\nstruct gfs2_glock, gl_list);\r\n}\r\nstatic inline struct gfs2_glock *glock_hash_next(struct gfs2_glock *gl)\r\n{\r\nreturn hlist_bl_entry(rcu_dereference(gl->gl_list.next),\r\nstruct gfs2_glock, gl_list);\r\n}\r\nstatic int gfs2_glock_iter_next(struct gfs2_glock_iter *gi)\r\n{\r\nstruct gfs2_glock *gl;\r\ndo {\r\ngl = gi->gl;\r\nif (gl) {\r\ngi->gl = glock_hash_next(gl);\r\n} else {\r\ngi->gl = glock_hash_chain(gi->hash);\r\n}\r\nwhile (gi->gl == NULL) {\r\ngi->hash++;\r\nif (gi->hash >= GFS2_GL_HASH_SIZE) {\r\nrcu_read_unlock();\r\nreturn 1;\r\n}\r\ngi->gl = glock_hash_chain(gi->hash);\r\n}\r\n} while (gi->sdp != gi->gl->gl_sbd || atomic_read(&gi->gl->gl_ref) == 0);\r\nreturn 0;\r\n}\r\nstatic void *gfs2_glock_seq_start(struct seq_file *seq, loff_t *pos)\r\n{\r\nstruct gfs2_glock_iter *gi = seq->private;\r\nloff_t n = *pos;\r\ngi->hash = 0;\r\nrcu_read_lock();\r\ndo {\r\nif (gfs2_glock_iter_next(gi))\r\nreturn NULL;\r\n} while (n--);\r\nreturn gi->gl;\r\n}\r\nstatic void *gfs2_glock_seq_next(struct seq_file *seq, void *iter_ptr,\r\nloff_t *pos)\r\n{\r\nstruct gfs2_glock_iter *gi = seq->private;\r\n(*pos)++;\r\nif (gfs2_glock_iter_next(gi))\r\nreturn NULL;\r\nreturn gi->gl;\r\n}\r\nstatic void gfs2_glock_seq_stop(struct seq_file *seq, void *iter_ptr)\r\n{\r\nstruct gfs2_glock_iter *gi = seq->private;\r\nif (gi->gl)\r\nrcu_read_unlock();\r\ngi->gl = NULL;\r\n}\r\nstatic int gfs2_glock_seq_show(struct seq_file *seq, void *iter_ptr)\r\n{\r\nreturn dump_glock(seq, iter_ptr);\r\n}\r\nstatic int gfs2_debugfs_open(struct inode *inode, struct file *file)\r\n{\r\nint ret = seq_open_private(file, &gfs2_glock_seq_ops,\r\nsizeof(struct gfs2_glock_iter));\r\nif (ret == 0) {\r\nstruct seq_file *seq = file->private_data;\r\nstruct gfs2_glock_iter *gi = seq->private;\r\ngi->sdp = inode->i_private;\r\n}\r\nreturn ret;\r\n}\r\nint gfs2_create_debugfs_file(struct gfs2_sbd *sdp)\r\n{\r\nsdp->debugfs_dir = debugfs_create_dir(sdp->sd_table_name, gfs2_root);\r\nif (!sdp->debugfs_dir)\r\nreturn -ENOMEM;\r\nsdp->debugfs_dentry_glocks = debugfs_create_file("glocks",\r\nS_IFREG | S_IRUGO,\r\nsdp->debugfs_dir, sdp,\r\n&gfs2_debug_fops);\r\nif (!sdp->debugfs_dentry_glocks)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid gfs2_delete_debugfs_file(struct gfs2_sbd *sdp)\r\n{\r\nif (sdp && sdp->debugfs_dir) {\r\nif (sdp->debugfs_dentry_glocks) {\r\ndebugfs_remove(sdp->debugfs_dentry_glocks);\r\nsdp->debugfs_dentry_glocks = NULL;\r\n}\r\ndebugfs_remove(sdp->debugfs_dir);\r\nsdp->debugfs_dir = NULL;\r\n}\r\n}\r\nint gfs2_register_debugfs(void)\r\n{\r\ngfs2_root = debugfs_create_dir("gfs2", NULL);\r\nreturn gfs2_root ? 0 : -ENOMEM;\r\n}\r\nvoid gfs2_unregister_debugfs(void)\r\n{\r\ndebugfs_remove(gfs2_root);\r\ngfs2_root = NULL;\r\n}
