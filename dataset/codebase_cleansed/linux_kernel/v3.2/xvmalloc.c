static void stat_inc(u64 *value)\r\n{\r\n*value = *value + 1;\r\n}\r\nstatic void stat_dec(u64 *value)\r\n{\r\n*value = *value - 1;\r\n}\r\nstatic int test_flag(struct block_header *block, enum blockflags flag)\r\n{\r\nreturn block->prev & BIT(flag);\r\n}\r\nstatic void set_flag(struct block_header *block, enum blockflags flag)\r\n{\r\nblock->prev |= BIT(flag);\r\n}\r\nstatic void clear_flag(struct block_header *block, enum blockflags flag)\r\n{\r\nblock->prev &= ~BIT(flag);\r\n}\r\nstatic void *get_ptr_atomic(struct page *page, u16 offset, enum km_type type)\r\n{\r\nunsigned char *base;\r\nbase = kmap_atomic(page, type);\r\nreturn base + offset;\r\n}\r\nstatic void put_ptr_atomic(void *ptr, enum km_type type)\r\n{\r\nkunmap_atomic(ptr, type);\r\n}\r\nstatic u32 get_blockprev(struct block_header *block)\r\n{\r\nreturn block->prev & PREV_MASK;\r\n}\r\nstatic void set_blockprev(struct block_header *block, u16 new_offset)\r\n{\r\nblock->prev = new_offset | (block->prev & FLAGS_MASK);\r\n}\r\nstatic struct block_header *BLOCK_NEXT(struct block_header *block)\r\n{\r\nreturn (struct block_header *)\r\n((char *)block + block->size + XV_ALIGN);\r\n}\r\nstatic u32 get_index_for_insert(u32 size)\r\n{\r\nif (unlikely(size > XV_MAX_ALLOC_SIZE))\r\nsize = XV_MAX_ALLOC_SIZE;\r\nsize &= ~FL_DELTA_MASK;\r\nreturn (size - XV_MIN_ALLOC_SIZE) >> FL_DELTA_SHIFT;\r\n}\r\nstatic u32 get_index(u32 size)\r\n{\r\nif (unlikely(size < XV_MIN_ALLOC_SIZE))\r\nsize = XV_MIN_ALLOC_SIZE;\r\nsize = ALIGN(size, FL_DELTA);\r\nreturn (size - XV_MIN_ALLOC_SIZE) >> FL_DELTA_SHIFT;\r\n}\r\nstatic u32 find_block(struct xv_pool *pool, u32 size,\r\nstruct page **page, u32 *offset)\r\n{\r\nulong flbitmap, slbitmap;\r\nu32 flindex, slindex, slbitstart;\r\nif (!pool->flbitmap)\r\nreturn 0;\r\nslindex = get_index(size);\r\nslbitmap = pool->slbitmap[slindex / BITS_PER_LONG];\r\nslbitstart = slindex % BITS_PER_LONG;\r\nif (test_bit(slbitstart, &slbitmap)) {\r\n*page = pool->freelist[slindex].page;\r\n*offset = pool->freelist[slindex].offset;\r\nreturn slindex;\r\n}\r\nslbitstart++;\r\nslbitmap >>= slbitstart;\r\nif ((slbitstart != BITS_PER_LONG) && slbitmap) {\r\nslindex += __ffs(slbitmap) + 1;\r\n*page = pool->freelist[slindex].page;\r\n*offset = pool->freelist[slindex].offset;\r\nreturn slindex;\r\n}\r\nflindex = slindex / BITS_PER_LONG;\r\nflbitmap = (pool->flbitmap) >> (flindex + 1);\r\nif (!flbitmap)\r\nreturn 0;\r\nflindex += __ffs(flbitmap) + 1;\r\nslbitmap = pool->slbitmap[flindex];\r\nslindex = (flindex * BITS_PER_LONG) + __ffs(slbitmap);\r\n*page = pool->freelist[slindex].page;\r\n*offset = pool->freelist[slindex].offset;\r\nreturn slindex;\r\n}\r\nstatic void insert_block(struct xv_pool *pool, struct page *page, u32 offset,\r\nstruct block_header *block)\r\n{\r\nu32 flindex, slindex;\r\nstruct block_header *nextblock;\r\nslindex = get_index_for_insert(block->size);\r\nflindex = slindex / BITS_PER_LONG;\r\nblock->link.prev_page = NULL;\r\nblock->link.prev_offset = 0;\r\nblock->link.next_page = pool->freelist[slindex].page;\r\nblock->link.next_offset = pool->freelist[slindex].offset;\r\npool->freelist[slindex].page = page;\r\npool->freelist[slindex].offset = offset;\r\nif (block->link.next_page) {\r\nnextblock = get_ptr_atomic(block->link.next_page,\r\nblock->link.next_offset, KM_USER1);\r\nnextblock->link.prev_page = page;\r\nnextblock->link.prev_offset = offset;\r\nput_ptr_atomic(nextblock, KM_USER1);\r\nreturn;\r\n}\r\n__set_bit(slindex % BITS_PER_LONG, &pool->slbitmap[flindex]);\r\n__set_bit(flindex, &pool->flbitmap);\r\n}\r\nstatic void remove_block(struct xv_pool *pool, struct page *page, u32 offset,\r\nstruct block_header *block, u32 slindex)\r\n{\r\nu32 flindex = slindex / BITS_PER_LONG;\r\nstruct block_header *tmpblock;\r\nif (block->link.prev_page) {\r\ntmpblock = get_ptr_atomic(block->link.prev_page,\r\nblock->link.prev_offset, KM_USER1);\r\ntmpblock->link.next_page = block->link.next_page;\r\ntmpblock->link.next_offset = block->link.next_offset;\r\nput_ptr_atomic(tmpblock, KM_USER1);\r\n}\r\nif (block->link.next_page) {\r\ntmpblock = get_ptr_atomic(block->link.next_page,\r\nblock->link.next_offset, KM_USER1);\r\ntmpblock->link.prev_page = block->link.prev_page;\r\ntmpblock->link.prev_offset = block->link.prev_offset;\r\nput_ptr_atomic(tmpblock, KM_USER1);\r\n}\r\nif (pool->freelist[slindex].page == page\r\n&& pool->freelist[slindex].offset == offset) {\r\npool->freelist[slindex].page = block->link.next_page;\r\npool->freelist[slindex].offset = block->link.next_offset;\r\nif (pool->freelist[slindex].page) {\r\nstruct block_header *tmpblock;\r\ntmpblock = get_ptr_atomic(pool->freelist[slindex].page,\r\npool->freelist[slindex].offset,\r\nKM_USER1);\r\ntmpblock->link.prev_page = NULL;\r\ntmpblock->link.prev_offset = 0;\r\nput_ptr_atomic(tmpblock, KM_USER1);\r\n} else {\r\n__clear_bit(slindex % BITS_PER_LONG,\r\n&pool->slbitmap[flindex]);\r\nif (!pool->slbitmap[flindex])\r\n__clear_bit(flindex, &pool->flbitmap);\r\n}\r\n}\r\nblock->link.prev_page = NULL;\r\nblock->link.prev_offset = 0;\r\nblock->link.next_page = NULL;\r\nblock->link.next_offset = 0;\r\n}\r\nstatic int grow_pool(struct xv_pool *pool, gfp_t flags)\r\n{\r\nstruct page *page;\r\nstruct block_header *block;\r\npage = alloc_page(flags);\r\nif (unlikely(!page))\r\nreturn -ENOMEM;\r\nstat_inc(&pool->total_pages);\r\nspin_lock(&pool->lock);\r\nblock = get_ptr_atomic(page, 0, KM_USER0);\r\nblock->size = PAGE_SIZE - XV_ALIGN;\r\nset_flag(block, BLOCK_FREE);\r\nclear_flag(block, PREV_FREE);\r\nset_blockprev(block, 0);\r\ninsert_block(pool, page, 0, block);\r\nput_ptr_atomic(block, KM_USER0);\r\nspin_unlock(&pool->lock);\r\nreturn 0;\r\n}\r\nstruct xv_pool *xv_create_pool(void)\r\n{\r\nu32 ovhd_size;\r\nstruct xv_pool *pool;\r\novhd_size = roundup(sizeof(*pool), PAGE_SIZE);\r\npool = kzalloc(ovhd_size, GFP_KERNEL);\r\nif (!pool)\r\nreturn NULL;\r\nspin_lock_init(&pool->lock);\r\nreturn pool;\r\n}\r\nvoid xv_destroy_pool(struct xv_pool *pool)\r\n{\r\nkfree(pool);\r\n}\r\nint xv_malloc(struct xv_pool *pool, u32 size, struct page **page,\r\nu32 *offset, gfp_t flags)\r\n{\r\nint error;\r\nu32 index, tmpsize, origsize, tmpoffset;\r\nstruct block_header *block, *tmpblock;\r\n*page = NULL;\r\n*offset = 0;\r\norigsize = size;\r\nif (unlikely(!size || size > XV_MAX_ALLOC_SIZE))\r\nreturn -ENOMEM;\r\nsize = ALIGN(size, XV_ALIGN);\r\nspin_lock(&pool->lock);\r\nindex = find_block(pool, size, page, offset);\r\nif (!*page) {\r\nspin_unlock(&pool->lock);\r\nif (flags & GFP_NOWAIT)\r\nreturn -ENOMEM;\r\nerror = grow_pool(pool, flags);\r\nif (unlikely(error))\r\nreturn error;\r\nspin_lock(&pool->lock);\r\nindex = find_block(pool, size, page, offset);\r\n}\r\nif (!*page) {\r\nspin_unlock(&pool->lock);\r\nreturn -ENOMEM;\r\n}\r\nblock = get_ptr_atomic(*page, *offset, KM_USER0);\r\nremove_block(pool, *page, *offset, block, index);\r\ntmpoffset = *offset + size + XV_ALIGN;\r\ntmpsize = block->size - size;\r\ntmpblock = (struct block_header *)((char *)block + size + XV_ALIGN);\r\nif (tmpsize) {\r\ntmpblock->size = tmpsize - XV_ALIGN;\r\nset_flag(tmpblock, BLOCK_FREE);\r\nclear_flag(tmpblock, PREV_FREE);\r\nset_blockprev(tmpblock, *offset);\r\nif (tmpblock->size >= XV_MIN_ALLOC_SIZE)\r\ninsert_block(pool, *page, tmpoffset, tmpblock);\r\nif (tmpoffset + XV_ALIGN + tmpblock->size != PAGE_SIZE) {\r\ntmpblock = BLOCK_NEXT(tmpblock);\r\nset_blockprev(tmpblock, tmpoffset);\r\n}\r\n} else {\r\nif (tmpoffset != PAGE_SIZE)\r\nclear_flag(tmpblock, PREV_FREE);\r\n}\r\nblock->size = origsize;\r\nclear_flag(block, BLOCK_FREE);\r\nput_ptr_atomic(block, KM_USER0);\r\nspin_unlock(&pool->lock);\r\n*offset += XV_ALIGN;\r\nreturn 0;\r\n}\r\nvoid xv_free(struct xv_pool *pool, struct page *page, u32 offset)\r\n{\r\nvoid *page_start;\r\nstruct block_header *block, *tmpblock;\r\noffset -= XV_ALIGN;\r\nspin_lock(&pool->lock);\r\npage_start = get_ptr_atomic(page, 0, KM_USER0);\r\nblock = (struct block_header *)((char *)page_start + offset);\r\nBUG_ON(test_flag(block, BLOCK_FREE));\r\nblock->size = ALIGN(block->size, XV_ALIGN);\r\ntmpblock = BLOCK_NEXT(block);\r\nif (offset + block->size + XV_ALIGN == PAGE_SIZE)\r\ntmpblock = NULL;\r\nif (tmpblock && test_flag(tmpblock, BLOCK_FREE)) {\r\nif (tmpblock->size >= XV_MIN_ALLOC_SIZE) {\r\nremove_block(pool, page,\r\noffset + block->size + XV_ALIGN, tmpblock,\r\nget_index_for_insert(tmpblock->size));\r\n}\r\nblock->size += tmpblock->size + XV_ALIGN;\r\n}\r\nif (test_flag(block, PREV_FREE)) {\r\ntmpblock = (struct block_header *)((char *)(page_start) +\r\nget_blockprev(block));\r\noffset = offset - tmpblock->size - XV_ALIGN;\r\nif (tmpblock->size >= XV_MIN_ALLOC_SIZE)\r\nremove_block(pool, page, offset, tmpblock,\r\nget_index_for_insert(tmpblock->size));\r\ntmpblock->size += block->size + XV_ALIGN;\r\nblock = tmpblock;\r\n}\r\nif (block->size == PAGE_SIZE - XV_ALIGN) {\r\nput_ptr_atomic(page_start, KM_USER0);\r\nspin_unlock(&pool->lock);\r\n__free_page(page);\r\nstat_dec(&pool->total_pages);\r\nreturn;\r\n}\r\nset_flag(block, BLOCK_FREE);\r\nif (block->size >= XV_MIN_ALLOC_SIZE)\r\ninsert_block(pool, page, offset, block);\r\nif (offset + block->size + XV_ALIGN != PAGE_SIZE) {\r\ntmpblock = BLOCK_NEXT(block);\r\nset_flag(tmpblock, PREV_FREE);\r\nset_blockprev(tmpblock, offset);\r\n}\r\nput_ptr_atomic(page_start, KM_USER0);\r\nspin_unlock(&pool->lock);\r\n}\r\nu32 xv_get_object_size(void *obj)\r\n{\r\nstruct block_header *blk;\r\nblk = (struct block_header *)((char *)(obj) - XV_ALIGN);\r\nreturn blk->size;\r\n}\r\nu64 xv_get_total_size_bytes(struct xv_pool *pool)\r\n{\r\nreturn pool->total_pages << PAGE_SHIFT;\r\n}
