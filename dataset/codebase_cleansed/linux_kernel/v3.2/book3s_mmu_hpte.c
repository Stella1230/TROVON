static inline u64 kvmppc_mmu_hash_pte(u64 eaddr)\r\n{\r\nreturn hash_64(eaddr >> PTE_SIZE, HPTEG_HASH_BITS_PTE);\r\n}\r\nstatic inline u64 kvmppc_mmu_hash_pte_long(u64 eaddr)\r\n{\r\nreturn hash_64((eaddr & 0x0ffff000) >> PTE_SIZE,\r\nHPTEG_HASH_BITS_PTE_LONG);\r\n}\r\nstatic inline u64 kvmppc_mmu_hash_vpte(u64 vpage)\r\n{\r\nreturn hash_64(vpage & 0xfffffffffULL, HPTEG_HASH_BITS_VPTE);\r\n}\r\nstatic inline u64 kvmppc_mmu_hash_vpte_long(u64 vpage)\r\n{\r\nreturn hash_64((vpage & 0xffffff000ULL) >> 12,\r\nHPTEG_HASH_BITS_VPTE_LONG);\r\n}\r\nvoid kvmppc_mmu_hpte_cache_map(struct kvm_vcpu *vcpu, struct hpte_cache *pte)\r\n{\r\nu64 index;\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\ntrace_kvm_book3s_mmu_map(pte);\r\nspin_lock(&vcpu3s->mmu_lock);\r\nindex = kvmppc_mmu_hash_pte(pte->pte.eaddr);\r\nhlist_add_head_rcu(&pte->list_pte, &vcpu3s->hpte_hash_pte[index]);\r\nindex = kvmppc_mmu_hash_pte_long(pte->pte.eaddr);\r\nhlist_add_head_rcu(&pte->list_pte_long,\r\n&vcpu3s->hpte_hash_pte_long[index]);\r\nindex = kvmppc_mmu_hash_vpte(pte->pte.vpage);\r\nhlist_add_head_rcu(&pte->list_vpte, &vcpu3s->hpte_hash_vpte[index]);\r\nindex = kvmppc_mmu_hash_vpte_long(pte->pte.vpage);\r\nhlist_add_head_rcu(&pte->list_vpte_long,\r\n&vcpu3s->hpte_hash_vpte_long[index]);\r\nspin_unlock(&vcpu3s->mmu_lock);\r\n}\r\nstatic void free_pte_rcu(struct rcu_head *head)\r\n{\r\nstruct hpte_cache *pte = container_of(head, struct hpte_cache, rcu_head);\r\nkmem_cache_free(hpte_cache, pte);\r\n}\r\nstatic void invalidate_pte(struct kvm_vcpu *vcpu, struct hpte_cache *pte)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\ntrace_kvm_book3s_mmu_invalidate(pte);\r\nkvmppc_mmu_invalidate_pte(vcpu, pte);\r\nspin_lock(&vcpu3s->mmu_lock);\r\nif (hlist_unhashed(&pte->list_pte)) {\r\nspin_unlock(&vcpu3s->mmu_lock);\r\nreturn;\r\n}\r\nhlist_del_init_rcu(&pte->list_pte);\r\nhlist_del_init_rcu(&pte->list_pte_long);\r\nhlist_del_init_rcu(&pte->list_vpte);\r\nhlist_del_init_rcu(&pte->list_vpte_long);\r\nif (pte->pte.may_write)\r\nkvm_release_pfn_dirty(pte->pfn);\r\nelse\r\nkvm_release_pfn_clean(pte->pfn);\r\nspin_unlock(&vcpu3s->mmu_lock);\r\nvcpu3s->hpte_cache_count--;\r\ncall_rcu(&pte->rcu_head, free_pte_rcu);\r\n}\r\nstatic void kvmppc_mmu_pte_flush_all(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nstruct hpte_cache *pte;\r\nstruct hlist_node *node;\r\nint i;\r\nrcu_read_lock();\r\nfor (i = 0; i < HPTEG_HASH_NUM_VPTE_LONG; i++) {\r\nstruct hlist_head *list = &vcpu3s->hpte_hash_vpte_long[i];\r\nhlist_for_each_entry_rcu(pte, node, list, list_vpte_long)\r\ninvalidate_pte(vcpu, pte);\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic void kvmppc_mmu_pte_flush_page(struct kvm_vcpu *vcpu, ulong guest_ea)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nstruct hlist_head *list;\r\nstruct hlist_node *node;\r\nstruct hpte_cache *pte;\r\nlist = &vcpu3s->hpte_hash_pte[kvmppc_mmu_hash_pte(guest_ea)];\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(pte, node, list, list_pte)\r\nif ((pte->pte.eaddr & ~0xfffUL) == guest_ea)\r\ninvalidate_pte(vcpu, pte);\r\nrcu_read_unlock();\r\n}\r\nstatic void kvmppc_mmu_pte_flush_long(struct kvm_vcpu *vcpu, ulong guest_ea)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nstruct hlist_head *list;\r\nstruct hlist_node *node;\r\nstruct hpte_cache *pte;\r\nlist = &vcpu3s->hpte_hash_pte_long[\r\nkvmppc_mmu_hash_pte_long(guest_ea)];\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(pte, node, list, list_pte_long)\r\nif ((pte->pte.eaddr & 0x0ffff000UL) == guest_ea)\r\ninvalidate_pte(vcpu, pte);\r\nrcu_read_unlock();\r\n}\r\nvoid kvmppc_mmu_pte_flush(struct kvm_vcpu *vcpu, ulong guest_ea, ulong ea_mask)\r\n{\r\ntrace_kvm_book3s_mmu_flush("", vcpu, guest_ea, ea_mask);\r\nguest_ea &= ea_mask;\r\nswitch (ea_mask) {\r\ncase ~0xfffUL:\r\nkvmppc_mmu_pte_flush_page(vcpu, guest_ea);\r\nbreak;\r\ncase 0x0ffff000:\r\nkvmppc_mmu_pte_flush_long(vcpu, guest_ea);\r\nbreak;\r\ncase 0:\r\nkvmppc_mmu_pte_flush_all(vcpu);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nbreak;\r\n}\r\n}\r\nstatic void kvmppc_mmu_pte_vflush_short(struct kvm_vcpu *vcpu, u64 guest_vp)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nstruct hlist_head *list;\r\nstruct hlist_node *node;\r\nstruct hpte_cache *pte;\r\nu64 vp_mask = 0xfffffffffULL;\r\nlist = &vcpu3s->hpte_hash_vpte[kvmppc_mmu_hash_vpte(guest_vp)];\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(pte, node, list, list_vpte)\r\nif ((pte->pte.vpage & vp_mask) == guest_vp)\r\ninvalidate_pte(vcpu, pte);\r\nrcu_read_unlock();\r\n}\r\nstatic void kvmppc_mmu_pte_vflush_long(struct kvm_vcpu *vcpu, u64 guest_vp)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nstruct hlist_head *list;\r\nstruct hlist_node *node;\r\nstruct hpte_cache *pte;\r\nu64 vp_mask = 0xffffff000ULL;\r\nlist = &vcpu3s->hpte_hash_vpte_long[\r\nkvmppc_mmu_hash_vpte_long(guest_vp)];\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(pte, node, list, list_vpte_long)\r\nif ((pte->pte.vpage & vp_mask) == guest_vp)\r\ninvalidate_pte(vcpu, pte);\r\nrcu_read_unlock();\r\n}\r\nvoid kvmppc_mmu_pte_vflush(struct kvm_vcpu *vcpu, u64 guest_vp, u64 vp_mask)\r\n{\r\ntrace_kvm_book3s_mmu_flush("v", vcpu, guest_vp, vp_mask);\r\nguest_vp &= vp_mask;\r\nswitch(vp_mask) {\r\ncase 0xfffffffffULL:\r\nkvmppc_mmu_pte_vflush_short(vcpu, guest_vp);\r\nbreak;\r\ncase 0xffffff000ULL:\r\nkvmppc_mmu_pte_vflush_long(vcpu, guest_vp);\r\nbreak;\r\ndefault:\r\nWARN_ON(1);\r\nreturn;\r\n}\r\n}\r\nvoid kvmppc_mmu_pte_pflush(struct kvm_vcpu *vcpu, ulong pa_start, ulong pa_end)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nstruct hlist_node *node;\r\nstruct hpte_cache *pte;\r\nint i;\r\ntrace_kvm_book3s_mmu_flush("p", vcpu, pa_start, pa_end);\r\nrcu_read_lock();\r\nfor (i = 0; i < HPTEG_HASH_NUM_VPTE_LONG; i++) {\r\nstruct hlist_head *list = &vcpu3s->hpte_hash_vpte_long[i];\r\nhlist_for_each_entry_rcu(pte, node, list, list_vpte_long)\r\nif ((pte->pte.raddr >= pa_start) &&\r\n(pte->pte.raddr < pa_end))\r\ninvalidate_pte(vcpu, pte);\r\n}\r\nrcu_read_unlock();\r\n}\r\nstruct hpte_cache *kvmppc_mmu_hpte_cache_next(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nstruct hpte_cache *pte;\r\npte = kmem_cache_zalloc(hpte_cache, GFP_KERNEL);\r\nvcpu3s->hpte_cache_count++;\r\nif (vcpu3s->hpte_cache_count == HPTEG_CACHE_NUM)\r\nkvmppc_mmu_pte_flush_all(vcpu);\r\nreturn pte;\r\n}\r\nvoid kvmppc_mmu_hpte_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_mmu_pte_flush(vcpu, 0, 0);\r\n}\r\nstatic void kvmppc_mmu_hpte_init_hash(struct hlist_head *hash_list, int len)\r\n{\r\nint i;\r\nfor (i = 0; i < len; i++)\r\nINIT_HLIST_HEAD(&hash_list[i]);\r\n}\r\nint kvmppc_mmu_hpte_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu3s = to_book3s(vcpu);\r\nkvmppc_mmu_hpte_init_hash(vcpu3s->hpte_hash_pte,\r\nARRAY_SIZE(vcpu3s->hpte_hash_pte));\r\nkvmppc_mmu_hpte_init_hash(vcpu3s->hpte_hash_pte_long,\r\nARRAY_SIZE(vcpu3s->hpte_hash_pte_long));\r\nkvmppc_mmu_hpte_init_hash(vcpu3s->hpte_hash_vpte,\r\nARRAY_SIZE(vcpu3s->hpte_hash_vpte));\r\nkvmppc_mmu_hpte_init_hash(vcpu3s->hpte_hash_vpte_long,\r\nARRAY_SIZE(vcpu3s->hpte_hash_vpte_long));\r\nspin_lock_init(&vcpu3s->mmu_lock);\r\nreturn 0;\r\n}\r\nint kvmppc_mmu_hpte_sysinit(void)\r\n{\r\nhpte_cache = kmem_cache_create("kvm-spt", sizeof(struct hpte_cache),\r\nsizeof(struct hpte_cache), 0, NULL);\r\nreturn 0;\r\n}\r\nvoid kvmppc_mmu_hpte_sysexit(void)\r\n{\r\nkmem_cache_destroy(hpte_cache);\r\n}
