int\r\nnouveau_gem_object_new(struct drm_gem_object *gem)\r\n{\r\nreturn 0;\r\n}\r\nvoid\r\nnouveau_gem_object_del(struct drm_gem_object *gem)\r\n{\r\nstruct nouveau_bo *nvbo = gem->driver_private;\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nif (!nvbo)\r\nreturn;\r\nnvbo->gem = NULL;\r\nif (unlikely(nvbo->pin_refcnt)) {\r\nnvbo->pin_refcnt = 1;\r\nnouveau_bo_unpin(nvbo);\r\n}\r\nttm_bo_unref(&bo);\r\ndrm_gem_object_release(gem);\r\nkfree(gem);\r\n}\r\nint\r\nnouveau_gem_object_open(struct drm_gem_object *gem, struct drm_file *file_priv)\r\n{\r\nstruct nouveau_fpriv *fpriv = nouveau_fpriv(file_priv);\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(gem);\r\nstruct nouveau_vma *vma;\r\nint ret;\r\nif (!fpriv->vm)\r\nreturn 0;\r\nret = ttm_bo_reserve(&nvbo->bo, false, false, false, 0);\r\nif (ret)\r\nreturn ret;\r\nvma = nouveau_bo_vma_find(nvbo, fpriv->vm);\r\nif (!vma) {\r\nvma = kzalloc(sizeof(*vma), GFP_KERNEL);\r\nif (!vma) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = nouveau_bo_vma_add(nvbo, fpriv->vm, vma);\r\nif (ret) {\r\nkfree(vma);\r\ngoto out;\r\n}\r\n} else {\r\nvma->refcount++;\r\n}\r\nout:\r\nttm_bo_unreserve(&nvbo->bo);\r\nreturn ret;\r\n}\r\nvoid\r\nnouveau_gem_object_close(struct drm_gem_object *gem, struct drm_file *file_priv)\r\n{\r\nstruct nouveau_fpriv *fpriv = nouveau_fpriv(file_priv);\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(gem);\r\nstruct nouveau_vma *vma;\r\nint ret;\r\nif (!fpriv->vm)\r\nreturn;\r\nret = ttm_bo_reserve(&nvbo->bo, false, false, false, 0);\r\nif (ret)\r\nreturn;\r\nvma = nouveau_bo_vma_find(nvbo, fpriv->vm);\r\nif (vma) {\r\nif (--vma->refcount == 0) {\r\nnouveau_bo_vma_del(nvbo, vma);\r\nkfree(vma);\r\n}\r\n}\r\nttm_bo_unreserve(&nvbo->bo);\r\n}\r\nint\r\nnouveau_gem_new(struct drm_device *dev, int size, int align, uint32_t domain,\r\nuint32_t tile_mode, uint32_t tile_flags,\r\nstruct nouveau_bo **pnvbo)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct nouveau_bo *nvbo;\r\nu32 flags = 0;\r\nint ret;\r\nif (domain & NOUVEAU_GEM_DOMAIN_VRAM)\r\nflags |= TTM_PL_FLAG_VRAM;\r\nif (domain & NOUVEAU_GEM_DOMAIN_GART)\r\nflags |= TTM_PL_FLAG_TT;\r\nif (!flags || domain & NOUVEAU_GEM_DOMAIN_CPU)\r\nflags |= TTM_PL_FLAG_SYSTEM;\r\nret = nouveau_bo_new(dev, size, align, flags, tile_mode,\r\ntile_flags, pnvbo);\r\nif (ret)\r\nreturn ret;\r\nnvbo = *pnvbo;\r\nnvbo->valid_domains = NOUVEAU_GEM_DOMAIN_VRAM |\r\nNOUVEAU_GEM_DOMAIN_GART;\r\nif (dev_priv->card_type >= NV_50)\r\nnvbo->valid_domains &= domain;\r\nnvbo->gem = drm_gem_object_alloc(dev, nvbo->bo.mem.size);\r\nif (!nvbo->gem) {\r\nnouveau_bo_ref(NULL, pnvbo);\r\nreturn -ENOMEM;\r\n}\r\nnvbo->bo.persistent_swap_storage = nvbo->gem->filp;\r\nnvbo->gem->driver_private = nvbo;\r\nreturn 0;\r\n}\r\nstatic int\r\nnouveau_gem_info(struct drm_file *file_priv, struct drm_gem_object *gem,\r\nstruct drm_nouveau_gem_info *rep)\r\n{\r\nstruct nouveau_fpriv *fpriv = nouveau_fpriv(file_priv);\r\nstruct nouveau_bo *nvbo = nouveau_gem_object(gem);\r\nstruct nouveau_vma *vma;\r\nif (nvbo->bo.mem.mem_type == TTM_PL_TT)\r\nrep->domain = NOUVEAU_GEM_DOMAIN_GART;\r\nelse\r\nrep->domain = NOUVEAU_GEM_DOMAIN_VRAM;\r\nrep->offset = nvbo->bo.offset;\r\nif (fpriv->vm) {\r\nvma = nouveau_bo_vma_find(nvbo, fpriv->vm);\r\nif (!vma)\r\nreturn -EINVAL;\r\nrep->offset = vma->offset;\r\n}\r\nrep->size = nvbo->bo.mem.num_pages << PAGE_SHIFT;\r\nrep->map_handle = nvbo->bo.addr_space_offset;\r\nrep->tile_mode = nvbo->tile_mode;\r\nrep->tile_flags = nvbo->tile_flags;\r\nreturn 0;\r\n}\r\nint\r\nnouveau_gem_ioctl_new(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct drm_nouveau_gem_new *req = data;\r\nstruct nouveau_bo *nvbo = NULL;\r\nint ret = 0;\r\nif (unlikely(dev_priv->ttm.bdev.dev_mapping == NULL))\r\ndev_priv->ttm.bdev.dev_mapping = dev_priv->dev->dev_mapping;\r\nif (!dev_priv->engine.vram.flags_valid(dev, req->info.tile_flags)) {\r\nNV_ERROR(dev, "bad page flags: 0x%08x\n", req->info.tile_flags);\r\nreturn -EINVAL;\r\n}\r\nret = nouveau_gem_new(dev, req->info.size, req->align,\r\nreq->info.domain, req->info.tile_mode,\r\nreq->info.tile_flags, &nvbo);\r\nif (ret)\r\nreturn ret;\r\nret = drm_gem_handle_create(file_priv, nvbo->gem, &req->info.handle);\r\nif (ret == 0) {\r\nret = nouveau_gem_info(file_priv, nvbo->gem, &req->info);\r\nif (ret)\r\ndrm_gem_handle_delete(file_priv, req->info.handle);\r\n}\r\ndrm_gem_object_unreference_unlocked(nvbo->gem);\r\nreturn ret;\r\n}\r\nstatic int\r\nnouveau_gem_set_domain(struct drm_gem_object *gem, uint32_t read_domains,\r\nuint32_t write_domains, uint32_t valid_domains)\r\n{\r\nstruct nouveau_bo *nvbo = gem->driver_private;\r\nstruct ttm_buffer_object *bo = &nvbo->bo;\r\nuint32_t domains = valid_domains & nvbo->valid_domains &\r\n(write_domains ? write_domains : read_domains);\r\nuint32_t pref_flags = 0, valid_flags = 0;\r\nif (!domains)\r\nreturn -EINVAL;\r\nif (valid_domains & NOUVEAU_GEM_DOMAIN_VRAM)\r\nvalid_flags |= TTM_PL_FLAG_VRAM;\r\nif (valid_domains & NOUVEAU_GEM_DOMAIN_GART)\r\nvalid_flags |= TTM_PL_FLAG_TT;\r\nif ((domains & NOUVEAU_GEM_DOMAIN_VRAM) &&\r\nbo->mem.mem_type == TTM_PL_VRAM)\r\npref_flags |= TTM_PL_FLAG_VRAM;\r\nelse if ((domains & NOUVEAU_GEM_DOMAIN_GART) &&\r\nbo->mem.mem_type == TTM_PL_TT)\r\npref_flags |= TTM_PL_FLAG_TT;\r\nelse if (domains & NOUVEAU_GEM_DOMAIN_VRAM)\r\npref_flags |= TTM_PL_FLAG_VRAM;\r\nelse\r\npref_flags |= TTM_PL_FLAG_TT;\r\nnouveau_bo_placement_set(nvbo, pref_flags, valid_flags);\r\nreturn 0;\r\n}\r\nstatic void\r\nvalidate_fini_list(struct list_head *list, struct nouveau_fence *fence)\r\n{\r\nstruct list_head *entry, *tmp;\r\nstruct nouveau_bo *nvbo;\r\nlist_for_each_safe(entry, tmp, list) {\r\nnvbo = list_entry(entry, struct nouveau_bo, entry);\r\nnouveau_bo_fence(nvbo, fence);\r\nif (unlikely(nvbo->validate_mapped)) {\r\nttm_bo_kunmap(&nvbo->kmap);\r\nnvbo->validate_mapped = false;\r\n}\r\nlist_del(&nvbo->entry);\r\nnvbo->reserved_by = NULL;\r\nttm_bo_unreserve(&nvbo->bo);\r\ndrm_gem_object_unreference_unlocked(nvbo->gem);\r\n}\r\n}\r\nstatic void\r\nvalidate_fini(struct validate_op *op, struct nouveau_fence* fence)\r\n{\r\nvalidate_fini_list(&op->vram_list, fence);\r\nvalidate_fini_list(&op->gart_list, fence);\r\nvalidate_fini_list(&op->both_list, fence);\r\n}\r\nstatic int\r\nvalidate_init(struct nouveau_channel *chan, struct drm_file *file_priv,\r\nstruct drm_nouveau_gem_pushbuf_bo *pbbo,\r\nint nr_buffers, struct validate_op *op)\r\n{\r\nstruct drm_device *dev = chan->dev;\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nuint32_t sequence;\r\nint trycnt = 0;\r\nint ret, i;\r\nsequence = atomic_add_return(1, &dev_priv->ttm.validate_sequence);\r\nretry:\r\nif (++trycnt > 100000) {\r\nNV_ERROR(dev, "%s failed and gave up.\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nfor (i = 0; i < nr_buffers; i++) {\r\nstruct drm_nouveau_gem_pushbuf_bo *b = &pbbo[i];\r\nstruct drm_gem_object *gem;\r\nstruct nouveau_bo *nvbo;\r\ngem = drm_gem_object_lookup(dev, file_priv, b->handle);\r\nif (!gem) {\r\nNV_ERROR(dev, "Unknown handle 0x%08x\n", b->handle);\r\nvalidate_fini(op, NULL);\r\nreturn -ENOENT;\r\n}\r\nnvbo = gem->driver_private;\r\nif (nvbo->reserved_by && nvbo->reserved_by == file_priv) {\r\nNV_ERROR(dev, "multiple instances of buffer %d on "\r\n"validation list\n", b->handle);\r\nvalidate_fini(op, NULL);\r\nreturn -EINVAL;\r\n}\r\nret = ttm_bo_reserve(&nvbo->bo, true, false, true, sequence);\r\nif (ret) {\r\nvalidate_fini(op, NULL);\r\nif (unlikely(ret == -EAGAIN))\r\nret = ttm_bo_wait_unreserved(&nvbo->bo, true);\r\ndrm_gem_object_unreference_unlocked(gem);\r\nif (unlikely(ret)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(dev, "fail reserve\n");\r\nreturn ret;\r\n}\r\ngoto retry;\r\n}\r\nb->user_priv = (uint64_t)(unsigned long)nvbo;\r\nnvbo->reserved_by = file_priv;\r\nnvbo->pbbo_index = i;\r\nif ((b->valid_domains & NOUVEAU_GEM_DOMAIN_VRAM) &&\r\n(b->valid_domains & NOUVEAU_GEM_DOMAIN_GART))\r\nlist_add_tail(&nvbo->entry, &op->both_list);\r\nelse\r\nif (b->valid_domains & NOUVEAU_GEM_DOMAIN_VRAM)\r\nlist_add_tail(&nvbo->entry, &op->vram_list);\r\nelse\r\nif (b->valid_domains & NOUVEAU_GEM_DOMAIN_GART)\r\nlist_add_tail(&nvbo->entry, &op->gart_list);\r\nelse {\r\nNV_ERROR(dev, "invalid valid domains: 0x%08x\n",\r\nb->valid_domains);\r\nlist_add_tail(&nvbo->entry, &op->both_list);\r\nvalidate_fini(op, NULL);\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nvalidate_list(struct nouveau_channel *chan, struct list_head *list,\r\nstruct drm_nouveau_gem_pushbuf_bo *pbbo, uint64_t user_pbbo_ptr)\r\n{\r\nstruct drm_nouveau_private *dev_priv = chan->dev->dev_private;\r\nstruct drm_nouveau_gem_pushbuf_bo __user *upbbo =\r\n(void __force __user *)(uintptr_t)user_pbbo_ptr;\r\nstruct drm_device *dev = chan->dev;\r\nstruct nouveau_bo *nvbo;\r\nint ret, relocs = 0;\r\nlist_for_each_entry(nvbo, list, entry) {\r\nstruct drm_nouveau_gem_pushbuf_bo *b = &pbbo[nvbo->pbbo_index];\r\nret = nouveau_fence_sync(nvbo->bo.sync_obj, chan);\r\nif (unlikely(ret)) {\r\nNV_ERROR(dev, "fail pre-validate sync\n");\r\nreturn ret;\r\n}\r\nret = nouveau_gem_set_domain(nvbo->gem, b->read_domains,\r\nb->write_domains,\r\nb->valid_domains);\r\nif (unlikely(ret)) {\r\nNV_ERROR(dev, "fail set_domain\n");\r\nreturn ret;\r\n}\r\nnvbo->channel = (b->read_domains & (1 << 31)) ? NULL : chan;\r\nret = nouveau_bo_validate(nvbo, true, false, false);\r\nnvbo->channel = NULL;\r\nif (unlikely(ret)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(dev, "fail ttm_validate\n");\r\nreturn ret;\r\n}\r\nret = nouveau_fence_sync(nvbo->bo.sync_obj, chan);\r\nif (unlikely(ret)) {\r\nNV_ERROR(dev, "fail post-validate sync\n");\r\nreturn ret;\r\n}\r\nif (dev_priv->card_type < NV_50) {\r\nif (nvbo->bo.offset == b->presumed.offset &&\r\n((nvbo->bo.mem.mem_type == TTM_PL_VRAM &&\r\nb->presumed.domain & NOUVEAU_GEM_DOMAIN_VRAM) ||\r\n(nvbo->bo.mem.mem_type == TTM_PL_TT &&\r\nb->presumed.domain & NOUVEAU_GEM_DOMAIN_GART)))\r\ncontinue;\r\nif (nvbo->bo.mem.mem_type == TTM_PL_TT)\r\nb->presumed.domain = NOUVEAU_GEM_DOMAIN_GART;\r\nelse\r\nb->presumed.domain = NOUVEAU_GEM_DOMAIN_VRAM;\r\nb->presumed.offset = nvbo->bo.offset;\r\nb->presumed.valid = 0;\r\nrelocs++;\r\nif (DRM_COPY_TO_USER(&upbbo[nvbo->pbbo_index].presumed,\r\n&b->presumed, sizeof(b->presumed)))\r\nreturn -EFAULT;\r\n}\r\n}\r\nreturn relocs;\r\n}\r\nstatic int\r\nnouveau_gem_pushbuf_validate(struct nouveau_channel *chan,\r\nstruct drm_file *file_priv,\r\nstruct drm_nouveau_gem_pushbuf_bo *pbbo,\r\nuint64_t user_buffers, int nr_buffers,\r\nstruct validate_op *op, int *apply_relocs)\r\n{\r\nstruct drm_device *dev = chan->dev;\r\nint ret, relocs = 0;\r\nINIT_LIST_HEAD(&op->vram_list);\r\nINIT_LIST_HEAD(&op->gart_list);\r\nINIT_LIST_HEAD(&op->both_list);\r\nif (nr_buffers == 0)\r\nreturn 0;\r\nret = validate_init(chan, file_priv, pbbo, nr_buffers, op);\r\nif (unlikely(ret)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(dev, "validate_init\n");\r\nreturn ret;\r\n}\r\nret = validate_list(chan, &op->vram_list, pbbo, user_buffers);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(dev, "validate vram_list\n");\r\nvalidate_fini(op, NULL);\r\nreturn ret;\r\n}\r\nrelocs += ret;\r\nret = validate_list(chan, &op->gart_list, pbbo, user_buffers);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(dev, "validate gart_list\n");\r\nvalidate_fini(op, NULL);\r\nreturn ret;\r\n}\r\nrelocs += ret;\r\nret = validate_list(chan, &op->both_list, pbbo, user_buffers);\r\nif (unlikely(ret < 0)) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(dev, "validate both_list\n");\r\nvalidate_fini(op, NULL);\r\nreturn ret;\r\n}\r\nrelocs += ret;\r\n*apply_relocs = relocs;\r\nreturn 0;\r\n}\r\nstatic inline void *\r\nu_memcpya(uint64_t user, unsigned nmemb, unsigned size)\r\n{\r\nvoid *mem;\r\nvoid __user *userptr = (void __force __user *)(uintptr_t)user;\r\nmem = kmalloc(nmemb * size, GFP_KERNEL);\r\nif (!mem)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (DRM_COPY_FROM_USER(mem, userptr, nmemb * size)) {\r\nkfree(mem);\r\nreturn ERR_PTR(-EFAULT);\r\n}\r\nreturn mem;\r\n}\r\nstatic int\r\nnouveau_gem_pushbuf_reloc_apply(struct drm_device *dev,\r\nstruct drm_nouveau_gem_pushbuf *req,\r\nstruct drm_nouveau_gem_pushbuf_bo *bo)\r\n{\r\nstruct drm_nouveau_gem_pushbuf_reloc *reloc = NULL;\r\nint ret = 0;\r\nunsigned i;\r\nreloc = u_memcpya(req->relocs, req->nr_relocs, sizeof(*reloc));\r\nif (IS_ERR(reloc))\r\nreturn PTR_ERR(reloc);\r\nfor (i = 0; i < req->nr_relocs; i++) {\r\nstruct drm_nouveau_gem_pushbuf_reloc *r = &reloc[i];\r\nstruct drm_nouveau_gem_pushbuf_bo *b;\r\nstruct nouveau_bo *nvbo;\r\nuint32_t data;\r\nif (unlikely(r->bo_index > req->nr_buffers)) {\r\nNV_ERROR(dev, "reloc bo index invalid\n");\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nb = &bo[r->bo_index];\r\nif (b->presumed.valid)\r\ncontinue;\r\nif (unlikely(r->reloc_bo_index > req->nr_buffers)) {\r\nNV_ERROR(dev, "reloc container bo index invalid\n");\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nnvbo = (void *)(unsigned long)bo[r->reloc_bo_index].user_priv;\r\nif (unlikely(r->reloc_bo_offset + 4 >\r\nnvbo->bo.mem.num_pages << PAGE_SHIFT)) {\r\nNV_ERROR(dev, "reloc outside of bo\n");\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nif (!nvbo->kmap.virtual) {\r\nret = ttm_bo_kmap(&nvbo->bo, 0, nvbo->bo.mem.num_pages,\r\n&nvbo->kmap);\r\nif (ret) {\r\nNV_ERROR(dev, "failed kmap for reloc\n");\r\nbreak;\r\n}\r\nnvbo->validate_mapped = true;\r\n}\r\nif (r->flags & NOUVEAU_GEM_RELOC_LOW)\r\ndata = b->presumed.offset + r->data;\r\nelse\r\nif (r->flags & NOUVEAU_GEM_RELOC_HIGH)\r\ndata = (b->presumed.offset + r->data) >> 32;\r\nelse\r\ndata = r->data;\r\nif (r->flags & NOUVEAU_GEM_RELOC_OR) {\r\nif (b->presumed.domain == NOUVEAU_GEM_DOMAIN_GART)\r\ndata |= r->tor;\r\nelse\r\ndata |= r->vor;\r\n}\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nret = ttm_bo_wait(&nvbo->bo, false, false, false);\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\nif (ret) {\r\nNV_ERROR(dev, "reloc wait_idle failed: %d\n", ret);\r\nbreak;\r\n}\r\nnouveau_bo_wr32(nvbo, r->reloc_bo_offset >> 2, data);\r\n}\r\nkfree(reloc);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_gem_ioctl_pushbuf(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nstruct drm_nouveau_gem_pushbuf *req = data;\r\nstruct drm_nouveau_gem_pushbuf_push *push;\r\nstruct drm_nouveau_gem_pushbuf_bo *bo;\r\nstruct nouveau_channel *chan;\r\nstruct validate_op op;\r\nstruct nouveau_fence *fence = NULL;\r\nint i, j, ret = 0, do_reloc = 0;\r\nchan = nouveau_channel_get(file_priv, req->channel);\r\nif (IS_ERR(chan))\r\nreturn PTR_ERR(chan);\r\nreq->vram_available = dev_priv->fb_aper_free;\r\nreq->gart_available = dev_priv->gart_info.aper_free;\r\nif (unlikely(req->nr_push == 0))\r\ngoto out_next;\r\nif (unlikely(req->nr_push > NOUVEAU_GEM_MAX_PUSH)) {\r\nNV_ERROR(dev, "pushbuf push count exceeds limit: %d max %d\n",\r\nreq->nr_push, NOUVEAU_GEM_MAX_PUSH);\r\nnouveau_channel_put(&chan);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(req->nr_buffers > NOUVEAU_GEM_MAX_BUFFERS)) {\r\nNV_ERROR(dev, "pushbuf bo count exceeds limit: %d max %d\n",\r\nreq->nr_buffers, NOUVEAU_GEM_MAX_BUFFERS);\r\nnouveau_channel_put(&chan);\r\nreturn -EINVAL;\r\n}\r\nif (unlikely(req->nr_relocs > NOUVEAU_GEM_MAX_RELOCS)) {\r\nNV_ERROR(dev, "pushbuf reloc count exceeds limit: %d max %d\n",\r\nreq->nr_relocs, NOUVEAU_GEM_MAX_RELOCS);\r\nnouveau_channel_put(&chan);\r\nreturn -EINVAL;\r\n}\r\npush = u_memcpya(req->push, req->nr_push, sizeof(*push));\r\nif (IS_ERR(push)) {\r\nnouveau_channel_put(&chan);\r\nreturn PTR_ERR(push);\r\n}\r\nbo = u_memcpya(req->buffers, req->nr_buffers, sizeof(*bo));\r\nif (IS_ERR(bo)) {\r\nkfree(push);\r\nnouveau_channel_put(&chan);\r\nreturn PTR_ERR(bo);\r\n}\r\nfor (i = 0; i < req->nr_push; i++) {\r\nif (push[i].bo_index >= req->nr_buffers) {\r\nNV_ERROR(dev, "push %d buffer not in list\n", i);\r\nret = -EINVAL;\r\ngoto out_prevalid;\r\n}\r\nbo[push[i].bo_index].read_domains |= (1 << 31);\r\n}\r\nret = nouveau_gem_pushbuf_validate(chan, file_priv, bo, req->buffers,\r\nreq->nr_buffers, &op, &do_reloc);\r\nif (ret) {\r\nif (ret != -ERESTARTSYS)\r\nNV_ERROR(dev, "validate: %d\n", ret);\r\ngoto out_prevalid;\r\n}\r\nif (do_reloc) {\r\nret = nouveau_gem_pushbuf_reloc_apply(dev, req, bo);\r\nif (ret) {\r\nNV_ERROR(dev, "reloc apply: %d\n", ret);\r\ngoto out;\r\n}\r\n}\r\nif (chan->dma.ib_max) {\r\nret = nouveau_dma_wait(chan, req->nr_push + 1, 6);\r\nif (ret) {\r\nNV_INFO(dev, "nv50cal_space: %d\n", ret);\r\ngoto out;\r\n}\r\nfor (i = 0; i < req->nr_push; i++) {\r\nstruct nouveau_bo *nvbo = (void *)(unsigned long)\r\nbo[push[i].bo_index].user_priv;\r\nnv50_dma_push(chan, nvbo, push[i].offset,\r\npush[i].length);\r\n}\r\n} else\r\nif (dev_priv->chipset >= 0x25) {\r\nret = RING_SPACE(chan, req->nr_push * 2);\r\nif (ret) {\r\nNV_ERROR(dev, "cal_space: %d\n", ret);\r\ngoto out;\r\n}\r\nfor (i = 0; i < req->nr_push; i++) {\r\nstruct nouveau_bo *nvbo = (void *)(unsigned long)\r\nbo[push[i].bo_index].user_priv;\r\nstruct drm_mm_node *mem = nvbo->bo.mem.mm_node;\r\nOUT_RING(chan, ((mem->start << PAGE_SHIFT) +\r\npush[i].offset) | 2);\r\nOUT_RING(chan, 0);\r\n}\r\n} else {\r\nret = RING_SPACE(chan, req->nr_push * (2 + NOUVEAU_DMA_SKIPS));\r\nif (ret) {\r\nNV_ERROR(dev, "jmp_space: %d\n", ret);\r\ngoto out;\r\n}\r\nfor (i = 0; i < req->nr_push; i++) {\r\nstruct nouveau_bo *nvbo = (void *)(unsigned long)\r\nbo[push[i].bo_index].user_priv;\r\nstruct drm_mm_node *mem = nvbo->bo.mem.mm_node;\r\nuint32_t cmd;\r\ncmd = chan->pushbuf_base + ((chan->dma.cur + 2) << 2);\r\ncmd |= 0x20000000;\r\nif (unlikely(cmd != req->suffix0)) {\r\nif (!nvbo->kmap.virtual) {\r\nret = ttm_bo_kmap(&nvbo->bo, 0,\r\nnvbo->bo.mem.\r\nnum_pages,\r\n&nvbo->kmap);\r\nif (ret) {\r\nWIND_RING(chan);\r\ngoto out;\r\n}\r\nnvbo->validate_mapped = true;\r\n}\r\nnouveau_bo_wr32(nvbo, (push[i].offset +\r\npush[i].length - 8) / 4, cmd);\r\n}\r\nOUT_RING(chan, ((mem->start << PAGE_SHIFT) +\r\npush[i].offset) | 0x20000000);\r\nOUT_RING(chan, 0);\r\nfor (j = 0; j < NOUVEAU_DMA_SKIPS; j++)\r\nOUT_RING(chan, 0);\r\n}\r\n}\r\nret = nouveau_fence_new(chan, &fence, true);\r\nif (ret) {\r\nNV_ERROR(dev, "error fencing pushbuf: %d\n", ret);\r\nWIND_RING(chan);\r\ngoto out;\r\n}\r\nout:\r\nvalidate_fini(&op, fence);\r\nnouveau_fence_unref(&fence);\r\nout_prevalid:\r\nkfree(bo);\r\nkfree(push);\r\nout_next:\r\nif (chan->dma.ib_max) {\r\nreq->suffix0 = 0x00000000;\r\nreq->suffix1 = 0x00000000;\r\n} else\r\nif (dev_priv->chipset >= 0x25) {\r\nreq->suffix0 = 0x00020000;\r\nreq->suffix1 = 0x00000000;\r\n} else {\r\nreq->suffix0 = 0x20000000 |\r\n(chan->pushbuf_base + ((chan->dma.cur + 2) << 2));\r\nreq->suffix1 = 0x00000000;\r\n}\r\nnouveau_channel_put(&chan);\r\nreturn ret;\r\n}\r\nstatic inline uint32_t\r\ndomain_to_ttm(struct nouveau_bo *nvbo, uint32_t domain)\r\n{\r\nuint32_t flags = 0;\r\nif (domain & NOUVEAU_GEM_DOMAIN_VRAM)\r\nflags |= TTM_PL_FLAG_VRAM;\r\nif (domain & NOUVEAU_GEM_DOMAIN_GART)\r\nflags |= TTM_PL_FLAG_TT;\r\nreturn flags;\r\n}\r\nint\r\nnouveau_gem_ioctl_cpu_prep(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_nouveau_gem_cpu_prep *req = data;\r\nstruct drm_gem_object *gem;\r\nstruct nouveau_bo *nvbo;\r\nbool no_wait = !!(req->flags & NOUVEAU_GEM_CPU_PREP_NOWAIT);\r\nint ret = -EINVAL;\r\ngem = drm_gem_object_lookup(dev, file_priv, req->handle);\r\nif (!gem)\r\nreturn -ENOENT;\r\nnvbo = nouveau_gem_object(gem);\r\nspin_lock(&nvbo->bo.bdev->fence_lock);\r\nret = ttm_bo_wait(&nvbo->bo, true, true, no_wait);\r\nspin_unlock(&nvbo->bo.bdev->fence_lock);\r\ndrm_gem_object_unreference_unlocked(gem);\r\nreturn ret;\r\n}\r\nint\r\nnouveau_gem_ioctl_cpu_fini(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nreturn 0;\r\n}\r\nint\r\nnouveau_gem_ioctl_info(struct drm_device *dev, void *data,\r\nstruct drm_file *file_priv)\r\n{\r\nstruct drm_nouveau_gem_info *req = data;\r\nstruct drm_gem_object *gem;\r\nint ret;\r\ngem = drm_gem_object_lookup(dev, file_priv, req->handle);\r\nif (!gem)\r\nreturn -ENOENT;\r\nret = nouveau_gem_info(file_priv, gem, req);\r\ndrm_gem_object_unreference_unlocked(gem);\r\nreturn ret;\r\n}
