static inline u32 omap_sham_read(struct omap_sham_dev *dd, u32 offset)\r\n{\r\nreturn __raw_readl(dd->io_base + offset);\r\n}\r\nstatic inline void omap_sham_write(struct omap_sham_dev *dd,\r\nu32 offset, u32 value)\r\n{\r\n__raw_writel(value, dd->io_base + offset);\r\n}\r\nstatic inline void omap_sham_write_mask(struct omap_sham_dev *dd, u32 address,\r\nu32 value, u32 mask)\r\n{\r\nu32 val;\r\nval = omap_sham_read(dd, address);\r\nval &= ~mask;\r\nval |= value;\r\nomap_sham_write(dd, address, val);\r\n}\r\nstatic inline int omap_sham_wait(struct omap_sham_dev *dd, u32 offset, u32 bit)\r\n{\r\nunsigned long timeout = jiffies + DEFAULT_TIMEOUT_INTERVAL;\r\nwhile (!(omap_sham_read(dd, offset) & bit)) {\r\nif (time_is_before_jiffies(timeout))\r\nreturn -ETIMEDOUT;\r\n}\r\nreturn 0;\r\n}\r\nstatic void omap_sham_copy_hash(struct ahash_request *req, int out)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nu32 *hash = (u32 *)ctx->digest;\r\nint i;\r\nfor (i = 0; i < SHA1_DIGEST_SIZE / sizeof(u32); i++) {\r\nif (out)\r\nhash[i] = omap_sham_read(ctx->dd,\r\nSHA_REG_DIGEST(i));\r\nelse\r\nomap_sham_write(ctx->dd,\r\nSHA_REG_DIGEST(i), hash[i]);\r\n}\r\n}\r\nstatic void omap_sham_copy_ready_hash(struct ahash_request *req)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nu32 *in = (u32 *)ctx->digest;\r\nu32 *hash = (u32 *)req->result;\r\nint i;\r\nif (!hash)\r\nreturn;\r\nif (likely(ctx->flags & BIT(FLAGS_SHA1))) {\r\nfor (i = 0; i < SHA1_DIGEST_SIZE / sizeof(u32); i++)\r\nhash[i] = be32_to_cpu(in[i]);\r\n} else {\r\nfor (i = 0; i < MD5_DIGEST_SIZE / sizeof(u32); i++)\r\nhash[i] = le32_to_cpu(in[i]);\r\n}\r\n}\r\nstatic int omap_sham_hw_init(struct omap_sham_dev *dd)\r\n{\r\nclk_enable(dd->iclk);\r\nif (!test_bit(FLAGS_INIT, &dd->flags)) {\r\nomap_sham_write_mask(dd, SHA_REG_MASK,\r\nSHA_REG_MASK_SOFTRESET, SHA_REG_MASK_SOFTRESET);\r\nif (omap_sham_wait(dd, SHA_REG_SYSSTATUS,\r\nSHA_REG_SYSSTATUS_RESETDONE))\r\nreturn -ETIMEDOUT;\r\nset_bit(FLAGS_INIT, &dd->flags);\r\ndd->err = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic void omap_sham_write_ctrl(struct omap_sham_dev *dd, size_t length,\r\nint final, int dma)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(dd->req);\r\nu32 val = length << 5, mask;\r\nif (likely(ctx->digcnt))\r\nomap_sham_write(dd, SHA_REG_DIGCNT, ctx->digcnt);\r\nomap_sham_write_mask(dd, SHA_REG_MASK,\r\nSHA_REG_MASK_IT_EN | (dma ? SHA_REG_MASK_DMA_EN : 0),\r\nSHA_REG_MASK_IT_EN | SHA_REG_MASK_DMA_EN);\r\nif (ctx->flags & BIT(FLAGS_SHA1))\r\nval |= SHA_REG_CTRL_ALGO;\r\nif (!ctx->digcnt)\r\nval |= SHA_REG_CTRL_ALGO_CONST;\r\nif (final)\r\nval |= SHA_REG_CTRL_CLOSE_HASH;\r\nmask = SHA_REG_CTRL_ALGO_CONST | SHA_REG_CTRL_CLOSE_HASH |\r\nSHA_REG_CTRL_ALGO | SHA_REG_CTRL_LENGTH;\r\nomap_sham_write_mask(dd, SHA_REG_CTRL, val, mask);\r\n}\r\nstatic int omap_sham_xmit_cpu(struct omap_sham_dev *dd, const u8 *buf,\r\nsize_t length, int final)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint count, len32;\r\nconst u32 *buffer = (const u32 *)buf;\r\ndev_dbg(dd->dev, "xmit_cpu: digcnt: %d, length: %d, final: %d\n",\r\nctx->digcnt, length, final);\r\nomap_sham_write_ctrl(dd, length, final, 0);\r\nctx->digcnt += length;\r\nif (omap_sham_wait(dd, SHA_REG_CTRL, SHA_REG_CTRL_INPUT_READY))\r\nreturn -ETIMEDOUT;\r\nif (final)\r\nset_bit(FLAGS_FINAL, &dd->flags);\r\nset_bit(FLAGS_CPU, &dd->flags);\r\nlen32 = DIV_ROUND_UP(length, sizeof(u32));\r\nfor (count = 0; count < len32; count++)\r\nomap_sham_write(dd, SHA_REG_DIN(count), buffer[count]);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int omap_sham_xmit_dma(struct omap_sham_dev *dd, dma_addr_t dma_addr,\r\nsize_t length, int final)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint len32;\r\ndev_dbg(dd->dev, "xmit_dma: digcnt: %d, length: %d, final: %d\n",\r\nctx->digcnt, length, final);\r\nlen32 = DIV_ROUND_UP(length, sizeof(u32));\r\nomap_set_dma_transfer_params(dd->dma_lch, OMAP_DMA_DATA_TYPE_S32, len32,\r\n1, OMAP_DMA_SYNC_PACKET, dd->dma,\r\nOMAP_DMA_DST_SYNC_PREFETCH);\r\nomap_set_dma_src_params(dd->dma_lch, 0, OMAP_DMA_AMODE_POST_INC,\r\ndma_addr, 0, 0);\r\nomap_sham_write_ctrl(dd, length, final, 1);\r\nctx->digcnt += length;\r\nif (final)\r\nset_bit(FLAGS_FINAL, &dd->flags);\r\nset_bit(FLAGS_DMA_ACTIVE, &dd->flags);\r\nomap_start_dma(dd->dma_lch);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic size_t omap_sham_append_buffer(struct omap_sham_reqctx *ctx,\r\nconst u8 *data, size_t length)\r\n{\r\nsize_t count = min(length, ctx->buflen - ctx->bufcnt);\r\ncount = min(count, ctx->total);\r\nif (count <= 0)\r\nreturn 0;\r\nmemcpy(ctx->buffer + ctx->bufcnt, data, count);\r\nctx->bufcnt += count;\r\nreturn count;\r\n}\r\nstatic size_t omap_sham_append_sg(struct omap_sham_reqctx *ctx)\r\n{\r\nsize_t count;\r\nwhile (ctx->sg) {\r\ncount = omap_sham_append_buffer(ctx,\r\nsg_virt(ctx->sg) + ctx->offset,\r\nctx->sg->length - ctx->offset);\r\nif (!count)\r\nbreak;\r\nctx->offset += count;\r\nctx->total -= count;\r\nif (ctx->offset == ctx->sg->length) {\r\nctx->sg = sg_next(ctx->sg);\r\nif (ctx->sg)\r\nctx->offset = 0;\r\nelse\r\nctx->total = 0;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_sham_xmit_dma_map(struct omap_sham_dev *dd,\r\nstruct omap_sham_reqctx *ctx,\r\nsize_t length, int final)\r\n{\r\nctx->dma_addr = dma_map_single(dd->dev, ctx->buffer, ctx->buflen,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(dd->dev, ctx->dma_addr)) {\r\ndev_err(dd->dev, "dma %u bytes error\n", ctx->buflen);\r\nreturn -EINVAL;\r\n}\r\nctx->flags &= ~BIT(FLAGS_SG);\r\nreturn omap_sham_xmit_dma(dd, ctx->dma_addr, length, final);\r\n}\r\nstatic int omap_sham_update_dma_slow(struct omap_sham_dev *dd)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(dd->req);\r\nunsigned int final;\r\nsize_t count;\r\nomap_sham_append_sg(ctx);\r\nfinal = (ctx->flags & BIT(FLAGS_FINUP)) && !ctx->total;\r\ndev_dbg(dd->dev, "slow: bufcnt: %u, digcnt: %d, final: %d\n",\r\nctx->bufcnt, ctx->digcnt, final);\r\nif (final || (ctx->bufcnt == ctx->buflen && ctx->total)) {\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn omap_sham_xmit_dma_map(dd, ctx, count, final);\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_sham_update_dma_start(struct omap_sham_dev *dd)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(dd->req);\r\nunsigned int length, final, tail;\r\nstruct scatterlist *sg;\r\nif (!ctx->total)\r\nreturn 0;\r\nif (ctx->bufcnt || ctx->offset)\r\nreturn omap_sham_update_dma_slow(dd);\r\ndev_dbg(dd->dev, "fast: digcnt: %d, bufcnt: %u, total: %u\n",\r\nctx->digcnt, ctx->bufcnt, ctx->total);\r\nsg = ctx->sg;\r\nif (!SG_AA(sg))\r\nreturn omap_sham_update_dma_slow(dd);\r\nif (!sg_is_last(sg) && !SG_SA(sg))\r\nreturn omap_sham_update_dma_slow(dd);\r\nlength = min(ctx->total, sg->length);\r\nif (sg_is_last(sg)) {\r\nif (!(ctx->flags & BIT(FLAGS_FINUP))) {\r\ntail = length & (SHA1_MD5_BLOCK_SIZE - 1);\r\nif (!tail)\r\ntail = SHA1_MD5_BLOCK_SIZE;\r\nlength -= tail;\r\n}\r\n}\r\nif (!dma_map_sg(dd->dev, ctx->sg, 1, DMA_TO_DEVICE)) {\r\ndev_err(dd->dev, "dma_map_sg error\n");\r\nreturn -EINVAL;\r\n}\r\nctx->flags |= BIT(FLAGS_SG);\r\nctx->total -= length;\r\nctx->offset = length;\r\nfinal = (ctx->flags & BIT(FLAGS_FINUP)) && !ctx->total;\r\nreturn omap_sham_xmit_dma(dd, sg_dma_address(ctx->sg), length, final);\r\n}\r\nstatic int omap_sham_update_cpu(struct omap_sham_dev *dd)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint bufcnt;\r\nomap_sham_append_sg(ctx);\r\nbufcnt = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn omap_sham_xmit_cpu(dd, ctx->buffer, bufcnt, 1);\r\n}\r\nstatic int omap_sham_update_dma_stop(struct omap_sham_dev *dd)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(dd->req);\r\nomap_stop_dma(dd->dma_lch);\r\nif (ctx->flags & BIT(FLAGS_SG)) {\r\ndma_unmap_sg(dd->dev, ctx->sg, 1, DMA_TO_DEVICE);\r\nif (ctx->sg->length == ctx->offset) {\r\nctx->sg = sg_next(ctx->sg);\r\nif (ctx->sg)\r\nctx->offset = 0;\r\n}\r\n} else {\r\ndma_unmap_single(dd->dev, ctx->dma_addr, ctx->buflen,\r\nDMA_TO_DEVICE);\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_sham_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct omap_sham_ctx *tctx = crypto_ahash_ctx(tfm);\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nstruct omap_sham_dev *dd = NULL, *tmp;\r\nspin_lock_bh(&sham.lock);\r\nif (!tctx->dd) {\r\nlist_for_each_entry(tmp, &sham.dev_list, list) {\r\ndd = tmp;\r\nbreak;\r\n}\r\ntctx->dd = dd;\r\n} else {\r\ndd = tctx->dd;\r\n}\r\nspin_unlock_bh(&sham.lock);\r\nctx->dd = dd;\r\nctx->flags = 0;\r\ndev_dbg(dd->dev, "init: digest size: %d\n",\r\ncrypto_ahash_digestsize(tfm));\r\nif (crypto_ahash_digestsize(tfm) == SHA1_DIGEST_SIZE)\r\nctx->flags |= BIT(FLAGS_SHA1);\r\nctx->bufcnt = 0;\r\nctx->digcnt = 0;\r\nctx->buflen = BUFLEN;\r\nif (tctx->flags & BIT(FLAGS_HMAC)) {\r\nstruct omap_sham_hmac_ctx *bctx = tctx->base;\r\nmemcpy(ctx->buffer, bctx->ipad, SHA1_MD5_BLOCK_SIZE);\r\nctx->bufcnt = SHA1_MD5_BLOCK_SIZE;\r\nctx->flags |= BIT(FLAGS_HMAC);\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_sham_update_req(struct omap_sham_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nint err;\r\ndev_dbg(dd->dev, "update_req: total: %u, digcnt: %d, finup: %d\n",\r\nctx->total, ctx->digcnt, (ctx->flags & BIT(FLAGS_FINUP)) != 0);\r\nif (ctx->flags & BIT(FLAGS_CPU))\r\nerr = omap_sham_update_cpu(dd);\r\nelse\r\nerr = omap_sham_update_dma_start(dd);\r\ndev_dbg(dd->dev, "update: err: %d, digcnt: %d\n", err, ctx->digcnt);\r\nreturn err;\r\n}\r\nstatic int omap_sham_final_req(struct omap_sham_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nint err = 0, use_dma = 1;\r\nif (ctx->bufcnt <= 64)\r\nuse_dma = 0;\r\nif (use_dma)\r\nerr = omap_sham_xmit_dma_map(dd, ctx, ctx->bufcnt, 1);\r\nelse\r\nerr = omap_sham_xmit_cpu(dd, ctx->buffer, ctx->bufcnt, 1);\r\nctx->bufcnt = 0;\r\ndev_dbg(dd->dev, "final_req: err: %d\n", err);\r\nreturn err;\r\n}\r\nstatic int omap_sham_finish_hmac(struct ahash_request *req)\r\n{\r\nstruct omap_sham_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct omap_sham_hmac_ctx *bctx = tctx->base;\r\nint bs = crypto_shash_blocksize(bctx->shash);\r\nint ds = crypto_shash_digestsize(bctx->shash);\r\nstruct {\r\nstruct shash_desc shash;\r\nchar ctx[crypto_shash_descsize(bctx->shash)];\r\n} desc;\r\ndesc.shash.tfm = bctx->shash;\r\ndesc.shash.flags = 0;\r\nreturn crypto_shash_init(&desc.shash) ?:\r\ncrypto_shash_update(&desc.shash, bctx->opad, bs) ?:\r\ncrypto_shash_finup(&desc.shash, req->result, ds, req->result);\r\n}\r\nstatic int omap_sham_finish(struct ahash_request *req)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nstruct omap_sham_dev *dd = ctx->dd;\r\nint err = 0;\r\nif (ctx->digcnt) {\r\nomap_sham_copy_ready_hash(req);\r\nif (ctx->flags & BIT(FLAGS_HMAC))\r\nerr = omap_sham_finish_hmac(req);\r\n}\r\ndev_dbg(dd->dev, "digcnt: %d, bufcnt: %d\n", ctx->digcnt, ctx->bufcnt);\r\nreturn err;\r\n}\r\nstatic void omap_sham_finish_req(struct ahash_request *req, int err)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nstruct omap_sham_dev *dd = ctx->dd;\r\nif (!err) {\r\nomap_sham_copy_hash(req, 1);\r\nif (test_bit(FLAGS_FINAL, &dd->flags))\r\nerr = omap_sham_finish(req);\r\n} else {\r\nctx->flags |= BIT(FLAGS_ERROR);\r\n}\r\ndd->flags &= ~(BIT(FLAGS_BUSY) | BIT(FLAGS_FINAL) | BIT(FLAGS_CPU) |\r\nBIT(FLAGS_DMA_READY) | BIT(FLAGS_OUTPUT_READY));\r\nclk_disable(dd->iclk);\r\nif (req->base.complete)\r\nreq->base.complete(&req->base, err);\r\ntasklet_schedule(&dd->done_task);\r\n}\r\nstatic int omap_sham_handle_queue(struct omap_sham_dev *dd,\r\nstruct ahash_request *req)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct omap_sham_reqctx *ctx;\r\nunsigned long flags;\r\nint err = 0, ret = 0;\r\nspin_lock_irqsave(&dd->lock, flags);\r\nif (req)\r\nret = ahash_enqueue_request(&dd->queue, req);\r\nif (test_bit(FLAGS_BUSY, &dd->flags)) {\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&dd->queue);\r\nasync_req = crypto_dequeue_request(&dd->queue);\r\nif (async_req)\r\nset_bit(FLAGS_BUSY, &dd->flags);\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nif (!async_req)\r\nreturn ret;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ahash_request_cast(async_req);\r\ndd->req = req;\r\nctx = ahash_request_ctx(req);\r\ndev_dbg(dd->dev, "handling new req, op: %lu, nbytes: %d\n",\r\nctx->op, req->nbytes);\r\nerr = omap_sham_hw_init(dd);\r\nif (err)\r\ngoto err1;\r\nomap_set_dma_dest_params(dd->dma_lch, 0,\r\nOMAP_DMA_AMODE_CONSTANT,\r\ndd->phys_base + SHA_REG_DIN(0), 0, 16);\r\nomap_set_dma_dest_burst_mode(dd->dma_lch,\r\nOMAP_DMA_DATA_BURST_16);\r\nomap_set_dma_src_burst_mode(dd->dma_lch,\r\nOMAP_DMA_DATA_BURST_4);\r\nif (ctx->digcnt)\r\nomap_sham_copy_hash(req, 0);\r\nif (ctx->op == OP_UPDATE) {\r\nerr = omap_sham_update_req(dd);\r\nif (err != -EINPROGRESS && (ctx->flags & BIT(FLAGS_FINUP)))\r\nerr = omap_sham_final_req(dd);\r\n} else if (ctx->op == OP_FINAL) {\r\nerr = omap_sham_final_req(dd);\r\n}\r\nerr1:\r\nif (err != -EINPROGRESS)\r\nomap_sham_finish_req(req, err);\r\ndev_dbg(dd->dev, "exit, err: %d\n", err);\r\nreturn ret;\r\n}\r\nstatic int omap_sham_enqueue(struct ahash_request *req, unsigned int op)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nstruct omap_sham_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct omap_sham_dev *dd = tctx->dd;\r\nctx->op = op;\r\nreturn omap_sham_handle_queue(dd, req);\r\n}\r\nstatic int omap_sham_update(struct ahash_request *req)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nif (!req->nbytes)\r\nreturn 0;\r\nctx->total = req->nbytes;\r\nctx->sg = req->src;\r\nctx->offset = 0;\r\nif (ctx->flags & BIT(FLAGS_FINUP)) {\r\nif ((ctx->digcnt + ctx->bufcnt + ctx->total) < 9) {\r\nomap_sham_append_sg(ctx);\r\nreturn 0;\r\n} else if (ctx->bufcnt + ctx->total <= SHA1_MD5_BLOCK_SIZE) {\r\nctx->flags |= BIT(FLAGS_CPU);\r\n}\r\n} else if (ctx->bufcnt + ctx->total < ctx->buflen) {\r\nomap_sham_append_sg(ctx);\r\nreturn 0;\r\n}\r\nreturn omap_sham_enqueue(req, OP_UPDATE);\r\n}\r\nstatic int omap_sham_shash_digest(struct crypto_shash *shash, u32 flags,\r\nconst u8 *data, unsigned int len, u8 *out)\r\n{\r\nstruct {\r\nstruct shash_desc shash;\r\nchar ctx[crypto_shash_descsize(shash)];\r\n} desc;\r\ndesc.shash.tfm = shash;\r\ndesc.shash.flags = flags & CRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_shash_digest(&desc.shash, data, len, out);\r\n}\r\nstatic int omap_sham_final_shash(struct ahash_request *req)\r\n{\r\nstruct omap_sham_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nreturn omap_sham_shash_digest(tctx->fallback, req->base.flags,\r\nctx->buffer, ctx->bufcnt, req->result);\r\n}\r\nstatic int omap_sham_final(struct ahash_request *req)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nctx->flags |= BIT(FLAGS_FINUP);\r\nif (ctx->flags & BIT(FLAGS_ERROR))\r\nreturn 0;\r\nif ((ctx->digcnt + ctx->bufcnt) < 9)\r\nreturn omap_sham_final_shash(req);\r\nelse if (ctx->bufcnt)\r\nreturn omap_sham_enqueue(req, OP_FINAL);\r\nreturn omap_sham_finish(req);\r\n}\r\nstatic int omap_sham_finup(struct ahash_request *req)\r\n{\r\nstruct omap_sham_reqctx *ctx = ahash_request_ctx(req);\r\nint err1, err2;\r\nctx->flags |= BIT(FLAGS_FINUP);\r\nerr1 = omap_sham_update(req);\r\nif (err1 == -EINPROGRESS || err1 == -EBUSY)\r\nreturn err1;\r\nerr2 = omap_sham_final(req);\r\nreturn err1 ?: err2;\r\n}\r\nstatic int omap_sham_digest(struct ahash_request *req)\r\n{\r\nreturn omap_sham_init(req) ?: omap_sham_finup(req);\r\n}\r\nstatic int omap_sham_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct omap_sham_ctx *tctx = crypto_ahash_ctx(tfm);\r\nstruct omap_sham_hmac_ctx *bctx = tctx->base;\r\nint bs = crypto_shash_blocksize(bctx->shash);\r\nint ds = crypto_shash_digestsize(bctx->shash);\r\nint err, i;\r\nerr = crypto_shash_setkey(tctx->fallback, key, keylen);\r\nif (err)\r\nreturn err;\r\nif (keylen > bs) {\r\nerr = omap_sham_shash_digest(bctx->shash,\r\ncrypto_shash_get_flags(bctx->shash),\r\nkey, keylen, bctx->ipad);\r\nif (err)\r\nreturn err;\r\nkeylen = ds;\r\n} else {\r\nmemcpy(bctx->ipad, key, keylen);\r\n}\r\nmemset(bctx->ipad + keylen, 0, bs - keylen);\r\nmemcpy(bctx->opad, bctx->ipad, bs);\r\nfor (i = 0; i < bs; i++) {\r\nbctx->ipad[i] ^= 0x36;\r\nbctx->opad[i] ^= 0x5c;\r\n}\r\nreturn err;\r\n}\r\nstatic int omap_sham_cra_init_alg(struct crypto_tfm *tfm, const char *alg_base)\r\n{\r\nstruct omap_sham_ctx *tctx = crypto_tfm_ctx(tfm);\r\nconst char *alg_name = crypto_tfm_alg_name(tfm);\r\ntctx->fallback = crypto_alloc_shash(alg_name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(tctx->fallback)) {\r\npr_err("omap-sham: fallback driver '%s' "\r\n"could not be loaded.\n", alg_name);\r\nreturn PTR_ERR(tctx->fallback);\r\n}\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct omap_sham_reqctx) + BUFLEN);\r\nif (alg_base) {\r\nstruct omap_sham_hmac_ctx *bctx = tctx->base;\r\ntctx->flags |= BIT(FLAGS_HMAC);\r\nbctx->shash = crypto_alloc_shash(alg_base, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(bctx->shash)) {\r\npr_err("omap-sham: base driver '%s' "\r\n"could not be loaded.\n", alg_base);\r\ncrypto_free_shash(tctx->fallback);\r\nreturn PTR_ERR(bctx->shash);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int omap_sham_cra_init(struct crypto_tfm *tfm)\r\n{\r\nreturn omap_sham_cra_init_alg(tfm, NULL);\r\n}\r\nstatic int omap_sham_cra_sha1_init(struct crypto_tfm *tfm)\r\n{\r\nreturn omap_sham_cra_init_alg(tfm, "sha1");\r\n}\r\nstatic int omap_sham_cra_md5_init(struct crypto_tfm *tfm)\r\n{\r\nreturn omap_sham_cra_init_alg(tfm, "md5");\r\n}\r\nstatic void omap_sham_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct omap_sham_ctx *tctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_shash(tctx->fallback);\r\ntctx->fallback = NULL;\r\nif (tctx->flags & BIT(FLAGS_HMAC)) {\r\nstruct omap_sham_hmac_ctx *bctx = tctx->base;\r\ncrypto_free_shash(bctx->shash);\r\n}\r\n}\r\nstatic void omap_sham_done_task(unsigned long data)\r\n{\r\nstruct omap_sham_dev *dd = (struct omap_sham_dev *)data;\r\nint err = 0;\r\nif (!test_bit(FLAGS_BUSY, &dd->flags)) {\r\nomap_sham_handle_queue(dd, NULL);\r\nreturn;\r\n}\r\nif (test_bit(FLAGS_CPU, &dd->flags)) {\r\nif (test_and_clear_bit(FLAGS_OUTPUT_READY, &dd->flags))\r\ngoto finish;\r\n} else if (test_bit(FLAGS_DMA_READY, &dd->flags)) {\r\nif (test_and_clear_bit(FLAGS_DMA_ACTIVE, &dd->flags)) {\r\nomap_sham_update_dma_stop(dd);\r\nif (dd->err) {\r\nerr = dd->err;\r\ngoto finish;\r\n}\r\n}\r\nif (test_and_clear_bit(FLAGS_OUTPUT_READY, &dd->flags)) {\r\nclear_bit(FLAGS_DMA_READY, &dd->flags);\r\nerr = omap_sham_update_dma_start(dd);\r\nif (err != -EINPROGRESS)\r\ngoto finish;\r\n}\r\n}\r\nreturn;\r\nfinish:\r\ndev_dbg(dd->dev, "update done: err: %d\n", err);\r\nomap_sham_finish_req(dd->req, err);\r\n}\r\nstatic irqreturn_t omap_sham_irq(int irq, void *dev_id)\r\n{\r\nstruct omap_sham_dev *dd = dev_id;\r\nif (unlikely(test_bit(FLAGS_FINAL, &dd->flags)))\r\nomap_sham_write_mask(dd, SHA_REG_CTRL, 0, SHA_REG_CTRL_LENGTH);\r\nomap_sham_write_mask(dd, SHA_REG_CTRL, SHA_REG_CTRL_OUTPUT_READY,\r\nSHA_REG_CTRL_OUTPUT_READY);\r\nomap_sham_read(dd, SHA_REG_CTRL);\r\nif (!test_bit(FLAGS_BUSY, &dd->flags)) {\r\ndev_warn(dd->dev, "Interrupt when no active requests.\n");\r\nreturn IRQ_HANDLED;\r\n}\r\nset_bit(FLAGS_OUTPUT_READY, &dd->flags);\r\ntasklet_schedule(&dd->done_task);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void omap_sham_dma_callback(int lch, u16 ch_status, void *data)\r\n{\r\nstruct omap_sham_dev *dd = data;\r\nif (ch_status != OMAP_DMA_BLOCK_IRQ) {\r\npr_err("omap-sham DMA error status: 0x%hx\n", ch_status);\r\ndd->err = -EIO;\r\nclear_bit(FLAGS_INIT, &dd->flags);\r\n}\r\nset_bit(FLAGS_DMA_READY, &dd->flags);\r\ntasklet_schedule(&dd->done_task);\r\n}\r\nstatic int omap_sham_dma_init(struct omap_sham_dev *dd)\r\n{\r\nint err;\r\ndd->dma_lch = -1;\r\nerr = omap_request_dma(dd->dma, dev_name(dd->dev),\r\nomap_sham_dma_callback, dd, &dd->dma_lch);\r\nif (err) {\r\ndev_err(dd->dev, "Unable to request DMA channel\n");\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void omap_sham_dma_cleanup(struct omap_sham_dev *dd)\r\n{\r\nif (dd->dma_lch >= 0) {\r\nomap_free_dma(dd->dma_lch);\r\ndd->dma_lch = -1;\r\n}\r\n}\r\nstatic int __devinit omap_sham_probe(struct platform_device *pdev)\r\n{\r\nstruct omap_sham_dev *dd;\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *res;\r\nint err, i, j;\r\ndd = kzalloc(sizeof(struct omap_sham_dev), GFP_KERNEL);\r\nif (dd == NULL) {\r\ndev_err(dev, "unable to alloc data struct.\n");\r\nerr = -ENOMEM;\r\ngoto data_err;\r\n}\r\ndd->dev = dev;\r\nplatform_set_drvdata(pdev, dd);\r\nINIT_LIST_HEAD(&dd->list);\r\nspin_lock_init(&dd->lock);\r\ntasklet_init(&dd->done_task, omap_sham_done_task, (unsigned long)dd);\r\ncrypto_init_queue(&dd->queue, OMAP_SHAM_QUEUE_LENGTH);\r\ndd->irq = -1;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!res) {\r\ndev_err(dev, "no MEM resource info\n");\r\nerr = -ENODEV;\r\ngoto res_err;\r\n}\r\ndd->phys_base = res->start;\r\nres = platform_get_resource(pdev, IORESOURCE_DMA, 0);\r\nif (!res) {\r\ndev_err(dev, "no DMA resource info\n");\r\nerr = -ENODEV;\r\ngoto res_err;\r\n}\r\ndd->dma = res->start;\r\ndd->irq = platform_get_irq(pdev, 0);\r\nif (dd->irq < 0) {\r\ndev_err(dev, "no IRQ resource info\n");\r\nerr = dd->irq;\r\ngoto res_err;\r\n}\r\nerr = request_irq(dd->irq, omap_sham_irq,\r\nIRQF_TRIGGER_LOW, dev_name(dev), dd);\r\nif (err) {\r\ndev_err(dev, "unable to request irq.\n");\r\ngoto res_err;\r\n}\r\nerr = omap_sham_dma_init(dd);\r\nif (err)\r\ngoto dma_err;\r\ndd->iclk = clk_get(dev, "ick");\r\nif (IS_ERR(dd->iclk)) {\r\ndev_err(dev, "clock intialization failed.\n");\r\nerr = PTR_ERR(dd->iclk);\r\ngoto clk_err;\r\n}\r\ndd->io_base = ioremap(dd->phys_base, SZ_4K);\r\nif (!dd->io_base) {\r\ndev_err(dev, "can't ioremap\n");\r\nerr = -ENOMEM;\r\ngoto io_err;\r\n}\r\nclk_enable(dd->iclk);\r\ndev_info(dev, "hw accel on OMAP rev %u.%u\n",\r\n(omap_sham_read(dd, SHA_REG_REV) & SHA_REG_REV_MAJOR) >> 4,\r\nomap_sham_read(dd, SHA_REG_REV) & SHA_REG_REV_MINOR);\r\nclk_disable(dd->iclk);\r\nspin_lock(&sham.lock);\r\nlist_add_tail(&dd->list, &sham.dev_list);\r\nspin_unlock(&sham.lock);\r\nfor (i = 0; i < ARRAY_SIZE(algs); i++) {\r\nerr = crypto_register_ahash(&algs[i]);\r\nif (err)\r\ngoto err_algs;\r\n}\r\nreturn 0;\r\nerr_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_ahash(&algs[j]);\r\niounmap(dd->io_base);\r\nio_err:\r\nclk_put(dd->iclk);\r\nclk_err:\r\nomap_sham_dma_cleanup(dd);\r\ndma_err:\r\nif (dd->irq >= 0)\r\nfree_irq(dd->irq, dd);\r\nres_err:\r\nkfree(dd);\r\ndd = NULL;\r\ndata_err:\r\ndev_err(dev, "initialization failed.\n");\r\nreturn err;\r\n}\r\nstatic int __devexit omap_sham_remove(struct platform_device *pdev)\r\n{\r\nstatic struct omap_sham_dev *dd;\r\nint i;\r\ndd = platform_get_drvdata(pdev);\r\nif (!dd)\r\nreturn -ENODEV;\r\nspin_lock(&sham.lock);\r\nlist_del(&dd->list);\r\nspin_unlock(&sham.lock);\r\nfor (i = 0; i < ARRAY_SIZE(algs); i++)\r\ncrypto_unregister_ahash(&algs[i]);\r\ntasklet_kill(&dd->done_task);\r\niounmap(dd->io_base);\r\nclk_put(dd->iclk);\r\nomap_sham_dma_cleanup(dd);\r\nif (dd->irq >= 0)\r\nfree_irq(dd->irq, dd);\r\nkfree(dd);\r\ndd = NULL;\r\nreturn 0;\r\n}\r\nstatic int __init omap_sham_mod_init(void)\r\n{\r\npr_info("loading %s driver\n", "omap-sham");\r\nif (!cpu_class_is_omap2() ||\r\n(omap_type() != OMAP2_DEVICE_TYPE_SEC &&\r\nomap_type() != OMAP2_DEVICE_TYPE_EMU)) {\r\npr_err("Unsupported cpu\n");\r\nreturn -ENODEV;\r\n}\r\nreturn platform_driver_register(&omap_sham_driver);\r\n}\r\nstatic void __exit omap_sham_mod_exit(void)\r\n{\r\nplatform_driver_unregister(&omap_sham_driver);\r\n}
