static inline void update_last_devid(u16 devid)\r\n{\r\nif (devid > amd_iommu_last_bdf)\r\namd_iommu_last_bdf = devid;\r\n}\r\nstatic inline unsigned long tbl_size(int entry_size)\r\n{\r\nunsigned shift = PAGE_SHIFT +\r\nget_order(((int)amd_iommu_last_bdf + 1) * entry_size);\r\nreturn 1UL << shift;\r\n}\r\nstatic u32 iommu_read_l1(struct amd_iommu *iommu, u16 l1, u8 address)\r\n{\r\nu32 val;\r\npci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16));\r\npci_read_config_dword(iommu->dev, 0xfc, &val);\r\nreturn val;\r\n}\r\nstatic void iommu_write_l1(struct amd_iommu *iommu, u16 l1, u8 address, u32 val)\r\n{\r\npci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16 | 1 << 31));\r\npci_write_config_dword(iommu->dev, 0xfc, val);\r\npci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16));\r\n}\r\nstatic u32 iommu_read_l2(struct amd_iommu *iommu, u8 address)\r\n{\r\nu32 val;\r\npci_write_config_dword(iommu->dev, 0xf0, address);\r\npci_read_config_dword(iommu->dev, 0xf4, &val);\r\nreturn val;\r\n}\r\nstatic void iommu_write_l2(struct amd_iommu *iommu, u8 address, u32 val)\r\n{\r\npci_write_config_dword(iommu->dev, 0xf0, (address | 1 << 8));\r\npci_write_config_dword(iommu->dev, 0xf4, val);\r\n}\r\nstatic void iommu_set_exclusion_range(struct amd_iommu *iommu)\r\n{\r\nu64 start = iommu->exclusion_start & PAGE_MASK;\r\nu64 limit = (start + iommu->exclusion_length) & PAGE_MASK;\r\nu64 entry;\r\nif (!iommu->exclusion_start)\r\nreturn;\r\nentry = start | MMIO_EXCL_ENABLE_MASK;\r\nmemcpy_toio(iommu->mmio_base + MMIO_EXCL_BASE_OFFSET,\r\n&entry, sizeof(entry));\r\nentry = limit;\r\nmemcpy_toio(iommu->mmio_base + MMIO_EXCL_LIMIT_OFFSET,\r\n&entry, sizeof(entry));\r\n}\r\nstatic void __init iommu_set_device_table(struct amd_iommu *iommu)\r\n{\r\nu64 entry;\r\nBUG_ON(iommu->mmio_base == NULL);\r\nentry = virt_to_phys(amd_iommu_dev_table);\r\nentry |= (dev_table_size >> 12) - 1;\r\nmemcpy_toio(iommu->mmio_base + MMIO_DEV_TABLE_OFFSET,\r\n&entry, sizeof(entry));\r\n}\r\nstatic void iommu_feature_enable(struct amd_iommu *iommu, u8 bit)\r\n{\r\nu32 ctrl;\r\nctrl = readl(iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\nctrl |= (1 << bit);\r\nwritel(ctrl, iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\n}\r\nstatic void iommu_feature_disable(struct amd_iommu *iommu, u8 bit)\r\n{\r\nu32 ctrl;\r\nctrl = readl(iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\nctrl &= ~(1 << bit);\r\nwritel(ctrl, iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\n}\r\nstatic void iommu_enable(struct amd_iommu *iommu)\r\n{\r\nstatic const char * const feat_str[] = {\r\n"PreF", "PPR", "X2APIC", "NX", "GT", "[5]",\r\n"IA", "GA", "HE", "PC", NULL\r\n};\r\nint i;\r\nprintk(KERN_INFO "AMD-Vi: Enabling IOMMU at %s cap 0x%hx",\r\ndev_name(&iommu->dev->dev), iommu->cap_ptr);\r\nif (iommu->cap & (1 << IOMMU_CAP_EFR)) {\r\nprintk(KERN_CONT " extended features: ");\r\nfor (i = 0; feat_str[i]; ++i)\r\nif (iommu_feature(iommu, (1ULL << i)))\r\nprintk(KERN_CONT " %s", feat_str[i]);\r\n}\r\nprintk(KERN_CONT "\n");\r\niommu_feature_enable(iommu, CONTROL_IOMMU_EN);\r\n}\r\nstatic void iommu_disable(struct amd_iommu *iommu)\r\n{\r\niommu_feature_disable(iommu, CONTROL_CMDBUF_EN);\r\niommu_feature_disable(iommu, CONTROL_EVT_INT_EN);\r\niommu_feature_disable(iommu, CONTROL_EVT_LOG_EN);\r\niommu_feature_disable(iommu, CONTROL_IOMMU_EN);\r\n}\r\nstatic u8 * __init iommu_map_mmio_space(u64 address)\r\n{\r\nu8 *ret;\r\nif (!request_mem_region(address, MMIO_REGION_LENGTH, "amd_iommu")) {\r\npr_err("AMD-Vi: Can not reserve memory region %llx for mmio\n",\r\naddress);\r\npr_err("AMD-Vi: This is a BIOS bug. Please contact your hardware vendor\n");\r\nreturn NULL;\r\n}\r\nret = ioremap_nocache(address, MMIO_REGION_LENGTH);\r\nif (ret != NULL)\r\nreturn ret;\r\nrelease_mem_region(address, MMIO_REGION_LENGTH);\r\nreturn NULL;\r\n}\r\nstatic void __init iommu_unmap_mmio_space(struct amd_iommu *iommu)\r\n{\r\nif (iommu->mmio_base)\r\niounmap(iommu->mmio_base);\r\nrelease_mem_region(iommu->mmio_phys, MMIO_REGION_LENGTH);\r\n}\r\nstatic inline int ivhd_entry_length(u8 *ivhd)\r\n{\r\nreturn 0x04 << (*ivhd >> 6);\r\n}\r\nstatic int __init find_last_devid_on_pci(int bus, int dev, int fn, int cap_ptr)\r\n{\r\nu32 cap;\r\ncap = read_pci_config(bus, dev, fn, cap_ptr+MMIO_RANGE_OFFSET);\r\nupdate_last_devid(calc_devid(MMIO_GET_BUS(cap), MMIO_GET_LD(cap)));\r\nreturn 0;\r\n}\r\nstatic int __init find_last_devid_from_ivhd(struct ivhd_header *h)\r\n{\r\nu8 *p = (void *)h, *end = (void *)h;\r\nstruct ivhd_entry *dev;\r\np += sizeof(*h);\r\nend += h->length;\r\nfind_last_devid_on_pci(PCI_BUS(h->devid),\r\nPCI_SLOT(h->devid),\r\nPCI_FUNC(h->devid),\r\nh->cap_ptr);\r\nwhile (p < end) {\r\ndev = (struct ivhd_entry *)p;\r\nswitch (dev->type) {\r\ncase IVHD_DEV_SELECT:\r\ncase IVHD_DEV_RANGE_END:\r\ncase IVHD_DEV_ALIAS:\r\ncase IVHD_DEV_EXT_SELECT:\r\nupdate_last_devid(dev->devid);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\np += ivhd_entry_length(p);\r\n}\r\nWARN_ON(p != end);\r\nreturn 0;\r\n}\r\nstatic int __init find_last_devid_acpi(struct acpi_table_header *table)\r\n{\r\nint i;\r\nu8 checksum = 0, *p = (u8 *)table, *end = (u8 *)table;\r\nstruct ivhd_header *h;\r\nfor (i = 0; i < table->length; ++i)\r\nchecksum += p[i];\r\nif (checksum != 0) {\r\namd_iommu_init_err = -ENODEV;\r\nreturn 0;\r\n}\r\np += IVRS_HEADER_LENGTH;\r\nend += table->length;\r\nwhile (p < end) {\r\nh = (struct ivhd_header *)p;\r\nswitch (h->type) {\r\ncase ACPI_IVHD_TYPE:\r\nfind_last_devid_from_ivhd(h);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\np += h->length;\r\n}\r\nWARN_ON(p != end);\r\nreturn 0;\r\n}\r\nstatic u8 * __init alloc_command_buffer(struct amd_iommu *iommu)\r\n{\r\nu8 *cmd_buf = (u8 *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(CMD_BUFFER_SIZE));\r\nif (cmd_buf == NULL)\r\nreturn NULL;\r\niommu->cmd_buf_size = CMD_BUFFER_SIZE | CMD_BUFFER_UNINITIALIZED;\r\nreturn cmd_buf;\r\n}\r\nvoid amd_iommu_reset_cmd_buffer(struct amd_iommu *iommu)\r\n{\r\niommu_feature_disable(iommu, CONTROL_CMDBUF_EN);\r\nwritel(0x00, iommu->mmio_base + MMIO_CMD_HEAD_OFFSET);\r\nwritel(0x00, iommu->mmio_base + MMIO_CMD_TAIL_OFFSET);\r\niommu_feature_enable(iommu, CONTROL_CMDBUF_EN);\r\n}\r\nstatic void iommu_enable_command_buffer(struct amd_iommu *iommu)\r\n{\r\nu64 entry;\r\nBUG_ON(iommu->cmd_buf == NULL);\r\nentry = (u64)virt_to_phys(iommu->cmd_buf);\r\nentry |= MMIO_CMD_SIZE_512;\r\nmemcpy_toio(iommu->mmio_base + MMIO_CMD_BUF_OFFSET,\r\n&entry, sizeof(entry));\r\namd_iommu_reset_cmd_buffer(iommu);\r\niommu->cmd_buf_size &= ~(CMD_BUFFER_UNINITIALIZED);\r\n}\r\nstatic void __init free_command_buffer(struct amd_iommu *iommu)\r\n{\r\nfree_pages((unsigned long)iommu->cmd_buf,\r\nget_order(iommu->cmd_buf_size & ~(CMD_BUFFER_UNINITIALIZED)));\r\n}\r\nstatic u8 * __init alloc_event_buffer(struct amd_iommu *iommu)\r\n{\r\niommu->evt_buf = (u8 *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(EVT_BUFFER_SIZE));\r\nif (iommu->evt_buf == NULL)\r\nreturn NULL;\r\niommu->evt_buf_size = EVT_BUFFER_SIZE;\r\nreturn iommu->evt_buf;\r\n}\r\nstatic void iommu_enable_event_buffer(struct amd_iommu *iommu)\r\n{\r\nu64 entry;\r\nBUG_ON(iommu->evt_buf == NULL);\r\nentry = (u64)virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;\r\nmemcpy_toio(iommu->mmio_base + MMIO_EVT_BUF_OFFSET,\r\n&entry, sizeof(entry));\r\nwritel(0x00, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);\r\nwritel(0x00, iommu->mmio_base + MMIO_EVT_TAIL_OFFSET);\r\niommu_feature_enable(iommu, CONTROL_EVT_LOG_EN);\r\n}\r\nstatic void __init free_event_buffer(struct amd_iommu *iommu)\r\n{\r\nfree_pages((unsigned long)iommu->evt_buf, get_order(EVT_BUFFER_SIZE));\r\n}\r\nstatic void set_dev_entry_bit(u16 devid, u8 bit)\r\n{\r\nint i = (bit >> 5) & 0x07;\r\nint _bit = bit & 0x1f;\r\namd_iommu_dev_table[devid].data[i] |= (1 << _bit);\r\n}\r\nstatic int get_dev_entry_bit(u16 devid, u8 bit)\r\n{\r\nint i = (bit >> 5) & 0x07;\r\nint _bit = bit & 0x1f;\r\nreturn (amd_iommu_dev_table[devid].data[i] & (1 << _bit)) >> _bit;\r\n}\r\nvoid amd_iommu_apply_erratum_63(u16 devid)\r\n{\r\nint sysmgt;\r\nsysmgt = get_dev_entry_bit(devid, DEV_ENTRY_SYSMGT1) |\r\n(get_dev_entry_bit(devid, DEV_ENTRY_SYSMGT2) << 1);\r\nif (sysmgt == 0x01)\r\nset_dev_entry_bit(devid, DEV_ENTRY_IW);\r\n}\r\nstatic void __init set_iommu_for_device(struct amd_iommu *iommu, u16 devid)\r\n{\r\namd_iommu_rlookup_table[devid] = iommu;\r\n}\r\nstatic void __init set_dev_entry_from_acpi(struct amd_iommu *iommu,\r\nu16 devid, u32 flags, u32 ext_flags)\r\n{\r\nif (flags & ACPI_DEVFLAG_INITPASS)\r\nset_dev_entry_bit(devid, DEV_ENTRY_INIT_PASS);\r\nif (flags & ACPI_DEVFLAG_EXTINT)\r\nset_dev_entry_bit(devid, DEV_ENTRY_EINT_PASS);\r\nif (flags & ACPI_DEVFLAG_NMI)\r\nset_dev_entry_bit(devid, DEV_ENTRY_NMI_PASS);\r\nif (flags & ACPI_DEVFLAG_SYSMGT1)\r\nset_dev_entry_bit(devid, DEV_ENTRY_SYSMGT1);\r\nif (flags & ACPI_DEVFLAG_SYSMGT2)\r\nset_dev_entry_bit(devid, DEV_ENTRY_SYSMGT2);\r\nif (flags & ACPI_DEVFLAG_LINT0)\r\nset_dev_entry_bit(devid, DEV_ENTRY_LINT0_PASS);\r\nif (flags & ACPI_DEVFLAG_LINT1)\r\nset_dev_entry_bit(devid, DEV_ENTRY_LINT1_PASS);\r\namd_iommu_apply_erratum_63(devid);\r\nset_iommu_for_device(iommu, devid);\r\n}\r\nstatic void __init set_device_exclusion_range(u16 devid, struct ivmd_header *m)\r\n{\r\nstruct amd_iommu *iommu = amd_iommu_rlookup_table[devid];\r\nif (!(m->flags & IVMD_FLAG_EXCL_RANGE))\r\nreturn;\r\nif (iommu) {\r\nset_dev_entry_bit(m->devid, DEV_ENTRY_EX);\r\niommu->exclusion_start = m->range_start;\r\niommu->exclusion_length = m->range_length;\r\n}\r\n}\r\nstatic void __init init_iommu_from_pci(struct amd_iommu *iommu)\r\n{\r\nint cap_ptr = iommu->cap_ptr;\r\nu32 range, misc, low, high;\r\nint i, j;\r\npci_read_config_dword(iommu->dev, cap_ptr + MMIO_CAP_HDR_OFFSET,\r\n&iommu->cap);\r\npci_read_config_dword(iommu->dev, cap_ptr + MMIO_RANGE_OFFSET,\r\n&range);\r\npci_read_config_dword(iommu->dev, cap_ptr + MMIO_MISC_OFFSET,\r\n&misc);\r\niommu->first_device = calc_devid(MMIO_GET_BUS(range),\r\nMMIO_GET_FD(range));\r\niommu->last_device = calc_devid(MMIO_GET_BUS(range),\r\nMMIO_GET_LD(range));\r\niommu->evt_msi_num = MMIO_MSI_NUM(misc);\r\nif (!(iommu->cap & (1 << IOMMU_CAP_IOTLB)))\r\namd_iommu_iotlb_sup = false;\r\nlow = readl(iommu->mmio_base + MMIO_EXT_FEATURES);\r\nhigh = readl(iommu->mmio_base + MMIO_EXT_FEATURES + 4);\r\niommu->features = ((u64)high << 32) | low;\r\nif (!is_rd890_iommu(iommu->dev))\r\nreturn;\r\npci_read_config_dword(iommu->dev, iommu->cap_ptr + 4,\r\n&iommu->stored_addr_lo);\r\npci_read_config_dword(iommu->dev, iommu->cap_ptr + 8,\r\n&iommu->stored_addr_hi);\r\niommu->stored_addr_lo &= ~1;\r\nfor (i = 0; i < 6; i++)\r\nfor (j = 0; j < 0x12; j++)\r\niommu->stored_l1[i][j] = iommu_read_l1(iommu, i, j);\r\nfor (i = 0; i < 0x83; i++)\r\niommu->stored_l2[i] = iommu_read_l2(iommu, i);\r\n}\r\nstatic void __init init_iommu_from_acpi(struct amd_iommu *iommu,\r\nstruct ivhd_header *h)\r\n{\r\nu8 *p = (u8 *)h;\r\nu8 *end = p, flags = 0;\r\nu16 devid = 0, devid_start = 0, devid_to = 0;\r\nu32 dev_i, ext_flags = 0;\r\nbool alias = false;\r\nstruct ivhd_entry *e;\r\niommu->acpi_flags = h->flags;\r\np += sizeof(struct ivhd_header);\r\nend += h->length;\r\nwhile (p < end) {\r\ne = (struct ivhd_entry *)p;\r\nswitch (e->type) {\r\ncase IVHD_DEV_ALL:\r\nDUMP_printk(" DEV_ALL\t\t\t first devid: %02x:%02x.%x"\r\n" last device %02x:%02x.%x flags: %02x\n",\r\nPCI_BUS(iommu->first_device),\r\nPCI_SLOT(iommu->first_device),\r\nPCI_FUNC(iommu->first_device),\r\nPCI_BUS(iommu->last_device),\r\nPCI_SLOT(iommu->last_device),\r\nPCI_FUNC(iommu->last_device),\r\ne->flags);\r\nfor (dev_i = iommu->first_device;\r\ndev_i <= iommu->last_device; ++dev_i)\r\nset_dev_entry_from_acpi(iommu, dev_i,\r\ne->flags, 0);\r\nbreak;\r\ncase IVHD_DEV_SELECT:\r\nDUMP_printk(" DEV_SELECT\t\t\t devid: %02x:%02x.%x "\r\n"flags: %02x\n",\r\nPCI_BUS(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags);\r\ndevid = e->devid;\r\nset_dev_entry_from_acpi(iommu, devid, e->flags, 0);\r\nbreak;\r\ncase IVHD_DEV_SELECT_RANGE_START:\r\nDUMP_printk(" DEV_SELECT_RANGE_START\t "\r\n"devid: %02x:%02x.%x flags: %02x\n",\r\nPCI_BUS(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags);\r\ndevid_start = e->devid;\r\nflags = e->flags;\r\next_flags = 0;\r\nalias = false;\r\nbreak;\r\ncase IVHD_DEV_ALIAS:\r\nDUMP_printk(" DEV_ALIAS\t\t\t devid: %02x:%02x.%x "\r\n"flags: %02x devid_to: %02x:%02x.%x\n",\r\nPCI_BUS(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags,\r\nPCI_BUS(e->ext >> 8),\r\nPCI_SLOT(e->ext >> 8),\r\nPCI_FUNC(e->ext >> 8));\r\ndevid = e->devid;\r\ndevid_to = e->ext >> 8;\r\nset_dev_entry_from_acpi(iommu, devid , e->flags, 0);\r\nset_dev_entry_from_acpi(iommu, devid_to, e->flags, 0);\r\namd_iommu_alias_table[devid] = devid_to;\r\nbreak;\r\ncase IVHD_DEV_ALIAS_RANGE:\r\nDUMP_printk(" DEV_ALIAS_RANGE\t\t "\r\n"devid: %02x:%02x.%x flags: %02x "\r\n"devid_to: %02x:%02x.%x\n",\r\nPCI_BUS(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags,\r\nPCI_BUS(e->ext >> 8),\r\nPCI_SLOT(e->ext >> 8),\r\nPCI_FUNC(e->ext >> 8));\r\ndevid_start = e->devid;\r\nflags = e->flags;\r\ndevid_to = e->ext >> 8;\r\next_flags = 0;\r\nalias = true;\r\nbreak;\r\ncase IVHD_DEV_EXT_SELECT:\r\nDUMP_printk(" DEV_EXT_SELECT\t\t devid: %02x:%02x.%x "\r\n"flags: %02x ext: %08x\n",\r\nPCI_BUS(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags, e->ext);\r\ndevid = e->devid;\r\nset_dev_entry_from_acpi(iommu, devid, e->flags,\r\ne->ext);\r\nbreak;\r\ncase IVHD_DEV_EXT_SELECT_RANGE:\r\nDUMP_printk(" DEV_EXT_SELECT_RANGE\t devid: "\r\n"%02x:%02x.%x flags: %02x ext: %08x\n",\r\nPCI_BUS(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags, e->ext);\r\ndevid_start = e->devid;\r\nflags = e->flags;\r\next_flags = e->ext;\r\nalias = false;\r\nbreak;\r\ncase IVHD_DEV_RANGE_END:\r\nDUMP_printk(" DEV_RANGE_END\t\t devid: %02x:%02x.%x\n",\r\nPCI_BUS(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid));\r\ndevid = e->devid;\r\nfor (dev_i = devid_start; dev_i <= devid; ++dev_i) {\r\nif (alias) {\r\namd_iommu_alias_table[dev_i] = devid_to;\r\nset_dev_entry_from_acpi(iommu,\r\ndevid_to, flags, ext_flags);\r\n}\r\nset_dev_entry_from_acpi(iommu, dev_i,\r\nflags, ext_flags);\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\np += ivhd_entry_length(p);\r\n}\r\n}\r\nstatic int __init init_iommu_devices(struct amd_iommu *iommu)\r\n{\r\nu32 i;\r\nfor (i = iommu->first_device; i <= iommu->last_device; ++i)\r\nset_iommu_for_device(iommu, i);\r\nreturn 0;\r\n}\r\nstatic void __init free_iommu_one(struct amd_iommu *iommu)\r\n{\r\nfree_command_buffer(iommu);\r\nfree_event_buffer(iommu);\r\niommu_unmap_mmio_space(iommu);\r\n}\r\nstatic void __init free_iommu_all(void)\r\n{\r\nstruct amd_iommu *iommu, *next;\r\nfor_each_iommu_safe(iommu, next) {\r\nlist_del(&iommu->list);\r\nfree_iommu_one(iommu);\r\nkfree(iommu);\r\n}\r\n}\r\nstatic int __init init_iommu_one(struct amd_iommu *iommu, struct ivhd_header *h)\r\n{\r\nspin_lock_init(&iommu->lock);\r\nlist_add_tail(&iommu->list, &amd_iommu_list);\r\niommu->index = amd_iommus_present++;\r\nif (unlikely(iommu->index >= MAX_IOMMUS)) {\r\nWARN(1, "AMD-Vi: System has more IOMMUs than supported by this driver\n");\r\nreturn -ENOSYS;\r\n}\r\namd_iommus[iommu->index] = iommu;\r\niommu->dev = pci_get_bus_and_slot(PCI_BUS(h->devid), h->devid & 0xff);\r\nif (!iommu->dev)\r\nreturn 1;\r\niommu->cap_ptr = h->cap_ptr;\r\niommu->pci_seg = h->pci_seg;\r\niommu->mmio_phys = h->mmio_phys;\r\niommu->mmio_base = iommu_map_mmio_space(h->mmio_phys);\r\nif (!iommu->mmio_base)\r\nreturn -ENOMEM;\r\niommu->cmd_buf = alloc_command_buffer(iommu);\r\nif (!iommu->cmd_buf)\r\nreturn -ENOMEM;\r\niommu->evt_buf = alloc_event_buffer(iommu);\r\nif (!iommu->evt_buf)\r\nreturn -ENOMEM;\r\niommu->int_enabled = false;\r\ninit_iommu_from_pci(iommu);\r\ninit_iommu_from_acpi(iommu, h);\r\ninit_iommu_devices(iommu);\r\nif (iommu->cap & (1UL << IOMMU_CAP_NPCACHE))\r\namd_iommu_np_cache = true;\r\nreturn pci_enable_device(iommu->dev);\r\n}\r\nstatic int __init init_iommu_all(struct acpi_table_header *table)\r\n{\r\nu8 *p = (u8 *)table, *end = (u8 *)table;\r\nstruct ivhd_header *h;\r\nstruct amd_iommu *iommu;\r\nint ret;\r\nend += table->length;\r\np += IVRS_HEADER_LENGTH;\r\nwhile (p < end) {\r\nh = (struct ivhd_header *)p;\r\nswitch (*p) {\r\ncase ACPI_IVHD_TYPE:\r\nDUMP_printk("device: %02x:%02x.%01x cap: %04x "\r\n"seg: %d flags: %01x info %04x\n",\r\nPCI_BUS(h->devid), PCI_SLOT(h->devid),\r\nPCI_FUNC(h->devid), h->cap_ptr,\r\nh->pci_seg, h->flags, h->info);\r\nDUMP_printk(" mmio-addr: %016llx\n",\r\nh->mmio_phys);\r\niommu = kzalloc(sizeof(struct amd_iommu), GFP_KERNEL);\r\nif (iommu == NULL) {\r\namd_iommu_init_err = -ENOMEM;\r\nreturn 0;\r\n}\r\nret = init_iommu_one(iommu, h);\r\nif (ret) {\r\namd_iommu_init_err = ret;\r\nreturn 0;\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\np += h->length;\r\n}\r\nWARN_ON(p != end);\r\nreturn 0;\r\n}\r\nstatic int iommu_setup_msi(struct amd_iommu *iommu)\r\n{\r\nint r;\r\nif (pci_enable_msi(iommu->dev))\r\nreturn 1;\r\nr = request_threaded_irq(iommu->dev->irq,\r\namd_iommu_int_handler,\r\namd_iommu_int_thread,\r\n0, "AMD-Vi",\r\niommu->dev);\r\nif (r) {\r\npci_disable_msi(iommu->dev);\r\nreturn 1;\r\n}\r\niommu->int_enabled = true;\r\niommu_feature_enable(iommu, CONTROL_EVT_INT_EN);\r\nreturn 0;\r\n}\r\nstatic int iommu_init_msi(struct amd_iommu *iommu)\r\n{\r\nif (iommu->int_enabled)\r\nreturn 0;\r\nif (pci_find_capability(iommu->dev, PCI_CAP_ID_MSI))\r\nreturn iommu_setup_msi(iommu);\r\nreturn 1;\r\n}\r\nstatic void __init free_unity_maps(void)\r\n{\r\nstruct unity_map_entry *entry, *next;\r\nlist_for_each_entry_safe(entry, next, &amd_iommu_unity_map, list) {\r\nlist_del(&entry->list);\r\nkfree(entry);\r\n}\r\n}\r\nstatic int __init init_exclusion_range(struct ivmd_header *m)\r\n{\r\nint i;\r\nswitch (m->type) {\r\ncase ACPI_IVMD_TYPE:\r\nset_device_exclusion_range(m->devid, m);\r\nbreak;\r\ncase ACPI_IVMD_TYPE_ALL:\r\nfor (i = 0; i <= amd_iommu_last_bdf; ++i)\r\nset_device_exclusion_range(i, m);\r\nbreak;\r\ncase ACPI_IVMD_TYPE_RANGE:\r\nfor (i = m->devid; i <= m->aux; ++i)\r\nset_device_exclusion_range(i, m);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init init_unity_map_range(struct ivmd_header *m)\r\n{\r\nstruct unity_map_entry *e = 0;\r\nchar *s;\r\ne = kzalloc(sizeof(*e), GFP_KERNEL);\r\nif (e == NULL)\r\nreturn -ENOMEM;\r\nswitch (m->type) {\r\ndefault:\r\nkfree(e);\r\nreturn 0;\r\ncase ACPI_IVMD_TYPE:\r\ns = "IVMD_TYPEi\t\t\t";\r\ne->devid_start = e->devid_end = m->devid;\r\nbreak;\r\ncase ACPI_IVMD_TYPE_ALL:\r\ns = "IVMD_TYPE_ALL\t\t";\r\ne->devid_start = 0;\r\ne->devid_end = amd_iommu_last_bdf;\r\nbreak;\r\ncase ACPI_IVMD_TYPE_RANGE:\r\ns = "IVMD_TYPE_RANGE\t\t";\r\ne->devid_start = m->devid;\r\ne->devid_end = m->aux;\r\nbreak;\r\n}\r\ne->address_start = PAGE_ALIGN(m->range_start);\r\ne->address_end = e->address_start + PAGE_ALIGN(m->range_length);\r\ne->prot = m->flags >> 1;\r\nDUMP_printk("%s devid_start: %02x:%02x.%x devid_end: %02x:%02x.%x"\r\n" range_start: %016llx range_end: %016llx flags: %x\n", s,\r\nPCI_BUS(e->devid_start), PCI_SLOT(e->devid_start),\r\nPCI_FUNC(e->devid_start), PCI_BUS(e->devid_end),\r\nPCI_SLOT(e->devid_end), PCI_FUNC(e->devid_end),\r\ne->address_start, e->address_end, m->flags);\r\nlist_add_tail(&e->list, &amd_iommu_unity_map);\r\nreturn 0;\r\n}\r\nstatic int __init init_memory_definitions(struct acpi_table_header *table)\r\n{\r\nu8 *p = (u8 *)table, *end = (u8 *)table;\r\nstruct ivmd_header *m;\r\nend += table->length;\r\np += IVRS_HEADER_LENGTH;\r\nwhile (p < end) {\r\nm = (struct ivmd_header *)p;\r\nif (m->flags & IVMD_FLAG_EXCL_RANGE)\r\ninit_exclusion_range(m);\r\nelse if (m->flags & IVMD_FLAG_UNITY_MAP)\r\ninit_unity_map_range(m);\r\np += m->length;\r\n}\r\nreturn 0;\r\n}\r\nstatic void init_device_table(void)\r\n{\r\nu32 devid;\r\nfor (devid = 0; devid <= amd_iommu_last_bdf; ++devid) {\r\nset_dev_entry_bit(devid, DEV_ENTRY_VALID);\r\nset_dev_entry_bit(devid, DEV_ENTRY_TRANSLATION);\r\n}\r\n}\r\nstatic void iommu_init_flags(struct amd_iommu *iommu)\r\n{\r\niommu->acpi_flags & IVHD_FLAG_HT_TUN_EN_MASK ?\r\niommu_feature_enable(iommu, CONTROL_HT_TUN_EN) :\r\niommu_feature_disable(iommu, CONTROL_HT_TUN_EN);\r\niommu->acpi_flags & IVHD_FLAG_PASSPW_EN_MASK ?\r\niommu_feature_enable(iommu, CONTROL_PASSPW_EN) :\r\niommu_feature_disable(iommu, CONTROL_PASSPW_EN);\r\niommu->acpi_flags & IVHD_FLAG_RESPASSPW_EN_MASK ?\r\niommu_feature_enable(iommu, CONTROL_RESPASSPW_EN) :\r\niommu_feature_disable(iommu, CONTROL_RESPASSPW_EN);\r\niommu->acpi_flags & IVHD_FLAG_ISOC_EN_MASK ?\r\niommu_feature_enable(iommu, CONTROL_ISOC_EN) :\r\niommu_feature_disable(iommu, CONTROL_ISOC_EN);\r\niommu_feature_enable(iommu, CONTROL_COHERENT_EN);\r\n}\r\nstatic void iommu_apply_resume_quirks(struct amd_iommu *iommu)\r\n{\r\nint i, j;\r\nu32 ioc_feature_control;\r\nstruct pci_dev *pdev = NULL;\r\nif (!is_rd890_iommu(iommu->dev))\r\nreturn;\r\npdev = pci_get_bus_and_slot(iommu->dev->bus->number, PCI_DEVFN(0, 0));\r\nif (!pdev)\r\nreturn;\r\npci_write_config_dword(pdev, 0x60, 0x75 | (1 << 7));\r\npci_read_config_dword(pdev, 0x64, &ioc_feature_control);\r\nif (!(ioc_feature_control & 0x1))\r\npci_write_config_dword(pdev, 0x64, ioc_feature_control | 1);\r\npci_dev_put(pdev);\r\npci_write_config_dword(iommu->dev, iommu->cap_ptr + 4,\r\niommu->stored_addr_lo);\r\npci_write_config_dword(iommu->dev, iommu->cap_ptr + 8,\r\niommu->stored_addr_hi);\r\nfor (i = 0; i < 6; i++)\r\nfor (j = 0; j < 0x12; j++)\r\niommu_write_l1(iommu, i, j, iommu->stored_l1[i][j]);\r\nfor (i = 0; i < 0x83; i++)\r\niommu_write_l2(iommu, i, iommu->stored_l2[i]);\r\npci_write_config_dword(iommu->dev, iommu->cap_ptr + 4,\r\niommu->stored_addr_lo | 1);\r\n}\r\nstatic void enable_iommus(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nfor_each_iommu(iommu) {\r\niommu_disable(iommu);\r\niommu_init_flags(iommu);\r\niommu_set_device_table(iommu);\r\niommu_enable_command_buffer(iommu);\r\niommu_enable_event_buffer(iommu);\r\niommu_set_exclusion_range(iommu);\r\niommu_init_msi(iommu);\r\niommu_enable(iommu);\r\niommu_flush_all_caches(iommu);\r\n}\r\n}\r\nstatic void disable_iommus(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nfor_each_iommu(iommu)\r\niommu_disable(iommu);\r\n}\r\nstatic void amd_iommu_resume(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nfor_each_iommu(iommu)\r\niommu_apply_resume_quirks(iommu);\r\nenable_iommus();\r\nfor_each_iommu(iommu)\r\niommu_flush_all_caches(iommu);\r\n}\r\nstatic int amd_iommu_suspend(void)\r\n{\r\ndisable_iommus();\r\nreturn 0;\r\n}\r\nstatic int __init amd_iommu_init(void)\r\n{\r\nint i, ret = 0;\r\nif (acpi_table_parse("IVRS", find_last_devid_acpi) != 0)\r\nreturn -ENODEV;\r\nret = amd_iommu_init_err;\r\nif (ret)\r\ngoto out;\r\ndev_table_size = tbl_size(DEV_TABLE_ENTRY_SIZE);\r\nalias_table_size = tbl_size(ALIAS_TABLE_ENTRY_SIZE);\r\nrlookup_table_size = tbl_size(RLOOKUP_TABLE_ENTRY_SIZE);\r\nret = -ENOMEM;\r\namd_iommu_dev_table = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(dev_table_size));\r\nif (amd_iommu_dev_table == NULL)\r\ngoto out;\r\namd_iommu_alias_table = (void *)__get_free_pages(GFP_KERNEL,\r\nget_order(alias_table_size));\r\nif (amd_iommu_alias_table == NULL)\r\ngoto free;\r\namd_iommu_rlookup_table = (void *)__get_free_pages(\r\nGFP_KERNEL | __GFP_ZERO,\r\nget_order(rlookup_table_size));\r\nif (amd_iommu_rlookup_table == NULL)\r\ngoto free;\r\namd_iommu_pd_alloc_bitmap = (void *)__get_free_pages(\r\nGFP_KERNEL | __GFP_ZERO,\r\nget_order(MAX_DOMAIN_ID/8));\r\nif (amd_iommu_pd_alloc_bitmap == NULL)\r\ngoto free;\r\ninit_device_table();\r\nfor (i = 0; i <= amd_iommu_last_bdf; ++i)\r\namd_iommu_alias_table[i] = i;\r\namd_iommu_pd_alloc_bitmap[0] = 1;\r\nspin_lock_init(&amd_iommu_pd_lock);\r\nret = -ENODEV;\r\nif (acpi_table_parse("IVRS", init_iommu_all) != 0)\r\ngoto free;\r\nif (amd_iommu_init_err) {\r\nret = amd_iommu_init_err;\r\ngoto free;\r\n}\r\nif (acpi_table_parse("IVRS", init_memory_definitions) != 0)\r\ngoto free;\r\nif (amd_iommu_init_err) {\r\nret = amd_iommu_init_err;\r\ngoto free;\r\n}\r\nret = amd_iommu_init_devices();\r\nif (ret)\r\ngoto free;\r\nenable_iommus();\r\nif (iommu_pass_through)\r\nret = amd_iommu_init_passthrough();\r\nelse\r\nret = amd_iommu_init_dma_ops();\r\nif (ret)\r\ngoto free_disable;\r\namd_iommu_init_api();\r\namd_iommu_init_notifier();\r\nregister_syscore_ops(&amd_iommu_syscore_ops);\r\nif (iommu_pass_through)\r\ngoto out;\r\nif (amd_iommu_unmap_flush)\r\nprintk(KERN_INFO "AMD-Vi: IO/TLB flush on unmap enabled\n");\r\nelse\r\nprintk(KERN_INFO "AMD-Vi: Lazy IO/TLB flushing enabled\n");\r\nx86_platform.iommu_shutdown = disable_iommus;\r\nout:\r\nreturn ret;\r\nfree_disable:\r\ndisable_iommus();\r\nfree:\r\namd_iommu_uninit_devices();\r\nfree_pages((unsigned long)amd_iommu_pd_alloc_bitmap,\r\nget_order(MAX_DOMAIN_ID/8));\r\nfree_pages((unsigned long)amd_iommu_rlookup_table,\r\nget_order(rlookup_table_size));\r\nfree_pages((unsigned long)amd_iommu_alias_table,\r\nget_order(alias_table_size));\r\nfree_pages((unsigned long)amd_iommu_dev_table,\r\nget_order(dev_table_size));\r\nfree_iommu_all();\r\nfree_unity_maps();\r\n#ifdef CONFIG_GART_IOMMU\r\ngart_iommu_init();\r\n#endif\r\ngoto out;\r\n}\r\nstatic int __init early_amd_iommu_detect(struct acpi_table_header *table)\r\n{\r\nreturn 0;\r\n}\r\nint __init amd_iommu_detect(void)\r\n{\r\nif (no_iommu || (iommu_detected && !gart_iommu_aperture))\r\nreturn -ENODEV;\r\nif (amd_iommu_disabled)\r\nreturn -ENODEV;\r\nif (acpi_table_parse("IVRS", early_amd_iommu_detect) == 0) {\r\niommu_detected = 1;\r\namd_iommu_detected = 1;\r\nx86_init.iommu.iommu_init = amd_iommu_init;\r\npci_request_acs();\r\nreturn 1;\r\n}\r\nreturn -ENODEV;\r\n}\r\nstatic int __init parse_amd_iommu_dump(char *str)\r\n{\r\namd_iommu_dump = true;\r\nreturn 1;\r\n}\r\nstatic int __init parse_amd_iommu_options(char *str)\r\n{\r\nfor (; *str; ++str) {\r\nif (strncmp(str, "fullflush", 9) == 0)\r\namd_iommu_unmap_flush = true;\r\nif (strncmp(str, "off", 3) == 0)\r\namd_iommu_disabled = true;\r\n}\r\nreturn 1;\r\n}
