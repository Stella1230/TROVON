void *ipz_qpageit_get_inc(struct ipz_queue *queue)\r\n{\r\nvoid *ret = ipz_qeit_get(queue);\r\nqueue->current_q_offset += queue->pagesize;\r\nif (queue->current_q_offset > queue->queue_length) {\r\nqueue->current_q_offset -= queue->pagesize;\r\nret = NULL;\r\n}\r\nif (((u64)ret) % queue->pagesize) {\r\nehca_gen_err("ERROR!! not at PAGE-Boundary");\r\nreturn NULL;\r\n}\r\nreturn ret;\r\n}\r\nvoid *ipz_qeit_eq_get_inc(struct ipz_queue *queue)\r\n{\r\nvoid *ret = ipz_qeit_get(queue);\r\nu64 last_entry_in_q = queue->queue_length - queue->qe_size;\r\nqueue->current_q_offset += queue->qe_size;\r\nif (queue->current_q_offset > last_entry_in_q) {\r\nqueue->current_q_offset = 0;\r\nqueue->toggle_state = (~queue->toggle_state) & 1;\r\n}\r\nreturn ret;\r\n}\r\nint ipz_queue_abs_to_offset(struct ipz_queue *queue, u64 addr, u64 *q_offset)\r\n{\r\nint i;\r\nfor (i = 0; i < queue->queue_length / queue->pagesize; i++) {\r\nu64 page = (u64)virt_to_abs(queue->queue_pages[i]);\r\nif (addr >= page && addr < page + queue->pagesize) {\r\n*q_offset = addr - page + i * queue->pagesize;\r\nreturn 0;\r\n}\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic int alloc_queue_pages(struct ipz_queue *queue, const u32 nr_of_pages)\r\n{\r\nint k, f = 0;\r\nu8 *kpage;\r\nwhile (f < nr_of_pages) {\r\nkpage = (u8 *)get_zeroed_page(GFP_KERNEL);\r\nif (!kpage)\r\ngoto out;\r\nfor (k = 0; k < PAGES_PER_KPAGE && f < nr_of_pages; k++) {\r\nqueue->queue_pages[f] = (struct ipz_page *)kpage;\r\nkpage += EHCA_PAGESIZE;\r\nf++;\r\n}\r\n}\r\nreturn 1;\r\nout:\r\nfor (f = 0; f < nr_of_pages && queue->queue_pages[f];\r\nf += PAGES_PER_KPAGE)\r\nfree_page((unsigned long)(queue->queue_pages)[f]);\r\nreturn 0;\r\n}\r\nstatic int alloc_small_queue_page(struct ipz_queue *queue, struct ehca_pd *pd)\r\n{\r\nint order = ilog2(queue->pagesize) - 9;\r\nstruct ipz_small_queue_page *page;\r\nunsigned long bit;\r\nmutex_lock(&pd->lock);\r\nif (!list_empty(&pd->free[order]))\r\npage = list_entry(pd->free[order].next,\r\nstruct ipz_small_queue_page, list);\r\nelse {\r\npage = kmem_cache_zalloc(small_qp_cache, GFP_KERNEL);\r\nif (!page)\r\ngoto out;\r\npage->page = get_zeroed_page(GFP_KERNEL);\r\nif (!page->page) {\r\nkmem_cache_free(small_qp_cache, page);\r\ngoto out;\r\n}\r\nlist_add(&page->list, &pd->free[order]);\r\n}\r\nbit = find_first_zero_bit(page->bitmap, IPZ_SPAGE_PER_KPAGE >> order);\r\n__set_bit(bit, page->bitmap);\r\npage->fill++;\r\nif (page->fill == IPZ_SPAGE_PER_KPAGE >> order)\r\nlist_move(&page->list, &pd->full[order]);\r\nmutex_unlock(&pd->lock);\r\nqueue->queue_pages[0] = (void *)(page->page | (bit << (order + 9)));\r\nqueue->small_page = page;\r\nqueue->offset = bit << (order + 9);\r\nreturn 1;\r\nout:\r\nehca_err(pd->ib_pd.device, "failed to allocate small queue page");\r\nmutex_unlock(&pd->lock);\r\nreturn 0;\r\n}\r\nstatic void free_small_queue_page(struct ipz_queue *queue, struct ehca_pd *pd)\r\n{\r\nint order = ilog2(queue->pagesize) - 9;\r\nstruct ipz_small_queue_page *page = queue->small_page;\r\nunsigned long bit;\r\nint free_page = 0;\r\nbit = ((unsigned long)queue->queue_pages[0] & ~PAGE_MASK)\r\n>> (order + 9);\r\nmutex_lock(&pd->lock);\r\n__clear_bit(bit, page->bitmap);\r\npage->fill--;\r\nif (page->fill == 0) {\r\nlist_del(&page->list);\r\nfree_page = 1;\r\n}\r\nif (page->fill == (IPZ_SPAGE_PER_KPAGE >> order) - 1)\r\nlist_move_tail(&page->list, &pd->free[order]);\r\nmutex_unlock(&pd->lock);\r\nif (free_page) {\r\nfree_page(page->page);\r\nkmem_cache_free(small_qp_cache, page);\r\n}\r\n}\r\nint ipz_queue_ctor(struct ehca_pd *pd, struct ipz_queue *queue,\r\nconst u32 nr_of_pages, const u32 pagesize,\r\nconst u32 qe_size, const u32 nr_of_sg,\r\nint is_small)\r\n{\r\nif (pagesize > PAGE_SIZE) {\r\nehca_gen_err("FATAL ERROR: pagesize=%x "\r\n"is greater than kernel page size", pagesize);\r\nreturn 0;\r\n}\r\nqueue->queue_length = nr_of_pages * pagesize;\r\nqueue->pagesize = pagesize;\r\nqueue->qe_size = qe_size;\r\nqueue->act_nr_of_sg = nr_of_sg;\r\nqueue->current_q_offset = 0;\r\nqueue->toggle_state = 1;\r\nqueue->small_page = NULL;\r\nqueue->queue_pages = kzalloc(nr_of_pages * sizeof(void *), GFP_KERNEL);\r\nif (!queue->queue_pages) {\r\nqueue->queue_pages = vzalloc(nr_of_pages * sizeof(void *));\r\nif (!queue->queue_pages) {\r\nehca_gen_err("Couldn't allocate queue page list");\r\nreturn 0;\r\n}\r\n}\r\nif (is_small) {\r\nif (!alloc_small_queue_page(queue, pd))\r\ngoto ipz_queue_ctor_exit0;\r\n} else\r\nif (!alloc_queue_pages(queue, nr_of_pages))\r\ngoto ipz_queue_ctor_exit0;\r\nreturn 1;\r\nipz_queue_ctor_exit0:\r\nehca_gen_err("Couldn't alloc pages queue=%p "\r\n"nr_of_pages=%x", queue, nr_of_pages);\r\nif (is_vmalloc_addr(queue->queue_pages))\r\nvfree(queue->queue_pages);\r\nelse\r\nkfree(queue->queue_pages);\r\nreturn 0;\r\n}\r\nint ipz_queue_dtor(struct ehca_pd *pd, struct ipz_queue *queue)\r\n{\r\nint i, nr_pages;\r\nif (!queue || !queue->queue_pages) {\r\nehca_gen_dbg("queue or queue_pages is NULL");\r\nreturn 0;\r\n}\r\nif (queue->small_page)\r\nfree_small_queue_page(queue, pd);\r\nelse {\r\nnr_pages = queue->queue_length / queue->pagesize;\r\nfor (i = 0; i < nr_pages; i += PAGES_PER_KPAGE)\r\nfree_page((unsigned long)queue->queue_pages[i]);\r\n}\r\nif (is_vmalloc_addr(queue->queue_pages))\r\nvfree(queue->queue_pages);\r\nelse\r\nkfree(queue->queue_pages);\r\nreturn 1;\r\n}\r\nint ehca_init_small_qp_cache(void)\r\n{\r\nsmall_qp_cache = kmem_cache_create("ehca_cache_small_qp",\r\nsizeof(struct ipz_small_queue_page),\r\n0, SLAB_HWCACHE_ALIGN, NULL);\r\nif (!small_qp_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid ehca_cleanup_small_qp_cache(void)\r\n{\r\nkmem_cache_destroy(small_qp_cache);\r\n}
