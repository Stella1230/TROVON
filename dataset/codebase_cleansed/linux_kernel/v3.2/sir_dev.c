static int sirdev_tx_complete_fsm(struct sir_dev *dev)\r\n{\r\nstruct sir_fsm *fsm = &dev->fsm;\r\nunsigned next_state, delay;\r\nunsigned bytes_left;\r\ndo {\r\nnext_state = fsm->substate;\r\ndelay = 0;\r\nswitch(fsm->substate) {\r\ncase SIRDEV_STATE_WAIT_XMIT:\r\nif (dev->drv->chars_in_buffer)\r\nbytes_left = dev->drv->chars_in_buffer(dev);\r\nelse\r\nbytes_left = 0;\r\nif (!bytes_left) {\r\nnext_state = SIRDEV_STATE_WAIT_UNTIL_SENT;\r\nbreak;\r\n}\r\nif (dev->speed > 115200)\r\ndelay = (bytes_left*8*10000) / (dev->speed/100);\r\nelse if (dev->speed > 0)\r\ndelay = (bytes_left*10*10000) / (dev->speed/100);\r\nelse\r\ndelay = 0;\r\nif (delay < 100) {\r\nudelay(delay);\r\ndelay = 0;\r\nbreak;\r\n}\r\ndelay = (delay+999) / 1000;\r\nbreak;\r\ncase SIRDEV_STATE_WAIT_UNTIL_SENT:\r\nif (dev->drv->wait_until_sent)\r\ndev->drv->wait_until_sent(dev);\r\nnext_state = SIRDEV_STATE_TX_DONE;\r\nbreak;\r\ncase SIRDEV_STATE_TX_DONE:\r\nreturn 0;\r\ndefault:\r\nIRDA_ERROR("%s - undefined state\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nfsm->substate = next_state;\r\n} while (delay == 0);\r\nreturn delay;\r\n}\r\nstatic void sirdev_config_fsm(struct work_struct *work)\r\n{\r\nstruct sir_dev *dev = container_of(work, struct sir_dev, fsm.work.work);\r\nstruct sir_fsm *fsm = &dev->fsm;\r\nint next_state;\r\nint ret = -1;\r\nunsigned delay;\r\nIRDA_DEBUG(2, "%s(), <%ld>\n", __func__, jiffies);\r\ndo {\r\nIRDA_DEBUG(3, "%s - state=0x%04x / substate=0x%04x\n",\r\n__func__, fsm->state, fsm->substate);\r\nnext_state = fsm->state;\r\ndelay = 0;\r\nswitch(fsm->state) {\r\ncase SIRDEV_STATE_DONGLE_OPEN:\r\nif (dev->dongle_drv != NULL) {\r\nret = sirdev_put_dongle(dev);\r\nif (ret) {\r\nfsm->result = -EINVAL;\r\nnext_state = SIRDEV_STATE_ERROR;\r\nbreak;\r\n}\r\n}\r\nret = sirdev_get_dongle(dev, fsm->param);\r\nif (ret) {\r\nfsm->result = ret;\r\nnext_state = SIRDEV_STATE_ERROR;\r\nbreak;\r\n}\r\ndelay = 50;\r\nfsm->substate = SIRDEV_STATE_DONGLE_RESET;\r\nnext_state = SIRDEV_STATE_DONGLE_RESET;\r\nfsm->param = 9600;\r\nbreak;\r\ncase SIRDEV_STATE_DONGLE_CLOSE:\r\nif (dev->dongle_drv == NULL) {\r\nfsm->result = -EINVAL;\r\nnext_state = SIRDEV_STATE_ERROR;\r\nbreak;\r\n}\r\nret = sirdev_put_dongle(dev);\r\nif (ret) {\r\nfsm->result = ret;\r\nnext_state = SIRDEV_STATE_ERROR;\r\nbreak;\r\n}\r\nnext_state = SIRDEV_STATE_DONE;\r\nbreak;\r\ncase SIRDEV_STATE_SET_DTR_RTS:\r\nret = sirdev_set_dtr_rts(dev,\r\n(fsm->param&0x02) ? TRUE : FALSE,\r\n(fsm->param&0x01) ? TRUE : FALSE);\r\nnext_state = SIRDEV_STATE_DONE;\r\nbreak;\r\ncase SIRDEV_STATE_SET_SPEED:\r\nfsm->substate = SIRDEV_STATE_WAIT_XMIT;\r\nnext_state = SIRDEV_STATE_DONGLE_CHECK;\r\nbreak;\r\ncase SIRDEV_STATE_DONGLE_CHECK:\r\nret = sirdev_tx_complete_fsm(dev);\r\nif (ret < 0) {\r\nfsm->result = ret;\r\nnext_state = SIRDEV_STATE_ERROR;\r\nbreak;\r\n}\r\nif ((delay=ret) != 0)\r\nbreak;\r\nif (dev->dongle_drv) {\r\nfsm->substate = SIRDEV_STATE_DONGLE_RESET;\r\nnext_state = SIRDEV_STATE_DONGLE_RESET;\r\n}\r\nelse {\r\ndev->speed = fsm->param;\r\nnext_state = SIRDEV_STATE_PORT_SPEED;\r\n}\r\nbreak;\r\ncase SIRDEV_STATE_DONGLE_RESET:\r\nif (dev->dongle_drv->reset) {\r\nret = dev->dongle_drv->reset(dev);\r\nif (ret < 0) {\r\nfsm->result = ret;\r\nnext_state = SIRDEV_STATE_ERROR;\r\nbreak;\r\n}\r\n}\r\nelse\r\nret = 0;\r\nif ((delay=ret) == 0) {\r\nif (dev->drv->set_speed)\r\ndev->drv->set_speed(dev, dev->speed);\r\nfsm->substate = SIRDEV_STATE_DONGLE_SPEED;\r\nnext_state = SIRDEV_STATE_DONGLE_SPEED;\r\n}\r\nbreak;\r\ncase SIRDEV_STATE_DONGLE_SPEED:\r\nif (dev->dongle_drv->reset) {\r\nret = dev->dongle_drv->set_speed(dev, fsm->param);\r\nif (ret < 0) {\r\nfsm->result = ret;\r\nnext_state = SIRDEV_STATE_ERROR;\r\nbreak;\r\n}\r\n}\r\nelse\r\nret = 0;\r\nif ((delay=ret) == 0)\r\nnext_state = SIRDEV_STATE_PORT_SPEED;\r\nbreak;\r\ncase SIRDEV_STATE_PORT_SPEED:\r\nif (dev->drv->set_speed)\r\ndev->drv->set_speed(dev, dev->speed);\r\ndev->new_speed = 0;\r\nnext_state = SIRDEV_STATE_DONE;\r\nbreak;\r\ncase SIRDEV_STATE_DONE:\r\nnetif_wake_queue(dev->netdev);\r\nnext_state = SIRDEV_STATE_COMPLETE;\r\nbreak;\r\ndefault:\r\nIRDA_ERROR("%s - undefined state\n", __func__);\r\nfsm->result = -EINVAL;\r\ncase SIRDEV_STATE_ERROR:\r\nIRDA_ERROR("%s - error: %d\n", __func__, fsm->result);\r\n#if 0\r\nnetif_stop_queue(dev->netdev);\r\n#else\r\nnetif_wake_queue(dev->netdev);\r\n#endif\r\ncase SIRDEV_STATE_COMPLETE:\r\nsirdev_enable_rx(dev);\r\nup(&fsm->sem);\r\nreturn;\r\n}\r\nfsm->state = next_state;\r\n} while(!delay);\r\nqueue_delayed_work(irda_sir_wq, &fsm->work, msecs_to_jiffies(delay));\r\n}\r\nint sirdev_schedule_request(struct sir_dev *dev, int initial_state, unsigned param)\r\n{\r\nstruct sir_fsm *fsm = &dev->fsm;\r\nIRDA_DEBUG(2, "%s - state=0x%04x / param=%u\n", __func__,\r\ninitial_state, param);\r\nif (down_trylock(&fsm->sem)) {\r\nif (in_interrupt() || in_atomic() || irqs_disabled()) {\r\nIRDA_DEBUG(1, "%s(), state machine busy!\n", __func__);\r\nreturn -EWOULDBLOCK;\r\n} else\r\ndown(&fsm->sem);\r\n}\r\nif (fsm->state == SIRDEV_STATE_DEAD) {\r\nIRDA_ERROR("%s(), instance staled!\n", __func__);\r\nup(&fsm->sem);\r\nreturn -ESTALE;\r\n}\r\nnetif_stop_queue(dev->netdev);\r\natomic_set(&dev->enable_rx, 0);\r\nfsm->state = initial_state;\r\nfsm->param = param;\r\nfsm->result = 0;\r\nINIT_DELAYED_WORK(&fsm->work, sirdev_config_fsm);\r\nqueue_delayed_work(irda_sir_wq, &fsm->work, 0);\r\nreturn 0;\r\n}\r\nvoid sirdev_enable_rx(struct sir_dev *dev)\r\n{\r\nif (unlikely(atomic_read(&dev->enable_rx)))\r\nreturn;\r\ndev->rx_buff.data = dev->rx_buff.head;\r\ndev->rx_buff.len = 0;\r\ndev->rx_buff.in_frame = FALSE;\r\ndev->rx_buff.state = OUTSIDE_FRAME;\r\natomic_set(&dev->enable_rx, 1);\r\n}\r\nstatic int sirdev_is_receiving(struct sir_dev *dev)\r\n{\r\nif (!atomic_read(&dev->enable_rx))\r\nreturn 0;\r\nreturn dev->rx_buff.state != OUTSIDE_FRAME;\r\n}\r\nint sirdev_set_dongle(struct sir_dev *dev, IRDA_DONGLE type)\r\n{\r\nint err;\r\nIRDA_DEBUG(3, "%s : requesting dongle %d.\n", __func__, type);\r\nerr = sirdev_schedule_dongle_open(dev, type);\r\nif (unlikely(err))\r\nreturn err;\r\ndown(&dev->fsm.sem);\r\nerr = dev->fsm.result;\r\nup(&dev->fsm.sem);\r\nreturn err;\r\n}\r\nint sirdev_raw_write(struct sir_dev *dev, const char *buf, int len)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nif (unlikely(len > dev->tx_buff.truesize))\r\nreturn -ENOSPC;\r\nspin_lock_irqsave(&dev->tx_lock, flags);\r\nwhile (dev->tx_buff.len > 0) {\r\nspin_unlock_irqrestore(&dev->tx_lock, flags);\r\nmsleep(10);\r\nspin_lock_irqsave(&dev->tx_lock, flags);\r\n}\r\ndev->tx_buff.data = dev->tx_buff.head;\r\nmemcpy(dev->tx_buff.data, buf, len);\r\ndev->tx_buff.len = len;\r\nret = dev->drv->do_write(dev, dev->tx_buff.data, dev->tx_buff.len);\r\nif (ret > 0) {\r\nIRDA_DEBUG(3, "%s(), raw-tx started\n", __func__);\r\ndev->tx_buff.data += ret;\r\ndev->tx_buff.len -= ret;\r\ndev->raw_tx = 1;\r\nret = len;\r\n}\r\nspin_unlock_irqrestore(&dev->tx_lock, flags);\r\nreturn ret;\r\n}\r\nint sirdev_raw_read(struct sir_dev *dev, char *buf, int len)\r\n{\r\nint count;\r\nif (atomic_read(&dev->enable_rx))\r\nreturn -EIO;\r\ncount = (len < dev->rx_buff.len) ? len : dev->rx_buff.len;\r\nif (count > 0) {\r\nmemcpy(buf, dev->rx_buff.data, count);\r\ndev->rx_buff.data += count;\r\ndev->rx_buff.len -= count;\r\n}\r\nreturn count;\r\n}\r\nint sirdev_set_dtr_rts(struct sir_dev *dev, int dtr, int rts)\r\n{\r\nint ret = -ENXIO;\r\nif (dev->drv->set_dtr_rts)\r\nret = dev->drv->set_dtr_rts(dev, dtr, rts);\r\nreturn ret;\r\n}\r\nvoid sirdev_write_complete(struct sir_dev *dev)\r\n{\r\nunsigned long flags;\r\nstruct sk_buff *skb;\r\nint actual = 0;\r\nint err;\r\nspin_lock_irqsave(&dev->tx_lock, flags);\r\nIRDA_DEBUG(3, "%s() - dev->tx_buff.len = %d\n",\r\n__func__, dev->tx_buff.len);\r\nif (likely(dev->tx_buff.len > 0)) {\r\nactual = dev->drv->do_write(dev, dev->tx_buff.data, dev->tx_buff.len);\r\nif (likely(actual>0)) {\r\ndev->tx_buff.data += actual;\r\ndev->tx_buff.len -= actual;\r\n}\r\nelse if (unlikely(actual<0)) {\r\nIRDA_ERROR("%s: drv->do_write failed (%d)\n",\r\n__func__, actual);\r\nif ((skb=dev->tx_skb) != NULL) {\r\ndev->tx_skb = NULL;\r\ndev_kfree_skb_any(skb);\r\ndev->netdev->stats.tx_errors++;\r\ndev->netdev->stats.tx_dropped++;\r\n}\r\ndev->tx_buff.len = 0;\r\n}\r\nif (dev->tx_buff.len > 0)\r\ngoto done;\r\n}\r\nif (unlikely(dev->raw_tx != 0)) {\r\nIRDA_DEBUG(3, "%s(), raw-tx done\n", __func__);\r\ndev->raw_tx = 0;\r\ngoto done;\r\n}\r\nIRDA_DEBUG(5, "%s(), finished with frame!\n", __func__);\r\nif ((skb=dev->tx_skb) != NULL) {\r\ndev->tx_skb = NULL;\r\ndev->netdev->stats.tx_packets++;\r\ndev->netdev->stats.tx_bytes += skb->len;\r\ndev_kfree_skb_any(skb);\r\n}\r\nif (unlikely(dev->new_speed > 0)) {\r\nIRDA_DEBUG(5, "%s(), Changing speed!\n", __func__);\r\nerr = sirdev_schedule_speed(dev, dev->new_speed);\r\nif (unlikely(err)) {\r\nIRDA_ERROR("%s - schedule speed change failed: %d\n",\r\n__func__, err);\r\nnetif_wake_queue(dev->netdev);\r\n}\r\n}\r\nelse {\r\nsirdev_enable_rx(dev);\r\nnetif_wake_queue(dev->netdev);\r\n}\r\ndone:\r\nspin_unlock_irqrestore(&dev->tx_lock, flags);\r\n}\r\nint sirdev_receive(struct sir_dev *dev, const unsigned char *cp, size_t count)\r\n{\r\nif (!dev || !dev->netdev) {\r\nIRDA_WARNING("%s(), not ready yet!\n", __func__);\r\nreturn -1;\r\n}\r\nif (!dev->irlap) {\r\nIRDA_WARNING("%s - too early: %p / %zd!\n",\r\n__func__, cp, count);\r\nreturn -1;\r\n}\r\nif (cp==NULL) {\r\nirda_device_set_media_busy(dev->netdev, TRUE);\r\ndev->netdev->stats.rx_dropped++;\r\nIRDA_DEBUG(0, "%s; rx-drop: %zd\n", __func__, count);\r\nreturn 0;\r\n}\r\nif (likely(atomic_read(&dev->enable_rx))) {\r\nwhile (count--)\r\nasync_unwrap_char(dev->netdev, &dev->netdev->stats,\r\n&dev->rx_buff, *cp++);\r\n} else {\r\nwhile (count--) {\r\ndev->rx_buff.data[dev->rx_buff.len++] = *cp++;\r\nif (unlikely(dev->rx_buff.len == dev->rx_buff.truesize))\r\ndev->rx_buff.len = 0;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic netdev_tx_t sirdev_hard_xmit(struct sk_buff *skb,\r\nstruct net_device *ndev)\r\n{\r\nstruct sir_dev *dev = netdev_priv(ndev);\r\nunsigned long flags;\r\nint actual = 0;\r\nint err;\r\ns32 speed;\r\nIRDA_ASSERT(dev != NULL, return NETDEV_TX_OK;);\r\nnetif_stop_queue(ndev);\r\nIRDA_DEBUG(3, "%s(), skb->len = %d\n", __func__, skb->len);\r\nspeed = irda_get_next_speed(skb);\r\nif ((speed != dev->speed) && (speed != -1)) {\r\nif (!skb->len) {\r\nerr = sirdev_schedule_speed(dev, speed);\r\nif (unlikely(err == -EWOULDBLOCK)) {\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nelse if (unlikely(err)) {\r\nnetif_start_queue(ndev);\r\n}\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n} else\r\ndev->new_speed = speed;\r\n}\r\ndev->tx_buff.data = dev->tx_buff.head;\r\nif(spin_is_locked(&dev->tx_lock)) {\r\nIRDA_DEBUG(3, "%s(), write not completed\n", __func__);\r\n}\r\nspin_lock_irqsave(&dev->tx_lock, flags);\r\ndev->tx_buff.len = async_wrap_skb(skb, dev->tx_buff.data, dev->tx_buff.truesize);\r\natomic_set(&dev->enable_rx, 0);\r\nif (unlikely(sirdev_is_receiving(dev)))\r\ndev->netdev->stats.collisions++;\r\nactual = dev->drv->do_write(dev, dev->tx_buff.data, dev->tx_buff.len);\r\nif (likely(actual > 0)) {\r\ndev->tx_skb = skb;\r\ndev->tx_buff.data += actual;\r\ndev->tx_buff.len -= actual;\r\n}\r\nelse if (unlikely(actual < 0)) {\r\nIRDA_ERROR("%s: drv->do_write failed (%d)\n",\r\n__func__, actual);\r\ndev_kfree_skb_any(skb);\r\ndev->netdev->stats.tx_errors++;\r\ndev->netdev->stats.tx_dropped++;\r\nnetif_wake_queue(ndev);\r\n}\r\nspin_unlock_irqrestore(&dev->tx_lock, flags);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int sirdev_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd)\r\n{\r\nstruct if_irda_req *irq = (struct if_irda_req *) rq;\r\nstruct sir_dev *dev = netdev_priv(ndev);\r\nint ret = 0;\r\nIRDA_ASSERT(dev != NULL, return -1;);\r\nIRDA_DEBUG(3, "%s(), %s, (cmd=0x%X)\n", __func__, ndev->name, cmd);\r\nswitch (cmd) {\r\ncase SIOCSBANDWIDTH:\r\nif (!capable(CAP_NET_ADMIN))\r\nret = -EPERM;\r\nelse\r\nret = sirdev_schedule_speed(dev, irq->ifr_baudrate);\r\nbreak;\r\ncase SIOCSDONGLE:\r\nif (!capable(CAP_NET_ADMIN))\r\nret = -EPERM;\r\nelse\r\nret = sirdev_schedule_dongle_open(dev, irq->ifr_dongle);\r\nbreak;\r\ncase SIOCSMEDIABUSY:\r\nif (!capable(CAP_NET_ADMIN))\r\nret = -EPERM;\r\nelse\r\nirda_device_set_media_busy(dev->netdev, TRUE);\r\nbreak;\r\ncase SIOCGRECEIVING:\r\nirq->ifr_receiving = sirdev_is_receiving(dev);\r\nbreak;\r\ncase SIOCSDTRRTS:\r\nif (!capable(CAP_NET_ADMIN))\r\nret = -EPERM;\r\nelse\r\nret = sirdev_schedule_dtr_rts(dev, irq->ifr_dtr, irq->ifr_rts);\r\nbreak;\r\ncase SIOCSMODE:\r\n#if 0\r\nif (!capable(CAP_NET_ADMIN))\r\nret = -EPERM;\r\nelse\r\nret = sirdev_schedule_mode(dev, irq->ifr_mode);\r\nbreak;\r\n#endif\r\ndefault:\r\nret = -EOPNOTSUPP;\r\n}\r\nreturn ret;\r\n}\r\nstatic int sirdev_alloc_buffers(struct sir_dev *dev)\r\n{\r\ndev->tx_buff.truesize = SIRBUF_ALLOCSIZE;\r\ndev->rx_buff.truesize = IRDA_SKB_MAX_MTU;\r\ndev->rx_buff.skb = __netdev_alloc_skb(dev->netdev, dev->rx_buff.truesize,\r\nGFP_KERNEL);\r\nif (dev->rx_buff.skb == NULL)\r\nreturn -ENOMEM;\r\nskb_reserve(dev->rx_buff.skb, 1);\r\ndev->rx_buff.head = dev->rx_buff.skb->data;\r\ndev->tx_buff.head = kmalloc(dev->tx_buff.truesize, GFP_KERNEL);\r\nif (dev->tx_buff.head == NULL) {\r\nkfree_skb(dev->rx_buff.skb);\r\ndev->rx_buff.skb = NULL;\r\ndev->rx_buff.head = NULL;\r\nreturn -ENOMEM;\r\n}\r\ndev->tx_buff.data = dev->tx_buff.head;\r\ndev->rx_buff.data = dev->rx_buff.head;\r\ndev->tx_buff.len = 0;\r\ndev->rx_buff.len = 0;\r\ndev->rx_buff.in_frame = FALSE;\r\ndev->rx_buff.state = OUTSIDE_FRAME;\r\nreturn 0;\r\n}\r\nstatic void sirdev_free_buffers(struct sir_dev *dev)\r\n{\r\nkfree_skb(dev->rx_buff.skb);\r\nkfree(dev->tx_buff.head);\r\ndev->rx_buff.head = dev->tx_buff.head = NULL;\r\ndev->rx_buff.skb = NULL;\r\n}\r\nstatic int sirdev_open(struct net_device *ndev)\r\n{\r\nstruct sir_dev *dev = netdev_priv(ndev);\r\nconst struct sir_driver *drv = dev->drv;\r\nif (!drv)\r\nreturn -ENODEV;\r\nif (!try_module_get(drv->owner))\r\nreturn -ESTALE;\r\nIRDA_DEBUG(2, "%s()\n", __func__);\r\nif (sirdev_alloc_buffers(dev))\r\ngoto errout_dec;\r\nif (!dev->drv->start_dev || dev->drv->start_dev(dev))\r\ngoto errout_free;\r\nsirdev_enable_rx(dev);\r\ndev->raw_tx = 0;\r\nnetif_start_queue(ndev);\r\ndev->irlap = irlap_open(ndev, &dev->qos, dev->hwname);\r\nif (!dev->irlap)\r\ngoto errout_stop;\r\nnetif_wake_queue(ndev);\r\nIRDA_DEBUG(2, "%s - done, speed = %d\n", __func__, dev->speed);\r\nreturn 0;\r\nerrout_stop:\r\natomic_set(&dev->enable_rx, 0);\r\nif (dev->drv->stop_dev)\r\ndev->drv->stop_dev(dev);\r\nerrout_free:\r\nsirdev_free_buffers(dev);\r\nerrout_dec:\r\nmodule_put(drv->owner);\r\nreturn -EAGAIN;\r\n}\r\nstatic int sirdev_close(struct net_device *ndev)\r\n{\r\nstruct sir_dev *dev = netdev_priv(ndev);\r\nconst struct sir_driver *drv;\r\nnetif_stop_queue(ndev);\r\ndown(&dev->fsm.sem);\r\natomic_set(&dev->enable_rx, 0);\r\nif (unlikely(!dev->irlap))\r\ngoto out;\r\nirlap_close(dev->irlap);\r\ndev->irlap = NULL;\r\ndrv = dev->drv;\r\nif (unlikely(!drv || !dev->priv))\r\ngoto out;\r\nif (drv->stop_dev)\r\ndrv->stop_dev(dev);\r\nsirdev_free_buffers(dev);\r\nmodule_put(drv->owner);\r\nout:\r\ndev->speed = 0;\r\nup(&dev->fsm.sem);\r\nreturn 0;\r\n}\r\nstruct sir_dev * sirdev_get_instance(const struct sir_driver *drv, const char *name)\r\n{\r\nstruct net_device *ndev;\r\nstruct sir_dev *dev;\r\nIRDA_DEBUG(0, "%s - %s\n", __func__, name);\r\nif (!drv || !drv->do_write)\r\nreturn NULL;\r\nndev = alloc_irdadev(sizeof(*dev));\r\nif (ndev == NULL) {\r\nIRDA_ERROR("%s - Can't allocate memory for IrDA control block!\n", __func__);\r\ngoto out;\r\n}\r\ndev = netdev_priv(ndev);\r\nirda_init_max_qos_capabilies(&dev->qos);\r\ndev->qos.baud_rate.bits = IR_9600|IR_19200|IR_38400|IR_57600|IR_115200;\r\ndev->qos.min_turn_time.bits = drv->qos_mtt_bits;\r\nirda_qos_bits_to_value(&dev->qos);\r\nstrncpy(dev->hwname, name, sizeof(dev->hwname)-1);\r\natomic_set(&dev->enable_rx, 0);\r\ndev->tx_skb = NULL;\r\nspin_lock_init(&dev->tx_lock);\r\nsema_init(&dev->fsm.sem, 1);\r\ndev->drv = drv;\r\ndev->netdev = ndev;\r\nndev->netdev_ops = &sirdev_ops;\r\nif (register_netdev(ndev)) {\r\nIRDA_ERROR("%s(), register_netdev() failed!\n", __func__);\r\ngoto out_freenetdev;\r\n}\r\nreturn dev;\r\nout_freenetdev:\r\nfree_netdev(ndev);\r\nout:\r\nreturn NULL;\r\n}\r\nint sirdev_put_instance(struct sir_dev *dev)\r\n{\r\nint err = 0;\r\nIRDA_DEBUG(0, "%s\n", __func__);\r\natomic_set(&dev->enable_rx, 0);\r\nnetif_carrier_off(dev->netdev);\r\nnetif_device_detach(dev->netdev);\r\nif (dev->dongle_drv)\r\nerr = sirdev_schedule_dongle_close(dev);\r\nif (err)\r\nIRDA_ERROR("%s - error %d\n", __func__, err);\r\nsirdev_close(dev->netdev);\r\ndown(&dev->fsm.sem);\r\ndev->fsm.state = SIRDEV_STATE_DEAD;\r\ndev->dongle_drv = NULL;\r\ndev->priv = NULL;\r\nup(&dev->fsm.sem);\r\nunregister_netdev(dev->netdev);\r\nfree_netdev(dev->netdev);\r\nreturn 0;\r\n}\r\nstatic int __init sir_wq_init(void)\r\n{\r\nirda_sir_wq = create_singlethread_workqueue("irda_sir_wq");\r\nif (!irda_sir_wq)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic void __exit sir_wq_exit(void)\r\n{\r\ndestroy_workqueue(irda_sir_wq);\r\n}
