static inline int *__atomic_hashed_lock(volatile void *v)\r\n{\r\n#if ATOMIC_LOCKS_FOUND_VIA_TABLE()\r\nunsigned long i =\r\n(unsigned long) v & ((PAGE_SIZE-1) & -sizeof(long long));\r\nunsigned long n = __insn_crc32_32(0, i);\r\nunsigned long l1_index = n >> ((sizeof(n) * 8) - ATOMIC_HASH_L1_SHIFT);\r\nunsigned long l2_index = n & (ATOMIC_HASH_L2_SIZE - 1);\r\nreturn &atomic_lock_ptr[l1_index]->lock[l2_index];\r\n#else\r\nunsigned long ptr = __insn_mm((unsigned long)v >> 1,\r\n(unsigned long)atomic_locks,\r\n2, (ATOMIC_HASH_SHIFT + 2) - 1);\r\nreturn (int *)ptr;\r\n#endif\r\n}\r\nstatic int is_atomic_lock(int *p)\r\n{\r\n#if ATOMIC_LOCKS_FOUND_VIA_TABLE()\r\nint i;\r\nfor (i = 0; i < ATOMIC_HASH_L1_SIZE; ++i) {\r\nif (p >= &atomic_lock_ptr[i]->lock[0] &&\r\np < &atomic_lock_ptr[i]->lock[ATOMIC_HASH_L2_SIZE]) {\r\nreturn 1;\r\n}\r\n}\r\nreturn 0;\r\n#else\r\nreturn p >= &atomic_locks[0] && p < &atomic_locks[ATOMIC_HASH_SIZE];\r\n#endif\r\n}\r\nvoid __atomic_fault_unlock(int *irqlock_word)\r\n{\r\nBUG_ON(!is_atomic_lock(irqlock_word));\r\nBUG_ON(*irqlock_word != 1);\r\n*irqlock_word = 0;\r\n}\r\nstatic inline int *__atomic_setup(volatile void *v)\r\n{\r\n*(volatile int *)v;\r\nreturn __atomic_hashed_lock(v);\r\n}\r\nint _atomic_xchg(atomic_t *v, int n)\r\n{\r\nreturn __atomic_xchg(&v->counter, __atomic_setup(v), n).val;\r\n}\r\nint _atomic_xchg_add(atomic_t *v, int i)\r\n{\r\nreturn __atomic_xchg_add(&v->counter, __atomic_setup(v), i).val;\r\n}\r\nint _atomic_xchg_add_unless(atomic_t *v, int a, int u)\r\n{\r\nreturn __atomic_xchg_add_unless(&v->counter, __atomic_setup(v), u, a)\r\n.val;\r\n}\r\nint _atomic_cmpxchg(atomic_t *v, int o, int n)\r\n{\r\nreturn __atomic_cmpxchg(&v->counter, __atomic_setup(v), o, n).val;\r\n}\r\nunsigned long _atomic_or(volatile unsigned long *p, unsigned long mask)\r\n{\r\nreturn __atomic_or((int *)p, __atomic_setup(p), mask).val;\r\n}\r\nunsigned long _atomic_andn(volatile unsigned long *p, unsigned long mask)\r\n{\r\nreturn __atomic_andn((int *)p, __atomic_setup(p), mask).val;\r\n}\r\nunsigned long _atomic_xor(volatile unsigned long *p, unsigned long mask)\r\n{\r\nreturn __atomic_xor((int *)p, __atomic_setup(p), mask).val;\r\n}\r\nu64 _atomic64_xchg(atomic64_t *v, u64 n)\r\n{\r\nreturn __atomic64_xchg(&v->counter, __atomic_setup(v), n);\r\n}\r\nu64 _atomic64_xchg_add(atomic64_t *v, u64 i)\r\n{\r\nreturn __atomic64_xchg_add(&v->counter, __atomic_setup(v), i);\r\n}\r\nu64 _atomic64_xchg_add_unless(atomic64_t *v, u64 a, u64 u)\r\n{\r\nreturn __atomic64_xchg_add_unless(&v->counter, __atomic_setup(v),\r\nu, a);\r\n}\r\nu64 _atomic64_cmpxchg(atomic64_t *v, u64 o, u64 n)\r\n{\r\nreturn __atomic64_cmpxchg(&v->counter, __atomic_setup(v), o, n);\r\n}\r\nstatic inline int *__futex_setup(int __user *v)\r\n{\r\n__insn_prefetch(v);\r\nreturn __atomic_hashed_lock((int __force *)v);\r\n}\r\nstruct __get_user futex_set(u32 __user *v, int i)\r\n{\r\nreturn __atomic_xchg((int __force *)v, __futex_setup(v), i);\r\n}\r\nstruct __get_user futex_add(u32 __user *v, int n)\r\n{\r\nreturn __atomic_xchg_add((int __force *)v, __futex_setup(v), n);\r\n}\r\nstruct __get_user futex_or(u32 __user *v, int n)\r\n{\r\nreturn __atomic_or((int __force *)v, __futex_setup(v), n);\r\n}\r\nstruct __get_user futex_andn(u32 __user *v, int n)\r\n{\r\nreturn __atomic_andn((int __force *)v, __futex_setup(v), n);\r\n}\r\nstruct __get_user futex_xor(u32 __user *v, int n)\r\n{\r\nreturn __atomic_xor((int __force *)v, __futex_setup(v), n);\r\n}\r\nstruct __get_user futex_cmpxchg(u32 __user *v, int o, int n)\r\n{\r\nreturn __atomic_cmpxchg((int __force *)v, __futex_setup(v), o, n);\r\n}\r\nstruct __get_user __atomic_bad_address(int __user *addr)\r\n{\r\nif (unlikely(!access_ok(VERIFY_WRITE, addr, sizeof(int))))\r\npanic("Bad address used for kernel atomic op: %p\n", addr);\r\nreturn (struct __get_user) { .err = -EFAULT };\r\n}\r\nstatic int __init noatomichash(char *str)\r\n{\r\npr_warning("noatomichash is deprecated.\n");\r\nreturn 1;\r\n}\r\nvoid __init __init_atomic_per_cpu(void)\r\n{\r\n#if ATOMIC_LOCKS_FOUND_VIA_TABLE()\r\nunsigned int i;\r\nint actual_cpu;\r\nactual_cpu = cpumask_first(cpu_possible_mask);\r\nfor (i = 0; i < ATOMIC_HASH_L1_SIZE; ++i) {\r\nactual_cpu = cpumask_next(actual_cpu, cpu_possible_mask);\r\nif (actual_cpu >= nr_cpu_ids)\r\nactual_cpu = cpumask_first(cpu_possible_mask);\r\natomic_lock_ptr[i] = &per_cpu(atomic_lock_pool, actual_cpu);\r\n}\r\n#else\r\nBUILD_BUG_ON(ATOMIC_HASH_SIZE & (ATOMIC_HASH_SIZE-1));\r\nBUG_ON(ATOMIC_HASH_SIZE < nr_cpu_ids);\r\nBUG_ON((unsigned long)atomic_locks % PAGE_SIZE != 0);\r\nBUILD_BUG_ON(ATOMIC_HASH_SIZE * sizeof(int) > PAGE_SIZE);\r\nBUILD_BUG_ON((PAGE_SIZE >> 3) > ATOMIC_HASH_SIZE);\r\n#endif\r\nBUILD_BUG_ON(sizeof(atomic_t) != sizeof(int));\r\n}
