static int\r\nfunc_prolog_preempt_disable(struct trace_array *tr,\r\nstruct trace_array_cpu **data,\r\nint *pc)\r\n{\r\nlong disabled;\r\nint cpu;\r\nif (likely(!wakeup_task))\r\nreturn 0;\r\n*pc = preempt_count();\r\npreempt_disable_notrace();\r\ncpu = raw_smp_processor_id();\r\nif (cpu != wakeup_current_cpu)\r\ngoto out_enable;\r\n*data = tr->data[cpu];\r\ndisabled = atomic_inc_return(&(*data)->disabled);\r\nif (unlikely(disabled != 1))\r\ngoto out;\r\nreturn 1;\r\nout:\r\natomic_dec(&(*data)->disabled);\r\nout_enable:\r\npreempt_enable_notrace();\r\nreturn 0;\r\n}\r\nstatic void\r\nwakeup_tracer_call(unsigned long ip, unsigned long parent_ip)\r\n{\r\nstruct trace_array *tr = wakeup_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint pc;\r\nif (!func_prolog_preempt_disable(tr, &data, &pc))\r\nreturn;\r\nlocal_irq_save(flags);\r\ntrace_function(tr, ip, parent_ip, flags, pc);\r\nlocal_irq_restore(flags);\r\natomic_dec(&data->disabled);\r\npreempt_enable_notrace();\r\n}\r\nstatic int start_func_tracer(int graph)\r\n{\r\nint ret;\r\nif (!graph)\r\nret = register_ftrace_function(&trace_ops);\r\nelse\r\nret = register_ftrace_graph(&wakeup_graph_return,\r\n&wakeup_graph_entry);\r\nif (!ret && tracing_is_enabled())\r\ntracer_enabled = 1;\r\nelse\r\ntracer_enabled = 0;\r\nreturn ret;\r\n}\r\nstatic void stop_func_tracer(int graph)\r\n{\r\ntracer_enabled = 0;\r\nif (!graph)\r\nunregister_ftrace_function(&trace_ops);\r\nelse\r\nunregister_ftrace_graph();\r\n}\r\nstatic int wakeup_set_flag(u32 old_flags, u32 bit, int set)\r\n{\r\nif (!(bit & TRACE_DISPLAY_GRAPH))\r\nreturn -EINVAL;\r\nif (!(is_graph() ^ set))\r\nreturn 0;\r\nstop_func_tracer(!set);\r\nwakeup_reset(wakeup_trace);\r\ntracing_max_latency = 0;\r\nreturn start_func_tracer(set);\r\n}\r\nstatic int wakeup_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nstruct trace_array *tr = wakeup_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint pc, ret = 0;\r\nif (!func_prolog_preempt_disable(tr, &data, &pc))\r\nreturn 0;\r\nlocal_save_flags(flags);\r\nret = __trace_graph_entry(tr, trace, flags, pc);\r\natomic_dec(&data->disabled);\r\npreempt_enable_notrace();\r\nreturn ret;\r\n}\r\nstatic void wakeup_graph_return(struct ftrace_graph_ret *trace)\r\n{\r\nstruct trace_array *tr = wakeup_trace;\r\nstruct trace_array_cpu *data;\r\nunsigned long flags;\r\nint pc;\r\nif (!func_prolog_preempt_disable(tr, &data, &pc))\r\nreturn;\r\nlocal_save_flags(flags);\r\n__trace_graph_return(tr, trace, flags, pc);\r\natomic_dec(&data->disabled);\r\npreempt_enable_notrace();\r\nreturn;\r\n}\r\nstatic void wakeup_trace_open(struct trace_iterator *iter)\r\n{\r\nif (is_graph())\r\ngraph_trace_open(iter);\r\n}\r\nstatic void wakeup_trace_close(struct trace_iterator *iter)\r\n{\r\nif (iter->private)\r\ngraph_trace_close(iter);\r\n}\r\nstatic enum print_line_t wakeup_print_line(struct trace_iterator *iter)\r\n{\r\nif (is_graph())\r\nreturn print_graph_function_flags(iter, GRAPH_TRACER_FLAGS);\r\nreturn TRACE_TYPE_UNHANDLED;\r\n}\r\nstatic void wakeup_print_header(struct seq_file *s)\r\n{\r\nif (is_graph())\r\nprint_graph_headers_flags(s, GRAPH_TRACER_FLAGS);\r\nelse\r\ntrace_default_header(s);\r\n}\r\nstatic void\r\n__trace_function(struct trace_array *tr,\r\nunsigned long ip, unsigned long parent_ip,\r\nunsigned long flags, int pc)\r\n{\r\nif (is_graph())\r\ntrace_graph_function(tr, ip, parent_ip, flags, pc);\r\nelse\r\ntrace_function(tr, ip, parent_ip, flags, pc);\r\n}\r\nstatic int wakeup_set_flag(u32 old_flags, u32 bit, int set)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic int wakeup_graph_entry(struct ftrace_graph_ent *trace)\r\n{\r\nreturn -1;\r\n}\r\nstatic enum print_line_t wakeup_print_line(struct trace_iterator *iter)\r\n{\r\nreturn TRACE_TYPE_UNHANDLED;\r\n}\r\nstatic void wakeup_graph_return(struct ftrace_graph_ret *trace) { }\r\nstatic void wakeup_print_header(struct seq_file *s) { }\r\nstatic void wakeup_trace_open(struct trace_iterator *iter) { }\r\nstatic void wakeup_trace_close(struct trace_iterator *iter) { }\r\nstatic int report_latency(cycle_t delta)\r\n{\r\nif (tracing_thresh) {\r\nif (delta < tracing_thresh)\r\nreturn 0;\r\n} else {\r\nif (delta <= tracing_max_latency)\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void\r\nprobe_wakeup_migrate_task(void *ignore, struct task_struct *task, int cpu)\r\n{\r\nif (task != wakeup_task)\r\nreturn;\r\nwakeup_current_cpu = cpu;\r\n}\r\nstatic void notrace\r\nprobe_wakeup_sched_switch(void *ignore,\r\nstruct task_struct *prev, struct task_struct *next)\r\n{\r\nstruct trace_array_cpu *data;\r\ncycle_t T0, T1, delta;\r\nunsigned long flags;\r\nlong disabled;\r\nint cpu;\r\nint pc;\r\ntracing_record_cmdline(prev);\r\nif (unlikely(!tracer_enabled))\r\nreturn;\r\nsmp_rmb();\r\nif (next != wakeup_task)\r\nreturn;\r\npc = preempt_count();\r\ncpu = raw_smp_processor_id();\r\ndisabled = atomic_inc_return(&wakeup_trace->data[cpu]->disabled);\r\nif (likely(disabled != 1))\r\ngoto out;\r\nlocal_irq_save(flags);\r\narch_spin_lock(&wakeup_lock);\r\nif (unlikely(!tracer_enabled || next != wakeup_task))\r\ngoto out_unlock;\r\ndata = wakeup_trace->data[wakeup_cpu];\r\n__trace_function(wakeup_trace, CALLER_ADDR0, CALLER_ADDR1, flags, pc);\r\ntracing_sched_switch_trace(wakeup_trace, prev, next, flags, pc);\r\nT0 = data->preempt_timestamp;\r\nT1 = ftrace_now(cpu);\r\ndelta = T1-T0;\r\nif (!report_latency(delta))\r\ngoto out_unlock;\r\nif (likely(!is_tracing_stopped())) {\r\ntracing_max_latency = delta;\r\nupdate_max_tr(wakeup_trace, wakeup_task, wakeup_cpu);\r\n}\r\nout_unlock:\r\n__wakeup_reset(wakeup_trace);\r\narch_spin_unlock(&wakeup_lock);\r\nlocal_irq_restore(flags);\r\nout:\r\natomic_dec(&wakeup_trace->data[cpu]->disabled);\r\n}\r\nstatic void __wakeup_reset(struct trace_array *tr)\r\n{\r\nwakeup_cpu = -1;\r\nwakeup_prio = -1;\r\nif (wakeup_task)\r\nput_task_struct(wakeup_task);\r\nwakeup_task = NULL;\r\n}\r\nstatic void wakeup_reset(struct trace_array *tr)\r\n{\r\nunsigned long flags;\r\ntracing_reset_online_cpus(tr);\r\nlocal_irq_save(flags);\r\narch_spin_lock(&wakeup_lock);\r\n__wakeup_reset(tr);\r\narch_spin_unlock(&wakeup_lock);\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void\r\nprobe_wakeup(void *ignore, struct task_struct *p, int success)\r\n{\r\nstruct trace_array_cpu *data;\r\nint cpu = smp_processor_id();\r\nunsigned long flags;\r\nlong disabled;\r\nint pc;\r\nif (likely(!tracer_enabled))\r\nreturn;\r\ntracing_record_cmdline(p);\r\ntracing_record_cmdline(current);\r\nif ((wakeup_rt && !rt_task(p)) ||\r\np->prio >= wakeup_prio ||\r\np->prio >= current->prio)\r\nreturn;\r\npc = preempt_count();\r\ndisabled = atomic_inc_return(&wakeup_trace->data[cpu]->disabled);\r\nif (unlikely(disabled != 1))\r\ngoto out;\r\narch_spin_lock(&wakeup_lock);\r\nif (!tracer_enabled || p->prio >= wakeup_prio)\r\ngoto out_locked;\r\n__wakeup_reset(wakeup_trace);\r\nwakeup_cpu = task_cpu(p);\r\nwakeup_current_cpu = wakeup_cpu;\r\nwakeup_prio = p->prio;\r\nwakeup_task = p;\r\nget_task_struct(wakeup_task);\r\nlocal_save_flags(flags);\r\ndata = wakeup_trace->data[wakeup_cpu];\r\ndata->preempt_timestamp = ftrace_now(cpu);\r\ntracing_sched_wakeup_trace(wakeup_trace, p, current, flags, pc);\r\n__trace_function(wakeup_trace, CALLER_ADDR1, CALLER_ADDR2, flags, pc);\r\nout_locked:\r\narch_spin_unlock(&wakeup_lock);\r\nout:\r\natomic_dec(&wakeup_trace->data[cpu]->disabled);\r\n}\r\nstatic void start_wakeup_tracer(struct trace_array *tr)\r\n{\r\nint ret;\r\nret = register_trace_sched_wakeup(probe_wakeup, NULL);\r\nif (ret) {\r\npr_info("wakeup trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_wakeup\n");\r\nreturn;\r\n}\r\nret = register_trace_sched_wakeup_new(probe_wakeup, NULL);\r\nif (ret) {\r\npr_info("wakeup trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_wakeup_new\n");\r\ngoto fail_deprobe;\r\n}\r\nret = register_trace_sched_switch(probe_wakeup_sched_switch, NULL);\r\nif (ret) {\r\npr_info("sched trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_switch\n");\r\ngoto fail_deprobe_wake_new;\r\n}\r\nret = register_trace_sched_migrate_task(probe_wakeup_migrate_task, NULL);\r\nif (ret) {\r\npr_info("wakeup trace: Couldn't activate tracepoint"\r\n" probe to kernel_sched_migrate_task\n");\r\nreturn;\r\n}\r\nwakeup_reset(tr);\r\nsmp_wmb();\r\nif (start_func_tracer(is_graph()))\r\nprintk(KERN_ERR "failed to start wakeup tracer\n");\r\nreturn;\r\nfail_deprobe_wake_new:\r\nunregister_trace_sched_wakeup_new(probe_wakeup, NULL);\r\nfail_deprobe:\r\nunregister_trace_sched_wakeup(probe_wakeup, NULL);\r\n}\r\nstatic void stop_wakeup_tracer(struct trace_array *tr)\r\n{\r\ntracer_enabled = 0;\r\nstop_func_tracer(is_graph());\r\nunregister_trace_sched_switch(probe_wakeup_sched_switch, NULL);\r\nunregister_trace_sched_wakeup_new(probe_wakeup, NULL);\r\nunregister_trace_sched_wakeup(probe_wakeup, NULL);\r\nunregister_trace_sched_migrate_task(probe_wakeup_migrate_task, NULL);\r\n}\r\nstatic int __wakeup_tracer_init(struct trace_array *tr)\r\n{\r\nsave_lat_flag = trace_flags & TRACE_ITER_LATENCY_FMT;\r\ntrace_flags |= TRACE_ITER_LATENCY_FMT;\r\ntracing_max_latency = 0;\r\nwakeup_trace = tr;\r\nstart_wakeup_tracer(tr);\r\nreturn 0;\r\n}\r\nstatic int wakeup_tracer_init(struct trace_array *tr)\r\n{\r\nwakeup_rt = 0;\r\nreturn __wakeup_tracer_init(tr);\r\n}\r\nstatic int wakeup_rt_tracer_init(struct trace_array *tr)\r\n{\r\nwakeup_rt = 1;\r\nreturn __wakeup_tracer_init(tr);\r\n}\r\nstatic void wakeup_tracer_reset(struct trace_array *tr)\r\n{\r\nstop_wakeup_tracer(tr);\r\nwakeup_reset(tr);\r\nif (!save_lat_flag)\r\ntrace_flags &= ~TRACE_ITER_LATENCY_FMT;\r\n}\r\nstatic void wakeup_tracer_start(struct trace_array *tr)\r\n{\r\nwakeup_reset(tr);\r\ntracer_enabled = 1;\r\n}\r\nstatic void wakeup_tracer_stop(struct trace_array *tr)\r\n{\r\ntracer_enabled = 0;\r\n}\r\n__init static int init_wakeup_tracer(void)\r\n{\r\nint ret;\r\nret = register_tracer(&wakeup_tracer);\r\nif (ret)\r\nreturn ret;\r\nret = register_tracer(&wakeup_rt_tracer);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}
