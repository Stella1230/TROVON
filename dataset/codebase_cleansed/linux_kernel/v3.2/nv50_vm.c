void\r\nnv50_vm_map_pgt(struct nouveau_gpuobj *pgd, u32 pde,\r\nstruct nouveau_gpuobj *pgt[2])\r\n{\r\nu64 phys = 0xdeadcafe00000000ULL;\r\nu32 coverage = 0;\r\nif (pgt[0]) {\r\nphys = 0x00000003 | pgt[0]->vinst;\r\ncoverage = (pgt[0]->size >> 3) << 12;\r\n} else\r\nif (pgt[1]) {\r\nphys = 0x00000001 | pgt[1]->vinst;\r\ncoverage = (pgt[1]->size >> 3) << 16;\r\n}\r\nif (phys & 1) {\r\nif (coverage <= 32 * 1024 * 1024)\r\nphys |= 0x60;\r\nelse if (coverage <= 64 * 1024 * 1024)\r\nphys |= 0x40;\r\nelse if (coverage < 128 * 1024 * 1024)\r\nphys |= 0x20;\r\n}\r\nnv_wo32(pgd, (pde * 8) + 0, lower_32_bits(phys));\r\nnv_wo32(pgd, (pde * 8) + 4, upper_32_bits(phys));\r\n}\r\nstatic inline u64\r\nnv50_vm_addr(struct nouveau_vma *vma, u64 phys, u32 memtype, u32 target)\r\n{\r\nstruct drm_nouveau_private *dev_priv = vma->vm->dev->dev_private;\r\nphys |= 1;\r\nphys |= (u64)memtype << 40;\r\nif (target == 0 && dev_priv->vram_sys_base) {\r\nphys += dev_priv->vram_sys_base;\r\ntarget = 3;\r\n}\r\nphys |= target << 4;\r\nif (vma->access & NV_MEM_ACCESS_SYS)\r\nphys |= (1 << 6);\r\nif (!(vma->access & NV_MEM_ACCESS_WO))\r\nphys |= (1 << 3);\r\nreturn phys;\r\n}\r\nvoid\r\nnv50_vm_map(struct nouveau_vma *vma, struct nouveau_gpuobj *pgt,\r\nstruct nouveau_mem *mem, u32 pte, u32 cnt, u64 phys, u64 delta)\r\n{\r\nu32 comp = (mem->memtype & 0x180) >> 7;\r\nu32 block;\r\nint i;\r\nphys = nv50_vm_addr(vma, phys, mem->memtype, 0);\r\npte <<= 3;\r\ncnt <<= 3;\r\nwhile (cnt) {\r\nu32 offset_h = upper_32_bits(phys);\r\nu32 offset_l = lower_32_bits(phys);\r\nfor (i = 7; i >= 0; i--) {\r\nblock = 1 << (i + 3);\r\nif (cnt >= block && !(pte & (block - 1)))\r\nbreak;\r\n}\r\noffset_l |= (i << 7);\r\nphys += block << (vma->node->type - 3);\r\ncnt -= block;\r\nif (comp) {\r\nu32 tag = mem->tag->start + ((delta >> 16) * comp);\r\noffset_h |= (tag << 17);\r\ndelta += block << (vma->node->type - 3);\r\n}\r\nwhile (block) {\r\nnv_wo32(pgt, pte + 0, offset_l);\r\nnv_wo32(pgt, pte + 4, offset_h);\r\npte += 8;\r\nblock -= 8;\r\n}\r\n}\r\n}\r\nvoid\r\nnv50_vm_map_sg(struct nouveau_vma *vma, struct nouveau_gpuobj *pgt,\r\nstruct nouveau_mem *mem, u32 pte, u32 cnt, dma_addr_t *list)\r\n{\r\npte <<= 3;\r\nwhile (cnt--) {\r\nu64 phys = nv50_vm_addr(vma, (u64)*list++, mem->memtype, 2);\r\nnv_wo32(pgt, pte + 0, lower_32_bits(phys));\r\nnv_wo32(pgt, pte + 4, upper_32_bits(phys));\r\npte += 8;\r\n}\r\n}\r\nvoid\r\nnv50_vm_unmap(struct nouveau_gpuobj *pgt, u32 pte, u32 cnt)\r\n{\r\npte <<= 3;\r\nwhile (cnt--) {\r\nnv_wo32(pgt, pte + 0, 0x00000000);\r\nnv_wo32(pgt, pte + 4, 0x00000000);\r\npte += 8;\r\n}\r\n}\r\nvoid\r\nnv50_vm_flush(struct nouveau_vm *vm)\r\n{\r\nstruct drm_nouveau_private *dev_priv = vm->dev->dev_private;\r\nstruct nouveau_instmem_engine *pinstmem = &dev_priv->engine.instmem;\r\nstruct nouveau_fifo_engine *pfifo = &dev_priv->engine.fifo;\r\nint i;\r\npinstmem->flush(vm->dev);\r\nif (vm == dev_priv->bar1_vm || vm == dev_priv->bar3_vm) {\r\nnv50_vm_flush_engine(vm->dev, 6);\r\nreturn;\r\n}\r\npfifo->tlb_flush(vm->dev);\r\nfor (i = 0; i < NVOBJ_ENGINE_NR; i++) {\r\nif (atomic_read(&vm->engref[i]))\r\ndev_priv->eng[i]->tlb_flush(vm->dev, i);\r\n}\r\n}\r\nvoid\r\nnv50_vm_flush_engine(struct drm_device *dev, int engine)\r\n{\r\nstruct drm_nouveau_private *dev_priv = dev->dev_private;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev_priv->vm_lock, flags);\r\nnv_wr32(dev, 0x100c80, (engine << 16) | 1);\r\nif (!nv_wait(dev, 0x100c80, 0x00000001, 0x00000000))\r\nNV_ERROR(dev, "vm flush timeout: engine %d\n", engine);\r\nspin_unlock_irqrestore(&dev_priv->vm_lock, flags);\r\n}
