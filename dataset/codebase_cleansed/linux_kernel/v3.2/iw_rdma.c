static int rds_iw_add_cm_id(struct rds_iw_device *rds_iwdev, struct rdma_cm_id *cm_id)\r\n{\r\nstruct rds_iw_cm_id *i_cm_id;\r\ni_cm_id = kmalloc(sizeof *i_cm_id, GFP_KERNEL);\r\nif (!i_cm_id)\r\nreturn -ENOMEM;\r\ni_cm_id->cm_id = cm_id;\r\nspin_lock_irq(&rds_iwdev->spinlock);\r\nlist_add_tail(&i_cm_id->list, &rds_iwdev->cm_id_list);\r\nspin_unlock_irq(&rds_iwdev->spinlock);\r\nreturn 0;\r\n}\r\nstatic void rds_iw_remove_cm_id(struct rds_iw_device *rds_iwdev,\r\nstruct rdma_cm_id *cm_id)\r\n{\r\nstruct rds_iw_cm_id *i_cm_id;\r\nspin_lock_irq(&rds_iwdev->spinlock);\r\nlist_for_each_entry(i_cm_id, &rds_iwdev->cm_id_list, list) {\r\nif (i_cm_id->cm_id == cm_id) {\r\nlist_del(&i_cm_id->list);\r\nkfree(i_cm_id);\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irq(&rds_iwdev->spinlock);\r\n}\r\nint rds_iw_update_cm_id(struct rds_iw_device *rds_iwdev, struct rdma_cm_id *cm_id)\r\n{\r\nstruct sockaddr_in *src_addr, *dst_addr;\r\nstruct rds_iw_device *rds_iwdev_old;\r\nstruct rds_sock rs;\r\nstruct rdma_cm_id *pcm_id;\r\nint rc;\r\nsrc_addr = (struct sockaddr_in *)&cm_id->route.addr.src_addr;\r\ndst_addr = (struct sockaddr_in *)&cm_id->route.addr.dst_addr;\r\nrs.rs_bound_addr = src_addr->sin_addr.s_addr;\r\nrs.rs_bound_port = src_addr->sin_port;\r\nrs.rs_conn_addr = dst_addr->sin_addr.s_addr;\r\nrs.rs_conn_port = dst_addr->sin_port;\r\nrc = rds_iw_get_device(&rs, &rds_iwdev_old, &pcm_id);\r\nif (rc)\r\nrds_iw_remove_cm_id(rds_iwdev, cm_id);\r\nreturn rds_iw_add_cm_id(rds_iwdev, cm_id);\r\n}\r\nvoid rds_iw_add_conn(struct rds_iw_device *rds_iwdev, struct rds_connection *conn)\r\n{\r\nstruct rds_iw_connection *ic = conn->c_transport_data;\r\nspin_lock_irq(&iw_nodev_conns_lock);\r\nBUG_ON(list_empty(&iw_nodev_conns));\r\nBUG_ON(list_empty(&ic->iw_node));\r\nlist_del(&ic->iw_node);\r\nspin_lock(&rds_iwdev->spinlock);\r\nlist_add_tail(&ic->iw_node, &rds_iwdev->conn_list);\r\nspin_unlock(&rds_iwdev->spinlock);\r\nspin_unlock_irq(&iw_nodev_conns_lock);\r\nic->rds_iwdev = rds_iwdev;\r\n}\r\nvoid rds_iw_remove_conn(struct rds_iw_device *rds_iwdev, struct rds_connection *conn)\r\n{\r\nstruct rds_iw_connection *ic = conn->c_transport_data;\r\nspin_lock(&iw_nodev_conns_lock);\r\nspin_lock_irq(&rds_iwdev->spinlock);\r\nBUG_ON(list_empty(&ic->iw_node));\r\nlist_del(&ic->iw_node);\r\nspin_unlock_irq(&rds_iwdev->spinlock);\r\nlist_add_tail(&ic->iw_node, &iw_nodev_conns);\r\nspin_unlock(&iw_nodev_conns_lock);\r\nrds_iw_remove_cm_id(ic->rds_iwdev, ic->i_cm_id);\r\nic->rds_iwdev = NULL;\r\n}\r\nvoid __rds_iw_destroy_conns(struct list_head *list, spinlock_t *list_lock)\r\n{\r\nstruct rds_iw_connection *ic, *_ic;\r\nLIST_HEAD(tmp_list);\r\nspin_lock_irq(list_lock);\r\nlist_splice(list, &tmp_list);\r\nINIT_LIST_HEAD(list);\r\nspin_unlock_irq(list_lock);\r\nlist_for_each_entry_safe(ic, _ic, &tmp_list, iw_node)\r\nrds_conn_destroy(ic->conn);\r\n}\r\nstatic void rds_iw_set_scatterlist(struct rds_iw_scatterlist *sg,\r\nstruct scatterlist *list, unsigned int sg_len)\r\n{\r\nsg->list = list;\r\nsg->len = sg_len;\r\nsg->dma_len = 0;\r\nsg->dma_npages = 0;\r\nsg->bytes = 0;\r\n}\r\nstatic u64 *rds_iw_map_scatterlist(struct rds_iw_device *rds_iwdev,\r\nstruct rds_iw_scatterlist *sg)\r\n{\r\nstruct ib_device *dev = rds_iwdev->dev;\r\nu64 *dma_pages = NULL;\r\nint i, j, ret;\r\nWARN_ON(sg->dma_len);\r\nsg->dma_len = ib_dma_map_sg(dev, sg->list, sg->len, DMA_BIDIRECTIONAL);\r\nif (unlikely(!sg->dma_len)) {\r\nprintk(KERN_WARNING "RDS/IW: dma_map_sg failed!\n");\r\nreturn ERR_PTR(-EBUSY);\r\n}\r\nsg->bytes = 0;\r\nsg->dma_npages = 0;\r\nret = -EINVAL;\r\nfor (i = 0; i < sg->dma_len; ++i) {\r\nunsigned int dma_len = ib_sg_dma_len(dev, &sg->list[i]);\r\nu64 dma_addr = ib_sg_dma_address(dev, &sg->list[i]);\r\nu64 end_addr;\r\nsg->bytes += dma_len;\r\nend_addr = dma_addr + dma_len;\r\nif (dma_addr & PAGE_MASK) {\r\nif (i > 0)\r\ngoto out_unmap;\r\ndma_addr &= ~PAGE_MASK;\r\n}\r\nif (end_addr & PAGE_MASK) {\r\nif (i < sg->dma_len - 1)\r\ngoto out_unmap;\r\nend_addr = (end_addr + PAGE_MASK) & ~PAGE_MASK;\r\n}\r\nsg->dma_npages += (end_addr - dma_addr) >> PAGE_SHIFT;\r\n}\r\nif (sg->dma_npages > fastreg_message_size)\r\ngoto out_unmap;\r\ndma_pages = kmalloc(sizeof(u64) * sg->dma_npages, GFP_ATOMIC);\r\nif (!dma_pages) {\r\nret = -ENOMEM;\r\ngoto out_unmap;\r\n}\r\nfor (i = j = 0; i < sg->dma_len; ++i) {\r\nunsigned int dma_len = ib_sg_dma_len(dev, &sg->list[i]);\r\nu64 dma_addr = ib_sg_dma_address(dev, &sg->list[i]);\r\nu64 end_addr;\r\nend_addr = dma_addr + dma_len;\r\ndma_addr &= ~PAGE_MASK;\r\nfor (; dma_addr < end_addr; dma_addr += PAGE_SIZE)\r\ndma_pages[j++] = dma_addr;\r\nBUG_ON(j > sg->dma_npages);\r\n}\r\nreturn dma_pages;\r\nout_unmap:\r\nib_dma_unmap_sg(rds_iwdev->dev, sg->list, sg->len, DMA_BIDIRECTIONAL);\r\nsg->dma_len = 0;\r\nkfree(dma_pages);\r\nreturn ERR_PTR(ret);\r\n}\r\nstruct rds_iw_mr_pool *rds_iw_create_mr_pool(struct rds_iw_device *rds_iwdev)\r\n{\r\nstruct rds_iw_mr_pool *pool;\r\npool = kzalloc(sizeof(*pool), GFP_KERNEL);\r\nif (!pool) {\r\nprintk(KERN_WARNING "RDS/IW: rds_iw_create_mr_pool alloc error\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\npool->device = rds_iwdev;\r\nINIT_LIST_HEAD(&pool->dirty_list);\r\nINIT_LIST_HEAD(&pool->clean_list);\r\nmutex_init(&pool->flush_lock);\r\nspin_lock_init(&pool->list_lock);\r\nINIT_WORK(&pool->flush_worker, rds_iw_mr_pool_flush_worker);\r\npool->max_message_size = fastreg_message_size;\r\npool->max_items = fastreg_pool_size;\r\npool->max_free_pinned = pool->max_items * pool->max_message_size / 4;\r\npool->max_pages = fastreg_message_size;\r\npool->max_items_soft = pool->max_items * 3 / 4;\r\nreturn pool;\r\n}\r\nvoid rds_iw_get_mr_info(struct rds_iw_device *rds_iwdev, struct rds_info_rdma_connection *iinfo)\r\n{\r\nstruct rds_iw_mr_pool *pool = rds_iwdev->mr_pool;\r\niinfo->rdma_mr_max = pool->max_items;\r\niinfo->rdma_mr_size = pool->max_pages;\r\n}\r\nvoid rds_iw_destroy_mr_pool(struct rds_iw_mr_pool *pool)\r\n{\r\nflush_workqueue(rds_wq);\r\nrds_iw_flush_mr_pool(pool, 1);\r\nBUG_ON(atomic_read(&pool->item_count));\r\nBUG_ON(atomic_read(&pool->free_pinned));\r\nkfree(pool);\r\n}\r\nstatic inline struct rds_iw_mr *rds_iw_reuse_fmr(struct rds_iw_mr_pool *pool)\r\n{\r\nstruct rds_iw_mr *ibmr = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->list_lock, flags);\r\nif (!list_empty(&pool->clean_list)) {\r\nibmr = list_entry(pool->clean_list.next, struct rds_iw_mr, mapping.m_list);\r\nlist_del_init(&ibmr->mapping.m_list);\r\n}\r\nspin_unlock_irqrestore(&pool->list_lock, flags);\r\nreturn ibmr;\r\n}\r\nstatic struct rds_iw_mr *rds_iw_alloc_mr(struct rds_iw_device *rds_iwdev)\r\n{\r\nstruct rds_iw_mr_pool *pool = rds_iwdev->mr_pool;\r\nstruct rds_iw_mr *ibmr = NULL;\r\nint err = 0, iter = 0;\r\nwhile (1) {\r\nibmr = rds_iw_reuse_fmr(pool);\r\nif (ibmr)\r\nreturn ibmr;\r\nif (atomic_inc_return(&pool->item_count) <= pool->max_items)\r\nbreak;\r\natomic_dec(&pool->item_count);\r\nif (++iter > 2) {\r\nrds_iw_stats_inc(s_iw_rdma_mr_pool_depleted);\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nrds_iw_stats_inc(s_iw_rdma_mr_pool_wait);\r\nrds_iw_flush_mr_pool(pool, 0);\r\n}\r\nibmr = kzalloc(sizeof(*ibmr), GFP_KERNEL);\r\nif (!ibmr) {\r\nerr = -ENOMEM;\r\ngoto out_no_cigar;\r\n}\r\nspin_lock_init(&ibmr->mapping.m_lock);\r\nINIT_LIST_HEAD(&ibmr->mapping.m_list);\r\nibmr->mapping.m_mr = ibmr;\r\nerr = rds_iw_init_fastreg(pool, ibmr);\r\nif (err)\r\ngoto out_no_cigar;\r\nrds_iw_stats_inc(s_iw_rdma_mr_alloc);\r\nreturn ibmr;\r\nout_no_cigar:\r\nif (ibmr) {\r\nrds_iw_destroy_fastreg(pool, ibmr);\r\nkfree(ibmr);\r\n}\r\natomic_dec(&pool->item_count);\r\nreturn ERR_PTR(err);\r\n}\r\nvoid rds_iw_sync_mr(void *trans_private, int direction)\r\n{\r\nstruct rds_iw_mr *ibmr = trans_private;\r\nstruct rds_iw_device *rds_iwdev = ibmr->device;\r\nswitch (direction) {\r\ncase DMA_FROM_DEVICE:\r\nib_dma_sync_sg_for_cpu(rds_iwdev->dev, ibmr->mapping.m_sg.list,\r\nibmr->mapping.m_sg.dma_len, DMA_BIDIRECTIONAL);\r\nbreak;\r\ncase DMA_TO_DEVICE:\r\nib_dma_sync_sg_for_device(rds_iwdev->dev, ibmr->mapping.m_sg.list,\r\nibmr->mapping.m_sg.dma_len, DMA_BIDIRECTIONAL);\r\nbreak;\r\n}\r\n}\r\nstatic inline unsigned int rds_iw_flush_goal(struct rds_iw_mr_pool *pool, int free_all)\r\n{\r\nunsigned int item_count;\r\nitem_count = atomic_read(&pool->item_count);\r\nif (free_all)\r\nreturn item_count;\r\nreturn 0;\r\n}\r\nstatic int rds_iw_flush_mr_pool(struct rds_iw_mr_pool *pool, int free_all)\r\n{\r\nstruct rds_iw_mr *ibmr, *next;\r\nLIST_HEAD(unmap_list);\r\nLIST_HEAD(kill_list);\r\nunsigned long flags;\r\nunsigned int nfreed = 0, ncleaned = 0, unpinned = 0, free_goal;\r\nint ret = 0;\r\nrds_iw_stats_inc(s_iw_rdma_mr_pool_flush);\r\nmutex_lock(&pool->flush_lock);\r\nspin_lock_irqsave(&pool->list_lock, flags);\r\nlist_splice_init(&pool->dirty_list, &unmap_list);\r\nif (free_all)\r\nlist_splice_init(&pool->clean_list, &kill_list);\r\nspin_unlock_irqrestore(&pool->list_lock, flags);\r\nfree_goal = rds_iw_flush_goal(pool, free_all);\r\nif (!list_empty(&unmap_list)) {\r\nncleaned = rds_iw_unmap_fastreg_list(pool, &unmap_list,\r\n&kill_list, &unpinned);\r\nif (free_all)\r\nlist_splice_init(&unmap_list, &kill_list);\r\n}\r\nlist_for_each_entry_safe(ibmr, next, &kill_list, mapping.m_list) {\r\nrds_iw_stats_inc(s_iw_rdma_mr_free);\r\nlist_del(&ibmr->mapping.m_list);\r\nrds_iw_destroy_fastreg(pool, ibmr);\r\nkfree(ibmr);\r\nnfreed++;\r\n}\r\nif (!list_empty(&unmap_list)) {\r\nspin_lock_irqsave(&pool->list_lock, flags);\r\nlist_splice(&unmap_list, &pool->clean_list);\r\nspin_unlock_irqrestore(&pool->list_lock, flags);\r\n}\r\natomic_sub(unpinned, &pool->free_pinned);\r\natomic_sub(ncleaned, &pool->dirty_count);\r\natomic_sub(nfreed, &pool->item_count);\r\nmutex_unlock(&pool->flush_lock);\r\nreturn ret;\r\n}\r\nstatic void rds_iw_mr_pool_flush_worker(struct work_struct *work)\r\n{\r\nstruct rds_iw_mr_pool *pool = container_of(work, struct rds_iw_mr_pool, flush_worker);\r\nrds_iw_flush_mr_pool(pool, 0);\r\n}\r\nvoid rds_iw_free_mr(void *trans_private, int invalidate)\r\n{\r\nstruct rds_iw_mr *ibmr = trans_private;\r\nstruct rds_iw_mr_pool *pool = ibmr->device->mr_pool;\r\nrdsdebug("RDS/IW: free_mr nents %u\n", ibmr->mapping.m_sg.len);\r\nif (!pool)\r\nreturn;\r\nrds_iw_free_fastreg(pool, ibmr);\r\nif (atomic_read(&pool->free_pinned) >= pool->max_free_pinned ||\r\natomic_read(&pool->dirty_count) >= pool->max_items / 10)\r\nqueue_work(rds_wq, &pool->flush_worker);\r\nif (invalidate) {\r\nif (likely(!in_interrupt())) {\r\nrds_iw_flush_mr_pool(pool, 0);\r\n} else {\r\nqueue_work(rds_wq, &pool->flush_worker);\r\n}\r\n}\r\n}\r\nvoid rds_iw_flush_mrs(void)\r\n{\r\nstruct rds_iw_device *rds_iwdev;\r\nlist_for_each_entry(rds_iwdev, &rds_iw_devices, list) {\r\nstruct rds_iw_mr_pool *pool = rds_iwdev->mr_pool;\r\nif (pool)\r\nrds_iw_flush_mr_pool(pool, 0);\r\n}\r\n}\r\nvoid *rds_iw_get_mr(struct scatterlist *sg, unsigned long nents,\r\nstruct rds_sock *rs, u32 *key_ret)\r\n{\r\nstruct rds_iw_device *rds_iwdev;\r\nstruct rds_iw_mr *ibmr = NULL;\r\nstruct rdma_cm_id *cm_id;\r\nint ret;\r\nret = rds_iw_get_device(rs, &rds_iwdev, &cm_id);\r\nif (ret || !cm_id) {\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\nif (!rds_iwdev->mr_pool) {\r\nret = -ENODEV;\r\ngoto out;\r\n}\r\nibmr = rds_iw_alloc_mr(rds_iwdev);\r\nif (IS_ERR(ibmr))\r\nreturn ibmr;\r\nibmr->cm_id = cm_id;\r\nibmr->device = rds_iwdev;\r\nret = rds_iw_map_fastreg(rds_iwdev->mr_pool, ibmr, sg, nents);\r\nif (ret == 0)\r\n*key_ret = ibmr->mr->rkey;\r\nelse\r\nprintk(KERN_WARNING "RDS/IW: failed to map mr (errno=%d)\n", ret);\r\nout:\r\nif (ret) {\r\nif (ibmr)\r\nrds_iw_free_mr(ibmr, 0);\r\nibmr = ERR_PTR(ret);\r\n}\r\nreturn ibmr;\r\n}\r\nstatic int rds_iw_init_fastreg(struct rds_iw_mr_pool *pool,\r\nstruct rds_iw_mr *ibmr)\r\n{\r\nstruct rds_iw_device *rds_iwdev = pool->device;\r\nstruct ib_fast_reg_page_list *page_list = NULL;\r\nstruct ib_mr *mr;\r\nint err;\r\nmr = ib_alloc_fast_reg_mr(rds_iwdev->pd, pool->max_message_size);\r\nif (IS_ERR(mr)) {\r\nerr = PTR_ERR(mr);\r\nprintk(KERN_WARNING "RDS/IW: ib_alloc_fast_reg_mr failed (err=%d)\n", err);\r\nreturn err;\r\n}\r\npage_list = ib_alloc_fast_reg_page_list(rds_iwdev->dev, pool->max_message_size);\r\nif (IS_ERR(page_list)) {\r\nerr = PTR_ERR(page_list);\r\nprintk(KERN_WARNING "RDS/IW: ib_alloc_fast_reg_page_list failed (err=%d)\n", err);\r\nib_dereg_mr(mr);\r\nreturn err;\r\n}\r\nibmr->page_list = page_list;\r\nibmr->mr = mr;\r\nreturn 0;\r\n}\r\nstatic int rds_iw_rdma_build_fastreg(struct rds_iw_mapping *mapping)\r\n{\r\nstruct rds_iw_mr *ibmr = mapping->m_mr;\r\nstruct ib_send_wr f_wr, *failed_wr;\r\nint ret;\r\nib_update_fast_reg_key(ibmr->mr, ibmr->remap_count++);\r\nmapping->m_rkey = ibmr->mr->rkey;\r\nmemset(&f_wr, 0, sizeof(f_wr));\r\nf_wr.wr_id = RDS_IW_FAST_REG_WR_ID;\r\nf_wr.opcode = IB_WR_FAST_REG_MR;\r\nf_wr.wr.fast_reg.length = mapping->m_sg.bytes;\r\nf_wr.wr.fast_reg.rkey = mapping->m_rkey;\r\nf_wr.wr.fast_reg.page_list = ibmr->page_list;\r\nf_wr.wr.fast_reg.page_list_len = mapping->m_sg.dma_len;\r\nf_wr.wr.fast_reg.page_shift = PAGE_SHIFT;\r\nf_wr.wr.fast_reg.access_flags = IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE;\r\nf_wr.wr.fast_reg.iova_start = 0;\r\nf_wr.send_flags = IB_SEND_SIGNALED;\r\nfailed_wr = &f_wr;\r\nret = ib_post_send(ibmr->cm_id->qp, &f_wr, &failed_wr);\r\nBUG_ON(failed_wr != &f_wr);\r\nif (ret)\r\nprintk_ratelimited(KERN_WARNING "RDS/IW: %s:%d ib_post_send returned %d\n",\r\n__func__, __LINE__, ret);\r\nreturn ret;\r\n}\r\nstatic int rds_iw_rdma_fastreg_inv(struct rds_iw_mr *ibmr)\r\n{\r\nstruct ib_send_wr s_wr, *failed_wr;\r\nint ret = 0;\r\nif (!ibmr->cm_id->qp || !ibmr->mr)\r\ngoto out;\r\nmemset(&s_wr, 0, sizeof(s_wr));\r\ns_wr.wr_id = RDS_IW_LOCAL_INV_WR_ID;\r\ns_wr.opcode = IB_WR_LOCAL_INV;\r\ns_wr.ex.invalidate_rkey = ibmr->mr->rkey;\r\ns_wr.send_flags = IB_SEND_SIGNALED;\r\nfailed_wr = &s_wr;\r\nret = ib_post_send(ibmr->cm_id->qp, &s_wr, &failed_wr);\r\nif (ret) {\r\nprintk_ratelimited(KERN_WARNING "RDS/IW: %s:%d ib_post_send returned %d\n",\r\n__func__, __LINE__, ret);\r\ngoto out;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int rds_iw_map_fastreg(struct rds_iw_mr_pool *pool,\r\nstruct rds_iw_mr *ibmr,\r\nstruct scatterlist *sg,\r\nunsigned int sg_len)\r\n{\r\nstruct rds_iw_device *rds_iwdev = pool->device;\r\nstruct rds_iw_mapping *mapping = &ibmr->mapping;\r\nu64 *dma_pages;\r\nint i, ret = 0;\r\nrds_iw_set_scatterlist(&mapping->m_sg, sg, sg_len);\r\ndma_pages = rds_iw_map_scatterlist(rds_iwdev, &mapping->m_sg);\r\nif (IS_ERR(dma_pages)) {\r\nret = PTR_ERR(dma_pages);\r\ndma_pages = NULL;\r\ngoto out;\r\n}\r\nif (mapping->m_sg.dma_len > pool->max_message_size) {\r\nret = -EMSGSIZE;\r\ngoto out;\r\n}\r\nfor (i = 0; i < mapping->m_sg.dma_npages; ++i)\r\nibmr->page_list->page_list[i] = dma_pages[i];\r\nret = rds_iw_rdma_build_fastreg(mapping);\r\nif (ret)\r\ngoto out;\r\nrds_iw_stats_inc(s_iw_rdma_mr_used);\r\nout:\r\nkfree(dma_pages);\r\nreturn ret;\r\n}\r\nstatic void rds_iw_free_fastreg(struct rds_iw_mr_pool *pool,\r\nstruct rds_iw_mr *ibmr)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nif (!ibmr->mapping.m_sg.dma_len)\r\nreturn;\r\nret = rds_iw_rdma_fastreg_inv(ibmr);\r\nif (ret)\r\nreturn;\r\nspin_lock_irqsave(&pool->list_lock, flags);\r\nlist_add_tail(&ibmr->mapping.m_list, &pool->dirty_list);\r\natomic_add(ibmr->mapping.m_sg.len, &pool->free_pinned);\r\natomic_inc(&pool->dirty_count);\r\nspin_unlock_irqrestore(&pool->list_lock, flags);\r\n}\r\nstatic unsigned int rds_iw_unmap_fastreg_list(struct rds_iw_mr_pool *pool,\r\nstruct list_head *unmap_list,\r\nstruct list_head *kill_list,\r\nint *unpinned)\r\n{\r\nstruct rds_iw_mapping *mapping, *next;\r\nunsigned int ncleaned = 0;\r\nLIST_HEAD(laundered);\r\nwhile (!list_empty(unmap_list)) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->list_lock, flags);\r\nlist_for_each_entry_safe(mapping, next, unmap_list, m_list) {\r\n*unpinned += mapping->m_sg.len;\r\nlist_move(&mapping->m_list, &laundered);\r\nncleaned++;\r\n}\r\nspin_unlock_irqrestore(&pool->list_lock, flags);\r\n}\r\nlist_splice_init(&laundered, unmap_list);\r\nreturn ncleaned;\r\n}\r\nstatic void rds_iw_destroy_fastreg(struct rds_iw_mr_pool *pool,\r\nstruct rds_iw_mr *ibmr)\r\n{\r\nif (ibmr->page_list)\r\nib_free_fast_reg_page_list(ibmr->page_list);\r\nif (ibmr->mr)\r\nib_dereg_mr(ibmr->mr);\r\n}
