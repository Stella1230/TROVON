static u64 get_nsecs(void)\r\n{\r\nstruct timespec ts;\r\nclock_gettime(CLOCK_MONOTONIC, &ts);\r\nreturn ts.tv_sec * 1000000000ULL + ts.tv_nsec;\r\n}\r\nstatic void burn_nsecs(u64 nsecs)\r\n{\r\nu64 T0 = get_nsecs(), T1;\r\ndo {\r\nT1 = get_nsecs();\r\n} while (T1 + run_measurement_overhead < T0 + nsecs);\r\n}\r\nstatic void sleep_nsecs(u64 nsecs)\r\n{\r\nstruct timespec ts;\r\nts.tv_nsec = nsecs % 999999999;\r\nts.tv_sec = nsecs / 999999999;\r\nnanosleep(&ts, NULL);\r\n}\r\nstatic void calibrate_run_measurement_overhead(void)\r\n{\r\nu64 T0, T1, delta, min_delta = 1000000000ULL;\r\nint i;\r\nfor (i = 0; i < 10; i++) {\r\nT0 = get_nsecs();\r\nburn_nsecs(0);\r\nT1 = get_nsecs();\r\ndelta = T1-T0;\r\nmin_delta = min(min_delta, delta);\r\n}\r\nrun_measurement_overhead = min_delta;\r\nprintf("run measurement overhead: %" PRIu64 " nsecs\n", min_delta);\r\n}\r\nstatic void calibrate_sleep_measurement_overhead(void)\r\n{\r\nu64 T0, T1, delta, min_delta = 1000000000ULL;\r\nint i;\r\nfor (i = 0; i < 10; i++) {\r\nT0 = get_nsecs();\r\nsleep_nsecs(10000);\r\nT1 = get_nsecs();\r\ndelta = T1-T0;\r\nmin_delta = min(min_delta, delta);\r\n}\r\nmin_delta -= 10000;\r\nsleep_measurement_overhead = min_delta;\r\nprintf("sleep measurement overhead: %" PRIu64 " nsecs\n", min_delta);\r\n}\r\nstatic struct sched_atom *\r\nget_new_event(struct task_desc *task, u64 timestamp)\r\n{\r\nstruct sched_atom *event = zalloc(sizeof(*event));\r\nunsigned long idx = task->nr_events;\r\nsize_t size;\r\nevent->timestamp = timestamp;\r\nevent->nr = idx;\r\ntask->nr_events++;\r\nsize = sizeof(struct sched_atom *) * task->nr_events;\r\ntask->atoms = realloc(task->atoms, size);\r\nBUG_ON(!task->atoms);\r\ntask->atoms[idx] = event;\r\nreturn event;\r\n}\r\nstatic struct sched_atom *last_event(struct task_desc *task)\r\n{\r\nif (!task->nr_events)\r\nreturn NULL;\r\nreturn task->atoms[task->nr_events - 1];\r\n}\r\nstatic void\r\nadd_sched_event_run(struct task_desc *task, u64 timestamp, u64 duration)\r\n{\r\nstruct sched_atom *event, *curr_event = last_event(task);\r\nif (curr_event && curr_event->type == SCHED_EVENT_RUN) {\r\nnr_run_events_optimized++;\r\ncurr_event->duration += duration;\r\nreturn;\r\n}\r\nevent = get_new_event(task, timestamp);\r\nevent->type = SCHED_EVENT_RUN;\r\nevent->duration = duration;\r\nnr_run_events++;\r\n}\r\nstatic void\r\nadd_sched_event_wakeup(struct task_desc *task, u64 timestamp,\r\nstruct task_desc *wakee)\r\n{\r\nstruct sched_atom *event, *wakee_event;\r\nevent = get_new_event(task, timestamp);\r\nevent->type = SCHED_EVENT_WAKEUP;\r\nevent->wakee = wakee;\r\nwakee_event = last_event(wakee);\r\nif (!wakee_event || wakee_event->type != SCHED_EVENT_SLEEP) {\r\ntargetless_wakeups++;\r\nreturn;\r\n}\r\nif (wakee_event->wait_sem) {\r\nmultitarget_wakeups++;\r\nreturn;\r\n}\r\nwakee_event->wait_sem = zalloc(sizeof(*wakee_event->wait_sem));\r\nsem_init(wakee_event->wait_sem, 0, 0);\r\nwakee_event->specific_wait = 1;\r\nevent->wait_sem = wakee_event->wait_sem;\r\nnr_wakeup_events++;\r\n}\r\nstatic void\r\nadd_sched_event_sleep(struct task_desc *task, u64 timestamp,\r\nu64 task_state __used)\r\n{\r\nstruct sched_atom *event = get_new_event(task, timestamp);\r\nevent->type = SCHED_EVENT_SLEEP;\r\nnr_sleep_events++;\r\n}\r\nstatic struct task_desc *register_pid(unsigned long pid, const char *comm)\r\n{\r\nstruct task_desc *task;\r\nBUG_ON(pid >= MAX_PID);\r\ntask = pid_to_task[pid];\r\nif (task)\r\nreturn task;\r\ntask = zalloc(sizeof(*task));\r\ntask->pid = pid;\r\ntask->nr = nr_tasks;\r\nstrcpy(task->comm, comm);\r\nadd_sched_event_sleep(task, 0, 0);\r\npid_to_task[pid] = task;\r\nnr_tasks++;\r\ntasks = realloc(tasks, nr_tasks*sizeof(struct task_task *));\r\nBUG_ON(!tasks);\r\ntasks[task->nr] = task;\r\nif (verbose)\r\nprintf("registered task #%ld, PID %ld (%s)\n", nr_tasks, pid, comm);\r\nreturn task;\r\n}\r\nstatic void print_task_traces(void)\r\n{\r\nstruct task_desc *task;\r\nunsigned long i;\r\nfor (i = 0; i < nr_tasks; i++) {\r\ntask = tasks[i];\r\nprintf("task %6ld (%20s:%10ld), nr_events: %ld\n",\r\ntask->nr, task->comm, task->pid, task->nr_events);\r\n}\r\n}\r\nstatic void add_cross_task_wakeups(void)\r\n{\r\nstruct task_desc *task1, *task2;\r\nunsigned long i, j;\r\nfor (i = 0; i < nr_tasks; i++) {\r\ntask1 = tasks[i];\r\nj = i + 1;\r\nif (j == nr_tasks)\r\nj = 0;\r\ntask2 = tasks[j];\r\nadd_sched_event_wakeup(task1, 0, task2);\r\n}\r\n}\r\nstatic void\r\nprocess_sched_event(struct task_desc *this_task __used, struct sched_atom *atom)\r\n{\r\nint ret = 0;\r\nswitch (atom->type) {\r\ncase SCHED_EVENT_RUN:\r\nburn_nsecs(atom->duration);\r\nbreak;\r\ncase SCHED_EVENT_SLEEP:\r\nif (atom->wait_sem)\r\nret = sem_wait(atom->wait_sem);\r\nBUG_ON(ret);\r\nbreak;\r\ncase SCHED_EVENT_WAKEUP:\r\nif (atom->wait_sem)\r\nret = sem_post(atom->wait_sem);\r\nBUG_ON(ret);\r\nbreak;\r\ncase SCHED_EVENT_MIGRATION:\r\nbreak;\r\ndefault:\r\nBUG_ON(1);\r\n}\r\n}\r\nstatic u64 get_cpu_usage_nsec_parent(void)\r\n{\r\nstruct rusage ru;\r\nu64 sum;\r\nint err;\r\nerr = getrusage(RUSAGE_SELF, &ru);\r\nBUG_ON(err);\r\nsum = ru.ru_utime.tv_sec*1e9 + ru.ru_utime.tv_usec*1e3;\r\nsum += ru.ru_stime.tv_sec*1e9 + ru.ru_stime.tv_usec*1e3;\r\nreturn sum;\r\n}\r\nstatic int self_open_counters(void)\r\n{\r\nstruct perf_event_attr attr;\r\nint fd;\r\nmemset(&attr, 0, sizeof(attr));\r\nattr.type = PERF_TYPE_SOFTWARE;\r\nattr.config = PERF_COUNT_SW_TASK_CLOCK;\r\nfd = sys_perf_event_open(&attr, 0, -1, -1, 0);\r\nif (fd < 0)\r\ndie("Error: sys_perf_event_open() syscall returned"\r\n"with %d (%s)\n", fd, strerror(errno));\r\nreturn fd;\r\n}\r\nstatic u64 get_cpu_usage_nsec_self(int fd)\r\n{\r\nu64 runtime;\r\nint ret;\r\nret = read(fd, &runtime, sizeof(runtime));\r\nBUG_ON(ret != sizeof(runtime));\r\nreturn runtime;\r\n}\r\nstatic void *thread_func(void *ctx)\r\n{\r\nstruct task_desc *this_task = ctx;\r\nu64 cpu_usage_0, cpu_usage_1;\r\nunsigned long i, ret;\r\nchar comm2[22];\r\nint fd;\r\nsprintf(comm2, ":%s", this_task->comm);\r\nprctl(PR_SET_NAME, comm2);\r\nfd = self_open_counters();\r\nagain:\r\nret = sem_post(&this_task->ready_for_work);\r\nBUG_ON(ret);\r\nret = pthread_mutex_lock(&start_work_mutex);\r\nBUG_ON(ret);\r\nret = pthread_mutex_unlock(&start_work_mutex);\r\nBUG_ON(ret);\r\ncpu_usage_0 = get_cpu_usage_nsec_self(fd);\r\nfor (i = 0; i < this_task->nr_events; i++) {\r\nthis_task->curr_event = i;\r\nprocess_sched_event(this_task, this_task->atoms[i]);\r\n}\r\ncpu_usage_1 = get_cpu_usage_nsec_self(fd);\r\nthis_task->cpu_usage = cpu_usage_1 - cpu_usage_0;\r\nret = sem_post(&this_task->work_done_sem);\r\nBUG_ON(ret);\r\nret = pthread_mutex_lock(&work_done_wait_mutex);\r\nBUG_ON(ret);\r\nret = pthread_mutex_unlock(&work_done_wait_mutex);\r\nBUG_ON(ret);\r\ngoto again;\r\n}\r\nstatic void create_tasks(void)\r\n{\r\nstruct task_desc *task;\r\npthread_attr_t attr;\r\nunsigned long i;\r\nint err;\r\nerr = pthread_attr_init(&attr);\r\nBUG_ON(err);\r\nerr = pthread_attr_setstacksize(&attr,\r\n(size_t) max(16 * 1024, PTHREAD_STACK_MIN));\r\nBUG_ON(err);\r\nerr = pthread_mutex_lock(&start_work_mutex);\r\nBUG_ON(err);\r\nerr = pthread_mutex_lock(&work_done_wait_mutex);\r\nBUG_ON(err);\r\nfor (i = 0; i < nr_tasks; i++) {\r\ntask = tasks[i];\r\nsem_init(&task->sleep_sem, 0, 0);\r\nsem_init(&task->ready_for_work, 0, 0);\r\nsem_init(&task->work_done_sem, 0, 0);\r\ntask->curr_event = 0;\r\nerr = pthread_create(&task->thread, &attr, thread_func, task);\r\nBUG_ON(err);\r\n}\r\n}\r\nstatic void wait_for_tasks(void)\r\n{\r\nu64 cpu_usage_0, cpu_usage_1;\r\nstruct task_desc *task;\r\nunsigned long i, ret;\r\nstart_time = get_nsecs();\r\ncpu_usage = 0;\r\npthread_mutex_unlock(&work_done_wait_mutex);\r\nfor (i = 0; i < nr_tasks; i++) {\r\ntask = tasks[i];\r\nret = sem_wait(&task->ready_for_work);\r\nBUG_ON(ret);\r\nsem_init(&task->ready_for_work, 0, 0);\r\n}\r\nret = pthread_mutex_lock(&work_done_wait_mutex);\r\nBUG_ON(ret);\r\ncpu_usage_0 = get_cpu_usage_nsec_parent();\r\npthread_mutex_unlock(&start_work_mutex);\r\nfor (i = 0; i < nr_tasks; i++) {\r\ntask = tasks[i];\r\nret = sem_wait(&task->work_done_sem);\r\nBUG_ON(ret);\r\nsem_init(&task->work_done_sem, 0, 0);\r\ncpu_usage += task->cpu_usage;\r\ntask->cpu_usage = 0;\r\n}\r\ncpu_usage_1 = get_cpu_usage_nsec_parent();\r\nif (!runavg_cpu_usage)\r\nrunavg_cpu_usage = cpu_usage;\r\nrunavg_cpu_usage = (runavg_cpu_usage*9 + cpu_usage)/10;\r\nparent_cpu_usage = cpu_usage_1 - cpu_usage_0;\r\nif (!runavg_parent_cpu_usage)\r\nrunavg_parent_cpu_usage = parent_cpu_usage;\r\nrunavg_parent_cpu_usage = (runavg_parent_cpu_usage*9 +\r\nparent_cpu_usage)/10;\r\nret = pthread_mutex_lock(&start_work_mutex);\r\nBUG_ON(ret);\r\nfor (i = 0; i < nr_tasks; i++) {\r\ntask = tasks[i];\r\nsem_init(&task->sleep_sem, 0, 0);\r\ntask->curr_event = 0;\r\n}\r\n}\r\nstatic void run_one_test(void)\r\n{\r\nu64 T0, T1, delta, avg_delta, fluct;\r\nT0 = get_nsecs();\r\nwait_for_tasks();\r\nT1 = get_nsecs();\r\ndelta = T1 - T0;\r\nsum_runtime += delta;\r\nnr_runs++;\r\navg_delta = sum_runtime / nr_runs;\r\nif (delta < avg_delta)\r\nfluct = avg_delta - delta;\r\nelse\r\nfluct = delta - avg_delta;\r\nsum_fluct += fluct;\r\nif (!run_avg)\r\nrun_avg = delta;\r\nrun_avg = (run_avg*9 + delta)/10;\r\nprintf("#%-3ld: %0.3f, ",\r\nnr_runs, (double)delta/1000000.0);\r\nprintf("ravg: %0.2f, ",\r\n(double)run_avg/1e6);\r\nprintf("cpu: %0.2f / %0.2f",\r\n(double)cpu_usage/1e6, (double)runavg_cpu_usage/1e6);\r\n#if 0\r\nprintf(" [%0.2f / %0.2f]",\r\n(double)parent_cpu_usage/1e6,\r\n(double)runavg_parent_cpu_usage/1e6);\r\n#endif\r\nprintf("\n");\r\nif (nr_sleep_corrections)\r\nprintf(" (%ld sleep corrections)\n", nr_sleep_corrections);\r\nnr_sleep_corrections = 0;\r\n}\r\nstatic void test_calibrations(void)\r\n{\r\nu64 T0, T1;\r\nT0 = get_nsecs();\r\nburn_nsecs(1e6);\r\nT1 = get_nsecs();\r\nprintf("the run test took %" PRIu64 " nsecs\n", T1 - T0);\r\nT0 = get_nsecs();\r\nsleep_nsecs(1e6);\r\nT1 = get_nsecs();\r\nprintf("the sleep test took %" PRIu64 " nsecs\n", T1 - T0);\r\n}\r\nstatic void\r\nreplay_wakeup_event(struct trace_wakeup_event *wakeup_event,\r\nstruct perf_session *session __used,\r\nstruct event *event,\r\nint cpu __used,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\nstruct task_desc *waker, *wakee;\r\nif (verbose) {\r\nprintf("sched_wakeup event %p\n", event);\r\nprintf(" ... pid %d woke up %s/%d\n",\r\nwakeup_event->common_pid,\r\nwakeup_event->comm,\r\nwakeup_event->pid);\r\n}\r\nwaker = register_pid(wakeup_event->common_pid, "<unknown>");\r\nwakee = register_pid(wakeup_event->pid, wakeup_event->comm);\r\nadd_sched_event_wakeup(waker, timestamp, wakee);\r\n}\r\nstatic void\r\nreplay_switch_event(struct trace_switch_event *switch_event,\r\nstruct perf_session *session __used,\r\nstruct event *event,\r\nint cpu,\r\nu64 timestamp,\r\nstruct thread *thread __used)\r\n{\r\nstruct task_desc *prev, __used *next;\r\nu64 timestamp0;\r\ns64 delta;\r\nif (verbose)\r\nprintf("sched_switch event %p\n", event);\r\nif (cpu >= MAX_CPUS || cpu < 0)\r\nreturn;\r\ntimestamp0 = cpu_last_switched[cpu];\r\nif (timestamp0)\r\ndelta = timestamp - timestamp0;\r\nelse\r\ndelta = 0;\r\nif (delta < 0)\r\ndie("hm, delta: %" PRIu64 " < 0 ?\n", delta);\r\nif (verbose) {\r\nprintf(" ... switch from %s/%d to %s/%d [ran %" PRIu64 " nsecs]\n",\r\nswitch_event->prev_comm, switch_event->prev_pid,\r\nswitch_event->next_comm, switch_event->next_pid,\r\ndelta);\r\n}\r\nprev = register_pid(switch_event->prev_pid, switch_event->prev_comm);\r\nnext = register_pid(switch_event->next_pid, switch_event->next_comm);\r\ncpu_last_switched[cpu] = timestamp;\r\nadd_sched_event_run(prev, timestamp, delta);\r\nadd_sched_event_sleep(prev, timestamp, switch_event->prev_state);\r\n}\r\nstatic void\r\nreplay_fork_event(struct trace_fork_event *fork_event,\r\nstruct event *event,\r\nint cpu __used,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\nif (verbose) {\r\nprintf("sched_fork event %p\n", event);\r\nprintf("... parent: %s/%d\n", fork_event->parent_comm, fork_event->parent_pid);\r\nprintf("... child: %s/%d\n", fork_event->child_comm, fork_event->child_pid);\r\n}\r\nregister_pid(fork_event->parent_pid, fork_event->parent_comm);\r\nregister_pid(fork_event->child_pid, fork_event->child_comm);\r\n}\r\nstatic int\r\nthread_lat_cmp(struct list_head *list, struct work_atoms *l, struct work_atoms *r)\r\n{\r\nstruct sort_dimension *sort;\r\nint ret = 0;\r\nBUG_ON(list_empty(list));\r\nlist_for_each_entry(sort, list, list) {\r\nret = sort->cmp(l, r);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn ret;\r\n}\r\nstatic struct work_atoms *\r\nthread_atoms_search(struct rb_root *root, struct thread *thread,\r\nstruct list_head *sort_list)\r\n{\r\nstruct rb_node *node = root->rb_node;\r\nstruct work_atoms key = { .thread = thread };\r\nwhile (node) {\r\nstruct work_atoms *atoms;\r\nint cmp;\r\natoms = container_of(node, struct work_atoms, node);\r\ncmp = thread_lat_cmp(sort_list, &key, atoms);\r\nif (cmp > 0)\r\nnode = node->rb_left;\r\nelse if (cmp < 0)\r\nnode = node->rb_right;\r\nelse {\r\nBUG_ON(thread != atoms->thread);\r\nreturn atoms;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic void\r\n__thread_latency_insert(struct rb_root *root, struct work_atoms *data,\r\nstruct list_head *sort_list)\r\n{\r\nstruct rb_node **new = &(root->rb_node), *parent = NULL;\r\nwhile (*new) {\r\nstruct work_atoms *this;\r\nint cmp;\r\nthis = container_of(*new, struct work_atoms, node);\r\nparent = *new;\r\ncmp = thread_lat_cmp(sort_list, data, this);\r\nif (cmp > 0)\r\nnew = &((*new)->rb_left);\r\nelse\r\nnew = &((*new)->rb_right);\r\n}\r\nrb_link_node(&data->node, parent, new);\r\nrb_insert_color(&data->node, root);\r\n}\r\nstatic void thread_atoms_insert(struct thread *thread)\r\n{\r\nstruct work_atoms *atoms = zalloc(sizeof(*atoms));\r\nif (!atoms)\r\ndie("No memory");\r\natoms->thread = thread;\r\nINIT_LIST_HEAD(&atoms->work_list);\r\n__thread_latency_insert(&atom_root, atoms, &cmp_pid);\r\n}\r\nstatic void\r\nlatency_fork_event(struct trace_fork_event *fork_event __used,\r\nstruct event *event __used,\r\nint cpu __used,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\n}\r\n__used\r\nstatic char sched_out_state(struct trace_switch_event *switch_event)\r\n{\r\nconst char *str = TASK_STATE_TO_CHAR_STR;\r\nreturn str[switch_event->prev_state];\r\n}\r\nstatic void\r\nadd_sched_out_event(struct work_atoms *atoms,\r\nchar run_state,\r\nu64 timestamp)\r\n{\r\nstruct work_atom *atom = zalloc(sizeof(*atom));\r\nif (!atom)\r\ndie("Non memory");\r\natom->sched_out_time = timestamp;\r\nif (run_state == 'R') {\r\natom->state = THREAD_WAIT_CPU;\r\natom->wake_up_time = atom->sched_out_time;\r\n}\r\nlist_add_tail(&atom->list, &atoms->work_list);\r\n}\r\nstatic void\r\nadd_runtime_event(struct work_atoms *atoms, u64 delta, u64 timestamp __used)\r\n{\r\nstruct work_atom *atom;\r\nBUG_ON(list_empty(&atoms->work_list));\r\natom = list_entry(atoms->work_list.prev, struct work_atom, list);\r\natom->runtime += delta;\r\natoms->total_runtime += delta;\r\n}\r\nstatic void\r\nadd_sched_in_event(struct work_atoms *atoms, u64 timestamp)\r\n{\r\nstruct work_atom *atom;\r\nu64 delta;\r\nif (list_empty(&atoms->work_list))\r\nreturn;\r\natom = list_entry(atoms->work_list.prev, struct work_atom, list);\r\nif (atom->state != THREAD_WAIT_CPU)\r\nreturn;\r\nif (timestamp < atom->wake_up_time) {\r\natom->state = THREAD_IGNORE;\r\nreturn;\r\n}\r\natom->state = THREAD_SCHED_IN;\r\natom->sched_in_time = timestamp;\r\ndelta = atom->sched_in_time - atom->wake_up_time;\r\natoms->total_lat += delta;\r\nif (delta > atoms->max_lat) {\r\natoms->max_lat = delta;\r\natoms->max_lat_at = timestamp;\r\n}\r\natoms->nb_atoms++;\r\n}\r\nstatic void\r\nlatency_switch_event(struct trace_switch_event *switch_event,\r\nstruct perf_session *session,\r\nstruct event *event __used,\r\nint cpu,\r\nu64 timestamp,\r\nstruct thread *thread __used)\r\n{\r\nstruct work_atoms *out_events, *in_events;\r\nstruct thread *sched_out, *sched_in;\r\nu64 timestamp0;\r\ns64 delta;\r\nBUG_ON(cpu >= MAX_CPUS || cpu < 0);\r\ntimestamp0 = cpu_last_switched[cpu];\r\ncpu_last_switched[cpu] = timestamp;\r\nif (timestamp0)\r\ndelta = timestamp - timestamp0;\r\nelse\r\ndelta = 0;\r\nif (delta < 0)\r\ndie("hm, delta: %" PRIu64 " < 0 ?\n", delta);\r\nsched_out = perf_session__findnew(session, switch_event->prev_pid);\r\nsched_in = perf_session__findnew(session, switch_event->next_pid);\r\nout_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);\r\nif (!out_events) {\r\nthread_atoms_insert(sched_out);\r\nout_events = thread_atoms_search(&atom_root, sched_out, &cmp_pid);\r\nif (!out_events)\r\ndie("out-event: Internal tree error");\r\n}\r\nadd_sched_out_event(out_events, sched_out_state(switch_event), timestamp);\r\nin_events = thread_atoms_search(&atom_root, sched_in, &cmp_pid);\r\nif (!in_events) {\r\nthread_atoms_insert(sched_in);\r\nin_events = thread_atoms_search(&atom_root, sched_in, &cmp_pid);\r\nif (!in_events)\r\ndie("in-event: Internal tree error");\r\nadd_sched_out_event(in_events, 'R', timestamp);\r\n}\r\nadd_sched_in_event(in_events, timestamp);\r\n}\r\nstatic void\r\nlatency_runtime_event(struct trace_runtime_event *runtime_event,\r\nstruct perf_session *session,\r\nstruct event *event __used,\r\nint cpu,\r\nu64 timestamp,\r\nstruct thread *this_thread __used)\r\n{\r\nstruct thread *thread = perf_session__findnew(session, runtime_event->pid);\r\nstruct work_atoms *atoms = thread_atoms_search(&atom_root, thread, &cmp_pid);\r\nBUG_ON(cpu >= MAX_CPUS || cpu < 0);\r\nif (!atoms) {\r\nthread_atoms_insert(thread);\r\natoms = thread_atoms_search(&atom_root, thread, &cmp_pid);\r\nif (!atoms)\r\ndie("in-event: Internal tree error");\r\nadd_sched_out_event(atoms, 'R', timestamp);\r\n}\r\nadd_runtime_event(atoms, runtime_event->runtime, timestamp);\r\n}\r\nstatic void\r\nlatency_wakeup_event(struct trace_wakeup_event *wakeup_event,\r\nstruct perf_session *session,\r\nstruct event *__event __used,\r\nint cpu __used,\r\nu64 timestamp,\r\nstruct thread *thread __used)\r\n{\r\nstruct work_atoms *atoms;\r\nstruct work_atom *atom;\r\nstruct thread *wakee;\r\nif (!wakeup_event->success)\r\nreturn;\r\nwakee = perf_session__findnew(session, wakeup_event->pid);\r\natoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);\r\nif (!atoms) {\r\nthread_atoms_insert(wakee);\r\natoms = thread_atoms_search(&atom_root, wakee, &cmp_pid);\r\nif (!atoms)\r\ndie("wakeup-event: Internal tree error");\r\nadd_sched_out_event(atoms, 'S', timestamp);\r\n}\r\nBUG_ON(list_empty(&atoms->work_list));\r\natom = list_entry(atoms->work_list.prev, struct work_atom, list);\r\nif (profile_cpu == -1 && atom->state != THREAD_SLEEPING)\r\nnr_state_machine_bugs++;\r\nnr_timestamps++;\r\nif (atom->sched_out_time > timestamp) {\r\nnr_unordered_timestamps++;\r\nreturn;\r\n}\r\natom->state = THREAD_WAIT_CPU;\r\natom->wake_up_time = timestamp;\r\n}\r\nstatic void\r\nlatency_migrate_task_event(struct trace_migrate_task_event *migrate_task_event,\r\nstruct perf_session *session,\r\nstruct event *__event __used,\r\nint cpu __used,\r\nu64 timestamp,\r\nstruct thread *thread __used)\r\n{\r\nstruct work_atoms *atoms;\r\nstruct work_atom *atom;\r\nstruct thread *migrant;\r\nif (profile_cpu == -1)\r\nreturn;\r\nmigrant = perf_session__findnew(session, migrate_task_event->pid);\r\natoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);\r\nif (!atoms) {\r\nthread_atoms_insert(migrant);\r\nregister_pid(migrant->pid, migrant->comm);\r\natoms = thread_atoms_search(&atom_root, migrant, &cmp_pid);\r\nif (!atoms)\r\ndie("migration-event: Internal tree error");\r\nadd_sched_out_event(atoms, 'R', timestamp);\r\n}\r\nBUG_ON(list_empty(&atoms->work_list));\r\natom = list_entry(atoms->work_list.prev, struct work_atom, list);\r\natom->sched_in_time = atom->sched_out_time = atom->wake_up_time = timestamp;\r\nnr_timestamps++;\r\nif (atom->sched_out_time > timestamp)\r\nnr_unordered_timestamps++;\r\n}\r\nstatic void output_lat_thread(struct work_atoms *work_list)\r\n{\r\nint i;\r\nint ret;\r\nu64 avg;\r\nif (!work_list->nb_atoms)\r\nreturn;\r\nif (!strcmp(work_list->thread->comm, "swapper"))\r\nreturn;\r\nall_runtime += work_list->total_runtime;\r\nall_count += work_list->nb_atoms;\r\nret = printf(" %s:%d ", work_list->thread->comm, work_list->thread->pid);\r\nfor (i = 0; i < 24 - ret; i++)\r\nprintf(" ");\r\navg = work_list->total_lat / work_list->nb_atoms;\r\nprintf("|%11.3f ms |%9" PRIu64 " | avg:%9.3f ms | max:%9.3f ms | max at: %9.6f s\n",\r\n(double)work_list->total_runtime / 1e6,\r\nwork_list->nb_atoms, (double)avg / 1e6,\r\n(double)work_list->max_lat / 1e6,\r\n(double)work_list->max_lat_at / 1e9);\r\n}\r\nstatic int pid_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nif (l->thread->pid < r->thread->pid)\r\nreturn -1;\r\nif (l->thread->pid > r->thread->pid)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int avg_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nu64 avgl, avgr;\r\nif (!l->nb_atoms)\r\nreturn -1;\r\nif (!r->nb_atoms)\r\nreturn 1;\r\navgl = l->total_lat / l->nb_atoms;\r\navgr = r->total_lat / r->nb_atoms;\r\nif (avgl < avgr)\r\nreturn -1;\r\nif (avgl > avgr)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int max_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nif (l->max_lat < r->max_lat)\r\nreturn -1;\r\nif (l->max_lat > r->max_lat)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int switch_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nif (l->nb_atoms < r->nb_atoms)\r\nreturn -1;\r\nif (l->nb_atoms > r->nb_atoms)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int runtime_cmp(struct work_atoms *l, struct work_atoms *r)\r\n{\r\nif (l->total_runtime < r->total_runtime)\r\nreturn -1;\r\nif (l->total_runtime > r->total_runtime)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int sort_dimension__add(const char *tok, struct list_head *list)\r\n{\r\nint i;\r\nfor (i = 0; i < NB_AVAILABLE_SORTS; i++) {\r\nif (!strcmp(available_sorts[i]->name, tok)) {\r\nlist_add_tail(&available_sorts[i]->list, list);\r\nreturn 0;\r\n}\r\n}\r\nreturn -1;\r\n}\r\nstatic void sort_lat(void)\r\n{\r\nstruct rb_node *node;\r\nfor (;;) {\r\nstruct work_atoms *data;\r\nnode = rb_first(&atom_root);\r\nif (!node)\r\nbreak;\r\nrb_erase(node, &atom_root);\r\ndata = rb_entry(node, struct work_atoms, node);\r\n__thread_latency_insert(&sorted_atom_root, data, &sort_list);\r\n}\r\n}\r\nstatic void\r\nprocess_sched_wakeup_event(void *data, struct perf_session *session,\r\nstruct event *event,\r\nint cpu __used,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\nstruct trace_wakeup_event wakeup_event;\r\nFILL_COMMON_FIELDS(wakeup_event, event, data);\r\nFILL_ARRAY(wakeup_event, comm, event, data);\r\nFILL_FIELD(wakeup_event, pid, event, data);\r\nFILL_FIELD(wakeup_event, prio, event, data);\r\nFILL_FIELD(wakeup_event, success, event, data);\r\nFILL_FIELD(wakeup_event, cpu, event, data);\r\nif (trace_handler->wakeup_event)\r\ntrace_handler->wakeup_event(&wakeup_event, session, event,\r\ncpu, timestamp, thread);\r\n}\r\nstatic void\r\nmap_switch_event(struct trace_switch_event *switch_event,\r\nstruct perf_session *session,\r\nstruct event *event __used,\r\nint this_cpu,\r\nu64 timestamp,\r\nstruct thread *thread __used)\r\n{\r\nstruct thread *sched_out __used, *sched_in;\r\nint new_shortname;\r\nu64 timestamp0;\r\ns64 delta;\r\nint cpu;\r\nBUG_ON(this_cpu >= MAX_CPUS || this_cpu < 0);\r\nif (this_cpu > max_cpu)\r\nmax_cpu = this_cpu;\r\ntimestamp0 = cpu_last_switched[this_cpu];\r\ncpu_last_switched[this_cpu] = timestamp;\r\nif (timestamp0)\r\ndelta = timestamp - timestamp0;\r\nelse\r\ndelta = 0;\r\nif (delta < 0)\r\ndie("hm, delta: %" PRIu64 " < 0 ?\n", delta);\r\nsched_out = perf_session__findnew(session, switch_event->prev_pid);\r\nsched_in = perf_session__findnew(session, switch_event->next_pid);\r\ncurr_thread[this_cpu] = sched_in;\r\nprintf(" ");\r\nnew_shortname = 0;\r\nif (!sched_in->shortname[0]) {\r\nsched_in->shortname[0] = next_shortname1;\r\nsched_in->shortname[1] = next_shortname2;\r\nif (next_shortname1 < 'Z') {\r\nnext_shortname1++;\r\n} else {\r\nnext_shortname1='A';\r\nif (next_shortname2 < '9') {\r\nnext_shortname2++;\r\n} else {\r\nnext_shortname2='0';\r\n}\r\n}\r\nnew_shortname = 1;\r\n}\r\nfor (cpu = 0; cpu <= max_cpu; cpu++) {\r\nif (cpu != this_cpu)\r\nprintf(" ");\r\nelse\r\nprintf("*");\r\nif (curr_thread[cpu]) {\r\nif (curr_thread[cpu]->pid)\r\nprintf("%2s ", curr_thread[cpu]->shortname);\r\nelse\r\nprintf(". ");\r\n} else\r\nprintf(" ");\r\n}\r\nprintf(" %12.6f secs ", (double)timestamp/1e9);\r\nif (new_shortname) {\r\nprintf("%s => %s:%d\n",\r\nsched_in->shortname, sched_in->comm, sched_in->pid);\r\n} else {\r\nprintf("\n");\r\n}\r\n}\r\nstatic void\r\nprocess_sched_switch_event(void *data, struct perf_session *session,\r\nstruct event *event,\r\nint this_cpu,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\nstruct trace_switch_event switch_event;\r\nFILL_COMMON_FIELDS(switch_event, event, data);\r\nFILL_ARRAY(switch_event, prev_comm, event, data);\r\nFILL_FIELD(switch_event, prev_pid, event, data);\r\nFILL_FIELD(switch_event, prev_prio, event, data);\r\nFILL_FIELD(switch_event, prev_state, event, data);\r\nFILL_ARRAY(switch_event, next_comm, event, data);\r\nFILL_FIELD(switch_event, next_pid, event, data);\r\nFILL_FIELD(switch_event, next_prio, event, data);\r\nif (curr_pid[this_cpu] != (u32)-1) {\r\nif (curr_pid[this_cpu] != switch_event.prev_pid)\r\nnr_context_switch_bugs++;\r\n}\r\nif (trace_handler->switch_event)\r\ntrace_handler->switch_event(&switch_event, session, event,\r\nthis_cpu, timestamp, thread);\r\ncurr_pid[this_cpu] = switch_event.next_pid;\r\n}\r\nstatic void\r\nprocess_sched_runtime_event(void *data, struct perf_session *session,\r\nstruct event *event,\r\nint cpu __used,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\nstruct trace_runtime_event runtime_event;\r\nFILL_ARRAY(runtime_event, comm, event, data);\r\nFILL_FIELD(runtime_event, pid, event, data);\r\nFILL_FIELD(runtime_event, runtime, event, data);\r\nFILL_FIELD(runtime_event, vruntime, event, data);\r\nif (trace_handler->runtime_event)\r\ntrace_handler->runtime_event(&runtime_event, session, event, cpu, timestamp, thread);\r\n}\r\nstatic void\r\nprocess_sched_fork_event(void *data,\r\nstruct event *event,\r\nint cpu __used,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\nstruct trace_fork_event fork_event;\r\nFILL_COMMON_FIELDS(fork_event, event, data);\r\nFILL_ARRAY(fork_event, parent_comm, event, data);\r\nFILL_FIELD(fork_event, parent_pid, event, data);\r\nFILL_ARRAY(fork_event, child_comm, event, data);\r\nFILL_FIELD(fork_event, child_pid, event, data);\r\nif (trace_handler->fork_event)\r\ntrace_handler->fork_event(&fork_event, event,\r\ncpu, timestamp, thread);\r\n}\r\nstatic void\r\nprocess_sched_exit_event(struct event *event,\r\nint cpu __used,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\nif (verbose)\r\nprintf("sched_exit event %p\n", event);\r\n}\r\nstatic void\r\nprocess_sched_migrate_task_event(void *data, struct perf_session *session,\r\nstruct event *event,\r\nint cpu __used,\r\nu64 timestamp __used,\r\nstruct thread *thread __used)\r\n{\r\nstruct trace_migrate_task_event migrate_task_event;\r\nFILL_COMMON_FIELDS(migrate_task_event, event, data);\r\nFILL_ARRAY(migrate_task_event, comm, event, data);\r\nFILL_FIELD(migrate_task_event, pid, event, data);\r\nFILL_FIELD(migrate_task_event, prio, event, data);\r\nFILL_FIELD(migrate_task_event, cpu, event, data);\r\nif (trace_handler->migrate_task_event)\r\ntrace_handler->migrate_task_event(&migrate_task_event, session,\r\nevent, cpu, timestamp, thread);\r\n}\r\nstatic void process_raw_event(union perf_event *raw_event __used,\r\nstruct perf_session *session, void *data, int cpu,\r\nu64 timestamp, struct thread *thread)\r\n{\r\nstruct event *event;\r\nint type;\r\ntype = trace_parse_common_type(data);\r\nevent = trace_find_event(type);\r\nif (!strcmp(event->name, "sched_switch"))\r\nprocess_sched_switch_event(data, session, event, cpu, timestamp, thread);\r\nif (!strcmp(event->name, "sched_stat_runtime"))\r\nprocess_sched_runtime_event(data, session, event, cpu, timestamp, thread);\r\nif (!strcmp(event->name, "sched_wakeup"))\r\nprocess_sched_wakeup_event(data, session, event, cpu, timestamp, thread);\r\nif (!strcmp(event->name, "sched_wakeup_new"))\r\nprocess_sched_wakeup_event(data, session, event, cpu, timestamp, thread);\r\nif (!strcmp(event->name, "sched_process_fork"))\r\nprocess_sched_fork_event(data, event, cpu, timestamp, thread);\r\nif (!strcmp(event->name, "sched_process_exit"))\r\nprocess_sched_exit_event(event, cpu, timestamp, thread);\r\nif (!strcmp(event->name, "sched_migrate_task"))\r\nprocess_sched_migrate_task_event(data, session, event, cpu, timestamp, thread);\r\n}\r\nstatic int process_sample_event(union perf_event *event,\r\nstruct perf_sample *sample,\r\nstruct perf_evsel *evsel __used,\r\nstruct perf_session *session)\r\n{\r\nstruct thread *thread;\r\nif (!(session->sample_type & PERF_SAMPLE_RAW))\r\nreturn 0;\r\nthread = perf_session__findnew(session, sample->pid);\r\nif (thread == NULL) {\r\npr_debug("problem processing %d event, skipping it.\n",\r\nevent->header.type);\r\nreturn -1;\r\n}\r\ndump_printf(" ... thread: %s:%d\n", thread->comm, thread->pid);\r\nif (profile_cpu != -1 && profile_cpu != (int)sample->cpu)\r\nreturn 0;\r\nprocess_raw_event(event, session, sample->raw_data, sample->cpu,\r\nsample->time, thread);\r\nreturn 0;\r\n}\r\nstatic void read_events(bool destroy, struct perf_session **psession)\r\n{\r\nint err = -EINVAL;\r\nstruct perf_session *session = perf_session__new(input_name, O_RDONLY,\r\n0, false, &event_ops);\r\nif (session == NULL)\r\ndie("No Memory");\r\nif (perf_session__has_traces(session, "record -R")) {\r\nerr = perf_session__process_events(session, &event_ops);\r\nif (err)\r\ndie("Failed to process events, error %d", err);\r\nnr_events = session->hists.stats.nr_events[0];\r\nnr_lost_events = session->hists.stats.total_lost;\r\nnr_lost_chunks = session->hists.stats.nr_events[PERF_RECORD_LOST];\r\n}\r\nif (destroy)\r\nperf_session__delete(session);\r\nif (psession)\r\n*psession = session;\r\n}\r\nstatic void print_bad_events(void)\r\n{\r\nif (nr_unordered_timestamps && nr_timestamps) {\r\nprintf(" INFO: %.3f%% unordered timestamps (%ld out of %ld)\n",\r\n(double)nr_unordered_timestamps/(double)nr_timestamps*100.0,\r\nnr_unordered_timestamps, nr_timestamps);\r\n}\r\nif (nr_lost_events && nr_events) {\r\nprintf(" INFO: %.3f%% lost events (%ld out of %ld, in %ld chunks)\n",\r\n(double)nr_lost_events/(double)nr_events*100.0,\r\nnr_lost_events, nr_events, nr_lost_chunks);\r\n}\r\nif (nr_state_machine_bugs && nr_timestamps) {\r\nprintf(" INFO: %.3f%% state machine bugs (%ld out of %ld)",\r\n(double)nr_state_machine_bugs/(double)nr_timestamps*100.0,\r\nnr_state_machine_bugs, nr_timestamps);\r\nif (nr_lost_events)\r\nprintf(" (due to lost events?)");\r\nprintf("\n");\r\n}\r\nif (nr_context_switch_bugs && nr_timestamps) {\r\nprintf(" INFO: %.3f%% context switch bugs (%ld out of %ld)",\r\n(double)nr_context_switch_bugs/(double)nr_timestamps*100.0,\r\nnr_context_switch_bugs, nr_timestamps);\r\nif (nr_lost_events)\r\nprintf(" (due to lost events?)");\r\nprintf("\n");\r\n}\r\n}\r\nstatic void __cmd_lat(void)\r\n{\r\nstruct rb_node *next;\r\nstruct perf_session *session;\r\nsetup_pager();\r\nread_events(false, &session);\r\nsort_lat();\r\nprintf("\n ---------------------------------------------------------------------------------------------------------------\n");\r\nprintf(" Task | Runtime ms | Switches | Average delay ms | Maximum delay ms | Maximum delay at |\n");\r\nprintf(" ---------------------------------------------------------------------------------------------------------------\n");\r\nnext = rb_first(&sorted_atom_root);\r\nwhile (next) {\r\nstruct work_atoms *work_list;\r\nwork_list = rb_entry(next, struct work_atoms, node);\r\noutput_lat_thread(work_list);\r\nnext = rb_next(next);\r\n}\r\nprintf(" -----------------------------------------------------------------------------------------\n");\r\nprintf(" TOTAL: |%11.3f ms |%9" PRIu64 " |\n",\r\n(double)all_runtime/1e6, all_count);\r\nprintf(" ---------------------------------------------------\n");\r\nprint_bad_events();\r\nprintf("\n");\r\nperf_session__delete(session);\r\n}\r\nstatic void __cmd_map(void)\r\n{\r\nmax_cpu = sysconf(_SC_NPROCESSORS_CONF);\r\nsetup_pager();\r\nread_events(true, NULL);\r\nprint_bad_events();\r\n}\r\nstatic void __cmd_replay(void)\r\n{\r\nunsigned long i;\r\ncalibrate_run_measurement_overhead();\r\ncalibrate_sleep_measurement_overhead();\r\ntest_calibrations();\r\nread_events(true, NULL);\r\nprintf("nr_run_events: %ld\n", nr_run_events);\r\nprintf("nr_sleep_events: %ld\n", nr_sleep_events);\r\nprintf("nr_wakeup_events: %ld\n", nr_wakeup_events);\r\nif (targetless_wakeups)\r\nprintf("target-less wakeups: %ld\n", targetless_wakeups);\r\nif (multitarget_wakeups)\r\nprintf("multi-target wakeups: %ld\n", multitarget_wakeups);\r\nif (nr_run_events_optimized)\r\nprintf("run atoms optimized: %ld\n",\r\nnr_run_events_optimized);\r\nprint_task_traces();\r\nadd_cross_task_wakeups();\r\ncreate_tasks();\r\nprintf("------------------------------------------------------------\n");\r\nfor (i = 0; i < replay_repeat; i++)\r\nrun_one_test();\r\n}\r\nstatic void setup_sorting(void)\r\n{\r\nchar *tmp, *tok, *str = strdup(sort_order);\r\nfor (tok = strtok_r(str, ", ", &tmp);\r\ntok; tok = strtok_r(NULL, ", ", &tmp)) {\r\nif (sort_dimension__add(tok, &sort_list) < 0) {\r\nerror("Unknown --sort key: `%s'", tok);\r\nusage_with_options(latency_usage, latency_options);\r\n}\r\n}\r\nfree(str);\r\nsort_dimension__add("pid", &cmp_pid);\r\n}\r\nstatic int __cmd_record(int argc, const char **argv)\r\n{\r\nunsigned int rec_argc, i, j;\r\nconst char **rec_argv;\r\nrec_argc = ARRAY_SIZE(record_args) + argc - 1;\r\nrec_argv = calloc(rec_argc + 1, sizeof(char *));\r\nif (rec_argv == NULL)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < ARRAY_SIZE(record_args); i++)\r\nrec_argv[i] = strdup(record_args[i]);\r\nfor (j = 1; j < (unsigned int)argc; j++, i++)\r\nrec_argv[i] = argv[j];\r\nBUG_ON(i != rec_argc);\r\nreturn cmd_record(i, rec_argv, NULL);\r\n}\r\nint cmd_sched(int argc, const char **argv, const char *prefix __used)\r\n{\r\nargc = parse_options(argc, argv, sched_options, sched_usage,\r\nPARSE_OPT_STOP_AT_NON_OPTION);\r\nif (!argc)\r\nusage_with_options(sched_usage, sched_options);\r\nif (!strcmp(argv[0], "script"))\r\nreturn cmd_script(argc, argv, prefix);\r\nsymbol__init();\r\nif (!strncmp(argv[0], "rec", 3)) {\r\nreturn __cmd_record(argc, argv);\r\n} else if (!strncmp(argv[0], "lat", 3)) {\r\ntrace_handler = &lat_ops;\r\nif (argc > 1) {\r\nargc = parse_options(argc, argv, latency_options, latency_usage, 0);\r\nif (argc)\r\nusage_with_options(latency_usage, latency_options);\r\n}\r\nsetup_sorting();\r\n__cmd_lat();\r\n} else if (!strcmp(argv[0], "map")) {\r\ntrace_handler = &map_ops;\r\nsetup_sorting();\r\n__cmd_map();\r\n} else if (!strncmp(argv[0], "rep", 3)) {\r\ntrace_handler = &replay_ops;\r\nif (argc) {\r\nargc = parse_options(argc, argv, replay_options, replay_usage, 0);\r\nif (argc)\r\nusage_with_options(replay_usage, replay_options);\r\n}\r\n__cmd_replay();\r\n} else {\r\nusage_with_options(sched_usage, sched_options);\r\n}\r\nreturn 0;\r\n}
