static inline unsigned long range_size(struct ashmem_range *range)\r\n{\r\nreturn range->pgend - range->pgstart + 1;\r\n}\r\nstatic inline bool range_on_lru(struct ashmem_range *range)\r\n{\r\nreturn range->purged == ASHMEM_NOT_PURGED;\r\n}\r\nstatic inline bool page_range_subsumes_range(struct ashmem_range *range,\r\nsize_t start, size_t end)\r\n{\r\nreturn (range->pgstart >= start) && (range->pgend <= end);\r\n}\r\nstatic inline bool page_range_subsumed_by_range(struct ashmem_range *range,\r\nsize_t start, size_t end)\r\n{\r\nreturn (range->pgstart <= start) && (range->pgend >= end);\r\n}\r\nstatic inline bool page_in_range(struct ashmem_range *range, size_t page)\r\n{\r\nreturn (range->pgstart <= page) && (range->pgend >= page);\r\n}\r\nstatic inline bool page_range_in_range(struct ashmem_range *range,\r\nsize_t start, size_t end)\r\n{\r\nreturn page_in_range(range, start) || page_in_range(range, end) ||\r\npage_range_subsumes_range(range, start, end);\r\n}\r\nstatic inline bool range_before_page(struct ashmem_range *range, size_t page)\r\n{\r\nreturn range->pgend < page;\r\n}\r\nstatic inline void lru_add(struct ashmem_range *range)\r\n{\r\nlist_add_tail(&range->lru, &ashmem_lru_list);\r\nlru_count += range_size(range);\r\n}\r\nstatic inline void lru_del(struct ashmem_range *range)\r\n{\r\nlist_del(&range->lru);\r\nlru_count -= range_size(range);\r\n}\r\nstatic int range_alloc(struct ashmem_area *asma,\r\nstruct ashmem_range *prev_range, unsigned int purged,\r\nsize_t start, size_t end)\r\n{\r\nstruct ashmem_range *range;\r\nrange = kmem_cache_zalloc(ashmem_range_cachep, GFP_KERNEL);\r\nif (unlikely(!range))\r\nreturn -ENOMEM;\r\nrange->asma = asma;\r\nrange->pgstart = start;\r\nrange->pgend = end;\r\nrange->purged = purged;\r\nlist_add_tail(&range->unpinned, &prev_range->unpinned);\r\nif (range_on_lru(range))\r\nlru_add(range);\r\nreturn 0;\r\n}\r\nstatic void range_del(struct ashmem_range *range)\r\n{\r\nlist_del(&range->unpinned);\r\nif (range_on_lru(range))\r\nlru_del(range);\r\nkmem_cache_free(ashmem_range_cachep, range);\r\n}\r\nstatic inline void range_shrink(struct ashmem_range *range,\r\nsize_t start, size_t end)\r\n{\r\nsize_t pre = range_size(range);\r\nrange->pgstart = start;\r\nrange->pgend = end;\r\nif (range_on_lru(range))\r\nlru_count -= pre - range_size(range);\r\n}\r\nstatic int ashmem_open(struct inode *inode, struct file *file)\r\n{\r\nstruct ashmem_area *asma;\r\nint ret;\r\nret = generic_file_open(inode, file);\r\nif (unlikely(ret))\r\nreturn ret;\r\nasma = kmem_cache_zalloc(ashmem_area_cachep, GFP_KERNEL);\r\nif (unlikely(!asma))\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&asma->unpinned_list);\r\nmemcpy(asma->name, ASHMEM_NAME_PREFIX, ASHMEM_NAME_PREFIX_LEN);\r\nasma->prot_mask = PROT_MASK;\r\nfile->private_data = asma;\r\nreturn 0;\r\n}\r\nstatic int ashmem_release(struct inode *ignored, struct file *file)\r\n{\r\nstruct ashmem_area *asma = file->private_data;\r\nstruct ashmem_range *range, *next;\r\nmutex_lock(&ashmem_mutex);\r\nlist_for_each_entry_safe(range, next, &asma->unpinned_list, unpinned)\r\nrange_del(range);\r\nmutex_unlock(&ashmem_mutex);\r\nif (asma->file)\r\nfput(asma->file);\r\nkmem_cache_free(ashmem_area_cachep, asma);\r\nreturn 0;\r\n}\r\nstatic ssize_t ashmem_read(struct file *file, char __user *buf,\r\nsize_t len, loff_t *pos)\r\n{\r\nstruct ashmem_area *asma = file->private_data;\r\nint ret = 0;\r\nmutex_lock(&ashmem_mutex);\r\nif (asma->size == 0)\r\ngoto out_unlock;\r\nif (!asma->file) {\r\nret = -EBADF;\r\ngoto out_unlock;\r\n}\r\nmutex_unlock(&ashmem_mutex);\r\nret = __vfs_read(asma->file, buf, len, pos);\r\nif (ret >= 0)\r\nasma->file->f_pos = *pos;\r\nreturn ret;\r\nout_unlock:\r\nmutex_unlock(&ashmem_mutex);\r\nreturn ret;\r\n}\r\nstatic loff_t ashmem_llseek(struct file *file, loff_t offset, int origin)\r\n{\r\nstruct ashmem_area *asma = file->private_data;\r\nint ret;\r\nmutex_lock(&ashmem_mutex);\r\nif (asma->size == 0) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (!asma->file) {\r\nret = -EBADF;\r\ngoto out;\r\n}\r\nret = vfs_llseek(asma->file, offset, origin);\r\nif (ret < 0)\r\ngoto out;\r\nfile->f_pos = asma->file->f_pos;\r\nout:\r\nmutex_unlock(&ashmem_mutex);\r\nreturn ret;\r\n}\r\nstatic inline vm_flags_t calc_vm_may_flags(unsigned long prot)\r\n{\r\nreturn _calc_vm_trans(prot, PROT_READ, VM_MAYREAD) |\r\n_calc_vm_trans(prot, PROT_WRITE, VM_MAYWRITE) |\r\n_calc_vm_trans(prot, PROT_EXEC, VM_MAYEXEC);\r\n}\r\nstatic int ashmem_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nstruct ashmem_area *asma = file->private_data;\r\nint ret = 0;\r\nmutex_lock(&ashmem_mutex);\r\nif (unlikely(!asma->size)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif (unlikely((vma->vm_flags & ~calc_vm_prot_bits(asma->prot_mask, 0)) &\r\ncalc_vm_prot_bits(PROT_MASK, 0))) {\r\nret = -EPERM;\r\ngoto out;\r\n}\r\nvma->vm_flags &= ~calc_vm_may_flags(~asma->prot_mask);\r\nif (!asma->file) {\r\nchar *name = ASHMEM_NAME_DEF;\r\nstruct file *vmfile;\r\nif (asma->name[ASHMEM_NAME_PREFIX_LEN] != '\0')\r\nname = asma->name;\r\nvmfile = shmem_file_setup(name, asma->size, vma->vm_flags);\r\nif (IS_ERR(vmfile)) {\r\nret = PTR_ERR(vmfile);\r\ngoto out;\r\n}\r\nvmfile->f_mode |= FMODE_LSEEK;\r\nasma->file = vmfile;\r\n}\r\nget_file(asma->file);\r\nif (vma->vm_flags & VM_SHARED) {\r\nret = shmem_zero_setup(vma);\r\nif (ret) {\r\nfput(asma->file);\r\ngoto out;\r\n}\r\n}\r\nif (vma->vm_file)\r\nfput(vma->vm_file);\r\nvma->vm_file = asma->file;\r\nout:\r\nmutex_unlock(&ashmem_mutex);\r\nreturn ret;\r\n}\r\nstatic unsigned long\r\nashmem_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nstruct ashmem_range *range, *next;\r\nunsigned long freed = 0;\r\nif (!(sc->gfp_mask & __GFP_FS))\r\nreturn SHRINK_STOP;\r\nif (!mutex_trylock(&ashmem_mutex))\r\nreturn -1;\r\nlist_for_each_entry_safe(range, next, &ashmem_lru_list, lru) {\r\nloff_t start = range->pgstart * PAGE_SIZE;\r\nloff_t end = (range->pgend + 1) * PAGE_SIZE;\r\nvfs_fallocate(range->asma->file,\r\nFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\r\nstart, end - start);\r\nrange->purged = ASHMEM_WAS_PURGED;\r\nlru_del(range);\r\nfreed += range_size(range);\r\nif (--sc->nr_to_scan <= 0)\r\nbreak;\r\n}\r\nmutex_unlock(&ashmem_mutex);\r\nreturn freed;\r\n}\r\nstatic unsigned long\r\nashmem_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\r\n{\r\nreturn lru_count;\r\n}\r\nstatic int set_prot_mask(struct ashmem_area *asma, unsigned long prot)\r\n{\r\nint ret = 0;\r\nmutex_lock(&ashmem_mutex);\r\nif (unlikely((asma->prot_mask & prot) != prot)) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nif ((prot & PROT_READ) && (current->personality & READ_IMPLIES_EXEC))\r\nprot |= PROT_EXEC;\r\nasma->prot_mask = prot;\r\nout:\r\nmutex_unlock(&ashmem_mutex);\r\nreturn ret;\r\n}\r\nstatic int set_name(struct ashmem_area *asma, void __user *name)\r\n{\r\nint len;\r\nint ret = 0;\r\nchar local_name[ASHMEM_NAME_LEN];\r\nlen = strncpy_from_user(local_name, name, ASHMEM_NAME_LEN);\r\nif (len < 0)\r\nreturn len;\r\nif (len == ASHMEM_NAME_LEN)\r\nlocal_name[ASHMEM_NAME_LEN - 1] = '\0';\r\nmutex_lock(&ashmem_mutex);\r\nif (unlikely(asma->file))\r\nret = -EINVAL;\r\nelse\r\nstrcpy(asma->name + ASHMEM_NAME_PREFIX_LEN, local_name);\r\nmutex_unlock(&ashmem_mutex);\r\nreturn ret;\r\n}\r\nstatic int get_name(struct ashmem_area *asma, void __user *name)\r\n{\r\nint ret = 0;\r\nsize_t len;\r\nchar local_name[ASHMEM_NAME_LEN];\r\nmutex_lock(&ashmem_mutex);\r\nif (asma->name[ASHMEM_NAME_PREFIX_LEN] != '\0') {\r\nlen = strlen(asma->name + ASHMEM_NAME_PREFIX_LEN) + 1;\r\nmemcpy(local_name, asma->name + ASHMEM_NAME_PREFIX_LEN, len);\r\n} else {\r\nlen = sizeof(ASHMEM_NAME_DEF);\r\nmemcpy(local_name, ASHMEM_NAME_DEF, len);\r\n}\r\nmutex_unlock(&ashmem_mutex);\r\nif (unlikely(copy_to_user(name, local_name, len)))\r\nret = -EFAULT;\r\nreturn ret;\r\n}\r\nstatic int ashmem_pin(struct ashmem_area *asma, size_t pgstart, size_t pgend)\r\n{\r\nstruct ashmem_range *range, *next;\r\nint ret = ASHMEM_NOT_PURGED;\r\nlist_for_each_entry_safe(range, next, &asma->unpinned_list, unpinned) {\r\nif (range_before_page(range, pgstart))\r\nbreak;\r\nif (page_range_in_range(range, pgstart, pgend)) {\r\nret |= range->purged;\r\nif (page_range_subsumes_range(range, pgstart, pgend)) {\r\nrange_del(range);\r\ncontinue;\r\n}\r\nif (range->pgstart >= pgstart) {\r\nrange_shrink(range, pgend + 1, range->pgend);\r\ncontinue;\r\n}\r\nif (range->pgend <= pgend) {\r\nrange_shrink(range, range->pgstart,\r\npgstart - 1);\r\ncontinue;\r\n}\r\nrange_alloc(asma, range, range->purged,\r\npgend + 1, range->pgend);\r\nrange_shrink(range, range->pgstart, pgstart - 1);\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int ashmem_unpin(struct ashmem_area *asma, size_t pgstart, size_t pgend)\r\n{\r\nstruct ashmem_range *range, *next;\r\nunsigned int purged = ASHMEM_NOT_PURGED;\r\nrestart:\r\nlist_for_each_entry_safe(range, next, &asma->unpinned_list, unpinned) {\r\nif (range_before_page(range, pgstart))\r\nbreak;\r\nif (page_range_subsumed_by_range(range, pgstart, pgend))\r\nreturn 0;\r\nif (page_range_in_range(range, pgstart, pgend)) {\r\npgstart = min(range->pgstart, pgstart);\r\npgend = max(range->pgend, pgend);\r\npurged |= range->purged;\r\nrange_del(range);\r\ngoto restart;\r\n}\r\n}\r\nreturn range_alloc(asma, range, purged, pgstart, pgend);\r\n}\r\nstatic int ashmem_get_pin_status(struct ashmem_area *asma, size_t pgstart,\r\nsize_t pgend)\r\n{\r\nstruct ashmem_range *range;\r\nint ret = ASHMEM_IS_PINNED;\r\nlist_for_each_entry(range, &asma->unpinned_list, unpinned) {\r\nif (range_before_page(range, pgstart))\r\nbreak;\r\nif (page_range_in_range(range, pgstart, pgend)) {\r\nret = ASHMEM_IS_UNPINNED;\r\nbreak;\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int ashmem_pin_unpin(struct ashmem_area *asma, unsigned long cmd,\r\nvoid __user *p)\r\n{\r\nstruct ashmem_pin pin;\r\nsize_t pgstart, pgend;\r\nint ret = -EINVAL;\r\nif (unlikely(!asma->file))\r\nreturn -EINVAL;\r\nif (unlikely(copy_from_user(&pin, p, sizeof(pin))))\r\nreturn -EFAULT;\r\nif (!pin.len)\r\npin.len = PAGE_ALIGN(asma->size) - pin.offset;\r\nif (unlikely((pin.offset | pin.len) & ~PAGE_MASK))\r\nreturn -EINVAL;\r\nif (unlikely(((__u32)-1) - pin.offset < pin.len))\r\nreturn -EINVAL;\r\nif (unlikely(PAGE_ALIGN(asma->size) < pin.offset + pin.len))\r\nreturn -EINVAL;\r\npgstart = pin.offset / PAGE_SIZE;\r\npgend = pgstart + (pin.len / PAGE_SIZE) - 1;\r\nmutex_lock(&ashmem_mutex);\r\nswitch (cmd) {\r\ncase ASHMEM_PIN:\r\nret = ashmem_pin(asma, pgstart, pgend);\r\nbreak;\r\ncase ASHMEM_UNPIN:\r\nret = ashmem_unpin(asma, pgstart, pgend);\r\nbreak;\r\ncase ASHMEM_GET_PIN_STATUS:\r\nret = ashmem_get_pin_status(asma, pgstart, pgend);\r\nbreak;\r\n}\r\nmutex_unlock(&ashmem_mutex);\r\nreturn ret;\r\n}\r\nstatic long ashmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\r\n{\r\nstruct ashmem_area *asma = file->private_data;\r\nlong ret = -ENOTTY;\r\nswitch (cmd) {\r\ncase ASHMEM_SET_NAME:\r\nret = set_name(asma, (void __user *)arg);\r\nbreak;\r\ncase ASHMEM_GET_NAME:\r\nret = get_name(asma, (void __user *)arg);\r\nbreak;\r\ncase ASHMEM_SET_SIZE:\r\nret = -EINVAL;\r\nif (!asma->file) {\r\nret = 0;\r\nasma->size = (size_t)arg;\r\n}\r\nbreak;\r\ncase ASHMEM_GET_SIZE:\r\nret = asma->size;\r\nbreak;\r\ncase ASHMEM_SET_PROT_MASK:\r\nret = set_prot_mask(asma, arg);\r\nbreak;\r\ncase ASHMEM_GET_PROT_MASK:\r\nret = asma->prot_mask;\r\nbreak;\r\ncase ASHMEM_PIN:\r\ncase ASHMEM_UNPIN:\r\ncase ASHMEM_GET_PIN_STATUS:\r\nret = ashmem_pin_unpin(asma, cmd, (void __user *)arg);\r\nbreak;\r\ncase ASHMEM_PURGE_ALL_CACHES:\r\nret = -EPERM;\r\nif (capable(CAP_SYS_ADMIN)) {\r\nstruct shrink_control sc = {\r\n.gfp_mask = GFP_KERNEL,\r\n.nr_to_scan = LONG_MAX,\r\n};\r\nret = ashmem_shrink_count(&ashmem_shrinker, &sc);\r\nashmem_shrink_scan(&ashmem_shrinker, &sc);\r\n}\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic long compat_ashmem_ioctl(struct file *file, unsigned int cmd,\r\nunsigned long arg)\r\n{\r\nswitch (cmd) {\r\ncase COMPAT_ASHMEM_SET_SIZE:\r\ncmd = ASHMEM_SET_SIZE;\r\nbreak;\r\ncase COMPAT_ASHMEM_SET_PROT_MASK:\r\ncmd = ASHMEM_SET_PROT_MASK;\r\nbreak;\r\n}\r\nreturn ashmem_ioctl(file, cmd, arg);\r\n}\r\nstatic int __init ashmem_init(void)\r\n{\r\nint ret = -ENOMEM;\r\nashmem_area_cachep = kmem_cache_create("ashmem_area_cache",\r\nsizeof(struct ashmem_area),\r\n0, 0, NULL);\r\nif (unlikely(!ashmem_area_cachep)) {\r\npr_err("failed to create slab cache\n");\r\ngoto out;\r\n}\r\nashmem_range_cachep = kmem_cache_create("ashmem_range_cache",\r\nsizeof(struct ashmem_range),\r\n0, 0, NULL);\r\nif (unlikely(!ashmem_range_cachep)) {\r\npr_err("failed to create slab cache\n");\r\ngoto out_free1;\r\n}\r\nret = misc_register(&ashmem_misc);\r\nif (unlikely(ret)) {\r\npr_err("failed to register misc device!\n");\r\ngoto out_free2;\r\n}\r\nregister_shrinker(&ashmem_shrinker);\r\npr_info("initialized\n");\r\nreturn 0;\r\nout_free2:\r\nkmem_cache_destroy(ashmem_range_cachep);\r\nout_free1:\r\nkmem_cache_destroy(ashmem_area_cachep);\r\nout:\r\nreturn ret;\r\n}
