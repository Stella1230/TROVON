static void wl_tree_add(struct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nstruct rb_node **p, *parent = NULL;\r\np = &root->rb_node;\r\nwhile (*p) {\r\nstruct ubi_wl_entry *e1;\r\nparent = *p;\r\ne1 = rb_entry(parent, struct ubi_wl_entry, u.rb);\r\nif (e->ec < e1->ec)\r\np = &(*p)->rb_left;\r\nelse if (e->ec > e1->ec)\r\np = &(*p)->rb_right;\r\nelse {\r\nubi_assert(e->pnum != e1->pnum);\r\nif (e->pnum < e1->pnum)\r\np = &(*p)->rb_left;\r\nelse\r\np = &(*p)->rb_right;\r\n}\r\n}\r\nrb_link_node(&e->u.rb, parent, p);\r\nrb_insert_color(&e->u.rb, root);\r\n}\r\nstatic void wl_entry_destroy(struct ubi_device *ubi, struct ubi_wl_entry *e)\r\n{\r\nubi->lookuptbl[e->pnum] = NULL;\r\nkmem_cache_free(ubi_wl_entry_slab, e);\r\n}\r\nstatic int do_work(struct ubi_device *ubi)\r\n{\r\nint err;\r\nstruct ubi_work *wrk;\r\ncond_resched();\r\ndown_read(&ubi->work_sem);\r\nspin_lock(&ubi->wl_lock);\r\nif (list_empty(&ubi->works)) {\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->work_sem);\r\nreturn 0;\r\n}\r\nwrk = list_entry(ubi->works.next, struct ubi_work, list);\r\nlist_del(&wrk->list);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\nspin_unlock(&ubi->wl_lock);\r\nerr = wrk->func(ubi, wrk, 0);\r\nif (err)\r\nubi_err(ubi, "work failed with error code %d", err);\r\nup_read(&ubi->work_sem);\r\nreturn err;\r\n}\r\nstatic int in_wl_tree(struct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nstruct rb_node *p;\r\np = root->rb_node;\r\nwhile (p) {\r\nstruct ubi_wl_entry *e1;\r\ne1 = rb_entry(p, struct ubi_wl_entry, u.rb);\r\nif (e->pnum == e1->pnum) {\r\nubi_assert(e == e1);\r\nreturn 1;\r\n}\r\nif (e->ec < e1->ec)\r\np = p->rb_left;\r\nelse if (e->ec > e1->ec)\r\np = p->rb_right;\r\nelse {\r\nubi_assert(e->pnum != e1->pnum);\r\nif (e->pnum < e1->pnum)\r\np = p->rb_left;\r\nelse\r\np = p->rb_right;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void prot_queue_add(struct ubi_device *ubi, struct ubi_wl_entry *e)\r\n{\r\nint pq_tail = ubi->pq_head - 1;\r\nif (pq_tail < 0)\r\npq_tail = UBI_PROT_QUEUE_LEN - 1;\r\nubi_assert(pq_tail >= 0 && pq_tail < UBI_PROT_QUEUE_LEN);\r\nlist_add_tail(&e->u.list, &ubi->pq[pq_tail]);\r\ndbg_wl("added PEB %d EC %d to the protection queue", e->pnum, e->ec);\r\n}\r\nstatic struct ubi_wl_entry *find_wl_entry(struct ubi_device *ubi,\r\nstruct rb_root *root, int diff)\r\n{\r\nstruct rb_node *p;\r\nstruct ubi_wl_entry *e, *prev_e = NULL;\r\nint max;\r\ne = rb_entry(rb_first(root), struct ubi_wl_entry, u.rb);\r\nmax = e->ec + diff;\r\np = root->rb_node;\r\nwhile (p) {\r\nstruct ubi_wl_entry *e1;\r\ne1 = rb_entry(p, struct ubi_wl_entry, u.rb);\r\nif (e1->ec >= max)\r\np = p->rb_left;\r\nelse {\r\np = p->rb_right;\r\nprev_e = e;\r\ne = e1;\r\n}\r\n}\r\nif (prev_e && !ubi->fm_disabled &&\r\n!ubi->fm && e->pnum < UBI_FM_MAX_START)\r\nreturn prev_e;\r\nreturn e;\r\n}\r\nstatic struct ubi_wl_entry *find_mean_wl_entry(struct ubi_device *ubi,\r\nstruct rb_root *root)\r\n{\r\nstruct ubi_wl_entry *e, *first, *last;\r\nfirst = rb_entry(rb_first(root), struct ubi_wl_entry, u.rb);\r\nlast = rb_entry(rb_last(root), struct ubi_wl_entry, u.rb);\r\nif (last->ec - first->ec < WL_FREE_MAX_DIFF) {\r\ne = rb_entry(root->rb_node, struct ubi_wl_entry, u.rb);\r\ne = may_reserve_for_fm(ubi, e, root);\r\n} else\r\ne = find_wl_entry(ubi, root, WL_FREE_MAX_DIFF/2);\r\nreturn e;\r\n}\r\nstatic struct ubi_wl_entry *wl_get_wle(struct ubi_device *ubi)\r\n{\r\nstruct ubi_wl_entry *e;\r\ne = find_mean_wl_entry(ubi, &ubi->free);\r\nif (!e) {\r\nubi_err(ubi, "no free eraseblocks");\r\nreturn NULL;\r\n}\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nrb_erase(&e->u.rb, &ubi->free);\r\nubi->free_count--;\r\ndbg_wl("PEB %d EC %d", e->pnum, e->ec);\r\nreturn e;\r\n}\r\nstatic int prot_queue_del(struct ubi_device *ubi, int pnum)\r\n{\r\nstruct ubi_wl_entry *e;\r\ne = ubi->lookuptbl[pnum];\r\nif (!e)\r\nreturn -ENODEV;\r\nif (self_check_in_pq(ubi, e))\r\nreturn -ENODEV;\r\nlist_del(&e->u.list);\r\ndbg_wl("deleted PEB %d from the protection queue", e->pnum);\r\nreturn 0;\r\n}\r\nstatic int sync_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\r\nint torture)\r\n{\r\nint err;\r\nstruct ubi_ec_hdr *ec_hdr;\r\nunsigned long long ec = e->ec;\r\ndbg_wl("erase PEB %d, old EC %llu", e->pnum, ec);\r\nerr = self_check_ec(ubi, e->pnum, e->ec);\r\nif (err)\r\nreturn -EINVAL;\r\nec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_NOFS);\r\nif (!ec_hdr)\r\nreturn -ENOMEM;\r\nerr = ubi_io_sync_erase(ubi, e->pnum, torture);\r\nif (err < 0)\r\ngoto out_free;\r\nec += err;\r\nif (ec > UBI_MAX_ERASECOUNTER) {\r\nubi_err(ubi, "erase counter overflow at PEB %d, EC %llu",\r\ne->pnum, ec);\r\nerr = -EINVAL;\r\ngoto out_free;\r\n}\r\ndbg_wl("erased PEB %d, new EC %llu", e->pnum, ec);\r\nec_hdr->ec = cpu_to_be64(ec);\r\nerr = ubi_io_write_ec_hdr(ubi, e->pnum, ec_hdr);\r\nif (err)\r\ngoto out_free;\r\ne->ec = ec;\r\nspin_lock(&ubi->wl_lock);\r\nif (e->ec > ubi->max_ec)\r\nubi->max_ec = e->ec;\r\nspin_unlock(&ubi->wl_lock);\r\nout_free:\r\nkfree(ec_hdr);\r\nreturn err;\r\n}\r\nstatic void serve_prot_queue(struct ubi_device *ubi)\r\n{\r\nstruct ubi_wl_entry *e, *tmp;\r\nint count;\r\nrepeat:\r\ncount = 0;\r\nspin_lock(&ubi->wl_lock);\r\nlist_for_each_entry_safe(e, tmp, &ubi->pq[ubi->pq_head], u.list) {\r\ndbg_wl("PEB %d EC %d protection over, move to used tree",\r\ne->pnum, e->ec);\r\nlist_del(&e->u.list);\r\nwl_tree_add(e, &ubi->used);\r\nif (count++ > 32) {\r\nspin_unlock(&ubi->wl_lock);\r\ncond_resched();\r\ngoto repeat;\r\n}\r\n}\r\nubi->pq_head += 1;\r\nif (ubi->pq_head == UBI_PROT_QUEUE_LEN)\r\nubi->pq_head = 0;\r\nubi_assert(ubi->pq_head >= 0 && ubi->pq_head < UBI_PROT_QUEUE_LEN);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nstatic void __schedule_ubi_work(struct ubi_device *ubi, struct ubi_work *wrk)\r\n{\r\nspin_lock(&ubi->wl_lock);\r\nlist_add_tail(&wrk->list, &ubi->works);\r\nubi_assert(ubi->works_count >= 0);\r\nubi->works_count += 1;\r\nif (ubi->thread_enabled && !ubi_dbg_is_bgt_disabled(ubi))\r\nwake_up_process(ubi->bgt_thread);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nstatic void schedule_ubi_work(struct ubi_device *ubi, struct ubi_work *wrk)\r\n{\r\ndown_read(&ubi->work_sem);\r\n__schedule_ubi_work(ubi, wrk);\r\nup_read(&ubi->work_sem);\r\n}\r\nstatic int schedule_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\r\nint vol_id, int lnum, int torture, bool nested)\r\n{\r\nstruct ubi_work *wl_wrk;\r\nubi_assert(e);\r\ndbg_wl("schedule erasure of PEB %d, EC %d, torture %d",\r\ne->pnum, e->ec, torture);\r\nwl_wrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wl_wrk)\r\nreturn -ENOMEM;\r\nwl_wrk->func = &erase_worker;\r\nwl_wrk->e = e;\r\nwl_wrk->vol_id = vol_id;\r\nwl_wrk->lnum = lnum;\r\nwl_wrk->torture = torture;\r\nif (nested)\r\n__schedule_ubi_work(ubi, wl_wrk);\r\nelse\r\nschedule_ubi_work(ubi, wl_wrk);\r\nreturn 0;\r\n}\r\nstatic int do_sync_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\r\nint vol_id, int lnum, int torture)\r\n{\r\nstruct ubi_work wl_wrk;\r\ndbg_wl("sync erase of PEB %i", e->pnum);\r\nwl_wrk.e = e;\r\nwl_wrk.vol_id = vol_id;\r\nwl_wrk.lnum = lnum;\r\nwl_wrk.torture = torture;\r\nreturn __erase_worker(ubi, &wl_wrk);\r\n}\r\nstatic int ensure_wear_leveling(struct ubi_device *ubi, int nested)\r\n{\r\nint err = 0;\r\nstruct ubi_wl_entry *e1;\r\nstruct ubi_wl_entry *e2;\r\nstruct ubi_work *wrk;\r\nspin_lock(&ubi->wl_lock);\r\nif (ubi->wl_scheduled)\r\ngoto out_unlock;\r\nif (!ubi->scrub.rb_node) {\r\nif (!ubi->used.rb_node || !ubi->free.rb_node)\r\ngoto out_unlock;\r\ne1 = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, u.rb);\r\ne2 = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\r\nif (!(e2->ec - e1->ec >= UBI_WL_THRESHOLD))\r\ngoto out_unlock;\r\ndbg_wl("schedule wear-leveling");\r\n} else\r\ndbg_wl("schedule scrubbing");\r\nubi->wl_scheduled = 1;\r\nspin_unlock(&ubi->wl_lock);\r\nwrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wrk) {\r\nerr = -ENOMEM;\r\ngoto out_cancel;\r\n}\r\nwrk->anchor = 0;\r\nwrk->func = &wear_leveling_worker;\r\nif (nested)\r\n__schedule_ubi_work(ubi, wrk);\r\nelse\r\nschedule_ubi_work(ubi, wrk);\r\nreturn err;\r\nout_cancel:\r\nspin_lock(&ubi->wl_lock);\r\nubi->wl_scheduled = 0;\r\nout_unlock:\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\nstatic int __erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk)\r\n{\r\nstruct ubi_wl_entry *e = wl_wrk->e;\r\nint pnum = e->pnum;\r\nint vol_id = wl_wrk->vol_id;\r\nint lnum = wl_wrk->lnum;\r\nint err, available_consumed = 0;\r\ndbg_wl("erase PEB %d EC %d LEB %d:%d",\r\npnum, e->ec, wl_wrk->vol_id, wl_wrk->lnum);\r\nerr = sync_erase(ubi, e, wl_wrk->torture);\r\nif (!err) {\r\nspin_lock(&ubi->wl_lock);\r\nwl_tree_add(e, &ubi->free);\r\nubi->free_count++;\r\nspin_unlock(&ubi->wl_lock);\r\nserve_prot_queue(ubi);\r\nerr = ensure_wear_leveling(ubi, 1);\r\nreturn err;\r\n}\r\nubi_err(ubi, "failed to erase PEB %d, error %d", pnum, err);\r\nif (err == -EINTR || err == -ENOMEM || err == -EAGAIN ||\r\nerr == -EBUSY) {\r\nint err1;\r\nerr1 = schedule_erase(ubi, e, vol_id, lnum, 0, false);\r\nif (err1) {\r\nwl_entry_destroy(ubi, e);\r\nerr = err1;\r\ngoto out_ro;\r\n}\r\nreturn err;\r\n}\r\nwl_entry_destroy(ubi, e);\r\nif (err != -EIO)\r\ngoto out_ro;\r\nif (!ubi->bad_allowed) {\r\nubi_err(ubi, "bad physical eraseblock %d detected", pnum);\r\ngoto out_ro;\r\n}\r\nspin_lock(&ubi->volumes_lock);\r\nif (ubi->beb_rsvd_pebs == 0) {\r\nif (ubi->avail_pebs == 0) {\r\nspin_unlock(&ubi->volumes_lock);\r\nubi_err(ubi, "no reserved/available physical eraseblocks");\r\ngoto out_ro;\r\n}\r\nubi->avail_pebs -= 1;\r\navailable_consumed = 1;\r\n}\r\nspin_unlock(&ubi->volumes_lock);\r\nubi_msg(ubi, "mark PEB %d as bad", pnum);\r\nerr = ubi_io_mark_bad(ubi, pnum);\r\nif (err)\r\ngoto out_ro;\r\nspin_lock(&ubi->volumes_lock);\r\nif (ubi->beb_rsvd_pebs > 0) {\r\nif (available_consumed) {\r\nubi->avail_pebs += 1;\r\navailable_consumed = 0;\r\n}\r\nubi->beb_rsvd_pebs -= 1;\r\n}\r\nubi->bad_peb_count += 1;\r\nubi->good_peb_count -= 1;\r\nubi_calculate_reserved(ubi);\r\nif (available_consumed)\r\nubi_warn(ubi, "no PEBs in the reserved pool, used an available PEB");\r\nelse if (ubi->beb_rsvd_pebs)\r\nubi_msg(ubi, "%d PEBs left in the reserve",\r\nubi->beb_rsvd_pebs);\r\nelse\r\nubi_warn(ubi, "last PEB from the reserve was used");\r\nspin_unlock(&ubi->volumes_lock);\r\nreturn err;\r\nout_ro:\r\nif (available_consumed) {\r\nspin_lock(&ubi->volumes_lock);\r\nubi->avail_pebs += 1;\r\nspin_unlock(&ubi->volumes_lock);\r\n}\r\nubi_ro_mode(ubi);\r\nreturn err;\r\n}\r\nstatic int erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk,\r\nint shutdown)\r\n{\r\nint ret;\r\nif (shutdown) {\r\nstruct ubi_wl_entry *e = wl_wrk->e;\r\ndbg_wl("cancel erasure of PEB %d EC %d", e->pnum, e->ec);\r\nkfree(wl_wrk);\r\nwl_entry_destroy(ubi, e);\r\nreturn 0;\r\n}\r\nret = __erase_worker(ubi, wl_wrk);\r\nkfree(wl_wrk);\r\nreturn ret;\r\n}\r\nint ubi_wl_put_peb(struct ubi_device *ubi, int vol_id, int lnum,\r\nint pnum, int torture)\r\n{\r\nint err;\r\nstruct ubi_wl_entry *e;\r\ndbg_wl("PEB %d", pnum);\r\nubi_assert(pnum >= 0);\r\nubi_assert(pnum < ubi->peb_count);\r\ndown_read(&ubi->fm_protect);\r\nretry:\r\nspin_lock(&ubi->wl_lock);\r\ne = ubi->lookuptbl[pnum];\r\nif (e == ubi->move_from) {\r\ndbg_wl("PEB %d is being moved, wait", pnum);\r\nspin_unlock(&ubi->wl_lock);\r\nmutex_lock(&ubi->move_mutex);\r\nmutex_unlock(&ubi->move_mutex);\r\ngoto retry;\r\n} else if (e == ubi->move_to) {\r\ndbg_wl("PEB %d is the target of data moving", pnum);\r\nubi_assert(!ubi->move_to_put);\r\nubi->move_to_put = 1;\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->fm_protect);\r\nreturn 0;\r\n} else {\r\nif (in_wl_tree(e, &ubi->used)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->used);\r\nrb_erase(&e->u.rb, &ubi->used);\r\n} else if (in_wl_tree(e, &ubi->scrub)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->scrub);\r\nrb_erase(&e->u.rb, &ubi->scrub);\r\n} else if (in_wl_tree(e, &ubi->erroneous)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->erroneous);\r\nrb_erase(&e->u.rb, &ubi->erroneous);\r\nubi->erroneous_peb_count -= 1;\r\nubi_assert(ubi->erroneous_peb_count >= 0);\r\ntorture = 1;\r\n} else {\r\nerr = prot_queue_del(ubi, e->pnum);\r\nif (err) {\r\nubi_err(ubi, "PEB %d not found", pnum);\r\nubi_ro_mode(ubi);\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->fm_protect);\r\nreturn err;\r\n}\r\n}\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nerr = schedule_erase(ubi, e, vol_id, lnum, torture, false);\r\nif (err) {\r\nspin_lock(&ubi->wl_lock);\r\nwl_tree_add(e, &ubi->used);\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nup_read(&ubi->fm_protect);\r\nreturn err;\r\n}\r\nint ubi_wl_scrub_peb(struct ubi_device *ubi, int pnum)\r\n{\r\nstruct ubi_wl_entry *e;\r\nubi_msg(ubi, "schedule PEB %d for scrubbing", pnum);\r\nretry:\r\nspin_lock(&ubi->wl_lock);\r\ne = ubi->lookuptbl[pnum];\r\nif (e == ubi->move_from || in_wl_tree(e, &ubi->scrub) ||\r\nin_wl_tree(e, &ubi->erroneous)) {\r\nspin_unlock(&ubi->wl_lock);\r\nreturn 0;\r\n}\r\nif (e == ubi->move_to) {\r\nspin_unlock(&ubi->wl_lock);\r\ndbg_wl("the PEB %d is not in proper tree, retry", pnum);\r\nyield();\r\ngoto retry;\r\n}\r\nif (in_wl_tree(e, &ubi->used)) {\r\nself_check_in_wl_tree(ubi, e, &ubi->used);\r\nrb_erase(&e->u.rb, &ubi->used);\r\n} else {\r\nint err;\r\nerr = prot_queue_del(ubi, e->pnum);\r\nif (err) {\r\nubi_err(ubi, "PEB %d not found", pnum);\r\nubi_ro_mode(ubi);\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\n}\r\nwl_tree_add(e, &ubi->scrub);\r\nspin_unlock(&ubi->wl_lock);\r\nreturn ensure_wear_leveling(ubi, 0);\r\n}\r\nint ubi_wl_flush(struct ubi_device *ubi, int vol_id, int lnum)\r\n{\r\nint err = 0;\r\nint found = 1;\r\ndbg_wl("flush pending work for LEB %d:%d (%d pending works)",\r\nvol_id, lnum, ubi->works_count);\r\nwhile (found) {\r\nstruct ubi_work *wrk, *tmp;\r\nfound = 0;\r\ndown_read(&ubi->work_sem);\r\nspin_lock(&ubi->wl_lock);\r\nlist_for_each_entry_safe(wrk, tmp, &ubi->works, list) {\r\nif ((vol_id == UBI_ALL || wrk->vol_id == vol_id) &&\r\n(lnum == UBI_ALL || wrk->lnum == lnum)) {\r\nlist_del(&wrk->list);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\nspin_unlock(&ubi->wl_lock);\r\nerr = wrk->func(ubi, wrk, 0);\r\nif (err) {\r\nup_read(&ubi->work_sem);\r\nreturn err;\r\n}\r\nspin_lock(&ubi->wl_lock);\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->work_sem);\r\n}\r\ndown_write(&ubi->work_sem);\r\nup_write(&ubi->work_sem);\r\nreturn err;\r\n}\r\nstatic void tree_destroy(struct ubi_device *ubi, struct rb_root *root)\r\n{\r\nstruct rb_node *rb;\r\nstruct ubi_wl_entry *e;\r\nrb = root->rb_node;\r\nwhile (rb) {\r\nif (rb->rb_left)\r\nrb = rb->rb_left;\r\nelse if (rb->rb_right)\r\nrb = rb->rb_right;\r\nelse {\r\ne = rb_entry(rb, struct ubi_wl_entry, u.rb);\r\nrb = rb_parent(rb);\r\nif (rb) {\r\nif (rb->rb_left == &e->u.rb)\r\nrb->rb_left = NULL;\r\nelse\r\nrb->rb_right = NULL;\r\n}\r\nwl_entry_destroy(ubi, e);\r\n}\r\n}\r\n}\r\nint ubi_thread(void *u)\r\n{\r\nint failures = 0;\r\nstruct ubi_device *ubi = u;\r\nubi_msg(ubi, "background thread \"%s\" started, PID %d",\r\nubi->bgt_name, task_pid_nr(current));\r\nset_freezable();\r\nfor (;;) {\r\nint err;\r\nif (kthread_should_stop())\r\nbreak;\r\nif (try_to_freeze())\r\ncontinue;\r\nspin_lock(&ubi->wl_lock);\r\nif (list_empty(&ubi->works) || ubi->ro_mode ||\r\n!ubi->thread_enabled || ubi_dbg_is_bgt_disabled(ubi)) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nspin_unlock(&ubi->wl_lock);\r\nschedule();\r\ncontinue;\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nerr = do_work(ubi);\r\nif (err) {\r\nubi_err(ubi, "%s: work failed with error code %d",\r\nubi->bgt_name, err);\r\nif (failures++ > WL_MAX_FAILURES) {\r\nubi_msg(ubi, "%s: %d consecutive failures",\r\nubi->bgt_name, WL_MAX_FAILURES);\r\nubi_ro_mode(ubi);\r\nubi->thread_enabled = 0;\r\ncontinue;\r\n}\r\n} else\r\nfailures = 0;\r\ncond_resched();\r\n}\r\ndbg_wl("background thread \"%s\" is killed", ubi->bgt_name);\r\nreturn 0;\r\n}\r\nstatic void shutdown_work(struct ubi_device *ubi)\r\n{\r\n#ifdef CONFIG_MTD_UBI_FASTMAP\r\nflush_work(&ubi->fm_work);\r\n#endif\r\nwhile (!list_empty(&ubi->works)) {\r\nstruct ubi_work *wrk;\r\nwrk = list_entry(ubi->works.next, struct ubi_work, list);\r\nlist_del(&wrk->list);\r\nwrk->func(ubi, wrk, 1);\r\nubi->works_count -= 1;\r\nubi_assert(ubi->works_count >= 0);\r\n}\r\n}\r\nint ubi_wl_init(struct ubi_device *ubi, struct ubi_attach_info *ai)\r\n{\r\nint err, i, reserved_pebs, found_pebs = 0;\r\nstruct rb_node *rb1, *rb2;\r\nstruct ubi_ainf_volume *av;\r\nstruct ubi_ainf_peb *aeb, *tmp;\r\nstruct ubi_wl_entry *e;\r\nubi->used = ubi->erroneous = ubi->free = ubi->scrub = RB_ROOT;\r\nspin_lock_init(&ubi->wl_lock);\r\nmutex_init(&ubi->move_mutex);\r\ninit_rwsem(&ubi->work_sem);\r\nubi->max_ec = ai->max_ec;\r\nINIT_LIST_HEAD(&ubi->works);\r\nsprintf(ubi->bgt_name, UBI_BGT_NAME_PATTERN, ubi->ubi_num);\r\nerr = -ENOMEM;\r\nubi->lookuptbl = kzalloc(ubi->peb_count * sizeof(void *), GFP_KERNEL);\r\nif (!ubi->lookuptbl)\r\nreturn err;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; i++)\r\nINIT_LIST_HEAD(&ubi->pq[i]);\r\nubi->pq_head = 0;\r\nubi->free_count = 0;\r\nlist_for_each_entry_safe(aeb, tmp, &ai->erase, u.list) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi->lookuptbl[e->pnum] = e;\r\nif (schedule_erase(ubi, e, aeb->vol_id, aeb->lnum, 0, false)) {\r\nwl_entry_destroy(ubi, e);\r\ngoto out_free;\r\n}\r\nfound_pebs++;\r\n}\r\nlist_for_each_entry(aeb, &ai->free, u.list) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi_assert(e->ec >= 0);\r\nwl_tree_add(e, &ubi->free);\r\nubi->free_count++;\r\nubi->lookuptbl[e->pnum] = e;\r\nfound_pebs++;\r\n}\r\nubi_rb_for_each_entry(rb1, av, &ai->volumes, rb) {\r\nubi_rb_for_each_entry(rb2, aeb, &av->root, u.rb) {\r\ncond_resched();\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi->lookuptbl[e->pnum] = e;\r\nif (!aeb->scrub) {\r\ndbg_wl("add PEB %d EC %d to the used tree",\r\ne->pnum, e->ec);\r\nwl_tree_add(e, &ubi->used);\r\n} else {\r\ndbg_wl("add PEB %d EC %d to the scrub tree",\r\ne->pnum, e->ec);\r\nwl_tree_add(e, &ubi->scrub);\r\n}\r\nfound_pebs++;\r\n}\r\n}\r\nlist_for_each_entry(aeb, &ai->fastmap, u.list) {\r\ncond_resched();\r\ne = ubi_find_fm_block(ubi, aeb->pnum);\r\nif (e) {\r\nubi_assert(!ubi->lookuptbl[e->pnum]);\r\nubi->lookuptbl[e->pnum] = e;\r\n} else {\r\nif (ubi->lookuptbl[aeb->pnum])\r\ncontinue;\r\ne = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\r\nif (!e)\r\ngoto out_free;\r\ne->pnum = aeb->pnum;\r\ne->ec = aeb->ec;\r\nubi_assert(!ubi->lookuptbl[e->pnum]);\r\nubi->lookuptbl[e->pnum] = e;\r\nif (schedule_erase(ubi, e, aeb->vol_id, aeb->lnum, 0, false)) {\r\nwl_entry_destroy(ubi, e);\r\ngoto out_free;\r\n}\r\n}\r\nfound_pebs++;\r\n}\r\ndbg_wl("found %i PEBs", found_pebs);\r\nubi_assert(ubi->good_peb_count == found_pebs);\r\nreserved_pebs = WL_RESERVED_PEBS;\r\nubi_fastmap_init(ubi, &reserved_pebs);\r\nif (ubi->avail_pebs < reserved_pebs) {\r\nubi_err(ubi, "no enough physical eraseblocks (%d, need %d)",\r\nubi->avail_pebs, reserved_pebs);\r\nif (ubi->corr_peb_count)\r\nubi_err(ubi, "%d PEBs are corrupted and not used",\r\nubi->corr_peb_count);\r\nerr = -ENOSPC;\r\ngoto out_free;\r\n}\r\nubi->avail_pebs -= reserved_pebs;\r\nubi->rsvd_pebs += reserved_pebs;\r\nerr = ensure_wear_leveling(ubi, 0);\r\nif (err)\r\ngoto out_free;\r\nreturn 0;\r\nout_free:\r\nshutdown_work(ubi);\r\ntree_destroy(ubi, &ubi->used);\r\ntree_destroy(ubi, &ubi->free);\r\ntree_destroy(ubi, &ubi->scrub);\r\nkfree(ubi->lookuptbl);\r\nreturn err;\r\n}\r\nstatic void protection_queue_destroy(struct ubi_device *ubi)\r\n{\r\nint i;\r\nstruct ubi_wl_entry *e, *tmp;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; ++i) {\r\nlist_for_each_entry_safe(e, tmp, &ubi->pq[i], u.list) {\r\nlist_del(&e->u.list);\r\nwl_entry_destroy(ubi, e);\r\n}\r\n}\r\n}\r\nvoid ubi_wl_close(struct ubi_device *ubi)\r\n{\r\ndbg_wl("close the WL sub-system");\r\nubi_fastmap_close(ubi);\r\nshutdown_work(ubi);\r\nprotection_queue_destroy(ubi);\r\ntree_destroy(ubi, &ubi->used);\r\ntree_destroy(ubi, &ubi->erroneous);\r\ntree_destroy(ubi, &ubi->free);\r\ntree_destroy(ubi, &ubi->scrub);\r\nkfree(ubi->lookuptbl);\r\n}\r\nstatic int self_check_ec(struct ubi_device *ubi, int pnum, int ec)\r\n{\r\nint err;\r\nlong long read_ec;\r\nstruct ubi_ec_hdr *ec_hdr;\r\nif (!ubi_dbg_chk_gen(ubi))\r\nreturn 0;\r\nec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_NOFS);\r\nif (!ec_hdr)\r\nreturn -ENOMEM;\r\nerr = ubi_io_read_ec_hdr(ubi, pnum, ec_hdr, 0);\r\nif (err && err != UBI_IO_BITFLIPS) {\r\nerr = 0;\r\ngoto out_free;\r\n}\r\nread_ec = be64_to_cpu(ec_hdr->ec);\r\nif (ec != read_ec && read_ec - ec > 1) {\r\nubi_err(ubi, "self-check failed for PEB %d", pnum);\r\nubi_err(ubi, "read EC is %lld, should be %d", read_ec, ec);\r\ndump_stack();\r\nerr = 1;\r\n} else\r\nerr = 0;\r\nout_free:\r\nkfree(ec_hdr);\r\nreturn err;\r\n}\r\nstatic int self_check_in_wl_tree(const struct ubi_device *ubi,\r\nstruct ubi_wl_entry *e, struct rb_root *root)\r\n{\r\nif (!ubi_dbg_chk_gen(ubi))\r\nreturn 0;\r\nif (in_wl_tree(e, root))\r\nreturn 0;\r\nubi_err(ubi, "self-check failed for PEB %d, EC %d, RB-tree %p ",\r\ne->pnum, e->ec, root);\r\ndump_stack();\r\nreturn -EINVAL;\r\n}\r\nstatic int self_check_in_pq(const struct ubi_device *ubi,\r\nstruct ubi_wl_entry *e)\r\n{\r\nstruct ubi_wl_entry *p;\r\nint i;\r\nif (!ubi_dbg_chk_gen(ubi))\r\nreturn 0;\r\nfor (i = 0; i < UBI_PROT_QUEUE_LEN; ++i)\r\nlist_for_each_entry(p, &ubi->pq[i], u.list)\r\nif (p == e)\r\nreturn 0;\r\nubi_err(ubi, "self-check failed for PEB %d, EC %d, Protect queue",\r\ne->pnum, e->ec);\r\ndump_stack();\r\nreturn -EINVAL;\r\n}\r\nstatic struct ubi_wl_entry *get_peb_for_wl(struct ubi_device *ubi)\r\n{\r\nstruct ubi_wl_entry *e;\r\ne = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nubi->free_count--;\r\nubi_assert(ubi->free_count >= 0);\r\nrb_erase(&e->u.rb, &ubi->free);\r\nreturn e;\r\n}\r\nstatic int produce_free_peb(struct ubi_device *ubi)\r\n{\r\nint err;\r\nwhile (!ubi->free.rb_node && ubi->works_count) {\r\nspin_unlock(&ubi->wl_lock);\r\ndbg_wl("do one work synchronously");\r\nerr = do_work(ubi);\r\nspin_lock(&ubi->wl_lock);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nint ubi_wl_get_peb(struct ubi_device *ubi)\r\n{\r\nint err;\r\nstruct ubi_wl_entry *e;\r\nretry:\r\ndown_read(&ubi->fm_eba_sem);\r\nspin_lock(&ubi->wl_lock);\r\nif (!ubi->free.rb_node) {\r\nif (ubi->works_count == 0) {\r\nubi_err(ubi, "no free eraseblocks");\r\nubi_assert(list_empty(&ubi->works));\r\nspin_unlock(&ubi->wl_lock);\r\nreturn -ENOSPC;\r\n}\r\nerr = produce_free_peb(ubi);\r\nif (err < 0) {\r\nspin_unlock(&ubi->wl_lock);\r\nreturn err;\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->fm_eba_sem);\r\ngoto retry;\r\n}\r\ne = wl_get_wle(ubi);\r\nprot_queue_add(ubi, e);\r\nspin_unlock(&ubi->wl_lock);\r\nerr = ubi_self_check_all_ff(ubi, e->pnum, ubi->vid_hdr_aloffset,\r\nubi->peb_size - ubi->vid_hdr_aloffset);\r\nif (err) {\r\nubi_err(ubi, "new PEB %d does not contain all 0xFF bytes", e->pnum);\r\nreturn err;\r\n}\r\nreturn e->pnum;\r\n}
