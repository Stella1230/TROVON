static void uv_program_mmr(struct irq_cfg *cfg, struct uv_irq_2_mmr_pnode *info)\r\n{\r\nunsigned long mmr_value;\r\nstruct uv_IO_APIC_route_entry *entry;\r\nBUILD_BUG_ON(sizeof(struct uv_IO_APIC_route_entry) !=\r\nsizeof(unsigned long));\r\nmmr_value = 0;\r\nentry = (struct uv_IO_APIC_route_entry *)&mmr_value;\r\nentry->vector = cfg->vector;\r\nentry->delivery_mode = apic->irq_delivery_mode;\r\nentry->dest_mode = apic->irq_dest_mode;\r\nentry->polarity = 0;\r\nentry->trigger = 0;\r\nentry->mask = 0;\r\nentry->dest = cfg->dest_apicid;\r\nuv_write_global_mmr64(info->pnode, info->offset, mmr_value);\r\n}\r\nstatic void uv_noop(struct irq_data *data) { }\r\nstatic void uv_ack_apic(struct irq_data *data)\r\n{\r\nack_APIC_irq();\r\n}\r\nstatic int\r\nuv_set_irq_affinity(struct irq_data *data, const struct cpumask *mask,\r\nbool force)\r\n{\r\nstruct irq_data *parent = data->parent_data;\r\nstruct irq_cfg *cfg = irqd_cfg(data);\r\nint ret;\r\nret = parent->chip->irq_set_affinity(parent, mask, force);\r\nif (ret >= 0) {\r\nuv_program_mmr(cfg, data->chip_data);\r\nsend_cleanup_vector(cfg);\r\n}\r\nreturn ret;\r\n}\r\nstatic int uv_domain_alloc(struct irq_domain *domain, unsigned int virq,\r\nunsigned int nr_irqs, void *arg)\r\n{\r\nstruct uv_irq_2_mmr_pnode *chip_data;\r\nstruct irq_alloc_info *info = arg;\r\nstruct irq_data *irq_data = irq_domain_get_irq_data(domain, virq);\r\nint ret;\r\nif (nr_irqs > 1 || !info || info->type != X86_IRQ_ALLOC_TYPE_UV)\r\nreturn -EINVAL;\r\nchip_data = kmalloc_node(sizeof(*chip_data), GFP_KERNEL,\r\nirq_data_get_node(irq_data));\r\nif (!chip_data)\r\nreturn -ENOMEM;\r\nret = irq_domain_alloc_irqs_parent(domain, virq, nr_irqs, arg);\r\nif (ret >= 0) {\r\nif (info->uv_limit == UV_AFFINITY_CPU)\r\nirq_set_status_flags(virq, IRQ_NO_BALANCING);\r\nelse\r\nirq_set_status_flags(virq, IRQ_MOVE_PCNTXT);\r\nchip_data->pnode = uv_blade_to_pnode(info->uv_blade);\r\nchip_data->offset = info->uv_offset;\r\nirq_domain_set_info(domain, virq, virq, &uv_irq_chip, chip_data,\r\nhandle_percpu_irq, NULL, info->uv_name);\r\n} else {\r\nkfree(chip_data);\r\n}\r\nreturn ret;\r\n}\r\nstatic void uv_domain_free(struct irq_domain *domain, unsigned int virq,\r\nunsigned int nr_irqs)\r\n{\r\nstruct irq_data *irq_data = irq_domain_get_irq_data(domain, virq);\r\nBUG_ON(nr_irqs != 1);\r\nkfree(irq_data->chip_data);\r\nirq_clear_status_flags(virq, IRQ_MOVE_PCNTXT);\r\nirq_clear_status_flags(virq, IRQ_NO_BALANCING);\r\nirq_domain_free_irqs_top(domain, virq, nr_irqs);\r\n}\r\nstatic void uv_domain_activate(struct irq_domain *domain,\r\nstruct irq_data *irq_data)\r\n{\r\nuv_program_mmr(irqd_cfg(irq_data), irq_data->chip_data);\r\n}\r\nstatic void uv_domain_deactivate(struct irq_domain *domain,\r\nstruct irq_data *irq_data)\r\n{\r\nunsigned long mmr_value;\r\nstruct uv_IO_APIC_route_entry *entry;\r\nmmr_value = 0;\r\nentry = (struct uv_IO_APIC_route_entry *)&mmr_value;\r\nentry->mask = 1;\r\nuv_program_mmr(irqd_cfg(irq_data), irq_data->chip_data);\r\n}\r\nstatic struct irq_domain *uv_get_irq_domain(void)\r\n{\r\nstatic struct irq_domain *uv_domain;\r\nstatic DEFINE_MUTEX(uv_lock);\r\nmutex_lock(&uv_lock);\r\nif (uv_domain == NULL) {\r\nuv_domain = irq_domain_add_tree(NULL, &uv_domain_ops, NULL);\r\nif (uv_domain)\r\nuv_domain->parent = x86_vector_domain;\r\n}\r\nmutex_unlock(&uv_lock);\r\nreturn uv_domain;\r\n}\r\nint uv_setup_irq(char *irq_name, int cpu, int mmr_blade,\r\nunsigned long mmr_offset, int limit)\r\n{\r\nstruct irq_alloc_info info;\r\nstruct irq_domain *domain = uv_get_irq_domain();\r\nif (!domain)\r\nreturn -ENOMEM;\r\ninit_irq_alloc_info(&info, cpumask_of(cpu));\r\ninfo.type = X86_IRQ_ALLOC_TYPE_UV;\r\ninfo.uv_limit = limit;\r\ninfo.uv_blade = mmr_blade;\r\ninfo.uv_offset = mmr_offset;\r\ninfo.uv_name = irq_name;\r\nreturn irq_domain_alloc_irqs(domain, 1,\r\nuv_blade_to_memory_nid(mmr_blade), &info);\r\n}\r\nvoid uv_teardown_irq(unsigned int irq)\r\n{\r\nirq_domain_free_irqs(irq, 1);\r\n}
