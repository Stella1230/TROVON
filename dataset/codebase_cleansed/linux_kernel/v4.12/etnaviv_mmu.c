static int etnaviv_fault_handler(struct iommu_domain *iommu, struct device *dev,\r\nunsigned long iova, int flags, void *arg)\r\n{\r\nDBG("*** fault: iova=%08lx, flags=%d", iova, flags);\r\nreturn 0;\r\n}\r\nint etnaviv_iommu_map(struct etnaviv_iommu *iommu, u32 iova,\r\nstruct sg_table *sgt, unsigned len, int prot)\r\n{\r\nstruct iommu_domain *domain = iommu->domain;\r\nstruct scatterlist *sg;\r\nunsigned int da = iova;\r\nunsigned int i, j;\r\nint ret;\r\nif (!domain || !sgt)\r\nreturn -EINVAL;\r\nfor_each_sg(sgt->sgl, sg, sgt->nents, i) {\r\nu32 pa = sg_dma_address(sg) - sg->offset;\r\nsize_t bytes = sg_dma_len(sg) + sg->offset;\r\nVERB("map[%d]: %08x %08x(%zx)", i, iova, pa, bytes);\r\nret = iommu_map(domain, da, pa, bytes, prot);\r\nif (ret)\r\ngoto fail;\r\nda += bytes;\r\n}\r\nreturn 0;\r\nfail:\r\nda = iova;\r\nfor_each_sg(sgt->sgl, sg, i, j) {\r\nsize_t bytes = sg_dma_len(sg) + sg->offset;\r\niommu_unmap(domain, da, bytes);\r\nda += bytes;\r\n}\r\nreturn ret;\r\n}\r\nint etnaviv_iommu_unmap(struct etnaviv_iommu *iommu, u32 iova,\r\nstruct sg_table *sgt, unsigned len)\r\n{\r\nstruct iommu_domain *domain = iommu->domain;\r\nstruct scatterlist *sg;\r\nunsigned int da = iova;\r\nint i;\r\nfor_each_sg(sgt->sgl, sg, sgt->nents, i) {\r\nsize_t bytes = sg_dma_len(sg) + sg->offset;\r\nsize_t unmapped;\r\nunmapped = iommu_unmap(domain, da, bytes);\r\nif (unmapped < bytes)\r\nreturn unmapped;\r\nVERB("unmap[%d]: %08x(%zx)", i, iova, bytes);\r\nBUG_ON(!PAGE_ALIGNED(bytes));\r\nda += bytes;\r\n}\r\nreturn 0;\r\n}\r\nstatic void etnaviv_iommu_remove_mapping(struct etnaviv_iommu *mmu,\r\nstruct etnaviv_vram_mapping *mapping)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = mapping->object;\r\netnaviv_iommu_unmap(mmu, mapping->vram_node.start,\r\netnaviv_obj->sgt, etnaviv_obj->base.size);\r\ndrm_mm_remove_node(&mapping->vram_node);\r\n}\r\nstatic int etnaviv_iommu_find_iova(struct etnaviv_iommu *mmu,\r\nstruct drm_mm_node *node, size_t size)\r\n{\r\nstruct etnaviv_vram_mapping *free = NULL;\r\nenum drm_mm_insert_mode mode = DRM_MM_INSERT_LOW;\r\nint ret;\r\nlockdep_assert_held(&mmu->lock);\r\nwhile (1) {\r\nstruct etnaviv_vram_mapping *m, *n;\r\nstruct drm_mm_scan scan;\r\nstruct list_head list;\r\nbool found;\r\nret = drm_mm_insert_node_in_range(&mmu->mm, node,\r\nsize, 0, 0,\r\nmmu->last_iova, U64_MAX,\r\nmode);\r\nif (ret != -ENOSPC)\r\nbreak;\r\nif (mmu->last_iova) {\r\nmmu->last_iova = 0;\r\nmmu->need_flush = true;\r\ncontinue;\r\n}\r\ndrm_mm_scan_init(&scan, &mmu->mm, size, 0, 0, mode);\r\nfound = 0;\r\nINIT_LIST_HEAD(&list);\r\nlist_for_each_entry(free, &mmu->mappings, mmu_node) {\r\nif (!free->vram_node.mm)\r\ncontinue;\r\nif (free->use)\r\ncontinue;\r\nlist_add(&free->scan_node, &list);\r\nif (drm_mm_scan_add_block(&scan, &free->vram_node)) {\r\nfound = true;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nlist_for_each_entry_safe(m, n, &list, scan_node)\r\nBUG_ON(drm_mm_scan_remove_block(&scan, &m->vram_node));\r\nbreak;\r\n}\r\nlist_for_each_entry_safe(m, n, &list, scan_node)\r\nif (!drm_mm_scan_remove_block(&scan, &m->vram_node))\r\nlist_del_init(&m->scan_node);\r\nlist_for_each_entry_safe(m, n, &list, scan_node) {\r\netnaviv_iommu_remove_mapping(mmu, m);\r\nm->mmu = NULL;\r\nlist_del_init(&m->mmu_node);\r\nlist_del_init(&m->scan_node);\r\n}\r\nmode = DRM_MM_INSERT_EVICT;\r\n}\r\nreturn ret;\r\n}\r\nint etnaviv_iommu_map_gem(struct etnaviv_iommu *mmu,\r\nstruct etnaviv_gem_object *etnaviv_obj, u32 memory_base,\r\nstruct etnaviv_vram_mapping *mapping)\r\n{\r\nstruct sg_table *sgt = etnaviv_obj->sgt;\r\nstruct drm_mm_node *node;\r\nint ret;\r\nlockdep_assert_held(&etnaviv_obj->lock);\r\nmutex_lock(&mmu->lock);\r\nif (mmu->version == ETNAVIV_IOMMU_V1 &&\r\nsgt->nents == 1 && !(etnaviv_obj->flags & ETNA_BO_FORCE_MMU)) {\r\nu32 iova;\r\niova = sg_dma_address(sgt->sgl) - memory_base;\r\nif (iova < 0x80000000 - sg_dma_len(sgt->sgl)) {\r\nmapping->iova = iova;\r\nlist_add_tail(&mapping->mmu_node, &mmu->mappings);\r\nmutex_unlock(&mmu->lock);\r\nreturn 0;\r\n}\r\n}\r\nnode = &mapping->vram_node;\r\nret = etnaviv_iommu_find_iova(mmu, node, etnaviv_obj->base.size);\r\nif (ret < 0) {\r\nmutex_unlock(&mmu->lock);\r\nreturn ret;\r\n}\r\nmmu->last_iova = node->start + etnaviv_obj->base.size;\r\nmapping->iova = node->start;\r\nret = etnaviv_iommu_map(mmu, node->start, sgt, etnaviv_obj->base.size,\r\nIOMMU_READ | IOMMU_WRITE);\r\nif (ret < 0) {\r\ndrm_mm_remove_node(node);\r\nmutex_unlock(&mmu->lock);\r\nreturn ret;\r\n}\r\nlist_add_tail(&mapping->mmu_node, &mmu->mappings);\r\nmmu->need_flush = true;\r\nmutex_unlock(&mmu->lock);\r\nreturn ret;\r\n}\r\nvoid etnaviv_iommu_unmap_gem(struct etnaviv_iommu *mmu,\r\nstruct etnaviv_vram_mapping *mapping)\r\n{\r\nWARN_ON(mapping->use);\r\nmutex_lock(&mmu->lock);\r\nif (mapping->vram_node.mm == &mmu->mm)\r\netnaviv_iommu_remove_mapping(mmu, mapping);\r\nlist_del(&mapping->mmu_node);\r\nmmu->need_flush = true;\r\nmutex_unlock(&mmu->lock);\r\n}\r\nvoid etnaviv_iommu_destroy(struct etnaviv_iommu *mmu)\r\n{\r\ndrm_mm_takedown(&mmu->mm);\r\niommu_domain_free(mmu->domain);\r\nkfree(mmu);\r\n}\r\nstruct etnaviv_iommu *etnaviv_iommu_new(struct etnaviv_gpu *gpu)\r\n{\r\nenum etnaviv_iommu_version version;\r\nstruct etnaviv_iommu *mmu;\r\nmmu = kzalloc(sizeof(*mmu), GFP_KERNEL);\r\nif (!mmu)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (!(gpu->identity.minor_features1 & chipMinorFeatures1_MMU_VERSION)) {\r\nmmu->domain = etnaviv_iommuv1_domain_alloc(gpu);\r\nversion = ETNAVIV_IOMMU_V1;\r\n} else {\r\nmmu->domain = etnaviv_iommuv2_domain_alloc(gpu);\r\nversion = ETNAVIV_IOMMU_V2;\r\n}\r\nif (!mmu->domain) {\r\ndev_err(gpu->dev, "Failed to allocate GPU IOMMU domain\n");\r\nkfree(mmu);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nmmu->gpu = gpu;\r\nmmu->version = version;\r\nmutex_init(&mmu->lock);\r\nINIT_LIST_HEAD(&mmu->mappings);\r\ndrm_mm_init(&mmu->mm, mmu->domain->geometry.aperture_start,\r\nmmu->domain->geometry.aperture_end -\r\nmmu->domain->geometry.aperture_start + 1);\r\niommu_set_fault_handler(mmu->domain, etnaviv_fault_handler, gpu->dev);\r\nreturn mmu;\r\n}\r\nvoid etnaviv_iommu_restore(struct etnaviv_gpu *gpu)\r\n{\r\nif (gpu->mmu->version == ETNAVIV_IOMMU_V1)\r\netnaviv_iommuv1_restore(gpu);\r\nelse\r\netnaviv_iommuv2_restore(gpu);\r\n}\r\nint etnaviv_iommu_get_suballoc_va(struct etnaviv_gpu *gpu, dma_addr_t paddr,\r\nstruct drm_mm_node *vram_node, size_t size,\r\nu32 *iova)\r\n{\r\nstruct etnaviv_iommu *mmu = gpu->mmu;\r\nif (mmu->version == ETNAVIV_IOMMU_V1) {\r\n*iova = paddr - gpu->memory_base;\r\nreturn 0;\r\n} else {\r\nint ret;\r\nmutex_lock(&mmu->lock);\r\nret = etnaviv_iommu_find_iova(mmu, vram_node, size);\r\nif (ret < 0) {\r\nmutex_unlock(&mmu->lock);\r\nreturn ret;\r\n}\r\nret = iommu_map(mmu->domain, vram_node->start, paddr, size,\r\nIOMMU_READ);\r\nif (ret < 0) {\r\ndrm_mm_remove_node(vram_node);\r\nmutex_unlock(&mmu->lock);\r\nreturn ret;\r\n}\r\nmmu->last_iova = vram_node->start + size;\r\ngpu->mmu->need_flush = true;\r\nmutex_unlock(&mmu->lock);\r\n*iova = (u32)vram_node->start;\r\nreturn 0;\r\n}\r\n}\r\nvoid etnaviv_iommu_put_suballoc_va(struct etnaviv_gpu *gpu,\r\nstruct drm_mm_node *vram_node, size_t size,\r\nu32 iova)\r\n{\r\nstruct etnaviv_iommu *mmu = gpu->mmu;\r\nif (mmu->version == ETNAVIV_IOMMU_V2) {\r\nmutex_lock(&mmu->lock);\r\niommu_unmap(mmu->domain,iova, size);\r\ndrm_mm_remove_node(vram_node);\r\nmutex_unlock(&mmu->lock);\r\n}\r\n}\r\nsize_t etnaviv_iommu_dump_size(struct etnaviv_iommu *iommu)\r\n{\r\nstruct etnaviv_iommu_ops *ops;\r\nops = container_of(iommu->domain->ops, struct etnaviv_iommu_ops, ops);\r\nreturn ops->dump_size(iommu->domain);\r\n}\r\nvoid etnaviv_iommu_dump(struct etnaviv_iommu *iommu, void *buf)\r\n{\r\nstruct etnaviv_iommu_ops *ops;\r\nops = container_of(iommu->domain->ops, struct etnaviv_iommu_ops, ops);\r\nops->dump(iommu->domain, buf);\r\n}
