static u8 select_channel(void)\r\n{\r\nu8 chan_idx = atomic_inc_return(&iproc_priv.next_chan);\r\nreturn chan_idx % iproc_priv.spu.num_spu;\r\n}\r\nstatic int\r\nspu_ablkcipher_rx_sg_create(struct brcm_message *mssg,\r\nstruct iproc_reqctx_s *rctx,\r\nu8 rx_frag_num,\r\nunsigned int chunksize, u32 stat_pad_len)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct scatterlist *sg;\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nu32 datalen;\r\nmssg->spu.dst = kcalloc(rx_frag_num, sizeof(struct scatterlist),\r\nrctx->gfp);\r\nif (!mssg->spu.dst)\r\nreturn -ENOMEM;\r\nsg = mssg->spu.dst;\r\nsg_init_table(sg, rx_frag_num);\r\nsg_set_buf(sg++, rctx->msg_buf.spu_resp_hdr, ctx->spu_resp_hdr_len);\r\nif ((ctx->cipher.mode == CIPHER_MODE_XTS) &&\r\nspu->spu_xts_tweak_in_payload())\r\nsg_set_buf(sg++, rctx->msg_buf.c.supdt_tweak,\r\nSPU_XTS_TWEAK_SIZE);\r\ndatalen = spu_msg_sg_add(&sg, &rctx->dst_sg, &rctx->dst_skip,\r\nrctx->dst_nents, chunksize);\r\nif (datalen < chunksize) {\r\npr_err("%s(): failed to copy dst sg to mbox msg. chunksize %u, datalen %u",\r\n__func__, chunksize, datalen);\r\nreturn -EFAULT;\r\n}\r\nif (ctx->cipher.alg == CIPHER_ALG_RC4)\r\nsg_set_buf(sg++, rctx->msg_buf.c.supdt_tweak, SPU_SUPDT_LEN);\r\nif (stat_pad_len)\r\nsg_set_buf(sg++, rctx->msg_buf.rx_stat_pad, stat_pad_len);\r\nmemset(rctx->msg_buf.rx_stat, 0, SPU_RX_STATUS_LEN);\r\nsg_set_buf(sg, rctx->msg_buf.rx_stat, spu->spu_rx_status_len());\r\nreturn 0;\r\n}\r\nstatic int\r\nspu_ablkcipher_tx_sg_create(struct brcm_message *mssg,\r\nstruct iproc_reqctx_s *rctx,\r\nu8 tx_frag_num, unsigned int chunksize, u32 pad_len)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct scatterlist *sg;\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nu32 datalen;\r\nu32 stat_len;\r\nmssg->spu.src = kcalloc(tx_frag_num, sizeof(struct scatterlist),\r\nrctx->gfp);\r\nif (unlikely(!mssg->spu.src))\r\nreturn -ENOMEM;\r\nsg = mssg->spu.src;\r\nsg_init_table(sg, tx_frag_num);\r\nsg_set_buf(sg++, rctx->msg_buf.bcm_spu_req_hdr,\r\nBCM_HDR_LEN + ctx->spu_req_hdr_len);\r\nif ((ctx->cipher.mode == CIPHER_MODE_XTS) &&\r\nspu->spu_xts_tweak_in_payload())\r\nsg_set_buf(sg++, rctx->msg_buf.iv_ctr, SPU_XTS_TWEAK_SIZE);\r\ndatalen = spu_msg_sg_add(&sg, &rctx->src_sg, &rctx->src_skip,\r\nrctx->src_nents, chunksize);\r\nif (unlikely(datalen < chunksize)) {\r\npr_err("%s(): failed to copy src sg to mbox msg",\r\n__func__);\r\nreturn -EFAULT;\r\n}\r\nif (pad_len)\r\nsg_set_buf(sg++, rctx->msg_buf.spu_req_pad, pad_len);\r\nstat_len = spu->spu_tx_status_len();\r\nif (stat_len) {\r\nmemset(rctx->msg_buf.tx_stat, 0, stat_len);\r\nsg_set_buf(sg, rctx->msg_buf.tx_stat, stat_len);\r\n}\r\nreturn 0;\r\n}\r\nstatic int handle_ablkcipher_req(struct iproc_reqctx_s *rctx)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct crypto_async_request *areq = rctx->parent;\r\nstruct ablkcipher_request *req =\r\ncontainer_of(areq, struct ablkcipher_request, base);\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nstruct spu_cipher_parms cipher_parms;\r\nint err = 0;\r\nunsigned int chunksize = 0;\r\nint remaining = 0;\r\nint chunk_start;\r\nu8 local_iv_ctr[MAX_IV_SIZE];\r\nu32 stat_pad_len;\r\nu32 pad_len;\r\nbool update_key = false;\r\nstruct brcm_message *mssg;\r\nint retry_cnt = 0;\r\nu8 rx_frag_num = 2;\r\nu8 tx_frag_num = 1;\r\nflow_log("%s\n", __func__);\r\ncipher_parms.alg = ctx->cipher.alg;\r\ncipher_parms.mode = ctx->cipher.mode;\r\ncipher_parms.type = ctx->cipher_type;\r\ncipher_parms.key_len = ctx->enckeylen;\r\ncipher_parms.key_buf = ctx->enckey;\r\ncipher_parms.iv_buf = local_iv_ctr;\r\ncipher_parms.iv_len = rctx->iv_ctr_len;\r\nmssg = &rctx->mb_mssg;\r\nchunk_start = rctx->src_sent;\r\nremaining = rctx->total_todo - chunk_start;\r\nif ((ctx->max_payload != SPU_MAX_PAYLOAD_INF) &&\r\n(remaining > ctx->max_payload))\r\nchunksize = ctx->max_payload;\r\nelse\r\nchunksize = remaining;\r\nrctx->src_sent += chunksize;\r\nrctx->total_sent = rctx->src_sent;\r\nrctx->src_nents = spu_sg_count(rctx->src_sg, rctx->src_skip, chunksize);\r\nrctx->dst_nents = spu_sg_count(rctx->dst_sg, rctx->dst_skip, chunksize);\r\nif ((ctx->cipher.mode == CIPHER_MODE_CBC) &&\r\nrctx->is_encrypt && chunk_start)\r\nsg_copy_part_to_buf(req->dst, rctx->msg_buf.iv_ctr,\r\nrctx->iv_ctr_len,\r\nchunk_start - rctx->iv_ctr_len);\r\nif (rctx->iv_ctr_len) {\r\n__builtin_memcpy(local_iv_ctr, rctx->msg_buf.iv_ctr,\r\nrctx->iv_ctr_len);\r\nif ((ctx->cipher.mode == CIPHER_MODE_CBC) &&\r\n!rctx->is_encrypt) {\r\nsg_copy_part_to_buf(req->src, rctx->msg_buf.iv_ctr,\r\nrctx->iv_ctr_len,\r\nrctx->src_sent - rctx->iv_ctr_len);\r\n} else if (ctx->cipher.mode == CIPHER_MODE_CTR) {\r\nadd_to_ctr(rctx->msg_buf.iv_ctr, chunksize >> 4);\r\n}\r\n}\r\nif (ctx->cipher.alg == CIPHER_ALG_RC4) {\r\nrx_frag_num++;\r\nif (chunk_start) {\r\ncipher_parms.key_buf = rctx->msg_buf.c.supdt_tweak;\r\nupdate_key = true;\r\ncipher_parms.type = CIPHER_TYPE_UPDT;\r\n} else if (!rctx->is_encrypt) {\r\nupdate_key = true;\r\ncipher_parms.type = CIPHER_TYPE_INIT;\r\n}\r\n}\r\nif (ctx->max_payload == SPU_MAX_PAYLOAD_INF)\r\nflow_log("max_payload infinite\n");\r\nelse\r\nflow_log("max_payload %u\n", ctx->max_payload);\r\nflow_log("sent:%u start:%u remains:%u size:%u\n",\r\nrctx->src_sent, chunk_start, remaining, chunksize);\r\nmemcpy(rctx->msg_buf.bcm_spu_req_hdr, ctx->bcm_spu_req_hdr,\r\nsizeof(rctx->msg_buf.bcm_spu_req_hdr));\r\nspu->spu_cipher_req_finish(rctx->msg_buf.bcm_spu_req_hdr + BCM_HDR_LEN,\r\nctx->spu_req_hdr_len, !(rctx->is_encrypt),\r\n&cipher_parms, update_key, chunksize);\r\natomic64_add(chunksize, &iproc_priv.bytes_out);\r\nstat_pad_len = spu->spu_wordalign_padlen(chunksize);\r\nif (stat_pad_len)\r\nrx_frag_num++;\r\npad_len = stat_pad_len;\r\nif (pad_len) {\r\ntx_frag_num++;\r\nspu->spu_request_pad(rctx->msg_buf.spu_req_pad, 0,\r\n0, ctx->auth.alg, ctx->auth.mode,\r\nrctx->total_sent, stat_pad_len);\r\n}\r\nspu->spu_dump_msg_hdr(rctx->msg_buf.bcm_spu_req_hdr + BCM_HDR_LEN,\r\nctx->spu_req_hdr_len);\r\npacket_log("payload:\n");\r\ndump_sg(rctx->src_sg, rctx->src_skip, chunksize);\r\npacket_dump(" pad: ", rctx->msg_buf.spu_req_pad, pad_len);\r\nmemset(mssg, 0, sizeof(*mssg));\r\nmssg->type = BRCM_MESSAGE_SPU;\r\nmssg->ctx = rctx;\r\nrx_frag_num += rctx->dst_nents;\r\nif ((ctx->cipher.mode == CIPHER_MODE_XTS) &&\r\nspu->spu_xts_tweak_in_payload())\r\nrx_frag_num++;\r\nerr = spu_ablkcipher_rx_sg_create(mssg, rctx, rx_frag_num, chunksize,\r\nstat_pad_len);\r\nif (err)\r\nreturn err;\r\ntx_frag_num += rctx->src_nents;\r\nif (spu->spu_tx_status_len())\r\ntx_frag_num++;\r\nif ((ctx->cipher.mode == CIPHER_MODE_XTS) &&\r\nspu->spu_xts_tweak_in_payload())\r\ntx_frag_num++;\r\nerr = spu_ablkcipher_tx_sg_create(mssg, rctx, tx_frag_num, chunksize,\r\npad_len);\r\nif (err)\r\nreturn err;\r\nerr = mbox_send_message(iproc_priv.mbox[rctx->chan_idx], mssg);\r\nif (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) {\r\nwhile ((err == -ENOBUFS) && (retry_cnt < SPU_MB_RETRY_MAX)) {\r\nretry_cnt++;\r\nusleep_range(MBOX_SLEEP_MIN, MBOX_SLEEP_MAX);\r\nerr = mbox_send_message(iproc_priv.mbox[rctx->chan_idx],\r\nmssg);\r\natomic_inc(&iproc_priv.mb_no_spc);\r\n}\r\n}\r\nif (unlikely(err < 0)) {\r\natomic_inc(&iproc_priv.mb_send_fail);\r\nreturn err;\r\n}\r\nreturn -EINPROGRESS;\r\n}\r\nstatic void handle_ablkcipher_resp(struct iproc_reqctx_s *rctx)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\n#ifdef DEBUG\r\nstruct crypto_async_request *areq = rctx->parent;\r\nstruct ablkcipher_request *req = ablkcipher_request_cast(areq);\r\n#endif\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nu32 payload_len;\r\npayload_len = spu->spu_payload_length(rctx->msg_buf.spu_resp_hdr);\r\nif ((ctx->cipher.mode == CIPHER_MODE_XTS) &&\r\nspu->spu_xts_tweak_in_payload() &&\r\n(payload_len >= SPU_XTS_TWEAK_SIZE))\r\npayload_len -= SPU_XTS_TWEAK_SIZE;\r\natomic64_add(payload_len, &iproc_priv.bytes_in);\r\nflow_log("%s() offset: %u, bd_len: %u BD:\n",\r\n__func__, rctx->total_received, payload_len);\r\ndump_sg(req->dst, rctx->total_received, payload_len);\r\nif (ctx->cipher.alg == CIPHER_ALG_RC4)\r\npacket_dump(" supdt ", rctx->msg_buf.c.supdt_tweak,\r\nSPU_SUPDT_LEN);\r\nrctx->total_received += payload_len;\r\nif (rctx->total_received == rctx->total_todo) {\r\natomic_inc(&iproc_priv.op_counts[SPU_OP_CIPHER]);\r\natomic_inc(\r\n&iproc_priv.cipher_cnt[ctx->cipher.alg][ctx->cipher.mode]);\r\n}\r\n}\r\nstatic int\r\nspu_ahash_rx_sg_create(struct brcm_message *mssg,\r\nstruct iproc_reqctx_s *rctx,\r\nu8 rx_frag_num, unsigned int digestsize,\r\nu32 stat_pad_len)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct scatterlist *sg;\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nmssg->spu.dst = kcalloc(rx_frag_num, sizeof(struct scatterlist),\r\nrctx->gfp);\r\nif (!mssg->spu.dst)\r\nreturn -ENOMEM;\r\nsg = mssg->spu.dst;\r\nsg_init_table(sg, rx_frag_num);\r\nsg_set_buf(sg++, rctx->msg_buf.spu_resp_hdr, ctx->spu_resp_hdr_len);\r\nsg_set_buf(sg++, rctx->msg_buf.digest, digestsize);\r\nif (stat_pad_len)\r\nsg_set_buf(sg++, rctx->msg_buf.rx_stat_pad, stat_pad_len);\r\nmemset(rctx->msg_buf.rx_stat, 0, SPU_RX_STATUS_LEN);\r\nsg_set_buf(sg, rctx->msg_buf.rx_stat, spu->spu_rx_status_len());\r\nreturn 0;\r\n}\r\nstatic int\r\nspu_ahash_tx_sg_create(struct brcm_message *mssg,\r\nstruct iproc_reqctx_s *rctx,\r\nu8 tx_frag_num,\r\nu32 spu_hdr_len,\r\nunsigned int hash_carry_len,\r\nunsigned int new_data_len, u32 pad_len)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct scatterlist *sg;\r\nu32 datalen;\r\nu32 stat_len;\r\nmssg->spu.src = kcalloc(tx_frag_num, sizeof(struct scatterlist),\r\nrctx->gfp);\r\nif (!mssg->spu.src)\r\nreturn -ENOMEM;\r\nsg = mssg->spu.src;\r\nsg_init_table(sg, tx_frag_num);\r\nsg_set_buf(sg++, rctx->msg_buf.bcm_spu_req_hdr,\r\nBCM_HDR_LEN + spu_hdr_len);\r\nif (hash_carry_len)\r\nsg_set_buf(sg++, rctx->hash_carry, hash_carry_len);\r\nif (new_data_len) {\r\ndatalen = spu_msg_sg_add(&sg, &rctx->src_sg, &rctx->src_skip,\r\nrctx->src_nents, new_data_len);\r\nif (datalen < new_data_len) {\r\npr_err("%s(): failed to copy src sg to mbox msg",\r\n__func__);\r\nreturn -EFAULT;\r\n}\r\n}\r\nif (pad_len)\r\nsg_set_buf(sg++, rctx->msg_buf.spu_req_pad, pad_len);\r\nstat_len = spu->spu_tx_status_len();\r\nif (stat_len) {\r\nmemset(rctx->msg_buf.tx_stat, 0, stat_len);\r\nsg_set_buf(sg, rctx->msg_buf.tx_stat, stat_len);\r\n}\r\nreturn 0;\r\n}\r\nstatic int handle_ahash_req(struct iproc_reqctx_s *rctx)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct crypto_async_request *areq = rctx->parent;\r\nstruct ahash_request *req = ahash_request_cast(areq);\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct crypto_tfm *tfm = crypto_ahash_tfm(ahash);\r\nunsigned int blocksize = crypto_tfm_alg_blocksize(tfm);\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nunsigned int nbytes_to_hash = 0;\r\nint err = 0;\r\nunsigned int chunksize = 0;\r\nunsigned int new_data_len;\r\nunsigned int chunk_start = 0;\r\nu32 db_size;\r\nint pad_len = 0;\r\nu32 data_pad_len = 0;\r\nu32 stat_pad_len = 0;\r\nstruct brcm_message *mssg;\r\nstruct spu_request_opts req_opts;\r\nstruct spu_cipher_parms cipher_parms;\r\nstruct spu_hash_parms hash_parms;\r\nstruct spu_aead_parms aead_parms;\r\nunsigned int local_nbuf;\r\nu32 spu_hdr_len;\r\nunsigned int digestsize;\r\nu16 rem = 0;\r\nint retry_cnt = 0;\r\nu8 rx_frag_num = 3;\r\nu8 tx_frag_num = 1;\r\nflow_log("total_todo %u, total_sent %u\n",\r\nrctx->total_todo, rctx->total_sent);\r\nmemset(&req_opts, 0, sizeof(req_opts));\r\nmemset(&cipher_parms, 0, sizeof(cipher_parms));\r\nmemset(&hash_parms, 0, sizeof(hash_parms));\r\nmemset(&aead_parms, 0, sizeof(aead_parms));\r\nreq_opts.bd_suppress = true;\r\nhash_parms.alg = ctx->auth.alg;\r\nhash_parms.mode = ctx->auth.mode;\r\nhash_parms.type = HASH_TYPE_NONE;\r\nhash_parms.key_buf = (u8 *)ctx->authkey;\r\nhash_parms.key_len = ctx->authkeylen;\r\ncipher_parms.type = ctx->cipher_type;\r\nmssg = &rctx->mb_mssg;\r\nchunk_start = rctx->src_sent;\r\nnbytes_to_hash = rctx->total_todo - rctx->total_sent;\r\nchunksize = nbytes_to_hash;\r\nif ((ctx->max_payload != SPU_MAX_PAYLOAD_INF) &&\r\n(chunksize > ctx->max_payload))\r\nchunksize = ctx->max_payload;\r\nif (!rctx->is_final) {\r\nu8 *dest = rctx->hash_carry + rctx->hash_carry_len;\r\nu16 new_len;\r\nrem = chunksize % blocksize;\r\nif (rem) {\r\nchunksize -= rem;\r\nif (chunksize == 0) {\r\nnew_len = rem - rctx->hash_carry_len;\r\nsg_copy_part_to_buf(req->src, dest, new_len,\r\nrctx->src_sent);\r\nrctx->hash_carry_len = rem;\r\nflow_log("Exiting with hash carry len: %u\n",\r\nrctx->hash_carry_len);\r\npacket_dump(" buf: ",\r\nrctx->hash_carry,\r\nrctx->hash_carry_len);\r\nreturn -EAGAIN;\r\n}\r\n}\r\n}\r\nlocal_nbuf = rctx->hash_carry_len;\r\nrctx->hash_carry_len = 0;\r\nif (local_nbuf)\r\ntx_frag_num++;\r\nnew_data_len = chunksize - local_nbuf;\r\nrctx->src_nents = spu_sg_count(rctx->src_sg, rctx->src_skip,\r\nnew_data_len);\r\nif (hash_parms.alg == HASH_ALG_AES)\r\nhash_parms.type = cipher_parms.type;\r\nelse\r\nhash_parms.type = spu->spu_hash_type(rctx->total_sent);\r\ndigestsize = spu->spu_digest_size(ctx->digestsize, ctx->auth.alg,\r\nhash_parms.type);\r\nhash_parms.digestsize = digestsize;\r\nrctx->total_sent += chunksize;\r\nrctx->src_sent += new_data_len;\r\nif ((rctx->total_sent == rctx->total_todo) && rctx->is_final)\r\nhash_parms.pad_len = spu->spu_hash_pad_len(hash_parms.alg,\r\nhash_parms.mode,\r\nchunksize,\r\nblocksize);\r\nif ((hash_parms.type == HASH_TYPE_UPDT) &&\r\n(hash_parms.alg != HASH_ALG_AES)) {\r\nhash_parms.key_buf = rctx->incr_hash;\r\nhash_parms.key_len = digestsize;\r\n}\r\natomic64_add(chunksize, &iproc_priv.bytes_out);\r\nflow_log("%s() final: %u nbuf: %u ",\r\n__func__, rctx->is_final, local_nbuf);\r\nif (ctx->max_payload == SPU_MAX_PAYLOAD_INF)\r\nflow_log("max_payload infinite\n");\r\nelse\r\nflow_log("max_payload %u\n", ctx->max_payload);\r\nflow_log("chunk_start: %u chunk_size: %u\n", chunk_start, chunksize);\r\nmemcpy(rctx->msg_buf.bcm_spu_req_hdr, BCMHEADER, BCM_HDR_LEN);\r\nhash_parms.prebuf_len = local_nbuf;\r\nspu_hdr_len = spu->spu_create_request(rctx->msg_buf.bcm_spu_req_hdr +\r\nBCM_HDR_LEN,\r\n&req_opts, &cipher_parms,\r\n&hash_parms, &aead_parms,\r\nnew_data_len);\r\nif (spu_hdr_len == 0) {\r\npr_err("Failed to create SPU request header\n");\r\nreturn -EFAULT;\r\n}\r\ndata_pad_len = spu->spu_gcm_ccm_pad_len(ctx->cipher.mode, chunksize);\r\ndb_size = spu_real_db_size(0, 0, local_nbuf, new_data_len,\r\n0, 0, hash_parms.pad_len);\r\nif (spu->spu_tx_status_len())\r\nstat_pad_len = spu->spu_wordalign_padlen(db_size);\r\nif (stat_pad_len)\r\nrx_frag_num++;\r\npad_len = hash_parms.pad_len + data_pad_len + stat_pad_len;\r\nif (pad_len) {\r\ntx_frag_num++;\r\nspu->spu_request_pad(rctx->msg_buf.spu_req_pad, data_pad_len,\r\nhash_parms.pad_len, ctx->auth.alg,\r\nctx->auth.mode, rctx->total_sent,\r\nstat_pad_len);\r\n}\r\nspu->spu_dump_msg_hdr(rctx->msg_buf.bcm_spu_req_hdr + BCM_HDR_LEN,\r\nspu_hdr_len);\r\npacket_dump(" prebuf: ", rctx->hash_carry, local_nbuf);\r\nflow_log("Data:\n");\r\ndump_sg(rctx->src_sg, rctx->src_skip, new_data_len);\r\npacket_dump(" pad: ", rctx->msg_buf.spu_req_pad, pad_len);\r\nmemset(mssg, 0, sizeof(*mssg));\r\nmssg->type = BRCM_MESSAGE_SPU;\r\nmssg->ctx = rctx;\r\nerr = spu_ahash_rx_sg_create(mssg, rctx, rx_frag_num, digestsize,\r\nstat_pad_len);\r\nif (err)\r\nreturn err;\r\ntx_frag_num += rctx->src_nents;\r\nif (spu->spu_tx_status_len())\r\ntx_frag_num++;\r\nerr = spu_ahash_tx_sg_create(mssg, rctx, tx_frag_num, spu_hdr_len,\r\nlocal_nbuf, new_data_len, pad_len);\r\nif (err)\r\nreturn err;\r\nerr = mbox_send_message(iproc_priv.mbox[rctx->chan_idx], mssg);\r\nif (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) {\r\nwhile ((err == -ENOBUFS) && (retry_cnt < SPU_MB_RETRY_MAX)) {\r\nretry_cnt++;\r\nusleep_range(MBOX_SLEEP_MIN, MBOX_SLEEP_MAX);\r\nerr = mbox_send_message(iproc_priv.mbox[rctx->chan_idx],\r\nmssg);\r\natomic_inc(&iproc_priv.mb_no_spc);\r\n}\r\n}\r\nif (err < 0) {\r\natomic_inc(&iproc_priv.mb_send_fail);\r\nreturn err;\r\n}\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int spu_hmac_outer_hash(struct ahash_request *req,\r\nstruct iproc_ctx_s *ctx)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nunsigned int blocksize =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(ahash));\r\nint rc;\r\nswitch (ctx->auth.alg) {\r\ncase HASH_ALG_MD5:\r\nrc = do_shash("md5", req->result, ctx->opad, blocksize,\r\nreq->result, ctx->digestsize, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA1:\r\nrc = do_shash("sha1", req->result, ctx->opad, blocksize,\r\nreq->result, ctx->digestsize, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA224:\r\nrc = do_shash("sha224", req->result, ctx->opad, blocksize,\r\nreq->result, ctx->digestsize, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA256:\r\nrc = do_shash("sha256", req->result, ctx->opad, blocksize,\r\nreq->result, ctx->digestsize, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA384:\r\nrc = do_shash("sha384", req->result, ctx->opad, blocksize,\r\nreq->result, ctx->digestsize, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA512:\r\nrc = do_shash("sha512", req->result, ctx->opad, blocksize,\r\nreq->result, ctx->digestsize, NULL, 0);\r\nbreak;\r\ndefault:\r\npr_err("%s() Error : unknown hmac type\n", __func__);\r\nrc = -EINVAL;\r\n}\r\nreturn rc;\r\n}\r\nstatic int ahash_req_done(struct iproc_reqctx_s *rctx)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct crypto_async_request *areq = rctx->parent;\r\nstruct ahash_request *req = ahash_request_cast(areq);\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nint err;\r\nmemcpy(req->result, rctx->msg_buf.digest, ctx->digestsize);\r\nif (spu->spu_type == SPU_TYPE_SPUM) {\r\nif (ctx->auth.alg == HASH_ALG_MD5) {\r\n__swab32s((u32 *)req->result);\r\n__swab32s(((u32 *)req->result) + 1);\r\n__swab32s(((u32 *)req->result) + 2);\r\n__swab32s(((u32 *)req->result) + 3);\r\n__swab32s(((u32 *)req->result) + 4);\r\n}\r\n}\r\nflow_dump(" digest ", req->result, ctx->digestsize);\r\nif (rctx->is_sw_hmac) {\r\nerr = spu_hmac_outer_hash(req, ctx);\r\nif (err < 0)\r\nreturn err;\r\nflow_dump(" hmac: ", req->result, ctx->digestsize);\r\n}\r\nif (rctx->is_sw_hmac || ctx->auth.mode == HASH_MODE_HMAC) {\r\natomic_inc(&iproc_priv.op_counts[SPU_OP_HMAC]);\r\natomic_inc(&iproc_priv.hmac_cnt[ctx->auth.alg]);\r\n} else {\r\natomic_inc(&iproc_priv.op_counts[SPU_OP_HASH]);\r\natomic_inc(&iproc_priv.hash_cnt[ctx->auth.alg]);\r\n}\r\nreturn 0;\r\n}\r\nstatic void handle_ahash_resp(struct iproc_reqctx_s *rctx)\r\n{\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\n#ifdef DEBUG\r\nstruct crypto_async_request *areq = rctx->parent;\r\nstruct ahash_request *req = ahash_request_cast(areq);\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nunsigned int blocksize =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(ahash));\r\n#endif\r\nmemcpy(rctx->incr_hash, rctx->msg_buf.digest, MAX_DIGEST_SIZE);\r\nflow_log("%s() blocksize:%u digestsize:%u\n",\r\n__func__, blocksize, ctx->digestsize);\r\natomic64_add(ctx->digestsize, &iproc_priv.bytes_in);\r\nif (rctx->is_final && (rctx->total_sent == rctx->total_todo))\r\nahash_req_done(rctx);\r\n}\r\nstatic int spu_aead_rx_sg_create(struct brcm_message *mssg,\r\nstruct aead_request *req,\r\nstruct iproc_reqctx_s *rctx,\r\nu8 rx_frag_num,\r\nunsigned int assoc_len,\r\nu32 ret_iv_len, unsigned int resp_len,\r\nunsigned int digestsize, u32 stat_pad_len)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct scatterlist *sg;\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nu32 datalen;\r\nu32 assoc_buf_len;\r\nu8 data_padlen = 0;\r\nif (ctx->is_rfc4543) {\r\ndata_padlen = spu->spu_gcm_ccm_pad_len(ctx->cipher.mode,\r\nassoc_len + resp_len);\r\nassoc_buf_len = assoc_len;\r\n} else {\r\ndata_padlen = spu->spu_gcm_ccm_pad_len(ctx->cipher.mode,\r\nresp_len);\r\nassoc_buf_len = spu->spu_assoc_resp_len(ctx->cipher.mode,\r\nassoc_len, ret_iv_len,\r\nrctx->is_encrypt);\r\n}\r\nif (ctx->cipher.mode == CIPHER_MODE_CCM)\r\ndata_padlen += spu->spu_wordalign_padlen(assoc_buf_len +\r\nresp_len +\r\ndata_padlen);\r\nif (data_padlen)\r\nrx_frag_num++;\r\nmssg->spu.dst = kcalloc(rx_frag_num, sizeof(struct scatterlist),\r\nrctx->gfp);\r\nif (!mssg->spu.dst)\r\nreturn -ENOMEM;\r\nsg = mssg->spu.dst;\r\nsg_init_table(sg, rx_frag_num);\r\nsg_set_buf(sg++, rctx->msg_buf.spu_resp_hdr, ctx->spu_resp_hdr_len);\r\nif (assoc_buf_len) {\r\nmemset(rctx->msg_buf.a.resp_aad, 0, assoc_buf_len);\r\nsg_set_buf(sg++, rctx->msg_buf.a.resp_aad, assoc_buf_len);\r\n}\r\nif (resp_len) {\r\ndatalen = spu_msg_sg_add(&sg, &rctx->dst_sg, &rctx->dst_skip,\r\nrctx->dst_nents, resp_len);\r\nif (datalen < (resp_len)) {\r\npr_err("%s(): failed to copy dst sg to mbox msg. expected len %u, datalen %u",\r\n__func__, resp_len, datalen);\r\nreturn -EFAULT;\r\n}\r\n}\r\nif (data_padlen) {\r\nmemset(rctx->msg_buf.a.gcmpad, 0, data_padlen);\r\nsg_set_buf(sg++, rctx->msg_buf.a.gcmpad, data_padlen);\r\n}\r\nsg_set_buf(sg++, rctx->msg_buf.digest, digestsize);\r\nflow_log("stat_pad_len %u\n", stat_pad_len);\r\nif (stat_pad_len) {\r\nmemset(rctx->msg_buf.rx_stat_pad, 0, stat_pad_len);\r\nsg_set_buf(sg++, rctx->msg_buf.rx_stat_pad, stat_pad_len);\r\n}\r\nmemset(rctx->msg_buf.rx_stat, 0, SPU_RX_STATUS_LEN);\r\nsg_set_buf(sg, rctx->msg_buf.rx_stat, spu->spu_rx_status_len());\r\nreturn 0;\r\n}\r\nstatic int spu_aead_tx_sg_create(struct brcm_message *mssg,\r\nstruct iproc_reqctx_s *rctx,\r\nu8 tx_frag_num,\r\nu32 spu_hdr_len,\r\nstruct scatterlist *assoc,\r\nunsigned int assoc_len,\r\nint assoc_nents,\r\nunsigned int aead_iv_len,\r\nunsigned int chunksize,\r\nu32 aad_pad_len, u32 pad_len, bool incl_icv)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct scatterlist *sg;\r\nstruct scatterlist *assoc_sg = assoc;\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nu32 datalen;\r\nu32 written;\r\nu32 assoc_offset = 0;\r\nu32 stat_len;\r\nmssg->spu.src = kcalloc(tx_frag_num, sizeof(struct scatterlist),\r\nrctx->gfp);\r\nif (!mssg->spu.src)\r\nreturn -ENOMEM;\r\nsg = mssg->spu.src;\r\nsg_init_table(sg, tx_frag_num);\r\nsg_set_buf(sg++, rctx->msg_buf.bcm_spu_req_hdr,\r\nBCM_HDR_LEN + spu_hdr_len);\r\nif (assoc_len) {\r\nwritten = spu_msg_sg_add(&sg, &assoc_sg, &assoc_offset,\r\nassoc_nents, assoc_len);\r\nif (written < assoc_len) {\r\npr_err("%s(): failed to copy assoc sg to mbox msg",\r\n__func__);\r\nreturn -EFAULT;\r\n}\r\n}\r\nif (aead_iv_len)\r\nsg_set_buf(sg++, rctx->msg_buf.iv_ctr, aead_iv_len);\r\nif (aad_pad_len) {\r\nmemset(rctx->msg_buf.a.req_aad_pad, 0, aad_pad_len);\r\nsg_set_buf(sg++, rctx->msg_buf.a.req_aad_pad, aad_pad_len);\r\n}\r\ndatalen = chunksize;\r\nif ((chunksize > ctx->digestsize) && incl_icv)\r\ndatalen -= ctx->digestsize;\r\nif (datalen) {\r\nwritten = spu_msg_sg_add(&sg, &rctx->src_sg, &rctx->src_skip,\r\nrctx->src_nents, datalen);\r\nif (written < datalen) {\r\npr_err("%s(): failed to copy src sg to mbox msg",\r\n__func__);\r\nreturn -EFAULT;\r\n}\r\n}\r\nif (pad_len) {\r\nmemset(rctx->msg_buf.spu_req_pad, 0, pad_len);\r\nsg_set_buf(sg++, rctx->msg_buf.spu_req_pad, pad_len);\r\n}\r\nif (incl_icv)\r\nsg_set_buf(sg++, rctx->msg_buf.digest, ctx->digestsize);\r\nstat_len = spu->spu_tx_status_len();\r\nif (stat_len) {\r\nmemset(rctx->msg_buf.tx_stat, 0, stat_len);\r\nsg_set_buf(sg, rctx->msg_buf.tx_stat, stat_len);\r\n}\r\nreturn 0;\r\n}\r\nstatic int handle_aead_req(struct iproc_reqctx_s *rctx)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct crypto_async_request *areq = rctx->parent;\r\nstruct aead_request *req = container_of(areq,\r\nstruct aead_request, base);\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nint err;\r\nunsigned int chunksize;\r\nunsigned int resp_len;\r\nu32 spu_hdr_len;\r\nu32 db_size;\r\nu32 stat_pad_len;\r\nu32 pad_len;\r\nstruct brcm_message *mssg;\r\nstruct spu_request_opts req_opts;\r\nstruct spu_cipher_parms cipher_parms;\r\nstruct spu_hash_parms hash_parms;\r\nstruct spu_aead_parms aead_parms;\r\nint assoc_nents = 0;\r\nbool incl_icv = false;\r\nunsigned int digestsize = ctx->digestsize;\r\nint retry_cnt = 0;\r\nu8 rx_frag_num = 2;\r\nu8 tx_frag_num = 1;\r\nchunksize = rctx->total_todo;\r\nflow_log("%s: chunksize %u\n", __func__, chunksize);\r\nmemset(&req_opts, 0, sizeof(req_opts));\r\nmemset(&hash_parms, 0, sizeof(hash_parms));\r\nmemset(&aead_parms, 0, sizeof(aead_parms));\r\nreq_opts.is_inbound = !(rctx->is_encrypt);\r\nreq_opts.auth_first = ctx->auth_first;\r\nreq_opts.is_aead = true;\r\nreq_opts.is_esp = ctx->is_esp;\r\ncipher_parms.alg = ctx->cipher.alg;\r\ncipher_parms.mode = ctx->cipher.mode;\r\ncipher_parms.type = ctx->cipher_type;\r\ncipher_parms.key_buf = ctx->enckey;\r\ncipher_parms.key_len = ctx->enckeylen;\r\ncipher_parms.iv_buf = rctx->msg_buf.iv_ctr;\r\ncipher_parms.iv_len = rctx->iv_ctr_len;\r\nhash_parms.alg = ctx->auth.alg;\r\nhash_parms.mode = ctx->auth.mode;\r\nhash_parms.type = HASH_TYPE_NONE;\r\nhash_parms.key_buf = (u8 *)ctx->authkey;\r\nhash_parms.key_len = ctx->authkeylen;\r\nhash_parms.digestsize = digestsize;\r\nif ((ctx->auth.alg == HASH_ALG_SHA224) &&\r\n(ctx->authkeylen < SHA224_DIGEST_SIZE))\r\nhash_parms.key_len = SHA224_DIGEST_SIZE;\r\naead_parms.assoc_size = req->assoclen;\r\nif (ctx->is_esp && !ctx->is_rfc4543) {\r\naead_parms.assoc_size -= GCM_ESP_IV_SIZE;\r\nif (rctx->is_encrypt) {\r\naead_parms.return_iv = true;\r\naead_parms.ret_iv_len = GCM_ESP_IV_SIZE;\r\naead_parms.ret_iv_off = GCM_ESP_SALT_SIZE;\r\n}\r\n} else {\r\naead_parms.ret_iv_len = 0;\r\n}\r\nrctx->src_nents = spu_sg_count(rctx->src_sg, rctx->src_skip, chunksize);\r\nrctx->dst_nents = spu_sg_count(rctx->dst_sg, rctx->dst_skip, chunksize);\r\nif (aead_parms.assoc_size)\r\nassoc_nents = spu_sg_count(rctx->assoc, 0,\r\naead_parms.assoc_size);\r\nmssg = &rctx->mb_mssg;\r\nrctx->total_sent = chunksize;\r\nrctx->src_sent = chunksize;\r\nif (spu->spu_assoc_resp_len(ctx->cipher.mode,\r\naead_parms.assoc_size,\r\naead_parms.ret_iv_len,\r\nrctx->is_encrypt))\r\nrx_frag_num++;\r\naead_parms.iv_len = spu->spu_aead_ivlen(ctx->cipher.mode,\r\nrctx->iv_ctr_len);\r\nif (ctx->auth.alg == HASH_ALG_AES)\r\nhash_parms.type = ctx->cipher_type;\r\naead_parms.aad_pad_len = spu->spu_gcm_ccm_pad_len(ctx->cipher.mode,\r\naead_parms.assoc_size);\r\naead_parms.data_pad_len = spu->spu_gcm_ccm_pad_len(ctx->cipher.mode,\r\nchunksize);\r\nif (ctx->cipher.mode == CIPHER_MODE_CCM) {\r\naead_parms.aad_pad_len = spu->spu_gcm_ccm_pad_len(\r\nctx->cipher.mode,\r\naead_parms.assoc_size + 2);\r\nif (!rctx->is_encrypt)\r\naead_parms.data_pad_len =\r\nspu->spu_gcm_ccm_pad_len(ctx->cipher.mode,\r\nchunksize - digestsize);\r\nspu->spu_ccm_update_iv(digestsize, &cipher_parms, req->assoclen,\r\nchunksize, rctx->is_encrypt,\r\nctx->is_esp);\r\n}\r\nif (ctx->is_rfc4543) {\r\naead_parms.aad_pad_len = 0;\r\nif (!rctx->is_encrypt)\r\naead_parms.data_pad_len = spu->spu_gcm_ccm_pad_len(\r\nctx->cipher.mode,\r\naead_parms.assoc_size + chunksize -\r\ndigestsize);\r\nelse\r\naead_parms.data_pad_len = spu->spu_gcm_ccm_pad_len(\r\nctx->cipher.mode,\r\naead_parms.assoc_size + chunksize);\r\nreq_opts.is_rfc4543 = true;\r\n}\r\nif (spu_req_incl_icv(ctx->cipher.mode, rctx->is_encrypt)) {\r\nincl_icv = true;\r\ntx_frag_num++;\r\nsg_copy_part_to_buf(req->src, rctx->msg_buf.digest, digestsize,\r\nreq->assoclen + rctx->total_sent -\r\ndigestsize);\r\n}\r\natomic64_add(chunksize, &iproc_priv.bytes_out);\r\nflow_log("%s()-sent chunksize:%u\n", __func__, chunksize);\r\nmemcpy(rctx->msg_buf.bcm_spu_req_hdr, BCMHEADER, BCM_HDR_LEN);\r\nspu_hdr_len = spu->spu_create_request(rctx->msg_buf.bcm_spu_req_hdr +\r\nBCM_HDR_LEN, &req_opts,\r\n&cipher_parms, &hash_parms,\r\n&aead_parms, chunksize);\r\ndb_size = spu_real_db_size(aead_parms.assoc_size, aead_parms.iv_len, 0,\r\nchunksize, aead_parms.aad_pad_len,\r\naead_parms.data_pad_len, 0);\r\nstat_pad_len = spu->spu_wordalign_padlen(db_size);\r\nif (stat_pad_len)\r\nrx_frag_num++;\r\npad_len = aead_parms.data_pad_len + stat_pad_len;\r\nif (pad_len) {\r\ntx_frag_num++;\r\nspu->spu_request_pad(rctx->msg_buf.spu_req_pad,\r\naead_parms.data_pad_len, 0,\r\nctx->auth.alg, ctx->auth.mode,\r\nrctx->total_sent, stat_pad_len);\r\n}\r\nspu->spu_dump_msg_hdr(rctx->msg_buf.bcm_spu_req_hdr + BCM_HDR_LEN,\r\nspu_hdr_len);\r\ndump_sg(rctx->assoc, 0, aead_parms.assoc_size);\r\npacket_dump(" aead iv: ", rctx->msg_buf.iv_ctr, aead_parms.iv_len);\r\npacket_log("BD:\n");\r\ndump_sg(rctx->src_sg, rctx->src_skip, chunksize);\r\npacket_dump(" pad: ", rctx->msg_buf.spu_req_pad, pad_len);\r\nmemset(mssg, 0, sizeof(*mssg));\r\nmssg->type = BRCM_MESSAGE_SPU;\r\nmssg->ctx = rctx;\r\nrx_frag_num += rctx->dst_nents;\r\nresp_len = chunksize;\r\nrx_frag_num++;\r\nif (((ctx->cipher.mode == CIPHER_MODE_GCM) ||\r\n(ctx->cipher.mode == CIPHER_MODE_CCM)) && !rctx->is_encrypt) {\r\nresp_len -= ctx->digestsize;\r\nif (resp_len == 0)\r\nrx_frag_num -= rctx->dst_nents;\r\n}\r\nerr = spu_aead_rx_sg_create(mssg, req, rctx, rx_frag_num,\r\naead_parms.assoc_size,\r\naead_parms.ret_iv_len, resp_len, digestsize,\r\nstat_pad_len);\r\nif (err)\r\nreturn err;\r\ntx_frag_num += rctx->src_nents;\r\ntx_frag_num += assoc_nents;\r\nif (aead_parms.aad_pad_len)\r\ntx_frag_num++;\r\nif (aead_parms.iv_len)\r\ntx_frag_num++;\r\nif (spu->spu_tx_status_len())\r\ntx_frag_num++;\r\nerr = spu_aead_tx_sg_create(mssg, rctx, tx_frag_num, spu_hdr_len,\r\nrctx->assoc, aead_parms.assoc_size,\r\nassoc_nents, aead_parms.iv_len, chunksize,\r\naead_parms.aad_pad_len, pad_len, incl_icv);\r\nif (err)\r\nreturn err;\r\nerr = mbox_send_message(iproc_priv.mbox[rctx->chan_idx], mssg);\r\nif (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) {\r\nwhile ((err == -ENOBUFS) && (retry_cnt < SPU_MB_RETRY_MAX)) {\r\nretry_cnt++;\r\nusleep_range(MBOX_SLEEP_MIN, MBOX_SLEEP_MAX);\r\nerr = mbox_send_message(iproc_priv.mbox[rctx->chan_idx],\r\nmssg);\r\natomic_inc(&iproc_priv.mb_no_spc);\r\n}\r\n}\r\nif (err < 0) {\r\natomic_inc(&iproc_priv.mb_send_fail);\r\nreturn err;\r\n}\r\nreturn -EINPROGRESS;\r\n}\r\nstatic void handle_aead_resp(struct iproc_reqctx_s *rctx)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct crypto_async_request *areq = rctx->parent;\r\nstruct aead_request *req = container_of(areq,\r\nstruct aead_request, base);\r\nstruct iproc_ctx_s *ctx = rctx->ctx;\r\nu32 payload_len;\r\nunsigned int icv_offset;\r\nu32 result_len;\r\npayload_len = spu->spu_payload_length(rctx->msg_buf.spu_resp_hdr);\r\nflow_log("payload_len %u\n", payload_len);\r\natomic64_add(payload_len, &iproc_priv.bytes_in);\r\nif (req->assoclen)\r\npacket_dump(" assoc_data ", rctx->msg_buf.a.resp_aad,\r\nreq->assoclen);\r\nresult_len = req->cryptlen;\r\nif (rctx->is_encrypt) {\r\nicv_offset = req->assoclen + rctx->total_sent;\r\npacket_dump(" ICV: ", rctx->msg_buf.digest, ctx->digestsize);\r\nflow_log("copying ICV to dst sg at offset %u\n", icv_offset);\r\nsg_copy_part_from_buf(req->dst, rctx->msg_buf.digest,\r\nctx->digestsize, icv_offset);\r\nresult_len += ctx->digestsize;\r\n}\r\npacket_log("response data: ");\r\ndump_sg(req->dst, req->assoclen, result_len);\r\natomic_inc(&iproc_priv.op_counts[SPU_OP_AEAD]);\r\nif (ctx->cipher.alg == CIPHER_ALG_AES) {\r\nif (ctx->cipher.mode == CIPHER_MODE_CCM)\r\natomic_inc(&iproc_priv.aead_cnt[AES_CCM]);\r\nelse if (ctx->cipher.mode == CIPHER_MODE_GCM)\r\natomic_inc(&iproc_priv.aead_cnt[AES_GCM]);\r\nelse\r\natomic_inc(&iproc_priv.aead_cnt[AUTHENC]);\r\n} else {\r\natomic_inc(&iproc_priv.aead_cnt[AUTHENC]);\r\n}\r\n}\r\nstatic void spu_chunk_cleanup(struct iproc_reqctx_s *rctx)\r\n{\r\nstruct brcm_message *mssg = &rctx->mb_mssg;\r\nkfree(mssg->spu.src);\r\nkfree(mssg->spu.dst);\r\nmemset(mssg, 0, sizeof(struct brcm_message));\r\n}\r\nstatic void finish_req(struct iproc_reqctx_s *rctx, int err)\r\n{\r\nstruct crypto_async_request *areq = rctx->parent;\r\nflow_log("%s() err:%d\n\n", __func__, err);\r\nspu_chunk_cleanup(rctx);\r\nif (areq)\r\nareq->complete(areq, err);\r\n}\r\nstatic void spu_rx_callback(struct mbox_client *cl, void *msg)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct brcm_message *mssg = msg;\r\nstruct iproc_reqctx_s *rctx;\r\nstruct iproc_ctx_s *ctx;\r\nstruct crypto_async_request *areq;\r\nint err = 0;\r\nrctx = mssg->ctx;\r\nif (unlikely(!rctx)) {\r\npr_err("%s(): no request context", __func__);\r\nerr = -EFAULT;\r\ngoto cb_finish;\r\n}\r\nareq = rctx->parent;\r\nctx = rctx->ctx;\r\nerr = spu->spu_status_process(rctx->msg_buf.rx_stat);\r\nif (err != 0) {\r\nif (err == SPU_INVALID_ICV)\r\natomic_inc(&iproc_priv.bad_icv);\r\nerr = -EBADMSG;\r\ngoto cb_finish;\r\n}\r\nswitch (rctx->ctx->alg->type) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\nhandle_ablkcipher_resp(rctx);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\nhandle_ahash_resp(rctx);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\nhandle_aead_resp(rctx);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\ngoto cb_finish;\r\n}\r\nif (rctx->total_sent < rctx->total_todo) {\r\nspu_chunk_cleanup(rctx);\r\nswitch (rctx->ctx->alg->type) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\nerr = handle_ablkcipher_req(rctx);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\nerr = handle_ahash_req(rctx);\r\nif (err == -EAGAIN)\r\nerr = 0;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\nerr = handle_aead_req(rctx);\r\nbreak;\r\ndefault:\r\nerr = -EINVAL;\r\n}\r\nif (err == -EINPROGRESS)\r\nreturn;\r\n}\r\ncb_finish:\r\nfinish_req(rctx, err);\r\n}\r\nstatic int ablkcipher_enqueue(struct ablkcipher_request *req, bool encrypt)\r\n{\r\nstruct iproc_reqctx_s *rctx = ablkcipher_request_ctx(req);\r\nstruct iproc_ctx_s *ctx =\r\ncrypto_ablkcipher_ctx(crypto_ablkcipher_reqtfm(req));\r\nint err;\r\nflow_log("%s() enc:%u\n", __func__, encrypt);\r\nrctx->gfp = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nrctx->parent = &req->base;\r\nrctx->is_encrypt = encrypt;\r\nrctx->bd_suppress = false;\r\nrctx->total_todo = req->nbytes;\r\nrctx->src_sent = 0;\r\nrctx->total_sent = 0;\r\nrctx->total_received = 0;\r\nrctx->ctx = ctx;\r\nrctx->src_sg = req->src;\r\nrctx->src_nents = 0;\r\nrctx->src_skip = 0;\r\nrctx->dst_sg = req->dst;\r\nrctx->dst_nents = 0;\r\nrctx->dst_skip = 0;\r\nif (ctx->cipher.mode == CIPHER_MODE_CBC ||\r\nctx->cipher.mode == CIPHER_MODE_CTR ||\r\nctx->cipher.mode == CIPHER_MODE_OFB ||\r\nctx->cipher.mode == CIPHER_MODE_XTS ||\r\nctx->cipher.mode == CIPHER_MODE_GCM ||\r\nctx->cipher.mode == CIPHER_MODE_CCM) {\r\nrctx->iv_ctr_len =\r\ncrypto_ablkcipher_ivsize(crypto_ablkcipher_reqtfm(req));\r\nmemcpy(rctx->msg_buf.iv_ctr, req->info, rctx->iv_ctr_len);\r\n} else {\r\nrctx->iv_ctr_len = 0;\r\n}\r\nrctx->chan_idx = select_channel();\r\nerr = handle_ablkcipher_req(rctx);\r\nif (err != -EINPROGRESS)\r\nspu_chunk_cleanup(rctx);\r\nreturn err;\r\n}\r\nstatic int des_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_ablkcipher_ctx(cipher);\r\nu32 tmp[DES_EXPKEY_WORDS];\r\nif (keylen == DES_KEY_SIZE) {\r\nif (des_ekey(tmp, key) == 0) {\r\nif (crypto_ablkcipher_get_flags(cipher) &\r\nCRYPTO_TFM_REQ_WEAK_KEY) {\r\nu32 flags = CRYPTO_TFM_RES_WEAK_KEY;\r\ncrypto_ablkcipher_set_flags(cipher, flags);\r\nreturn -EINVAL;\r\n}\r\n}\r\nctx->cipher_type = CIPHER_TYPE_DES;\r\n} else {\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int threedes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_ablkcipher_ctx(cipher);\r\nif (keylen == (DES_KEY_SIZE * 3)) {\r\nconst u32 *K = (const u32 *)key;\r\nu32 flags = CRYPTO_TFM_RES_BAD_KEY_SCHED;\r\nif (!((K[0] ^ K[2]) | (K[1] ^ K[3])) ||\r\n!((K[2] ^ K[4]) | (K[3] ^ K[5]))) {\r\ncrypto_ablkcipher_set_flags(cipher, flags);\r\nreturn -EINVAL;\r\n}\r\nctx->cipher_type = CIPHER_TYPE_3DES;\r\n} else {\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int aes_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_ablkcipher_ctx(cipher);\r\nif (ctx->cipher.mode == CIPHER_MODE_XTS)\r\nkeylen = keylen / 2;\r\nswitch (keylen) {\r\ncase AES_KEYSIZE_128:\r\nctx->cipher_type = CIPHER_TYPE_AES128;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nctx->cipher_type = CIPHER_TYPE_AES192;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nctx->cipher_type = CIPHER_TYPE_AES256;\r\nbreak;\r\ndefault:\r\ncrypto_ablkcipher_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nWARN_ON((ctx->max_payload != SPU_MAX_PAYLOAD_INF) &&\r\n((ctx->max_payload % AES_BLOCK_SIZE) != 0));\r\nreturn 0;\r\n}\r\nstatic int rc4_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_ablkcipher_ctx(cipher);\r\nint i;\r\nctx->enckeylen = ARC4_MAX_KEY_SIZE + ARC4_STATE_SIZE;\r\nctx->enckey[0] = 0x00;\r\nctx->enckey[1] = 0x00;\r\nctx->enckey[2] = 0x00;\r\nctx->enckey[3] = 0x00;\r\nfor (i = 0; i < ARC4_MAX_KEY_SIZE; i++)\r\nctx->enckey[i + ARC4_STATE_SIZE] = key[i % keylen];\r\nctx->cipher_type = CIPHER_TYPE_INIT;\r\nreturn 0;\r\n}\r\nstatic int ablkcipher_setkey(struct crypto_ablkcipher *cipher, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct iproc_ctx_s *ctx = crypto_ablkcipher_ctx(cipher);\r\nstruct spu_cipher_parms cipher_parms;\r\nu32 alloc_len = 0;\r\nint err;\r\nflow_log("ablkcipher_setkey() keylen: %d\n", keylen);\r\nflow_dump(" key: ", key, keylen);\r\nswitch (ctx->cipher.alg) {\r\ncase CIPHER_ALG_DES:\r\nerr = des_setkey(cipher, key, keylen);\r\nbreak;\r\ncase CIPHER_ALG_3DES:\r\nerr = threedes_setkey(cipher, key, keylen);\r\nbreak;\r\ncase CIPHER_ALG_AES:\r\nerr = aes_setkey(cipher, key, keylen);\r\nbreak;\r\ncase CIPHER_ALG_RC4:\r\nerr = rc4_setkey(cipher, key, keylen);\r\nbreak;\r\ndefault:\r\npr_err("%s() Error: unknown cipher alg\n", __func__);\r\nerr = -EINVAL;\r\n}\r\nif (err)\r\nreturn err;\r\nif (ctx->cipher.alg != CIPHER_ALG_RC4) {\r\nmemcpy(ctx->enckey, key, keylen);\r\nctx->enckeylen = keylen;\r\n}\r\nif ((ctx->cipher.alg == CIPHER_ALG_AES) &&\r\n(ctx->cipher.mode == CIPHER_MODE_XTS)) {\r\nunsigned int xts_keylen = keylen / 2;\r\nmemcpy(ctx->enckey, key + xts_keylen, xts_keylen);\r\nmemcpy(ctx->enckey + xts_keylen, key, xts_keylen);\r\n}\r\nif (spu->spu_type == SPU_TYPE_SPUM)\r\nalloc_len = BCM_HDR_LEN + SPU_HEADER_ALLOC_LEN;\r\nelse if (spu->spu_type == SPU_TYPE_SPU2)\r\nalloc_len = BCM_HDR_LEN + SPU2_HEADER_ALLOC_LEN;\r\nmemset(ctx->bcm_spu_req_hdr, 0, alloc_len);\r\ncipher_parms.iv_buf = NULL;\r\ncipher_parms.iv_len = crypto_ablkcipher_ivsize(cipher);\r\nflow_log("%s: iv_len %u\n", __func__, cipher_parms.iv_len);\r\ncipher_parms.alg = ctx->cipher.alg;\r\ncipher_parms.mode = ctx->cipher.mode;\r\ncipher_parms.type = ctx->cipher_type;\r\ncipher_parms.key_buf = ctx->enckey;\r\ncipher_parms.key_len = ctx->enckeylen;\r\nmemcpy(ctx->bcm_spu_req_hdr, BCMHEADER, BCM_HDR_LEN);\r\nctx->spu_req_hdr_len =\r\nspu->spu_cipher_req_init(ctx->bcm_spu_req_hdr + BCM_HDR_LEN,\r\n&cipher_parms);\r\nctx->spu_resp_hdr_len = spu->spu_response_hdr_len(ctx->authkeylen,\r\nctx->enckeylen,\r\nfalse);\r\natomic_inc(&iproc_priv.setkey_cnt[SPU_OP_CIPHER]);\r\nreturn 0;\r\n}\r\nstatic int ablkcipher_encrypt(struct ablkcipher_request *req)\r\n{\r\nflow_log("ablkcipher_encrypt() nbytes:%u\n", req->nbytes);\r\nreturn ablkcipher_enqueue(req, true);\r\n}\r\nstatic int ablkcipher_decrypt(struct ablkcipher_request *req)\r\n{\r\nflow_log("ablkcipher_decrypt() nbytes:%u\n", req->nbytes);\r\nreturn ablkcipher_enqueue(req, false);\r\n}\r\nstatic int ahash_enqueue(struct ahash_request *req)\r\n{\r\nstruct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(tfm);\r\nint err = 0;\r\nconst char *alg_name;\r\nflow_log("ahash_enqueue() nbytes:%u\n", req->nbytes);\r\nrctx->gfp = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nrctx->parent = &req->base;\r\nrctx->ctx = ctx;\r\nrctx->bd_suppress = true;\r\nmemset(&rctx->mb_mssg, 0, sizeof(struct brcm_message));\r\nrctx->src_sg = req->src;\r\nrctx->src_skip = 0;\r\nrctx->src_nents = 0;\r\nrctx->dst_sg = NULL;\r\nrctx->dst_skip = 0;\r\nrctx->dst_nents = 0;\r\nif ((rctx->is_final == 1) && (rctx->total_todo == 0) &&\r\n(iproc_priv.spu.spu_type == SPU_TYPE_SPU2)) {\r\nalg_name = crypto_tfm_alg_name(crypto_ahash_tfm(tfm));\r\nflow_log("Doing %sfinal %s zero-len hash request in software\n",\r\nrctx->is_final ? "" : "non-", alg_name);\r\nerr = do_shash((unsigned char *)alg_name, req->result,\r\nNULL, 0, NULL, 0, ctx->authkey,\r\nctx->authkeylen);\r\nif (err < 0)\r\nflow_log("Hash request failed with error %d\n", err);\r\nreturn err;\r\n}\r\nrctx->chan_idx = select_channel();\r\nerr = handle_ahash_req(rctx);\r\nif (err != -EINPROGRESS)\r\nspu_chunk_cleanup(rctx);\r\nif (err == -EAGAIN)\r\nerr = 0;\r\nreturn err;\r\n}\r\nstatic int __ahash_init(struct ahash_request *req)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(tfm);\r\nflow_log("%s()\n", __func__);\r\nrctx->hash_carry_len = 0;\r\nrctx->is_final = 0;\r\nrctx->total_todo = 0;\r\nrctx->src_sent = 0;\r\nrctx->total_sent = 0;\r\nrctx->total_received = 0;\r\nctx->digestsize = crypto_ahash_digestsize(tfm);\r\nWARN_ON(ctx->digestsize > MAX_DIGEST_SIZE);\r\nrctx->is_sw_hmac = false;\r\nctx->spu_resp_hdr_len = spu->spu_response_hdr_len(ctx->authkeylen, 0,\r\ntrue);\r\nreturn 0;\r\n}\r\nbool spu_no_incr_hash(struct iproc_ctx_s *ctx)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nif (spu->spu_type == SPU_TYPE_SPU2)\r\nreturn true;\r\nif ((ctx->auth.alg == HASH_ALG_AES) &&\r\n(ctx->auth.mode == HASH_MODE_XCBC))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic int ahash_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(tfm);\r\nconst char *alg_name;\r\nstruct crypto_shash *hash;\r\nint ret;\r\ngfp_t gfp;\r\nif (spu_no_incr_hash(ctx)) {\r\nalg_name = crypto_tfm_alg_name(crypto_ahash_tfm(tfm));\r\nhash = crypto_alloc_shash(alg_name, 0, 0);\r\nif (IS_ERR(hash)) {\r\nret = PTR_ERR(hash);\r\ngoto err;\r\n}\r\ngfp = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nctx->shash = kmalloc(sizeof(*ctx->shash) +\r\ncrypto_shash_descsize(hash), gfp);\r\nif (!ctx->shash) {\r\nret = -ENOMEM;\r\ngoto err_hash;\r\n}\r\nctx->shash->tfm = hash;\r\nctx->shash->flags = 0;\r\nif (ctx->authkeylen > 0) {\r\nret = crypto_shash_setkey(hash, ctx->authkey,\r\nctx->authkeylen);\r\nif (ret)\r\ngoto err_shash;\r\n}\r\nret = crypto_shash_init(ctx->shash);\r\nif (ret)\r\ngoto err_shash;\r\n} else {\r\nret = __ahash_init(req);\r\n}\r\nreturn ret;\r\nerr_shash:\r\nkfree(ctx->shash);\r\nerr_hash:\r\ncrypto_free_shash(hash);\r\nerr:\r\nreturn ret;\r\n}\r\nstatic int __ahash_update(struct ahash_request *req)\r\n{\r\nstruct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nflow_log("ahash_update() nbytes:%u\n", req->nbytes);\r\nif (!req->nbytes)\r\nreturn 0;\r\nrctx->total_todo += req->nbytes;\r\nrctx->src_sent = 0;\r\nreturn ahash_enqueue(req);\r\n}\r\nstatic int ahash_update(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(tfm);\r\nu8 *tmpbuf;\r\nint ret;\r\nint nents;\r\ngfp_t gfp;\r\nif (spu_no_incr_hash(ctx)) {\r\nif (req->src)\r\nnents = sg_nents(req->src);\r\nelse\r\nreturn -EINVAL;\r\ngfp = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\ntmpbuf = kmalloc(req->nbytes, gfp);\r\nif (!tmpbuf)\r\nreturn -ENOMEM;\r\nif (sg_copy_to_buffer(req->src, nents, tmpbuf, req->nbytes) !=\r\nreq->nbytes) {\r\nkfree(tmpbuf);\r\nreturn -EINVAL;\r\n}\r\nret = crypto_shash_update(ctx->shash, tmpbuf, req->nbytes);\r\nkfree(tmpbuf);\r\n} else {\r\nret = __ahash_update(req);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __ahash_final(struct ahash_request *req)\r\n{\r\nstruct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nflow_log("ahash_final() nbytes:%u\n", req->nbytes);\r\nrctx->is_final = 1;\r\nreturn ahash_enqueue(req);\r\n}\r\nstatic int ahash_final(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(tfm);\r\nint ret;\r\nif (spu_no_incr_hash(ctx)) {\r\nret = crypto_shash_final(ctx->shash, req->result);\r\ncrypto_free_shash(ctx->shash->tfm);\r\nkfree(ctx->shash);\r\n} else {\r\nret = __ahash_final(req);\r\n}\r\nreturn ret;\r\n}\r\nstatic int __ahash_finup(struct ahash_request *req)\r\n{\r\nstruct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nflow_log("ahash_finup() nbytes:%u\n", req->nbytes);\r\nrctx->total_todo += req->nbytes;\r\nrctx->src_sent = 0;\r\nrctx->is_final = 1;\r\nreturn ahash_enqueue(req);\r\n}\r\nstatic int ahash_finup(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(tfm);\r\nu8 *tmpbuf;\r\nint ret;\r\nint nents;\r\ngfp_t gfp;\r\nif (spu_no_incr_hash(ctx)) {\r\nif (req->src) {\r\nnents = sg_nents(req->src);\r\n} else {\r\nret = -EINVAL;\r\ngoto ahash_finup_exit;\r\n}\r\ngfp = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\ntmpbuf = kmalloc(req->nbytes, gfp);\r\nif (!tmpbuf) {\r\nret = -ENOMEM;\r\ngoto ahash_finup_exit;\r\n}\r\nif (sg_copy_to_buffer(req->src, nents, tmpbuf, req->nbytes) !=\r\nreq->nbytes) {\r\nret = -EINVAL;\r\ngoto ahash_finup_free;\r\n}\r\nret = crypto_shash_finup(ctx->shash, tmpbuf, req->nbytes,\r\nreq->result);\r\n} else {\r\nreturn __ahash_finup(req);\r\n}\r\nahash_finup_free:\r\nkfree(tmpbuf);\r\nahash_finup_exit:\r\ncrypto_free_shash(ctx->shash->tfm);\r\nkfree(ctx->shash);\r\nreturn ret;\r\n}\r\nstatic int ahash_digest(struct ahash_request *req)\r\n{\r\nint err = 0;\r\nflow_log("ahash_digest() nbytes:%u\n", req->nbytes);\r\nerr = __ahash_init(req);\r\nif (!err)\r\nerr = __ahash_finup(req);\r\nreturn err;\r\n}\r\nstatic int ahash_setkey(struct crypto_ahash *ahash, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(ahash);\r\nflow_log("%s() ahash:%p key:%p keylen:%u\n",\r\n__func__, ahash, key, keylen);\r\nflow_dump(" key: ", key, keylen);\r\nif (ctx->auth.alg == HASH_ALG_AES) {\r\nswitch (keylen) {\r\ncase AES_KEYSIZE_128:\r\nctx->cipher_type = CIPHER_TYPE_AES128;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nctx->cipher_type = CIPHER_TYPE_AES192;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nctx->cipher_type = CIPHER_TYPE_AES256;\r\nbreak;\r\ndefault:\r\npr_err("%s() Error: Invalid key length\n", __func__);\r\nreturn -EINVAL;\r\n}\r\n} else {\r\npr_err("%s() Error: unknown hash alg\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nmemcpy(ctx->authkey, key, keylen);\r\nctx->authkeylen = keylen;\r\nreturn 0;\r\n}\r\nstatic int ahash_export(struct ahash_request *req, void *out)\r\n{\r\nconst struct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nstruct spu_hash_export_s *spu_exp = (struct spu_hash_export_s *)out;\r\nspu_exp->total_todo = rctx->total_todo;\r\nspu_exp->total_sent = rctx->total_sent;\r\nspu_exp->is_sw_hmac = rctx->is_sw_hmac;\r\nmemcpy(spu_exp->hash_carry, rctx->hash_carry, sizeof(rctx->hash_carry));\r\nspu_exp->hash_carry_len = rctx->hash_carry_len;\r\nmemcpy(spu_exp->incr_hash, rctx->incr_hash, sizeof(rctx->incr_hash));\r\nreturn 0;\r\n}\r\nstatic int ahash_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nstruct spu_hash_export_s *spu_exp = (struct spu_hash_export_s *)in;\r\nrctx->total_todo = spu_exp->total_todo;\r\nrctx->total_sent = spu_exp->total_sent;\r\nrctx->is_sw_hmac = spu_exp->is_sw_hmac;\r\nmemcpy(rctx->hash_carry, spu_exp->hash_carry, sizeof(rctx->hash_carry));\r\nrctx->hash_carry_len = spu_exp->hash_carry_len;\r\nmemcpy(rctx->incr_hash, spu_exp->incr_hash, sizeof(rctx->incr_hash));\r\nreturn 0;\r\n}\r\nstatic int ahash_hmac_setkey(struct crypto_ahash *ahash, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(ahash);\r\nunsigned int blocksize =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(ahash));\r\nunsigned int digestsize = crypto_ahash_digestsize(ahash);\r\nunsigned int index;\r\nint rc;\r\nflow_log("%s() ahash:%p key:%p keylen:%u blksz:%u digestsz:%u\n",\r\n__func__, ahash, key, keylen, blocksize, digestsize);\r\nflow_dump(" key: ", key, keylen);\r\nif (keylen > blocksize) {\r\nswitch (ctx->auth.alg) {\r\ncase HASH_ALG_MD5:\r\nrc = do_shash("md5", ctx->authkey, key, keylen, NULL,\r\n0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA1:\r\nrc = do_shash("sha1", ctx->authkey, key, keylen, NULL,\r\n0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA224:\r\nrc = do_shash("sha224", ctx->authkey, key, keylen, NULL,\r\n0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA256:\r\nrc = do_shash("sha256", ctx->authkey, key, keylen, NULL,\r\n0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA384:\r\nrc = do_shash("sha384", ctx->authkey, key, keylen, NULL,\r\n0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA512:\r\nrc = do_shash("sha512", ctx->authkey, key, keylen, NULL,\r\n0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA3_224:\r\nrc = do_shash("sha3-224", ctx->authkey, key, keylen,\r\nNULL, 0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA3_256:\r\nrc = do_shash("sha3-256", ctx->authkey, key, keylen,\r\nNULL, 0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA3_384:\r\nrc = do_shash("sha3-384", ctx->authkey, key, keylen,\r\nNULL, 0, NULL, 0);\r\nbreak;\r\ncase HASH_ALG_SHA3_512:\r\nrc = do_shash("sha3-512", ctx->authkey, key, keylen,\r\nNULL, 0, NULL, 0);\r\nbreak;\r\ndefault:\r\npr_err("%s() Error: unknown hash alg\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nif (rc < 0) {\r\npr_err("%s() Error %d computing shash for %s\n",\r\n__func__, rc, hash_alg_name[ctx->auth.alg]);\r\nreturn rc;\r\n}\r\nctx->authkeylen = digestsize;\r\nflow_log(" keylen > digestsize... hashed\n");\r\nflow_dump(" newkey: ", ctx->authkey, ctx->authkeylen);\r\n} else {\r\nmemcpy(ctx->authkey, key, keylen);\r\nctx->authkeylen = keylen;\r\n}\r\nif (iproc_priv.spu.spu_type == SPU_TYPE_SPUM) {\r\nmemcpy(ctx->ipad, ctx->authkey, ctx->authkeylen);\r\nmemset(ctx->ipad + ctx->authkeylen, 0,\r\nblocksize - ctx->authkeylen);\r\nctx->authkeylen = 0;\r\nmemcpy(ctx->opad, ctx->ipad, blocksize);\r\nfor (index = 0; index < blocksize; index++) {\r\nctx->ipad[index] ^= 0x36;\r\nctx->opad[index] ^= 0x5c;\r\n}\r\nflow_dump(" ipad: ", ctx->ipad, blocksize);\r\nflow_dump(" opad: ", ctx->opad, blocksize);\r\n}\r\nctx->digestsize = digestsize;\r\natomic_inc(&iproc_priv.setkey_cnt[SPU_OP_HMAC]);\r\nreturn 0;\r\n}\r\nstatic int ahash_hmac_init(struct ahash_request *req)\r\n{\r\nstruct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(tfm);\r\nunsigned int blocksize =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nflow_log("ahash_hmac_init()\n");\r\nahash_init(req);\r\nif (!spu_no_incr_hash(ctx)) {\r\nrctx->is_sw_hmac = true;\r\nctx->auth.mode = HASH_MODE_HASH;\r\nmemcpy(rctx->hash_carry, ctx->ipad, blocksize);\r\nrctx->hash_carry_len = blocksize;\r\nrctx->total_todo += blocksize;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ahash_hmac_update(struct ahash_request *req)\r\n{\r\nflow_log("ahash_hmac_update() nbytes:%u\n", req->nbytes);\r\nif (!req->nbytes)\r\nreturn 0;\r\nreturn ahash_update(req);\r\n}\r\nstatic int ahash_hmac_final(struct ahash_request *req)\r\n{\r\nflow_log("ahash_hmac_final() nbytes:%u\n", req->nbytes);\r\nreturn ahash_final(req);\r\n}\r\nstatic int ahash_hmac_finup(struct ahash_request *req)\r\n{\r\nflow_log("ahash_hmac_finupl() nbytes:%u\n", req->nbytes);\r\nreturn ahash_finup(req);\r\n}\r\nstatic int ahash_hmac_digest(struct ahash_request *req)\r\n{\r\nstruct iproc_reqctx_s *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_ahash_ctx(tfm);\r\nunsigned int blocksize =\r\ncrypto_tfm_alg_blocksize(crypto_ahash_tfm(tfm));\r\nflow_log("ahash_hmac_digest() nbytes:%u\n", req->nbytes);\r\n__ahash_init(req);\r\nif (iproc_priv.spu.spu_type == SPU_TYPE_SPU2) {\r\nrctx->is_sw_hmac = false;\r\nctx->auth.mode = HASH_MODE_HMAC;\r\n} else {\r\nrctx->is_sw_hmac = true;\r\nctx->auth.mode = HASH_MODE_HASH;\r\nmemcpy(rctx->hash_carry, ctx->ipad, blocksize);\r\nrctx->hash_carry_len = blocksize;\r\nrctx->total_todo += blocksize;\r\n}\r\nreturn __ahash_finup(req);\r\n}\r\nstatic int aead_need_fallback(struct aead_request *req)\r\n{\r\nstruct iproc_reqctx_s *rctx = aead_request_ctx(req);\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_aead_ctx(aead);\r\nu32 payload_len;\r\nif (((ctx->cipher.mode == CIPHER_MODE_GCM) ||\r\n(ctx->cipher.mode == CIPHER_MODE_CCM)) &&\r\n(req->assoclen == 0)) {\r\nif ((rctx->is_encrypt && (req->cryptlen == 0)) ||\r\n(!rctx->is_encrypt && (req->cryptlen == ctx->digestsize))) {\r\nflow_log("AES GCM/CCM needs fallback for 0 len req\n");\r\nreturn 1;\r\n}\r\n}\r\nif ((ctx->cipher.mode == CIPHER_MODE_CCM) &&\r\n(spu->spu_type == SPU_TYPE_SPUM) &&\r\n(ctx->digestsize != 8) && (ctx->digestsize != 12) &&\r\n(ctx->digestsize != 16)) {\r\nflow_log("%s() AES CCM needs fallbck for digest size %d\n",\r\n__func__, ctx->digestsize);\r\nreturn 1;\r\n}\r\nif ((ctx->cipher.mode == CIPHER_MODE_CCM) &&\r\n(spu->spu_subtype == SPU_SUBTYPE_SPUM_NSP) &&\r\n(req->assoclen == 0)) {\r\nflow_log("%s() AES_CCM needs fallback for 0 len AAD on NSP\n",\r\n__func__);\r\nreturn 1;\r\n}\r\npayload_len = req->cryptlen;\r\nif (spu->spu_type == SPU_TYPE_SPUM)\r\npayload_len += req->assoclen;\r\nflow_log("%s() payload len: %u\n", __func__, payload_len);\r\nif (ctx->max_payload == SPU_MAX_PAYLOAD_INF)\r\nreturn 0;\r\nelse\r\nreturn payload_len > ctx->max_payload;\r\n}\r\nstatic void aead_complete(struct crypto_async_request *areq, int err)\r\n{\r\nstruct aead_request *req =\r\ncontainer_of(areq, struct aead_request, base);\r\nstruct iproc_reqctx_s *rctx = aead_request_ctx(req);\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\nflow_log("%s() err:%d\n", __func__, err);\r\nareq->tfm = crypto_aead_tfm(aead);\r\nareq->complete = rctx->old_complete;\r\nareq->data = rctx->old_data;\r\nareq->complete(areq, err);\r\n}\r\nstatic int aead_do_fallback(struct aead_request *req, bool is_encrypt)\r\n{\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\nstruct crypto_tfm *tfm = crypto_aead_tfm(aead);\r\nstruct iproc_reqctx_s *rctx = aead_request_ctx(req);\r\nstruct iproc_ctx_s *ctx = crypto_tfm_ctx(tfm);\r\nint err;\r\nu32 req_flags;\r\nflow_log("%s() enc:%u\n", __func__, is_encrypt);\r\nif (ctx->fallback_cipher) {\r\nrctx->old_tfm = tfm;\r\naead_request_set_tfm(req, ctx->fallback_cipher);\r\nrctx->old_complete = req->base.complete;\r\nrctx->old_data = req->base.data;\r\nreq_flags = aead_request_flags(req);\r\naead_request_set_callback(req, req_flags, aead_complete, req);\r\nerr = is_encrypt ? crypto_aead_encrypt(req) :\r\ncrypto_aead_decrypt(req);\r\nif (err == 0) {\r\naead_request_set_callback(req, req_flags,\r\nrctx->old_complete, req);\r\nreq->base.data = rctx->old_data;\r\naead_request_set_tfm(req, aead);\r\nflow_log("%s() fallback completed successfully\n\n",\r\n__func__);\r\n}\r\n} else {\r\nerr = -EINVAL;\r\n}\r\nreturn err;\r\n}\r\nstatic int aead_enqueue(struct aead_request *req, bool is_encrypt)\r\n{\r\nstruct iproc_reqctx_s *rctx = aead_request_ctx(req);\r\nstruct crypto_aead *aead = crypto_aead_reqtfm(req);\r\nstruct iproc_ctx_s *ctx = crypto_aead_ctx(aead);\r\nint err;\r\nflow_log("%s() enc:%u\n", __func__, is_encrypt);\r\nif (req->assoclen > MAX_ASSOC_SIZE) {\r\npr_err\r\n("%s() Error: associated data too long. (%u > %u bytes)\n",\r\n__func__, req->assoclen, MAX_ASSOC_SIZE);\r\nreturn -EINVAL;\r\n}\r\nrctx->gfp = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nrctx->parent = &req->base;\r\nrctx->is_encrypt = is_encrypt;\r\nrctx->bd_suppress = false;\r\nrctx->total_todo = req->cryptlen;\r\nrctx->src_sent = 0;\r\nrctx->total_sent = 0;\r\nrctx->total_received = 0;\r\nrctx->is_sw_hmac = false;\r\nrctx->ctx = ctx;\r\nmemset(&rctx->mb_mssg, 0, sizeof(struct brcm_message));\r\nrctx->assoc = req->src;\r\nif (spu_sg_at_offset(req->src, req->assoclen, &rctx->src_sg,\r\n&rctx->src_skip) < 0) {\r\npr_err("%s() Error: Unable to find start of src data\n",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nrctx->src_nents = 0;\r\nrctx->dst_nents = 0;\r\nif (req->dst == req->src) {\r\nrctx->dst_sg = rctx->src_sg;\r\nrctx->dst_skip = rctx->src_skip;\r\n} else {\r\nif (spu_sg_at_offset(req->dst, req->assoclen, &rctx->dst_sg,\r\n&rctx->dst_skip) < 0) {\r\npr_err("%s() Error: Unable to find start of dst data\n",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (ctx->cipher.mode == CIPHER_MODE_CBC ||\r\nctx->cipher.mode == CIPHER_MODE_CTR ||\r\nctx->cipher.mode == CIPHER_MODE_OFB ||\r\nctx->cipher.mode == CIPHER_MODE_XTS ||\r\nctx->cipher.mode == CIPHER_MODE_GCM) {\r\nrctx->iv_ctr_len =\r\nctx->salt_len +\r\ncrypto_aead_ivsize(crypto_aead_reqtfm(req));\r\n} else if (ctx->cipher.mode == CIPHER_MODE_CCM) {\r\nrctx->iv_ctr_len = CCM_AES_IV_SIZE;\r\n} else {\r\nrctx->iv_ctr_len = 0;\r\n}\r\nrctx->hash_carry_len = 0;\r\nflow_log(" src sg: %p\n", req->src);\r\nflow_log(" rctx->src_sg: %p, src_skip %u\n",\r\nrctx->src_sg, rctx->src_skip);\r\nflow_log(" assoc: %p, assoclen %u\n", rctx->assoc, req->assoclen);\r\nflow_log(" dst sg: %p\n", req->dst);\r\nflow_log(" rctx->dst_sg: %p, dst_skip %u\n",\r\nrctx->dst_sg, rctx->dst_skip);\r\nflow_log(" iv_ctr_len:%u\n", rctx->iv_ctr_len);\r\nflow_dump(" iv: ", req->iv, rctx->iv_ctr_len);\r\nflow_log(" authkeylen:%u\n", ctx->authkeylen);\r\nflow_log(" is_esp: %s\n", ctx->is_esp ? "yes" : "no");\r\nif (ctx->max_payload == SPU_MAX_PAYLOAD_INF)\r\nflow_log(" max_payload infinite");\r\nelse\r\nflow_log(" max_payload: %u\n", ctx->max_payload);\r\nif (unlikely(aead_need_fallback(req)))\r\nreturn aead_do_fallback(req, is_encrypt);\r\nif (rctx->iv_ctr_len) {\r\nif (ctx->salt_len)\r\nmemcpy(rctx->msg_buf.iv_ctr + ctx->salt_offset,\r\nctx->salt, ctx->salt_len);\r\nmemcpy(rctx->msg_buf.iv_ctr + ctx->salt_offset + ctx->salt_len,\r\nreq->iv,\r\nrctx->iv_ctr_len - ctx->salt_len - ctx->salt_offset);\r\n}\r\nrctx->chan_idx = select_channel();\r\nerr = handle_aead_req(rctx);\r\nif (err != -EINPROGRESS)\r\nspu_chunk_cleanup(rctx);\r\nreturn err;\r\n}\r\nstatic int aead_authenc_setkey(struct crypto_aead *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct iproc_ctx_s *ctx = crypto_aead_ctx(cipher);\r\nstruct crypto_tfm *tfm = crypto_aead_tfm(cipher);\r\nstruct rtattr *rta = (void *)key;\r\nstruct crypto_authenc_key_param *param;\r\nconst u8 *origkey = key;\r\nconst unsigned int origkeylen = keylen;\r\nint ret = 0;\r\nflow_log("%s() aead:%p key:%p keylen:%u\n", __func__, cipher, key,\r\nkeylen);\r\nflow_dump(" key: ", key, keylen);\r\nif (!RTA_OK(rta, keylen))\r\ngoto badkey;\r\nif (rta->rta_type != CRYPTO_AUTHENC_KEYA_PARAM)\r\ngoto badkey;\r\nif (RTA_PAYLOAD(rta) < sizeof(*param))\r\ngoto badkey;\r\nparam = RTA_DATA(rta);\r\nctx->enckeylen = be32_to_cpu(param->enckeylen);\r\nkey += RTA_ALIGN(rta->rta_len);\r\nkeylen -= RTA_ALIGN(rta->rta_len);\r\nif (keylen < ctx->enckeylen)\r\ngoto badkey;\r\nif (ctx->enckeylen > MAX_KEY_SIZE)\r\ngoto badkey;\r\nctx->authkeylen = keylen - ctx->enckeylen;\r\nif (ctx->authkeylen > MAX_KEY_SIZE)\r\ngoto badkey;\r\nmemcpy(ctx->enckey, key + ctx->authkeylen, ctx->enckeylen);\r\nmemset(ctx->authkey, 0, sizeof(ctx->authkey));\r\nmemcpy(ctx->authkey, key, ctx->authkeylen);\r\nswitch (ctx->alg->cipher_info.alg) {\r\ncase CIPHER_ALG_DES:\r\nif (ctx->enckeylen == DES_KEY_SIZE) {\r\nu32 tmp[DES_EXPKEY_WORDS];\r\nu32 flags = CRYPTO_TFM_RES_WEAK_KEY;\r\nif (des_ekey(tmp, key) == 0) {\r\nif (crypto_aead_get_flags(cipher) &\r\nCRYPTO_TFM_REQ_WEAK_KEY) {\r\ncrypto_aead_set_flags(cipher, flags);\r\nreturn -EINVAL;\r\n}\r\n}\r\nctx->cipher_type = CIPHER_TYPE_DES;\r\n} else {\r\ngoto badkey;\r\n}\r\nbreak;\r\ncase CIPHER_ALG_3DES:\r\nif (ctx->enckeylen == (DES_KEY_SIZE * 3)) {\r\nconst u32 *K = (const u32 *)key;\r\nu32 flags = CRYPTO_TFM_RES_BAD_KEY_SCHED;\r\nif (!((K[0] ^ K[2]) | (K[1] ^ K[3])) ||\r\n!((K[2] ^ K[4]) | (K[3] ^ K[5]))) {\r\ncrypto_aead_set_flags(cipher, flags);\r\nreturn -EINVAL;\r\n}\r\nctx->cipher_type = CIPHER_TYPE_3DES;\r\n} else {\r\ncrypto_aead_set_flags(cipher,\r\nCRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase CIPHER_ALG_AES:\r\nswitch (ctx->enckeylen) {\r\ncase AES_KEYSIZE_128:\r\nctx->cipher_type = CIPHER_TYPE_AES128;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nctx->cipher_type = CIPHER_TYPE_AES192;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nctx->cipher_type = CIPHER_TYPE_AES256;\r\nbreak;\r\ndefault:\r\ngoto badkey;\r\n}\r\nbreak;\r\ncase CIPHER_ALG_RC4:\r\nctx->cipher_type = CIPHER_TYPE_INIT;\r\nbreak;\r\ndefault:\r\npr_err("%s() Error: Unknown cipher alg\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nflow_log(" enckeylen:%u authkeylen:%u\n", ctx->enckeylen,\r\nctx->authkeylen);\r\nflow_dump(" enc: ", ctx->enckey, ctx->enckeylen);\r\nflow_dump(" auth: ", ctx->authkey, ctx->authkeylen);\r\nif (ctx->fallback_cipher) {\r\nflow_log(" running fallback setkey()\n");\r\nctx->fallback_cipher->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nctx->fallback_cipher->base.crt_flags |=\r\ntfm->crt_flags & CRYPTO_TFM_REQ_MASK;\r\nret =\r\ncrypto_aead_setkey(ctx->fallback_cipher, origkey,\r\norigkeylen);\r\nif (ret) {\r\nflow_log(" fallback setkey() returned:%d\n", ret);\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |=\r\n(ctx->fallback_cipher->base.crt_flags &\r\nCRYPTO_TFM_RES_MASK);\r\n}\r\n}\r\nctx->spu_resp_hdr_len = spu->spu_response_hdr_len(ctx->authkeylen,\r\nctx->enckeylen,\r\nfalse);\r\natomic_inc(&iproc_priv.setkey_cnt[SPU_OP_AEAD]);\r\nreturn ret;\r\nbadkey:\r\nctx->enckeylen = 0;\r\nctx->authkeylen = 0;\r\nctx->digestsize = 0;\r\ncrypto_aead_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic int aead_gcm_ccm_setkey(struct crypto_aead *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct iproc_ctx_s *ctx = crypto_aead_ctx(cipher);\r\nstruct crypto_tfm *tfm = crypto_aead_tfm(cipher);\r\nint ret = 0;\r\nflow_log("%s() keylen:%u\n", __func__, keylen);\r\nflow_dump(" key: ", key, keylen);\r\nif (!ctx->is_esp)\r\nctx->digestsize = keylen;\r\nctx->enckeylen = keylen;\r\nctx->authkeylen = 0;\r\nmemcpy(ctx->enckey, key, ctx->enckeylen);\r\nswitch (ctx->enckeylen) {\r\ncase AES_KEYSIZE_128:\r\nctx->cipher_type = CIPHER_TYPE_AES128;\r\nbreak;\r\ncase AES_KEYSIZE_192:\r\nctx->cipher_type = CIPHER_TYPE_AES192;\r\nbreak;\r\ncase AES_KEYSIZE_256:\r\nctx->cipher_type = CIPHER_TYPE_AES256;\r\nbreak;\r\ndefault:\r\ngoto badkey;\r\n}\r\nflow_log(" enckeylen:%u authkeylen:%u\n", ctx->enckeylen,\r\nctx->authkeylen);\r\nflow_dump(" enc: ", ctx->enckey, ctx->enckeylen);\r\nflow_dump(" auth: ", ctx->authkey, ctx->authkeylen);\r\nif (ctx->fallback_cipher) {\r\nflow_log(" running fallback setkey()\n");\r\nctx->fallback_cipher->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;\r\nctx->fallback_cipher->base.crt_flags |=\r\ntfm->crt_flags & CRYPTO_TFM_REQ_MASK;\r\nret = crypto_aead_setkey(ctx->fallback_cipher, key,\r\nkeylen + ctx->salt_len);\r\nif (ret) {\r\nflow_log(" fallback setkey() returned:%d\n", ret);\r\ntfm->crt_flags &= ~CRYPTO_TFM_RES_MASK;\r\ntfm->crt_flags |=\r\n(ctx->fallback_cipher->base.crt_flags &\r\nCRYPTO_TFM_RES_MASK);\r\n}\r\n}\r\nctx->spu_resp_hdr_len = spu->spu_response_hdr_len(ctx->authkeylen,\r\nctx->enckeylen,\r\nfalse);\r\natomic_inc(&iproc_priv.setkey_cnt[SPU_OP_AEAD]);\r\nflow_log(" enckeylen:%u authkeylen:%u\n", ctx->enckeylen,\r\nctx->authkeylen);\r\nreturn ret;\r\nbadkey:\r\nctx->enckeylen = 0;\r\nctx->authkeylen = 0;\r\nctx->digestsize = 0;\r\ncrypto_aead_set_flags(cipher, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic int aead_gcm_esp_setkey(struct crypto_aead *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_aead_ctx(cipher);\r\nflow_log("%s\n", __func__);\r\nctx->salt_len = GCM_ESP_SALT_SIZE;\r\nctx->salt_offset = GCM_ESP_SALT_OFFSET;\r\nmemcpy(ctx->salt, key + keylen - GCM_ESP_SALT_SIZE, GCM_ESP_SALT_SIZE);\r\nkeylen -= GCM_ESP_SALT_SIZE;\r\nctx->digestsize = GCM_ESP_DIGESTSIZE;\r\nctx->is_esp = true;\r\nflow_dump("salt: ", ctx->salt, GCM_ESP_SALT_SIZE);\r\nreturn aead_gcm_ccm_setkey(cipher, key, keylen);\r\n}\r\nstatic int rfc4543_gcm_esp_setkey(struct crypto_aead *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_aead_ctx(cipher);\r\nflow_log("%s\n", __func__);\r\nctx->salt_len = GCM_ESP_SALT_SIZE;\r\nctx->salt_offset = GCM_ESP_SALT_OFFSET;\r\nmemcpy(ctx->salt, key + keylen - GCM_ESP_SALT_SIZE, GCM_ESP_SALT_SIZE);\r\nkeylen -= GCM_ESP_SALT_SIZE;\r\nctx->digestsize = GCM_ESP_DIGESTSIZE;\r\nctx->is_esp = true;\r\nctx->is_rfc4543 = true;\r\nflow_dump("salt: ", ctx->salt, GCM_ESP_SALT_SIZE);\r\nreturn aead_gcm_ccm_setkey(cipher, key, keylen);\r\n}\r\nstatic int aead_ccm_esp_setkey(struct crypto_aead *cipher,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_aead_ctx(cipher);\r\nflow_log("%s\n", __func__);\r\nctx->salt_len = CCM_ESP_SALT_SIZE;\r\nctx->salt_offset = CCM_ESP_SALT_OFFSET;\r\nmemcpy(ctx->salt, key + keylen - CCM_ESP_SALT_SIZE, CCM_ESP_SALT_SIZE);\r\nkeylen -= CCM_ESP_SALT_SIZE;\r\nctx->is_esp = true;\r\nflow_dump("salt: ", ctx->salt, CCM_ESP_SALT_SIZE);\r\nreturn aead_gcm_ccm_setkey(cipher, key, keylen);\r\n}\r\nstatic int aead_setauthsize(struct crypto_aead *cipher, unsigned int authsize)\r\n{\r\nstruct iproc_ctx_s *ctx = crypto_aead_ctx(cipher);\r\nint ret = 0;\r\nflow_log("%s() authkeylen:%u authsize:%u\n",\r\n__func__, ctx->authkeylen, authsize);\r\nctx->digestsize = authsize;\r\nif (ctx->fallback_cipher) {\r\nflow_log(" running fallback setauth()\n");\r\nret = crypto_aead_setauthsize(ctx->fallback_cipher, authsize);\r\nif (ret)\r\nflow_log(" fallback setauth() returned:%d\n", ret);\r\n}\r\nreturn ret;\r\n}\r\nstatic int aead_encrypt(struct aead_request *req)\r\n{\r\nflow_log("%s() cryptlen:%u %08x\n", __func__, req->cryptlen,\r\nreq->cryptlen);\r\ndump_sg(req->src, 0, req->cryptlen + req->assoclen);\r\nflow_log(" assoc_len:%u\n", req->assoclen);\r\nreturn aead_enqueue(req, true);\r\n}\r\nstatic int aead_decrypt(struct aead_request *req)\r\n{\r\nflow_log("%s() cryptlen:%u\n", __func__, req->cryptlen);\r\ndump_sg(req->src, 0, req->cryptlen + req->assoclen);\r\nflow_log(" assoc_len:%u\n", req->assoclen);\r\nreturn aead_enqueue(req, false);\r\n}\r\nstatic int generic_cra_init(struct crypto_tfm *tfm,\r\nstruct iproc_alg_s *cipher_alg)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct iproc_ctx_s *ctx = crypto_tfm_ctx(tfm);\r\nunsigned int blocksize = crypto_tfm_alg_blocksize(tfm);\r\nflow_log("%s()\n", __func__);\r\nctx->alg = cipher_alg;\r\nctx->cipher = cipher_alg->cipher_info;\r\nctx->auth = cipher_alg->auth_info;\r\nctx->auth_first = cipher_alg->auth_first;\r\nctx->max_payload = spu->spu_ctx_max_payload(ctx->cipher.alg,\r\nctx->cipher.mode,\r\nblocksize);\r\nctx->fallback_cipher = NULL;\r\nctx->enckeylen = 0;\r\nctx->authkeylen = 0;\r\natomic_inc(&iproc_priv.stream_count);\r\natomic_inc(&iproc_priv.session_count);\r\nreturn 0;\r\n}\r\nstatic int ablkcipher_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct iproc_alg_s *cipher_alg;\r\nflow_log("%s()\n", __func__);\r\ntfm->crt_ablkcipher.reqsize = sizeof(struct iproc_reqctx_s);\r\ncipher_alg = container_of(alg, struct iproc_alg_s, alg.crypto);\r\nreturn generic_cra_init(tfm, cipher_alg);\r\n}\r\nstatic int ahash_cra_init(struct crypto_tfm *tfm)\r\n{\r\nint err;\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct iproc_alg_s *cipher_alg;\r\ncipher_alg = container_of(__crypto_ahash_alg(alg), struct iproc_alg_s,\r\nalg.hash);\r\nerr = generic_cra_init(tfm, cipher_alg);\r\nflow_log("%s()\n", __func__);\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct iproc_reqctx_s));\r\nreturn err;\r\n}\r\nstatic int aead_cra_init(struct crypto_aead *aead)\r\n{\r\nstruct crypto_tfm *tfm = crypto_aead_tfm(aead);\r\nstruct iproc_ctx_s *ctx = crypto_tfm_ctx(tfm);\r\nstruct crypto_alg *alg = tfm->__crt_alg;\r\nstruct aead_alg *aalg = container_of(alg, struct aead_alg, base);\r\nstruct iproc_alg_s *cipher_alg = container_of(aalg, struct iproc_alg_s,\r\nalg.aead);\r\nint err = generic_cra_init(tfm, cipher_alg);\r\nflow_log("%s()\n", __func__);\r\ncrypto_aead_set_reqsize(aead, sizeof(struct iproc_reqctx_s));\r\nctx->is_esp = false;\r\nctx->salt_len = 0;\r\nctx->salt_offset = 0;\r\nget_random_bytes(ctx->iv, MAX_IV_SIZE);\r\nflow_dump(" iv: ", ctx->iv, MAX_IV_SIZE);\r\nif (!err) {\r\nif (alg->cra_flags & CRYPTO_ALG_NEED_FALLBACK) {\r\nflow_log("%s() creating fallback cipher\n", __func__);\r\nctx->fallback_cipher =\r\ncrypto_alloc_aead(alg->cra_name, 0,\r\nCRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ctx->fallback_cipher)) {\r\npr_err("%s() Error: failed to allocate fallback for %s\n",\r\n__func__, alg->cra_name);\r\nreturn PTR_ERR(ctx->fallback_cipher);\r\n}\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic void generic_cra_exit(struct crypto_tfm *tfm)\r\n{\r\natomic_dec(&iproc_priv.session_count);\r\n}\r\nstatic void aead_cra_exit(struct crypto_aead *aead)\r\n{\r\nstruct crypto_tfm *tfm = crypto_aead_tfm(aead);\r\nstruct iproc_ctx_s *ctx = crypto_tfm_ctx(tfm);\r\ngeneric_cra_exit(tfm);\r\nif (ctx->fallback_cipher) {\r\ncrypto_free_aead(ctx->fallback_cipher);\r\nctx->fallback_cipher = NULL;\r\n}\r\n}\r\nstatic void spu_functions_register(struct device *dev,\r\nenum spu_spu_type spu_type,\r\nenum spu_spu_subtype spu_subtype)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nif (spu_type == SPU_TYPE_SPUM) {\r\ndev_dbg(dev, "Registering SPUM functions");\r\nspu->spu_dump_msg_hdr = spum_dump_msg_hdr;\r\nspu->spu_payload_length = spum_payload_length;\r\nspu->spu_response_hdr_len = spum_response_hdr_len;\r\nspu->spu_hash_pad_len = spum_hash_pad_len;\r\nspu->spu_gcm_ccm_pad_len = spum_gcm_ccm_pad_len;\r\nspu->spu_assoc_resp_len = spum_assoc_resp_len;\r\nspu->spu_aead_ivlen = spum_aead_ivlen;\r\nspu->spu_hash_type = spum_hash_type;\r\nspu->spu_digest_size = spum_digest_size;\r\nspu->spu_create_request = spum_create_request;\r\nspu->spu_cipher_req_init = spum_cipher_req_init;\r\nspu->spu_cipher_req_finish = spum_cipher_req_finish;\r\nspu->spu_request_pad = spum_request_pad;\r\nspu->spu_tx_status_len = spum_tx_status_len;\r\nspu->spu_rx_status_len = spum_rx_status_len;\r\nspu->spu_status_process = spum_status_process;\r\nspu->spu_xts_tweak_in_payload = spum_xts_tweak_in_payload;\r\nspu->spu_ccm_update_iv = spum_ccm_update_iv;\r\nspu->spu_wordalign_padlen = spum_wordalign_padlen;\r\nif (spu_subtype == SPU_SUBTYPE_SPUM_NS2)\r\nspu->spu_ctx_max_payload = spum_ns2_ctx_max_payload;\r\nelse\r\nspu->spu_ctx_max_payload = spum_nsp_ctx_max_payload;\r\n} else {\r\ndev_dbg(dev, "Registering SPU2 functions");\r\nspu->spu_dump_msg_hdr = spu2_dump_msg_hdr;\r\nspu->spu_ctx_max_payload = spu2_ctx_max_payload;\r\nspu->spu_payload_length = spu2_payload_length;\r\nspu->spu_response_hdr_len = spu2_response_hdr_len;\r\nspu->spu_hash_pad_len = spu2_hash_pad_len;\r\nspu->spu_gcm_ccm_pad_len = spu2_gcm_ccm_pad_len;\r\nspu->spu_assoc_resp_len = spu2_assoc_resp_len;\r\nspu->spu_aead_ivlen = spu2_aead_ivlen;\r\nspu->spu_hash_type = spu2_hash_type;\r\nspu->spu_digest_size = spu2_digest_size;\r\nspu->spu_create_request = spu2_create_request;\r\nspu->spu_cipher_req_init = spu2_cipher_req_init;\r\nspu->spu_cipher_req_finish = spu2_cipher_req_finish;\r\nspu->spu_request_pad = spu2_request_pad;\r\nspu->spu_tx_status_len = spu2_tx_status_len;\r\nspu->spu_rx_status_len = spu2_rx_status_len;\r\nspu->spu_status_process = spu2_status_process;\r\nspu->spu_xts_tweak_in_payload = spu2_xts_tweak_in_payload;\r\nspu->spu_ccm_update_iv = spu2_ccm_update_iv;\r\nspu->spu_wordalign_padlen = spu2_wordalign_padlen;\r\n}\r\n}\r\nstatic int spu_mb_init(struct device *dev)\r\n{\r\nstruct mbox_client *mcl = &iproc_priv.mcl[iproc_priv.spu.num_spu];\r\nint err;\r\nmcl->dev = dev;\r\nmcl->tx_block = false;\r\nmcl->tx_tout = 0;\r\nmcl->knows_txdone = false;\r\nmcl->rx_callback = spu_rx_callback;\r\nmcl->tx_done = NULL;\r\niproc_priv.mbox[iproc_priv.spu.num_spu] =\r\nmbox_request_channel(mcl, 0);\r\nif (IS_ERR(iproc_priv.mbox[iproc_priv.spu.num_spu])) {\r\nerr = (int)PTR_ERR(iproc_priv.mbox[iproc_priv.spu.num_spu]);\r\ndev_err(dev,\r\n"Mbox channel %d request failed with err %d",\r\niproc_priv.spu.num_spu, err);\r\niproc_priv.mbox[iproc_priv.spu.num_spu] = NULL;\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void spu_mb_release(struct platform_device *pdev)\r\n{\r\nint i;\r\nfor (i = 0; i < iproc_priv.spu.num_spu; i++)\r\nmbox_free_channel(iproc_priv.mbox[i]);\r\n}\r\nstatic void spu_counters_init(void)\r\n{\r\nint i;\r\nint j;\r\natomic_set(&iproc_priv.session_count, 0);\r\natomic_set(&iproc_priv.stream_count, 0);\r\natomic_set(&iproc_priv.next_chan, (int)iproc_priv.spu.num_spu);\r\natomic64_set(&iproc_priv.bytes_in, 0);\r\natomic64_set(&iproc_priv.bytes_out, 0);\r\nfor (i = 0; i < SPU_OP_NUM; i++) {\r\natomic_set(&iproc_priv.op_counts[i], 0);\r\natomic_set(&iproc_priv.setkey_cnt[i], 0);\r\n}\r\nfor (i = 0; i < CIPHER_ALG_LAST; i++)\r\nfor (j = 0; j < CIPHER_MODE_LAST; j++)\r\natomic_set(&iproc_priv.cipher_cnt[i][j], 0);\r\nfor (i = 0; i < HASH_ALG_LAST; i++) {\r\natomic_set(&iproc_priv.hash_cnt[i], 0);\r\natomic_set(&iproc_priv.hmac_cnt[i], 0);\r\n}\r\nfor (i = 0; i < AEAD_TYPE_LAST; i++)\r\natomic_set(&iproc_priv.aead_cnt[i], 0);\r\natomic_set(&iproc_priv.mb_no_spc, 0);\r\natomic_set(&iproc_priv.mb_send_fail, 0);\r\natomic_set(&iproc_priv.bad_icv, 0);\r\n}\r\nstatic int spu_register_ablkcipher(struct iproc_alg_s *driver_alg)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct crypto_alg *crypto = &driver_alg->alg.crypto;\r\nint err;\r\nif ((driver_alg->cipher_info.alg == CIPHER_ALG_RC4) &&\r\n(spu->spu_type == SPU_TYPE_SPU2))\r\nreturn 0;\r\ncrypto->cra_module = THIS_MODULE;\r\ncrypto->cra_priority = cipher_pri;\r\ncrypto->cra_alignmask = 0;\r\ncrypto->cra_ctxsize = sizeof(struct iproc_ctx_s);\r\nINIT_LIST_HEAD(&crypto->cra_list);\r\ncrypto->cra_init = ablkcipher_cra_init;\r\ncrypto->cra_exit = generic_cra_exit;\r\ncrypto->cra_type = &crypto_ablkcipher_type;\r\ncrypto->cra_flags = CRYPTO_ALG_TYPE_ABLKCIPHER | CRYPTO_ALG_ASYNC |\r\nCRYPTO_ALG_KERN_DRIVER_ONLY;\r\ncrypto->cra_ablkcipher.setkey = ablkcipher_setkey;\r\ncrypto->cra_ablkcipher.encrypt = ablkcipher_encrypt;\r\ncrypto->cra_ablkcipher.decrypt = ablkcipher_decrypt;\r\nerr = crypto_register_alg(crypto);\r\nif (err == 0)\r\ndriver_alg->registered = true;\r\npr_debug(" registered ablkcipher %s\n", crypto->cra_driver_name);\r\nreturn err;\r\n}\r\nstatic int spu_register_ahash(struct iproc_alg_s *driver_alg)\r\n{\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct ahash_alg *hash = &driver_alg->alg.hash;\r\nint err;\r\nif ((driver_alg->auth_info.alg == HASH_ALG_AES) &&\r\n(driver_alg->auth_info.mode != HASH_MODE_XCBC) &&\r\n(spu->spu_type == SPU_TYPE_SPUM))\r\nreturn 0;\r\nif ((driver_alg->auth_info.alg >= HASH_ALG_SHA3_224) &&\r\n(spu->spu_subtype != SPU_SUBTYPE_SPU2_V2))\r\nreturn 0;\r\nhash->halg.base.cra_module = THIS_MODULE;\r\nhash->halg.base.cra_priority = hash_pri;\r\nhash->halg.base.cra_alignmask = 0;\r\nhash->halg.base.cra_ctxsize = sizeof(struct iproc_ctx_s);\r\nhash->halg.base.cra_init = ahash_cra_init;\r\nhash->halg.base.cra_exit = generic_cra_exit;\r\nhash->halg.base.cra_type = &crypto_ahash_type;\r\nhash->halg.base.cra_flags = CRYPTO_ALG_TYPE_AHASH | CRYPTO_ALG_ASYNC;\r\nhash->halg.statesize = sizeof(struct spu_hash_export_s);\r\nif (driver_alg->auth_info.mode != HASH_MODE_HMAC) {\r\nhash->setkey = ahash_setkey;\r\nhash->init = ahash_init;\r\nhash->update = ahash_update;\r\nhash->final = ahash_final;\r\nhash->finup = ahash_finup;\r\nhash->digest = ahash_digest;\r\n} else {\r\nhash->setkey = ahash_hmac_setkey;\r\nhash->init = ahash_hmac_init;\r\nhash->update = ahash_hmac_update;\r\nhash->final = ahash_hmac_final;\r\nhash->finup = ahash_hmac_finup;\r\nhash->digest = ahash_hmac_digest;\r\n}\r\nhash->export = ahash_export;\r\nhash->import = ahash_import;\r\nerr = crypto_register_ahash(hash);\r\nif (err == 0)\r\ndriver_alg->registered = true;\r\npr_debug(" registered ahash %s\n",\r\nhash->halg.base.cra_driver_name);\r\nreturn err;\r\n}\r\nstatic int spu_register_aead(struct iproc_alg_s *driver_alg)\r\n{\r\nstruct aead_alg *aead = &driver_alg->alg.aead;\r\nint err;\r\naead->base.cra_module = THIS_MODULE;\r\naead->base.cra_priority = aead_pri;\r\naead->base.cra_alignmask = 0;\r\naead->base.cra_ctxsize = sizeof(struct iproc_ctx_s);\r\nINIT_LIST_HEAD(&aead->base.cra_list);\r\naead->base.cra_flags |= CRYPTO_ALG_TYPE_AEAD | CRYPTO_ALG_ASYNC;\r\naead->setauthsize = aead_setauthsize;\r\naead->encrypt = aead_encrypt;\r\naead->decrypt = aead_decrypt;\r\naead->init = aead_cra_init;\r\naead->exit = aead_cra_exit;\r\nerr = crypto_register_aead(aead);\r\nif (err == 0)\r\ndriver_alg->registered = true;\r\npr_debug(" registered aead %s\n", aead->base.cra_driver_name);\r\nreturn err;\r\n}\r\nstatic int spu_algs_register(struct device *dev)\r\n{\r\nint i, j;\r\nint err;\r\nfor (i = 0; i < ARRAY_SIZE(driver_algs); i++) {\r\nswitch (driver_algs[i].type) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\nerr = spu_register_ablkcipher(&driver_algs[i]);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\nerr = spu_register_ahash(&driver_algs[i]);\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\nerr = spu_register_aead(&driver_algs[i]);\r\nbreak;\r\ndefault:\r\ndev_err(dev,\r\n"iproc-crypto: unknown alg type: %d",\r\ndriver_algs[i].type);\r\nerr = -EINVAL;\r\n}\r\nif (err) {\r\ndev_err(dev, "alg registration failed with error %d\n",\r\nerr);\r\ngoto err_algs;\r\n}\r\n}\r\nreturn 0;\r\nerr_algs:\r\nfor (j = 0; j < i; j++) {\r\nif (!driver_algs[j].registered)\r\ncontinue;\r\nswitch (driver_algs[j].type) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\ncrypto_unregister_alg(&driver_algs[j].alg.crypto);\r\ndriver_algs[j].registered = false;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\ncrypto_unregister_ahash(&driver_algs[j].alg.hash);\r\ndriver_algs[j].registered = false;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\ncrypto_unregister_aead(&driver_algs[j].alg.aead);\r\ndriver_algs[j].registered = false;\r\nbreak;\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic int spu_dt_read(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nstruct resource *spu_ctrl_regs;\r\nconst struct of_device_id *match;\r\nconst struct spu_type_subtype *matched_spu_type;\r\nvoid __iomem *spu_reg_vbase[MAX_SPUS];\r\nint err;\r\nmatch = of_match_device(of_match_ptr(bcm_spu_dt_ids), dev);\r\nmatched_spu_type = match->data;\r\nif (iproc_priv.spu.num_spu > 1) {\r\nif ((spu->spu_type != matched_spu_type->type) ||\r\n(spu->spu_subtype != matched_spu_type->subtype)) {\r\nerr = -EINVAL;\r\ndev_err(&pdev->dev, "Multiple SPU types not allowed");\r\nreturn err;\r\n}\r\n} else {\r\nspu->spu_type = matched_spu_type->type;\r\nspu->spu_subtype = matched_spu_type->subtype;\r\n}\r\nspu_ctrl_regs = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!spu_ctrl_regs) {\r\nerr = -EINVAL;\r\ndev_err(&pdev->dev, "Invalid/missing registers for SPU\n");\r\nreturn err;\r\n}\r\nspu_reg_vbase[iproc_priv.spu.num_spu] =\r\ndevm_ioremap_resource(dev, spu_ctrl_regs);\r\nif (IS_ERR(spu_reg_vbase[iproc_priv.spu.num_spu])) {\r\nerr = PTR_ERR(spu_reg_vbase[iproc_priv.spu.num_spu]);\r\ndev_err(&pdev->dev, "Failed to map registers: %d\n",\r\nerr);\r\nspu_reg_vbase[iproc_priv.spu.num_spu] = NULL;\r\nreturn err;\r\n}\r\ndev_dbg(dev, "SPU %d detected.", iproc_priv.spu.num_spu);\r\nspu->reg_vbase[iproc_priv.spu.num_spu] = spu_reg_vbase;\r\nreturn 0;\r\n}\r\nint bcm_spu_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct spu_hw *spu = &iproc_priv.spu;\r\nint err = 0;\r\niproc_priv.pdev[iproc_priv.spu.num_spu] = pdev;\r\nplatform_set_drvdata(iproc_priv.pdev[iproc_priv.spu.num_spu],\r\n&iproc_priv);\r\nerr = spu_dt_read(pdev);\r\nif (err < 0)\r\ngoto failure;\r\nerr = spu_mb_init(&pdev->dev);\r\nif (err < 0)\r\ngoto failure;\r\niproc_priv.spu.num_spu++;\r\nif (iproc_priv.inited)\r\nreturn 0;\r\nif (spu->spu_type == SPU_TYPE_SPUM)\r\niproc_priv.bcm_hdr_len = 8;\r\nelse if (spu->spu_type == SPU_TYPE_SPU2)\r\niproc_priv.bcm_hdr_len = 0;\r\nspu_functions_register(&pdev->dev, spu->spu_type, spu->spu_subtype);\r\nspu_counters_init();\r\nspu_setup_debugfs();\r\nerr = spu_algs_register(dev);\r\nif (err < 0)\r\ngoto fail_reg;\r\niproc_priv.inited = true;\r\nreturn 0;\r\nfail_reg:\r\nspu_free_debugfs();\r\nfailure:\r\nspu_mb_release(pdev);\r\ndev_err(dev, "%s failed with error %d.\n", __func__, err);\r\nreturn err;\r\n}\r\nint bcm_spu_remove(struct platform_device *pdev)\r\n{\r\nint i;\r\nstruct device *dev = &pdev->dev;\r\nchar *cdn;\r\nfor (i = 0; i < ARRAY_SIZE(driver_algs); i++) {\r\nif (!driver_algs[i].registered)\r\ncontinue;\r\nswitch (driver_algs[i].type) {\r\ncase CRYPTO_ALG_TYPE_ABLKCIPHER:\r\ncrypto_unregister_alg(&driver_algs[i].alg.crypto);\r\ndev_dbg(dev, " unregistered cipher %s\n",\r\ndriver_algs[i].alg.crypto.cra_driver_name);\r\ndriver_algs[i].registered = false;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\ncrypto_unregister_ahash(&driver_algs[i].alg.hash);\r\ncdn = driver_algs[i].alg.hash.halg.base.cra_driver_name;\r\ndev_dbg(dev, " unregistered hash %s\n", cdn);\r\ndriver_algs[i].registered = false;\r\nbreak;\r\ncase CRYPTO_ALG_TYPE_AEAD:\r\ncrypto_unregister_aead(&driver_algs[i].alg.aead);\r\ndev_dbg(dev, " unregistered aead %s\n",\r\ndriver_algs[i].alg.aead.base.cra_driver_name);\r\ndriver_algs[i].registered = false;\r\nbreak;\r\n}\r\n}\r\nspu_free_debugfs();\r\nspu_mb_release(pdev);\r\nreturn 0;\r\n}
