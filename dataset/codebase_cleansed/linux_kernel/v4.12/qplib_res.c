static void __free_pbl(struct pci_dev *pdev, struct bnxt_qplib_pbl *pbl,\r\nbool is_umem)\r\n{\r\nint i;\r\nif (!is_umem) {\r\nfor (i = 0; i < pbl->pg_count; i++) {\r\nif (pbl->pg_arr[i])\r\ndma_free_coherent(&pdev->dev, pbl->pg_size,\r\n(void *)((unsigned long)\r\npbl->pg_arr[i] &\r\nPAGE_MASK),\r\npbl->pg_map_arr[i]);\r\nelse\r\ndev_warn(&pdev->dev,\r\n"QPLIB: PBL free pg_arr[%d] empty?!",\r\ni);\r\npbl->pg_arr[i] = NULL;\r\n}\r\n}\r\nkfree(pbl->pg_arr);\r\npbl->pg_arr = NULL;\r\nkfree(pbl->pg_map_arr);\r\npbl->pg_map_arr = NULL;\r\npbl->pg_count = 0;\r\npbl->pg_size = 0;\r\n}\r\nstatic int __alloc_pbl(struct pci_dev *pdev, struct bnxt_qplib_pbl *pbl,\r\nstruct scatterlist *sghead, u32 pages, u32 pg_size)\r\n{\r\nstruct scatterlist *sg;\r\nbool is_umem = false;\r\nint i;\r\npbl->pg_arr = kcalloc(pages, sizeof(void *), GFP_KERNEL);\r\nif (!pbl->pg_arr)\r\nreturn -ENOMEM;\r\npbl->pg_map_arr = kcalloc(pages, sizeof(dma_addr_t), GFP_KERNEL);\r\nif (!pbl->pg_map_arr) {\r\nkfree(pbl->pg_arr);\r\npbl->pg_arr = NULL;\r\nreturn -ENOMEM;\r\n}\r\npbl->pg_count = 0;\r\npbl->pg_size = pg_size;\r\nif (!sghead) {\r\nfor (i = 0; i < pages; i++) {\r\npbl->pg_arr[i] = dma_alloc_coherent(&pdev->dev,\r\npbl->pg_size,\r\n&pbl->pg_map_arr[i],\r\nGFP_KERNEL);\r\nif (!pbl->pg_arr[i])\r\ngoto fail;\r\nmemset(pbl->pg_arr[i], 0, pbl->pg_size);\r\npbl->pg_count++;\r\n}\r\n} else {\r\ni = 0;\r\nis_umem = true;\r\nfor_each_sg(sghead, sg, pages, i) {\r\npbl->pg_map_arr[i] = sg_dma_address(sg);\r\npbl->pg_arr[i] = sg_virt(sg);\r\nif (!pbl->pg_arr[i])\r\ngoto fail;\r\npbl->pg_count++;\r\n}\r\n}\r\nreturn 0;\r\nfail:\r\n__free_pbl(pdev, pbl, is_umem);\r\nreturn -ENOMEM;\r\n}\r\nvoid bnxt_qplib_free_hwq(struct pci_dev *pdev, struct bnxt_qplib_hwq *hwq)\r\n{\r\nint i;\r\nif (!hwq->max_elements)\r\nreturn;\r\nif (hwq->level >= PBL_LVL_MAX)\r\nreturn;\r\nfor (i = 0; i < hwq->level + 1; i++) {\r\nif (i == hwq->level)\r\n__free_pbl(pdev, &hwq->pbl[i], hwq->is_user);\r\nelse\r\n__free_pbl(pdev, &hwq->pbl[i], false);\r\n}\r\nhwq->level = PBL_LVL_MAX;\r\nhwq->max_elements = 0;\r\nhwq->element_size = 0;\r\nhwq->prod = 0;\r\nhwq->cons = 0;\r\nhwq->cp_bit = 0;\r\n}\r\nint bnxt_qplib_alloc_init_hwq(struct pci_dev *pdev, struct bnxt_qplib_hwq *hwq,\r\nstruct scatterlist *sghead, int nmap,\r\nu32 *elements, u32 element_size, u32 aux,\r\nu32 pg_size, enum bnxt_qplib_hwq_type hwq_type)\r\n{\r\nu32 pages, slots, size, aux_pages = 0, aux_size = 0;\r\ndma_addr_t *src_phys_ptr, **dst_virt_ptr;\r\nint i, rc;\r\nhwq->level = PBL_LVL_MAX;\r\nslots = roundup_pow_of_two(*elements);\r\nif (aux) {\r\naux_size = roundup_pow_of_two(aux);\r\naux_pages = (slots * aux_size) / pg_size;\r\nif ((slots * aux_size) % pg_size)\r\naux_pages++;\r\n}\r\nsize = roundup_pow_of_two(element_size);\r\nif (!sghead) {\r\nhwq->is_user = false;\r\npages = (slots * size) / pg_size + aux_pages;\r\nif ((slots * size) % pg_size)\r\npages++;\r\nif (!pages)\r\nreturn -EINVAL;\r\n} else {\r\nhwq->is_user = true;\r\npages = nmap;\r\n}\r\nif (sghead && (pages == MAX_PBL_LVL_0_PGS))\r\nrc = __alloc_pbl(pdev, &hwq->pbl[PBL_LVL_0], sghead,\r\npages, pg_size);\r\nelse\r\nrc = __alloc_pbl(pdev, &hwq->pbl[PBL_LVL_0], NULL, 1, pg_size);\r\nif (rc)\r\ngoto fail;\r\nhwq->level = PBL_LVL_0;\r\nif (pages > MAX_PBL_LVL_0_PGS) {\r\nif (pages > MAX_PBL_LVL_1_PGS) {\r\nrc = __alloc_pbl(pdev, &hwq->pbl[PBL_LVL_1], NULL,\r\nMAX_PBL_LVL_1_PGS_FOR_LVL_2, pg_size);\r\nif (rc)\r\ngoto fail;\r\ndst_virt_ptr =\r\n(dma_addr_t **)hwq->pbl[PBL_LVL_0].pg_arr;\r\nsrc_phys_ptr = hwq->pbl[PBL_LVL_1].pg_map_arr;\r\nfor (i = 0; i < hwq->pbl[PBL_LVL_1].pg_count; i++)\r\ndst_virt_ptr[PTR_PG(i)][PTR_IDX(i)] =\r\nsrc_phys_ptr[i] | PTU_PDE_VALID;\r\nhwq->level = PBL_LVL_1;\r\nrc = __alloc_pbl(pdev, &hwq->pbl[PBL_LVL_2], sghead,\r\npages, pg_size);\r\nif (rc)\r\ngoto fail;\r\ndst_virt_ptr =\r\n(dma_addr_t **)hwq->pbl[PBL_LVL_1].pg_arr;\r\nsrc_phys_ptr = hwq->pbl[PBL_LVL_2].pg_map_arr;\r\nfor (i = 0; i < hwq->pbl[PBL_LVL_2].pg_count; i++) {\r\ndst_virt_ptr[PTR_PG(i)][PTR_IDX(i)] =\r\nsrc_phys_ptr[i] | PTU_PTE_VALID;\r\n}\r\nif (hwq_type == HWQ_TYPE_QUEUE) {\r\ni = hwq->pbl[PBL_LVL_2].pg_count;\r\ndst_virt_ptr[PTR_PG(i - 1)][PTR_IDX(i - 1)] |=\r\nPTU_PTE_LAST;\r\nif (i > 1)\r\ndst_virt_ptr[PTR_PG(i - 2)]\r\n[PTR_IDX(i - 2)] |=\r\nPTU_PTE_NEXT_TO_LAST;\r\n}\r\nhwq->level = PBL_LVL_2;\r\n} else {\r\nu32 flag = hwq_type == HWQ_TYPE_L2_CMPL ? 0 :\r\nPTU_PTE_VALID;\r\nrc = __alloc_pbl(pdev, &hwq->pbl[PBL_LVL_1], sghead,\r\npages, pg_size);\r\nif (rc)\r\ngoto fail;\r\ndst_virt_ptr =\r\n(dma_addr_t **)hwq->pbl[PBL_LVL_0].pg_arr;\r\nsrc_phys_ptr = hwq->pbl[PBL_LVL_1].pg_map_arr;\r\nfor (i = 0; i < hwq->pbl[PBL_LVL_1].pg_count; i++) {\r\ndst_virt_ptr[PTR_PG(i)][PTR_IDX(i)] =\r\nsrc_phys_ptr[i] | flag;\r\n}\r\nif (hwq_type == HWQ_TYPE_QUEUE) {\r\ni = hwq->pbl[PBL_LVL_1].pg_count;\r\ndst_virt_ptr[PTR_PG(i - 1)][PTR_IDX(i - 1)] |=\r\nPTU_PTE_LAST;\r\nif (i > 1)\r\ndst_virt_ptr[PTR_PG(i - 2)]\r\n[PTR_IDX(i - 2)] |=\r\nPTU_PTE_NEXT_TO_LAST;\r\n}\r\nhwq->level = PBL_LVL_1;\r\n}\r\n}\r\nhwq->pdev = pdev;\r\nspin_lock_init(&hwq->lock);\r\nhwq->prod = 0;\r\nhwq->cons = 0;\r\n*elements = hwq->max_elements = slots;\r\nhwq->element_size = size;\r\nhwq->pbl_ptr = hwq->pbl[hwq->level].pg_arr;\r\nhwq->pbl_dma_ptr = hwq->pbl[hwq->level].pg_map_arr;\r\nreturn 0;\r\nfail:\r\nbnxt_qplib_free_hwq(pdev, hwq);\r\nreturn -ENOMEM;\r\n}\r\nvoid bnxt_qplib_free_ctx(struct pci_dev *pdev,\r\nstruct bnxt_qplib_ctx *ctx)\r\n{\r\nint i;\r\nbnxt_qplib_free_hwq(pdev, &ctx->qpc_tbl);\r\nbnxt_qplib_free_hwq(pdev, &ctx->mrw_tbl);\r\nbnxt_qplib_free_hwq(pdev, &ctx->srqc_tbl);\r\nbnxt_qplib_free_hwq(pdev, &ctx->cq_tbl);\r\nbnxt_qplib_free_hwq(pdev, &ctx->tim_tbl);\r\nfor (i = 0; i < MAX_TQM_ALLOC_REQ; i++)\r\nbnxt_qplib_free_hwq(pdev, &ctx->tqm_tbl[i]);\r\nbnxt_qplib_free_hwq(pdev, &ctx->tqm_pde);\r\nbnxt_qplib_free_stats_ctx(pdev, &ctx->stats);\r\n}\r\nint bnxt_qplib_alloc_ctx(struct pci_dev *pdev,\r\nstruct bnxt_qplib_ctx *ctx,\r\nbool virt_fn)\r\n{\r\nint i, j, k, rc = 0;\r\nint fnz_idx = -1;\r\n__le64 **pbl_ptr;\r\nif (virt_fn)\r\ngoto stats_alloc;\r\nctx->qpc_tbl.max_elements = ctx->qpc_count;\r\nrc = bnxt_qplib_alloc_init_hwq(pdev, &ctx->qpc_tbl, NULL, 0,\r\n&ctx->qpc_tbl.max_elements,\r\nBNXT_QPLIB_MAX_QP_CTX_ENTRY_SIZE, 0,\r\nPAGE_SIZE, HWQ_TYPE_CTX);\r\nif (rc)\r\ngoto fail;\r\nctx->mrw_tbl.max_elements = ctx->mrw_count;\r\nrc = bnxt_qplib_alloc_init_hwq(pdev, &ctx->mrw_tbl, NULL, 0,\r\n&ctx->mrw_tbl.max_elements,\r\nBNXT_QPLIB_MAX_MRW_CTX_ENTRY_SIZE, 0,\r\nPAGE_SIZE, HWQ_TYPE_CTX);\r\nif (rc)\r\ngoto fail;\r\nctx->srqc_tbl.max_elements = ctx->srqc_count;\r\nrc = bnxt_qplib_alloc_init_hwq(pdev, &ctx->srqc_tbl, NULL, 0,\r\n&ctx->srqc_tbl.max_elements,\r\nBNXT_QPLIB_MAX_SRQ_CTX_ENTRY_SIZE, 0,\r\nPAGE_SIZE, HWQ_TYPE_CTX);\r\nif (rc)\r\ngoto fail;\r\nctx->cq_tbl.max_elements = ctx->cq_count;\r\nrc = bnxt_qplib_alloc_init_hwq(pdev, &ctx->cq_tbl, NULL, 0,\r\n&ctx->cq_tbl.max_elements,\r\nBNXT_QPLIB_MAX_CQ_CTX_ENTRY_SIZE, 0,\r\nPAGE_SIZE, HWQ_TYPE_CTX);\r\nif (rc)\r\ngoto fail;\r\nctx->tqm_pde.max_elements = 512;\r\nrc = bnxt_qplib_alloc_init_hwq(pdev, &ctx->tqm_pde, NULL, 0,\r\n&ctx->tqm_pde.max_elements, sizeof(u64),\r\n0, PAGE_SIZE, HWQ_TYPE_CTX);\r\nif (rc)\r\ngoto fail;\r\nfor (i = 0; i < MAX_TQM_ALLOC_REQ; i++) {\r\nif (!ctx->tqm_count[i])\r\ncontinue;\r\nctx->tqm_tbl[i].max_elements = ctx->qpc_count *\r\nctx->tqm_count[i];\r\nrc = bnxt_qplib_alloc_init_hwq(pdev, &ctx->tqm_tbl[i], NULL, 0,\r\n&ctx->tqm_tbl[i].max_elements, 1,\r\n0, PAGE_SIZE, HWQ_TYPE_CTX);\r\nif (rc)\r\ngoto fail;\r\n}\r\npbl_ptr = (__le64 **)ctx->tqm_pde.pbl_ptr;\r\nfor (i = 0, j = 0; i < MAX_TQM_ALLOC_REQ;\r\ni++, j += MAX_TQM_ALLOC_BLK_SIZE) {\r\nif (!ctx->tqm_tbl[i].max_elements)\r\ncontinue;\r\nif (fnz_idx == -1)\r\nfnz_idx = i;\r\nswitch (ctx->tqm_tbl[i].level) {\r\ncase PBL_LVL_2:\r\nfor (k = 0; k < ctx->tqm_tbl[i].pbl[PBL_LVL_1].pg_count;\r\nk++)\r\npbl_ptr[PTR_PG(j + k)][PTR_IDX(j + k)] =\r\ncpu_to_le64(\r\nctx->tqm_tbl[i].pbl[PBL_LVL_1].pg_map_arr[k]\r\n| PTU_PTE_VALID);\r\nbreak;\r\ncase PBL_LVL_1:\r\ncase PBL_LVL_0:\r\ndefault:\r\npbl_ptr[PTR_PG(j)][PTR_IDX(j)] = cpu_to_le64(\r\nctx->tqm_tbl[i].pbl[PBL_LVL_0].pg_map_arr[0] |\r\nPTU_PTE_VALID);\r\nbreak;\r\n}\r\n}\r\nif (fnz_idx == -1)\r\nfnz_idx = 0;\r\nctx->tqm_pde_level = ctx->tqm_tbl[fnz_idx].level == PBL_LVL_2 ?\r\nPBL_LVL_2 : ctx->tqm_tbl[fnz_idx].level + 1;\r\nctx->tim_tbl.max_elements = ctx->qpc_count * 16;\r\nrc = bnxt_qplib_alloc_init_hwq(pdev, &ctx->tim_tbl, NULL, 0,\r\n&ctx->tim_tbl.max_elements, 1,\r\n0, PAGE_SIZE, HWQ_TYPE_CTX);\r\nif (rc)\r\ngoto fail;\r\nstats_alloc:\r\nrc = bnxt_qplib_alloc_stats_ctx(pdev, &ctx->stats);\r\nif (rc)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\nbnxt_qplib_free_ctx(pdev, ctx);\r\nreturn rc;\r\n}\r\nvoid bnxt_qplib_get_guid(u8 *dev_addr, u8 *guid)\r\n{\r\nu8 mac[ETH_ALEN];\r\nmemcpy(mac, dev_addr, ETH_ALEN);\r\nguid[0] = mac[0] ^ 2;\r\nguid[1] = mac[1];\r\nguid[2] = mac[2];\r\nguid[3] = 0xff;\r\nguid[4] = 0xfe;\r\nguid[5] = mac[3];\r\nguid[6] = mac[4];\r\nguid[7] = mac[5];\r\n}\r\nstatic void bnxt_qplib_free_sgid_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_sgid_tbl *sgid_tbl)\r\n{\r\nkfree(sgid_tbl->tbl);\r\nkfree(sgid_tbl->hw_id);\r\nkfree(sgid_tbl->ctx);\r\nsgid_tbl->tbl = NULL;\r\nsgid_tbl->hw_id = NULL;\r\nsgid_tbl->ctx = NULL;\r\nsgid_tbl->max = 0;\r\nsgid_tbl->active = 0;\r\n}\r\nstatic int bnxt_qplib_alloc_sgid_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_sgid_tbl *sgid_tbl,\r\nu16 max)\r\n{\r\nsgid_tbl->tbl = kcalloc(max, sizeof(struct bnxt_qplib_gid), GFP_KERNEL);\r\nif (!sgid_tbl->tbl)\r\nreturn -ENOMEM;\r\nsgid_tbl->hw_id = kcalloc(max, sizeof(u16), GFP_KERNEL);\r\nif (!sgid_tbl->hw_id)\r\ngoto out_free1;\r\nsgid_tbl->ctx = kcalloc(max, sizeof(void *), GFP_KERNEL);\r\nif (!sgid_tbl->ctx)\r\ngoto out_free2;\r\nsgid_tbl->max = max;\r\nreturn 0;\r\nout_free2:\r\nkfree(sgid_tbl->hw_id);\r\nsgid_tbl->hw_id = NULL;\r\nout_free1:\r\nkfree(sgid_tbl->tbl);\r\nsgid_tbl->tbl = NULL;\r\nreturn -ENOMEM;\r\n}\r\nstatic void bnxt_qplib_cleanup_sgid_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_sgid_tbl *sgid_tbl)\r\n{\r\nint i;\r\nfor (i = 0; i < sgid_tbl->max; i++) {\r\nif (memcmp(&sgid_tbl->tbl[i], &bnxt_qplib_gid_zero,\r\nsizeof(bnxt_qplib_gid_zero)))\r\nbnxt_qplib_del_sgid(sgid_tbl, &sgid_tbl->tbl[i], true);\r\n}\r\nmemset(sgid_tbl->tbl, 0, sizeof(struct bnxt_qplib_gid) * sgid_tbl->max);\r\nmemset(sgid_tbl->hw_id, -1, sizeof(u16) * sgid_tbl->max);\r\nsgid_tbl->active = 0;\r\n}\r\nstatic void bnxt_qplib_init_sgid_tbl(struct bnxt_qplib_sgid_tbl *sgid_tbl,\r\nstruct net_device *netdev)\r\n{\r\nmemset(sgid_tbl->tbl, 0, sizeof(struct bnxt_qplib_gid) * sgid_tbl->max);\r\nmemset(sgid_tbl->hw_id, -1, sizeof(u16) * sgid_tbl->max);\r\n}\r\nstatic void bnxt_qplib_free_pkey_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_pkey_tbl *pkey_tbl)\r\n{\r\nif (!pkey_tbl->tbl)\r\ndev_dbg(&res->pdev->dev, "QPLIB: PKEY tbl not present");\r\nelse\r\nkfree(pkey_tbl->tbl);\r\npkey_tbl->tbl = NULL;\r\npkey_tbl->max = 0;\r\npkey_tbl->active = 0;\r\n}\r\nstatic int bnxt_qplib_alloc_pkey_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_pkey_tbl *pkey_tbl,\r\nu16 max)\r\n{\r\npkey_tbl->tbl = kcalloc(max, sizeof(u16), GFP_KERNEL);\r\nif (!pkey_tbl->tbl)\r\nreturn -ENOMEM;\r\npkey_tbl->max = max;\r\nreturn 0;\r\n}\r\nint bnxt_qplib_alloc_pd(struct bnxt_qplib_pd_tbl *pdt, struct bnxt_qplib_pd *pd)\r\n{\r\nu32 bit_num;\r\nbit_num = find_first_bit(pdt->tbl, pdt->max);\r\nif (bit_num == pdt->max)\r\nreturn -ENOMEM;\r\nclear_bit(bit_num, pdt->tbl);\r\npd->id = bit_num;\r\nreturn 0;\r\n}\r\nint bnxt_qplib_dealloc_pd(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_pd_tbl *pdt,\r\nstruct bnxt_qplib_pd *pd)\r\n{\r\nif (test_and_set_bit(pd->id, pdt->tbl)) {\r\ndev_warn(&res->pdev->dev, "Freeing an unused PD? pdn = %d",\r\npd->id);\r\nreturn -EINVAL;\r\n}\r\npd->id = 0;\r\nreturn 0;\r\n}\r\nstatic void bnxt_qplib_free_pd_tbl(struct bnxt_qplib_pd_tbl *pdt)\r\n{\r\nkfree(pdt->tbl);\r\npdt->tbl = NULL;\r\npdt->max = 0;\r\n}\r\nstatic int bnxt_qplib_alloc_pd_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_pd_tbl *pdt,\r\nu32 max)\r\n{\r\nu32 bytes;\r\nbytes = max >> 3;\r\nif (!bytes)\r\nbytes = 1;\r\npdt->tbl = kmalloc(bytes, GFP_KERNEL);\r\nif (!pdt->tbl)\r\nreturn -ENOMEM;\r\npdt->max = max;\r\nmemset((u8 *)pdt->tbl, 0xFF, bytes);\r\nreturn 0;\r\n}\r\nint bnxt_qplib_alloc_dpi(struct bnxt_qplib_dpi_tbl *dpit,\r\nstruct bnxt_qplib_dpi *dpi,\r\nvoid *app)\r\n{\r\nu32 bit_num;\r\nbit_num = find_first_bit(dpit->tbl, dpit->max);\r\nif (bit_num == dpit->max)\r\nreturn -ENOMEM;\r\nclear_bit(bit_num, dpit->tbl);\r\ndpit->app_tbl[bit_num] = app;\r\ndpi->dpi = bit_num;\r\ndpi->dbr = dpit->dbr_bar_reg_iomem + (bit_num * PAGE_SIZE);\r\ndpi->umdbr = dpit->unmapped_dbr + (bit_num * PAGE_SIZE);\r\nreturn 0;\r\n}\r\nint bnxt_qplib_dealloc_dpi(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_dpi_tbl *dpit,\r\nstruct bnxt_qplib_dpi *dpi)\r\n{\r\nif (dpi->dpi >= dpit->max) {\r\ndev_warn(&res->pdev->dev, "Invalid DPI? dpi = %d", dpi->dpi);\r\nreturn -EINVAL;\r\n}\r\nif (test_and_set_bit(dpi->dpi, dpit->tbl)) {\r\ndev_warn(&res->pdev->dev, "Freeing an unused DPI? dpi = %d",\r\ndpi->dpi);\r\nreturn -EINVAL;\r\n}\r\nif (dpit->app_tbl)\r\ndpit->app_tbl[dpi->dpi] = NULL;\r\nmemset(dpi, 0, sizeof(*dpi));\r\nreturn 0;\r\n}\r\nstatic void bnxt_qplib_free_dpi_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_dpi_tbl *dpit)\r\n{\r\nkfree(dpit->tbl);\r\nkfree(dpit->app_tbl);\r\nif (dpit->dbr_bar_reg_iomem)\r\npci_iounmap(res->pdev, dpit->dbr_bar_reg_iomem);\r\nmemset(dpit, 0, sizeof(*dpit));\r\n}\r\nstatic int bnxt_qplib_alloc_dpi_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_dpi_tbl *dpit,\r\nu32 dbr_offset)\r\n{\r\nu32 dbr_bar_reg = RCFW_DBR_PCI_BAR_REGION;\r\nresource_size_t bar_reg_base;\r\nu32 dbr_len, bytes;\r\nif (dpit->dbr_bar_reg_iomem) {\r\ndev_err(&res->pdev->dev,\r\n"QPLIB: DBR BAR region %d already mapped", dbr_bar_reg);\r\nreturn -EALREADY;\r\n}\r\nbar_reg_base = pci_resource_start(res->pdev, dbr_bar_reg);\r\nif (!bar_reg_base) {\r\ndev_err(&res->pdev->dev,\r\n"QPLIB: BAR region %d resc start failed", dbr_bar_reg);\r\nreturn -ENOMEM;\r\n}\r\ndbr_len = pci_resource_len(res->pdev, dbr_bar_reg) - dbr_offset;\r\nif (!dbr_len || ((dbr_len & (PAGE_SIZE - 1)) != 0)) {\r\ndev_err(&res->pdev->dev, "QPLIB: Invalid DBR length %d",\r\ndbr_len);\r\nreturn -ENOMEM;\r\n}\r\ndpit->dbr_bar_reg_iomem = ioremap_nocache(bar_reg_base + dbr_offset,\r\ndbr_len);\r\nif (!dpit->dbr_bar_reg_iomem) {\r\ndev_err(&res->pdev->dev,\r\n"QPLIB: FP: DBR BAR region %d mapping failed",\r\ndbr_bar_reg);\r\nreturn -ENOMEM;\r\n}\r\ndpit->unmapped_dbr = bar_reg_base + dbr_offset;\r\ndpit->max = dbr_len / PAGE_SIZE;\r\ndpit->app_tbl = kcalloc(dpit->max, sizeof(void *), GFP_KERNEL);\r\nif (!dpit->app_tbl) {\r\npci_iounmap(res->pdev, dpit->dbr_bar_reg_iomem);\r\ndev_err(&res->pdev->dev,\r\n"QPLIB: DPI app tbl allocation failed");\r\nreturn -ENOMEM;\r\n}\r\nbytes = dpit->max >> 3;\r\nif (!bytes)\r\nbytes = 1;\r\ndpit->tbl = kmalloc(bytes, GFP_KERNEL);\r\nif (!dpit->tbl) {\r\npci_iounmap(res->pdev, dpit->dbr_bar_reg_iomem);\r\nkfree(dpit->app_tbl);\r\ndpit->app_tbl = NULL;\r\ndev_err(&res->pdev->dev,\r\n"QPLIB: DPI tbl allocation failed for size = %d",\r\nbytes);\r\nreturn -ENOMEM;\r\n}\r\nmemset((u8 *)dpit->tbl, 0xFF, bytes);\r\nreturn 0;\r\n}\r\nstatic void bnxt_qplib_cleanup_pkey_tbl(struct bnxt_qplib_pkey_tbl *pkey_tbl)\r\n{\r\nmemset(pkey_tbl->tbl, 0, sizeof(u16) * pkey_tbl->max);\r\npkey_tbl->active = 0;\r\n}\r\nstatic void bnxt_qplib_init_pkey_tbl(struct bnxt_qplib_res *res,\r\nstruct bnxt_qplib_pkey_tbl *pkey_tbl)\r\n{\r\nu16 pkey = 0xFFFF;\r\nmemset(pkey_tbl->tbl, 0, sizeof(u16) * pkey_tbl->max);\r\nbnxt_qplib_add_pkey(res, pkey_tbl, &pkey, false);\r\n}\r\nstatic void bnxt_qplib_free_stats_ctx(struct pci_dev *pdev,\r\nstruct bnxt_qplib_stats *stats)\r\n{\r\nif (stats->dma) {\r\ndma_free_coherent(&pdev->dev, stats->size,\r\nstats->dma, stats->dma_map);\r\n}\r\nmemset(stats, 0, sizeof(*stats));\r\nstats->fw_id = -1;\r\n}\r\nstatic int bnxt_qplib_alloc_stats_ctx(struct pci_dev *pdev,\r\nstruct bnxt_qplib_stats *stats)\r\n{\r\nmemset(stats, 0, sizeof(*stats));\r\nstats->fw_id = -1;\r\nstats->size = sizeof(struct ctx_hw_stats);\r\nstats->dma = dma_alloc_coherent(&pdev->dev, stats->size,\r\n&stats->dma_map, GFP_KERNEL);\r\nif (!stats->dma) {\r\ndev_err(&pdev->dev, "QPLIB: Stats DMA allocation failed");\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nvoid bnxt_qplib_cleanup_res(struct bnxt_qplib_res *res)\r\n{\r\nbnxt_qplib_cleanup_pkey_tbl(&res->pkey_tbl);\r\nbnxt_qplib_cleanup_sgid_tbl(res, &res->sgid_tbl);\r\n}\r\nint bnxt_qplib_init_res(struct bnxt_qplib_res *res)\r\n{\r\nbnxt_qplib_init_sgid_tbl(&res->sgid_tbl, res->netdev);\r\nbnxt_qplib_init_pkey_tbl(res, &res->pkey_tbl);\r\nreturn 0;\r\n}\r\nvoid bnxt_qplib_free_res(struct bnxt_qplib_res *res)\r\n{\r\nbnxt_qplib_free_pkey_tbl(res, &res->pkey_tbl);\r\nbnxt_qplib_free_sgid_tbl(res, &res->sgid_tbl);\r\nbnxt_qplib_free_pd_tbl(&res->pd_tbl);\r\nbnxt_qplib_free_dpi_tbl(res, &res->dpi_tbl);\r\nres->netdev = NULL;\r\nres->pdev = NULL;\r\n}\r\nint bnxt_qplib_alloc_res(struct bnxt_qplib_res *res, struct pci_dev *pdev,\r\nstruct net_device *netdev,\r\nstruct bnxt_qplib_dev_attr *dev_attr)\r\n{\r\nint rc = 0;\r\nres->pdev = pdev;\r\nres->netdev = netdev;\r\nrc = bnxt_qplib_alloc_sgid_tbl(res, &res->sgid_tbl, dev_attr->max_sgid);\r\nif (rc)\r\ngoto fail;\r\nrc = bnxt_qplib_alloc_pkey_tbl(res, &res->pkey_tbl, dev_attr->max_pkey);\r\nif (rc)\r\ngoto fail;\r\nrc = bnxt_qplib_alloc_pd_tbl(res, &res->pd_tbl, dev_attr->max_pd);\r\nif (rc)\r\ngoto fail;\r\nrc = bnxt_qplib_alloc_dpi_tbl(res, &res->dpi_tbl, dev_attr->l2_db_size);\r\nif (rc)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\nbnxt_qplib_free_res(res);\r\nreturn rc;\r\n}
