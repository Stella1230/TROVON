unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)\r\n{\r\nreturn per_cpu(cpu_scale, cpu);\r\n}\r\nstatic void set_capacity_scale(unsigned int cpu, unsigned long capacity)\r\n{\r\nper_cpu(cpu_scale, cpu) = capacity;\r\n}\r\nstatic ssize_t cpu_capacity_show(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct cpu *cpu = container_of(dev, struct cpu, dev);\r\nreturn sprintf(buf, "%lu\n",\r\narch_scale_cpu_capacity(NULL, cpu->dev.id));\r\n}\r\nstatic ssize_t cpu_capacity_store(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf,\r\nsize_t count)\r\n{\r\nstruct cpu *cpu = container_of(dev, struct cpu, dev);\r\nint this_cpu = cpu->dev.id, i;\r\nunsigned long new_capacity;\r\nssize_t ret;\r\nif (count) {\r\nret = kstrtoul(buf, 0, &new_capacity);\r\nif (ret)\r\nreturn ret;\r\nif (new_capacity > SCHED_CAPACITY_SCALE)\r\nreturn -EINVAL;\r\nmutex_lock(&cpu_scale_mutex);\r\nfor_each_cpu(i, &cpu_topology[this_cpu].core_sibling)\r\nset_capacity_scale(i, new_capacity);\r\nmutex_unlock(&cpu_scale_mutex);\r\n}\r\nreturn count;\r\n}\r\nstatic int register_cpu_capacity_sysctl(void)\r\n{\r\nint i;\r\nstruct device *cpu;\r\nfor_each_possible_cpu(i) {\r\ncpu = get_cpu_device(i);\r\nif (!cpu) {\r\npr_err("%s: too early to get CPU%d device!\n",\r\n__func__, i);\r\ncontinue;\r\n}\r\ndevice_create_file(cpu, &dev_attr_cpu_capacity);\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init parse_cpu_capacity(struct device_node *cpu_node, int cpu)\r\n{\r\nint ret = 1;\r\nu32 cpu_capacity;\r\nif (cap_parsing_failed)\r\nreturn !ret;\r\nret = of_property_read_u32(cpu_node,\r\n"capacity-dmips-mhz",\r\n&cpu_capacity);\r\nif (!ret) {\r\nif (!raw_capacity) {\r\nraw_capacity = kcalloc(num_possible_cpus(),\r\nsizeof(*raw_capacity),\r\nGFP_KERNEL);\r\nif (!raw_capacity) {\r\npr_err("cpu_capacity: failed to allocate memory for raw capacities\n");\r\ncap_parsing_failed = true;\r\nreturn !ret;\r\n}\r\n}\r\ncapacity_scale = max(cpu_capacity, capacity_scale);\r\nraw_capacity[cpu] = cpu_capacity;\r\npr_debug("cpu_capacity: %s cpu_capacity=%u (raw)\n",\r\ncpu_node->full_name, raw_capacity[cpu]);\r\n} else {\r\nif (raw_capacity) {\r\npr_err("cpu_capacity: missing %s raw capacity\n",\r\ncpu_node->full_name);\r\npr_err("cpu_capacity: partial information: fallback to 1024 for all CPUs\n");\r\n}\r\ncap_parsing_failed = true;\r\nkfree(raw_capacity);\r\n}\r\nreturn !ret;\r\n}\r\nstatic void normalize_cpu_capacity(void)\r\n{\r\nu64 capacity;\r\nint cpu;\r\nif (!raw_capacity || cap_parsing_failed)\r\nreturn;\r\npr_debug("cpu_capacity: capacity_scale=%u\n", capacity_scale);\r\nmutex_lock(&cpu_scale_mutex);\r\nfor_each_possible_cpu(cpu) {\r\ncapacity = (raw_capacity[cpu] << SCHED_CAPACITY_SHIFT)\r\n/ capacity_scale;\r\nset_capacity_scale(cpu, capacity);\r\npr_debug("cpu_capacity: CPU%d cpu_capacity=%lu\n",\r\ncpu, arch_scale_cpu_capacity(NULL, cpu));\r\n}\r\nmutex_unlock(&cpu_scale_mutex);\r\n}\r\nstatic int\r\ninit_cpu_capacity_callback(struct notifier_block *nb,\r\nunsigned long val,\r\nvoid *data)\r\n{\r\nstruct cpufreq_policy *policy = data;\r\nint cpu;\r\nif (cap_parsing_failed || cap_parsing_done)\r\nreturn 0;\r\nswitch (val) {\r\ncase CPUFREQ_NOTIFY:\r\npr_debug("cpu_capacity: init cpu capacity for CPUs [%*pbl] (to_visit=%*pbl)\n",\r\ncpumask_pr_args(policy->related_cpus),\r\ncpumask_pr_args(cpus_to_visit));\r\ncpumask_andnot(cpus_to_visit,\r\ncpus_to_visit,\r\npolicy->related_cpus);\r\nfor_each_cpu(cpu, policy->related_cpus) {\r\nraw_capacity[cpu] = arch_scale_cpu_capacity(NULL, cpu) *\r\npolicy->cpuinfo.max_freq / 1000UL;\r\ncapacity_scale = max(raw_capacity[cpu], capacity_scale);\r\n}\r\nif (cpumask_empty(cpus_to_visit)) {\r\nnormalize_cpu_capacity();\r\nkfree(raw_capacity);\r\npr_debug("cpu_capacity: parsing done\n");\r\ncap_parsing_done = true;\r\nschedule_work(&parsing_done_work);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init register_cpufreq_notifier(void)\r\n{\r\nif (cap_parsing_failed)\r\nreturn -EINVAL;\r\nif (!alloc_cpumask_var(&cpus_to_visit, GFP_KERNEL)) {\r\npr_err("cpu_capacity: failed to allocate memory for cpus_to_visit\n");\r\nreturn -ENOMEM;\r\n}\r\ncpumask_copy(cpus_to_visit, cpu_possible_mask);\r\nreturn cpufreq_register_notifier(&init_cpu_capacity_notifier,\r\nCPUFREQ_POLICY_NOTIFIER);\r\n}\r\nstatic void parsing_done_workfn(struct work_struct *work)\r\n{\r\ncpufreq_unregister_notifier(&init_cpu_capacity_notifier,\r\nCPUFREQ_POLICY_NOTIFIER);\r\n}\r\nstatic int __init free_raw_capacity(void)\r\n{\r\nkfree(raw_capacity);\r\nreturn 0;\r\n}\r\nstatic void __init parse_dt_topology(void)\r\n{\r\nconst struct cpu_efficiency *cpu_eff;\r\nstruct device_node *cn = NULL;\r\nunsigned long min_capacity = ULONG_MAX;\r\nunsigned long max_capacity = 0;\r\nunsigned long capacity = 0;\r\nint cpu = 0;\r\n__cpu_capacity = kcalloc(nr_cpu_ids, sizeof(*__cpu_capacity),\r\nGFP_NOWAIT);\r\ncn = of_find_node_by_path("/cpus");\r\nif (!cn) {\r\npr_err("No CPU information found in DT\n");\r\nreturn;\r\n}\r\nfor_each_possible_cpu(cpu) {\r\nconst u32 *rate;\r\nint len;\r\ncn = of_get_cpu_node(cpu, NULL);\r\nif (!cn) {\r\npr_err("missing device node for CPU %d\n", cpu);\r\ncontinue;\r\n}\r\nif (parse_cpu_capacity(cn, cpu)) {\r\nof_node_put(cn);\r\ncontinue;\r\n}\r\ncap_from_dt = false;\r\nfor (cpu_eff = table_efficiency; cpu_eff->compatible; cpu_eff++)\r\nif (of_device_is_compatible(cn, cpu_eff->compatible))\r\nbreak;\r\nif (cpu_eff->compatible == NULL)\r\ncontinue;\r\nrate = of_get_property(cn, "clock-frequency", &len);\r\nif (!rate || len != 4) {\r\npr_err("%s missing clock-frequency property\n",\r\ncn->full_name);\r\ncontinue;\r\n}\r\ncapacity = ((be32_to_cpup(rate)) >> 20) * cpu_eff->efficiency;\r\nif (capacity < min_capacity)\r\nmin_capacity = capacity;\r\nif (capacity > max_capacity)\r\nmax_capacity = capacity;\r\ncpu_capacity(cpu) = capacity;\r\n}\r\nif (4*max_capacity < (3*(max_capacity + min_capacity)))\r\nmiddle_capacity = (min_capacity + max_capacity)\r\n>> (SCHED_CAPACITY_SHIFT+1);\r\nelse\r\nmiddle_capacity = ((max_capacity / 3)\r\n>> (SCHED_CAPACITY_SHIFT-1)) + 1;\r\nif (cap_from_dt && !cap_parsing_failed)\r\nnormalize_cpu_capacity();\r\n}\r\nstatic void update_cpu_capacity(unsigned int cpu)\r\n{\r\nif (!cpu_capacity(cpu) || cap_from_dt)\r\nreturn;\r\nset_capacity_scale(cpu, cpu_capacity(cpu) / middle_capacity);\r\npr_info("CPU%u: update cpu_capacity %lu\n",\r\ncpu, arch_scale_cpu_capacity(NULL, cpu));\r\n}\r\nstatic inline void parse_dt_topology(void) {}\r\nstatic inline void update_cpu_capacity(unsigned int cpuid) {}\r\nconst struct cpumask *cpu_coregroup_mask(int cpu)\r\n{\r\nreturn &cpu_topology[cpu].core_sibling;\r\n}\r\nconst struct cpumask *cpu_corepower_mask(int cpu)\r\n{\r\nreturn &cpu_topology[cpu].thread_sibling;\r\n}\r\nstatic void update_siblings_masks(unsigned int cpuid)\r\n{\r\nstruct cputopo_arm *cpu_topo, *cpuid_topo = &cpu_topology[cpuid];\r\nint cpu;\r\nfor_each_possible_cpu(cpu) {\r\ncpu_topo = &cpu_topology[cpu];\r\nif (cpuid_topo->socket_id != cpu_topo->socket_id)\r\ncontinue;\r\ncpumask_set_cpu(cpuid, &cpu_topo->core_sibling);\r\nif (cpu != cpuid)\r\ncpumask_set_cpu(cpu, &cpuid_topo->core_sibling);\r\nif (cpuid_topo->core_id != cpu_topo->core_id)\r\ncontinue;\r\ncpumask_set_cpu(cpuid, &cpu_topo->thread_sibling);\r\nif (cpu != cpuid)\r\ncpumask_set_cpu(cpu, &cpuid_topo->thread_sibling);\r\n}\r\nsmp_wmb();\r\n}\r\nvoid store_cpu_topology(unsigned int cpuid)\r\n{\r\nstruct cputopo_arm *cpuid_topo = &cpu_topology[cpuid];\r\nunsigned int mpidr;\r\nif (cpuid_topo->core_id != -1)\r\nreturn;\r\nmpidr = read_cpuid_mpidr();\r\nif ((mpidr & MPIDR_SMP_BITMASK) == MPIDR_SMP_VALUE) {\r\nif (mpidr & MPIDR_MT_BITMASK) {\r\ncpuid_topo->thread_id = MPIDR_AFFINITY_LEVEL(mpidr, 0);\r\ncpuid_topo->core_id = MPIDR_AFFINITY_LEVEL(mpidr, 1);\r\ncpuid_topo->socket_id = MPIDR_AFFINITY_LEVEL(mpidr, 2);\r\n} else {\r\ncpuid_topo->thread_id = -1;\r\ncpuid_topo->core_id = MPIDR_AFFINITY_LEVEL(mpidr, 0);\r\ncpuid_topo->socket_id = MPIDR_AFFINITY_LEVEL(mpidr, 1);\r\n}\r\n} else {\r\ncpuid_topo->thread_id = -1;\r\ncpuid_topo->core_id = 0;\r\ncpuid_topo->socket_id = -1;\r\n}\r\nupdate_siblings_masks(cpuid);\r\nupdate_cpu_capacity(cpuid);\r\npr_info("CPU%u: thread %d, cpu %d, socket %d, mpidr %x\n",\r\ncpuid, cpu_topology[cpuid].thread_id,\r\ncpu_topology[cpuid].core_id,\r\ncpu_topology[cpuid].socket_id, mpidr);\r\n}\r\nstatic inline int cpu_corepower_flags(void)\r\n{\r\nreturn SD_SHARE_PKG_RESOURCES | SD_SHARE_POWERDOMAIN;\r\n}\r\nvoid __init init_cpu_topology(void)\r\n{\r\nunsigned int cpu;\r\nfor_each_possible_cpu(cpu) {\r\nstruct cputopo_arm *cpu_topo = &(cpu_topology[cpu]);\r\ncpu_topo->thread_id = -1;\r\ncpu_topo->core_id = -1;\r\ncpu_topo->socket_id = -1;\r\ncpumask_clear(&cpu_topo->core_sibling);\r\ncpumask_clear(&cpu_topo->thread_sibling);\r\n}\r\nsmp_wmb();\r\nparse_dt_topology();\r\nset_sched_topology(arm_topology);\r\n}
