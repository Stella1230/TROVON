static const char *atmel_sha_reg_name(u32 offset, char *tmp, size_t sz, bool wr)\r\n{\r\nswitch (offset) {\r\ncase SHA_CR:\r\nreturn "CR";\r\ncase SHA_MR:\r\nreturn "MR";\r\ncase SHA_IER:\r\nreturn "IER";\r\ncase SHA_IDR:\r\nreturn "IDR";\r\ncase SHA_IMR:\r\nreturn "IMR";\r\ncase SHA_ISR:\r\nreturn "ISR";\r\ncase SHA_MSR:\r\nreturn "MSR";\r\ncase SHA_BCR:\r\nreturn "BCR";\r\ncase SHA_REG_DIN(0):\r\ncase SHA_REG_DIN(1):\r\ncase SHA_REG_DIN(2):\r\ncase SHA_REG_DIN(3):\r\ncase SHA_REG_DIN(4):\r\ncase SHA_REG_DIN(5):\r\ncase SHA_REG_DIN(6):\r\ncase SHA_REG_DIN(7):\r\ncase SHA_REG_DIN(8):\r\ncase SHA_REG_DIN(9):\r\ncase SHA_REG_DIN(10):\r\ncase SHA_REG_DIN(11):\r\ncase SHA_REG_DIN(12):\r\ncase SHA_REG_DIN(13):\r\ncase SHA_REG_DIN(14):\r\ncase SHA_REG_DIN(15):\r\nsnprintf(tmp, sz, "IDATAR[%u]", (offset - SHA_REG_DIN(0)) >> 2);\r\nbreak;\r\ncase SHA_REG_DIGEST(0):\r\ncase SHA_REG_DIGEST(1):\r\ncase SHA_REG_DIGEST(2):\r\ncase SHA_REG_DIGEST(3):\r\ncase SHA_REG_DIGEST(4):\r\ncase SHA_REG_DIGEST(5):\r\ncase SHA_REG_DIGEST(6):\r\ncase SHA_REG_DIGEST(7):\r\ncase SHA_REG_DIGEST(8):\r\ncase SHA_REG_DIGEST(9):\r\ncase SHA_REG_DIGEST(10):\r\ncase SHA_REG_DIGEST(11):\r\ncase SHA_REG_DIGEST(12):\r\ncase SHA_REG_DIGEST(13):\r\ncase SHA_REG_DIGEST(14):\r\ncase SHA_REG_DIGEST(15):\r\nif (wr)\r\nsnprintf(tmp, sz, "IDATAR[%u]",\r\n16u + ((offset - SHA_REG_DIGEST(0)) >> 2));\r\nelse\r\nsnprintf(tmp, sz, "ODATAR[%u]",\r\n(offset - SHA_REG_DIGEST(0)) >> 2);\r\nbreak;\r\ncase SHA_HW_VERSION:\r\nreturn "HWVER";\r\ndefault:\r\nsnprintf(tmp, sz, "0x%02x", offset);\r\nbreak;\r\n}\r\nreturn tmp;\r\n}\r\nstatic inline u32 atmel_sha_read(struct atmel_sha_dev *dd, u32 offset)\r\n{\r\nu32 value = readl_relaxed(dd->io_base + offset);\r\n#ifdef VERBOSE_DEBUG\r\nif (dd->flags & SHA_FLAGS_DUMP_REG) {\r\nchar tmp[16];\r\ndev_vdbg(dd->dev, "read 0x%08x from %s\n", value,\r\natmel_sha_reg_name(offset, tmp, sizeof(tmp), false));\r\n}\r\n#endif\r\nreturn value;\r\n}\r\nstatic inline void atmel_sha_write(struct atmel_sha_dev *dd,\r\nu32 offset, u32 value)\r\n{\r\n#ifdef VERBOSE_DEBUG\r\nif (dd->flags & SHA_FLAGS_DUMP_REG) {\r\nchar tmp[16];\r\ndev_vdbg(dd->dev, "write 0x%08x into %s\n", value,\r\natmel_sha_reg_name(offset, tmp, sizeof(tmp), true));\r\n}\r\n#endif\r\nwritel_relaxed(value, dd->io_base + offset);\r\n}\r\nstatic inline int atmel_sha_complete(struct atmel_sha_dev *dd, int err)\r\n{\r\nstruct ahash_request *req = dd->req;\r\ndd->flags &= ~(SHA_FLAGS_BUSY | SHA_FLAGS_FINAL | SHA_FLAGS_CPU |\r\nSHA_FLAGS_DMA_READY | SHA_FLAGS_OUTPUT_READY |\r\nSHA_FLAGS_DUMP_REG);\r\nclk_disable(dd->iclk);\r\nif ((dd->is_async || dd->force_complete) && req->base.complete)\r\nreq->base.complete(&req->base, err);\r\ntasklet_schedule(&dd->queue_task);\r\nreturn err;\r\n}\r\nstatic size_t atmel_sha_append_sg(struct atmel_sha_reqctx *ctx)\r\n{\r\nsize_t count;\r\nwhile ((ctx->bufcnt < ctx->buflen) && ctx->total) {\r\ncount = min(ctx->sg->length - ctx->offset, ctx->total);\r\ncount = min(count, ctx->buflen - ctx->bufcnt);\r\nif (count <= 0) {\r\nif ((ctx->sg->length == 0) && !sg_is_last(ctx->sg)) {\r\nctx->sg = sg_next(ctx->sg);\r\ncontinue;\r\n} else {\r\nbreak;\r\n}\r\n}\r\nscatterwalk_map_and_copy(ctx->buffer + ctx->bufcnt, ctx->sg,\r\nctx->offset, count, 0);\r\nctx->bufcnt += count;\r\nctx->offset += count;\r\nctx->total -= count;\r\nif (ctx->offset == ctx->sg->length) {\r\nctx->sg = sg_next(ctx->sg);\r\nif (ctx->sg)\r\nctx->offset = 0;\r\nelse\r\nctx->total = 0;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void atmel_sha_fill_padding(struct atmel_sha_reqctx *ctx, int length)\r\n{\r\nunsigned int index, padlen;\r\nu64 bits[2];\r\nu64 size[2];\r\nsize[0] = ctx->digcnt[0];\r\nsize[1] = ctx->digcnt[1];\r\nsize[0] += ctx->bufcnt;\r\nif (size[0] < ctx->bufcnt)\r\nsize[1]++;\r\nsize[0] += length;\r\nif (size[0] < length)\r\nsize[1]++;\r\nbits[1] = cpu_to_be64(size[0] << 3);\r\nbits[0] = cpu_to_be64(size[1] << 3 | size[0] >> 61);\r\nswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\r\ncase SHA_FLAGS_SHA384:\r\ncase SHA_FLAGS_SHA512:\r\nindex = ctx->bufcnt & 0x7f;\r\npadlen = (index < 112) ? (112 - index) : ((128+112) - index);\r\n*(ctx->buffer + ctx->bufcnt) = 0x80;\r\nmemset(ctx->buffer + ctx->bufcnt + 1, 0, padlen-1);\r\nmemcpy(ctx->buffer + ctx->bufcnt + padlen, bits, 16);\r\nctx->bufcnt += padlen + 16;\r\nctx->flags |= SHA_FLAGS_PAD;\r\nbreak;\r\ndefault:\r\nindex = ctx->bufcnt & 0x3f;\r\npadlen = (index < 56) ? (56 - index) : ((64+56) - index);\r\n*(ctx->buffer + ctx->bufcnt) = 0x80;\r\nmemset(ctx->buffer + ctx->bufcnt + 1, 0, padlen-1);\r\nmemcpy(ctx->buffer + ctx->bufcnt + padlen, &bits[1], 8);\r\nctx->bufcnt += padlen + 8;\r\nctx->flags |= SHA_FLAGS_PAD;\r\nbreak;\r\n}\r\n}\r\nstatic struct atmel_sha_dev *atmel_sha_find_dev(struct atmel_sha_ctx *tctx)\r\n{\r\nstruct atmel_sha_dev *dd = NULL;\r\nstruct atmel_sha_dev *tmp;\r\nspin_lock_bh(&atmel_sha.lock);\r\nif (!tctx->dd) {\r\nlist_for_each_entry(tmp, &atmel_sha.dev_list, list) {\r\ndd = tmp;\r\nbreak;\r\n}\r\ntctx->dd = dd;\r\n} else {\r\ndd = tctx->dd;\r\n}\r\nspin_unlock_bh(&atmel_sha.lock);\r\nreturn dd;\r\n}\r\nstatic int atmel_sha_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_ctx *tctx = crypto_ahash_ctx(tfm);\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_dev *dd = atmel_sha_find_dev(tctx);\r\nctx->dd = dd;\r\nctx->flags = 0;\r\ndev_dbg(dd->dev, "init: digest size: %d\n",\r\ncrypto_ahash_digestsize(tfm));\r\nswitch (crypto_ahash_digestsize(tfm)) {\r\ncase SHA1_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA1;\r\nctx->block_size = SHA1_BLOCK_SIZE;\r\nbreak;\r\ncase SHA224_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA224;\r\nctx->block_size = SHA224_BLOCK_SIZE;\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA256;\r\nctx->block_size = SHA256_BLOCK_SIZE;\r\nbreak;\r\ncase SHA384_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA384;\r\nctx->block_size = SHA384_BLOCK_SIZE;\r\nbreak;\r\ncase SHA512_DIGEST_SIZE:\r\nctx->flags |= SHA_FLAGS_SHA512;\r\nctx->block_size = SHA512_BLOCK_SIZE;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\nbreak;\r\n}\r\nctx->bufcnt = 0;\r\nctx->digcnt[0] = 0;\r\nctx->digcnt[1] = 0;\r\nctx->buflen = SHA_BUFFER_LEN;\r\nreturn 0;\r\n}\r\nstatic void atmel_sha_write_ctrl(struct atmel_sha_dev *dd, int dma)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nu32 valmr = SHA_MR_MODE_AUTO;\r\nunsigned int i, hashsize = 0;\r\nif (likely(dma)) {\r\nif (!dd->caps.has_dma)\r\natmel_sha_write(dd, SHA_IER, SHA_INT_TXBUFE);\r\nvalmr = SHA_MR_MODE_PDC;\r\nif (dd->caps.has_dualbuff)\r\nvalmr |= SHA_MR_DUALBUFF;\r\n} else {\r\natmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\r\n}\r\nswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\r\ncase SHA_FLAGS_SHA1:\r\nvalmr |= SHA_MR_ALGO_SHA1;\r\nhashsize = SHA1_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA224:\r\nvalmr |= SHA_MR_ALGO_SHA224;\r\nhashsize = SHA256_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA256:\r\nvalmr |= SHA_MR_ALGO_SHA256;\r\nhashsize = SHA256_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA384:\r\nvalmr |= SHA_MR_ALGO_SHA384;\r\nhashsize = SHA512_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA512:\r\nvalmr |= SHA_MR_ALGO_SHA512;\r\nhashsize = SHA512_DIGEST_SIZE;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (!(ctx->digcnt[0] || ctx->digcnt[1])) {\r\natmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\r\n} else if (dd->caps.has_uihv && (ctx->flags & SHA_FLAGS_RESTORE)) {\r\nconst u32 *hash = (const u32 *)ctx->digest;\r\nctx->flags &= ~SHA_FLAGS_RESTORE;\r\natmel_sha_write(dd, SHA_CR, SHA_CR_WUIHV);\r\nfor (i = 0; i < hashsize / sizeof(u32); ++i)\r\natmel_sha_write(dd, SHA_REG_DIN(i), hash[i]);\r\natmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\r\nvalmr |= SHA_MR_UIHV;\r\n}\r\natmel_sha_write(dd, SHA_MR, valmr);\r\n}\r\nstatic inline int atmel_sha_wait_for_data_ready(struct atmel_sha_dev *dd,\r\natmel_sha_fn_t resume)\r\n{\r\nu32 isr = atmel_sha_read(dd, SHA_ISR);\r\nif (unlikely(isr & SHA_INT_DATARDY))\r\nreturn resume(dd);\r\ndd->resume = resume;\r\natmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int atmel_sha_xmit_cpu(struct atmel_sha_dev *dd, const u8 *buf,\r\nsize_t length, int final)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint count, len32;\r\nconst u32 *buffer = (const u32 *)buf;\r\ndev_dbg(dd->dev, "xmit_cpu: digcnt: 0x%llx 0x%llx, length: %zd, final: %d\n",\r\nctx->digcnt[1], ctx->digcnt[0], length, final);\r\natmel_sha_write_ctrl(dd, 0);\r\nctx->digcnt[0] += length;\r\nif (ctx->digcnt[0] < length)\r\nctx->digcnt[1]++;\r\nif (final)\r\ndd->flags |= SHA_FLAGS_FINAL;\r\nlen32 = DIV_ROUND_UP(length, sizeof(u32));\r\ndd->flags |= SHA_FLAGS_CPU;\r\nfor (count = 0; count < len32; count++)\r\natmel_sha_write(dd, SHA_REG_DIN(count), buffer[count]);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int atmel_sha_xmit_pdc(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\r\nsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint len32;\r\ndev_dbg(dd->dev, "xmit_pdc: digcnt: 0x%llx 0x%llx, length: %zd, final: %d\n",\r\nctx->digcnt[1], ctx->digcnt[0], length1, final);\r\nlen32 = DIV_ROUND_UP(length1, sizeof(u32));\r\natmel_sha_write(dd, SHA_PTCR, SHA_PTCR_TXTDIS);\r\natmel_sha_write(dd, SHA_TPR, dma_addr1);\r\natmel_sha_write(dd, SHA_TCR, len32);\r\nlen32 = DIV_ROUND_UP(length2, sizeof(u32));\r\natmel_sha_write(dd, SHA_TNPR, dma_addr2);\r\natmel_sha_write(dd, SHA_TNCR, len32);\r\natmel_sha_write_ctrl(dd, 1);\r\nctx->digcnt[0] += length1;\r\nif (ctx->digcnt[0] < length1)\r\nctx->digcnt[1]++;\r\nif (final)\r\ndd->flags |= SHA_FLAGS_FINAL;\r\ndd->flags |= SHA_FLAGS_DMA_ACTIVE;\r\natmel_sha_write(dd, SHA_PTCR, SHA_PTCR_TXTEN);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic void atmel_sha_dma_callback(void *data)\r\n{\r\nstruct atmel_sha_dev *dd = data;\r\ndd->is_async = true;\r\natmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\r\n}\r\nstatic int atmel_sha_xmit_dma(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\r\nsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nstruct dma_async_tx_descriptor *in_desc;\r\nstruct scatterlist sg[2];\r\ndev_dbg(dd->dev, "xmit_dma: digcnt: 0x%llx 0x%llx, length: %zd, final: %d\n",\r\nctx->digcnt[1], ctx->digcnt[0], length1, final);\r\ndd->dma_lch_in.dma_conf.src_maxburst = 16;\r\ndd->dma_lch_in.dma_conf.dst_maxburst = 16;\r\ndmaengine_slave_config(dd->dma_lch_in.chan, &dd->dma_lch_in.dma_conf);\r\nif (length2) {\r\nsg_init_table(sg, 2);\r\nsg_dma_address(&sg[0]) = dma_addr1;\r\nsg_dma_len(&sg[0]) = length1;\r\nsg_dma_address(&sg[1]) = dma_addr2;\r\nsg_dma_len(&sg[1]) = length2;\r\nin_desc = dmaengine_prep_slave_sg(dd->dma_lch_in.chan, sg, 2,\r\nDMA_MEM_TO_DEV, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\n} else {\r\nsg_init_table(sg, 1);\r\nsg_dma_address(&sg[0]) = dma_addr1;\r\nsg_dma_len(&sg[0]) = length1;\r\nin_desc = dmaengine_prep_slave_sg(dd->dma_lch_in.chan, sg, 1,\r\nDMA_MEM_TO_DEV, DMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\n}\r\nif (!in_desc)\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\nin_desc->callback = atmel_sha_dma_callback;\r\nin_desc->callback_param = dd;\r\natmel_sha_write_ctrl(dd, 1);\r\nctx->digcnt[0] += length1;\r\nif (ctx->digcnt[0] < length1)\r\nctx->digcnt[1]++;\r\nif (final)\r\ndd->flags |= SHA_FLAGS_FINAL;\r\ndd->flags |= SHA_FLAGS_DMA_ACTIVE;\r\ndmaengine_submit(in_desc);\r\ndma_async_issue_pending(dd->dma_lch_in.chan);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int atmel_sha_xmit_start(struct atmel_sha_dev *dd, dma_addr_t dma_addr1,\r\nsize_t length1, dma_addr_t dma_addr2, size_t length2, int final)\r\n{\r\nif (dd->caps.has_dma)\r\nreturn atmel_sha_xmit_dma(dd, dma_addr1, length1,\r\ndma_addr2, length2, final);\r\nelse\r\nreturn atmel_sha_xmit_pdc(dd, dma_addr1, length1,\r\ndma_addr2, length2, final);\r\n}\r\nstatic int atmel_sha_update_cpu(struct atmel_sha_dev *dd)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nint bufcnt;\r\natmel_sha_append_sg(ctx);\r\natmel_sha_fill_padding(ctx, 0);\r\nbufcnt = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn atmel_sha_xmit_cpu(dd, ctx->buffer, bufcnt, 1);\r\n}\r\nstatic int atmel_sha_xmit_dma_map(struct atmel_sha_dev *dd,\r\nstruct atmel_sha_reqctx *ctx,\r\nsize_t length, int final)\r\n{\r\nctx->dma_addr = dma_map_single(dd->dev, ctx->buffer,\r\nctx->buflen + ctx->block_size, DMA_TO_DEVICE);\r\nif (dma_mapping_error(dd->dev, ctx->dma_addr)) {\r\ndev_err(dd->dev, "dma %zu bytes error\n", ctx->buflen +\r\nctx->block_size);\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\n}\r\nctx->flags &= ~SHA_FLAGS_SG;\r\nreturn atmel_sha_xmit_start(dd, ctx->dma_addr, length, 0, 0, final);\r\n}\r\nstatic int atmel_sha_update_dma_slow(struct atmel_sha_dev *dd)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nunsigned int final;\r\nsize_t count;\r\natmel_sha_append_sg(ctx);\r\nfinal = (ctx->flags & SHA_FLAGS_FINUP) && !ctx->total;\r\ndev_dbg(dd->dev, "slow: bufcnt: %zu, digcnt: 0x%llx 0x%llx, final: %d\n",\r\nctx->bufcnt, ctx->digcnt[1], ctx->digcnt[0], final);\r\nif (final)\r\natmel_sha_fill_padding(ctx, 0);\r\nif (final || (ctx->bufcnt == ctx->buflen)) {\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn atmel_sha_xmit_dma_map(dd, ctx, count, final);\r\n}\r\nreturn 0;\r\n}\r\nstatic int atmel_sha_update_dma_start(struct atmel_sha_dev *dd)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nunsigned int length, final, tail;\r\nstruct scatterlist *sg;\r\nunsigned int count;\r\nif (!ctx->total)\r\nreturn 0;\r\nif (ctx->bufcnt || ctx->offset)\r\nreturn atmel_sha_update_dma_slow(dd);\r\ndev_dbg(dd->dev, "fast: digcnt: 0x%llx 0x%llx, bufcnt: %zd, total: %u\n",\r\nctx->digcnt[1], ctx->digcnt[0], ctx->bufcnt, ctx->total);\r\nsg = ctx->sg;\r\nif (!IS_ALIGNED(sg->offset, sizeof(u32)))\r\nreturn atmel_sha_update_dma_slow(dd);\r\nif (!sg_is_last(sg) && !IS_ALIGNED(sg->length, ctx->block_size))\r\nreturn atmel_sha_update_dma_slow(dd);\r\nlength = min(ctx->total, sg->length);\r\nif (sg_is_last(sg)) {\r\nif (!(ctx->flags & SHA_FLAGS_FINUP)) {\r\ntail = length & (ctx->block_size - 1);\r\nlength -= tail;\r\n}\r\n}\r\nctx->total -= length;\r\nctx->offset = length;\r\nfinal = (ctx->flags & SHA_FLAGS_FINUP) && !ctx->total;\r\nif (final) {\r\ntail = length & (ctx->block_size - 1);\r\nlength -= tail;\r\nctx->total += tail;\r\nctx->offset = length;\r\nsg = ctx->sg;\r\natmel_sha_append_sg(ctx);\r\natmel_sha_fill_padding(ctx, length);\r\nctx->dma_addr = dma_map_single(dd->dev, ctx->buffer,\r\nctx->buflen + ctx->block_size, DMA_TO_DEVICE);\r\nif (dma_mapping_error(dd->dev, ctx->dma_addr)) {\r\ndev_err(dd->dev, "dma %zu bytes error\n",\r\nctx->buflen + ctx->block_size);\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\n}\r\nif (length == 0) {\r\nctx->flags &= ~SHA_FLAGS_SG;\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn atmel_sha_xmit_start(dd, ctx->dma_addr, count, 0,\r\n0, final);\r\n} else {\r\nctx->sg = sg;\r\nif (!dma_map_sg(dd->dev, ctx->sg, 1,\r\nDMA_TO_DEVICE)) {\r\ndev_err(dd->dev, "dma_map_sg error\n");\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\n}\r\nctx->flags |= SHA_FLAGS_SG;\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nreturn atmel_sha_xmit_start(dd, sg_dma_address(ctx->sg),\r\nlength, ctx->dma_addr, count, final);\r\n}\r\n}\r\nif (!dma_map_sg(dd->dev, ctx->sg, 1, DMA_TO_DEVICE)) {\r\ndev_err(dd->dev, "dma_map_sg error\n");\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\n}\r\nctx->flags |= SHA_FLAGS_SG;\r\nreturn atmel_sha_xmit_start(dd, sg_dma_address(ctx->sg), length, 0,\r\n0, final);\r\n}\r\nstatic int atmel_sha_update_dma_stop(struct atmel_sha_dev *dd)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(dd->req);\r\nif (ctx->flags & SHA_FLAGS_SG) {\r\ndma_unmap_sg(dd->dev, ctx->sg, 1, DMA_TO_DEVICE);\r\nif (ctx->sg->length == ctx->offset) {\r\nctx->sg = sg_next(ctx->sg);\r\nif (ctx->sg)\r\nctx->offset = 0;\r\n}\r\nif (ctx->flags & SHA_FLAGS_PAD) {\r\ndma_unmap_single(dd->dev, ctx->dma_addr,\r\nctx->buflen + ctx->block_size, DMA_TO_DEVICE);\r\n}\r\n} else {\r\ndma_unmap_single(dd->dev, ctx->dma_addr, ctx->buflen +\r\nctx->block_size, DMA_TO_DEVICE);\r\n}\r\nreturn 0;\r\n}\r\nstatic int atmel_sha_update_req(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err;\r\ndev_dbg(dd->dev, "update_req: total: %u, digcnt: 0x%llx 0x%llx\n",\r\nctx->total, ctx->digcnt[1], ctx->digcnt[0]);\r\nif (ctx->flags & SHA_FLAGS_CPU)\r\nerr = atmel_sha_update_cpu(dd);\r\nelse\r\nerr = atmel_sha_update_dma_start(dd);\r\ndev_dbg(dd->dev, "update: err: %d, digcnt: 0x%llx 0%llx\n",\r\nerr, ctx->digcnt[1], ctx->digcnt[0]);\r\nreturn err;\r\n}\r\nstatic int atmel_sha_final_req(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err = 0;\r\nint count;\r\nif (ctx->bufcnt >= ATMEL_SHA_DMA_THRESHOLD) {\r\natmel_sha_fill_padding(ctx, 0);\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nerr = atmel_sha_xmit_dma_map(dd, ctx, count, 1);\r\n}\r\nelse {\r\natmel_sha_fill_padding(ctx, 0);\r\ncount = ctx->bufcnt;\r\nctx->bufcnt = 0;\r\nerr = atmel_sha_xmit_cpu(dd, ctx->buffer, count, 1);\r\n}\r\ndev_dbg(dd->dev, "final_req: err: %d\n", err);\r\nreturn err;\r\n}\r\nstatic void atmel_sha_copy_hash(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nu32 *hash = (u32 *)ctx->digest;\r\nunsigned int i, hashsize;\r\nswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\r\ncase SHA_FLAGS_SHA1:\r\nhashsize = SHA1_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA224:\r\ncase SHA_FLAGS_SHA256:\r\nhashsize = SHA256_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA384:\r\ncase SHA_FLAGS_SHA512:\r\nhashsize = SHA512_DIGEST_SIZE;\r\nbreak;\r\ndefault:\r\nreturn;\r\n}\r\nfor (i = 0; i < hashsize / sizeof(u32); ++i)\r\nhash[i] = atmel_sha_read(ctx->dd, SHA_REG_DIGEST(i));\r\nctx->flags |= SHA_FLAGS_RESTORE;\r\n}\r\nstatic void atmel_sha_copy_ready_hash(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nif (!req->result)\r\nreturn;\r\nswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\r\ndefault:\r\ncase SHA_FLAGS_SHA1:\r\nmemcpy(req->result, ctx->digest, SHA1_DIGEST_SIZE);\r\nbreak;\r\ncase SHA_FLAGS_SHA224:\r\nmemcpy(req->result, ctx->digest, SHA224_DIGEST_SIZE);\r\nbreak;\r\ncase SHA_FLAGS_SHA256:\r\nmemcpy(req->result, ctx->digest, SHA256_DIGEST_SIZE);\r\nbreak;\r\ncase SHA_FLAGS_SHA384:\r\nmemcpy(req->result, ctx->digest, SHA384_DIGEST_SIZE);\r\nbreak;\r\ncase SHA_FLAGS_SHA512:\r\nmemcpy(req->result, ctx->digest, SHA512_DIGEST_SIZE);\r\nbreak;\r\n}\r\n}\r\nstatic int atmel_sha_finish(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_dev *dd = ctx->dd;\r\nif (ctx->digcnt[0] || ctx->digcnt[1])\r\natmel_sha_copy_ready_hash(req);\r\ndev_dbg(dd->dev, "digcnt: 0x%llx 0x%llx, bufcnt: %zd\n", ctx->digcnt[1],\r\nctx->digcnt[0], ctx->bufcnt);\r\nreturn 0;\r\n}\r\nstatic void atmel_sha_finish_req(struct ahash_request *req, int err)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_dev *dd = ctx->dd;\r\nif (!err) {\r\natmel_sha_copy_hash(req);\r\nif (SHA_FLAGS_FINAL & dd->flags)\r\nerr = atmel_sha_finish(req);\r\n} else {\r\nctx->flags |= SHA_FLAGS_ERROR;\r\n}\r\n(void)atmel_sha_complete(dd, err);\r\n}\r\nstatic int atmel_sha_hw_init(struct atmel_sha_dev *dd)\r\n{\r\nint err;\r\nerr = clk_enable(dd->iclk);\r\nif (err)\r\nreturn err;\r\nif (!(SHA_FLAGS_INIT & dd->flags)) {\r\natmel_sha_write(dd, SHA_CR, SHA_CR_SWRST);\r\ndd->flags |= SHA_FLAGS_INIT;\r\ndd->err = 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic inline unsigned int atmel_sha_get_version(struct atmel_sha_dev *dd)\r\n{\r\nreturn atmel_sha_read(dd, SHA_HW_VERSION) & 0x00000fff;\r\n}\r\nstatic void atmel_sha_hw_version_init(struct atmel_sha_dev *dd)\r\n{\r\natmel_sha_hw_init(dd);\r\ndd->hw_version = atmel_sha_get_version(dd);\r\ndev_info(dd->dev,\r\n"version: 0x%x\n", dd->hw_version);\r\nclk_disable(dd->iclk);\r\n}\r\nstatic int atmel_sha_handle_queue(struct atmel_sha_dev *dd,\r\nstruct ahash_request *req)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct atmel_sha_ctx *ctx;\r\nunsigned long flags;\r\nbool start_async;\r\nint err = 0, ret = 0;\r\nspin_lock_irqsave(&dd->lock, flags);\r\nif (req)\r\nret = ahash_enqueue_request(&dd->queue, req);\r\nif (SHA_FLAGS_BUSY & dd->flags) {\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nreturn ret;\r\n}\r\nbacklog = crypto_get_backlog(&dd->queue);\r\nasync_req = crypto_dequeue_request(&dd->queue);\r\nif (async_req)\r\ndd->flags |= SHA_FLAGS_BUSY;\r\nspin_unlock_irqrestore(&dd->lock, flags);\r\nif (!async_req)\r\nreturn ret;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nctx = crypto_tfm_ctx(async_req->tfm);\r\ndd->req = ahash_request_cast(async_req);\r\nstart_async = (dd->req != req);\r\ndd->is_async = start_async;\r\ndd->force_complete = false;\r\nerr = ctx->start(dd);\r\nreturn (start_async) ? ret : err;\r\n}\r\nstatic int atmel_sha_start(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err;\r\ndev_dbg(dd->dev, "handling new req, op: %lu, nbytes: %d\n",\r\nctx->op, req->nbytes);\r\nerr = atmel_sha_hw_init(dd);\r\nif (err)\r\nreturn atmel_sha_complete(dd, err);\r\ndd->resume = atmel_sha_done;\r\nif (ctx->op == SHA_OP_UPDATE) {\r\nerr = atmel_sha_update_req(dd);\r\nif (!err && (ctx->flags & SHA_FLAGS_FINUP))\r\nerr = atmel_sha_final_req(dd);\r\n} else if (ctx->op == SHA_OP_FINAL) {\r\nerr = atmel_sha_final_req(dd);\r\n}\r\nif (!err)\r\natmel_sha_finish_req(req, err);\r\ndev_dbg(dd->dev, "exit, err: %d\n", err);\r\nreturn err;\r\n}\r\nstatic int atmel_sha_enqueue(struct ahash_request *req, unsigned int op)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct atmel_sha_ctx *tctx = crypto_tfm_ctx(req->base.tfm);\r\nstruct atmel_sha_dev *dd = tctx->dd;\r\nctx->op = op;\r\nreturn atmel_sha_handle_queue(dd, req);\r\n}\r\nstatic int atmel_sha_update(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nif (!req->nbytes)\r\nreturn 0;\r\nctx->total = req->nbytes;\r\nctx->sg = req->src;\r\nctx->offset = 0;\r\nif (ctx->flags & SHA_FLAGS_FINUP) {\r\nif (ctx->bufcnt + ctx->total < ATMEL_SHA_DMA_THRESHOLD)\r\nctx->flags |= SHA_FLAGS_CPU;\r\n} else if (ctx->bufcnt + ctx->total < ctx->buflen) {\r\natmel_sha_append_sg(ctx);\r\nreturn 0;\r\n}\r\nreturn atmel_sha_enqueue(req, SHA_OP_UPDATE);\r\n}\r\nstatic int atmel_sha_final(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nctx->flags |= SHA_FLAGS_FINUP;\r\nif (ctx->flags & SHA_FLAGS_ERROR)\r\nreturn 0;\r\nif (ctx->flags & SHA_FLAGS_PAD)\r\nreturn atmel_sha_finish(req);\r\nreturn atmel_sha_enqueue(req, SHA_OP_FINAL);\r\n}\r\nstatic int atmel_sha_finup(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err1, err2;\r\nctx->flags |= SHA_FLAGS_FINUP;\r\nerr1 = atmel_sha_update(req);\r\nif (err1 == -EINPROGRESS || err1 == -EBUSY)\r\nreturn err1;\r\nerr2 = atmel_sha_final(req);\r\nreturn err1 ?: err2;\r\n}\r\nstatic int atmel_sha_digest(struct ahash_request *req)\r\n{\r\nreturn atmel_sha_init(req) ?: atmel_sha_finup(req);\r\n}\r\nstatic int atmel_sha_export(struct ahash_request *req, void *out)\r\n{\r\nconst struct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nmemcpy(out, ctx, sizeof(*ctx));\r\nreturn 0;\r\n}\r\nstatic int atmel_sha_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nmemcpy(ctx, in, sizeof(*ctx));\r\nreturn 0;\r\n}\r\nstatic int atmel_sha_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct atmel_sha_ctx *ctx = crypto_tfm_ctx(tfm);\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct atmel_sha_reqctx));\r\nctx->start = atmel_sha_start;\r\nreturn 0;\r\n}\r\nstatic void atmel_sha_queue_task(unsigned long data)\r\n{\r\nstruct atmel_sha_dev *dd = (struct atmel_sha_dev *)data;\r\natmel_sha_handle_queue(dd, NULL);\r\n}\r\nstatic int atmel_sha_done(struct atmel_sha_dev *dd)\r\n{\r\nint err = 0;\r\nif (SHA_FLAGS_CPU & dd->flags) {\r\nif (SHA_FLAGS_OUTPUT_READY & dd->flags) {\r\ndd->flags &= ~SHA_FLAGS_OUTPUT_READY;\r\ngoto finish;\r\n}\r\n} else if (SHA_FLAGS_DMA_READY & dd->flags) {\r\nif (SHA_FLAGS_DMA_ACTIVE & dd->flags) {\r\ndd->flags &= ~SHA_FLAGS_DMA_ACTIVE;\r\natmel_sha_update_dma_stop(dd);\r\nif (dd->err) {\r\nerr = dd->err;\r\ngoto finish;\r\n}\r\n}\r\nif (SHA_FLAGS_OUTPUT_READY & dd->flags) {\r\ndd->flags &= ~(SHA_FLAGS_DMA_READY |\r\nSHA_FLAGS_OUTPUT_READY);\r\nerr = atmel_sha_update_dma_start(dd);\r\nif (err != -EINPROGRESS)\r\ngoto finish;\r\n}\r\n}\r\nreturn err;\r\nfinish:\r\natmel_sha_finish_req(dd->req, err);\r\nreturn err;\r\n}\r\nstatic void atmel_sha_done_task(unsigned long data)\r\n{\r\nstruct atmel_sha_dev *dd = (struct atmel_sha_dev *)data;\r\ndd->is_async = true;\r\n(void)dd->resume(dd);\r\n}\r\nstatic irqreturn_t atmel_sha_irq(int irq, void *dev_id)\r\n{\r\nstruct atmel_sha_dev *sha_dd = dev_id;\r\nu32 reg;\r\nreg = atmel_sha_read(sha_dd, SHA_ISR);\r\nif (reg & atmel_sha_read(sha_dd, SHA_IMR)) {\r\natmel_sha_write(sha_dd, SHA_IDR, reg);\r\nif (SHA_FLAGS_BUSY & sha_dd->flags) {\r\nsha_dd->flags |= SHA_FLAGS_OUTPUT_READY;\r\nif (!(SHA_FLAGS_CPU & sha_dd->flags))\r\nsha_dd->flags |= SHA_FLAGS_DMA_READY;\r\ntasklet_schedule(&sha_dd->done_task);\r\n} else {\r\ndev_warn(sha_dd->dev, "SHA interrupt when no active requests.\n");\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nreturn IRQ_NONE;\r\n}\r\nstatic bool atmel_sha_dma_check_aligned(struct atmel_sha_dev *dd,\r\nstruct scatterlist *sg,\r\nsize_t len)\r\n{\r\nstruct atmel_sha_dma *dma = &dd->dma_lch_in;\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nsize_t bs = ctx->block_size;\r\nint nents;\r\nfor (nents = 0; sg; sg = sg_next(sg), ++nents) {\r\nif (!IS_ALIGNED(sg->offset, sizeof(u32)))\r\nreturn false;\r\nif (len <= sg->length) {\r\ndma->nents = nents + 1;\r\ndma->last_sg_length = sg->length;\r\nsg->length = ALIGN(len, sizeof(u32));\r\nreturn true;\r\n}\r\nif (!IS_ALIGNED(sg->length, bs))\r\nreturn false;\r\nlen -= sg->length;\r\n}\r\nreturn false;\r\n}\r\nstatic void atmel_sha_dma_callback2(void *data)\r\n{\r\nstruct atmel_sha_dev *dd = data;\r\nstruct atmel_sha_dma *dma = &dd->dma_lch_in;\r\nstruct scatterlist *sg;\r\nint nents;\r\ndmaengine_terminate_all(dma->chan);\r\ndma_unmap_sg(dd->dev, dma->sg, dma->nents, DMA_TO_DEVICE);\r\nsg = dma->sg;\r\nfor (nents = 0; nents < dma->nents - 1; ++nents)\r\nsg = sg_next(sg);\r\nsg->length = dma->last_sg_length;\r\ndd->is_async = true;\r\n(void)atmel_sha_wait_for_data_ready(dd, dd->resume);\r\n}\r\nstatic int atmel_sha_dma_start(struct atmel_sha_dev *dd,\r\nstruct scatterlist *src,\r\nsize_t len,\r\natmel_sha_fn_t resume)\r\n{\r\nstruct atmel_sha_dma *dma = &dd->dma_lch_in;\r\nstruct dma_slave_config *config = &dma->dma_conf;\r\nstruct dma_chan *chan = dma->chan;\r\nstruct dma_async_tx_descriptor *desc;\r\ndma_cookie_t cookie;\r\nunsigned int sg_len;\r\nint err;\r\ndd->resume = resume;\r\ndma->sg = src;\r\nsg_len = dma_map_sg(dd->dev, dma->sg, dma->nents, DMA_TO_DEVICE);\r\nif (!sg_len) {\r\nerr = -ENOMEM;\r\ngoto exit;\r\n}\r\nconfig->src_maxburst = 16;\r\nconfig->dst_maxburst = 16;\r\nerr = dmaengine_slave_config(chan, config);\r\nif (err)\r\ngoto unmap_sg;\r\ndesc = dmaengine_prep_slave_sg(chan, dma->sg, sg_len, DMA_MEM_TO_DEV,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!desc) {\r\nerr = -ENOMEM;\r\ngoto unmap_sg;\r\n}\r\ndesc->callback = atmel_sha_dma_callback2;\r\ndesc->callback_param = dd;\r\ncookie = dmaengine_submit(desc);\r\nerr = dma_submit_error(cookie);\r\nif (err)\r\ngoto unmap_sg;\r\ndma_async_issue_pending(chan);\r\nreturn -EINPROGRESS;\r\nunmap_sg:\r\ndma_unmap_sg(dd->dev, dma->sg, dma->nents, DMA_TO_DEVICE);\r\nexit:\r\nreturn atmel_sha_complete(dd, err);\r\n}\r\nstatic int atmel_sha_cpu_transfer(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nconst u32 *words = (const u32 *)ctx->buffer;\r\nsize_t i, num_words;\r\nu32 isr, din, din_inc;\r\ndin_inc = (ctx->flags & SHA_FLAGS_IDATAR0) ? 0 : 1;\r\nfor (;;) {\r\nnum_words = DIV_ROUND_UP(ctx->bufcnt, sizeof(u32));\r\nfor (i = 0, din = 0; i < num_words; ++i, din += din_inc)\r\natmel_sha_write(dd, SHA_REG_DIN(din), words[i]);\r\nctx->offset += ctx->bufcnt;\r\nctx->total -= ctx->bufcnt;\r\nif (!ctx->total)\r\nbreak;\r\nctx->bufcnt = min_t(size_t, ctx->block_size, ctx->total);\r\nscatterwalk_map_and_copy(ctx->buffer, ctx->sg,\r\nctx->offset, ctx->bufcnt, 0);\r\nisr = atmel_sha_read(dd, SHA_ISR);\r\nif (!(isr & SHA_INT_DATARDY)) {\r\ndd->resume = atmel_sha_cpu_transfer;\r\natmel_sha_write(dd, SHA_IER, SHA_INT_DATARDY);\r\nreturn -EINPROGRESS;\r\n}\r\n}\r\nif (unlikely(!(ctx->flags & SHA_FLAGS_WAIT_DATARDY)))\r\nreturn dd->cpu_transfer_complete(dd);\r\nreturn atmel_sha_wait_for_data_ready(dd, dd->cpu_transfer_complete);\r\n}\r\nstatic int atmel_sha_cpu_start(struct atmel_sha_dev *dd,\r\nstruct scatterlist *sg,\r\nunsigned int len,\r\nbool idatar0_only,\r\nbool wait_data_ready,\r\natmel_sha_fn_t resume)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nif (!len)\r\nreturn resume(dd);\r\nctx->flags &= ~(SHA_FLAGS_IDATAR0 | SHA_FLAGS_WAIT_DATARDY);\r\nif (idatar0_only)\r\nctx->flags |= SHA_FLAGS_IDATAR0;\r\nif (wait_data_ready)\r\nctx->flags |= SHA_FLAGS_WAIT_DATARDY;\r\nctx->sg = sg;\r\nctx->total = len;\r\nctx->offset = 0;\r\nctx->bufcnt = min_t(size_t, ctx->block_size, ctx->total);\r\nscatterwalk_map_and_copy(ctx->buffer, ctx->sg,\r\nctx->offset, ctx->bufcnt, 0);\r\ndd->cpu_transfer_complete = resume;\r\nreturn atmel_sha_cpu_transfer(dd);\r\n}\r\nstatic int atmel_sha_cpu_hash(struct atmel_sha_dev *dd,\r\nconst void *data, unsigned int datalen,\r\nbool auto_padding,\r\natmel_sha_fn_t resume)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nu32 msglen = (auto_padding) ? datalen : 0;\r\nu32 mr = SHA_MR_MODE_AUTO;\r\nif (!(IS_ALIGNED(datalen, ctx->block_size) || auto_padding))\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\nmr |= (ctx->flags & SHA_FLAGS_ALGO_MASK);\r\natmel_sha_write(dd, SHA_MR, mr);\r\natmel_sha_write(dd, SHA_MSR, msglen);\r\natmel_sha_write(dd, SHA_BCR, msglen);\r\natmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\r\nsg_init_one(&dd->tmp, data, datalen);\r\nreturn atmel_sha_cpu_start(dd, &dd->tmp, datalen, false, true, resume);\r\n}\r\nstatic inline void atmel_sha_hmac_key_init(struct atmel_sha_hmac_key *hkey)\r\n{\r\nmemset(hkey, 0, sizeof(*hkey));\r\n}\r\nstatic inline void atmel_sha_hmac_key_release(struct atmel_sha_hmac_key *hkey)\r\n{\r\nkfree(hkey->keydup);\r\nmemset(hkey, 0, sizeof(*hkey));\r\n}\r\nstatic inline int atmel_sha_hmac_key_set(struct atmel_sha_hmac_key *hkey,\r\nconst u8 *key,\r\nunsigned int keylen)\r\n{\r\natmel_sha_hmac_key_release(hkey);\r\nif (keylen > sizeof(hkey->buffer)) {\r\nhkey->keydup = kmemdup(key, keylen, GFP_KERNEL);\r\nif (!hkey->keydup)\r\nreturn -ENOMEM;\r\n} else {\r\nmemcpy(hkey->buffer, key, keylen);\r\n}\r\nhkey->valid = true;\r\nhkey->keylen = keylen;\r\nreturn 0;\r\n}\r\nstatic inline bool atmel_sha_hmac_key_get(const struct atmel_sha_hmac_key *hkey,\r\nconst u8 **key,\r\nunsigned int *keylen)\r\n{\r\nif (!hkey->valid)\r\nreturn false;\r\n*keylen = hkey->keylen;\r\n*key = (hkey->keydup) ? hkey->keydup : hkey->buffer;\r\nreturn true;\r\n}\r\nstatic int atmel_sha_hmac_setup(struct atmel_sha_dev *dd,\r\natmel_sha_fn_t resume)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nunsigned int keylen;\r\nconst u8 *key;\r\nsize_t bs;\r\nhmac->resume = resume;\r\nswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\r\ncase SHA_FLAGS_SHA1:\r\nctx->block_size = SHA1_BLOCK_SIZE;\r\nctx->hash_size = SHA1_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA224:\r\nctx->block_size = SHA224_BLOCK_SIZE;\r\nctx->hash_size = SHA256_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA256:\r\nctx->block_size = SHA256_BLOCK_SIZE;\r\nctx->hash_size = SHA256_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA384:\r\nctx->block_size = SHA384_BLOCK_SIZE;\r\nctx->hash_size = SHA512_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA512:\r\nctx->block_size = SHA512_BLOCK_SIZE;\r\nctx->hash_size = SHA512_DIGEST_SIZE;\r\nbreak;\r\ndefault:\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\n}\r\nbs = ctx->block_size;\r\nif (likely(!atmel_sha_hmac_key_get(&hmac->hkey, &key, &keylen)))\r\nreturn resume(dd);\r\nif (unlikely(keylen > bs))\r\nreturn atmel_sha_hmac_prehash_key(dd, key, keylen);\r\nmemcpy((u8 *)hmac->ipad, key, keylen);\r\nmemset((u8 *)hmac->ipad + keylen, 0, bs - keylen);\r\nreturn atmel_sha_hmac_compute_ipad_hash(dd);\r\n}\r\nstatic int atmel_sha_hmac_prehash_key(struct atmel_sha_dev *dd,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nreturn atmel_sha_cpu_hash(dd, key, keylen, true,\r\natmel_sha_hmac_prehash_key_done);\r\n}\r\nstatic int atmel_sha_hmac_prehash_key_done(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nsize_t ds = crypto_ahash_digestsize(tfm);\r\nsize_t bs = ctx->block_size;\r\nsize_t i, num_words = ds / sizeof(u32);\r\nfor (i = 0; i < num_words; ++i)\r\nhmac->ipad[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\r\nmemset((u8 *)hmac->ipad + ds, 0, bs - ds);\r\nreturn atmel_sha_hmac_compute_ipad_hash(dd);\r\n}\r\nstatic int atmel_sha_hmac_compute_ipad_hash(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nsize_t bs = ctx->block_size;\r\nsize_t i, num_words = bs / sizeof(u32);\r\nmemcpy(hmac->opad, hmac->ipad, bs);\r\nfor (i = 0; i < num_words; ++i) {\r\nhmac->ipad[i] ^= 0x36363636;\r\nhmac->opad[i] ^= 0x5c5c5c5c;\r\n}\r\nreturn atmel_sha_cpu_hash(dd, hmac->ipad, bs, false,\r\natmel_sha_hmac_compute_opad_hash);\r\n}\r\nstatic int atmel_sha_hmac_compute_opad_hash(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nsize_t bs = ctx->block_size;\r\nsize_t hs = ctx->hash_size;\r\nsize_t i, num_words = hs / sizeof(u32);\r\nfor (i = 0; i < num_words; ++i)\r\nhmac->ipad[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\r\nreturn atmel_sha_cpu_hash(dd, hmac->opad, bs, false,\r\natmel_sha_hmac_setup_done);\r\n}\r\nstatic int atmel_sha_hmac_setup_done(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nsize_t hs = ctx->hash_size;\r\nsize_t i, num_words = hs / sizeof(u32);\r\nfor (i = 0; i < num_words; ++i)\r\nhmac->opad[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\r\natmel_sha_hmac_key_release(&hmac->hkey);\r\nreturn hmac->resume(dd);\r\n}\r\nstatic int atmel_sha_hmac_start(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nint err;\r\nerr = atmel_sha_hw_init(dd);\r\nif (err)\r\nreturn atmel_sha_complete(dd, err);\r\nswitch (ctx->op) {\r\ncase SHA_OP_INIT:\r\nerr = atmel_sha_hmac_setup(dd, atmel_sha_hmac_init_done);\r\nbreak;\r\ncase SHA_OP_UPDATE:\r\ndd->resume = atmel_sha_done;\r\nerr = atmel_sha_update_req(dd);\r\nbreak;\r\ncase SHA_OP_FINAL:\r\ndd->resume = atmel_sha_hmac_final;\r\nerr = atmel_sha_final_req(dd);\r\nbreak;\r\ncase SHA_OP_DIGEST:\r\nerr = atmel_sha_hmac_setup(dd, atmel_sha_hmac_digest2);\r\nbreak;\r\ndefault:\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\n}\r\nreturn err;\r\n}\r\nstatic int atmel_sha_hmac_setkey(struct crypto_ahash *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nif (atmel_sha_hmac_key_set(&hmac->hkey, key, keylen)) {\r\ncrypto_ahash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int atmel_sha_hmac_init(struct ahash_request *req)\r\n{\r\nint err;\r\nerr = atmel_sha_init(req);\r\nif (err)\r\nreturn err;\r\nreturn atmel_sha_enqueue(req, SHA_OP_INIT);\r\n}\r\nstatic int atmel_sha_hmac_init_done(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nsize_t bs = ctx->block_size;\r\nsize_t hs = ctx->hash_size;\r\nctx->bufcnt = 0;\r\nctx->digcnt[0] = bs;\r\nctx->digcnt[1] = 0;\r\nctx->flags |= SHA_FLAGS_RESTORE;\r\nmemcpy(ctx->digest, hmac->ipad, hs);\r\nreturn atmel_sha_complete(dd, 0);\r\n}\r\nstatic int atmel_sha_hmac_final(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nu32 *digest = (u32 *)ctx->digest;\r\nsize_t ds = crypto_ahash_digestsize(tfm);\r\nsize_t bs = ctx->block_size;\r\nsize_t hs = ctx->hash_size;\r\nsize_t i, num_words;\r\nu32 mr;\r\nnum_words = ds / sizeof(u32);\r\nfor (i = 0; i < num_words; ++i)\r\ndigest[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\r\natmel_sha_write(dd, SHA_CR, SHA_CR_WUIHV);\r\nnum_words = hs / sizeof(u32);\r\nfor (i = 0; i < num_words; ++i)\r\natmel_sha_write(dd, SHA_REG_DIN(i), hmac->opad[i]);\r\nmr = SHA_MR_MODE_AUTO | SHA_MR_UIHV;\r\nmr |= (ctx->flags & SHA_FLAGS_ALGO_MASK);\r\natmel_sha_write(dd, SHA_MR, mr);\r\natmel_sha_write(dd, SHA_MSR, bs + ds);\r\natmel_sha_write(dd, SHA_BCR, ds);\r\natmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\r\nsg_init_one(&dd->tmp, digest, ds);\r\nreturn atmel_sha_cpu_start(dd, &dd->tmp, ds, false, true,\r\natmel_sha_hmac_final_done);\r\n}\r\nstatic int atmel_sha_hmac_final_done(struct atmel_sha_dev *dd)\r\n{\r\natmel_sha_copy_hash(dd->req);\r\natmel_sha_copy_ready_hash(dd->req);\r\nreturn atmel_sha_complete(dd, 0);\r\n}\r\nstatic int atmel_sha_hmac_digest(struct ahash_request *req)\r\n{\r\nint err;\r\nerr = atmel_sha_init(req);\r\nif (err)\r\nreturn err;\r\nreturn atmel_sha_enqueue(req, SHA_OP_DIGEST);\r\n}\r\nstatic int atmel_sha_hmac_digest2(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_reqctx *ctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nsize_t hs = ctx->hash_size;\r\nsize_t i, num_words = hs / sizeof(u32);\r\nbool use_dma = false;\r\nu32 mr;\r\nif (!req->nbytes)\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\nif (req->nbytes > ATMEL_SHA_DMA_THRESHOLD &&\r\natmel_sha_dma_check_aligned(dd, req->src, req->nbytes))\r\nuse_dma = true;\r\natmel_sha_write(dd, SHA_CR, SHA_CR_WUIHV);\r\nfor (i = 0; i < num_words; ++i)\r\natmel_sha_write(dd, SHA_REG_DIN(i), hmac->ipad[i]);\r\natmel_sha_write(dd, SHA_CR, SHA_CR_WUIEHV);\r\nfor (i = 0; i < num_words; ++i)\r\natmel_sha_write(dd, SHA_REG_DIN(i), hmac->opad[i]);\r\nmr = (SHA_MR_HMAC | SHA_MR_DUALBUFF);\r\nmr |= ctx->flags & SHA_FLAGS_ALGO_MASK;\r\nif (use_dma)\r\nmr |= SHA_MR_MODE_IDATAR0;\r\nelse\r\nmr |= SHA_MR_MODE_AUTO;\r\natmel_sha_write(dd, SHA_MR, mr);\r\natmel_sha_write(dd, SHA_MSR, req->nbytes);\r\natmel_sha_write(dd, SHA_BCR, req->nbytes);\r\natmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\r\nif (use_dma)\r\nreturn atmel_sha_dma_start(dd, req->src, req->nbytes,\r\natmel_sha_hmac_final_done);\r\nreturn atmel_sha_cpu_start(dd, req->src, req->nbytes, false, true,\r\natmel_sha_hmac_final_done);\r\n}\r\nstatic int atmel_sha_hmac_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_tfm_ctx(tfm);\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct atmel_sha_reqctx));\r\nhmac->base.start = atmel_sha_hmac_start;\r\natmel_sha_hmac_key_init(&hmac->hkey);\r\nreturn 0;\r\n}\r\nstatic void atmel_sha_hmac_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_tfm_ctx(tfm);\r\natmel_sha_hmac_key_release(&hmac->hkey);\r\n}\r\nstatic void atmel_sha_authenc_complete(struct crypto_async_request *areq,\r\nint err)\r\n{\r\nstruct ahash_request *req = areq->data;\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nauthctx->cb(authctx->aes_dev, err, authctx->base.dd->is_async);\r\n}\r\nstatic int atmel_sha_authenc_start(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nint err;\r\ndd->force_complete = true;\r\nerr = atmel_sha_hw_init(dd);\r\nreturn authctx->cb(authctx->aes_dev, err, dd->is_async);\r\n}\r\nbool atmel_sha_authenc_is_ready(void)\r\n{\r\nstruct atmel_sha_ctx dummy;\r\ndummy.dd = NULL;\r\nreturn (atmel_sha_find_dev(&dummy) != NULL);\r\n}\r\nunsigned int atmel_sha_authenc_get_reqsize(void)\r\n{\r\nreturn sizeof(struct atmel_sha_authenc_reqctx);\r\n}\r\nstruct atmel_sha_authenc_ctx *atmel_sha_authenc_spawn(unsigned long mode)\r\n{\r\nstruct atmel_sha_authenc_ctx *auth;\r\nstruct crypto_ahash *tfm;\r\nstruct atmel_sha_ctx *tctx;\r\nconst char *name;\r\nint err = -EINVAL;\r\nswitch (mode & SHA_FLAGS_MODE_MASK) {\r\ncase SHA_FLAGS_HMAC_SHA1:\r\nname = "atmel-hmac-sha1";\r\nbreak;\r\ncase SHA_FLAGS_HMAC_SHA224:\r\nname = "atmel-hmac-sha224";\r\nbreak;\r\ncase SHA_FLAGS_HMAC_SHA256:\r\nname = "atmel-hmac-sha256";\r\nbreak;\r\ncase SHA_FLAGS_HMAC_SHA384:\r\nname = "atmel-hmac-sha384";\r\nbreak;\r\ncase SHA_FLAGS_HMAC_SHA512:\r\nname = "atmel-hmac-sha512";\r\nbreak;\r\ndefault:\r\ngoto error;\r\n}\r\ntfm = crypto_alloc_ahash(name,\r\nCRYPTO_ALG_TYPE_AHASH,\r\nCRYPTO_ALG_TYPE_AHASH_MASK);\r\nif (IS_ERR(tfm)) {\r\nerr = PTR_ERR(tfm);\r\ngoto error;\r\n}\r\ntctx = crypto_ahash_ctx(tfm);\r\ntctx->start = atmel_sha_authenc_start;\r\ntctx->flags = mode;\r\nauth = kzalloc(sizeof(*auth), GFP_KERNEL);\r\nif (!auth) {\r\nerr = -ENOMEM;\r\ngoto err_free_ahash;\r\n}\r\nauth->tfm = tfm;\r\nreturn auth;\r\nerr_free_ahash:\r\ncrypto_free_ahash(tfm);\r\nerror:\r\nreturn ERR_PTR(err);\r\n}\r\nvoid atmel_sha_authenc_free(struct atmel_sha_authenc_ctx *auth)\r\n{\r\nif (auth)\r\ncrypto_free_ahash(auth->tfm);\r\nkfree(auth);\r\n}\r\nint atmel_sha_authenc_setkey(struct atmel_sha_authenc_ctx *auth,\r\nconst u8 *key, unsigned int keylen,\r\nu32 *flags)\r\n{\r\nstruct crypto_ahash *tfm = auth->tfm;\r\nint err;\r\ncrypto_ahash_clear_flags(tfm, CRYPTO_TFM_REQ_MASK);\r\ncrypto_ahash_set_flags(tfm, *flags & CRYPTO_TFM_REQ_MASK);\r\nerr = crypto_ahash_setkey(tfm, key, keylen);\r\n*flags = crypto_ahash_get_flags(tfm);\r\nreturn err;\r\n}\r\nint atmel_sha_authenc_schedule(struct ahash_request *req,\r\nstruct atmel_sha_authenc_ctx *auth,\r\natmel_aes_authenc_fn_t cb,\r\nstruct atmel_aes_dev *aes_dev)\r\n{\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nstruct atmel_sha_reqctx *ctx = &authctx->base;\r\nstruct crypto_ahash *tfm = auth->tfm;\r\nstruct atmel_sha_ctx *tctx = crypto_ahash_ctx(tfm);\r\nstruct atmel_sha_dev *dd;\r\nmemset(authctx, 0, sizeof(*authctx));\r\ndd = atmel_sha_find_dev(tctx);\r\nif (!dd)\r\nreturn cb(aes_dev, -ENODEV, false);\r\nctx->dd = dd;\r\nctx->buflen = SHA_BUFFER_LEN;\r\nauthctx->cb = cb;\r\nauthctx->aes_dev = aes_dev;\r\nahash_request_set_tfm(req, tfm);\r\nahash_request_set_callback(req, 0, atmel_sha_authenc_complete, req);\r\nreturn atmel_sha_handle_queue(dd, req);\r\n}\r\nint atmel_sha_authenc_init(struct ahash_request *req,\r\nstruct scatterlist *assoc, unsigned int assoclen,\r\nunsigned int textlen,\r\natmel_aes_authenc_fn_t cb,\r\nstruct atmel_aes_dev *aes_dev)\r\n{\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nstruct atmel_sha_reqctx *ctx = &authctx->base;\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nstruct atmel_sha_dev *dd = ctx->dd;\r\nif (unlikely(!IS_ALIGNED(assoclen, sizeof(u32))))\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\nauthctx->cb = cb;\r\nauthctx->aes_dev = aes_dev;\r\nauthctx->assoc = assoc;\r\nauthctx->assoclen = assoclen;\r\nauthctx->textlen = textlen;\r\nctx->flags = hmac->base.flags;\r\nreturn atmel_sha_hmac_setup(dd, atmel_sha_authenc_init2);\r\n}\r\nstatic int atmel_sha_authenc_init2(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nstruct atmel_sha_reqctx *ctx = &authctx->base;\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct atmel_sha_hmac_ctx *hmac = crypto_ahash_ctx(tfm);\r\nsize_t hs = ctx->hash_size;\r\nsize_t i, num_words = hs / sizeof(u32);\r\nu32 mr, msg_size;\r\natmel_sha_write(dd, SHA_CR, SHA_CR_WUIHV);\r\nfor (i = 0; i < num_words; ++i)\r\natmel_sha_write(dd, SHA_REG_DIN(i), hmac->ipad[i]);\r\natmel_sha_write(dd, SHA_CR, SHA_CR_WUIEHV);\r\nfor (i = 0; i < num_words; ++i)\r\natmel_sha_write(dd, SHA_REG_DIN(i), hmac->opad[i]);\r\nmr = (SHA_MR_MODE_IDATAR0 |\r\nSHA_MR_HMAC |\r\nSHA_MR_DUALBUFF);\r\nmr |= ctx->flags & SHA_FLAGS_ALGO_MASK;\r\natmel_sha_write(dd, SHA_MR, mr);\r\nmsg_size = authctx->assoclen + authctx->textlen;\r\natmel_sha_write(dd, SHA_MSR, msg_size);\r\natmel_sha_write(dd, SHA_BCR, msg_size);\r\natmel_sha_write(dd, SHA_CR, SHA_CR_FIRST);\r\nreturn atmel_sha_cpu_start(dd, authctx->assoc, authctx->assoclen,\r\ntrue, false,\r\natmel_sha_authenc_init_done);\r\n}\r\nstatic int atmel_sha_authenc_init_done(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nreturn authctx->cb(authctx->aes_dev, 0, dd->is_async);\r\n}\r\nint atmel_sha_authenc_final(struct ahash_request *req,\r\nu32 *digest, unsigned int digestlen,\r\natmel_aes_authenc_fn_t cb,\r\nstruct atmel_aes_dev *aes_dev)\r\n{\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nstruct atmel_sha_reqctx *ctx = &authctx->base;\r\nstruct atmel_sha_dev *dd = ctx->dd;\r\nswitch (ctx->flags & SHA_FLAGS_ALGO_MASK) {\r\ncase SHA_FLAGS_SHA1:\r\nauthctx->digestlen = SHA1_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA224:\r\nauthctx->digestlen = SHA224_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA256:\r\nauthctx->digestlen = SHA256_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA384:\r\nauthctx->digestlen = SHA384_DIGEST_SIZE;\r\nbreak;\r\ncase SHA_FLAGS_SHA512:\r\nauthctx->digestlen = SHA512_DIGEST_SIZE;\r\nbreak;\r\ndefault:\r\nreturn atmel_sha_complete(dd, -EINVAL);\r\n}\r\nif (authctx->digestlen > digestlen)\r\nauthctx->digestlen = digestlen;\r\nauthctx->cb = cb;\r\nauthctx->aes_dev = aes_dev;\r\nauthctx->digest = digest;\r\nreturn atmel_sha_wait_for_data_ready(dd,\r\natmel_sha_authenc_final_done);\r\n}\r\nstatic int atmel_sha_authenc_final_done(struct atmel_sha_dev *dd)\r\n{\r\nstruct ahash_request *req = dd->req;\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nsize_t i, num_words = authctx->digestlen / sizeof(u32);\r\nfor (i = 0; i < num_words; ++i)\r\nauthctx->digest[i] = atmel_sha_read(dd, SHA_REG_DIGEST(i));\r\nreturn atmel_sha_complete(dd, 0);\r\n}\r\nvoid atmel_sha_authenc_abort(struct ahash_request *req)\r\n{\r\nstruct atmel_sha_authenc_reqctx *authctx = ahash_request_ctx(req);\r\nstruct atmel_sha_reqctx *ctx = &authctx->base;\r\nstruct atmel_sha_dev *dd = ctx->dd;\r\ndd->is_async = false;\r\ndd->force_complete = false;\r\n(void)atmel_sha_complete(dd, 0);\r\n}\r\nstatic void atmel_sha_unregister_algs(struct atmel_sha_dev *dd)\r\n{\r\nint i;\r\nif (dd->caps.has_hmac)\r\nfor (i = 0; i < ARRAY_SIZE(sha_hmac_algs); i++)\r\ncrypto_unregister_ahash(&sha_hmac_algs[i]);\r\nfor (i = 0; i < ARRAY_SIZE(sha_1_256_algs); i++)\r\ncrypto_unregister_ahash(&sha_1_256_algs[i]);\r\nif (dd->caps.has_sha224)\r\ncrypto_unregister_ahash(&sha_224_alg);\r\nif (dd->caps.has_sha_384_512) {\r\nfor (i = 0; i < ARRAY_SIZE(sha_384_512_algs); i++)\r\ncrypto_unregister_ahash(&sha_384_512_algs[i]);\r\n}\r\n}\r\nstatic int atmel_sha_register_algs(struct atmel_sha_dev *dd)\r\n{\r\nint err, i, j;\r\nfor (i = 0; i < ARRAY_SIZE(sha_1_256_algs); i++) {\r\nerr = crypto_register_ahash(&sha_1_256_algs[i]);\r\nif (err)\r\ngoto err_sha_1_256_algs;\r\n}\r\nif (dd->caps.has_sha224) {\r\nerr = crypto_register_ahash(&sha_224_alg);\r\nif (err)\r\ngoto err_sha_224_algs;\r\n}\r\nif (dd->caps.has_sha_384_512) {\r\nfor (i = 0; i < ARRAY_SIZE(sha_384_512_algs); i++) {\r\nerr = crypto_register_ahash(&sha_384_512_algs[i]);\r\nif (err)\r\ngoto err_sha_384_512_algs;\r\n}\r\n}\r\nif (dd->caps.has_hmac) {\r\nfor (i = 0; i < ARRAY_SIZE(sha_hmac_algs); i++) {\r\nerr = crypto_register_ahash(&sha_hmac_algs[i]);\r\nif (err)\r\ngoto err_sha_hmac_algs;\r\n}\r\n}\r\nreturn 0;\r\nerr_sha_hmac_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_ahash(&sha_hmac_algs[j]);\r\ni = ARRAY_SIZE(sha_384_512_algs);\r\nerr_sha_384_512_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_ahash(&sha_384_512_algs[j]);\r\ncrypto_unregister_ahash(&sha_224_alg);\r\nerr_sha_224_algs:\r\ni = ARRAY_SIZE(sha_1_256_algs);\r\nerr_sha_1_256_algs:\r\nfor (j = 0; j < i; j++)\r\ncrypto_unregister_ahash(&sha_1_256_algs[j]);\r\nreturn err;\r\n}\r\nstatic bool atmel_sha_filter(struct dma_chan *chan, void *slave)\r\n{\r\nstruct at_dma_slave *sl = slave;\r\nif (sl && sl->dma_dev == chan->device->dev) {\r\nchan->private = sl;\r\nreturn true;\r\n} else {\r\nreturn false;\r\n}\r\n}\r\nstatic int atmel_sha_dma_init(struct atmel_sha_dev *dd,\r\nstruct crypto_platform_data *pdata)\r\n{\r\nint err = -ENOMEM;\r\ndma_cap_mask_t mask_in;\r\ndma_cap_zero(mask_in);\r\ndma_cap_set(DMA_SLAVE, mask_in);\r\ndd->dma_lch_in.chan = dma_request_slave_channel_compat(mask_in,\r\natmel_sha_filter, &pdata->dma_slave->rxdata, dd->dev, "tx");\r\nif (!dd->dma_lch_in.chan) {\r\ndev_warn(dd->dev, "no DMA channel available\n");\r\nreturn err;\r\n}\r\ndd->dma_lch_in.dma_conf.direction = DMA_MEM_TO_DEV;\r\ndd->dma_lch_in.dma_conf.dst_addr = dd->phys_base +\r\nSHA_REG_DIN(0);\r\ndd->dma_lch_in.dma_conf.src_maxburst = 1;\r\ndd->dma_lch_in.dma_conf.src_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_in.dma_conf.dst_maxburst = 1;\r\ndd->dma_lch_in.dma_conf.dst_addr_width =\r\nDMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndd->dma_lch_in.dma_conf.device_fc = false;\r\nreturn 0;\r\n}\r\nstatic void atmel_sha_dma_cleanup(struct atmel_sha_dev *dd)\r\n{\r\ndma_release_channel(dd->dma_lch_in.chan);\r\n}\r\nstatic void atmel_sha_get_cap(struct atmel_sha_dev *dd)\r\n{\r\ndd->caps.has_dma = 0;\r\ndd->caps.has_dualbuff = 0;\r\ndd->caps.has_sha224 = 0;\r\ndd->caps.has_sha_384_512 = 0;\r\ndd->caps.has_uihv = 0;\r\ndd->caps.has_hmac = 0;\r\nswitch (dd->hw_version & 0xff0) {\r\ncase 0x510:\r\ndd->caps.has_dma = 1;\r\ndd->caps.has_dualbuff = 1;\r\ndd->caps.has_sha224 = 1;\r\ndd->caps.has_sha_384_512 = 1;\r\ndd->caps.has_uihv = 1;\r\ndd->caps.has_hmac = 1;\r\nbreak;\r\ncase 0x420:\r\ndd->caps.has_dma = 1;\r\ndd->caps.has_dualbuff = 1;\r\ndd->caps.has_sha224 = 1;\r\ndd->caps.has_sha_384_512 = 1;\r\ndd->caps.has_uihv = 1;\r\nbreak;\r\ncase 0x410:\r\ndd->caps.has_dma = 1;\r\ndd->caps.has_dualbuff = 1;\r\ndd->caps.has_sha224 = 1;\r\ndd->caps.has_sha_384_512 = 1;\r\nbreak;\r\ncase 0x400:\r\ndd->caps.has_dma = 1;\r\ndd->caps.has_dualbuff = 1;\r\ndd->caps.has_sha224 = 1;\r\nbreak;\r\ncase 0x320:\r\nbreak;\r\ndefault:\r\ndev_warn(dd->dev,\r\n"Unmanaged sha version, set minimum capabilities\n");\r\nbreak;\r\n}\r\n}\r\nstatic struct crypto_platform_data *atmel_sha_of_init(struct platform_device *pdev)\r\n{\r\nstruct device_node *np = pdev->dev.of_node;\r\nstruct crypto_platform_data *pdata;\r\nif (!np) {\r\ndev_err(&pdev->dev, "device node not found\n");\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\npdata = devm_kzalloc(&pdev->dev, sizeof(*pdata), GFP_KERNEL);\r\nif (!pdata) {\r\ndev_err(&pdev->dev, "could not allocate memory for pdata\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\npdata->dma_slave = devm_kzalloc(&pdev->dev,\r\nsizeof(*(pdata->dma_slave)),\r\nGFP_KERNEL);\r\nif (!pdata->dma_slave) {\r\ndev_err(&pdev->dev, "could not allocate memory for dma_slave\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nreturn pdata;\r\n}\r\nstatic inline struct crypto_platform_data *atmel_sha_of_init(struct platform_device *dev)\r\n{\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstatic int atmel_sha_probe(struct platform_device *pdev)\r\n{\r\nstruct atmel_sha_dev *sha_dd;\r\nstruct crypto_platform_data *pdata;\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *sha_res;\r\nint err;\r\nsha_dd = devm_kzalloc(&pdev->dev, sizeof(*sha_dd), GFP_KERNEL);\r\nif (sha_dd == NULL) {\r\ndev_err(dev, "unable to alloc data struct.\n");\r\nerr = -ENOMEM;\r\ngoto sha_dd_err;\r\n}\r\nsha_dd->dev = dev;\r\nplatform_set_drvdata(pdev, sha_dd);\r\nINIT_LIST_HEAD(&sha_dd->list);\r\nspin_lock_init(&sha_dd->lock);\r\ntasklet_init(&sha_dd->done_task, atmel_sha_done_task,\r\n(unsigned long)sha_dd);\r\ntasklet_init(&sha_dd->queue_task, atmel_sha_queue_task,\r\n(unsigned long)sha_dd);\r\ncrypto_init_queue(&sha_dd->queue, ATMEL_SHA_QUEUE_LENGTH);\r\nsha_dd->irq = -1;\r\nsha_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!sha_res) {\r\ndev_err(dev, "no MEM resource info\n");\r\nerr = -ENODEV;\r\ngoto res_err;\r\n}\r\nsha_dd->phys_base = sha_res->start;\r\nsha_dd->irq = platform_get_irq(pdev, 0);\r\nif (sha_dd->irq < 0) {\r\ndev_err(dev, "no IRQ resource info\n");\r\nerr = sha_dd->irq;\r\ngoto res_err;\r\n}\r\nerr = devm_request_irq(&pdev->dev, sha_dd->irq, atmel_sha_irq,\r\nIRQF_SHARED, "atmel-sha", sha_dd);\r\nif (err) {\r\ndev_err(dev, "unable to request sha irq.\n");\r\ngoto res_err;\r\n}\r\nsha_dd->iclk = devm_clk_get(&pdev->dev, "sha_clk");\r\nif (IS_ERR(sha_dd->iclk)) {\r\ndev_err(dev, "clock initialization failed.\n");\r\nerr = PTR_ERR(sha_dd->iclk);\r\ngoto res_err;\r\n}\r\nsha_dd->io_base = devm_ioremap_resource(&pdev->dev, sha_res);\r\nif (IS_ERR(sha_dd->io_base)) {\r\ndev_err(dev, "can't ioremap\n");\r\nerr = PTR_ERR(sha_dd->io_base);\r\ngoto res_err;\r\n}\r\nerr = clk_prepare(sha_dd->iclk);\r\nif (err)\r\ngoto res_err;\r\natmel_sha_hw_version_init(sha_dd);\r\natmel_sha_get_cap(sha_dd);\r\nif (sha_dd->caps.has_dma) {\r\npdata = pdev->dev.platform_data;\r\nif (!pdata) {\r\npdata = atmel_sha_of_init(pdev);\r\nif (IS_ERR(pdata)) {\r\ndev_err(&pdev->dev, "platform data not available\n");\r\nerr = PTR_ERR(pdata);\r\ngoto iclk_unprepare;\r\n}\r\n}\r\nif (!pdata->dma_slave) {\r\nerr = -ENXIO;\r\ngoto iclk_unprepare;\r\n}\r\nerr = atmel_sha_dma_init(sha_dd, pdata);\r\nif (err)\r\ngoto err_sha_dma;\r\ndev_info(dev, "using %s for DMA transfers\n",\r\ndma_chan_name(sha_dd->dma_lch_in.chan));\r\n}\r\nspin_lock(&atmel_sha.lock);\r\nlist_add_tail(&sha_dd->list, &atmel_sha.dev_list);\r\nspin_unlock(&atmel_sha.lock);\r\nerr = atmel_sha_register_algs(sha_dd);\r\nif (err)\r\ngoto err_algs;\r\ndev_info(dev, "Atmel SHA1/SHA256%s%s\n",\r\nsha_dd->caps.has_sha224 ? "/SHA224" : "",\r\nsha_dd->caps.has_sha_384_512 ? "/SHA384/SHA512" : "");\r\nreturn 0;\r\nerr_algs:\r\nspin_lock(&atmel_sha.lock);\r\nlist_del(&sha_dd->list);\r\nspin_unlock(&atmel_sha.lock);\r\nif (sha_dd->caps.has_dma)\r\natmel_sha_dma_cleanup(sha_dd);\r\nerr_sha_dma:\r\niclk_unprepare:\r\nclk_unprepare(sha_dd->iclk);\r\nres_err:\r\ntasklet_kill(&sha_dd->queue_task);\r\ntasklet_kill(&sha_dd->done_task);\r\nsha_dd_err:\r\ndev_err(dev, "initialization failed.\n");\r\nreturn err;\r\n}\r\nstatic int atmel_sha_remove(struct platform_device *pdev)\r\n{\r\nstatic struct atmel_sha_dev *sha_dd;\r\nsha_dd = platform_get_drvdata(pdev);\r\nif (!sha_dd)\r\nreturn -ENODEV;\r\nspin_lock(&atmel_sha.lock);\r\nlist_del(&sha_dd->list);\r\nspin_unlock(&atmel_sha.lock);\r\natmel_sha_unregister_algs(sha_dd);\r\ntasklet_kill(&sha_dd->queue_task);\r\ntasklet_kill(&sha_dd->done_task);\r\nif (sha_dd->caps.has_dma)\r\natmel_sha_dma_cleanup(sha_dd);\r\nclk_unprepare(sha_dd->iclk);\r\nreturn 0;\r\n}
