static ssize_t dmabounce_show(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct dmabounce_device_info *device_info = dev->archdata.dmabounce;\r\nreturn sprintf(buf, "%lu %lu %lu %lu %lu %lu\n",\r\ndevice_info->small.allocs,\r\ndevice_info->large.allocs,\r\ndevice_info->total_allocs - device_info->small.allocs -\r\ndevice_info->large.allocs,\r\ndevice_info->total_allocs,\r\ndevice_info->map_op_count,\r\ndevice_info->bounce_count);\r\n}\r\nstatic inline struct safe_buffer *\r\nalloc_safe_buffer(struct dmabounce_device_info *device_info, void *ptr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nstruct safe_buffer *buf;\r\nstruct dmabounce_pool *pool;\r\nstruct device *dev = device_info->dev;\r\nunsigned long flags;\r\ndev_dbg(dev, "%s(ptr=%p, size=%d, dir=%d)\n",\r\n__func__, ptr, size, dir);\r\nif (size <= device_info->small.size) {\r\npool = &device_info->small;\r\n} else if (size <= device_info->large.size) {\r\npool = &device_info->large;\r\n} else {\r\npool = NULL;\r\n}\r\nbuf = kmalloc(sizeof(struct safe_buffer), GFP_ATOMIC);\r\nif (buf == NULL) {\r\ndev_warn(dev, "%s: kmalloc failed\n", __func__);\r\nreturn NULL;\r\n}\r\nbuf->ptr = ptr;\r\nbuf->size = size;\r\nbuf->direction = dir;\r\nbuf->pool = pool;\r\nif (pool) {\r\nbuf->safe = dma_pool_alloc(pool->pool, GFP_ATOMIC,\r\n&buf->safe_dma_addr);\r\n} else {\r\nbuf->safe = dma_alloc_coherent(dev, size, &buf->safe_dma_addr,\r\nGFP_ATOMIC);\r\n}\r\nif (buf->safe == NULL) {\r\ndev_warn(dev,\r\n"%s: could not alloc dma memory (size=%d)\n",\r\n__func__, size);\r\nkfree(buf);\r\nreturn NULL;\r\n}\r\n#ifdef STATS\r\nif (pool)\r\npool->allocs++;\r\ndevice_info->total_allocs++;\r\n#endif\r\nwrite_lock_irqsave(&device_info->lock, flags);\r\nlist_add(&buf->node, &device_info->safe_buffers);\r\nwrite_unlock_irqrestore(&device_info->lock, flags);\r\nreturn buf;\r\n}\r\nstatic inline struct safe_buffer *\r\nfind_safe_buffer(struct dmabounce_device_info *device_info, dma_addr_t safe_dma_addr)\r\n{\r\nstruct safe_buffer *b, *rb = NULL;\r\nunsigned long flags;\r\nread_lock_irqsave(&device_info->lock, flags);\r\nlist_for_each_entry(b, &device_info->safe_buffers, node)\r\nif (b->safe_dma_addr <= safe_dma_addr &&\r\nb->safe_dma_addr + b->size > safe_dma_addr) {\r\nrb = b;\r\nbreak;\r\n}\r\nread_unlock_irqrestore(&device_info->lock, flags);\r\nreturn rb;\r\n}\r\nstatic inline void\r\nfree_safe_buffer(struct dmabounce_device_info *device_info, struct safe_buffer *buf)\r\n{\r\nunsigned long flags;\r\ndev_dbg(device_info->dev, "%s(buf=%p)\n", __func__, buf);\r\nwrite_lock_irqsave(&device_info->lock, flags);\r\nlist_del(&buf->node);\r\nwrite_unlock_irqrestore(&device_info->lock, flags);\r\nif (buf->pool)\r\ndma_pool_free(buf->pool->pool, buf->safe, buf->safe_dma_addr);\r\nelse\r\ndma_free_coherent(device_info->dev, buf->size, buf->safe,\r\nbuf->safe_dma_addr);\r\nkfree(buf);\r\n}\r\nstatic struct safe_buffer *find_safe_buffer_dev(struct device *dev,\r\ndma_addr_t dma_addr, const char *where)\r\n{\r\nif (!dev || !dev->archdata.dmabounce)\r\nreturn NULL;\r\nif (dma_mapping_error(dev, dma_addr)) {\r\ndev_err(dev, "Trying to %s invalid mapping\n", where);\r\nreturn NULL;\r\n}\r\nreturn find_safe_buffer(dev->archdata.dmabounce, dma_addr);\r\n}\r\nstatic int needs_bounce(struct device *dev, dma_addr_t dma_addr, size_t size)\r\n{\r\nif (!dev || !dev->archdata.dmabounce)\r\nreturn 0;\r\nif (dev->dma_mask) {\r\nunsigned long limit, mask = *dev->dma_mask;\r\nlimit = (mask + 1) & ~mask;\r\nif (limit && size > limit) {\r\ndev_err(dev, "DMA mapping too big (requested %#x "\r\n"mask %#Lx)\n", size, *dev->dma_mask);\r\nreturn -E2BIG;\r\n}\r\nif ((dma_addr | (dma_addr + size - 1)) & ~mask)\r\nreturn 1;\r\n}\r\nreturn !!dev->archdata.dmabounce->needs_bounce(dev, dma_addr, size);\r\n}\r\nstatic inline dma_addr_t map_single(struct device *dev, void *ptr, size_t size,\r\nenum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nstruct dmabounce_device_info *device_info = dev->archdata.dmabounce;\r\nstruct safe_buffer *buf;\r\nif (device_info)\r\nDO_STATS ( device_info->map_op_count++ );\r\nbuf = alloc_safe_buffer(device_info, ptr, size, dir);\r\nif (buf == NULL) {\r\ndev_err(dev, "%s: unable to map unsafe buffer %p!\n",\r\n__func__, ptr);\r\nreturn DMA_ERROR_CODE;\r\n}\r\ndev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",\r\n__func__, buf->ptr, virt_to_dma(dev, buf->ptr),\r\nbuf->safe, buf->safe_dma_addr);\r\nif ((dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL) &&\r\n!(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {\r\ndev_dbg(dev, "%s: copy unsafe %p to safe %p, size %d\n",\r\n__func__, ptr, buf->safe, size);\r\nmemcpy(buf->safe, ptr, size);\r\n}\r\nreturn buf->safe_dma_addr;\r\n}\r\nstatic inline void unmap_single(struct device *dev, struct safe_buffer *buf,\r\nsize_t size, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nBUG_ON(buf->size != size);\r\nBUG_ON(buf->direction != dir);\r\ndev_dbg(dev, "%s: unsafe buffer %p (dma=%#x) mapped to %p (dma=%#x)\n",\r\n__func__, buf->ptr, virt_to_dma(dev, buf->ptr),\r\nbuf->safe, buf->safe_dma_addr);\r\nDO_STATS(dev->archdata.dmabounce->bounce_count++);\r\nif ((dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) &&\r\n!(attrs & DMA_ATTR_SKIP_CPU_SYNC)) {\r\nvoid *ptr = buf->ptr;\r\ndev_dbg(dev, "%s: copy back safe %p to unsafe %p size %d\n",\r\n__func__, buf->safe, ptr, size);\r\nmemcpy(ptr, buf->safe, size);\r\n__cpuc_flush_dcache_area(ptr, size);\r\n}\r\nfree_safe_buffer(dev->archdata.dmabounce, buf);\r\n}\r\nstatic dma_addr_t dmabounce_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size, enum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\ndma_addr_t dma_addr;\r\nint ret;\r\ndev_dbg(dev, "%s(page=%p,off=%#lx,size=%zx,dir=%x)\n",\r\n__func__, page, offset, size, dir);\r\ndma_addr = pfn_to_dma(dev, page_to_pfn(page)) + offset;\r\nret = needs_bounce(dev, dma_addr, size);\r\nif (ret < 0)\r\nreturn DMA_ERROR_CODE;\r\nif (ret == 0) {\r\narm_dma_ops.sync_single_for_device(dev, dma_addr, size, dir);\r\nreturn dma_addr;\r\n}\r\nif (PageHighMem(page)) {\r\ndev_err(dev, "DMA buffer bouncing of HIGHMEM pages is not supported\n");\r\nreturn DMA_ERROR_CODE;\r\n}\r\nreturn map_single(dev, page_address(page) + offset, size, dir, attrs);\r\n}\r\nstatic void dmabounce_unmap_page(struct device *dev, dma_addr_t dma_addr, size_t size,\r\nenum dma_data_direction dir, unsigned long attrs)\r\n{\r\nstruct safe_buffer *buf;\r\ndev_dbg(dev, "%s(dma=%#x,size=%d,dir=%x)\n",\r\n__func__, dma_addr, size, dir);\r\nbuf = find_safe_buffer_dev(dev, dma_addr, __func__);\r\nif (!buf) {\r\narm_dma_ops.sync_single_for_cpu(dev, dma_addr, size, dir);\r\nreturn;\r\n}\r\nunmap_single(dev, buf, size, dir, attrs);\r\n}\r\nstatic int __dmabounce_sync_for_cpu(struct device *dev, dma_addr_t addr,\r\nsize_t sz, enum dma_data_direction dir)\r\n{\r\nstruct safe_buffer *buf;\r\nunsigned long off;\r\ndev_dbg(dev, "%s(dma=%#x,sz=%zx,dir=%x)\n",\r\n__func__, addr, sz, dir);\r\nbuf = find_safe_buffer_dev(dev, addr, __func__);\r\nif (!buf)\r\nreturn 1;\r\noff = addr - buf->safe_dma_addr;\r\nBUG_ON(buf->direction != dir);\r\ndev_dbg(dev, "%s: unsafe buffer %p (dma=%#x off=%#lx) mapped to %p (dma=%#x)\n",\r\n__func__, buf->ptr, virt_to_dma(dev, buf->ptr), off,\r\nbuf->safe, buf->safe_dma_addr);\r\nDO_STATS(dev->archdata.dmabounce->bounce_count++);\r\nif (dir == DMA_FROM_DEVICE || dir == DMA_BIDIRECTIONAL) {\r\ndev_dbg(dev, "%s: copy back safe %p to unsafe %p size %d\n",\r\n__func__, buf->safe + off, buf->ptr + off, sz);\r\nmemcpy(buf->ptr + off, buf->safe + off, sz);\r\n}\r\nreturn 0;\r\n}\r\nstatic void dmabounce_sync_for_cpu(struct device *dev,\r\ndma_addr_t handle, size_t size, enum dma_data_direction dir)\r\n{\r\nif (!__dmabounce_sync_for_cpu(dev, handle, size, dir))\r\nreturn;\r\narm_dma_ops.sync_single_for_cpu(dev, handle, size, dir);\r\n}\r\nstatic int __dmabounce_sync_for_device(struct device *dev, dma_addr_t addr,\r\nsize_t sz, enum dma_data_direction dir)\r\n{\r\nstruct safe_buffer *buf;\r\nunsigned long off;\r\ndev_dbg(dev, "%s(dma=%#x,sz=%zx,dir=%x)\n",\r\n__func__, addr, sz, dir);\r\nbuf = find_safe_buffer_dev(dev, addr, __func__);\r\nif (!buf)\r\nreturn 1;\r\noff = addr - buf->safe_dma_addr;\r\nBUG_ON(buf->direction != dir);\r\ndev_dbg(dev, "%s: unsafe buffer %p (dma=%#x off=%#lx) mapped to %p (dma=%#x)\n",\r\n__func__, buf->ptr, virt_to_dma(dev, buf->ptr), off,\r\nbuf->safe, buf->safe_dma_addr);\r\nDO_STATS(dev->archdata.dmabounce->bounce_count++);\r\nif (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL) {\r\ndev_dbg(dev, "%s: copy out unsafe %p to safe %p, size %d\n",\r\n__func__,buf->ptr + off, buf->safe + off, sz);\r\nmemcpy(buf->safe + off, buf->ptr + off, sz);\r\n}\r\nreturn 0;\r\n}\r\nstatic void dmabounce_sync_for_device(struct device *dev,\r\ndma_addr_t handle, size_t size, enum dma_data_direction dir)\r\n{\r\nif (!__dmabounce_sync_for_device(dev, handle, size, dir))\r\nreturn;\r\narm_dma_ops.sync_single_for_device(dev, handle, size, dir);\r\n}\r\nstatic int dmabounce_set_mask(struct device *dev, u64 dma_mask)\r\n{\r\nif (dev->archdata.dmabounce)\r\nreturn 0;\r\nreturn arm_dma_ops.set_dma_mask(dev, dma_mask);\r\n}\r\nstatic int dmabounce_init_pool(struct dmabounce_pool *pool, struct device *dev,\r\nconst char *name, unsigned long size)\r\n{\r\npool->size = size;\r\nDO_STATS(pool->allocs = 0);\r\npool->pool = dma_pool_create(name, dev, size,\r\n0 ,\r\n0 );\r\nreturn pool->pool ? 0 : -ENOMEM;\r\n}\r\nint dmabounce_register_dev(struct device *dev, unsigned long small_buffer_size,\r\nunsigned long large_buffer_size,\r\nint (*needs_bounce_fn)(struct device *, dma_addr_t, size_t))\r\n{\r\nstruct dmabounce_device_info *device_info;\r\nint ret;\r\ndevice_info = kmalloc(sizeof(struct dmabounce_device_info), GFP_ATOMIC);\r\nif (!device_info) {\r\ndev_err(dev,\r\n"Could not allocated dmabounce_device_info\n");\r\nreturn -ENOMEM;\r\n}\r\nret = dmabounce_init_pool(&device_info->small, dev,\r\n"small_dmabounce_pool", small_buffer_size);\r\nif (ret) {\r\ndev_err(dev,\r\n"dmabounce: could not allocate DMA pool for %ld byte objects\n",\r\nsmall_buffer_size);\r\ngoto err_free;\r\n}\r\nif (large_buffer_size) {\r\nret = dmabounce_init_pool(&device_info->large, dev,\r\n"large_dmabounce_pool",\r\nlarge_buffer_size);\r\nif (ret) {\r\ndev_err(dev,\r\n"dmabounce: could not allocate DMA pool for %ld byte objects\n",\r\nlarge_buffer_size);\r\ngoto err_destroy;\r\n}\r\n}\r\ndevice_info->dev = dev;\r\nINIT_LIST_HEAD(&device_info->safe_buffers);\r\nrwlock_init(&device_info->lock);\r\ndevice_info->needs_bounce = needs_bounce_fn;\r\n#ifdef STATS\r\ndevice_info->total_allocs = 0;\r\ndevice_info->map_op_count = 0;\r\ndevice_info->bounce_count = 0;\r\ndevice_info->attr_res = device_create_file(dev, &dev_attr_dmabounce_stats);\r\n#endif\r\ndev->archdata.dmabounce = device_info;\r\nset_dma_ops(dev, &dmabounce_ops);\r\ndev_info(dev, "dmabounce: registered device\n");\r\nreturn 0;\r\nerr_destroy:\r\ndma_pool_destroy(device_info->small.pool);\r\nerr_free:\r\nkfree(device_info);\r\nreturn ret;\r\n}\r\nvoid dmabounce_unregister_dev(struct device *dev)\r\n{\r\nstruct dmabounce_device_info *device_info = dev->archdata.dmabounce;\r\ndev->archdata.dmabounce = NULL;\r\nset_dma_ops(dev, NULL);\r\nif (!device_info) {\r\ndev_warn(dev,\r\n"Never registered with dmabounce but attempting"\r\n"to unregister!\n");\r\nreturn;\r\n}\r\nif (!list_empty(&device_info->safe_buffers)) {\r\ndev_err(dev,\r\n"Removing from dmabounce with pending buffers!\n");\r\nBUG();\r\n}\r\nif (device_info->small.pool)\r\ndma_pool_destroy(device_info->small.pool);\r\nif (device_info->large.pool)\r\ndma_pool_destroy(device_info->large.pool);\r\n#ifdef STATS\r\nif (device_info->attr_res == 0)\r\ndevice_remove_file(dev, &dev_attr_dmabounce_stats);\r\n#endif\r\nkfree(device_info);\r\ndev_info(dev, "dmabounce: device unregistered\n");\r\n}
