static int sched_itmt_update_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp,\r\nloff_t *ppos)\r\n{\r\nunsigned int old_sysctl;\r\nint ret;\r\nmutex_lock(&itmt_update_mutex);\r\nif (!sched_itmt_capable) {\r\nmutex_unlock(&itmt_update_mutex);\r\nreturn -EINVAL;\r\n}\r\nold_sysctl = sysctl_sched_itmt_enabled;\r\nret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\r\nif (!ret && write && old_sysctl != sysctl_sched_itmt_enabled) {\r\nx86_topology_update = true;\r\nrebuild_sched_domains();\r\n}\r\nmutex_unlock(&itmt_update_mutex);\r\nreturn ret;\r\n}\r\nint sched_set_itmt_support(void)\r\n{\r\nmutex_lock(&itmt_update_mutex);\r\nif (sched_itmt_capable) {\r\nmutex_unlock(&itmt_update_mutex);\r\nreturn 0;\r\n}\r\nitmt_sysctl_header = register_sysctl_table(itmt_root_table);\r\nif (!itmt_sysctl_header) {\r\nmutex_unlock(&itmt_update_mutex);\r\nreturn -ENOMEM;\r\n}\r\nsched_itmt_capable = true;\r\nsysctl_sched_itmt_enabled = 1;\r\nx86_topology_update = true;\r\nrebuild_sched_domains();\r\nmutex_unlock(&itmt_update_mutex);\r\nreturn 0;\r\n}\r\nvoid sched_clear_itmt_support(void)\r\n{\r\nmutex_lock(&itmt_update_mutex);\r\nif (!sched_itmt_capable) {\r\nmutex_unlock(&itmt_update_mutex);\r\nreturn;\r\n}\r\nsched_itmt_capable = false;\r\nif (itmt_sysctl_header) {\r\nunregister_sysctl_table(itmt_sysctl_header);\r\nitmt_sysctl_header = NULL;\r\n}\r\nif (sysctl_sched_itmt_enabled) {\r\nsysctl_sched_itmt_enabled = 0;\r\nx86_topology_update = true;\r\nrebuild_sched_domains();\r\n}\r\nmutex_unlock(&itmt_update_mutex);\r\n}\r\nint arch_asym_cpu_priority(int cpu)\r\n{\r\nreturn per_cpu(sched_core_priority, cpu);\r\n}\r\nvoid sched_set_itmt_core_prio(int prio, int core_cpu)\r\n{\r\nint cpu, i = 1;\r\nfor_each_cpu(cpu, topology_sibling_cpumask(core_cpu)) {\r\nint smt_prio;\r\nsmt_prio = prio * smp_num_siblings / i;\r\nper_cpu(sched_core_priority, cpu) = smt_prio;\r\ni++;\r\n}\r\n}
