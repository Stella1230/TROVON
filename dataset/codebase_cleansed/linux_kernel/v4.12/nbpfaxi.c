static inline u32 nbpf_chan_read(struct nbpf_channel *chan,\r\nunsigned int offset)\r\n{\r\nu32 data = ioread32(chan->base + offset);\r\ndev_dbg(chan->dma_chan.device->dev, "%s(0x%p + 0x%x) = 0x%x\n",\r\n__func__, chan->base, offset, data);\r\nreturn data;\r\n}\r\nstatic inline void nbpf_chan_write(struct nbpf_channel *chan,\r\nunsigned int offset, u32 data)\r\n{\r\niowrite32(data, chan->base + offset);\r\ndev_dbg(chan->dma_chan.device->dev, "%s(0x%p + 0x%x) = 0x%x\n",\r\n__func__, chan->base, offset, data);\r\n}\r\nstatic inline u32 nbpf_read(struct nbpf_device *nbpf,\r\nunsigned int offset)\r\n{\r\nu32 data = ioread32(nbpf->base + offset);\r\ndev_dbg(nbpf->dma_dev.dev, "%s(0x%p + 0x%x) = 0x%x\n",\r\n__func__, nbpf->base, offset, data);\r\nreturn data;\r\n}\r\nstatic inline void nbpf_write(struct nbpf_device *nbpf,\r\nunsigned int offset, u32 data)\r\n{\r\niowrite32(data, nbpf->base + offset);\r\ndev_dbg(nbpf->dma_dev.dev, "%s(0x%p + 0x%x) = 0x%x\n",\r\n__func__, nbpf->base, offset, data);\r\n}\r\nstatic void nbpf_chan_halt(struct nbpf_channel *chan)\r\n{\r\nnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_CLREN);\r\n}\r\nstatic bool nbpf_status_get(struct nbpf_channel *chan)\r\n{\r\nu32 status = nbpf_read(chan->nbpf, NBPF_DSTAT_END);\r\nreturn status & BIT(chan - chan->nbpf->chan);\r\n}\r\nstatic void nbpf_status_ack(struct nbpf_channel *chan)\r\n{\r\nnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_CLREND);\r\n}\r\nstatic u32 nbpf_error_get(struct nbpf_device *nbpf)\r\n{\r\nreturn nbpf_read(nbpf, NBPF_DSTAT_ER);\r\n}\r\nstatic struct nbpf_channel *nbpf_error_get_channel(struct nbpf_device *nbpf, u32 error)\r\n{\r\nreturn nbpf->chan + __ffs(error);\r\n}\r\nstatic void nbpf_error_clear(struct nbpf_channel *chan)\r\n{\r\nu32 status;\r\nint i;\r\nnbpf_chan_halt(chan);\r\nfor (i = 1000; i; i--) {\r\nstatus = nbpf_chan_read(chan, NBPF_CHAN_STAT);\r\nif (!(status & NBPF_CHAN_STAT_TACT))\r\nbreak;\r\ncpu_relax();\r\n}\r\nif (!i)\r\ndev_err(chan->dma_chan.device->dev,\r\n"%s(): abort timeout, channel status 0x%x\n", __func__, status);\r\nnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_SWRST);\r\n}\r\nstatic int nbpf_start(struct nbpf_desc *desc)\r\n{\r\nstruct nbpf_channel *chan = desc->chan;\r\nstruct nbpf_link_desc *ldesc = list_first_entry(&desc->sg, struct nbpf_link_desc, node);\r\nnbpf_chan_write(chan, NBPF_CHAN_NXLA, (u32)ldesc->hwdesc_dma_addr);\r\nnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_SETEN | NBPF_CHAN_CTRL_CLRSUS);\r\nchan->paused = false;\r\nif (ldesc->hwdesc->config & NBPF_CHAN_CFG_TM)\r\nnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_STG);\r\ndev_dbg(chan->nbpf->dma_dev.dev, "%s(): next 0x%x, cur 0x%x\n", __func__,\r\nnbpf_chan_read(chan, NBPF_CHAN_NXLA), nbpf_chan_read(chan, NBPF_CHAN_CRLA));\r\nreturn 0;\r\n}\r\nstatic void nbpf_chan_prepare(struct nbpf_channel *chan)\r\n{\r\nchan->dmarq_cfg = (chan->flags & NBPF_SLAVE_RQ_HIGH ? NBPF_CHAN_CFG_HIEN : 0) |\r\n(chan->flags & NBPF_SLAVE_RQ_LOW ? NBPF_CHAN_CFG_LOEN : 0) |\r\n(chan->flags & NBPF_SLAVE_RQ_LEVEL ?\r\nNBPF_CHAN_CFG_LVL | (NBPF_CHAN_CFG_AM & 0x200) : 0) |\r\nchan->terminal;\r\n}\r\nstatic void nbpf_chan_prepare_default(struct nbpf_channel *chan)\r\n{\r\nchan->dmarq_cfg = NBPF_CHAN_CFG_AM & 0x400;\r\nchan->terminal = 0;\r\nchan->flags = 0;\r\n}\r\nstatic void nbpf_chan_configure(struct nbpf_channel *chan)\r\n{\r\nnbpf_chan_write(chan, NBPF_CHAN_CFG, NBPF_CHAN_CFG_DMS | chan->dmarq_cfg);\r\n}\r\nstatic u32 nbpf_xfer_ds(struct nbpf_device *nbpf, size_t size,\r\nenum dma_transfer_direction direction)\r\n{\r\nint max_burst = nbpf->config->buffer_size * 8;\r\nif (nbpf->max_burst_mem_read || nbpf->max_burst_mem_write) {\r\nswitch (direction) {\r\ncase DMA_MEM_TO_MEM:\r\nmax_burst = min_not_zero(nbpf->max_burst_mem_read,\r\nnbpf->max_burst_mem_write);\r\nbreak;\r\ncase DMA_MEM_TO_DEV:\r\nif (nbpf->max_burst_mem_read)\r\nmax_burst = nbpf->max_burst_mem_read;\r\nbreak;\r\ncase DMA_DEV_TO_MEM:\r\nif (nbpf->max_burst_mem_write)\r\nmax_burst = nbpf->max_burst_mem_write;\r\nbreak;\r\ncase DMA_DEV_TO_DEV:\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nreturn min_t(int, __ffs(size), ilog2(max_burst));\r\n}\r\nstatic size_t nbpf_xfer_size(struct nbpf_device *nbpf,\r\nenum dma_slave_buswidth width, u32 burst)\r\n{\r\nsize_t size;\r\nif (!burst)\r\nburst = 1;\r\nswitch (width) {\r\ncase DMA_SLAVE_BUSWIDTH_8_BYTES:\r\nsize = 8 * burst;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\nsize = 4 * burst;\r\nbreak;\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\nsize = 2 * burst;\r\nbreak;\r\ndefault:\r\npr_warn("%s(): invalid bus width %u\n", __func__, width);\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\nsize = burst;\r\n}\r\nreturn nbpf_xfer_ds(nbpf, size, DMA_TRANS_NONE);\r\n}\r\nstatic int nbpf_prep_one(struct nbpf_link_desc *ldesc,\r\nenum dma_transfer_direction direction,\r\ndma_addr_t src, dma_addr_t dst, size_t size, bool last)\r\n{\r\nstruct nbpf_link_reg *hwdesc = ldesc->hwdesc;\r\nstruct nbpf_desc *desc = ldesc->desc;\r\nstruct nbpf_channel *chan = desc->chan;\r\nstruct device *dev = chan->dma_chan.device->dev;\r\nsize_t mem_xfer, slave_xfer;\r\nbool can_burst;\r\nhwdesc->header = NBPF_HEADER_WBD | NBPF_HEADER_LV |\r\n(last ? NBPF_HEADER_LE : 0);\r\nhwdesc->src_addr = src;\r\nhwdesc->dst_addr = dst;\r\nhwdesc->transaction_size = size;\r\nmem_xfer = nbpf_xfer_ds(chan->nbpf, size, direction);\r\nswitch (direction) {\r\ncase DMA_DEV_TO_MEM:\r\ncan_burst = chan->slave_src_width >= 3;\r\nslave_xfer = min(mem_xfer, can_burst ?\r\nchan->slave_src_burst : chan->slave_src_width);\r\nif (mem_xfer > chan->slave_src_burst && !can_burst)\r\nmem_xfer = chan->slave_src_burst;\r\nhwdesc->config = NBPF_CHAN_CFG_SAD | (NBPF_CHAN_CFG_DDS & (mem_xfer << 16)) |\r\n(NBPF_CHAN_CFG_SDS & (slave_xfer << 12)) | NBPF_CHAN_CFG_REQD |\r\nNBPF_CHAN_CFG_SBE;\r\nbreak;\r\ncase DMA_MEM_TO_DEV:\r\nslave_xfer = min(mem_xfer, chan->slave_dst_width >= 3 ?\r\nchan->slave_dst_burst : chan->slave_dst_width);\r\nhwdesc->config = NBPF_CHAN_CFG_DAD | (NBPF_CHAN_CFG_SDS & (mem_xfer << 12)) |\r\n(NBPF_CHAN_CFG_DDS & (slave_xfer << 16)) | NBPF_CHAN_CFG_REQD;\r\nbreak;\r\ncase DMA_MEM_TO_MEM:\r\nhwdesc->config = NBPF_CHAN_CFG_TCM | NBPF_CHAN_CFG_TM |\r\n(NBPF_CHAN_CFG_SDS & (mem_xfer << 12)) |\r\n(NBPF_CHAN_CFG_DDS & (mem_xfer << 16));\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nhwdesc->config |= chan->dmarq_cfg | (last ? 0 : NBPF_CHAN_CFG_DEM) |\r\nNBPF_CHAN_CFG_DMS;\r\ndev_dbg(dev, "%s(): desc @ %pad: hdr 0x%x, cfg 0x%x, %zu @ %pad -> %pad\n",\r\n__func__, &ldesc->hwdesc_dma_addr, hwdesc->header,\r\nhwdesc->config, size, &src, &dst);\r\ndma_sync_single_for_device(dev, ldesc->hwdesc_dma_addr, sizeof(*hwdesc),\r\nDMA_TO_DEVICE);\r\nreturn 0;\r\n}\r\nstatic size_t nbpf_bytes_left(struct nbpf_channel *chan)\r\n{\r\nreturn nbpf_chan_read(chan, NBPF_CHAN_CUR_TR_BYTE);\r\n}\r\nstatic void nbpf_configure(struct nbpf_device *nbpf)\r\n{\r\nnbpf_write(nbpf, NBPF_CTRL, NBPF_CTRL_LVINT);\r\n}\r\nstatic void nbpf_issue_pending(struct dma_chan *dchan)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\nunsigned long flags;\r\ndev_dbg(dchan->device->dev, "Entry %s()\n", __func__);\r\nspin_lock_irqsave(&chan->lock, flags);\r\nif (list_empty(&chan->queued))\r\ngoto unlock;\r\nlist_splice_tail_init(&chan->queued, &chan->active);\r\nif (!chan->running) {\r\nstruct nbpf_desc *desc = list_first_entry(&chan->active,\r\nstruct nbpf_desc, node);\r\nif (!nbpf_start(desc))\r\nchan->running = desc;\r\n}\r\nunlock:\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic enum dma_status nbpf_tx_status(struct dma_chan *dchan,\r\ndma_cookie_t cookie, struct dma_tx_state *state)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\nenum dma_status status = dma_cookie_status(dchan, cookie, state);\r\nif (state) {\r\ndma_cookie_t running;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nrunning = chan->running ? chan->running->async_tx.cookie : -EINVAL;\r\nif (cookie == running) {\r\nstate->residue = nbpf_bytes_left(chan);\r\ndev_dbg(dchan->device->dev, "%s(): residue %u\n", __func__,\r\nstate->residue);\r\n} else if (status == DMA_IN_PROGRESS) {\r\nstruct nbpf_desc *desc;\r\nbool found = false;\r\nlist_for_each_entry(desc, &chan->active, node)\r\nif (desc->async_tx.cookie == cookie) {\r\nfound = true;\r\nbreak;\r\n}\r\nif (!found)\r\nlist_for_each_entry(desc, &chan->queued, node)\r\nif (desc->async_tx.cookie == cookie) {\r\nfound = true;\r\nbreak;\r\n}\r\nstate->residue = found ? desc->length : 0;\r\n}\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nif (chan->paused)\r\nstatus = DMA_PAUSED;\r\nreturn status;\r\n}\r\nstatic dma_cookie_t nbpf_tx_submit(struct dma_async_tx_descriptor *tx)\r\n{\r\nstruct nbpf_desc *desc = container_of(tx, struct nbpf_desc, async_tx);\r\nstruct nbpf_channel *chan = desc->chan;\r\nunsigned long flags;\r\ndma_cookie_t cookie;\r\nspin_lock_irqsave(&chan->lock, flags);\r\ncookie = dma_cookie_assign(tx);\r\nlist_add_tail(&desc->node, &chan->queued);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\ndev_dbg(chan->dma_chan.device->dev, "Entry %s(%d)\n", __func__, cookie);\r\nreturn cookie;\r\n}\r\nstatic int nbpf_desc_page_alloc(struct nbpf_channel *chan)\r\n{\r\nstruct dma_chan *dchan = &chan->dma_chan;\r\nstruct nbpf_desc_page *dpage = (void *)get_zeroed_page(GFP_KERNEL | GFP_DMA);\r\nstruct nbpf_link_desc *ldesc;\r\nstruct nbpf_link_reg *hwdesc;\r\nstruct nbpf_desc *desc;\r\nLIST_HEAD(head);\r\nLIST_HEAD(lhead);\r\nint i;\r\nstruct device *dev = dchan->device->dev;\r\nif (!dpage)\r\nreturn -ENOMEM;\r\ndev_dbg(dev, "%s(): alloc %lu descriptors, %lu segments, total alloc %zu\n",\r\n__func__, NBPF_DESCS_PER_PAGE, NBPF_SEGMENTS_PER_PAGE, sizeof(*dpage));\r\nfor (i = 0, ldesc = dpage->ldesc, hwdesc = dpage->hwdesc;\r\ni < ARRAY_SIZE(dpage->ldesc);\r\ni++, ldesc++, hwdesc++) {\r\nldesc->hwdesc = hwdesc;\r\nlist_add_tail(&ldesc->node, &lhead);\r\nldesc->hwdesc_dma_addr = dma_map_single(dchan->device->dev,\r\nhwdesc, sizeof(*hwdesc), DMA_TO_DEVICE);\r\ndev_dbg(dev, "%s(): mapped 0x%p to %pad\n", __func__,\r\nhwdesc, &ldesc->hwdesc_dma_addr);\r\n}\r\nfor (i = 0, desc = dpage->desc;\r\ni < ARRAY_SIZE(dpage->desc);\r\ni++, desc++) {\r\ndma_async_tx_descriptor_init(&desc->async_tx, dchan);\r\ndesc->async_tx.tx_submit = nbpf_tx_submit;\r\ndesc->chan = chan;\r\nINIT_LIST_HEAD(&desc->sg);\r\nlist_add_tail(&desc->node, &head);\r\n}\r\nspin_lock_irq(&chan->lock);\r\nlist_splice_tail(&lhead, &chan->free_links);\r\nlist_splice_tail(&head, &chan->free);\r\nlist_add(&dpage->node, &chan->desc_page);\r\nspin_unlock_irq(&chan->lock);\r\nreturn ARRAY_SIZE(dpage->desc);\r\n}\r\nstatic void nbpf_desc_put(struct nbpf_desc *desc)\r\n{\r\nstruct nbpf_channel *chan = desc->chan;\r\nstruct nbpf_link_desc *ldesc, *tmp;\r\nunsigned long flags;\r\nspin_lock_irqsave(&chan->lock, flags);\r\nlist_for_each_entry_safe(ldesc, tmp, &desc->sg, node)\r\nlist_move(&ldesc->node, &chan->free_links);\r\nlist_add(&desc->node, &chan->free);\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\n}\r\nstatic void nbpf_scan_acked(struct nbpf_channel *chan)\r\n{\r\nstruct nbpf_desc *desc, *tmp;\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&chan->lock, flags);\r\nlist_for_each_entry_safe(desc, tmp, &chan->done, node)\r\nif (async_tx_test_ack(&desc->async_tx) && desc->user_wait) {\r\nlist_move(&desc->node, &head);\r\ndesc->user_wait = false;\r\n}\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nlist_for_each_entry_safe(desc, tmp, &head, node) {\r\nlist_del(&desc->node);\r\nnbpf_desc_put(desc);\r\n}\r\n}\r\nstatic struct nbpf_desc *nbpf_desc_get(struct nbpf_channel *chan, size_t len)\r\n{\r\nstruct nbpf_desc *desc = NULL;\r\nstruct nbpf_link_desc *ldesc, *prev = NULL;\r\nnbpf_scan_acked(chan);\r\nspin_lock_irq(&chan->lock);\r\ndo {\r\nint i = 0, ret;\r\nif (list_empty(&chan->free)) {\r\nspin_unlock_irq(&chan->lock);\r\nret = nbpf_desc_page_alloc(chan);\r\nif (ret < 0)\r\nreturn NULL;\r\nspin_lock_irq(&chan->lock);\r\ncontinue;\r\n}\r\ndesc = list_first_entry(&chan->free, struct nbpf_desc, node);\r\nlist_del(&desc->node);\r\ndo {\r\nif (list_empty(&chan->free_links)) {\r\nspin_unlock_irq(&chan->lock);\r\nret = nbpf_desc_page_alloc(chan);\r\nif (ret < 0) {\r\nnbpf_desc_put(desc);\r\nreturn NULL;\r\n}\r\nspin_lock_irq(&chan->lock);\r\ncontinue;\r\n}\r\nldesc = list_first_entry(&chan->free_links,\r\nstruct nbpf_link_desc, node);\r\nldesc->desc = desc;\r\nif (prev)\r\nprev->hwdesc->next = (u32)ldesc->hwdesc_dma_addr;\r\nprev = ldesc;\r\nlist_move_tail(&ldesc->node, &desc->sg);\r\ni++;\r\n} while (i < len);\r\n} while (!desc);\r\nprev->hwdesc->next = 0;\r\nspin_unlock_irq(&chan->lock);\r\nreturn desc;\r\n}\r\nstatic void nbpf_chan_idle(struct nbpf_channel *chan)\r\n{\r\nstruct nbpf_desc *desc, *tmp;\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&chan->lock, flags);\r\nlist_splice_init(&chan->done, &head);\r\nlist_splice_init(&chan->active, &head);\r\nlist_splice_init(&chan->queued, &head);\r\nchan->running = NULL;\r\nspin_unlock_irqrestore(&chan->lock, flags);\r\nlist_for_each_entry_safe(desc, tmp, &head, node) {\r\ndev_dbg(chan->nbpf->dma_dev.dev, "%s(): force-free desc %p cookie %d\n",\r\n__func__, desc, desc->async_tx.cookie);\r\nlist_del(&desc->node);\r\nnbpf_desc_put(desc);\r\n}\r\n}\r\nstatic int nbpf_pause(struct dma_chan *dchan)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\ndev_dbg(dchan->device->dev, "Entry %s\n", __func__);\r\nchan->paused = true;\r\nnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_SETSUS);\r\nnbpf_chan_write(chan, NBPF_CHAN_CTRL, NBPF_CHAN_CTRL_CLREN);\r\nreturn 0;\r\n}\r\nstatic int nbpf_terminate_all(struct dma_chan *dchan)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\ndev_dbg(dchan->device->dev, "Entry %s\n", __func__);\r\ndev_dbg(dchan->device->dev, "Terminating\n");\r\nnbpf_chan_halt(chan);\r\nnbpf_chan_idle(chan);\r\nreturn 0;\r\n}\r\nstatic int nbpf_config(struct dma_chan *dchan,\r\nstruct dma_slave_config *config)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\ndev_dbg(dchan->device->dev, "Entry %s\n", __func__);\r\nchan->slave_dst_addr = config->dst_addr;\r\nchan->slave_dst_width = nbpf_xfer_size(chan->nbpf,\r\nconfig->dst_addr_width, 1);\r\nchan->slave_dst_burst = nbpf_xfer_size(chan->nbpf,\r\nconfig->dst_addr_width,\r\nconfig->dst_maxburst);\r\nchan->slave_src_addr = config->src_addr;\r\nchan->slave_src_width = nbpf_xfer_size(chan->nbpf,\r\nconfig->src_addr_width, 1);\r\nchan->slave_src_burst = nbpf_xfer_size(chan->nbpf,\r\nconfig->src_addr_width,\r\nconfig->src_maxburst);\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *nbpf_prep_sg(struct nbpf_channel *chan,\r\nstruct scatterlist *src_sg, struct scatterlist *dst_sg,\r\nsize_t len, enum dma_transfer_direction direction,\r\nunsigned long flags)\r\n{\r\nstruct nbpf_link_desc *ldesc;\r\nstruct scatterlist *mem_sg;\r\nstruct nbpf_desc *desc;\r\nbool inc_src, inc_dst;\r\nsize_t data_len = 0;\r\nint i = 0;\r\nswitch (direction) {\r\ncase DMA_DEV_TO_MEM:\r\nmem_sg = dst_sg;\r\ninc_src = false;\r\ninc_dst = true;\r\nbreak;\r\ncase DMA_MEM_TO_DEV:\r\nmem_sg = src_sg;\r\ninc_src = true;\r\ninc_dst = false;\r\nbreak;\r\ndefault:\r\ncase DMA_MEM_TO_MEM:\r\nmem_sg = src_sg;\r\ninc_src = true;\r\ninc_dst = true;\r\n}\r\ndesc = nbpf_desc_get(chan, len);\r\nif (!desc)\r\nreturn NULL;\r\ndesc->async_tx.flags = flags;\r\ndesc->async_tx.cookie = -EBUSY;\r\ndesc->user_wait = false;\r\nlist_for_each_entry(ldesc, &desc->sg, node) {\r\nint ret = nbpf_prep_one(ldesc, direction,\r\nsg_dma_address(src_sg),\r\nsg_dma_address(dst_sg),\r\nsg_dma_len(mem_sg),\r\ni == len - 1);\r\nif (ret < 0) {\r\nnbpf_desc_put(desc);\r\nreturn NULL;\r\n}\r\ndata_len += sg_dma_len(mem_sg);\r\nif (inc_src)\r\nsrc_sg = sg_next(src_sg);\r\nif (inc_dst)\r\ndst_sg = sg_next(dst_sg);\r\nmem_sg = direction == DMA_DEV_TO_MEM ? dst_sg : src_sg;\r\ni++;\r\n}\r\ndesc->length = data_len;\r\nreturn &desc->async_tx;\r\n}\r\nstatic struct dma_async_tx_descriptor *nbpf_prep_memcpy(\r\nstruct dma_chan *dchan, dma_addr_t dst, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\nstruct scatterlist dst_sg;\r\nstruct scatterlist src_sg;\r\nsg_init_table(&dst_sg, 1);\r\nsg_init_table(&src_sg, 1);\r\nsg_dma_address(&dst_sg) = dst;\r\nsg_dma_address(&src_sg) = src;\r\nsg_dma_len(&dst_sg) = len;\r\nsg_dma_len(&src_sg) = len;\r\ndev_dbg(dchan->device->dev, "%s(): %zu @ %pad -> %pad\n",\r\n__func__, len, &src, &dst);\r\nreturn nbpf_prep_sg(chan, &src_sg, &dst_sg, 1,\r\nDMA_MEM_TO_MEM, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *nbpf_prep_memcpy_sg(\r\nstruct dma_chan *dchan,\r\nstruct scatterlist *dst_sg, unsigned int dst_nents,\r\nstruct scatterlist *src_sg, unsigned int src_nents,\r\nunsigned long flags)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\nif (dst_nents != src_nents)\r\nreturn NULL;\r\nreturn nbpf_prep_sg(chan, src_sg, dst_sg, src_nents,\r\nDMA_MEM_TO_MEM, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *nbpf_prep_slave_sg(\r\nstruct dma_chan *dchan, struct scatterlist *sgl, unsigned int sg_len,\r\nenum dma_transfer_direction direction, unsigned long flags, void *context)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\nstruct scatterlist slave_sg;\r\ndev_dbg(dchan->device->dev, "Entry %s()\n", __func__);\r\nsg_init_table(&slave_sg, 1);\r\nswitch (direction) {\r\ncase DMA_MEM_TO_DEV:\r\nsg_dma_address(&slave_sg) = chan->slave_dst_addr;\r\nreturn nbpf_prep_sg(chan, sgl, &slave_sg, sg_len,\r\ndirection, flags);\r\ncase DMA_DEV_TO_MEM:\r\nsg_dma_address(&slave_sg) = chan->slave_src_addr;\r\nreturn nbpf_prep_sg(chan, &slave_sg, sgl, sg_len,\r\ndirection, flags);\r\ndefault:\r\nreturn NULL;\r\n}\r\n}\r\nstatic int nbpf_alloc_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\nint ret;\r\nINIT_LIST_HEAD(&chan->free);\r\nINIT_LIST_HEAD(&chan->free_links);\r\nINIT_LIST_HEAD(&chan->queued);\r\nINIT_LIST_HEAD(&chan->active);\r\nINIT_LIST_HEAD(&chan->done);\r\nret = nbpf_desc_page_alloc(chan);\r\nif (ret < 0)\r\nreturn ret;\r\ndev_dbg(dchan->device->dev, "Entry %s(): terminal %u\n", __func__,\r\nchan->terminal);\r\nnbpf_chan_configure(chan);\r\nreturn ret;\r\n}\r\nstatic void nbpf_free_chan_resources(struct dma_chan *dchan)\r\n{\r\nstruct nbpf_channel *chan = nbpf_to_chan(dchan);\r\nstruct nbpf_desc_page *dpage, *tmp;\r\ndev_dbg(dchan->device->dev, "Entry %s()\n", __func__);\r\nnbpf_chan_halt(chan);\r\nnbpf_chan_idle(chan);\r\nnbpf_chan_prepare_default(chan);\r\nlist_for_each_entry_safe(dpage, tmp, &chan->desc_page, node) {\r\nstruct nbpf_link_desc *ldesc;\r\nint i;\r\nlist_del(&dpage->node);\r\nfor (i = 0, ldesc = dpage->ldesc;\r\ni < ARRAY_SIZE(dpage->ldesc);\r\ni++, ldesc++)\r\ndma_unmap_single(dchan->device->dev, ldesc->hwdesc_dma_addr,\r\nsizeof(*ldesc->hwdesc), DMA_TO_DEVICE);\r\nfree_page((unsigned long)dpage);\r\n}\r\n}\r\nstatic struct dma_chan *nbpf_of_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct nbpf_device *nbpf = ofdma->of_dma_data;\r\nstruct dma_chan *dchan;\r\nstruct nbpf_channel *chan;\r\nif (dma_spec->args_count != 2)\r\nreturn NULL;\r\ndchan = dma_get_any_slave_channel(&nbpf->dma_dev);\r\nif (!dchan)\r\nreturn NULL;\r\ndev_dbg(dchan->device->dev, "Entry %s(%s)\n", __func__,\r\ndma_spec->np->name);\r\nchan = nbpf_to_chan(dchan);\r\nchan->terminal = dma_spec->args[0];\r\nchan->flags = dma_spec->args[1];\r\nnbpf_chan_prepare(chan);\r\nnbpf_chan_configure(chan);\r\nreturn dchan;\r\n}\r\nstatic void nbpf_chan_tasklet(unsigned long data)\r\n{\r\nstruct nbpf_channel *chan = (struct nbpf_channel *)data;\r\nstruct nbpf_desc *desc, *tmp;\r\nstruct dmaengine_desc_callback cb;\r\nwhile (!list_empty(&chan->done)) {\r\nbool found = false, must_put, recycling = false;\r\nspin_lock_irq(&chan->lock);\r\nlist_for_each_entry_safe(desc, tmp, &chan->done, node) {\r\nif (!desc->user_wait) {\r\nfound = true;\r\nbreak;\r\n} else if (async_tx_test_ack(&desc->async_tx)) {\r\nlist_del(&desc->node);\r\nspin_unlock_irq(&chan->lock);\r\nnbpf_desc_put(desc);\r\nrecycling = true;\r\nbreak;\r\n}\r\n}\r\nif (recycling)\r\ncontinue;\r\nif (!found) {\r\nspin_unlock_irq(&chan->lock);\r\nbreak;\r\n}\r\ndma_cookie_complete(&desc->async_tx);\r\nif (async_tx_test_ack(&desc->async_tx)) {\r\nlist_del(&desc->node);\r\nmust_put = true;\r\n} else {\r\ndesc->user_wait = true;\r\nmust_put = false;\r\n}\r\ndmaengine_desc_get_callback(&desc->async_tx, &cb);\r\nspin_unlock_irq(&chan->lock);\r\ndmaengine_desc_callback_invoke(&cb, NULL);\r\nif (must_put)\r\nnbpf_desc_put(desc);\r\n}\r\n}\r\nstatic irqreturn_t nbpf_chan_irq(int irq, void *dev)\r\n{\r\nstruct nbpf_channel *chan = dev;\r\nbool done = nbpf_status_get(chan);\r\nstruct nbpf_desc *desc;\r\nirqreturn_t ret;\r\nbool bh = false;\r\nif (!done)\r\nreturn IRQ_NONE;\r\nnbpf_status_ack(chan);\r\ndev_dbg(&chan->dma_chan.dev->device, "%s()\n", __func__);\r\nspin_lock(&chan->lock);\r\ndesc = chan->running;\r\nif (WARN_ON(!desc)) {\r\nret = IRQ_NONE;\r\ngoto unlock;\r\n} else {\r\nret = IRQ_HANDLED;\r\nbh = true;\r\n}\r\nlist_move_tail(&desc->node, &chan->done);\r\nchan->running = NULL;\r\nif (!list_empty(&chan->active)) {\r\ndesc = list_first_entry(&chan->active,\r\nstruct nbpf_desc, node);\r\nif (!nbpf_start(desc))\r\nchan->running = desc;\r\n}\r\nunlock:\r\nspin_unlock(&chan->lock);\r\nif (bh)\r\ntasklet_schedule(&chan->tasklet);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t nbpf_err_irq(int irq, void *dev)\r\n{\r\nstruct nbpf_device *nbpf = dev;\r\nu32 error = nbpf_error_get(nbpf);\r\ndev_warn(nbpf->dma_dev.dev, "DMA error IRQ %u\n", irq);\r\nif (!error)\r\nreturn IRQ_NONE;\r\ndo {\r\nstruct nbpf_channel *chan = nbpf_error_get_channel(nbpf, error);\r\nnbpf_error_clear(chan);\r\nnbpf_chan_idle(chan);\r\nerror = nbpf_error_get(nbpf);\r\n} while (error);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int nbpf_chan_probe(struct nbpf_device *nbpf, int n)\r\n{\r\nstruct dma_device *dma_dev = &nbpf->dma_dev;\r\nstruct nbpf_channel *chan = nbpf->chan + n;\r\nint ret;\r\nchan->nbpf = nbpf;\r\nchan->base = nbpf->base + NBPF_REG_CHAN_OFFSET + NBPF_REG_CHAN_SIZE * n;\r\nINIT_LIST_HEAD(&chan->desc_page);\r\nspin_lock_init(&chan->lock);\r\nchan->dma_chan.device = dma_dev;\r\ndma_cookie_init(&chan->dma_chan);\r\nnbpf_chan_prepare_default(chan);\r\ndev_dbg(dma_dev->dev, "%s(): channel %d: -> %p\n", __func__, n, chan->base);\r\nsnprintf(chan->name, sizeof(chan->name), "nbpf %d", n);\r\ntasklet_init(&chan->tasklet, nbpf_chan_tasklet, (unsigned long)chan);\r\nret = devm_request_irq(dma_dev->dev, chan->irq,\r\nnbpf_chan_irq, IRQF_SHARED,\r\nchan->name, chan);\r\nif (ret < 0)\r\nreturn ret;\r\nlist_add_tail(&chan->dma_chan.device_node,\r\n&dma_dev->channels);\r\nreturn 0;\r\n}\r\nstatic int nbpf_probe(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nconst struct of_device_id *of_id = of_match_device(nbpf_match, dev);\r\nstruct device_node *np = dev->of_node;\r\nstruct nbpf_device *nbpf;\r\nstruct dma_device *dma_dev;\r\nstruct resource *iomem, *irq_res;\r\nconst struct nbpf_config *cfg;\r\nint num_channels;\r\nint ret, irq, eirq, i;\r\nint irqbuf[9] ;\r\nunsigned int irqs = 0;\r\nBUILD_BUG_ON(sizeof(struct nbpf_desc_page) > PAGE_SIZE);\r\nif (!np || !of_id || !of_id->data)\r\nreturn -ENODEV;\r\ncfg = of_id->data;\r\nnum_channels = cfg->num_channels;\r\nnbpf = devm_kzalloc(dev, sizeof(*nbpf) + num_channels *\r\nsizeof(nbpf->chan[0]), GFP_KERNEL);\r\nif (!nbpf)\r\nreturn -ENOMEM;\r\ndma_dev = &nbpf->dma_dev;\r\ndma_dev->dev = dev;\r\niomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nnbpf->base = devm_ioremap_resource(dev, iomem);\r\nif (IS_ERR(nbpf->base))\r\nreturn PTR_ERR(nbpf->base);\r\nnbpf->clk = devm_clk_get(dev, NULL);\r\nif (IS_ERR(nbpf->clk))\r\nreturn PTR_ERR(nbpf->clk);\r\nof_property_read_u32(np, "max-burst-mem-read",\r\n&nbpf->max_burst_mem_read);\r\nof_property_read_u32(np, "max-burst-mem-write",\r\n&nbpf->max_burst_mem_write);\r\nnbpf->config = cfg;\r\nfor (i = 0; irqs < ARRAY_SIZE(irqbuf); i++) {\r\nirq_res = platform_get_resource(pdev, IORESOURCE_IRQ, i);\r\nif (!irq_res)\r\nbreak;\r\nfor (irq = irq_res->start; irq <= irq_res->end;\r\nirq++, irqs++)\r\nirqbuf[irqs] = irq;\r\n}\r\nif (irqs != 1 && irqs != 2 && irqs != num_channels + 1)\r\nreturn -ENXIO;\r\nif (irqs == 1) {\r\neirq = irqbuf[0];\r\nfor (i = 0; i <= num_channels; i++)\r\nnbpf->chan[i].irq = irqbuf[0];\r\n} else {\r\neirq = platform_get_irq_byname(pdev, "error");\r\nif (eirq < 0)\r\nreturn eirq;\r\nif (irqs == num_channels + 1) {\r\nstruct nbpf_channel *chan;\r\nfor (i = 0, chan = nbpf->chan; i <= num_channels;\r\ni++, chan++) {\r\nif (irqbuf[i] == eirq)\r\ni++;\r\nchan->irq = irqbuf[i];\r\n}\r\nif (chan != nbpf->chan + num_channels)\r\nreturn -EINVAL;\r\n} else {\r\nif (irqbuf[0] == eirq)\r\nirq = irqbuf[1];\r\nelse\r\nirq = irqbuf[0];\r\nfor (i = 0; i <= num_channels; i++)\r\nnbpf->chan[i].irq = irq;\r\n}\r\n}\r\nret = devm_request_irq(dev, eirq, nbpf_err_irq,\r\nIRQF_SHARED, "dma error", nbpf);\r\nif (ret < 0)\r\nreturn ret;\r\nnbpf->eirq = eirq;\r\nINIT_LIST_HEAD(&dma_dev->channels);\r\nfor (i = 0; i < num_channels; i++) {\r\nret = nbpf_chan_probe(nbpf, i);\r\nif (ret < 0)\r\nreturn ret;\r\n}\r\ndma_cap_set(DMA_MEMCPY, dma_dev->cap_mask);\r\ndma_cap_set(DMA_SLAVE, dma_dev->cap_mask);\r\ndma_cap_set(DMA_PRIVATE, dma_dev->cap_mask);\r\ndma_cap_set(DMA_SG, dma_dev->cap_mask);\r\ndma_dev->device_alloc_chan_resources\r\n= nbpf_alloc_chan_resources;\r\ndma_dev->device_free_chan_resources = nbpf_free_chan_resources;\r\ndma_dev->device_prep_dma_sg = nbpf_prep_memcpy_sg;\r\ndma_dev->device_prep_dma_memcpy = nbpf_prep_memcpy;\r\ndma_dev->device_tx_status = nbpf_tx_status;\r\ndma_dev->device_issue_pending = nbpf_issue_pending;\r\ndma_dev->device_prep_slave_sg = nbpf_prep_slave_sg;\r\ndma_dev->device_config = nbpf_config;\r\ndma_dev->device_pause = nbpf_pause;\r\ndma_dev->device_terminate_all = nbpf_terminate_all;\r\ndma_dev->src_addr_widths = NBPF_DMA_BUSWIDTHS;\r\ndma_dev->dst_addr_widths = NBPF_DMA_BUSWIDTHS;\r\ndma_dev->directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV);\r\nplatform_set_drvdata(pdev, nbpf);\r\nret = clk_prepare_enable(nbpf->clk);\r\nif (ret < 0)\r\nreturn ret;\r\nnbpf_configure(nbpf);\r\nret = dma_async_device_register(dma_dev);\r\nif (ret < 0)\r\ngoto e_clk_off;\r\nret = of_dma_controller_register(np, nbpf_of_xlate, nbpf);\r\nif (ret < 0)\r\ngoto e_dma_dev_unreg;\r\nreturn 0;\r\ne_dma_dev_unreg:\r\ndma_async_device_unregister(dma_dev);\r\ne_clk_off:\r\nclk_disable_unprepare(nbpf->clk);\r\nreturn ret;\r\n}\r\nstatic int nbpf_remove(struct platform_device *pdev)\r\n{\r\nstruct nbpf_device *nbpf = platform_get_drvdata(pdev);\r\nint i;\r\ndevm_free_irq(&pdev->dev, nbpf->eirq, nbpf);\r\nfor (i = 0; i < nbpf->config->num_channels; i++) {\r\nstruct nbpf_channel *chan = nbpf->chan + i;\r\ndevm_free_irq(&pdev->dev, chan->irq, chan);\r\ntasklet_kill(&chan->tasklet);\r\n}\r\nof_dma_controller_free(pdev->dev.of_node);\r\ndma_async_device_unregister(&nbpf->dma_dev);\r\nclk_disable_unprepare(nbpf->clk);\r\nreturn 0;\r\n}\r\nstatic int nbpf_runtime_suspend(struct device *dev)\r\n{\r\nstruct nbpf_device *nbpf = platform_get_drvdata(to_platform_device(dev));\r\nclk_disable_unprepare(nbpf->clk);\r\nreturn 0;\r\n}\r\nstatic int nbpf_runtime_resume(struct device *dev)\r\n{\r\nstruct nbpf_device *nbpf = platform_get_drvdata(to_platform_device(dev));\r\nreturn clk_prepare_enable(nbpf->clk);\r\n}
