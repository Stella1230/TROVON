static int sha1_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, sha1_transform_fn *sha1_xform)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\nif (!irq_fpu_usable() ||\r\n(sctx->count % SHA1_BLOCK_SIZE) + len < SHA1_BLOCK_SIZE)\r\nreturn crypto_sha1_update(desc, data, len);\r\nBUILD_BUG_ON(offsetof(struct sha1_state, state) != 0);\r\nkernel_fpu_begin();\r\nsha1_base_do_update(desc, data, len,\r\n(sha1_block_fn *)sha1_xform);\r\nkernel_fpu_end();\r\nreturn 0;\r\n}\r\nstatic int sha1_finup(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, u8 *out, sha1_transform_fn *sha1_xform)\r\n{\r\nif (!irq_fpu_usable())\r\nreturn crypto_sha1_finup(desc, data, len, out);\r\nkernel_fpu_begin();\r\nif (len)\r\nsha1_base_do_update(desc, data, len,\r\n(sha1_block_fn *)sha1_xform);\r\nsha1_base_do_finalize(desc, (sha1_block_fn *)sha1_xform);\r\nkernel_fpu_end();\r\nreturn sha1_base_finish(desc, out);\r\n}\r\nstatic int sha1_ssse3_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nreturn sha1_update(desc, data, len,\r\n(sha1_transform_fn *) sha1_transform_ssse3);\r\n}\r\nstatic int sha1_ssse3_finup(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, u8 *out)\r\n{\r\nreturn sha1_finup(desc, data, len, out,\r\n(sha1_transform_fn *) sha1_transform_ssse3);\r\n}\r\nstatic int sha1_ssse3_final(struct shash_desc *desc, u8 *out)\r\n{\r\nreturn sha1_ssse3_finup(desc, NULL, 0, out);\r\n}\r\nstatic int register_sha1_ssse3(void)\r\n{\r\nif (boot_cpu_has(X86_FEATURE_SSSE3))\r\nreturn crypto_register_shash(&sha1_ssse3_alg);\r\nreturn 0;\r\n}\r\nstatic void unregister_sha1_ssse3(void)\r\n{\r\nif (boot_cpu_has(X86_FEATURE_SSSE3))\r\ncrypto_unregister_shash(&sha1_ssse3_alg);\r\n}\r\nstatic int sha1_avx_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nreturn sha1_update(desc, data, len,\r\n(sha1_transform_fn *) sha1_transform_avx);\r\n}\r\nstatic int sha1_avx_finup(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, u8 *out)\r\n{\r\nreturn sha1_finup(desc, data, len, out,\r\n(sha1_transform_fn *) sha1_transform_avx);\r\n}\r\nstatic int sha1_avx_final(struct shash_desc *desc, u8 *out)\r\n{\r\nreturn sha1_avx_finup(desc, NULL, 0, out);\r\n}\r\nstatic bool avx_usable(void)\r\n{\r\nif (!cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM, NULL)) {\r\nif (boot_cpu_has(X86_FEATURE_AVX))\r\npr_info("AVX detected but unusable.\n");\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int register_sha1_avx(void)\r\n{\r\nif (avx_usable())\r\nreturn crypto_register_shash(&sha1_avx_alg);\r\nreturn 0;\r\n}\r\nstatic void unregister_sha1_avx(void)\r\n{\r\nif (avx_usable())\r\ncrypto_unregister_shash(&sha1_avx_alg);\r\n}\r\nstatic inline int register_sha1_avx(void) { return 0; }\r\nstatic inline void unregister_sha1_avx(void) { }\r\nstatic bool avx2_usable(void)\r\n{\r\nif (avx_usable() && boot_cpu_has(X86_FEATURE_AVX2)\r\n&& boot_cpu_has(X86_FEATURE_BMI1)\r\n&& boot_cpu_has(X86_FEATURE_BMI2))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void sha1_apply_transform_avx2(u32 *digest, const char *data,\r\nunsigned int rounds)\r\n{\r\nif (rounds >= SHA1_AVX2_BLOCK_OPTSIZE)\r\nsha1_transform_avx2(digest, data, rounds);\r\nelse\r\nsha1_transform_avx(digest, data, rounds);\r\n}\r\nstatic int sha1_avx2_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nreturn sha1_update(desc, data, len,\r\n(sha1_transform_fn *) sha1_apply_transform_avx2);\r\n}\r\nstatic int sha1_avx2_finup(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, u8 *out)\r\n{\r\nreturn sha1_finup(desc, data, len, out,\r\n(sha1_transform_fn *) sha1_apply_transform_avx2);\r\n}\r\nstatic int sha1_avx2_final(struct shash_desc *desc, u8 *out)\r\n{\r\nreturn sha1_avx2_finup(desc, NULL, 0, out);\r\n}\r\nstatic int register_sha1_avx2(void)\r\n{\r\nif (avx2_usable())\r\nreturn crypto_register_shash(&sha1_avx2_alg);\r\nreturn 0;\r\n}\r\nstatic void unregister_sha1_avx2(void)\r\n{\r\nif (avx2_usable())\r\ncrypto_unregister_shash(&sha1_avx2_alg);\r\n}\r\nstatic inline int register_sha1_avx2(void) { return 0; }\r\nstatic inline void unregister_sha1_avx2(void) { }\r\nstatic int sha1_ni_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nreturn sha1_update(desc, data, len,\r\n(sha1_transform_fn *) sha1_ni_transform);\r\n}\r\nstatic int sha1_ni_finup(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, u8 *out)\r\n{\r\nreturn sha1_finup(desc, data, len, out,\r\n(sha1_transform_fn *) sha1_ni_transform);\r\n}\r\nstatic int sha1_ni_final(struct shash_desc *desc, u8 *out)\r\n{\r\nreturn sha1_ni_finup(desc, NULL, 0, out);\r\n}\r\nstatic int register_sha1_ni(void)\r\n{\r\nif (boot_cpu_has(X86_FEATURE_SHA_NI))\r\nreturn crypto_register_shash(&sha1_ni_alg);\r\nreturn 0;\r\n}\r\nstatic void unregister_sha1_ni(void)\r\n{\r\nif (boot_cpu_has(X86_FEATURE_SHA_NI))\r\ncrypto_unregister_shash(&sha1_ni_alg);\r\n}\r\nstatic inline int register_sha1_ni(void) { return 0; }\r\nstatic inline void unregister_sha1_ni(void) { }\r\nstatic int __init sha1_ssse3_mod_init(void)\r\n{\r\nif (register_sha1_ssse3())\r\ngoto fail;\r\nif (register_sha1_avx()) {\r\nunregister_sha1_ssse3();\r\ngoto fail;\r\n}\r\nif (register_sha1_avx2()) {\r\nunregister_sha1_avx();\r\nunregister_sha1_ssse3();\r\ngoto fail;\r\n}\r\nif (register_sha1_ni()) {\r\nunregister_sha1_avx2();\r\nunregister_sha1_avx();\r\nunregister_sha1_ssse3();\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nreturn -ENODEV;\r\n}\r\nstatic void __exit sha1_ssse3_mod_fini(void)\r\n{\r\nunregister_sha1_ni();\r\nunregister_sha1_avx2();\r\nunregister_sha1_avx();\r\nunregister_sha1_ssse3();\r\n}
