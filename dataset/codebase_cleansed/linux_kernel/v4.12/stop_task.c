static int\r\nselect_task_rq_stop(struct task_struct *p, int cpu, int sd_flag, int flags)\r\n{\r\nreturn task_cpu(p);\r\n}\r\nstatic void\r\ncheck_preempt_curr_stop(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\n}\r\nstatic struct task_struct *\r\npick_next_task_stop(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\r\n{\r\nstruct task_struct *stop = rq->stop;\r\nif (!stop || !task_on_rq_queued(stop))\r\nreturn NULL;\r\nput_prev_task(rq, prev);\r\nstop->se.exec_start = rq_clock_task(rq);\r\nreturn stop;\r\n}\r\nstatic void\r\nenqueue_task_stop(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nadd_nr_running(rq, 1);\r\n}\r\nstatic void\r\ndequeue_task_stop(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nsub_nr_running(rq, 1);\r\n}\r\nstatic void yield_task_stop(struct rq *rq)\r\n{\r\nBUG();\r\n}\r\nstatic void put_prev_task_stop(struct rq *rq, struct task_struct *prev)\r\n{\r\nstruct task_struct *curr = rq->curr;\r\nu64 delta_exec;\r\ndelta_exec = rq_clock_task(rq) - curr->se.exec_start;\r\nif (unlikely((s64)delta_exec < 0))\r\ndelta_exec = 0;\r\nschedstat_set(curr->se.statistics.exec_max,\r\nmax(curr->se.statistics.exec_max, delta_exec));\r\ncurr->se.sum_exec_runtime += delta_exec;\r\naccount_group_exec_runtime(curr, delta_exec);\r\ncurr->se.exec_start = rq_clock_task(rq);\r\ncpuacct_charge(curr, delta_exec);\r\n}\r\nstatic void task_tick_stop(struct rq *rq, struct task_struct *curr, int queued)\r\n{\r\n}\r\nstatic void set_curr_task_stop(struct rq *rq)\r\n{\r\nstruct task_struct *stop = rq->stop;\r\nstop->se.exec_start = rq_clock_task(rq);\r\n}\r\nstatic void switched_to_stop(struct rq *rq, struct task_struct *p)\r\n{\r\nBUG();\r\n}\r\nstatic void\r\nprio_changed_stop(struct rq *rq, struct task_struct *p, int oldprio)\r\n{\r\nBUG();\r\n}\r\nstatic unsigned int\r\nget_rr_interval_stop(struct rq *rq, struct task_struct *task)\r\n{\r\nreturn 0;\r\n}\r\nstatic void update_curr_stop(struct rq *rq)\r\n{\r\n}
