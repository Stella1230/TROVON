static struct aq_ring_s *aq_ring_alloc(struct aq_ring_s *self,\r\nstruct aq_nic_s *aq_nic)\r\n{\r\nint err = 0;\r\nself->buff_ring =\r\nkcalloc(self->size, sizeof(struct aq_ring_buff_s), GFP_KERNEL);\r\nif (!self->buff_ring) {\r\nerr = -ENOMEM;\r\ngoto err_exit;\r\n}\r\nself->dx_ring = dma_alloc_coherent(aq_nic_get_dev(aq_nic),\r\nself->size * self->dx_size,\r\n&self->dx_ring_pa, GFP_KERNEL);\r\nif (!self->dx_ring) {\r\nerr = -ENOMEM;\r\ngoto err_exit;\r\n}\r\nerr_exit:\r\nif (err < 0) {\r\naq_ring_free(self);\r\nself = NULL;\r\n}\r\nreturn self;\r\n}\r\nstruct aq_ring_s *aq_ring_tx_alloc(struct aq_ring_s *self,\r\nstruct aq_nic_s *aq_nic,\r\nunsigned int idx,\r\nstruct aq_nic_cfg_s *aq_nic_cfg)\r\n{\r\nint err = 0;\r\nself->aq_nic = aq_nic;\r\nself->idx = idx;\r\nself->size = aq_nic_cfg->txds;\r\nself->dx_size = aq_nic_cfg->aq_hw_caps->txd_size;\r\nself = aq_ring_alloc(self, aq_nic);\r\nif (!self) {\r\nerr = -ENOMEM;\r\ngoto err_exit;\r\n}\r\nerr_exit:\r\nif (err < 0) {\r\naq_ring_free(self);\r\nself = NULL;\r\n}\r\nreturn self;\r\n}\r\nstruct aq_ring_s *aq_ring_rx_alloc(struct aq_ring_s *self,\r\nstruct aq_nic_s *aq_nic,\r\nunsigned int idx,\r\nstruct aq_nic_cfg_s *aq_nic_cfg)\r\n{\r\nint err = 0;\r\nself->aq_nic = aq_nic;\r\nself->idx = idx;\r\nself->size = aq_nic_cfg->rxds;\r\nself->dx_size = aq_nic_cfg->aq_hw_caps->rxd_size;\r\nself = aq_ring_alloc(self, aq_nic);\r\nif (!self) {\r\nerr = -ENOMEM;\r\ngoto err_exit;\r\n}\r\nerr_exit:\r\nif (err < 0) {\r\naq_ring_free(self);\r\nself = NULL;\r\n}\r\nreturn self;\r\n}\r\nint aq_ring_init(struct aq_ring_s *self)\r\n{\r\nself->hw_head = 0;\r\nself->sw_head = 0;\r\nself->sw_tail = 0;\r\nspin_lock_init(&self->header.lock);\r\nreturn 0;\r\n}\r\nvoid aq_ring_tx_clean(struct aq_ring_s *self)\r\n{\r\nstruct device *dev = aq_nic_get_dev(self->aq_nic);\r\nfor (; self->sw_head != self->hw_head;\r\nself->sw_head = aq_ring_next_dx(self, self->sw_head)) {\r\nstruct aq_ring_buff_s *buff = &self->buff_ring[self->sw_head];\r\nif (likely(buff->is_mapped)) {\r\nif (unlikely(buff->is_sop))\r\ndma_unmap_single(dev, buff->pa, buff->len,\r\nDMA_TO_DEVICE);\r\nelse\r\ndma_unmap_page(dev, buff->pa, buff->len,\r\nDMA_TO_DEVICE);\r\n}\r\nif (unlikely(buff->is_eop))\r\ndev_kfree_skb_any(buff->skb);\r\n}\r\n}\r\nstatic inline unsigned int aq_ring_dx_in_range(unsigned int h, unsigned int i,\r\nunsigned int t)\r\n{\r\nreturn (h < t) ? ((h < i) && (i < t)) : ((h < i) || (i < t));\r\n}\r\nint aq_ring_rx_clean(struct aq_ring_s *self, int *work_done, int budget)\r\n{\r\nstruct net_device *ndev = aq_nic_get_ndev(self->aq_nic);\r\nint err = 0;\r\nbool is_rsc_completed = true;\r\nfor (; (self->sw_head != self->hw_head) && budget;\r\nself->sw_head = aq_ring_next_dx(self, self->sw_head),\r\n--budget, ++(*work_done)) {\r\nstruct aq_ring_buff_s *buff = &self->buff_ring[self->sw_head];\r\nstruct sk_buff *skb = NULL;\r\nunsigned int next_ = 0U;\r\nunsigned int i = 0U;\r\nstruct aq_ring_buff_s *buff_ = NULL;\r\nif (buff->is_error) {\r\n__free_pages(buff->page, 0);\r\ncontinue;\r\n}\r\nif (buff->is_cleaned)\r\ncontinue;\r\nif (!buff->is_eop) {\r\nfor (next_ = buff->next,\r\nbuff_ = &self->buff_ring[next_]; true;\r\nnext_ = buff_->next,\r\nbuff_ = &self->buff_ring[next_]) {\r\nis_rsc_completed =\r\naq_ring_dx_in_range(self->sw_head,\r\nnext_,\r\nself->hw_head);\r\nif (unlikely(!is_rsc_completed)) {\r\nis_rsc_completed = false;\r\nbreak;\r\n}\r\nif (buff_->is_eop)\r\nbreak;\r\n}\r\nif (!is_rsc_completed) {\r\nerr = 0;\r\ngoto err_exit;\r\n}\r\n}\r\nif (buff->is_eop) {\r\nskb = build_skb(page_address(buff->page),\r\nbuff->len + AQ_SKB_ALIGN);\r\nif (unlikely(!skb)) {\r\nerr = -ENOMEM;\r\ngoto err_exit;\r\n}\r\nskb_put(skb, buff->len);\r\n} else {\r\nskb = netdev_alloc_skb(ndev, ETH_HLEN);\r\nif (unlikely(!skb)) {\r\nerr = -ENOMEM;\r\ngoto err_exit;\r\n}\r\nskb_put(skb, ETH_HLEN);\r\nmemcpy(skb->data, page_address(buff->page), ETH_HLEN);\r\nskb_add_rx_frag(skb, 0, buff->page, ETH_HLEN,\r\nbuff->len - ETH_HLEN,\r\nSKB_TRUESIZE(buff->len - ETH_HLEN));\r\nfor (i = 1U, next_ = buff->next,\r\nbuff_ = &self->buff_ring[next_]; true;\r\nnext_ = buff_->next,\r\nbuff_ = &self->buff_ring[next_], ++i) {\r\nskb_add_rx_frag(skb, i, buff_->page, 0,\r\nbuff_->len,\r\nSKB_TRUESIZE(buff->len -\r\nETH_HLEN));\r\nbuff_->is_cleaned = 1;\r\nif (buff_->is_eop)\r\nbreak;\r\n}\r\n}\r\nskb->protocol = eth_type_trans(skb, ndev);\r\nif (unlikely(buff->is_cso_err)) {\r\n++self->stats.rx.errors;\r\n__skb_mark_checksum_bad(skb);\r\n} else {\r\nif (buff->is_ip_cso) {\r\n__skb_incr_checksum_unnecessary(skb);\r\nif (buff->is_udp_cso || buff->is_tcp_cso)\r\n__skb_incr_checksum_unnecessary(skb);\r\n} else {\r\nskb->ip_summed = CHECKSUM_NONE;\r\n}\r\n}\r\nskb_set_hash(skb, buff->rss_hash,\r\nbuff->is_hash_l4 ? PKT_HASH_TYPE_L4 :\r\nPKT_HASH_TYPE_NONE);\r\nskb_record_rx_queue(skb, self->idx);\r\nnetif_receive_skb(skb);\r\n++self->stats.rx.packets;\r\nself->stats.rx.bytes += skb->len;\r\n}\r\nerr_exit:\r\nreturn err;\r\n}\r\nint aq_ring_rx_fill(struct aq_ring_s *self)\r\n{\r\nunsigned int pages_order = fls(AQ_CFG_RX_FRAME_MAX / PAGE_SIZE +\r\n(AQ_CFG_RX_FRAME_MAX % PAGE_SIZE ? 1 : 0)) - 1;\r\nstruct aq_ring_buff_s *buff = NULL;\r\nint err = 0;\r\nint i = 0;\r\nfor (i = aq_ring_avail_dx(self); i--;\r\nself->sw_tail = aq_ring_next_dx(self, self->sw_tail)) {\r\nbuff = &self->buff_ring[self->sw_tail];\r\nbuff->flags = 0U;\r\nbuff->len = AQ_CFG_RX_FRAME_MAX;\r\nbuff->page = alloc_pages(GFP_ATOMIC | __GFP_COLD |\r\n__GFP_COMP, pages_order);\r\nif (!buff->page) {\r\nerr = -ENOMEM;\r\ngoto err_exit;\r\n}\r\nbuff->pa = dma_map_page(aq_nic_get_dev(self->aq_nic),\r\nbuff->page, 0,\r\nAQ_CFG_RX_FRAME_MAX, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(aq_nic_get_dev(self->aq_nic), buff->pa)) {\r\nerr = -ENOMEM;\r\ngoto err_exit;\r\n}\r\nbuff = NULL;\r\n}\r\nerr_exit:\r\nif (err < 0) {\r\nif (buff && buff->page)\r\n__free_pages(buff->page, 0);\r\n}\r\nreturn err;\r\n}\r\nvoid aq_ring_rx_deinit(struct aq_ring_s *self)\r\n{\r\nif (!self)\r\ngoto err_exit;\r\nfor (; self->sw_head != self->sw_tail;\r\nself->sw_head = aq_ring_next_dx(self, self->sw_head)) {\r\nstruct aq_ring_buff_s *buff = &self->buff_ring[self->sw_head];\r\ndma_unmap_page(aq_nic_get_dev(self->aq_nic), buff->pa,\r\nAQ_CFG_RX_FRAME_MAX, DMA_FROM_DEVICE);\r\n__free_pages(buff->page, 0);\r\n}\r\nerr_exit:;\r\n}\r\nvoid aq_ring_free(struct aq_ring_s *self)\r\n{\r\nif (!self)\r\ngoto err_exit;\r\nkfree(self->buff_ring);\r\nif (self->dx_ring)\r\ndma_free_coherent(aq_nic_get_dev(self->aq_nic),\r\nself->size * self->dx_size, self->dx_ring,\r\nself->dx_ring_pa);\r\nerr_exit:;\r\n}
