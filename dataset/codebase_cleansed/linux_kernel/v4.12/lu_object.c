void lu_object_put(const struct lu_env *env, struct lu_object *o)\r\n{\r\nstruct lu_site_bkt_data *bkt;\r\nstruct lu_object_header *top;\r\nstruct lu_site *site;\r\nstruct lu_object *orig;\r\nstruct cfs_hash_bd bd;\r\nconst struct lu_fid *fid;\r\ntop = o->lo_header;\r\nsite = o->lo_dev->ld_site;\r\norig = o;\r\nfid = lu_object_fid(o);\r\nif (fid_is_zero(fid)) {\r\nLASSERT(!top->loh_hash.next && !top->loh_hash.pprev);\r\nLASSERT(list_empty(&top->loh_lru));\r\nif (!atomic_dec_and_test(&top->loh_ref))\r\nreturn;\r\nlist_for_each_entry_reverse(o, &top->loh_layers, lo_linkage) {\r\nif (o->lo_ops->loo_object_release)\r\no->lo_ops->loo_object_release(env, o);\r\n}\r\nlu_object_free(env, orig);\r\nreturn;\r\n}\r\ncfs_hash_bd_get(site->ls_obj_hash, &top->loh_fid, &bd);\r\nbkt = cfs_hash_bd_extra_get(site->ls_obj_hash, &bd);\r\nif (!cfs_hash_bd_dec_and_lock(site->ls_obj_hash, &bd, &top->loh_ref)) {\r\nif (lu_object_is_dying(top)) {\r\nwake_up_all(&bkt->lsb_marche_funebre);\r\n}\r\nreturn;\r\n}\r\nlist_for_each_entry_reverse(o, &top->loh_layers, lo_linkage) {\r\nif (o->lo_ops->loo_object_release)\r\no->lo_ops->loo_object_release(env, o);\r\n}\r\nif (!lu_object_is_dying(top)) {\r\nLASSERT(list_empty(&top->loh_lru));\r\nlist_add_tail(&top->loh_lru, &bkt->lsb_lru);\r\nbkt->lsb_lru_len++;\r\npercpu_counter_inc(&site->ls_lru_len_counter);\r\nCDEBUG(D_INODE, "Add %p to site lru. hash: %p, bkt: %p, lru_len: %ld\n",\r\no, site->ls_obj_hash, bkt, bkt->lsb_lru_len);\r\ncfs_hash_bd_unlock(site->ls_obj_hash, &bd, 1);\r\nreturn;\r\n}\r\nif (!test_and_set_bit(LU_OBJECT_UNHASHED, &top->loh_flags))\r\ncfs_hash_bd_del_locked(site->ls_obj_hash, &bd, &top->loh_hash);\r\ncfs_hash_bd_unlock(site->ls_obj_hash, &bd, 1);\r\nlu_object_free(env, orig);\r\n}\r\nvoid lu_object_unhash(const struct lu_env *env, struct lu_object *o)\r\n{\r\nstruct lu_object_header *top;\r\ntop = o->lo_header;\r\nset_bit(LU_OBJECT_HEARD_BANSHEE, &top->loh_flags);\r\nif (!test_and_set_bit(LU_OBJECT_UNHASHED, &top->loh_flags)) {\r\nstruct lu_site *site = o->lo_dev->ld_site;\r\nstruct cfs_hash *obj_hash = site->ls_obj_hash;\r\nstruct cfs_hash_bd bd;\r\ncfs_hash_bd_get_and_lock(obj_hash, &top->loh_fid, &bd, 1);\r\nif (!list_empty(&top->loh_lru)) {\r\nstruct lu_site_bkt_data *bkt;\r\nlist_del_init(&top->loh_lru);\r\nbkt = cfs_hash_bd_extra_get(obj_hash, &bd);\r\nbkt->lsb_lru_len--;\r\npercpu_counter_dec(&site->ls_lru_len_counter);\r\n}\r\ncfs_hash_bd_del_locked(obj_hash, &bd, &top->loh_hash);\r\ncfs_hash_bd_unlock(obj_hash, &bd, 1);\r\n}\r\n}\r\nstatic struct lu_object *lu_object_alloc(const struct lu_env *env,\r\nstruct lu_device *dev,\r\nconst struct lu_fid *f,\r\nconst struct lu_object_conf *conf)\r\n{\r\nstruct lu_object *scan;\r\nstruct lu_object *top;\r\nstruct list_head *layers;\r\nunsigned int init_mask = 0;\r\nunsigned int init_flag;\r\nint clean;\r\nint result;\r\ntop = dev->ld_ops->ldo_object_alloc(env, NULL, dev);\r\nif (!top)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (IS_ERR(top))\r\nreturn top;\r\ntop->lo_header->loh_fid = *f;\r\nlayers = &top->lo_header->loh_layers;\r\ndo {\r\nclean = 1;\r\ninit_flag = 1;\r\nlist_for_each_entry(scan, layers, lo_linkage) {\r\nif (init_mask & init_flag)\r\ngoto next;\r\nclean = 0;\r\nscan->lo_header = top->lo_header;\r\nresult = scan->lo_ops->loo_object_init(env, scan, conf);\r\nif (result != 0) {\r\nlu_object_free(env, top);\r\nreturn ERR_PTR(result);\r\n}\r\ninit_mask |= init_flag;\r\nnext:\r\ninit_flag <<= 1;\r\n}\r\n} while (!clean);\r\nlist_for_each_entry_reverse(scan, layers, lo_linkage) {\r\nif (scan->lo_ops->loo_object_start) {\r\nresult = scan->lo_ops->loo_object_start(env, scan);\r\nif (result != 0) {\r\nlu_object_free(env, top);\r\nreturn ERR_PTR(result);\r\n}\r\n}\r\n}\r\nlprocfs_counter_incr(dev->ld_site->ls_stats, LU_SS_CREATED);\r\nreturn top;\r\n}\r\nstatic void lu_object_free(const struct lu_env *env, struct lu_object *o)\r\n{\r\nstruct lu_site_bkt_data *bkt;\r\nstruct lu_site *site;\r\nstruct lu_object *scan;\r\nstruct list_head *layers;\r\nstruct list_head splice;\r\nsite = o->lo_dev->ld_site;\r\nlayers = &o->lo_header->loh_layers;\r\nbkt = lu_site_bkt_from_fid(site, &o->lo_header->loh_fid);\r\nlist_for_each_entry_reverse(scan, layers, lo_linkage) {\r\nif (scan->lo_ops->loo_object_delete)\r\nscan->lo_ops->loo_object_delete(env, scan);\r\n}\r\nINIT_LIST_HEAD(&splice);\r\nlist_splice_init(layers, &splice);\r\nwhile (!list_empty(&splice)) {\r\no = container_of0(splice.prev, struct lu_object, lo_linkage);\r\nlist_del_init(&o->lo_linkage);\r\no->lo_ops->loo_object_free(env, o);\r\n}\r\nif (waitqueue_active(&bkt->lsb_marche_funebre))\r\nwake_up_all(&bkt->lsb_marche_funebre);\r\n}\r\nint lu_site_purge_objects(const struct lu_env *env, struct lu_site *s,\r\nint nr, bool canblock)\r\n{\r\nstruct lu_object_header *h;\r\nstruct lu_object_header *temp;\r\nstruct lu_site_bkt_data *bkt;\r\nstruct cfs_hash_bd bd;\r\nstruct cfs_hash_bd bd2;\r\nstruct list_head dispose;\r\nint did_sth;\r\nunsigned int start = 0;\r\nint count;\r\nint bnr;\r\nunsigned int i;\r\nif (OBD_FAIL_CHECK(OBD_FAIL_OBD_NO_LRU))\r\nreturn 0;\r\nINIT_LIST_HEAD(&dispose);\r\nif (nr != ~0)\r\nstart = s->ls_purge_start;\r\nbnr = (nr == ~0) ? -1 : nr / (int)CFS_HASH_NBKT(s->ls_obj_hash) + 1;\r\nagain:\r\nif (canblock)\r\nmutex_lock(&s->ls_purge_mutex);\r\nelse if (!mutex_trylock(&s->ls_purge_mutex))\r\ngoto out;\r\ndid_sth = 0;\r\ncfs_hash_for_each_bucket(s->ls_obj_hash, &bd, i) {\r\nif (i < start)\r\ncontinue;\r\ncount = bnr;\r\ncfs_hash_bd_lock(s->ls_obj_hash, &bd, 1);\r\nbkt = cfs_hash_bd_extra_get(s->ls_obj_hash, &bd);\r\nlist_for_each_entry_safe(h, temp, &bkt->lsb_lru, loh_lru) {\r\nLASSERT(atomic_read(&h->loh_ref) == 0);\r\ncfs_hash_bd_get(s->ls_obj_hash, &h->loh_fid, &bd2);\r\nLASSERT(bd.bd_bucket == bd2.bd_bucket);\r\ncfs_hash_bd_del_locked(s->ls_obj_hash,\r\n&bd2, &h->loh_hash);\r\nlist_move(&h->loh_lru, &dispose);\r\nbkt->lsb_lru_len--;\r\npercpu_counter_dec(&s->ls_lru_len_counter);\r\nif (did_sth == 0)\r\ndid_sth = 1;\r\nif (nr != ~0 && --nr == 0)\r\nbreak;\r\nif (count > 0 && --count == 0)\r\nbreak;\r\n}\r\ncfs_hash_bd_unlock(s->ls_obj_hash, &bd, 1);\r\ncond_resched();\r\nwhile (!list_empty(&dispose)) {\r\nh = container_of0(dispose.next,\r\nstruct lu_object_header, loh_lru);\r\nlist_del_init(&h->loh_lru);\r\nlu_object_free(env, lu_object_top(h));\r\nlprocfs_counter_incr(s->ls_stats, LU_SS_LRU_PURGED);\r\n}\r\nif (nr == 0)\r\nbreak;\r\n}\r\nmutex_unlock(&s->ls_purge_mutex);\r\nif (nr != 0 && did_sth && start != 0) {\r\nstart = 0;\r\ngoto again;\r\n}\r\ns->ls_purge_start = i % CFS_HASH_NBKT(s->ls_obj_hash);\r\nout:\r\nreturn nr;\r\n}\r\nint lu_cdebug_printer(const struct lu_env *env,\r\nvoid *cookie, const char *format, ...)\r\n{\r\nstruct libcfs_debug_msg_data *msgdata = cookie;\r\nstruct lu_cdebug_data *key;\r\nint used;\r\nint complete;\r\nva_list args;\r\nva_start(args, format);\r\nkey = lu_context_key_get(&env->le_ctx, &lu_global_key);\r\nused = strlen(key->lck_area);\r\ncomplete = format[strlen(format) - 1] == '\n';\r\nvsnprintf(key->lck_area + used,\r\nARRAY_SIZE(key->lck_area) - used, format, args);\r\nif (complete) {\r\nif (cfs_cdebug_show(msgdata->msg_mask, msgdata->msg_subsys))\r\nlibcfs_debug_msg(msgdata, "%s\n", key->lck_area);\r\nkey->lck_area[0] = 0;\r\n}\r\nva_end(args);\r\nreturn 0;\r\n}\r\nvoid lu_object_header_print(const struct lu_env *env, void *cookie,\r\nlu_printer_t printer,\r\nconst struct lu_object_header *hdr)\r\n{\r\n(*printer)(env, cookie, "header@%p[%#lx, %d, "DFID"%s%s%s]",\r\nhdr, hdr->loh_flags, atomic_read(&hdr->loh_ref),\r\nPFID(&hdr->loh_fid),\r\nhlist_unhashed(&hdr->loh_hash) ? "" : " hash",\r\nlist_empty((struct list_head *)&hdr->loh_lru) ? \\r\n"" : " lru",\r\nhdr->loh_attr & LOHA_EXISTS ? " exist":"");\r\n}\r\nvoid lu_object_print(const struct lu_env *env, void *cookie,\r\nlu_printer_t printer, const struct lu_object *o)\r\n{\r\nstatic const char ruler[] = "........................................";\r\nstruct lu_object_header *top;\r\nint depth = 4;\r\ntop = o->lo_header;\r\nlu_object_header_print(env, cookie, printer, top);\r\n(*printer)(env, cookie, "{\n");\r\nlist_for_each_entry(o, &top->loh_layers, lo_linkage) {\r\n(*printer)(env, cookie, "%*.*s%s@%p", depth, depth, ruler,\r\no->lo_dev->ld_type->ldt_name, o);\r\nif (o->lo_ops->loo_object_print)\r\n(*o->lo_ops->loo_object_print)(env, cookie, printer, o);\r\n(*printer)(env, cookie, "\n");\r\n}\r\n(*printer)(env, cookie, "} header@%p\n", top);\r\n}\r\nstatic struct lu_object *htable_lookup(struct lu_site *s,\r\nstruct cfs_hash_bd *bd,\r\nconst struct lu_fid *f,\r\nwait_queue_t *waiter,\r\n__u64 *version)\r\n{\r\nstruct lu_site_bkt_data *bkt;\r\nstruct lu_object_header *h;\r\nstruct hlist_node *hnode;\r\n__u64 ver = cfs_hash_bd_version_get(bd);\r\nif (*version == ver)\r\nreturn ERR_PTR(-ENOENT);\r\n*version = ver;\r\nbkt = cfs_hash_bd_extra_get(s->ls_obj_hash, bd);\r\nhnode = cfs_hash_bd_peek_locked(s->ls_obj_hash, bd, (void *)f);\r\nif (!hnode) {\r\nlprocfs_counter_incr(s->ls_stats, LU_SS_CACHE_MISS);\r\nreturn ERR_PTR(-ENOENT);\r\n}\r\nh = container_of0(hnode, struct lu_object_header, loh_hash);\r\nif (likely(!lu_object_is_dying(h))) {\r\ncfs_hash_get(s->ls_obj_hash, hnode);\r\nlprocfs_counter_incr(s->ls_stats, LU_SS_CACHE_HIT);\r\nif (!list_empty(&h->loh_lru)) {\r\nlist_del_init(&h->loh_lru);\r\nbkt->lsb_lru_len--;\r\npercpu_counter_dec(&s->ls_lru_len_counter);\r\n}\r\nreturn lu_object_top(h);\r\n}\r\ninit_waitqueue_entry(waiter, current);\r\nadd_wait_queue(&bkt->lsb_marche_funebre, waiter);\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nlprocfs_counter_incr(s->ls_stats, LU_SS_CACHE_DEATH_RACE);\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nstatic struct lu_object *lu_object_find(const struct lu_env *env,\r\nstruct lu_device *dev,\r\nconst struct lu_fid *f,\r\nconst struct lu_object_conf *conf)\r\n{\r\nreturn lu_object_find_at(env, dev->ld_site->ls_top_dev, f, conf);\r\n}\r\nstatic void lu_object_limit(const struct lu_env *env, struct lu_device *dev)\r\n{\r\n__u64 size, nr;\r\nif (lu_cache_nr == LU_CACHE_NR_UNLIMITED)\r\nreturn;\r\nsize = cfs_hash_size_get(dev->ld_site->ls_obj_hash);\r\nnr = (__u64)lu_cache_nr;\r\nif (size <= nr)\r\nreturn;\r\nlu_site_purge_objects(env, dev->ld_site,\r\nmin_t(__u64, size - nr, LU_CACHE_NR_MAX_ADJUST),\r\nfalse);\r\n}\r\nstatic struct lu_object *lu_object_new(const struct lu_env *env,\r\nstruct lu_device *dev,\r\nconst struct lu_fid *f,\r\nconst struct lu_object_conf *conf)\r\n{\r\nstruct lu_object *o;\r\nstruct cfs_hash *hs;\r\nstruct cfs_hash_bd bd;\r\no = lu_object_alloc(env, dev, f, conf);\r\nif (IS_ERR(o))\r\nreturn o;\r\nhs = dev->ld_site->ls_obj_hash;\r\ncfs_hash_bd_get_and_lock(hs, (void *)f, &bd, 1);\r\ncfs_hash_bd_add_locked(hs, &bd, &o->lo_header->loh_hash);\r\ncfs_hash_bd_unlock(hs, &bd, 1);\r\nlu_object_limit(env, dev);\r\nreturn o;\r\n}\r\nstatic struct lu_object *lu_object_find_try(const struct lu_env *env,\r\nstruct lu_device *dev,\r\nconst struct lu_fid *f,\r\nconst struct lu_object_conf *conf,\r\nwait_queue_t *waiter)\r\n{\r\nstruct lu_object *o;\r\nstruct lu_object *shadow;\r\nstruct lu_site *s;\r\nstruct cfs_hash *hs;\r\nstruct cfs_hash_bd bd;\r\n__u64 version = 0;\r\nif (conf && conf->loc_flags & LOC_F_NEW)\r\nreturn lu_object_new(env, dev, f, conf);\r\ns = dev->ld_site;\r\nhs = s->ls_obj_hash;\r\ncfs_hash_bd_get_and_lock(hs, (void *)f, &bd, 1);\r\no = htable_lookup(s, &bd, f, waiter, &version);\r\ncfs_hash_bd_unlock(hs, &bd, 1);\r\nif (!IS_ERR(o) || PTR_ERR(o) != -ENOENT)\r\nreturn o;\r\no = lu_object_alloc(env, dev, f, conf);\r\nif (IS_ERR(o))\r\nreturn o;\r\nLASSERT(lu_fid_eq(lu_object_fid(o), f));\r\ncfs_hash_bd_lock(hs, &bd, 1);\r\nshadow = htable_lookup(s, &bd, f, waiter, &version);\r\nif (likely(PTR_ERR(shadow) == -ENOENT)) {\r\ncfs_hash_bd_add_locked(hs, &bd, &o->lo_header->loh_hash);\r\ncfs_hash_bd_unlock(hs, &bd, 1);\r\nlu_object_limit(env, dev);\r\nreturn o;\r\n}\r\nlprocfs_counter_incr(s->ls_stats, LU_SS_CACHE_RACE);\r\ncfs_hash_bd_unlock(hs, &bd, 1);\r\nlu_object_free(env, o);\r\nreturn shadow;\r\n}\r\nstruct lu_object *lu_object_find_at(const struct lu_env *env,\r\nstruct lu_device *dev,\r\nconst struct lu_fid *f,\r\nconst struct lu_object_conf *conf)\r\n{\r\nstruct lu_site_bkt_data *bkt;\r\nstruct lu_object *obj;\r\nwait_queue_t wait;\r\nwhile (1) {\r\nobj = lu_object_find_try(env, dev, f, conf, &wait);\r\nif (obj != ERR_PTR(-EAGAIN))\r\nreturn obj;\r\nschedule();\r\nbkt = lu_site_bkt_from_fid(dev->ld_site, (void *)f);\r\nremove_wait_queue(&bkt->lsb_marche_funebre, &wait);\r\n}\r\n}\r\nstruct lu_object *lu_object_find_slice(const struct lu_env *env,\r\nstruct lu_device *dev,\r\nconst struct lu_fid *f,\r\nconst struct lu_object_conf *conf)\r\n{\r\nstruct lu_object *top;\r\nstruct lu_object *obj;\r\ntop = lu_object_find(env, dev, f, conf);\r\nif (IS_ERR(top))\r\nreturn top;\r\nobj = lu_object_locate(top->lo_header, dev->ld_type);\r\nif (unlikely(!obj)) {\r\nlu_object_put(env, top);\r\nobj = ERR_PTR(-ENOENT);\r\n}\r\nreturn obj;\r\n}\r\nint lu_device_type_init(struct lu_device_type *ldt)\r\n{\r\nint result = 0;\r\natomic_set(&ldt->ldt_device_nr, 0);\r\nINIT_LIST_HEAD(&ldt->ldt_linkage);\r\nif (ldt->ldt_ops->ldto_init)\r\nresult = ldt->ldt_ops->ldto_init(ldt);\r\nif (!result) {\r\nspin_lock(&obd_types_lock);\r\nlist_add(&ldt->ldt_linkage, &lu_device_types);\r\nspin_unlock(&obd_types_lock);\r\n}\r\nreturn result;\r\n}\r\nvoid lu_device_type_fini(struct lu_device_type *ldt)\r\n{\r\nspin_lock(&obd_types_lock);\r\nlist_del_init(&ldt->ldt_linkage);\r\nspin_unlock(&obd_types_lock);\r\nif (ldt->ldt_ops->ldto_fini)\r\nldt->ldt_ops->ldto_fini(ldt);\r\n}\r\nstatic int\r\nlu_site_obj_print(struct cfs_hash *hs, struct cfs_hash_bd *bd,\r\nstruct hlist_node *hnode, void *data)\r\n{\r\nstruct lu_site_print_arg *arg = (struct lu_site_print_arg *)data;\r\nstruct lu_object_header *h;\r\nh = hlist_entry(hnode, struct lu_object_header, loh_hash);\r\nif (!list_empty(&h->loh_layers)) {\r\nconst struct lu_object *o;\r\no = lu_object_top(h);\r\nlu_object_print(arg->lsp_env, arg->lsp_cookie,\r\narg->lsp_printer, o);\r\n} else {\r\nlu_object_header_print(arg->lsp_env, arg->lsp_cookie,\r\narg->lsp_printer, h);\r\n}\r\nreturn 0;\r\n}\r\nvoid lu_site_print(const struct lu_env *env, struct lu_site *s, void *cookie,\r\nlu_printer_t printer)\r\n{\r\nstruct lu_site_print_arg arg = {\r\n.lsp_env = (struct lu_env *)env,\r\n.lsp_cookie = cookie,\r\n.lsp_printer = printer,\r\n};\r\ncfs_hash_for_each(s->ls_obj_hash, lu_site_obj_print, &arg);\r\n}\r\nstatic unsigned long lu_htable_order(struct lu_device *top)\r\n{\r\nunsigned long bits_max = LU_SITE_BITS_MAX;\r\nunsigned long cache_size;\r\nunsigned long bits;\r\nif (!strcmp(top->ld_type->ldt_name, LUSTRE_VVP_NAME))\r\nbits_max = LU_SITE_BITS_MAX_CL;\r\ncache_size = totalram_pages;\r\n#if BITS_PER_LONG == 32\r\nif (cache_size > 1 << (30 - PAGE_SHIFT))\r\ncache_size = 1 << (30 - PAGE_SHIFT) * 3 / 4;\r\n#endif\r\nif (lu_cache_percent == 0 || lu_cache_percent > LU_CACHE_PERCENT_MAX) {\r\nCWARN("obdclass: invalid lu_cache_percent: %u, it must be in the range of (0, %u]. Will use default value: %u.\n",\r\nlu_cache_percent, LU_CACHE_PERCENT_MAX,\r\nLU_CACHE_PERCENT_DEFAULT);\r\nlu_cache_percent = LU_CACHE_PERCENT_DEFAULT;\r\n}\r\ncache_size = cache_size / 100 * lu_cache_percent *\r\n(PAGE_SIZE / 1024);\r\nfor (bits = 1; (1 << bits) < cache_size; ++bits) {\r\n;\r\n}\r\nreturn clamp_t(typeof(bits), bits, LU_SITE_BITS_MIN, bits_max);\r\n}\r\nstatic unsigned int lu_obj_hop_hash(struct cfs_hash *hs,\r\nconst void *key, unsigned int mask)\r\n{\r\nstruct lu_fid *fid = (struct lu_fid *)key;\r\n__u32 hash;\r\nhash = fid_flatten32(fid);\r\nhash += (hash >> 4) + (hash << 12);\r\nhash = hash_long(hash, hs->hs_bkt_bits);\r\nhash -= hash_long((unsigned long)hs, fid_oid(fid) % 11 + 3);\r\nhash <<= hs->hs_cur_bits - hs->hs_bkt_bits;\r\nhash |= (fid_seq(fid) + fid_oid(fid)) & (CFS_HASH_NBKT(hs) - 1);\r\nreturn hash & mask;\r\n}\r\nstatic void *lu_obj_hop_object(struct hlist_node *hnode)\r\n{\r\nreturn hlist_entry(hnode, struct lu_object_header, loh_hash);\r\n}\r\nstatic void *lu_obj_hop_key(struct hlist_node *hnode)\r\n{\r\nstruct lu_object_header *h;\r\nh = hlist_entry(hnode, struct lu_object_header, loh_hash);\r\nreturn &h->loh_fid;\r\n}\r\nstatic int lu_obj_hop_keycmp(const void *key, struct hlist_node *hnode)\r\n{\r\nstruct lu_object_header *h;\r\nh = hlist_entry(hnode, struct lu_object_header, loh_hash);\r\nreturn lu_fid_eq(&h->loh_fid, (struct lu_fid *)key);\r\n}\r\nstatic void lu_obj_hop_get(struct cfs_hash *hs, struct hlist_node *hnode)\r\n{\r\nstruct lu_object_header *h;\r\nh = hlist_entry(hnode, struct lu_object_header, loh_hash);\r\natomic_inc(&h->loh_ref);\r\n}\r\nstatic void lu_obj_hop_put_locked(struct cfs_hash *hs, struct hlist_node *hnode)\r\n{\r\nLBUG();\r\n}\r\nstatic void lu_dev_add_linkage(struct lu_site *s, struct lu_device *d)\r\n{\r\nspin_lock(&s->ls_ld_lock);\r\nif (list_empty(&d->ld_linkage))\r\nlist_add(&d->ld_linkage, &s->ls_ld_linkage);\r\nspin_unlock(&s->ls_ld_lock);\r\n}\r\nint lu_site_init(struct lu_site *s, struct lu_device *top)\r\n{\r\nstruct lu_site_bkt_data *bkt;\r\nstruct cfs_hash_bd bd;\r\nunsigned long bits;\r\nunsigned long i;\r\nchar name[16];\r\nint rc;\r\nmemset(s, 0, sizeof(*s));\r\nmutex_init(&s->ls_purge_mutex);\r\nrc = percpu_counter_init(&s->ls_lru_len_counter, 0, GFP_NOFS);\r\nif (rc)\r\nreturn -ENOMEM;\r\nsnprintf(name, sizeof(name), "lu_site_%s", top->ld_type->ldt_name);\r\nfor (bits = lu_htable_order(top); bits >= LU_SITE_BITS_MIN; bits--) {\r\ns->ls_obj_hash = cfs_hash_create(name, bits, bits,\r\nbits - LU_SITE_BKT_BITS,\r\nsizeof(*bkt), 0, 0,\r\n&lu_site_hash_ops,\r\nCFS_HASH_SPIN_BKTLOCK |\r\nCFS_HASH_NO_ITEMREF |\r\nCFS_HASH_DEPTH |\r\nCFS_HASH_ASSERT_EMPTY |\r\nCFS_HASH_COUNTER);\r\nif (s->ls_obj_hash)\r\nbreak;\r\n}\r\nif (!s->ls_obj_hash) {\r\nCERROR("failed to create lu_site hash with bits: %lu\n", bits);\r\nreturn -ENOMEM;\r\n}\r\ncfs_hash_for_each_bucket(s->ls_obj_hash, &bd, i) {\r\nbkt = cfs_hash_bd_extra_get(s->ls_obj_hash, &bd);\r\nINIT_LIST_HEAD(&bkt->lsb_lru);\r\ninit_waitqueue_head(&bkt->lsb_marche_funebre);\r\n}\r\ns->ls_stats = lprocfs_alloc_stats(LU_SS_LAST_STAT, 0);\r\nif (!s->ls_stats) {\r\ncfs_hash_putref(s->ls_obj_hash);\r\ns->ls_obj_hash = NULL;\r\nreturn -ENOMEM;\r\n}\r\nlprocfs_counter_init(s->ls_stats, LU_SS_CREATED,\r\n0, "created", "created");\r\nlprocfs_counter_init(s->ls_stats, LU_SS_CACHE_HIT,\r\n0, "cache_hit", "cache_hit");\r\nlprocfs_counter_init(s->ls_stats, LU_SS_CACHE_MISS,\r\n0, "cache_miss", "cache_miss");\r\nlprocfs_counter_init(s->ls_stats, LU_SS_CACHE_RACE,\r\n0, "cache_race", "cache_race");\r\nlprocfs_counter_init(s->ls_stats, LU_SS_CACHE_DEATH_RACE,\r\n0, "cache_death_race", "cache_death_race");\r\nlprocfs_counter_init(s->ls_stats, LU_SS_LRU_PURGED,\r\n0, "lru_purged", "lru_purged");\r\nINIT_LIST_HEAD(&s->ls_linkage);\r\ns->ls_top_dev = top;\r\ntop->ld_site = s;\r\nlu_device_get(top);\r\nlu_ref_add(&top->ld_reference, "site-top", s);\r\nINIT_LIST_HEAD(&s->ls_ld_linkage);\r\nspin_lock_init(&s->ls_ld_lock);\r\nlu_dev_add_linkage(s, top);\r\nreturn 0;\r\n}\r\nvoid lu_site_fini(struct lu_site *s)\r\n{\r\ndown_write(&lu_sites_guard);\r\nlist_del_init(&s->ls_linkage);\r\nup_write(&lu_sites_guard);\r\npercpu_counter_destroy(&s->ls_lru_len_counter);\r\nif (s->ls_obj_hash) {\r\ncfs_hash_putref(s->ls_obj_hash);\r\ns->ls_obj_hash = NULL;\r\n}\r\nif (s->ls_top_dev) {\r\ns->ls_top_dev->ld_site = NULL;\r\nlu_ref_del(&s->ls_top_dev->ld_reference, "site-top", s);\r\nlu_device_put(s->ls_top_dev);\r\ns->ls_top_dev = NULL;\r\n}\r\nif (s->ls_stats)\r\nlprocfs_free_stats(&s->ls_stats);\r\n}\r\nint lu_site_init_finish(struct lu_site *s)\r\n{\r\nint result;\r\ndown_write(&lu_sites_guard);\r\nresult = lu_context_refill(&lu_shrink_env.le_ctx);\r\nif (result == 0)\r\nlist_add(&s->ls_linkage, &lu_sites);\r\nup_write(&lu_sites_guard);\r\nreturn result;\r\n}\r\nvoid lu_device_get(struct lu_device *d)\r\n{\r\natomic_inc(&d->ld_ref);\r\n}\r\nvoid lu_device_put(struct lu_device *d)\r\n{\r\nLASSERT(atomic_read(&d->ld_ref) > 0);\r\natomic_dec(&d->ld_ref);\r\n}\r\nint lu_device_init(struct lu_device *d, struct lu_device_type *t)\r\n{\r\nif (atomic_inc_return(&t->ldt_device_nr) == 1 &&\r\nt->ldt_ops->ldto_start)\r\nt->ldt_ops->ldto_start(t);\r\nmemset(d, 0, sizeof(*d));\r\natomic_set(&d->ld_ref, 0);\r\nd->ld_type = t;\r\nlu_ref_init(&d->ld_reference);\r\nINIT_LIST_HEAD(&d->ld_linkage);\r\nreturn 0;\r\n}\r\nvoid lu_device_fini(struct lu_device *d)\r\n{\r\nstruct lu_device_type *t = d->ld_type;\r\nif (d->ld_obd) {\r\nd->ld_obd->obd_lu_dev = NULL;\r\nd->ld_obd = NULL;\r\n}\r\nlu_ref_fini(&d->ld_reference);\r\nLASSERTF(atomic_read(&d->ld_ref) == 0,\r\n"Refcount is %u\n", atomic_read(&d->ld_ref));\r\nLASSERT(atomic_read(&t->ldt_device_nr) > 0);\r\nif (atomic_dec_and_test(&t->ldt_device_nr) &&\r\nt->ldt_ops->ldto_stop)\r\nt->ldt_ops->ldto_stop(t);\r\n}\r\nint lu_object_init(struct lu_object *o, struct lu_object_header *h,\r\nstruct lu_device *d)\r\n{\r\nmemset(o, 0, sizeof(*o));\r\no->lo_header = h;\r\no->lo_dev = d;\r\nlu_device_get(d);\r\nlu_ref_add_at(&d->ld_reference, &o->lo_dev_ref, "lu_object", o);\r\nINIT_LIST_HEAD(&o->lo_linkage);\r\nreturn 0;\r\n}\r\nvoid lu_object_fini(struct lu_object *o)\r\n{\r\nstruct lu_device *dev = o->lo_dev;\r\nLASSERT(list_empty(&o->lo_linkage));\r\nif (dev) {\r\nlu_ref_del_at(&dev->ld_reference, &o->lo_dev_ref,\r\n"lu_object", o);\r\nlu_device_put(dev);\r\no->lo_dev = NULL;\r\n}\r\n}\r\nvoid lu_object_add_top(struct lu_object_header *h, struct lu_object *o)\r\n{\r\nlist_move(&o->lo_linkage, &h->loh_layers);\r\n}\r\nvoid lu_object_add(struct lu_object *before, struct lu_object *o)\r\n{\r\nlist_move(&o->lo_linkage, &before->lo_linkage);\r\n}\r\nint lu_object_header_init(struct lu_object_header *h)\r\n{\r\nmemset(h, 0, sizeof(*h));\r\natomic_set(&h->loh_ref, 1);\r\nINIT_HLIST_NODE(&h->loh_hash);\r\nINIT_LIST_HEAD(&h->loh_lru);\r\nINIT_LIST_HEAD(&h->loh_layers);\r\nlu_ref_init(&h->loh_reference);\r\nreturn 0;\r\n}\r\nvoid lu_object_header_fini(struct lu_object_header *h)\r\n{\r\nLASSERT(list_empty(&h->loh_layers));\r\nLASSERT(list_empty(&h->loh_lru));\r\nLASSERT(hlist_unhashed(&h->loh_hash));\r\nlu_ref_fini(&h->loh_reference);\r\n}\r\nstruct lu_object *lu_object_locate(struct lu_object_header *h,\r\nconst struct lu_device_type *dtype)\r\n{\r\nstruct lu_object *o;\r\nlist_for_each_entry(o, &h->loh_layers, lo_linkage) {\r\nif (o->lo_dev->ld_type == dtype)\r\nreturn o;\r\n}\r\nreturn NULL;\r\n}\r\nvoid lu_stack_fini(const struct lu_env *env, struct lu_device *top)\r\n{\r\nstruct lu_site *site = top->ld_site;\r\nstruct lu_device *scan;\r\nstruct lu_device *next;\r\nlu_site_purge(env, site, ~0);\r\nfor (scan = top; scan; scan = next) {\r\nnext = scan->ld_type->ldt_ops->ldto_device_fini(env, scan);\r\nlu_ref_del(&scan->ld_reference, "lu-stack", &lu_site_init);\r\nlu_device_put(scan);\r\n}\r\nlu_site_purge(env, site, ~0);\r\nfor (scan = top; scan; scan = next) {\r\nconst struct lu_device_type *ldt = scan->ld_type;\r\nstruct obd_type *type;\r\nnext = ldt->ldt_ops->ldto_device_free(env, scan);\r\ntype = ldt->ldt_obd_type;\r\nif (type) {\r\ntype->typ_refcnt--;\r\nclass_put_type(type);\r\n}\r\n}\r\n}\r\nint lu_context_key_register(struct lu_context_key *key)\r\n{\r\nint result;\r\nunsigned int i;\r\nLASSERT(key->lct_init);\r\nLASSERT(key->lct_fini);\r\nLASSERT(key->lct_tags != 0);\r\nresult = -ENFILE;\r\nspin_lock(&lu_keys_guard);\r\nfor (i = 0; i < ARRAY_SIZE(lu_keys); ++i) {\r\nif (!lu_keys[i]) {\r\nkey->lct_index = i;\r\natomic_set(&key->lct_used, 1);\r\nlu_keys[i] = key;\r\nlu_ref_init(&key->lct_reference);\r\nresult = 0;\r\n++key_set_version;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&lu_keys_guard);\r\nreturn result;\r\n}\r\nstatic void key_fini(struct lu_context *ctx, int index)\r\n{\r\nif (ctx->lc_value && ctx->lc_value[index]) {\r\nstruct lu_context_key *key;\r\nkey = lu_keys[index];\r\nLASSERT(atomic_read(&key->lct_used) > 1);\r\nkey->lct_fini(ctx, key, ctx->lc_value[index]);\r\nlu_ref_del(&key->lct_reference, "ctx", ctx);\r\natomic_dec(&key->lct_used);\r\nif ((ctx->lc_tags & LCT_NOREF) == 0) {\r\n#ifdef CONFIG_MODULE_UNLOAD\r\nLINVRNT(module_refcount(key->lct_owner) > 0);\r\n#endif\r\nmodule_put(key->lct_owner);\r\n}\r\nctx->lc_value[index] = NULL;\r\n}\r\n}\r\nvoid lu_context_key_degister(struct lu_context_key *key)\r\n{\r\nLASSERT(atomic_read(&key->lct_used) >= 1);\r\nLINVRNT(0 <= key->lct_index && key->lct_index < ARRAY_SIZE(lu_keys));\r\nlu_context_key_quiesce(key);\r\n++key_set_version;\r\nspin_lock(&lu_keys_guard);\r\nkey_fini(&lu_shrink_env.le_ctx, key->lct_index);\r\nwhile (atomic_read(&key->lct_used) > 1) {\r\nspin_unlock(&lu_keys_guard);\r\nCDEBUG(D_INFO, "lu_context_key_degister: \"%s\" %p, %d\n",\r\nkey->lct_owner ? key->lct_owner->name : "", key,\r\natomic_read(&key->lct_used));\r\nschedule();\r\nspin_lock(&lu_keys_guard);\r\n}\r\nif (lu_keys[key->lct_index]) {\r\nlu_keys[key->lct_index] = NULL;\r\nlu_ref_fini(&key->lct_reference);\r\n}\r\nspin_unlock(&lu_keys_guard);\r\nLASSERTF(atomic_read(&key->lct_used) == 1,\r\n"key has instances: %d\n",\r\natomic_read(&key->lct_used));\r\n}\r\nint lu_context_key_register_many(struct lu_context_key *k, ...)\r\n{\r\nstruct lu_context_key *key = k;\r\nva_list args;\r\nint result;\r\nva_start(args, k);\r\ndo {\r\nresult = lu_context_key_register(key);\r\nif (result)\r\nbreak;\r\nkey = va_arg(args, struct lu_context_key *);\r\n} while (key);\r\nva_end(args);\r\nif (result != 0) {\r\nva_start(args, k);\r\nwhile (k != key) {\r\nlu_context_key_degister(k);\r\nk = va_arg(args, struct lu_context_key *);\r\n}\r\nva_end(args);\r\n}\r\nreturn result;\r\n}\r\nvoid lu_context_key_degister_many(struct lu_context_key *k, ...)\r\n{\r\nva_list args;\r\nva_start(args, k);\r\ndo {\r\nlu_context_key_degister(k);\r\nk = va_arg(args, struct lu_context_key*);\r\n} while (k);\r\nva_end(args);\r\n}\r\nvoid lu_context_key_revive_many(struct lu_context_key *k, ...)\r\n{\r\nva_list args;\r\nva_start(args, k);\r\ndo {\r\nlu_context_key_revive(k);\r\nk = va_arg(args, struct lu_context_key*);\r\n} while (k);\r\nva_end(args);\r\n}\r\nvoid lu_context_key_quiesce_many(struct lu_context_key *k, ...)\r\n{\r\nva_list args;\r\nva_start(args, k);\r\ndo {\r\nlu_context_key_quiesce(k);\r\nk = va_arg(args, struct lu_context_key*);\r\n} while (k);\r\nva_end(args);\r\n}\r\nvoid *lu_context_key_get(const struct lu_context *ctx,\r\nconst struct lu_context_key *key)\r\n{\r\nLINVRNT(ctx->lc_state == LCS_ENTERED);\r\nLINVRNT(0 <= key->lct_index && key->lct_index < ARRAY_SIZE(lu_keys));\r\nLASSERT(lu_keys[key->lct_index] == key);\r\nreturn ctx->lc_value[key->lct_index];\r\n}\r\nvoid lu_context_key_quiesce(struct lu_context_key *key)\r\n{\r\nstruct lu_context *ctx;\r\nif (!(key->lct_tags & LCT_QUIESCENT)) {\r\nspin_lock(&lu_keys_guard);\r\nkey->lct_tags |= LCT_QUIESCENT;\r\nwhile (atomic_read(&lu_key_initing_cnt) > 0) {\r\nspin_unlock(&lu_keys_guard);\r\nCDEBUG(D_INFO, "lu_context_key_quiesce: \"%s\" %p, %d (%d)\n",\r\nkey->lct_owner ? key->lct_owner->name : "",\r\nkey, atomic_read(&key->lct_used),\r\natomic_read(&lu_key_initing_cnt));\r\nschedule();\r\nspin_lock(&lu_keys_guard);\r\n}\r\nlist_for_each_entry(ctx, &lu_context_remembered, lc_remember)\r\nkey_fini(ctx, key->lct_index);\r\nspin_unlock(&lu_keys_guard);\r\n++key_set_version;\r\n}\r\n}\r\nvoid lu_context_key_revive(struct lu_context_key *key)\r\n{\r\nkey->lct_tags &= ~LCT_QUIESCENT;\r\n++key_set_version;\r\n}\r\nstatic void keys_fini(struct lu_context *ctx)\r\n{\r\nunsigned int i;\r\nif (!ctx->lc_value)\r\nreturn;\r\nfor (i = 0; i < ARRAY_SIZE(lu_keys); ++i)\r\nkey_fini(ctx, i);\r\nkfree(ctx->lc_value);\r\nctx->lc_value = NULL;\r\n}\r\nstatic int keys_fill(struct lu_context *ctx)\r\n{\r\nunsigned int i;\r\nspin_lock(&lu_keys_guard);\r\natomic_inc(&lu_key_initing_cnt);\r\nspin_unlock(&lu_keys_guard);\r\nLINVRNT(ctx->lc_value);\r\nfor (i = 0; i < ARRAY_SIZE(lu_keys); ++i) {\r\nstruct lu_context_key *key;\r\nkey = lu_keys[i];\r\nif (!ctx->lc_value[i] && key &&\r\n(key->lct_tags & ctx->lc_tags) &&\r\n!(key->lct_tags & LCT_QUIESCENT)) {\r\nvoid *value;\r\nLINVRNT(key->lct_init);\r\nLINVRNT(key->lct_index == i);\r\nLASSERT(key->lct_owner);\r\nif (!(ctx->lc_tags & LCT_NOREF) &&\r\n!try_module_get(key->lct_owner)) {\r\ncontinue;\r\n}\r\nvalue = key->lct_init(ctx, key);\r\nif (unlikely(IS_ERR(value))) {\r\natomic_dec(&lu_key_initing_cnt);\r\nreturn PTR_ERR(value);\r\n}\r\nlu_ref_add_atomic(&key->lct_reference, "ctx", ctx);\r\natomic_inc(&key->lct_used);\r\nctx->lc_value[i] = value;\r\nif (key->lct_exit)\r\nctx->lc_tags |= LCT_HAS_EXIT;\r\n}\r\nctx->lc_version = key_set_version;\r\n}\r\natomic_dec(&lu_key_initing_cnt);\r\nreturn 0;\r\n}\r\nstatic int keys_init(struct lu_context *ctx)\r\n{\r\nctx->lc_value = kcalloc(ARRAY_SIZE(lu_keys), sizeof(ctx->lc_value[0]),\r\nGFP_NOFS);\r\nif (likely(ctx->lc_value))\r\nreturn keys_fill(ctx);\r\nreturn -ENOMEM;\r\n}\r\nint lu_context_init(struct lu_context *ctx, __u32 tags)\r\n{\r\nint rc;\r\nmemset(ctx, 0, sizeof(*ctx));\r\nctx->lc_state = LCS_INITIALIZED;\r\nctx->lc_tags = tags;\r\nif (tags & LCT_REMEMBER) {\r\nspin_lock(&lu_keys_guard);\r\nlist_add(&ctx->lc_remember, &lu_context_remembered);\r\nspin_unlock(&lu_keys_guard);\r\n} else {\r\nINIT_LIST_HEAD(&ctx->lc_remember);\r\n}\r\nrc = keys_init(ctx);\r\nif (rc != 0)\r\nlu_context_fini(ctx);\r\nreturn rc;\r\n}\r\nvoid lu_context_fini(struct lu_context *ctx)\r\n{\r\nLINVRNT(ctx->lc_state == LCS_INITIALIZED || ctx->lc_state == LCS_LEFT);\r\nctx->lc_state = LCS_FINALIZED;\r\nif ((ctx->lc_tags & LCT_REMEMBER) == 0) {\r\nLASSERT(list_empty(&ctx->lc_remember));\r\nkeys_fini(ctx);\r\n} else {\r\nspin_lock(&lu_keys_guard);\r\nkeys_fini(ctx);\r\nlist_del_init(&ctx->lc_remember);\r\nspin_unlock(&lu_keys_guard);\r\n}\r\n}\r\nvoid lu_context_enter(struct lu_context *ctx)\r\n{\r\nLINVRNT(ctx->lc_state == LCS_INITIALIZED || ctx->lc_state == LCS_LEFT);\r\nctx->lc_state = LCS_ENTERED;\r\n}\r\nvoid lu_context_exit(struct lu_context *ctx)\r\n{\r\nunsigned int i;\r\nLINVRNT(ctx->lc_state == LCS_ENTERED);\r\nctx->lc_state = LCS_LEFT;\r\nif (ctx->lc_tags & LCT_HAS_EXIT && ctx->lc_value) {\r\nfor (i = 0; i < ARRAY_SIZE(lu_keys); ++i) {\r\nif (ctx->lc_tags & LCT_REMEMBER)\r\nspin_lock(&lu_keys_guard);\r\nif (ctx->lc_value[i]) {\r\nstruct lu_context_key *key;\r\nkey = lu_keys[i];\r\nif (key->lct_exit)\r\nkey->lct_exit(ctx,\r\nkey, ctx->lc_value[i]);\r\n}\r\nif (ctx->lc_tags & LCT_REMEMBER)\r\nspin_unlock(&lu_keys_guard);\r\n}\r\n}\r\n}\r\nint lu_context_refill(struct lu_context *ctx)\r\n{\r\nreturn likely(ctx->lc_version == key_set_version) ? 0 : keys_fill(ctx);\r\n}\r\nint lu_env_init(struct lu_env *env, __u32 tags)\r\n{\r\nint result;\r\nenv->le_ses = NULL;\r\nresult = lu_context_init(&env->le_ctx, tags);\r\nif (likely(result == 0))\r\nlu_context_enter(&env->le_ctx);\r\nreturn result;\r\n}\r\nvoid lu_env_fini(struct lu_env *env)\r\n{\r\nlu_context_exit(&env->le_ctx);\r\nlu_context_fini(&env->le_ctx);\r\nenv->le_ses = NULL;\r\n}\r\nint lu_env_refill(struct lu_env *env)\r\n{\r\nint result;\r\nresult = lu_context_refill(&env->le_ctx);\r\nif (result == 0 && env->le_ses)\r\nresult = lu_context_refill(env->le_ses);\r\nreturn result;\r\n}\r\nstatic void lu_site_stats_get(struct cfs_hash *hs,\r\nstruct lu_site_stats *stats, int populated)\r\n{\r\nstruct cfs_hash_bd bd;\r\nunsigned int i;\r\ncfs_hash_for_each_bucket(hs, &bd, i) {\r\nstruct lu_site_bkt_data *bkt = cfs_hash_bd_extra_get(hs, &bd);\r\nstruct hlist_head *hhead;\r\ncfs_hash_bd_lock(hs, &bd, 1);\r\nstats->lss_busy +=\r\ncfs_hash_bd_count_get(&bd) - bkt->lsb_lru_len;\r\nstats->lss_total += cfs_hash_bd_count_get(&bd);\r\nstats->lss_max_search = max((int)stats->lss_max_search,\r\ncfs_hash_bd_depmax_get(&bd));\r\nif (!populated) {\r\ncfs_hash_bd_unlock(hs, &bd, 1);\r\ncontinue;\r\n}\r\ncfs_hash_bd_for_each_hlist(hs, &bd, hhead) {\r\nif (!hlist_empty(hhead))\r\nstats->lss_populated++;\r\n}\r\ncfs_hash_bd_unlock(hs, &bd, 1);\r\n}\r\n}\r\nstatic unsigned long lu_cache_shrink_count(struct shrinker *sk,\r\nstruct shrink_control *sc)\r\n{\r\nstruct lu_site *s;\r\nstruct lu_site *tmp;\r\nunsigned long cached = 0;\r\nif (!(sc->gfp_mask & __GFP_FS))\r\nreturn 0;\r\ndown_read(&lu_sites_guard);\r\nlist_for_each_entry_safe(s, tmp, &lu_sites, ls_linkage)\r\ncached += percpu_counter_read_positive(&s->ls_lru_len_counter);\r\nup_read(&lu_sites_guard);\r\ncached = (cached / 100) * sysctl_vfs_cache_pressure;\r\nCDEBUG(D_INODE, "%ld objects cached, cache pressure %d\n",\r\ncached, sysctl_vfs_cache_pressure);\r\nreturn cached;\r\n}\r\nstatic unsigned long lu_cache_shrink_scan(struct shrinker *sk,\r\nstruct shrink_control *sc)\r\n{\r\nstruct lu_site *s;\r\nstruct lu_site *tmp;\r\nunsigned long remain = sc->nr_to_scan, freed = 0;\r\nLIST_HEAD(splice);\r\nif (!(sc->gfp_mask & __GFP_FS))\r\nreturn SHRINK_STOP;\r\ndown_write(&lu_sites_guard);\r\nlist_for_each_entry_safe(s, tmp, &lu_sites, ls_linkage) {\r\nfreed = lu_site_purge(&lu_shrink_env, s, remain);\r\nremain -= freed;\r\nlist_move_tail(&s->ls_linkage, &splice);\r\n}\r\nlist_splice(&splice, lu_sites.prev);\r\nup_write(&lu_sites_guard);\r\nreturn sc->nr_to_scan - remain;\r\n}\r\nint lu_global_init(void)\r\n{\r\nint result;\r\nCDEBUG(D_INFO, "Lustre LU module (%p).\n", &lu_keys);\r\nresult = lu_ref_global_init();\r\nif (result != 0)\r\nreturn result;\r\nLU_CONTEXT_KEY_INIT(&lu_global_key);\r\nresult = lu_context_key_register(&lu_global_key);\r\nif (result != 0)\r\nreturn result;\r\ndown_write(&lu_sites_guard);\r\nresult = lu_env_init(&lu_shrink_env, LCT_SHRINKER);\r\nup_write(&lu_sites_guard);\r\nif (result != 0)\r\nreturn result;\r\nregister_shrinker(&lu_site_shrinker);\r\nreturn result;\r\n}\r\nvoid lu_global_fini(void)\r\n{\r\nunregister_shrinker(&lu_site_shrinker);\r\nlu_context_key_degister(&lu_global_key);\r\ndown_write(&lu_sites_guard);\r\nlu_env_fini(&lu_shrink_env);\r\nup_write(&lu_sites_guard);\r\nlu_ref_global_fini();\r\n}\r\nstatic __u32 ls_stats_read(struct lprocfs_stats *stats, int idx)\r\n{\r\nstruct lprocfs_counter ret;\r\nlprocfs_stats_collect(stats, idx, &ret);\r\nreturn (__u32)ret.lc_count;\r\n}\r\nint lu_site_stats_print(const struct lu_site *s, struct seq_file *m)\r\n{\r\nstruct lu_site_stats stats;\r\nmemset(&stats, 0, sizeof(stats));\r\nlu_site_stats_get(s->ls_obj_hash, &stats, 1);\r\nseq_printf(m, "%d/%d %d/%ld %d %d %d %d %d %d %d\n",\r\nstats.lss_busy,\r\nstats.lss_total,\r\nstats.lss_populated,\r\nCFS_HASH_NHLIST(s->ls_obj_hash),\r\nstats.lss_max_search,\r\nls_stats_read(s->ls_stats, LU_SS_CREATED),\r\nls_stats_read(s->ls_stats, LU_SS_CACHE_HIT),\r\nls_stats_read(s->ls_stats, LU_SS_CACHE_MISS),\r\nls_stats_read(s->ls_stats, LU_SS_CACHE_RACE),\r\nls_stats_read(s->ls_stats, LU_SS_CACHE_DEATH_RACE),\r\nls_stats_read(s->ls_stats, LU_SS_LRU_PURGED));\r\nreturn 0;\r\n}\r\nint lu_kmem_init(struct lu_kmem_descr *caches)\r\n{\r\nint result;\r\nstruct lu_kmem_descr *iter = caches;\r\nfor (result = 0; iter->ckd_cache; ++iter) {\r\n*iter->ckd_cache = kmem_cache_create(iter->ckd_name,\r\niter->ckd_size,\r\n0, 0, NULL);\r\nif (!*iter->ckd_cache) {\r\nresult = -ENOMEM;\r\nlu_kmem_fini(caches);\r\nbreak;\r\n}\r\n}\r\nreturn result;\r\n}\r\nvoid lu_kmem_fini(struct lu_kmem_descr *caches)\r\n{\r\nfor (; caches->ckd_cache; ++caches) {\r\nkmem_cache_destroy(*caches->ckd_cache);\r\n*caches->ckd_cache = NULL;\r\n}\r\n}\r\nvoid lu_buf_free(struct lu_buf *buf)\r\n{\r\nLASSERT(buf);\r\nif (buf->lb_buf) {\r\nLASSERT(buf->lb_len > 0);\r\nkvfree(buf->lb_buf);\r\nbuf->lb_buf = NULL;\r\nbuf->lb_len = 0;\r\n}\r\n}\r\nvoid lu_buf_alloc(struct lu_buf *buf, size_t size)\r\n{\r\nLASSERT(buf);\r\nLASSERT(!buf->lb_buf);\r\nLASSERT(!buf->lb_len);\r\nbuf->lb_buf = libcfs_kvzalloc(size, GFP_NOFS);\r\nif (likely(buf->lb_buf))\r\nbuf->lb_len = size;\r\n}\r\nvoid lu_buf_realloc(struct lu_buf *buf, size_t size)\r\n{\r\nlu_buf_free(buf);\r\nlu_buf_alloc(buf, size);\r\n}\r\nstruct lu_buf *lu_buf_check_and_alloc(struct lu_buf *buf, size_t len)\r\n{\r\nif (!buf->lb_buf && !buf->lb_len)\r\nlu_buf_alloc(buf, len);\r\nif ((len > buf->lb_len) && buf->lb_buf)\r\nlu_buf_realloc(buf, len);\r\nreturn buf;\r\n}\r\nint lu_buf_check_and_grow(struct lu_buf *buf, size_t len)\r\n{\r\nchar *ptr;\r\nif (len <= buf->lb_len)\r\nreturn 0;\r\nptr = libcfs_kvzalloc(len, GFP_NOFS);\r\nif (!ptr)\r\nreturn -ENOMEM;\r\nif (buf->lb_buf) {\r\nmemcpy(ptr, buf->lb_buf, buf->lb_len);\r\nkvfree(buf->lb_buf);\r\n}\r\nbuf->lb_buf = ptr;\r\nbuf->lb_len = len;\r\nreturn 0;\r\n}
