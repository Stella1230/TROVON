int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order)\r\n{\r\nunsigned long hpt = 0;\r\nint cma = 0;\r\nstruct page *page = NULL;\r\nstruct revmap_entry *rev;\r\nunsigned long npte;\r\nif ((order < PPC_MIN_HPT_ORDER) || (order > PPC_MAX_HPT_ORDER))\r\nreturn -EINVAL;\r\npage = kvm_alloc_hpt_cma(1ul << (order - PAGE_SHIFT));\r\nif (page) {\r\nhpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));\r\nmemset((void *)hpt, 0, (1ul << order));\r\ncma = 1;\r\n}\r\nif (!hpt)\r\nhpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT\r\n|__GFP_NOWARN, order - PAGE_SHIFT);\r\nif (!hpt)\r\nreturn -ENOMEM;\r\nnpte = 1ul << (order - 4);\r\nrev = vmalloc(sizeof(struct revmap_entry) * npte);\r\nif (!rev) {\r\npr_err("kvmppc_allocate_hpt: Couldn't alloc reverse map array\n");\r\nif (cma)\r\nkvm_free_hpt_cma(page, 1 << (order - PAGE_SHIFT));\r\nelse\r\nfree_pages(hpt, order - PAGE_SHIFT);\r\nreturn -ENOMEM;\r\n}\r\ninfo->order = order;\r\ninfo->virt = hpt;\r\ninfo->cma = cma;\r\ninfo->rev = rev;\r\nreturn 0;\r\n}\r\nvoid kvmppc_set_hpt(struct kvm *kvm, struct kvm_hpt_info *info)\r\n{\r\natomic64_set(&kvm->arch.mmio_update, 0);\r\nkvm->arch.hpt = *info;\r\nkvm->arch.sdr1 = __pa(info->virt) | (info->order - 18);\r\npr_debug("KVM guest htab at %lx (order %ld), LPID %x\n",\r\ninfo->virt, (long)info->order, kvm->arch.lpid);\r\n}\r\nlong kvmppc_alloc_reset_hpt(struct kvm *kvm, int order)\r\n{\r\nlong err = -EBUSY;\r\nstruct kvm_hpt_info info;\r\nif (kvm_is_radix(kvm))\r\nreturn -EINVAL;\r\nmutex_lock(&kvm->lock);\r\nif (kvm->arch.hpte_setup_done) {\r\nkvm->arch.hpte_setup_done = 0;\r\nsmp_mb();\r\nif (atomic_read(&kvm->arch.vcpus_running)) {\r\nkvm->arch.hpte_setup_done = 1;\r\ngoto out;\r\n}\r\n}\r\nif (kvm->arch.hpt.order == order) {\r\nmemset((void *)kvm->arch.hpt.virt, 0, 1ul << order);\r\nkvmppc_rmap_reset(kvm);\r\ncpumask_setall(&kvm->arch.need_tlb_flush);\r\nerr = 0;\r\ngoto out;\r\n}\r\nif (kvm->arch.hpt.virt)\r\nkvmppc_free_hpt(&kvm->arch.hpt);\r\nerr = kvmppc_allocate_hpt(&info, order);\r\nif (err < 0)\r\ngoto out;\r\nkvmppc_set_hpt(kvm, &info);\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn err;\r\n}\r\nvoid kvmppc_free_hpt(struct kvm_hpt_info *info)\r\n{\r\nvfree(info->rev);\r\nif (info->cma)\r\nkvm_free_hpt_cma(virt_to_page(info->virt),\r\n1 << (info->order - PAGE_SHIFT));\r\nelse if (info->virt)\r\nfree_pages(info->virt, info->order - PAGE_SHIFT);\r\ninfo->virt = 0;\r\ninfo->order = 0;\r\n}\r\nstatic inline unsigned long hpte0_pgsize_encoding(unsigned long pgsize)\r\n{\r\nreturn (pgsize > 0x1000) ? HPTE_V_LARGE : 0;\r\n}\r\nstatic inline unsigned long hpte1_pgsize_encoding(unsigned long pgsize)\r\n{\r\nreturn (pgsize == 0x10000) ? 0x1000 : 0;\r\n}\r\nvoid kvmppc_map_vrma(struct kvm_vcpu *vcpu, struct kvm_memory_slot *memslot,\r\nunsigned long porder)\r\n{\r\nunsigned long i;\r\nunsigned long npages;\r\nunsigned long hp_v, hp_r;\r\nunsigned long addr, hash;\r\nunsigned long psize;\r\nunsigned long hp0, hp1;\r\nunsigned long idx_ret;\r\nlong ret;\r\nstruct kvm *kvm = vcpu->kvm;\r\npsize = 1ul << porder;\r\nnpages = memslot->npages >> (porder - PAGE_SHIFT);\r\nif (npages > 1ul << (40 - porder))\r\nnpages = 1ul << (40 - porder);\r\nif (npages > kvmppc_hpt_mask(&kvm->arch.hpt) + 1)\r\nnpages = kvmppc_hpt_mask(&kvm->arch.hpt) + 1;\r\nhp0 = HPTE_V_1TB_SEG | (VRMA_VSID << (40 - 16)) |\r\nHPTE_V_BOLTED | hpte0_pgsize_encoding(psize);\r\nhp1 = hpte1_pgsize_encoding(psize) |\r\nHPTE_R_R | HPTE_R_C | HPTE_R_M | PP_RWXX;\r\nfor (i = 0; i < npages; ++i) {\r\naddr = i << porder;\r\nhash = (i ^ (VRMA_VSID ^ (VRMA_VSID << 25)))\r\n& kvmppc_hpt_mask(&kvm->arch.hpt);\r\nhash = (hash << 3) + 7;\r\nhp_v = hp0 | ((addr >> 16) & ~0x7fUL);\r\nhp_r = hp1 | addr;\r\nret = kvmppc_virtmode_do_h_enter(kvm, H_EXACT, hash, hp_v, hp_r,\r\n&idx_ret);\r\nif (ret != H_SUCCESS) {\r\npr_err("KVM: map_vrma at %lx failed, ret=%ld\n",\r\naddr, ret);\r\nbreak;\r\n}\r\n}\r\n}\r\nint kvmppc_mmu_hv_init(void)\r\n{\r\nunsigned long host_lpid, rsvd_lpid;\r\nif (!cpu_has_feature(CPU_FTR_HVMODE))\r\nreturn -EINVAL;\r\nhost_lpid = mfspr(SPRN_LPID);\r\nrsvd_lpid = LPID_RSVD;\r\nkvmppc_init_lpid(rsvd_lpid + 1);\r\nkvmppc_claim_lpid(host_lpid);\r\nkvmppc_claim_lpid(rsvd_lpid);\r\nreturn 0;\r\n}\r\nstatic void kvmppc_mmu_book3s_64_hv_reset_msr(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long msr = vcpu->arch.intr_msr;\r\nif (MSR_TM_TRANSACTIONAL(vcpu->arch.shregs.msr))\r\nmsr |= MSR_TS_S;\r\nelse\r\nmsr |= vcpu->arch.shregs.msr & MSR_TS_MASK;\r\nkvmppc_set_msr(vcpu, msr);\r\n}\r\nstatic long kvmppc_virtmode_do_h_enter(struct kvm *kvm, unsigned long flags,\r\nlong pte_index, unsigned long pteh,\r\nunsigned long ptel, unsigned long *pte_idx_ret)\r\n{\r\nlong ret;\r\nrcu_read_lock_sched();\r\nret = kvmppc_do_h_enter(kvm, flags, pte_index, pteh, ptel,\r\ncurrent->mm->pgd, false, pte_idx_ret);\r\nrcu_read_unlock_sched();\r\nif (ret == H_TOO_HARD) {\r\npr_err("KVM: Oops, kvmppc_h_enter returned too hard!\n");\r\nret = H_RESOURCE;\r\n}\r\nreturn ret;\r\n}\r\nstatic struct kvmppc_slb *kvmppc_mmu_book3s_hv_find_slbe(struct kvm_vcpu *vcpu,\r\ngva_t eaddr)\r\n{\r\nu64 mask;\r\nint i;\r\nfor (i = 0; i < vcpu->arch.slb_nr; i++) {\r\nif (!(vcpu->arch.slb[i].orige & SLB_ESID_V))\r\ncontinue;\r\nif (vcpu->arch.slb[i].origv & SLB_VSID_B_1T)\r\nmask = ESID_MASK_1T;\r\nelse\r\nmask = ESID_MASK;\r\nif (((vcpu->arch.slb[i].orige ^ eaddr) & mask) == 0)\r\nreturn &vcpu->arch.slb[i];\r\n}\r\nreturn NULL;\r\n}\r\nstatic unsigned long kvmppc_mmu_get_real_addr(unsigned long v, unsigned long r,\r\nunsigned long ea)\r\n{\r\nunsigned long ra_mask;\r\nra_mask = hpte_page_size(v, r) - 1;\r\nreturn (r & HPTE_R_RPN & ~ra_mask) | (ea & ra_mask);\r\n}\r\nstatic int kvmppc_mmu_book3s_64_hv_xlate(struct kvm_vcpu *vcpu, gva_t eaddr,\r\nstruct kvmppc_pte *gpte, bool data, bool iswrite)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nstruct kvmppc_slb *slbe;\r\nunsigned long slb_v;\r\nunsigned long pp, key;\r\nunsigned long v, orig_v, gr;\r\n__be64 *hptep;\r\nint index;\r\nint virtmode = vcpu->arch.shregs.msr & (data ? MSR_DR : MSR_IR);\r\nif (virtmode) {\r\nslbe = kvmppc_mmu_book3s_hv_find_slbe(vcpu, eaddr);\r\nif (!slbe)\r\nreturn -EINVAL;\r\nslb_v = slbe->origv;\r\n} else {\r\nslb_v = vcpu->kvm->arch.vrma_slb_v;\r\n}\r\npreempt_disable();\r\nindex = kvmppc_hv_find_lock_hpte(kvm, eaddr, slb_v,\r\nHPTE_V_VALID | HPTE_V_ABSENT);\r\nif (index < 0) {\r\npreempt_enable();\r\nreturn -ENOENT;\r\n}\r\nhptep = (__be64 *)(kvm->arch.hpt.virt + (index << 4));\r\nv = orig_v = be64_to_cpu(hptep[0]) & ~HPTE_V_HVLOCK;\r\nif (cpu_has_feature(CPU_FTR_ARCH_300))\r\nv = hpte_new_to_old_v(v, be64_to_cpu(hptep[1]));\r\ngr = kvm->arch.hpt.rev[index].guest_rpte;\r\nunlock_hpte(hptep, orig_v);\r\npreempt_enable();\r\ngpte->eaddr = eaddr;\r\ngpte->vpage = ((v & HPTE_V_AVPN) << 4) | ((eaddr >> 12) & 0xfff);\r\npp = gr & (HPTE_R_PP0 | HPTE_R_PP);\r\nkey = (vcpu->arch.shregs.msr & MSR_PR) ? SLB_VSID_KP : SLB_VSID_KS;\r\nkey &= slb_v;\r\ngpte->may_read = hpte_read_permission(pp, key);\r\ngpte->may_write = hpte_write_permission(pp, key);\r\ngpte->may_execute = gpte->may_read && !(gr & (HPTE_R_N | HPTE_R_G));\r\nif (data && virtmode) {\r\nint amrfield = hpte_get_skey_perm(gr, vcpu->arch.amr);\r\nif (amrfield & 1)\r\ngpte->may_read = 0;\r\nif (amrfield & 2)\r\ngpte->may_write = 0;\r\n}\r\ngpte->raddr = kvmppc_mmu_get_real_addr(v, gr, eaddr);\r\nreturn 0;\r\n}\r\nstatic int instruction_is_store(unsigned int instr)\r\n{\r\nunsigned int mask;\r\nmask = 0x10000000;\r\nif ((instr & 0xfc000000) == 0x7c000000)\r\nmask = 0x100;\r\nreturn (instr & mask) != 0;\r\n}\r\nint kvmppc_hv_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned long gpa, gva_t ea, int is_store)\r\n{\r\nu32 last_inst;\r\nif (kvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst) !=\r\nEMULATE_DONE)\r\nreturn RESUME_GUEST;\r\nif (instruction_is_store(last_inst) != !!is_store)\r\nreturn RESUME_GUEST;\r\nvcpu->arch.paddr_accessed = gpa;\r\nvcpu->arch.vaddr_accessed = ea;\r\nreturn kvmppc_emulate_mmio(run, vcpu);\r\n}\r\nint kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned long ea, unsigned long dsisr)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nunsigned long hpte[3], r;\r\nunsigned long hnow_v, hnow_r;\r\n__be64 *hptep;\r\nunsigned long mmu_seq, psize, pte_size;\r\nunsigned long gpa_base, gfn_base;\r\nunsigned long gpa, gfn, hva, pfn;\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long *rmap;\r\nstruct revmap_entry *rev;\r\nstruct page *page, *pages[1];\r\nlong index, ret, npages;\r\nbool is_ci;\r\nunsigned int writing, write_ok;\r\nstruct vm_area_struct *vma;\r\nunsigned long rcbits;\r\nlong mmio_update;\r\nif (kvm_is_radix(kvm))\r\nreturn kvmppc_book3s_radix_page_fault(run, vcpu, ea, dsisr);\r\nif (ea != vcpu->arch.pgfault_addr)\r\nreturn RESUME_GUEST;\r\nif (vcpu->arch.pgfault_cache) {\r\nmmio_update = atomic64_read(&kvm->arch.mmio_update);\r\nif (mmio_update == vcpu->arch.pgfault_cache->mmio_update) {\r\nr = vcpu->arch.pgfault_cache->rpte;\r\npsize = hpte_page_size(vcpu->arch.pgfault_hpte[0], r);\r\ngpa_base = r & HPTE_R_RPN & ~(psize - 1);\r\ngfn_base = gpa_base >> PAGE_SHIFT;\r\ngpa = gpa_base | (ea & (psize - 1));\r\nreturn kvmppc_hv_emulate_mmio(run, vcpu, gpa, ea,\r\ndsisr & DSISR_ISSTORE);\r\n}\r\n}\r\nindex = vcpu->arch.pgfault_index;\r\nhptep = (__be64 *)(kvm->arch.hpt.virt + (index << 4));\r\nrev = &kvm->arch.hpt.rev[index];\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptep, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nhpte[0] = be64_to_cpu(hptep[0]) & ~HPTE_V_HVLOCK;\r\nhpte[1] = be64_to_cpu(hptep[1]);\r\nhpte[2] = r = rev->guest_rpte;\r\nunlock_hpte(hptep, hpte[0]);\r\npreempt_enable();\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nhpte[0] = hpte_new_to_old_v(hpte[0], hpte[1]);\r\nhpte[1] = hpte_new_to_old_r(hpte[1]);\r\n}\r\nif (hpte[0] != vcpu->arch.pgfault_hpte[0] ||\r\nhpte[1] != vcpu->arch.pgfault_hpte[1])\r\nreturn RESUME_GUEST;\r\npsize = hpte_page_size(hpte[0], r);\r\ngpa_base = r & HPTE_R_RPN & ~(psize - 1);\r\ngfn_base = gpa_base >> PAGE_SHIFT;\r\ngpa = gpa_base | (ea & (psize - 1));\r\ngfn = gpa >> PAGE_SHIFT;\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\ntrace_kvm_page_fault_enter(vcpu, hpte, memslot, ea, dsisr);\r\nif (!memslot || (memslot->flags & KVM_MEMSLOT_INVALID))\r\nreturn kvmppc_hv_emulate_mmio(run, vcpu, gpa, ea,\r\ndsisr & DSISR_ISSTORE);\r\nif (gfn_base < memslot->base_gfn)\r\nreturn -EFAULT;\r\nmmu_seq = kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\nret = -EFAULT;\r\nis_ci = false;\r\npfn = 0;\r\npage = NULL;\r\npte_size = PAGE_SIZE;\r\nwriting = (dsisr & DSISR_ISSTORE) != 0;\r\nwrite_ok = writing;\r\nhva = gfn_to_hva_memslot(memslot, gfn);\r\nnpages = get_user_pages_fast(hva, 1, writing, pages);\r\nif (npages < 1) {\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma(current->mm, hva);\r\nif (vma && vma->vm_start <= hva && hva + psize <= vma->vm_end &&\r\n(vma->vm_flags & VM_PFNMAP)) {\r\npfn = vma->vm_pgoff +\r\n((hva - vma->vm_start) >> PAGE_SHIFT);\r\npte_size = psize;\r\nis_ci = pte_ci(__pte((pgprot_val(vma->vm_page_prot))));\r\nwrite_ok = vma->vm_flags & VM_WRITE;\r\n}\r\nup_read(&current->mm->mmap_sem);\r\nif (!pfn)\r\ngoto out_put;\r\n} else {\r\npage = pages[0];\r\npfn = page_to_pfn(page);\r\nif (PageHuge(page)) {\r\npage = compound_head(page);\r\npte_size <<= compound_order(page);\r\n}\r\nif (!writing && hpte_is_writable(r)) {\r\npte_t *ptep, pte;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nptep = find_linux_pte_or_hugepte(current->mm->pgd,\r\nhva, NULL, NULL);\r\nif (ptep) {\r\npte = kvmppc_read_update_linux_pte(ptep, 1);\r\nif (__pte_write(pte))\r\nwrite_ok = 1;\r\n}\r\nlocal_irq_restore(flags);\r\n}\r\n}\r\nif (psize > pte_size)\r\ngoto out_put;\r\nif (!hpte_cache_flags_ok(r, is_ci)) {\r\nif (is_ci)\r\ngoto out_put;\r\nr = (r & ~(HPTE_R_W|HPTE_R_I|HPTE_R_G)) | HPTE_R_M;\r\n}\r\nif (psize < PAGE_SIZE)\r\npsize = PAGE_SIZE;\r\nr = (r & HPTE_R_KEY_HI) | (r & ~(HPTE_R_PP0 - psize)) |\r\n((pfn << PAGE_SHIFT) & ~(psize - 1));\r\nif (hpte_is_writable(r) && !write_ok)\r\nr = hpte_make_readonly(r);\r\nret = RESUME_GUEST;\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptep, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nhnow_v = be64_to_cpu(hptep[0]);\r\nhnow_r = be64_to_cpu(hptep[1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nhnow_v = hpte_new_to_old_v(hnow_v, hnow_r);\r\nhnow_r = hpte_new_to_old_r(hnow_r);\r\n}\r\nif ((hnow_v & ~HPTE_V_HVLOCK) != hpte[0] || hnow_r != hpte[1] ||\r\nrev->guest_rpte != hpte[2])\r\ngoto out_unlock;\r\nhpte[0] = (hpte[0] & ~HPTE_V_ABSENT) | HPTE_V_VALID;\r\nrmap = &memslot->arch.rmap[gfn_base - memslot->base_gfn];\r\nlock_rmap(rmap);\r\nret = RESUME_GUEST;\r\nif (mmu_notifier_retry(vcpu->kvm, mmu_seq)) {\r\nunlock_rmap(rmap);\r\ngoto out_unlock;\r\n}\r\nrcbits = *rmap >> KVMPPC_RMAP_RC_SHIFT;\r\nr &= rcbits | ~(HPTE_R_R | HPTE_R_C);\r\nif (be64_to_cpu(hptep[0]) & HPTE_V_VALID) {\r\nunlock_rmap(rmap);\r\nhptep[0] |= cpu_to_be64(HPTE_V_ABSENT);\r\nkvmppc_invalidate_hpte(kvm, hptep, index);\r\nr |= be64_to_cpu(hptep[1]) & (HPTE_R_R | HPTE_R_C);\r\n} else {\r\nkvmppc_add_revmap_chain(kvm, rev, rmap, index, 0);\r\n}\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nr = hpte_old_to_new_r(hpte[0], r);\r\nhpte[0] = hpte_old_to_new_v(hpte[0]);\r\n}\r\nhptep[1] = cpu_to_be64(r);\r\neieio();\r\n__unlock_hpte(hptep, hpte[0]);\r\nasm volatile("ptesync" : : : "memory");\r\npreempt_enable();\r\nif (page && hpte_is_writable(r))\r\nSetPageDirty(page);\r\nout_put:\r\ntrace_kvm_page_fault_exit(vcpu, hpte, ret);\r\nif (page) {\r\nput_page(pages[0]);\r\n}\r\nreturn ret;\r\nout_unlock:\r\n__unlock_hpte(hptep, be64_to_cpu(hptep[0]));\r\npreempt_enable();\r\ngoto out_put;\r\n}\r\nstatic void kvmppc_rmap_reset(struct kvm *kvm)\r\n{\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nint srcu_idx;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots) {\r\nmemset(memslot->arch.rmap, 0,\r\nmemslot->npages * sizeof(*memslot->arch.rmap));\r\n}\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\n}\r\nstatic int kvm_handle_hva_range(struct kvm *kvm,\r\nunsigned long start,\r\nunsigned long end,\r\nhva_handler_fn handler)\r\n{\r\nint ret;\r\nint retval = 0;\r\nstruct kvm_memslots *slots;\r\nstruct kvm_memory_slot *memslot;\r\nslots = kvm_memslots(kvm);\r\nkvm_for_each_memslot(memslot, slots) {\r\nunsigned long hva_start, hva_end;\r\ngfn_t gfn, gfn_end;\r\nhva_start = max(start, memslot->userspace_addr);\r\nhva_end = min(end, memslot->userspace_addr +\r\n(memslot->npages << PAGE_SHIFT));\r\nif (hva_start >= hva_end)\r\ncontinue;\r\ngfn = hva_to_gfn_memslot(hva_start, memslot);\r\ngfn_end = hva_to_gfn_memslot(hva_end + PAGE_SIZE - 1, memslot);\r\nfor (; gfn < gfn_end; ++gfn) {\r\nret = handler(kvm, memslot, gfn);\r\nretval |= ret;\r\n}\r\n}\r\nreturn retval;\r\n}\r\nstatic int kvm_handle_hva(struct kvm *kvm, unsigned long hva,\r\nhva_handler_fn handler)\r\n{\r\nreturn kvm_handle_hva_range(kvm, hva, hva + 1, handler);\r\n}\r\nstatic void kvmppc_unmap_hpte(struct kvm *kvm, unsigned long i,\r\nunsigned long *rmapp, unsigned long gfn)\r\n{\r\n__be64 *hptep = (__be64 *) (kvm->arch.hpt.virt + (i << 4));\r\nstruct revmap_entry *rev = kvm->arch.hpt.rev;\r\nunsigned long j, h;\r\nunsigned long ptel, psize, rcbits;\r\nj = rev[i].forw;\r\nif (j == i) {\r\n*rmapp &= ~(KVMPPC_RMAP_PRESENT | KVMPPC_RMAP_INDEX);\r\n} else {\r\nh = rev[i].back;\r\nrev[h].forw = j;\r\nrev[j].back = h;\r\nrev[i].forw = rev[i].back = i;\r\n*rmapp = (*rmapp & ~KVMPPC_RMAP_INDEX) | j;\r\n}\r\nptel = rev[i].guest_rpte;\r\npsize = hpte_page_size(be64_to_cpu(hptep[0]), ptel);\r\nif ((be64_to_cpu(hptep[0]) & HPTE_V_VALID) &&\r\nhpte_rpn(ptel, psize) == gfn) {\r\nhptep[0] |= cpu_to_be64(HPTE_V_ABSENT);\r\nkvmppc_invalidate_hpte(kvm, hptep, i);\r\nhptep[1] &= ~cpu_to_be64(HPTE_R_KEY_HI | HPTE_R_KEY_LO);\r\nrcbits = be64_to_cpu(hptep[1]) & (HPTE_R_R | HPTE_R_C);\r\n*rmapp |= rcbits << KVMPPC_RMAP_RC_SHIFT;\r\nif (rcbits & HPTE_R_C)\r\nkvmppc_update_rmap_change(rmapp, psize);\r\nif (rcbits & ~rev[i].guest_rpte) {\r\nrev[i].guest_rpte = ptel | rcbits;\r\nnote_hpte_modification(kvm, &rev[i]);\r\n}\r\n}\r\n}\r\nstatic int kvm_unmap_rmapp(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long gfn)\r\n{\r\nunsigned long i;\r\n__be64 *hptep;\r\nunsigned long *rmapp;\r\nrmapp = &memslot->arch.rmap[gfn - memslot->base_gfn];\r\nfor (;;) {\r\nlock_rmap(rmapp);\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nbreak;\r\n}\r\ni = *rmapp & KVMPPC_RMAP_INDEX;\r\nhptep = (__be64 *) (kvm->arch.hpt.virt + (i << 4));\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (be64_to_cpu(hptep[0]) & HPTE_V_HVLOCK)\r\ncpu_relax();\r\ncontinue;\r\n}\r\nkvmppc_unmap_hpte(kvm, i, rmapp, gfn);\r\nunlock_rmap(rmapp);\r\n__unlock_hpte(hptep, be64_to_cpu(hptep[0]));\r\n}\r\nreturn 0;\r\n}\r\nint kvm_unmap_hva_hv(struct kvm *kvm, unsigned long hva)\r\n{\r\nhva_handler_fn handler;\r\nhandler = kvm_is_radix(kvm) ? kvm_unmap_radix : kvm_unmap_rmapp;\r\nkvm_handle_hva(kvm, hva, handler);\r\nreturn 0;\r\n}\r\nint kvm_unmap_hva_range_hv(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nhva_handler_fn handler;\r\nhandler = kvm_is_radix(kvm) ? kvm_unmap_radix : kvm_unmap_rmapp;\r\nkvm_handle_hva_range(kvm, start, end, handler);\r\nreturn 0;\r\n}\r\nvoid kvmppc_core_flush_memslot_hv(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot)\r\n{\r\nunsigned long gfn;\r\nunsigned long n;\r\nunsigned long *rmapp;\r\ngfn = memslot->base_gfn;\r\nrmapp = memslot->arch.rmap;\r\nfor (n = memslot->npages; n; --n, ++gfn) {\r\nif (kvm_is_radix(kvm)) {\r\nkvm_unmap_radix(kvm, memslot, gfn);\r\ncontinue;\r\n}\r\nif (*rmapp & KVMPPC_RMAP_PRESENT)\r\nkvm_unmap_rmapp(kvm, memslot, gfn);\r\n++rmapp;\r\n}\r\n}\r\nstatic int kvm_age_rmapp(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long gfn)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.hpt.rev;\r\nunsigned long head, i, j;\r\n__be64 *hptep;\r\nint ret = 0;\r\nunsigned long *rmapp;\r\nrmapp = &memslot->arch.rmap[gfn - memslot->base_gfn];\r\nretry:\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED) {\r\n*rmapp &= ~KVMPPC_RMAP_REFERENCED;\r\nret = 1;\r\n}\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nhptep = (__be64 *) (kvm->arch.hpt.virt + (i << 4));\r\nj = rev[i].forw;\r\nif (!(be64_to_cpu(hptep[1]) & HPTE_R_R))\r\ncontinue;\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (be64_to_cpu(hptep[0]) & HPTE_V_HVLOCK)\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nif ((be64_to_cpu(hptep[0]) & HPTE_V_VALID) &&\r\n(be64_to_cpu(hptep[1]) & HPTE_R_R)) {\r\nkvmppc_clear_ref_hpte(kvm, hptep, i);\r\nif (!(rev[i].guest_rpte & HPTE_R_R)) {\r\nrev[i].guest_rpte |= HPTE_R_R;\r\nnote_hpte_modification(kvm, &rev[i]);\r\n}\r\nret = 1;\r\n}\r\n__unlock_hpte(hptep, be64_to_cpu(hptep[0]));\r\n} while ((i = j) != head);\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\nint kvm_age_hva_hv(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nhva_handler_fn handler;\r\nhandler = kvm_is_radix(kvm) ? kvm_age_radix : kvm_age_rmapp;\r\nreturn kvm_handle_hva_range(kvm, start, end, handler);\r\n}\r\nstatic int kvm_test_age_rmapp(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long gfn)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.hpt.rev;\r\nunsigned long head, i, j;\r\nunsigned long *hp;\r\nint ret = 1;\r\nunsigned long *rmapp;\r\nrmapp = &memslot->arch.rmap[gfn - memslot->base_gfn];\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED)\r\nreturn 1;\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_REFERENCED)\r\ngoto out;\r\nif (*rmapp & KVMPPC_RMAP_PRESENT) {\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nhp = (unsigned long *)(kvm->arch.hpt.virt + (i << 4));\r\nj = rev[i].forw;\r\nif (be64_to_cpu(hp[1]) & HPTE_R_R)\r\ngoto out;\r\n} while ((i = j) != head);\r\n}\r\nret = 0;\r\nout:\r\nunlock_rmap(rmapp);\r\nreturn ret;\r\n}\r\nint kvm_test_age_hva_hv(struct kvm *kvm, unsigned long hva)\r\n{\r\nhva_handler_fn handler;\r\nhandler = kvm_is_radix(kvm) ? kvm_test_age_radix : kvm_test_age_rmapp;\r\nreturn kvm_handle_hva(kvm, hva, handler);\r\n}\r\nvoid kvm_set_spte_hva_hv(struct kvm *kvm, unsigned long hva, pte_t pte)\r\n{\r\nhva_handler_fn handler;\r\nhandler = kvm_is_radix(kvm) ? kvm_unmap_radix : kvm_unmap_rmapp;\r\nkvm_handle_hva(kvm, hva, handler);\r\n}\r\nstatic int vcpus_running(struct kvm *kvm)\r\n{\r\nreturn atomic_read(&kvm->arch.vcpus_running) != 0;\r\n}\r\nstatic int kvm_test_clear_dirty_npages(struct kvm *kvm, unsigned long *rmapp)\r\n{\r\nstruct revmap_entry *rev = kvm->arch.hpt.rev;\r\nunsigned long head, i, j;\r\nunsigned long n;\r\nunsigned long v, r;\r\n__be64 *hptep;\r\nint npages_dirty = 0;\r\nretry:\r\nlock_rmap(rmapp);\r\nif (*rmapp & KVMPPC_RMAP_CHANGED) {\r\nlong change_order = (*rmapp & KVMPPC_RMAP_CHG_ORDER)\r\n>> KVMPPC_RMAP_CHG_SHIFT;\r\n*rmapp &= ~(KVMPPC_RMAP_CHANGED | KVMPPC_RMAP_CHG_ORDER);\r\nnpages_dirty = 1;\r\nif (change_order > PAGE_SHIFT)\r\nnpages_dirty = 1ul << (change_order - PAGE_SHIFT);\r\n}\r\nif (!(*rmapp & KVMPPC_RMAP_PRESENT)) {\r\nunlock_rmap(rmapp);\r\nreturn npages_dirty;\r\n}\r\ni = head = *rmapp & KVMPPC_RMAP_INDEX;\r\ndo {\r\nunsigned long hptep1;\r\nhptep = (__be64 *) (kvm->arch.hpt.virt + (i << 4));\r\nj = rev[i].forw;\r\nhptep1 = be64_to_cpu(hptep[1]);\r\nif (!(hptep1 & HPTE_R_C) &&\r\n(!hpte_is_writable(hptep1) || vcpus_running(kvm)))\r\ncontinue;\r\nif (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {\r\nunlock_rmap(rmapp);\r\nwhile (hptep[0] & cpu_to_be64(HPTE_V_HVLOCK))\r\ncpu_relax();\r\ngoto retry;\r\n}\r\nif (!(hptep[0] & cpu_to_be64(HPTE_V_VALID))) {\r\n__unlock_hpte(hptep, be64_to_cpu(hptep[0]));\r\ncontinue;\r\n}\r\nhptep[0] |= cpu_to_be64(HPTE_V_ABSENT);\r\nkvmppc_invalidate_hpte(kvm, hptep, i);\r\nv = be64_to_cpu(hptep[0]);\r\nr = be64_to_cpu(hptep[1]);\r\nif (r & HPTE_R_C) {\r\nhptep[1] = cpu_to_be64(r & ~HPTE_R_C);\r\nif (!(rev[i].guest_rpte & HPTE_R_C)) {\r\nrev[i].guest_rpte |= HPTE_R_C;\r\nnote_hpte_modification(kvm, &rev[i]);\r\n}\r\nn = hpte_page_size(v, r);\r\nn = (n + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nif (n > npages_dirty)\r\nnpages_dirty = n;\r\neieio();\r\n}\r\nv &= ~HPTE_V_ABSENT;\r\nv |= HPTE_V_VALID;\r\n__unlock_hpte(hptep, v);\r\n} while ((i = j) != head);\r\nunlock_rmap(rmapp);\r\nreturn npages_dirty;\r\n}\r\nvoid kvmppc_harvest_vpa_dirty(struct kvmppc_vpa *vpa,\r\nstruct kvm_memory_slot *memslot,\r\nunsigned long *map)\r\n{\r\nunsigned long gfn;\r\nif (!vpa->dirty || !vpa->pinned_addr)\r\nreturn;\r\ngfn = vpa->gpa >> PAGE_SHIFT;\r\nif (gfn < memslot->base_gfn ||\r\ngfn >= memslot->base_gfn + memslot->npages)\r\nreturn;\r\nvpa->dirty = false;\r\nif (map)\r\n__set_bit_le(gfn - memslot->base_gfn, map);\r\n}\r\nlong kvmppc_hv_get_dirty_log_hpt(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot, unsigned long *map)\r\n{\r\nunsigned long i, j;\r\nunsigned long *rmapp;\r\npreempt_disable();\r\nrmapp = memslot->arch.rmap;\r\nfor (i = 0; i < memslot->npages; ++i) {\r\nint npages = kvm_test_clear_dirty_npages(kvm, rmapp);\r\nif (npages && map)\r\nfor (j = i; npages; ++j, --npages)\r\n__set_bit_le(j, map);\r\n++rmapp;\r\n}\r\npreempt_enable();\r\nreturn 0;\r\n}\r\nvoid *kvmppc_pin_guest_page(struct kvm *kvm, unsigned long gpa,\r\nunsigned long *nb_ret)\r\n{\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long gfn = gpa >> PAGE_SHIFT;\r\nstruct page *page, *pages[1];\r\nint npages;\r\nunsigned long hva, offset;\r\nint srcu_idx;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\nif (!memslot || (memslot->flags & KVM_MEMSLOT_INVALID))\r\ngoto err;\r\nhva = gfn_to_hva_memslot(memslot, gfn);\r\nnpages = get_user_pages_fast(hva, 1, 1, pages);\r\nif (npages < 1)\r\ngoto err;\r\npage = pages[0];\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\noffset = gpa & (PAGE_SIZE - 1);\r\nif (nb_ret)\r\n*nb_ret = PAGE_SIZE - offset;\r\nreturn page_address(page) + offset;\r\nerr:\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\nreturn NULL;\r\n}\r\nvoid kvmppc_unpin_guest_page(struct kvm *kvm, void *va, unsigned long gpa,\r\nbool dirty)\r\n{\r\nstruct page *page = virt_to_page(va);\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long gfn;\r\nunsigned long *rmap;\r\nint srcu_idx;\r\nput_page(page);\r\nif (!dirty)\r\nreturn;\r\ngfn = gpa >> PAGE_SHIFT;\r\nsrcu_idx = srcu_read_lock(&kvm->srcu);\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\nif (memslot) {\r\nif (!kvm_is_radix(kvm)) {\r\nrmap = &memslot->arch.rmap[gfn - memslot->base_gfn];\r\nlock_rmap(rmap);\r\n*rmap |= KVMPPC_RMAP_CHANGED;\r\nunlock_rmap(rmap);\r\n} else if (memslot->dirty_bitmap) {\r\nmark_page_dirty(kvm, gfn);\r\n}\r\n}\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\n}\r\nstatic int resize_hpt_allocate(struct kvm_resize_hpt *resize)\r\n{\r\nint rc;\r\nrc = kvmppc_allocate_hpt(&resize->hpt, resize->order);\r\nif (rc < 0)\r\nreturn rc;\r\nresize_hpt_debug(resize, "resize_hpt_allocate(): HPT @ 0x%lx\n",\r\nresize->hpt.virt);\r\nreturn 0;\r\n}\r\nstatic unsigned long resize_hpt_rehash_hpte(struct kvm_resize_hpt *resize,\r\nunsigned long idx)\r\n{\r\nstruct kvm *kvm = resize->kvm;\r\nstruct kvm_hpt_info *old = &kvm->arch.hpt;\r\nstruct kvm_hpt_info *new = &resize->hpt;\r\nunsigned long old_hash_mask = (1ULL << (old->order - 7)) - 1;\r\nunsigned long new_hash_mask = (1ULL << (new->order - 7)) - 1;\r\n__be64 *hptep, *new_hptep;\r\nunsigned long vpte, rpte, guest_rpte;\r\nint ret;\r\nstruct revmap_entry *rev;\r\nunsigned long apsize, psize, avpn, pteg, hash;\r\nunsigned long new_idx, new_pteg, replace_vpte;\r\nhptep = (__be64 *)(old->virt + (idx << 4));\r\nvpte = be64_to_cpu(hptep[0]);\r\nif (!(vpte & HPTE_V_VALID) && !(vpte & HPTE_V_ABSENT))\r\nreturn 0;\r\nwhile (!try_lock_hpte(hptep, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nvpte = be64_to_cpu(hptep[0]);\r\nret = 0;\r\nif (!(vpte & HPTE_V_VALID) && !(vpte & HPTE_V_ABSENT))\r\ngoto out;\r\nrev = &old->rev[idx];\r\nguest_rpte = rev->guest_rpte;\r\nret = -EIO;\r\napsize = hpte_page_size(vpte, guest_rpte);\r\nif (!apsize)\r\ngoto out;\r\nif (vpte & HPTE_V_VALID) {\r\nunsigned long gfn = hpte_rpn(guest_rpte, apsize);\r\nint srcu_idx = srcu_read_lock(&kvm->srcu);\r\nstruct kvm_memory_slot *memslot =\r\n__gfn_to_memslot(kvm_memslots(kvm), gfn);\r\nif (memslot) {\r\nunsigned long *rmapp;\r\nrmapp = &memslot->arch.rmap[gfn - memslot->base_gfn];\r\nlock_rmap(rmapp);\r\nkvmppc_unmap_hpte(kvm, idx, rmapp, gfn);\r\nunlock_rmap(rmapp);\r\n}\r\nsrcu_read_unlock(&kvm->srcu, srcu_idx);\r\n}\r\nvpte = be64_to_cpu(hptep[0]);\r\nBUG_ON(vpte & HPTE_V_VALID);\r\nBUG_ON(!(vpte & HPTE_V_ABSENT));\r\nret = 0;\r\nif (!(vpte & HPTE_V_BOLTED))\r\ngoto out;\r\nrpte = be64_to_cpu(hptep[1]);\r\npsize = hpte_base_page_size(vpte, rpte);\r\navpn = HPTE_V_AVPN_VAL(vpte) & ~((psize - 1) >> 23);\r\npteg = idx / HPTES_PER_GROUP;\r\nif (vpte & HPTE_V_SECONDARY)\r\npteg = ~pteg;\r\nif (!(vpte & HPTE_V_1TB_SEG)) {\r\nunsigned long offset, vsid;\r\noffset = (avpn & 0x1f) << 23;\r\nvsid = avpn >> 5;\r\nif (psize < (1ULL << 23))\r\noffset |= ((vsid ^ pteg) & old_hash_mask) * psize;\r\nhash = vsid ^ (offset / psize);\r\n} else {\r\nunsigned long offset, vsid;\r\noffset = (avpn & 0x1ffff) << 23;\r\nvsid = avpn >> 17;\r\nif (psize < (1ULL << 23))\r\noffset |= ((vsid ^ (vsid << 25) ^ pteg) & old_hash_mask) * psize;\r\nhash = vsid ^ (vsid << 25) ^ (offset / psize);\r\n}\r\nnew_pteg = hash & new_hash_mask;\r\nif (vpte & HPTE_V_SECONDARY) {\r\nBUG_ON(~pteg != (hash & old_hash_mask));\r\nnew_pteg = ~new_pteg;\r\n} else {\r\nBUG_ON(pteg != (hash & old_hash_mask));\r\n}\r\nnew_idx = new_pteg * HPTES_PER_GROUP + (idx % HPTES_PER_GROUP);\r\nnew_hptep = (__be64 *)(new->virt + (new_idx << 4));\r\nreplace_vpte = be64_to_cpu(new_hptep[0]);\r\nif (replace_vpte & (HPTE_V_VALID | HPTE_V_ABSENT)) {\r\nBUG_ON(new->order >= old->order);\r\nif (replace_vpte & HPTE_V_BOLTED) {\r\nif (vpte & HPTE_V_BOLTED)\r\nret = -ENOSPC;\r\ngoto out;\r\n}\r\n}\r\nnew_hptep[1] = cpu_to_be64(rpte);\r\nnew->rev[new_idx].guest_rpte = guest_rpte;\r\nnew_hptep[0] = cpu_to_be64(vpte);\r\nunlock_hpte(new_hptep, vpte);\r\nout:\r\nunlock_hpte(hptep, vpte);\r\nreturn ret;\r\n}\r\nstatic int resize_hpt_rehash(struct kvm_resize_hpt *resize)\r\n{\r\nstruct kvm *kvm = resize->kvm;\r\nunsigned long i;\r\nint rc;\r\nif (cpu_has_feature(CPU_FTR_ARCH_300))\r\nreturn -EIO;\r\nfor (i = 0; i < kvmppc_hpt_npte(&kvm->arch.hpt); i++) {\r\nrc = resize_hpt_rehash_hpte(resize, i);\r\nif (rc != 0)\r\nreturn rc;\r\n}\r\nreturn 0;\r\n}\r\nstatic void resize_hpt_pivot(struct kvm_resize_hpt *resize)\r\n{\r\nstruct kvm *kvm = resize->kvm;\r\nstruct kvm_hpt_info hpt_tmp;\r\nresize_hpt_debug(resize, "resize_hpt_pivot()\n");\r\nspin_lock(&kvm->mmu_lock);\r\nasm volatile("ptesync" : : : "memory");\r\nhpt_tmp = kvm->arch.hpt;\r\nkvmppc_set_hpt(kvm, &resize->hpt);\r\nresize->hpt = hpt_tmp;\r\nspin_unlock(&kvm->mmu_lock);\r\nsynchronize_srcu_expedited(&kvm->srcu);\r\nresize_hpt_debug(resize, "resize_hpt_pivot() done\n");\r\n}\r\nstatic void resize_hpt_release(struct kvm *kvm, struct kvm_resize_hpt *resize)\r\n{\r\nBUG_ON(kvm->arch.resize_hpt != resize);\r\nif (!resize)\r\nreturn;\r\nif (resize->hpt.virt)\r\nkvmppc_free_hpt(&resize->hpt);\r\nkvm->arch.resize_hpt = NULL;\r\nkfree(resize);\r\n}\r\nstatic void resize_hpt_prepare_work(struct work_struct *work)\r\n{\r\nstruct kvm_resize_hpt *resize = container_of(work,\r\nstruct kvm_resize_hpt,\r\nwork);\r\nstruct kvm *kvm = resize->kvm;\r\nint err;\r\nresize_hpt_debug(resize, "resize_hpt_prepare_work(): order = %d\n",\r\nresize->order);\r\nerr = resize_hpt_allocate(resize);\r\nmutex_lock(&kvm->lock);\r\nresize->error = err;\r\nresize->prepare_done = true;\r\nmutex_unlock(&kvm->lock);\r\n}\r\nlong kvm_vm_ioctl_resize_hpt_prepare(struct kvm *kvm,\r\nstruct kvm_ppc_resize_hpt *rhpt)\r\n{\r\nunsigned long flags = rhpt->flags;\r\nunsigned long shift = rhpt->shift;\r\nstruct kvm_resize_hpt *resize;\r\nint ret;\r\nif (flags != 0)\r\nreturn -EINVAL;\r\nif (shift && ((shift < 18) || (shift > 46)))\r\nreturn -EINVAL;\r\nmutex_lock(&kvm->lock);\r\nresize = kvm->arch.resize_hpt;\r\nif (resize) {\r\nif (resize->order == shift) {\r\nif (resize->prepare_done) {\r\nret = resize->error;\r\nif (ret != 0)\r\nresize_hpt_release(kvm, resize);\r\n} else {\r\nret = 100;\r\n}\r\ngoto out;\r\n}\r\nresize_hpt_release(kvm, resize);\r\n}\r\nret = 0;\r\nif (!shift)\r\ngoto out;\r\nresize = kzalloc(sizeof(*resize), GFP_KERNEL);\r\nif (!resize) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nresize->order = shift;\r\nresize->kvm = kvm;\r\nINIT_WORK(&resize->work, resize_hpt_prepare_work);\r\nkvm->arch.resize_hpt = resize;\r\nschedule_work(&resize->work);\r\nret = 100;\r\nout:\r\nmutex_unlock(&kvm->lock);\r\nreturn ret;\r\n}\r\nstatic void resize_hpt_boot_vcpu(void *opaque)\r\n{\r\n}\r\nlong kvm_vm_ioctl_resize_hpt_commit(struct kvm *kvm,\r\nstruct kvm_ppc_resize_hpt *rhpt)\r\n{\r\nunsigned long flags = rhpt->flags;\r\nunsigned long shift = rhpt->shift;\r\nstruct kvm_resize_hpt *resize;\r\nlong ret;\r\nif (flags != 0)\r\nreturn -EINVAL;\r\nif (shift && ((shift < 18) || (shift > 46)))\r\nreturn -EINVAL;\r\nmutex_lock(&kvm->lock);\r\nresize = kvm->arch.resize_hpt;\r\nret = -EIO;\r\nif (WARN_ON(!kvm->arch.hpte_setup_done))\r\ngoto out_no_hpt;\r\nkvm->arch.hpte_setup_done = 0;\r\nsmp_mb();\r\non_each_cpu(resize_hpt_boot_vcpu, NULL, 1);\r\nret = -ENXIO;\r\nif (!resize || (resize->order != shift))\r\ngoto out;\r\nret = -EBUSY;\r\nif (!resize->prepare_done)\r\ngoto out;\r\nret = resize->error;\r\nif (ret != 0)\r\ngoto out;\r\nret = resize_hpt_rehash(resize);\r\nif (ret != 0)\r\ngoto out;\r\nresize_hpt_pivot(resize);\r\nout:\r\nkvm->arch.hpte_setup_done = 1;\r\nsmp_mb();\r\nout_no_hpt:\r\nresize_hpt_release(kvm, resize);\r\nmutex_unlock(&kvm->lock);\r\nreturn ret;\r\n}\r\nstatic int hpte_dirty(struct revmap_entry *revp, __be64 *hptp)\r\n{\r\nunsigned long rcbits_unset;\r\nif (revp->guest_rpte & HPTE_GR_MODIFIED)\r\nreturn 1;\r\nrcbits_unset = ~revp->guest_rpte & (HPTE_R_R | HPTE_R_C);\r\nif ((be64_to_cpu(hptp[0]) & HPTE_V_VALID) &&\r\n(be64_to_cpu(hptp[1]) & rcbits_unset))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic long record_hpte(unsigned long flags, __be64 *hptp,\r\nunsigned long *hpte, struct revmap_entry *revp,\r\nint want_valid, int first_pass)\r\n{\r\nunsigned long v, r, hr;\r\nunsigned long rcbits_unset;\r\nint ok = 1;\r\nint valid, dirty;\r\ndirty = hpte_dirty(revp, hptp);\r\nif (!first_pass && !dirty)\r\nreturn 0;\r\nvalid = 0;\r\nif (be64_to_cpu(hptp[0]) & (HPTE_V_VALID | HPTE_V_ABSENT)) {\r\nvalid = 1;\r\nif ((flags & KVM_GET_HTAB_BOLTED_ONLY) &&\r\n!(be64_to_cpu(hptp[0]) & HPTE_V_BOLTED))\r\nvalid = 0;\r\n}\r\nif (valid != want_valid)\r\nreturn 0;\r\nv = r = 0;\r\nif (valid || dirty) {\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptp, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = be64_to_cpu(hptp[0]);\r\nhr = be64_to_cpu(hptp[1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nv = hpte_new_to_old_v(v, hr);\r\nhr = hpte_new_to_old_r(hr);\r\n}\r\nvalid = !!(v & HPTE_V_VALID);\r\ndirty = !!(revp->guest_rpte & HPTE_GR_MODIFIED);\r\nrcbits_unset = ~revp->guest_rpte & (HPTE_R_R | HPTE_R_C);\r\nif (valid && (rcbits_unset & hr)) {\r\nrevp->guest_rpte |= (hr &\r\n(HPTE_R_R | HPTE_R_C)) | HPTE_GR_MODIFIED;\r\ndirty = 1;\r\n}\r\nif (v & HPTE_V_ABSENT) {\r\nv &= ~HPTE_V_ABSENT;\r\nv |= HPTE_V_VALID;\r\nvalid = 1;\r\n}\r\nif ((flags & KVM_GET_HTAB_BOLTED_ONLY) && !(v & HPTE_V_BOLTED))\r\nvalid = 0;\r\nr = revp->guest_rpte;\r\nif (valid == want_valid && dirty) {\r\nr &= ~HPTE_GR_MODIFIED;\r\nrevp->guest_rpte = r;\r\n}\r\nunlock_hpte(hptp, be64_to_cpu(hptp[0]));\r\npreempt_enable();\r\nif (!(valid == want_valid && (first_pass || dirty)))\r\nok = 0;\r\n}\r\nhpte[0] = cpu_to_be64(v);\r\nhpte[1] = cpu_to_be64(r);\r\nreturn ok;\r\n}\r\nstatic ssize_t kvm_htab_read(struct file *file, char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct kvm_htab_ctx *ctx = file->private_data;\r\nstruct kvm *kvm = ctx->kvm;\r\nstruct kvm_get_htab_header hdr;\r\n__be64 *hptp;\r\nstruct revmap_entry *revp;\r\nunsigned long i, nb, nw;\r\nunsigned long __user *lbuf;\r\nstruct kvm_get_htab_header __user *hptr;\r\nunsigned long flags;\r\nint first_pass;\r\nunsigned long hpte[2];\r\nif (!access_ok(VERIFY_WRITE, buf, count))\r\nreturn -EFAULT;\r\nfirst_pass = ctx->first_pass;\r\nflags = ctx->flags;\r\ni = ctx->index;\r\nhptp = (__be64 *)(kvm->arch.hpt.virt + (i * HPTE_SIZE));\r\nrevp = kvm->arch.hpt.rev + i;\r\nlbuf = (unsigned long __user *)buf;\r\nnb = 0;\r\nwhile (nb + sizeof(hdr) + HPTE_SIZE < count) {\r\nhptr = (struct kvm_get_htab_header __user *)buf;\r\nhdr.n_valid = 0;\r\nhdr.n_invalid = 0;\r\nnw = nb;\r\nnb += sizeof(hdr);\r\nlbuf = (unsigned long __user *)(buf + sizeof(hdr));\r\nif (!first_pass) {\r\nwhile (i < kvmppc_hpt_npte(&kvm->arch.hpt) &&\r\n!hpte_dirty(revp, hptp)) {\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\n}\r\nhdr.index = i;\r\nwhile (i < kvmppc_hpt_npte(&kvm->arch.hpt) &&\r\nhdr.n_valid < 0xffff &&\r\nnb + HPTE_SIZE < count &&\r\nrecord_hpte(flags, hptp, hpte, revp, 1, first_pass)) {\r\n++hdr.n_valid;\r\nif (__put_user(hpte[0], lbuf) ||\r\n__put_user(hpte[1], lbuf + 1))\r\nreturn -EFAULT;\r\nnb += HPTE_SIZE;\r\nlbuf += 2;\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\nwhile (i < kvmppc_hpt_npte(&kvm->arch.hpt) &&\r\nhdr.n_invalid < 0xffff &&\r\nrecord_hpte(flags, hptp, hpte, revp, 0, first_pass)) {\r\n++hdr.n_invalid;\r\n++i;\r\nhptp += 2;\r\n++revp;\r\n}\r\nif (hdr.n_valid || hdr.n_invalid) {\r\nif (__copy_to_user(hptr, &hdr, sizeof(hdr)))\r\nreturn -EFAULT;\r\nnw = nb;\r\nbuf = (char __user *)lbuf;\r\n} else {\r\nnb = nw;\r\n}\r\nif (i >= kvmppc_hpt_npte(&kvm->arch.hpt)) {\r\ni = 0;\r\nctx->first_pass = 0;\r\nbreak;\r\n}\r\n}\r\nctx->index = i;\r\nreturn nb;\r\n}\r\nstatic ssize_t kvm_htab_write(struct file *file, const char __user *buf,\r\nsize_t count, loff_t *ppos)\r\n{\r\nstruct kvm_htab_ctx *ctx = file->private_data;\r\nstruct kvm *kvm = ctx->kvm;\r\nstruct kvm_get_htab_header hdr;\r\nunsigned long i, j;\r\nunsigned long v, r;\r\nunsigned long __user *lbuf;\r\n__be64 *hptp;\r\nunsigned long tmp[2];\r\nssize_t nb;\r\nlong int err, ret;\r\nint hpte_setup;\r\nif (!access_ok(VERIFY_READ, buf, count))\r\nreturn -EFAULT;\r\nmutex_lock(&kvm->lock);\r\nhpte_setup = kvm->arch.hpte_setup_done;\r\nif (hpte_setup) {\r\nkvm->arch.hpte_setup_done = 0;\r\nsmp_mb();\r\nif (atomic_read(&kvm->arch.vcpus_running)) {\r\nkvm->arch.hpte_setup_done = 1;\r\nmutex_unlock(&kvm->lock);\r\nreturn -EBUSY;\r\n}\r\n}\r\nerr = 0;\r\nfor (nb = 0; nb + sizeof(hdr) <= count; ) {\r\nerr = -EFAULT;\r\nif (__copy_from_user(&hdr, buf, sizeof(hdr)))\r\nbreak;\r\nerr = 0;\r\nif (nb + hdr.n_valid * HPTE_SIZE > count)\r\nbreak;\r\nnb += sizeof(hdr);\r\nbuf += sizeof(hdr);\r\nerr = -EINVAL;\r\ni = hdr.index;\r\nif (i >= kvmppc_hpt_npte(&kvm->arch.hpt) ||\r\ni + hdr.n_valid + hdr.n_invalid > kvmppc_hpt_npte(&kvm->arch.hpt))\r\nbreak;\r\nhptp = (__be64 *)(kvm->arch.hpt.virt + (i * HPTE_SIZE));\r\nlbuf = (unsigned long __user *)buf;\r\nfor (j = 0; j < hdr.n_valid; ++j) {\r\n__be64 hpte_v;\r\n__be64 hpte_r;\r\nerr = -EFAULT;\r\nif (__get_user(hpte_v, lbuf) ||\r\n__get_user(hpte_r, lbuf + 1))\r\ngoto out;\r\nv = be64_to_cpu(hpte_v);\r\nr = be64_to_cpu(hpte_r);\r\nerr = -EINVAL;\r\nif (!(v & HPTE_V_VALID))\r\ngoto out;\r\nlbuf += 2;\r\nnb += HPTE_SIZE;\r\nif (be64_to_cpu(hptp[0]) & (HPTE_V_VALID | HPTE_V_ABSENT))\r\nkvmppc_do_h_remove(kvm, 0, i, 0, tmp);\r\nerr = -EIO;\r\nret = kvmppc_virtmode_do_h_enter(kvm, H_EXACT, i, v, r,\r\ntmp);\r\nif (ret != H_SUCCESS) {\r\npr_err("kvm_htab_write ret %ld i=%ld v=%lx "\r\n"r=%lx\n", ret, i, v, r);\r\ngoto out;\r\n}\r\nif (!hpte_setup && is_vrma_hpte(v)) {\r\nunsigned long psize = hpte_base_page_size(v, r);\r\nunsigned long senc = slb_pgsize_encoding(psize);\r\nunsigned long lpcr;\r\nkvm->arch.vrma_slb_v = senc | SLB_VSID_B_1T |\r\n(VRMA_VSID << SLB_VSID_SHIFT_1T);\r\nlpcr = senc << (LPCR_VRMASD_SH - 4);\r\nkvmppc_update_lpcr(kvm, lpcr, LPCR_VRMASD);\r\nhpte_setup = 1;\r\n}\r\n++i;\r\nhptp += 2;\r\n}\r\nfor (j = 0; j < hdr.n_invalid; ++j) {\r\nif (be64_to_cpu(hptp[0]) & (HPTE_V_VALID | HPTE_V_ABSENT))\r\nkvmppc_do_h_remove(kvm, 0, i, 0, tmp);\r\n++i;\r\nhptp += 2;\r\n}\r\nerr = 0;\r\n}\r\nout:\r\nsmp_wmb();\r\nkvm->arch.hpte_setup_done = hpte_setup;\r\nmutex_unlock(&kvm->lock);\r\nif (err)\r\nreturn err;\r\nreturn nb;\r\n}\r\nstatic int kvm_htab_release(struct inode *inode, struct file *filp)\r\n{\r\nstruct kvm_htab_ctx *ctx = filp->private_data;\r\nfilp->private_data = NULL;\r\nif (!(ctx->flags & KVM_GET_HTAB_WRITE))\r\natomic_dec(&ctx->kvm->arch.hpte_mod_interest);\r\nkvm_put_kvm(ctx->kvm);\r\nkfree(ctx);\r\nreturn 0;\r\n}\r\nint kvm_vm_ioctl_get_htab_fd(struct kvm *kvm, struct kvm_get_htab_fd *ghf)\r\n{\r\nint ret;\r\nstruct kvm_htab_ctx *ctx;\r\nint rwflag;\r\nif (ghf->flags & ~(KVM_GET_HTAB_BOLTED_ONLY | KVM_GET_HTAB_WRITE))\r\nreturn -EINVAL;\r\nctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\r\nif (!ctx)\r\nreturn -ENOMEM;\r\nkvm_get_kvm(kvm);\r\nctx->kvm = kvm;\r\nctx->index = ghf->start_index;\r\nctx->flags = ghf->flags;\r\nctx->first_pass = 1;\r\nrwflag = (ghf->flags & KVM_GET_HTAB_WRITE) ? O_WRONLY : O_RDONLY;\r\nret = anon_inode_getfd("kvm-htab", &kvm_htab_fops, ctx, rwflag | O_CLOEXEC);\r\nif (ret < 0) {\r\nkvm_put_kvm(kvm);\r\nreturn ret;\r\n}\r\nif (rwflag == O_RDONLY) {\r\nmutex_lock(&kvm->slots_lock);\r\natomic_inc(&kvm->arch.hpte_mod_interest);\r\nsynchronize_srcu_expedited(&kvm->srcu);\r\nmutex_unlock(&kvm->slots_lock);\r\n}\r\nreturn ret;\r\n}\r\nstatic int debugfs_htab_open(struct inode *inode, struct file *file)\r\n{\r\nstruct kvm *kvm = inode->i_private;\r\nstruct debugfs_htab_state *p;\r\np = kzalloc(sizeof(*p), GFP_KERNEL);\r\nif (!p)\r\nreturn -ENOMEM;\r\nkvm_get_kvm(kvm);\r\np->kvm = kvm;\r\nmutex_init(&p->mutex);\r\nfile->private_data = p;\r\nreturn nonseekable_open(inode, file);\r\n}\r\nstatic int debugfs_htab_release(struct inode *inode, struct file *file)\r\n{\r\nstruct debugfs_htab_state *p = file->private_data;\r\nkvm_put_kvm(p->kvm);\r\nkfree(p);\r\nreturn 0;\r\n}\r\nstatic ssize_t debugfs_htab_read(struct file *file, char __user *buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nstruct debugfs_htab_state *p = file->private_data;\r\nssize_t ret, r;\r\nunsigned long i, n;\r\nunsigned long v, hr, gr;\r\nstruct kvm *kvm;\r\n__be64 *hptp;\r\nret = mutex_lock_interruptible(&p->mutex);\r\nif (ret)\r\nreturn ret;\r\nif (p->chars_left) {\r\nn = p->chars_left;\r\nif (n > len)\r\nn = len;\r\nr = copy_to_user(buf, p->buf + p->buf_index, n);\r\nn -= r;\r\np->chars_left -= n;\r\np->buf_index += n;\r\nbuf += n;\r\nlen -= n;\r\nret = n;\r\nif (r) {\r\nif (!n)\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\n}\r\nkvm = p->kvm;\r\ni = p->hpt_index;\r\nhptp = (__be64 *)(kvm->arch.hpt.virt + (i * HPTE_SIZE));\r\nfor (; len != 0 && i < kvmppc_hpt_npte(&kvm->arch.hpt);\r\n++i, hptp += 2) {\r\nif (!(be64_to_cpu(hptp[0]) & (HPTE_V_VALID | HPTE_V_ABSENT)))\r\ncontinue;\r\npreempt_disable();\r\nwhile (!try_lock_hpte(hptp, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = be64_to_cpu(hptp[0]) & ~HPTE_V_HVLOCK;\r\nhr = be64_to_cpu(hptp[1]);\r\ngr = kvm->arch.hpt.rev[i].guest_rpte;\r\nunlock_hpte(hptp, v);\r\npreempt_enable();\r\nif (!(v & (HPTE_V_VALID | HPTE_V_ABSENT)))\r\ncontinue;\r\nn = scnprintf(p->buf, sizeof(p->buf),\r\n"%6lx %.16lx %.16lx %.16lx\n",\r\ni, v, hr, gr);\r\np->chars_left = n;\r\nif (n > len)\r\nn = len;\r\nr = copy_to_user(buf, p->buf, n);\r\nn -= r;\r\np->chars_left -= n;\r\np->buf_index = n;\r\nbuf += n;\r\nlen -= n;\r\nret += n;\r\nif (r) {\r\nif (!ret)\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\n}\r\np->hpt_index = i;\r\nout:\r\nmutex_unlock(&p->mutex);\r\nreturn ret;\r\n}\r\nstatic ssize_t debugfs_htab_write(struct file *file, const char __user *buf,\r\nsize_t len, loff_t *ppos)\r\n{\r\nreturn -EACCES;\r\n}\r\nvoid kvmppc_mmu_debugfs_init(struct kvm *kvm)\r\n{\r\nkvm->arch.htab_dentry = debugfs_create_file("htab", 0400,\r\nkvm->arch.debugfs_dir, kvm,\r\n&debugfs_htab_fops);\r\n}\r\nvoid kvmppc_mmu_book3s_hv_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvmppc_mmu *mmu = &vcpu->arch.mmu;\r\nvcpu->arch.slb_nr = 32;\r\nif (kvm_is_radix(vcpu->kvm))\r\nmmu->xlate = kvmppc_mmu_radix_xlate;\r\nelse\r\nmmu->xlate = kvmppc_mmu_book3s_64_hv_xlate;\r\nmmu->reset_msr = kvmppc_mmu_book3s_64_hv_reset_msr;\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_SLB;\r\n}
