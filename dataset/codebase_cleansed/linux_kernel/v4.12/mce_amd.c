const char *smca_get_name(enum smca_bank_types t)\r\n{\r\nif (t >= N_SMCA_BANK_TYPES)\r\nreturn NULL;\r\nreturn smca_names[t].name;\r\n}\r\nconst char *smca_get_long_name(enum smca_bank_types t)\r\n{\r\nif (t >= N_SMCA_BANK_TYPES)\r\nreturn NULL;\r\nreturn smca_names[t].long_name;\r\n}\r\nstatic void default_deferred_error_interrupt(void)\r\n{\r\npr_err("Unexpected deferred interrupt at vector %x\n", DEFERRED_ERROR_VECTOR);\r\n}\r\nstatic void get_smca_bank_info(unsigned int bank)\r\n{\r\nunsigned int i, hwid_mcatype, cpu = smp_processor_id();\r\nstruct smca_hwid *s_hwid;\r\nu32 high, instance_id;\r\nif (cpu)\r\nreturn;\r\nif (rdmsr_safe_on_cpu(cpu, MSR_AMD64_SMCA_MCx_IPID(bank), &instance_id, &high)) {\r\npr_warn("Failed to read MCA_IPID for bank %d\n", bank);\r\nreturn;\r\n}\r\nhwid_mcatype = HWID_MCATYPE(high & MCI_IPID_HWID,\r\n(high & MCI_IPID_MCATYPE) >> 16);\r\nfor (i = 0; i < ARRAY_SIZE(smca_hwid_mcatypes); i++) {\r\ns_hwid = &smca_hwid_mcatypes[i];\r\nif (hwid_mcatype == s_hwid->hwid_mcatype) {\r\nWARN(smca_banks[bank].hwid,\r\n"Bank %s already initialized!\n",\r\nsmca_get_name(s_hwid->bank_type));\r\nsmca_banks[bank].hwid = s_hwid;\r\nsmca_banks[bank].id = instance_id;\r\nsmca_banks[bank].sysfs_id = s_hwid->count++;\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic inline bool is_shared_bank(int bank)\r\n{\r\nif (mce_flags.smca)\r\nreturn false;\r\nreturn (bank == 4);\r\n}\r\nstatic const char *bank4_names(const struct threshold_block *b)\r\n{\r\nswitch (b->address) {\r\ncase 0x00000413:\r\nreturn "dram";\r\ncase 0xc0000408:\r\nreturn "ht_links";\r\ncase 0xc0000409:\r\nreturn "l3_cache";\r\ndefault:\r\nWARN(1, "Funny MSR: 0x%08x\n", b->address);\r\nreturn "";\r\n}\r\n}\r\nstatic bool lvt_interrupt_supported(unsigned int bank, u32 msr_high_bits)\r\n{\r\nif (bank == 4)\r\nreturn true;\r\nreturn msr_high_bits & BIT(28);\r\n}\r\nstatic int lvt_off_valid(struct threshold_block *b, int apic, u32 lo, u32 hi)\r\n{\r\nint msr = (hi & MASK_LVTOFF_HI) >> 20;\r\nif (apic < 0) {\r\npr_err(FW_BUG "cpu %d, failed to setup threshold interrupt "\r\n"for bank %d, block %d (MSR%08X=0x%x%08x)\n", b->cpu,\r\nb->bank, b->block, b->address, hi, lo);\r\nreturn 0;\r\n}\r\nif (apic != msr) {\r\nif (mce_flags.smca)\r\nreturn 0;\r\npr_err(FW_BUG "cpu %d, invalid threshold interrupt offset %d "\r\n"for bank %d, block %d (MSR%08X=0x%x%08x)\n",\r\nb->cpu, apic, b->bank, b->block, b->address, hi, lo);\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic void threshold_restart_bank(void *_tr)\r\n{\r\nstruct thresh_restart *tr = _tr;\r\nu32 hi, lo;\r\nrdmsr(tr->b->address, lo, hi);\r\nif (tr->b->threshold_limit < (hi & THRESHOLD_MAX))\r\ntr->reset = 1;\r\nif (tr->reset) {\r\nhi =\r\n(hi & ~(MASK_ERR_COUNT_HI | MASK_OVERFLOW_HI)) |\r\n(THRESHOLD_MAX - tr->b->threshold_limit);\r\n} else if (tr->old_limit) {\r\nint new_count = (hi & THRESHOLD_MAX) +\r\n(tr->old_limit - tr->b->threshold_limit);\r\nhi = (hi & ~MASK_ERR_COUNT_HI) |\r\n(new_count & THRESHOLD_MAX);\r\n}\r\nhi &= ~MASK_INT_TYPE_HI;\r\nif (!tr->b->interrupt_capable)\r\ngoto done;\r\nif (tr->set_lvt_off) {\r\nif (lvt_off_valid(tr->b, tr->lvt_off, lo, hi)) {\r\nhi &= ~MASK_LVTOFF_HI;\r\nhi |= tr->lvt_off << 20;\r\n}\r\n}\r\nif (tr->b->interrupt_enable)\r\nhi |= INT_TYPE_APIC;\r\ndone:\r\nhi |= MASK_COUNT_EN_HI;\r\nwrmsr(tr->b->address, lo, hi);\r\n}\r\nstatic void mce_threshold_block_init(struct threshold_block *b, int offset)\r\n{\r\nstruct thresh_restart tr = {\r\n.b = b,\r\n.set_lvt_off = 1,\r\n.lvt_off = offset,\r\n};\r\nb->threshold_limit = THRESHOLD_MAX;\r\nthreshold_restart_bank(&tr);\r\n}\r\nstatic int setup_APIC_mce_threshold(int reserved, int new)\r\n{\r\nif (reserved < 0 && !setup_APIC_eilvt(new, THRESHOLD_APIC_VECTOR,\r\nAPIC_EILVT_MSG_FIX, 0))\r\nreturn new;\r\nreturn reserved;\r\n}\r\nstatic int setup_APIC_deferred_error(int reserved, int new)\r\n{\r\nif (reserved < 0 && !setup_APIC_eilvt(new, DEFERRED_ERROR_VECTOR,\r\nAPIC_EILVT_MSG_FIX, 0))\r\nreturn new;\r\nreturn reserved;\r\n}\r\nstatic void deferred_error_interrupt_enable(struct cpuinfo_x86 *c)\r\n{\r\nu32 low = 0, high = 0;\r\nint def_offset = -1, def_new;\r\nif (rdmsr_safe(MSR_CU_DEF_ERR, &low, &high))\r\nreturn;\r\ndef_new = (low & MASK_DEF_LVTOFF) >> 4;\r\nif (!(low & MASK_DEF_LVTOFF)) {\r\npr_err(FW_BUG "Your BIOS is not setting up LVT offset 0x2 for deferred error IRQs correctly.\n");\r\ndef_new = DEF_LVT_OFF;\r\nlow = (low & ~MASK_DEF_LVTOFF) | (DEF_LVT_OFF << 4);\r\n}\r\ndef_offset = setup_APIC_deferred_error(def_offset, def_new);\r\nif ((def_offset == def_new) &&\r\n(deferred_error_int_vector != amd_deferred_error_interrupt))\r\ndeferred_error_int_vector = amd_deferred_error_interrupt;\r\nlow = (low & ~MASK_DEF_INT_TYPE) | DEF_INT_TYPE_APIC;\r\nwrmsr(MSR_CU_DEF_ERR, low, high);\r\n}\r\nstatic u32 get_block_address(unsigned int cpu, u32 current_addr, u32 low, u32 high,\r\nunsigned int bank, unsigned int block)\r\n{\r\nu32 addr = 0, offset = 0;\r\nif (mce_flags.smca) {\r\nif (!block) {\r\naddr = MSR_AMD64_SMCA_MCx_MISC(bank);\r\n} else {\r\nu32 low, high;\r\nif (rdmsr_safe_on_cpu(cpu, MSR_AMD64_SMCA_MCx_CONFIG(bank), &low, &high))\r\nreturn addr;\r\nif (!(low & MCI_CONFIG_MCAX))\r\nreturn addr;\r\nif (!rdmsr_safe_on_cpu(cpu, MSR_AMD64_SMCA_MCx_MISC(bank), &low, &high) &&\r\n(low & MASK_BLKPTR_LO))\r\naddr = MSR_AMD64_SMCA_MCx_MISCy(bank, block - 1);\r\n}\r\nreturn addr;\r\n}\r\nswitch (block) {\r\ncase 0:\r\naddr = msr_ops.misc(bank);\r\nbreak;\r\ncase 1:\r\noffset = ((low & MASK_BLKPTR_LO) >> 21);\r\nif (offset)\r\naddr = MCG_XBLK_ADDR + offset;\r\nbreak;\r\ndefault:\r\naddr = ++current_addr;\r\n}\r\nreturn addr;\r\n}\r\nstatic int\r\nprepare_threshold_block(unsigned int bank, unsigned int block, u32 addr,\r\nint offset, u32 misc_high)\r\n{\r\nunsigned int cpu = smp_processor_id();\r\nu32 smca_low, smca_high, smca_addr;\r\nstruct threshold_block b;\r\nint new;\r\nif (!block)\r\nper_cpu(bank_map, cpu) |= (1 << bank);\r\nmemset(&b, 0, sizeof(b));\r\nb.cpu = cpu;\r\nb.bank = bank;\r\nb.block = block;\r\nb.address = addr;\r\nb.interrupt_capable = lvt_interrupt_supported(bank, misc_high);\r\nif (!b.interrupt_capable)\r\ngoto done;\r\nb.interrupt_enable = 1;\r\nif (!mce_flags.smca) {\r\nnew = (misc_high & MASK_LVTOFF_HI) >> 20;\r\ngoto set_offset;\r\n}\r\nsmca_addr = MSR_AMD64_SMCA_MCx_CONFIG(bank);\r\nif (!rdmsr_safe(smca_addr, &smca_low, &smca_high)) {\r\nsmca_high |= BIT(0);\r\nsmca_high &= ~BIT(2);\r\nif ((smca_low & BIT(5)) && !((smca_high >> 5) & 0x3))\r\nsmca_high |= BIT(5);\r\nwrmsr(smca_addr, smca_low, smca_high);\r\n}\r\nif (rdmsr_safe(MSR_CU_DEF_ERR, &smca_low, &smca_high))\r\ngoto out;\r\nnew = (smca_low & SMCA_THR_LVT_OFF) >> 12;\r\nset_offset:\r\noffset = setup_APIC_mce_threshold(offset, new);\r\nif ((offset == new) && (mce_threshold_vector != amd_threshold_interrupt))\r\nmce_threshold_vector = amd_threshold_interrupt;\r\ndone:\r\nmce_threshold_block_init(&b, offset);\r\nout:\r\nreturn offset;\r\n}\r\nvoid mce_amd_feature_init(struct cpuinfo_x86 *c)\r\n{\r\nu32 low = 0, high = 0, address = 0;\r\nunsigned int bank, block, cpu = smp_processor_id();\r\nint offset = -1;\r\nfor (bank = 0; bank < mca_cfg.banks; ++bank) {\r\nif (mce_flags.smca)\r\nget_smca_bank_info(bank);\r\nfor (block = 0; block < NR_BLOCKS; ++block) {\r\naddress = get_block_address(cpu, address, low, high, bank, block);\r\nif (!address)\r\nbreak;\r\nif (rdmsr_safe(address, &low, &high))\r\nbreak;\r\nif (!(high & MASK_VALID_HI))\r\ncontinue;\r\nif (!(high & MASK_CNTP_HI) ||\r\n(high & MASK_LOCKED_HI))\r\ncontinue;\r\noffset = prepare_threshold_block(bank, block, address, offset, high);\r\n}\r\n}\r\nif (mce_flags.succor)\r\ndeferred_error_interrupt_enable(c);\r\n}\r\nint umc_normaddr_to_sysaddr(u64 norm_addr, u16 nid, u8 umc, u64 *sys_addr)\r\n{\r\nu64 dram_base_addr, dram_limit_addr, dram_hole_base;\r\nu64 ret_addr = norm_addr;\r\nu32 tmp;\r\nu8 die_id_shift, die_id_mask, socket_id_shift, socket_id_mask;\r\nu8 intlv_num_dies, intlv_num_chan, intlv_num_sockets;\r\nu8 intlv_addr_sel, intlv_addr_bit;\r\nu8 num_intlv_bits, hashed_bit;\r\nu8 lgcy_mmio_hole_en, base = 0;\r\nu8 cs_mask, cs_id = 0;\r\nbool hash_enabled = false;\r\nif (amd_df_indirect_read(nid, 0, 0x1B4, umc, &tmp))\r\ngoto out_err;\r\nif (tmp & BIT(0)) {\r\nu64 hi_addr_offset = (tmp & GENMASK_ULL(31, 20)) << 8;\r\nif (norm_addr >= hi_addr_offset) {\r\nret_addr -= hi_addr_offset;\r\nbase = 1;\r\n}\r\n}\r\nif (amd_df_indirect_read(nid, 0, 0x110 + (8 * base), umc, &tmp))\r\ngoto out_err;\r\nif (!(tmp & BIT(0))) {\r\npr_err("%s: Invalid DramBaseAddress range: 0x%x.\n",\r\n__func__, tmp);\r\ngoto out_err;\r\n}\r\nlgcy_mmio_hole_en = tmp & BIT(1);\r\nintlv_num_chan = (tmp >> 4) & 0xF;\r\nintlv_addr_sel = (tmp >> 8) & 0x7;\r\ndram_base_addr = (tmp & GENMASK_ULL(31, 12)) << 16;\r\nif (intlv_addr_sel > 3) {\r\npr_err("%s: Invalid interleave address select %d.\n",\r\n__func__, intlv_addr_sel);\r\ngoto out_err;\r\n}\r\nif (amd_df_indirect_read(nid, 0, 0x114 + (8 * base), umc, &tmp))\r\ngoto out_err;\r\nintlv_num_sockets = (tmp >> 8) & 0x1;\r\nintlv_num_dies = (tmp >> 10) & 0x3;\r\ndram_limit_addr = ((tmp & GENMASK_ULL(31, 12)) << 16) | GENMASK_ULL(27, 0);\r\nintlv_addr_bit = intlv_addr_sel + 8;\r\nswitch (intlv_num_chan) {\r\ncase 0: intlv_num_chan = 0; break;\r\ncase 1: intlv_num_chan = 1; break;\r\ncase 3: intlv_num_chan = 2; break;\r\ncase 5: intlv_num_chan = 3; break;\r\ncase 7: intlv_num_chan = 4; break;\r\ncase 8: intlv_num_chan = 1;\r\nhash_enabled = true;\r\nbreak;\r\ndefault:\r\npr_err("%s: Invalid number of interleaved channels %d.\n",\r\n__func__, intlv_num_chan);\r\ngoto out_err;\r\n}\r\nnum_intlv_bits = intlv_num_chan;\r\nif (intlv_num_dies > 2) {\r\npr_err("%s: Invalid number of interleaved nodes/dies %d.\n",\r\n__func__, intlv_num_dies);\r\ngoto out_err;\r\n}\r\nnum_intlv_bits += intlv_num_dies;\r\nnum_intlv_bits += intlv_num_sockets;\r\nif (num_intlv_bits > 4) {\r\npr_err("%s: Invalid interleave bits %d.\n",\r\n__func__, num_intlv_bits);\r\ngoto out_err;\r\n}\r\nif (num_intlv_bits > 0) {\r\nu64 temp_addr_x, temp_addr_i, temp_addr_y;\r\nu8 die_id_bit, sock_id_bit, cs_fabric_id;\r\nif (amd_df_indirect_read(nid, 0, 0x50, umc, &tmp))\r\ngoto out_err;\r\ncs_fabric_id = (tmp >> 8) & 0xFF;\r\ndie_id_bit = 0;\r\nif (intlv_num_chan) {\r\ndie_id_bit = intlv_num_chan;\r\ncs_mask = (1 << die_id_bit) - 1;\r\ncs_id = cs_fabric_id & cs_mask;\r\n}\r\nsock_id_bit = die_id_bit;\r\nif (intlv_num_dies || intlv_num_sockets)\r\nif (amd_df_indirect_read(nid, 1, 0x208, umc, &tmp))\r\ngoto out_err;\r\nif (intlv_num_dies) {\r\nsock_id_bit = die_id_bit + intlv_num_dies;\r\ndie_id_shift = (tmp >> 24) & 0xF;\r\ndie_id_mask = (tmp >> 8) & 0xFF;\r\ncs_id |= ((cs_fabric_id & die_id_mask) >> die_id_shift) << die_id_bit;\r\n}\r\nif (intlv_num_sockets) {\r\nsocket_id_shift = (tmp >> 28) & 0xF;\r\nsocket_id_mask = (tmp >> 16) & 0xFF;\r\ncs_id |= ((cs_fabric_id & socket_id_mask) >> socket_id_shift) << sock_id_bit;\r\n}\r\ntemp_addr_y = ret_addr & GENMASK_ULL(intlv_addr_bit-1, 0);\r\ntemp_addr_i = (cs_id << intlv_addr_bit);\r\ntemp_addr_x = (ret_addr & GENMASK_ULL(63, intlv_addr_bit)) << num_intlv_bits;\r\nret_addr = temp_addr_x | temp_addr_i | temp_addr_y;\r\n}\r\nret_addr += dram_base_addr;\r\nif (lgcy_mmio_hole_en) {\r\nif (amd_df_indirect_read(nid, 0, 0x104, umc, &tmp))\r\ngoto out_err;\r\ndram_hole_base = tmp & GENMASK(31, 24);\r\nif (ret_addr >= dram_hole_base)\r\nret_addr += (BIT_ULL(32) - dram_hole_base);\r\n}\r\nif (hash_enabled) {\r\nhashed_bit = (ret_addr >> 12) ^\r\n(ret_addr >> 18) ^\r\n(ret_addr >> 21) ^\r\n(ret_addr >> 30) ^\r\ncs_id;\r\nhashed_bit &= BIT(0);\r\nif (hashed_bit != ((ret_addr >> intlv_addr_bit) & BIT(0)))\r\nret_addr ^= BIT(intlv_addr_bit);\r\n}\r\nif (ret_addr > dram_limit_addr)\r\ngoto out_err;\r\n*sys_addr = ret_addr;\r\nreturn 0;\r\nout_err:\r\nreturn -EINVAL;\r\n}\r\nstatic void\r\n__log_error(unsigned int bank, bool deferred_err, bool threshold_err, u64 misc)\r\n{\r\nu32 msr_status = msr_ops.status(bank);\r\nu32 msr_addr = msr_ops.addr(bank);\r\nstruct mce m;\r\nu64 status;\r\nWARN_ON_ONCE(deferred_err && threshold_err);\r\nif (deferred_err && mce_flags.smca) {\r\nmsr_status = MSR_AMD64_SMCA_MCx_DESTAT(bank);\r\nmsr_addr = MSR_AMD64_SMCA_MCx_DEADDR(bank);\r\n}\r\nrdmsrl(msr_status, status);\r\nif (!(status & MCI_STATUS_VAL))\r\nreturn;\r\nmce_setup(&m);\r\nm.status = status;\r\nm.bank = bank;\r\nm.tsc = rdtsc();\r\nif (threshold_err)\r\nm.misc = misc;\r\nif (m.status & MCI_STATUS_ADDRV) {\r\nrdmsrl(msr_addr, m.addr);\r\nif (mce_flags.smca) {\r\nu8 lsb = (m.addr >> 56) & 0x3f;\r\nm.addr &= GENMASK_ULL(55, lsb);\r\n}\r\n}\r\nif (mce_flags.smca) {\r\nrdmsrl(MSR_AMD64_SMCA_MCx_IPID(bank), m.ipid);\r\nif (m.status & MCI_STATUS_SYNDV)\r\nrdmsrl(MSR_AMD64_SMCA_MCx_SYND(bank), m.synd);\r\n}\r\nmce_log(&m);\r\nwrmsrl(msr_status, 0);\r\n}\r\nstatic inline void __smp_deferred_error_interrupt(void)\r\n{\r\ninc_irq_stat(irq_deferred_error_count);\r\ndeferred_error_int_vector();\r\n}\r\nasmlinkage __visible void __irq_entry smp_deferred_error_interrupt(void)\r\n{\r\nentering_irq();\r\n__smp_deferred_error_interrupt();\r\nexiting_ack_irq();\r\n}\r\nasmlinkage __visible void __irq_entry smp_trace_deferred_error_interrupt(void)\r\n{\r\nentering_irq();\r\ntrace_deferred_error_apic_entry(DEFERRED_ERROR_VECTOR);\r\n__smp_deferred_error_interrupt();\r\ntrace_deferred_error_apic_exit(DEFERRED_ERROR_VECTOR);\r\nexiting_ack_irq();\r\n}\r\nstatic void amd_deferred_error_interrupt(void)\r\n{\r\nunsigned int bank;\r\nu32 msr_status;\r\nu64 status;\r\nfor (bank = 0; bank < mca_cfg.banks; ++bank) {\r\nmsr_status = (mce_flags.smca) ? MSR_AMD64_SMCA_MCx_DESTAT(bank)\r\n: msr_ops.status(bank);\r\nrdmsrl(msr_status, status);\r\nif (!(status & MCI_STATUS_VAL) ||\r\n!(status & MCI_STATUS_DEFERRED))\r\ncontinue;\r\n__log_error(bank, true, false, 0);\r\nbreak;\r\n}\r\n}\r\nstatic void amd_threshold_interrupt(void)\r\n{\r\nu32 low = 0, high = 0, address = 0;\r\nunsigned int bank, block, cpu = smp_processor_id();\r\nstruct thresh_restart tr;\r\nfor (bank = 0; bank < mca_cfg.banks; ++bank) {\r\nif (!(per_cpu(bank_map, cpu) & (1 << bank)))\r\ncontinue;\r\nfor (block = 0; block < NR_BLOCKS; ++block) {\r\naddress = get_block_address(cpu, address, low, high, bank, block);\r\nif (!address)\r\nbreak;\r\nif (rdmsr_safe(address, &low, &high))\r\nbreak;\r\nif (!(high & MASK_VALID_HI)) {\r\nif (block)\r\ncontinue;\r\nelse\r\nbreak;\r\n}\r\nif (!(high & MASK_CNTP_HI) ||\r\n(high & MASK_LOCKED_HI))\r\ncontinue;\r\nif (high & MASK_OVERFLOW_HI)\r\ngoto log;\r\n}\r\n}\r\nreturn;\r\nlog:\r\n__log_error(bank, false, true, ((u64)high << 32) | low);\r\nmemset(&tr, 0, sizeof(tr));\r\ntr.b = &per_cpu(threshold_banks, cpu)[bank]->blocks[block];\r\nthreshold_restart_bank(&tr);\r\n}\r\nstatic ssize_t\r\nstore_interrupt_enable(struct threshold_block *b, const char *buf, size_t size)\r\n{\r\nstruct thresh_restart tr;\r\nunsigned long new;\r\nif (!b->interrupt_capable)\r\nreturn -EINVAL;\r\nif (kstrtoul(buf, 0, &new) < 0)\r\nreturn -EINVAL;\r\nb->interrupt_enable = !!new;\r\nmemset(&tr, 0, sizeof(tr));\r\ntr.b = b;\r\nsmp_call_function_single(b->cpu, threshold_restart_bank, &tr, 1);\r\nreturn size;\r\n}\r\nstatic ssize_t\r\nstore_threshold_limit(struct threshold_block *b, const char *buf, size_t size)\r\n{\r\nstruct thresh_restart tr;\r\nunsigned long new;\r\nif (kstrtoul(buf, 0, &new) < 0)\r\nreturn -EINVAL;\r\nif (new > THRESHOLD_MAX)\r\nnew = THRESHOLD_MAX;\r\nif (new < 1)\r\nnew = 1;\r\nmemset(&tr, 0, sizeof(tr));\r\ntr.old_limit = b->threshold_limit;\r\nb->threshold_limit = new;\r\ntr.b = b;\r\nsmp_call_function_single(b->cpu, threshold_restart_bank, &tr, 1);\r\nreturn size;\r\n}\r\nstatic ssize_t show_error_count(struct threshold_block *b, char *buf)\r\n{\r\nu32 lo, hi;\r\nrdmsr_on_cpu(b->cpu, b->address, &lo, &hi);\r\nreturn sprintf(buf, "%u\n", ((hi & THRESHOLD_MAX) -\r\n(THRESHOLD_MAX - b->threshold_limit)));\r\n}\r\nstatic ssize_t show(struct kobject *kobj, struct attribute *attr, char *buf)\r\n{\r\nstruct threshold_block *b = to_block(kobj);\r\nstruct threshold_attr *a = to_attr(attr);\r\nssize_t ret;\r\nret = a->show ? a->show(b, buf) : -EIO;\r\nreturn ret;\r\n}\r\nstatic ssize_t store(struct kobject *kobj, struct attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct threshold_block *b = to_block(kobj);\r\nstruct threshold_attr *a = to_attr(attr);\r\nssize_t ret;\r\nret = a->store ? a->store(b, buf, count) : -EIO;\r\nreturn ret;\r\n}\r\nstatic const char *get_name(unsigned int bank, struct threshold_block *b)\r\n{\r\nunsigned int bank_type;\r\nif (!mce_flags.smca) {\r\nif (b && bank == 4)\r\nreturn bank4_names(b);\r\nreturn th_names[bank];\r\n}\r\nif (!smca_banks[bank].hwid)\r\nreturn NULL;\r\nbank_type = smca_banks[bank].hwid->bank_type;\r\nif (b && bank_type == SMCA_UMC) {\r\nif (b->block < ARRAY_SIZE(smca_umc_block_names))\r\nreturn smca_umc_block_names[b->block];\r\nreturn NULL;\r\n}\r\nif (smca_banks[bank].hwid->count == 1)\r\nreturn smca_get_name(bank_type);\r\nsnprintf(buf_mcatype, MAX_MCATYPE_NAME_LEN,\r\n"%s_%x", smca_get_name(bank_type),\r\nsmca_banks[bank].sysfs_id);\r\nreturn buf_mcatype;\r\n}\r\nstatic int allocate_threshold_blocks(unsigned int cpu, unsigned int bank,\r\nunsigned int block, u32 address)\r\n{\r\nstruct threshold_block *b = NULL;\r\nu32 low, high;\r\nint err;\r\nif ((bank >= mca_cfg.banks) || (block >= NR_BLOCKS))\r\nreturn 0;\r\nif (rdmsr_safe_on_cpu(cpu, address, &low, &high))\r\nreturn 0;\r\nif (!(high & MASK_VALID_HI)) {\r\nif (block)\r\ngoto recurse;\r\nelse\r\nreturn 0;\r\n}\r\nif (!(high & MASK_CNTP_HI) ||\r\n(high & MASK_LOCKED_HI))\r\ngoto recurse;\r\nb = kzalloc(sizeof(struct threshold_block), GFP_KERNEL);\r\nif (!b)\r\nreturn -ENOMEM;\r\nb->block = block;\r\nb->bank = bank;\r\nb->cpu = cpu;\r\nb->address = address;\r\nb->interrupt_enable = 0;\r\nb->interrupt_capable = lvt_interrupt_supported(bank, high);\r\nb->threshold_limit = THRESHOLD_MAX;\r\nif (b->interrupt_capable) {\r\nthreshold_ktype.default_attrs[2] = &interrupt_enable.attr;\r\nb->interrupt_enable = 1;\r\n} else {\r\nthreshold_ktype.default_attrs[2] = NULL;\r\n}\r\nINIT_LIST_HEAD(&b->miscj);\r\nif (per_cpu(threshold_banks, cpu)[bank]->blocks) {\r\nlist_add(&b->miscj,\r\n&per_cpu(threshold_banks, cpu)[bank]->blocks->miscj);\r\n} else {\r\nper_cpu(threshold_banks, cpu)[bank]->blocks = b;\r\n}\r\nerr = kobject_init_and_add(&b->kobj, &threshold_ktype,\r\nper_cpu(threshold_banks, cpu)[bank]->kobj,\r\nget_name(bank, b));\r\nif (err)\r\ngoto out_free;\r\nrecurse:\r\naddress = get_block_address(cpu, address, low, high, bank, ++block);\r\nif (!address)\r\nreturn 0;\r\nerr = allocate_threshold_blocks(cpu, bank, block, address);\r\nif (err)\r\ngoto out_free;\r\nif (b)\r\nkobject_uevent(&b->kobj, KOBJ_ADD);\r\nreturn err;\r\nout_free:\r\nif (b) {\r\nkobject_put(&b->kobj);\r\nlist_del(&b->miscj);\r\nkfree(b);\r\n}\r\nreturn err;\r\n}\r\nstatic int __threshold_add_blocks(struct threshold_bank *b)\r\n{\r\nstruct list_head *head = &b->blocks->miscj;\r\nstruct threshold_block *pos = NULL;\r\nstruct threshold_block *tmp = NULL;\r\nint err = 0;\r\nerr = kobject_add(&b->blocks->kobj, b->kobj, b->blocks->kobj.name);\r\nif (err)\r\nreturn err;\r\nlist_for_each_entry_safe(pos, tmp, head, miscj) {\r\nerr = kobject_add(&pos->kobj, b->kobj, pos->kobj.name);\r\nif (err) {\r\nlist_for_each_entry_safe_reverse(pos, tmp, head, miscj)\r\nkobject_del(&pos->kobj);\r\nreturn err;\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic int threshold_create_bank(unsigned int cpu, unsigned int bank)\r\n{\r\nstruct device *dev = per_cpu(mce_device, cpu);\r\nstruct amd_northbridge *nb = NULL;\r\nstruct threshold_bank *b = NULL;\r\nconst char *name = get_name(bank, NULL);\r\nint err = 0;\r\nif (!dev)\r\nreturn -ENODEV;\r\nif (is_shared_bank(bank)) {\r\nnb = node_to_amd_nb(amd_get_nb_id(cpu));\r\nif (nb && nb->bank4) {\r\nb = nb->bank4;\r\nerr = kobject_add(b->kobj, &dev->kobj, name);\r\nif (err)\r\ngoto out;\r\nper_cpu(threshold_banks, cpu)[bank] = b;\r\natomic_inc(&b->cpus);\r\nerr = __threshold_add_blocks(b);\r\ngoto out;\r\n}\r\n}\r\nb = kzalloc(sizeof(struct threshold_bank), GFP_KERNEL);\r\nif (!b) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nb->kobj = kobject_create_and_add(name, &dev->kobj);\r\nif (!b->kobj) {\r\nerr = -EINVAL;\r\ngoto out_free;\r\n}\r\nper_cpu(threshold_banks, cpu)[bank] = b;\r\nif (is_shared_bank(bank)) {\r\natomic_set(&b->cpus, 1);\r\nif (nb) {\r\nWARN_ON(nb->bank4);\r\nnb->bank4 = b;\r\n}\r\n}\r\nerr = allocate_threshold_blocks(cpu, bank, 0, msr_ops.misc(bank));\r\nif (!err)\r\ngoto out;\r\nout_free:\r\nkfree(b);\r\nout:\r\nreturn err;\r\n}\r\nstatic void deallocate_threshold_block(unsigned int cpu,\r\nunsigned int bank)\r\n{\r\nstruct threshold_block *pos = NULL;\r\nstruct threshold_block *tmp = NULL;\r\nstruct threshold_bank *head = per_cpu(threshold_banks, cpu)[bank];\r\nif (!head)\r\nreturn;\r\nlist_for_each_entry_safe(pos, tmp, &head->blocks->miscj, miscj) {\r\nkobject_put(&pos->kobj);\r\nlist_del(&pos->miscj);\r\nkfree(pos);\r\n}\r\nkfree(per_cpu(threshold_banks, cpu)[bank]->blocks);\r\nper_cpu(threshold_banks, cpu)[bank]->blocks = NULL;\r\n}\r\nstatic void __threshold_remove_blocks(struct threshold_bank *b)\r\n{\r\nstruct threshold_block *pos = NULL;\r\nstruct threshold_block *tmp = NULL;\r\nkobject_del(b->kobj);\r\nlist_for_each_entry_safe(pos, tmp, &b->blocks->miscj, miscj)\r\nkobject_del(&pos->kobj);\r\n}\r\nstatic void threshold_remove_bank(unsigned int cpu, int bank)\r\n{\r\nstruct amd_northbridge *nb;\r\nstruct threshold_bank *b;\r\nb = per_cpu(threshold_banks, cpu)[bank];\r\nif (!b)\r\nreturn;\r\nif (!b->blocks)\r\ngoto free_out;\r\nif (is_shared_bank(bank)) {\r\nif (!atomic_dec_and_test(&b->cpus)) {\r\n__threshold_remove_blocks(b);\r\nper_cpu(threshold_banks, cpu)[bank] = NULL;\r\nreturn;\r\n} else {\r\nnb = node_to_amd_nb(amd_get_nb_id(cpu));\r\nnb->bank4 = NULL;\r\n}\r\n}\r\ndeallocate_threshold_block(cpu, bank);\r\nfree_out:\r\nkobject_del(b->kobj);\r\nkobject_put(b->kobj);\r\nkfree(b);\r\nper_cpu(threshold_banks, cpu)[bank] = NULL;\r\n}\r\nint mce_threshold_remove_device(unsigned int cpu)\r\n{\r\nunsigned int bank;\r\nif (!thresholding_en)\r\nreturn 0;\r\nfor (bank = 0; bank < mca_cfg.banks; ++bank) {\r\nif (!(per_cpu(bank_map, cpu) & (1 << bank)))\r\ncontinue;\r\nthreshold_remove_bank(cpu, bank);\r\n}\r\nkfree(per_cpu(threshold_banks, cpu));\r\nper_cpu(threshold_banks, cpu) = NULL;\r\nreturn 0;\r\n}\r\nint mce_threshold_create_device(unsigned int cpu)\r\n{\r\nunsigned int bank;\r\nstruct threshold_bank **bp;\r\nint err = 0;\r\nif (!thresholding_en)\r\nreturn 0;\r\nbp = per_cpu(threshold_banks, cpu);\r\nif (bp)\r\nreturn 0;\r\nbp = kzalloc(sizeof(struct threshold_bank *) * mca_cfg.banks,\r\nGFP_KERNEL);\r\nif (!bp)\r\nreturn -ENOMEM;\r\nper_cpu(threshold_banks, cpu) = bp;\r\nfor (bank = 0; bank < mca_cfg.banks; ++bank) {\r\nif (!(per_cpu(bank_map, cpu) & (1 << bank)))\r\ncontinue;\r\nerr = threshold_create_bank(cpu, bank);\r\nif (err)\r\ngoto err;\r\n}\r\nreturn err;\r\nerr:\r\nmce_threshold_remove_device(cpu);\r\nreturn err;\r\n}\r\nstatic __init int threshold_init_device(void)\r\n{\r\nunsigned lcpu = 0;\r\nif (mce_threshold_vector == amd_threshold_interrupt)\r\nthresholding_en = true;\r\nfor_each_online_cpu(lcpu) {\r\nint err = mce_threshold_create_device(lcpu);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}
