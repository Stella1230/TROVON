static void update_fastmap_work_fn(struct work_struct *wrk)\r\n{\r\nstruct ubi_device *ubi = container_of(wrk, struct ubi_device, fm_work);\r\nubi_update_fastmap(ubi);\r\nspin_lock(&ubi->wl_lock);\r\nubi->fm_work_scheduled = 0;\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nstatic struct ubi_wl_entry *find_anchor_wl_entry(struct rb_root *root)\r\n{\r\nstruct rb_node *p;\r\nstruct ubi_wl_entry *e, *victim = NULL;\r\nint max_ec = UBI_MAX_ERASECOUNTER;\r\nubi_rb_for_each_entry(p, e, root, u.rb) {\r\nif (e->pnum < UBI_FM_MAX_START && e->ec < max_ec) {\r\nvictim = e;\r\nmax_ec = e->ec;\r\n}\r\n}\r\nreturn victim;\r\n}\r\nstatic void return_unused_pool_pebs(struct ubi_device *ubi,\r\nstruct ubi_fm_pool *pool)\r\n{\r\nint i;\r\nstruct ubi_wl_entry *e;\r\nfor (i = pool->used; i < pool->size; i++) {\r\ne = ubi->lookuptbl[pool->pebs[i]];\r\nwl_tree_add(e, &ubi->free);\r\nubi->free_count++;\r\n}\r\n}\r\nstatic int anchor_pebs_avalible(struct rb_root *root)\r\n{\r\nstruct rb_node *p;\r\nstruct ubi_wl_entry *e;\r\nubi_rb_for_each_entry(p, e, root, u.rb)\r\nif (e->pnum < UBI_FM_MAX_START)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstruct ubi_wl_entry *ubi_wl_get_fm_peb(struct ubi_device *ubi, int anchor)\r\n{\r\nstruct ubi_wl_entry *e = NULL;\r\nif (!ubi->free.rb_node || (ubi->free_count - ubi->beb_rsvd_pebs < 1))\r\ngoto out;\r\nif (anchor)\r\ne = find_anchor_wl_entry(&ubi->free);\r\nelse\r\ne = find_mean_wl_entry(ubi, &ubi->free);\r\nif (!e)\r\ngoto out;\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nrb_erase(&e->u.rb, &ubi->free);\r\nubi->free_count--;\r\nout:\r\nreturn e;\r\n}\r\nvoid ubi_refill_pools(struct ubi_device *ubi)\r\n{\r\nstruct ubi_fm_pool *wl_pool = &ubi->fm_wl_pool;\r\nstruct ubi_fm_pool *pool = &ubi->fm_pool;\r\nstruct ubi_wl_entry *e;\r\nint enough;\r\nspin_lock(&ubi->wl_lock);\r\nreturn_unused_pool_pebs(ubi, wl_pool);\r\nreturn_unused_pool_pebs(ubi, pool);\r\nwl_pool->size = 0;\r\npool->size = 0;\r\nfor (;;) {\r\nenough = 0;\r\nif (pool->size < pool->max_size) {\r\nif (!ubi->free.rb_node)\r\nbreak;\r\ne = wl_get_wle(ubi);\r\nif (!e)\r\nbreak;\r\npool->pebs[pool->size] = e->pnum;\r\npool->size++;\r\n} else\r\nenough++;\r\nif (wl_pool->size < wl_pool->max_size) {\r\nif (!ubi->free.rb_node ||\r\n(ubi->free_count - ubi->beb_rsvd_pebs < 5))\r\nbreak;\r\ne = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\r\nself_check_in_wl_tree(ubi, e, &ubi->free);\r\nrb_erase(&e->u.rb, &ubi->free);\r\nubi->free_count--;\r\nwl_pool->pebs[wl_pool->size] = e->pnum;\r\nwl_pool->size++;\r\n} else\r\nenough++;\r\nif (enough == 2)\r\nbreak;\r\n}\r\nwl_pool->used = 0;\r\npool->used = 0;\r\nspin_unlock(&ubi->wl_lock);\r\n}\r\nstatic int produce_free_peb(struct ubi_device *ubi)\r\n{\r\nint err;\r\nwhile (!ubi->free.rb_node && ubi->works_count) {\r\ndbg_wl("do one work synchronously");\r\nerr = do_work(ubi);\r\nif (err)\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nint ubi_wl_get_peb(struct ubi_device *ubi)\r\n{\r\nint ret, retried = 0;\r\nstruct ubi_fm_pool *pool = &ubi->fm_pool;\r\nstruct ubi_fm_pool *wl_pool = &ubi->fm_wl_pool;\r\nagain:\r\ndown_read(&ubi->fm_eba_sem);\r\nspin_lock(&ubi->wl_lock);\r\nif (pool->used == pool->size || wl_pool->used == wl_pool->size) {\r\nspin_unlock(&ubi->wl_lock);\r\nup_read(&ubi->fm_eba_sem);\r\nret = ubi_update_fastmap(ubi);\r\nif (ret) {\r\nubi_msg(ubi, "Unable to write a new fastmap: %i", ret);\r\ndown_read(&ubi->fm_eba_sem);\r\nreturn -ENOSPC;\r\n}\r\ndown_read(&ubi->fm_eba_sem);\r\nspin_lock(&ubi->wl_lock);\r\n}\r\nif (pool->used == pool->size) {\r\nspin_unlock(&ubi->wl_lock);\r\nif (retried) {\r\nubi_err(ubi, "Unable to get a free PEB from user WL pool");\r\nret = -ENOSPC;\r\ngoto out;\r\n}\r\nretried = 1;\r\nup_read(&ubi->fm_eba_sem);\r\nret = produce_free_peb(ubi);\r\nif (ret < 0) {\r\ndown_read(&ubi->fm_eba_sem);\r\ngoto out;\r\n}\r\ngoto again;\r\n}\r\nubi_assert(pool->used < pool->size);\r\nret = pool->pebs[pool->used++];\r\nprot_queue_add(ubi, ubi->lookuptbl[ret]);\r\nspin_unlock(&ubi->wl_lock);\r\nout:\r\nreturn ret;\r\n}\r\nstatic struct ubi_wl_entry *get_peb_for_wl(struct ubi_device *ubi)\r\n{\r\nstruct ubi_fm_pool *pool = &ubi->fm_wl_pool;\r\nint pnum;\r\nubi_assert(rwsem_is_locked(&ubi->fm_eba_sem));\r\nif (pool->used == pool->size) {\r\nif (!ubi->fm_work_scheduled) {\r\nubi->fm_work_scheduled = 1;\r\nschedule_work(&ubi->fm_work);\r\n}\r\nreturn NULL;\r\n}\r\npnum = pool->pebs[pool->used++];\r\nreturn ubi->lookuptbl[pnum];\r\n}\r\nint ubi_ensure_anchor_pebs(struct ubi_device *ubi)\r\n{\r\nstruct ubi_work *wrk;\r\nspin_lock(&ubi->wl_lock);\r\nif (ubi->wl_scheduled) {\r\nspin_unlock(&ubi->wl_lock);\r\nreturn 0;\r\n}\r\nubi->wl_scheduled = 1;\r\nspin_unlock(&ubi->wl_lock);\r\nwrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\r\nif (!wrk) {\r\nspin_lock(&ubi->wl_lock);\r\nubi->wl_scheduled = 0;\r\nspin_unlock(&ubi->wl_lock);\r\nreturn -ENOMEM;\r\n}\r\nwrk->anchor = 1;\r\nwrk->func = &wear_leveling_worker;\r\n__schedule_ubi_work(ubi, wrk);\r\nreturn 0;\r\n}\r\nint ubi_wl_put_fm_peb(struct ubi_device *ubi, struct ubi_wl_entry *fm_e,\r\nint lnum, int torture)\r\n{\r\nstruct ubi_wl_entry *e;\r\nint vol_id, pnum = fm_e->pnum;\r\ndbg_wl("PEB %d", pnum);\r\nubi_assert(pnum >= 0);\r\nubi_assert(pnum < ubi->peb_count);\r\nspin_lock(&ubi->wl_lock);\r\ne = ubi->lookuptbl[pnum];\r\nif (!e) {\r\ne = fm_e;\r\nubi_assert(e->ec >= 0);\r\nubi->lookuptbl[pnum] = e;\r\n}\r\nspin_unlock(&ubi->wl_lock);\r\nvol_id = lnum ? UBI_FM_DATA_VOLUME_ID : UBI_FM_SB_VOLUME_ID;\r\nreturn schedule_erase(ubi, e, vol_id, lnum, torture, true);\r\n}\r\nint ubi_is_erase_work(struct ubi_work *wrk)\r\n{\r\nreturn wrk->func == erase_worker;\r\n}\r\nstatic void ubi_fastmap_close(struct ubi_device *ubi)\r\n{\r\nint i;\r\nflush_work(&ubi->fm_work);\r\nreturn_unused_pool_pebs(ubi, &ubi->fm_pool);\r\nreturn_unused_pool_pebs(ubi, &ubi->fm_wl_pool);\r\nif (ubi->fm) {\r\nfor (i = 0; i < ubi->fm->used_blocks; i++)\r\nkfree(ubi->fm->e[i]);\r\n}\r\nkfree(ubi->fm);\r\n}\r\nstatic struct ubi_wl_entry *may_reserve_for_fm(struct ubi_device *ubi,\r\nstruct ubi_wl_entry *e,\r\nstruct rb_root *root) {\r\nif (e && !ubi->fm_disabled && !ubi->fm &&\r\ne->pnum < UBI_FM_MAX_START)\r\ne = rb_entry(rb_next(root->rb_node),\r\nstruct ubi_wl_entry, u.rb);\r\nreturn e;\r\n}
