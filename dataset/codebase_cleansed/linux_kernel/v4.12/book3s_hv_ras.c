static void reload_slb(struct kvm_vcpu *vcpu)\r\n{\r\nstruct slb_shadow *slb;\r\nunsigned long i, n;\r\nasm volatile("slbmte %0,%0; slbia" : : "r" (0));\r\nslb = vcpu->arch.slb_shadow.pinned_addr;\r\nif (!slb)\r\nreturn;\r\nn = min_t(u32, be32_to_cpu(slb->persistent), SLB_MIN_SIZE);\r\nif ((void *) &slb->save_area[n] > vcpu->arch.slb_shadow.pinned_end)\r\nreturn;\r\nfor (i = 0; i < n; ++i) {\r\nunsigned long rb = be64_to_cpu(slb->save_area[i].esid);\r\nunsigned long rs = be64_to_cpu(slb->save_area[i].vsid);\r\nrb = (rb & ~0xFFFul) | i;\r\nasm volatile("slbmte %0,%1" : : "r" (rs), "r" (rb));\r\n}\r\n}\r\nstatic long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long srr1 = vcpu->arch.shregs.msr;\r\nstruct machine_check_event mce_evt;\r\nlong handled = 1;\r\nif (srr1 & SRR1_MC_LDSTERR) {\r\nunsigned long dsisr = vcpu->arch.shregs.dsisr;\r\nif (dsisr & (DSISR_MC_SLB_PARMULTI | DSISR_MC_SLB_MULTI |\r\nDSISR_MC_SLB_PARITY | DSISR_MC_DERAT_MULTI)) {\r\nreload_slb(vcpu);\r\ndsisr &= ~(DSISR_MC_SLB_PARMULTI | DSISR_MC_SLB_MULTI |\r\nDSISR_MC_SLB_PARITY | DSISR_MC_DERAT_MULTI);\r\n}\r\nif (dsisr & DSISR_MC_TLB_MULTI) {\r\nif (cur_cpu_spec && cur_cpu_spec->flush_tlb)\r\ncur_cpu_spec->flush_tlb(TLB_INVAL_SCOPE_LPID);\r\ndsisr &= ~DSISR_MC_TLB_MULTI;\r\n}\r\nif (dsisr & 0xffffffffUL)\r\nhandled = 0;\r\n}\r\nswitch ((srr1 >> SRR1_MC_IFETCH_SH) & SRR1_MC_IFETCH_MASK) {\r\ncase 0:\r\nbreak;\r\ncase SRR1_MC_IFETCH_SLBPAR:\r\ncase SRR1_MC_IFETCH_SLBMULTI:\r\ncase SRR1_MC_IFETCH_SLBPARMULTI:\r\nreload_slb(vcpu);\r\nbreak;\r\ncase SRR1_MC_IFETCH_TLBMULTI:\r\nif (cur_cpu_spec && cur_cpu_spec->flush_tlb)\r\ncur_cpu_spec->flush_tlb(TLB_INVAL_SCOPE_LPID);\r\nbreak;\r\ndefault:\r\nhandled = 0;\r\n}\r\nif (!get_mce_event(&mce_evt, MCE_EVENT_DONTRELEASE))\r\ngoto out;\r\nif (mce_evt.version == MCE_V1 &&\r\n(mce_evt.severity == MCE_SEV_NO_ERROR ||\r\nmce_evt.disposition == MCE_DISPOSITION_RECOVERED))\r\nhandled = 1;\r\nout:\r\nmachine_check_queue_event();\r\nreturn handled;\r\n}\r\nlong kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu)\r\n{\r\nreturn kvmppc_realmode_mc_power7(vcpu);\r\n}\r\nstatic inline int kvmppc_cur_subcore_size(void)\r\n{\r\nif (local_paca->kvm_hstate.kvm_split_mode)\r\nreturn local_paca->kvm_hstate.kvm_split_mode->subcore_size;\r\nreturn threads_per_subcore;\r\n}\r\nvoid kvmppc_subcore_enter_guest(void)\r\n{\r\nint thread_id, subcore_id;\r\nthread_id = cpu_thread_in_core(local_paca->paca_index);\r\nsubcore_id = thread_id / kvmppc_cur_subcore_size();\r\nlocal_paca->sibling_subcore_state->in_guest[subcore_id] = 1;\r\n}\r\nvoid kvmppc_subcore_exit_guest(void)\r\n{\r\nint thread_id, subcore_id;\r\nthread_id = cpu_thread_in_core(local_paca->paca_index);\r\nsubcore_id = thread_id / kvmppc_cur_subcore_size();\r\nlocal_paca->sibling_subcore_state->in_guest[subcore_id] = 0;\r\n}\r\nstatic bool kvmppc_tb_resync_required(void)\r\n{\r\nif (test_and_set_bit(CORE_TB_RESYNC_REQ_BIT,\r\n&local_paca->sibling_subcore_state->flags))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic void kvmppc_tb_resync_done(void)\r\n{\r\nclear_bit(CORE_TB_RESYNC_REQ_BIT,\r\n&local_paca->sibling_subcore_state->flags);\r\n}\r\nlong kvmppc_realmode_hmi_handler(void)\r\n{\r\nint ptid = local_paca->kvm_hstate.ptid;\r\nbool resync_req;\r\nBUG_ON(ptid != 0);\r\n__this_cpu_inc(irq_stat.hmi_exceptions);\r\nresync_req = kvmppc_tb_resync_required();\r\nkvmppc_subcore_exit_guest();\r\nwait_for_subcore_guest_exit();\r\nif (ppc_md.hmi_exception_early)\r\nppc_md.hmi_exception_early(NULL);\r\nif (resync_req) {\r\nopal_resync_timebase();\r\nkvmppc_tb_resync_done();\r\n} else {\r\nwait_for_tb_resync();\r\n}\r\nreturn 0;\r\n}
