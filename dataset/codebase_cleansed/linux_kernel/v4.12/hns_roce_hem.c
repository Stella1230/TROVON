struct hns_roce_hem *hns_roce_alloc_hem(struct hns_roce_dev *hr_dev, int npages,\r\ngfp_t gfp_mask)\r\n{\r\nstruct hns_roce_hem_chunk *chunk = NULL;\r\nstruct hns_roce_hem *hem;\r\nstruct scatterlist *mem;\r\nint order;\r\nvoid *buf;\r\nWARN_ON(gfp_mask & __GFP_HIGHMEM);\r\nhem = kmalloc(sizeof(*hem),\r\ngfp_mask & ~(__GFP_HIGHMEM | __GFP_NOWARN));\r\nif (!hem)\r\nreturn NULL;\r\nhem->refcount = 0;\r\nINIT_LIST_HEAD(&hem->chunk_list);\r\norder = get_order(HNS_ROCE_HEM_ALLOC_SIZE);\r\nwhile (npages > 0) {\r\nif (!chunk) {\r\nchunk = kmalloc(sizeof(*chunk),\r\ngfp_mask & ~(__GFP_HIGHMEM | __GFP_NOWARN));\r\nif (!chunk)\r\ngoto fail;\r\nsg_init_table(chunk->mem, HNS_ROCE_HEM_CHUNK_LEN);\r\nchunk->npages = 0;\r\nchunk->nsg = 0;\r\nlist_add_tail(&chunk->list, &hem->chunk_list);\r\n}\r\nwhile (1 << order > npages)\r\n--order;\r\nmem = &chunk->mem[chunk->npages];\r\nbuf = dma_alloc_coherent(&hr_dev->pdev->dev, PAGE_SIZE << order,\r\n&sg_dma_address(mem), gfp_mask);\r\nif (!buf)\r\ngoto fail;\r\nsg_set_buf(mem, buf, PAGE_SIZE << order);\r\nWARN_ON(mem->offset);\r\nsg_dma_len(mem) = PAGE_SIZE << order;\r\n++chunk->npages;\r\n++chunk->nsg;\r\nnpages -= 1 << order;\r\n}\r\nreturn hem;\r\nfail:\r\nhns_roce_free_hem(hr_dev, hem);\r\nreturn NULL;\r\n}\r\nvoid hns_roce_free_hem(struct hns_roce_dev *hr_dev, struct hns_roce_hem *hem)\r\n{\r\nstruct hns_roce_hem_chunk *chunk, *tmp;\r\nint i;\r\nif (!hem)\r\nreturn;\r\nlist_for_each_entry_safe(chunk, tmp, &hem->chunk_list, list) {\r\nfor (i = 0; i < chunk->npages; ++i)\r\ndma_free_coherent(&hr_dev->pdev->dev,\r\nchunk->mem[i].length,\r\nlowmem_page_address(sg_page(&chunk->mem[i])),\r\nsg_dma_address(&chunk->mem[i]));\r\nkfree(chunk);\r\n}\r\nkfree(hem);\r\n}\r\nstatic int hns_roce_set_hem(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_hem_table *table, unsigned long obj)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nspinlock_t *lock = &hr_dev->bt_cmd_lock;\r\nunsigned long end = 0;\r\nunsigned long flags;\r\nstruct hns_roce_hem_iter iter;\r\nvoid __iomem *bt_cmd;\r\nu32 bt_cmd_h_val = 0;\r\nu32 bt_cmd_val[2];\r\nu32 bt_cmd_l = 0;\r\nu64 bt_ba = 0;\r\nint ret = 0;\r\nunsigned long i = (obj & (table->num_obj - 1)) /\r\n(HNS_ROCE_TABLE_CHUNK_SIZE / table->obj_size);\r\nswitch (table->type) {\r\ncase HEM_TYPE_QPC:\r\nroce_set_field(bt_cmd_h_val, ROCEE_BT_CMD_H_ROCEE_BT_CMD_MDF_M,\r\nROCEE_BT_CMD_H_ROCEE_BT_CMD_MDF_S, HEM_TYPE_QPC);\r\nbreak;\r\ncase HEM_TYPE_MTPT:\r\nroce_set_field(bt_cmd_h_val, ROCEE_BT_CMD_H_ROCEE_BT_CMD_MDF_M,\r\nROCEE_BT_CMD_H_ROCEE_BT_CMD_MDF_S,\r\nHEM_TYPE_MTPT);\r\nbreak;\r\ncase HEM_TYPE_CQC:\r\nroce_set_field(bt_cmd_h_val, ROCEE_BT_CMD_H_ROCEE_BT_CMD_MDF_M,\r\nROCEE_BT_CMD_H_ROCEE_BT_CMD_MDF_S, HEM_TYPE_CQC);\r\nbreak;\r\ncase HEM_TYPE_SRQC:\r\nroce_set_field(bt_cmd_h_val, ROCEE_BT_CMD_H_ROCEE_BT_CMD_MDF_M,\r\nROCEE_BT_CMD_H_ROCEE_BT_CMD_MDF_S,\r\nHEM_TYPE_SRQC);\r\nbreak;\r\ndefault:\r\nreturn ret;\r\n}\r\nroce_set_field(bt_cmd_h_val, ROCEE_BT_CMD_H_ROCEE_BT_CMD_IN_MDF_M,\r\nROCEE_BT_CMD_H_ROCEE_BT_CMD_IN_MDF_S, obj);\r\nroce_set_bit(bt_cmd_h_val, ROCEE_BT_CMD_H_ROCEE_BT_CMD_S, 0);\r\nroce_set_bit(bt_cmd_h_val, ROCEE_BT_CMD_H_ROCEE_BT_CMD_HW_SYNS_S, 1);\r\nfor (hns_roce_hem_first(table->hem[i], &iter);\r\n!hns_roce_hem_last(&iter); hns_roce_hem_next(&iter)) {\r\nbt_ba = hns_roce_hem_addr(&iter) >> DMA_ADDR_T_SHIFT;\r\nspin_lock_irqsave(lock, flags);\r\nbt_cmd = hr_dev->reg_base + ROCEE_BT_CMD_H_REG;\r\nend = msecs_to_jiffies(HW_SYNC_TIMEOUT_MSECS) + jiffies;\r\nwhile (1) {\r\nif (readl(bt_cmd) >> BT_CMD_SYNC_SHIFT) {\r\nif (!(time_before(jiffies, end))) {\r\ndev_err(dev, "Write bt_cmd err,hw_sync is not zero.\n");\r\nspin_unlock_irqrestore(lock, flags);\r\nreturn -EBUSY;\r\n}\r\n} else {\r\nbreak;\r\n}\r\nmsleep(HW_SYNC_SLEEP_TIME_INTERVAL);\r\n}\r\nbt_cmd_l = (u32)bt_ba;\r\nroce_set_field(bt_cmd_h_val, ROCEE_BT_CMD_H_ROCEE_BT_CMD_BA_H_M,\r\nROCEE_BT_CMD_H_ROCEE_BT_CMD_BA_H_S,\r\nbt_ba >> BT_BA_SHIFT);\r\nbt_cmd_val[0] = bt_cmd_l;\r\nbt_cmd_val[1] = bt_cmd_h_val;\r\nhns_roce_write64_k(bt_cmd_val,\r\nhr_dev->reg_base + ROCEE_BT_CMD_L_REG);\r\nspin_unlock_irqrestore(lock, flags);\r\n}\r\nreturn ret;\r\n}\r\nint hns_roce_table_get(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_hem_table *table, unsigned long obj)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nint ret = 0;\r\nunsigned long i;\r\ni = (obj & (table->num_obj - 1)) / (HNS_ROCE_TABLE_CHUNK_SIZE /\r\ntable->obj_size);\r\nmutex_lock(&table->mutex);\r\nif (table->hem[i]) {\r\n++table->hem[i]->refcount;\r\ngoto out;\r\n}\r\ntable->hem[i] = hns_roce_alloc_hem(hr_dev,\r\nHNS_ROCE_TABLE_CHUNK_SIZE >> PAGE_SHIFT,\r\n(table->lowmem ? GFP_KERNEL :\r\nGFP_HIGHUSER) | __GFP_NOWARN);\r\nif (!table->hem[i]) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (hns_roce_set_hem(hr_dev, table, obj)) {\r\nret = -ENODEV;\r\ndev_err(dev, "set HEM base address to HW failed.\n");\r\ngoto out;\r\n}\r\n++table->hem[i]->refcount;\r\nout:\r\nmutex_unlock(&table->mutex);\r\nreturn ret;\r\n}\r\nvoid hns_roce_table_put(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_hem_table *table, unsigned long obj)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nunsigned long i;\r\ni = (obj & (table->num_obj - 1)) /\r\n(HNS_ROCE_TABLE_CHUNK_SIZE / table->obj_size);\r\nmutex_lock(&table->mutex);\r\nif (--table->hem[i]->refcount == 0) {\r\nif (hr_dev->hw->clear_hem(hr_dev, table, obj))\r\ndev_warn(dev, "Clear HEM base address failed.\n");\r\nhns_roce_free_hem(hr_dev, table->hem[i]);\r\ntable->hem[i] = NULL;\r\n}\r\nmutex_unlock(&table->mutex);\r\n}\r\nvoid *hns_roce_table_find(struct hns_roce_hem_table *table, unsigned long obj,\r\ndma_addr_t *dma_handle)\r\n{\r\nstruct hns_roce_hem_chunk *chunk;\r\nunsigned long idx;\r\nint i;\r\nint offset, dma_offset;\r\nstruct hns_roce_hem *hem;\r\nstruct page *page = NULL;\r\nif (!table->lowmem)\r\nreturn NULL;\r\nmutex_lock(&table->mutex);\r\nidx = (obj & (table->num_obj - 1)) * table->obj_size;\r\nhem = table->hem[idx / HNS_ROCE_TABLE_CHUNK_SIZE];\r\ndma_offset = offset = idx % HNS_ROCE_TABLE_CHUNK_SIZE;\r\nif (!hem)\r\ngoto out;\r\nlist_for_each_entry(chunk, &hem->chunk_list, list) {\r\nfor (i = 0; i < chunk->npages; ++i) {\r\nif (dma_handle && dma_offset >= 0) {\r\nif (sg_dma_len(&chunk->mem[i]) >\r\n(u32)dma_offset)\r\n*dma_handle = sg_dma_address(\r\n&chunk->mem[i]) + dma_offset;\r\ndma_offset -= sg_dma_len(&chunk->mem[i]);\r\n}\r\nif (chunk->mem[i].length > (u32)offset) {\r\npage = sg_page(&chunk->mem[i]);\r\ngoto out;\r\n}\r\noffset -= chunk->mem[i].length;\r\n}\r\n}\r\nout:\r\nmutex_unlock(&table->mutex);\r\nreturn page ? lowmem_page_address(page) + offset : NULL;\r\n}\r\nint hns_roce_table_get_range(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_hem_table *table,\r\nunsigned long start, unsigned long end)\r\n{\r\nunsigned long inc = HNS_ROCE_TABLE_CHUNK_SIZE / table->obj_size;\r\nunsigned long i = 0;\r\nint ret = 0;\r\nfor (i = start; i <= end; i += inc) {\r\nret = hns_roce_table_get(hr_dev, table, i);\r\nif (ret)\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nwhile (i > start) {\r\ni -= inc;\r\nhns_roce_table_put(hr_dev, table, i);\r\n}\r\nreturn ret;\r\n}\r\nvoid hns_roce_table_put_range(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_hem_table *table,\r\nunsigned long start, unsigned long end)\r\n{\r\nunsigned long i;\r\nfor (i = start; i <= end;\r\ni += HNS_ROCE_TABLE_CHUNK_SIZE / table->obj_size)\r\nhns_roce_table_put(hr_dev, table, i);\r\n}\r\nint hns_roce_init_hem_table(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_hem_table *table, u32 type,\r\nunsigned long obj_size, unsigned long nobj,\r\nint use_lowmem)\r\n{\r\nunsigned long obj_per_chunk;\r\nunsigned long num_hem;\r\nobj_per_chunk = HNS_ROCE_TABLE_CHUNK_SIZE / obj_size;\r\nnum_hem = (nobj + obj_per_chunk - 1) / obj_per_chunk;\r\ntable->hem = kcalloc(num_hem, sizeof(*table->hem), GFP_KERNEL);\r\nif (!table->hem)\r\nreturn -ENOMEM;\r\ntable->type = type;\r\ntable->num_hem = num_hem;\r\ntable->num_obj = nobj;\r\ntable->obj_size = obj_size;\r\ntable->lowmem = use_lowmem;\r\nmutex_init(&table->mutex);\r\nreturn 0;\r\n}\r\nvoid hns_roce_cleanup_hem_table(struct hns_roce_dev *hr_dev,\r\nstruct hns_roce_hem_table *table)\r\n{\r\nstruct device *dev = &hr_dev->pdev->dev;\r\nunsigned long i;\r\nfor (i = 0; i < table->num_hem; ++i)\r\nif (table->hem[i]) {\r\nif (hr_dev->hw->clear_hem(hr_dev, table,\r\ni * HNS_ROCE_TABLE_CHUNK_SIZE / table->obj_size))\r\ndev_err(dev, "Clear HEM base address failed.\n");\r\nhns_roce_free_hem(hr_dev, table->hem[i]);\r\n}\r\nkfree(table->hem);\r\n}\r\nvoid hns_roce_cleanup_hem(struct hns_roce_dev *hr_dev)\r\n{\r\nhns_roce_cleanup_hem_table(hr_dev, &hr_dev->cq_table.table);\r\nhns_roce_cleanup_hem_table(hr_dev, &hr_dev->qp_table.irrl_table);\r\nhns_roce_cleanup_hem_table(hr_dev, &hr_dev->qp_table.qp_table);\r\nhns_roce_cleanup_hem_table(hr_dev, &hr_dev->mr_table.mtpt_table);\r\nhns_roce_cleanup_hem_table(hr_dev, &hr_dev->mr_table.mtt_table);\r\n}
