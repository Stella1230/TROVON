static ssize_t\r\ncsio_mem_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)\r\n{\r\nloff_t pos = *ppos;\r\nloff_t avail = file_inode(file)->i_size;\r\nunsigned int mem = (uintptr_t)file->private_data & 3;\r\nstruct csio_hw *hw = file->private_data - mem;\r\nif (pos < 0)\r\nreturn -EINVAL;\r\nif (pos >= avail)\r\nreturn 0;\r\nif (count > avail - pos)\r\ncount = avail - pos;\r\nwhile (count) {\r\nsize_t len;\r\nint ret, ofst;\r\n__be32 data[16];\r\nif (mem == MEM_MC)\r\nret = hw->chip_ops->chip_mc_read(hw, 0, pos,\r\ndata, NULL);\r\nelse\r\nret = hw->chip_ops->chip_edc_read(hw, mem, pos,\r\ndata, NULL);\r\nif (ret)\r\nreturn ret;\r\nofst = pos % sizeof(data);\r\nlen = min(count, sizeof(data) - ofst);\r\nif (copy_to_user(buf, (u8 *)data + ofst, len))\r\nreturn -EFAULT;\r\nbuf += len;\r\npos += len;\r\ncount -= len;\r\n}\r\ncount = pos - *ppos;\r\n*ppos = pos;\r\nreturn count;\r\n}\r\nvoid csio_add_debugfs_mem(struct csio_hw *hw, const char *name,\r\nunsigned int idx, unsigned int size_mb)\r\n{\r\ndebugfs_create_file_size(name, S_IRUSR, hw->debugfs_root,\r\n(void *)hw + idx, &csio_mem_debugfs_fops,\r\nsize_mb << 20);\r\n}\r\nstatic int csio_setup_debugfs(struct csio_hw *hw)\r\n{\r\nint i;\r\nif (IS_ERR_OR_NULL(hw->debugfs_root))\r\nreturn -1;\r\ni = csio_rd_reg32(hw, MA_TARGET_MEM_ENABLE_A);\r\nif (i & EDRAM0_ENABLE_F)\r\ncsio_add_debugfs_mem(hw, "edc0", MEM_EDC0, 5);\r\nif (i & EDRAM1_ENABLE_F)\r\ncsio_add_debugfs_mem(hw, "edc1", MEM_EDC1, 5);\r\nhw->chip_ops->chip_dfs_create_ext_mem(hw);\r\nreturn 0;\r\n}\r\nstatic int\r\ncsio_dfs_create(struct csio_hw *hw)\r\n{\r\nif (csio_debugfs_root) {\r\nhw->debugfs_root = debugfs_create_dir(pci_name(hw->pdev),\r\ncsio_debugfs_root);\r\ncsio_setup_debugfs(hw);\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\ncsio_dfs_destroy(struct csio_hw *hw)\r\n{\r\nif (hw->debugfs_root)\r\ndebugfs_remove_recursive(hw->debugfs_root);\r\nreturn 0;\r\n}\r\nstatic int\r\ncsio_dfs_init(void)\r\n{\r\ncsio_debugfs_root = debugfs_create_dir(KBUILD_MODNAME, NULL);\r\nif (!csio_debugfs_root)\r\npr_warn("Could not create debugfs entry, continuing\n");\r\nreturn 0;\r\n}\r\nstatic void\r\ncsio_dfs_exit(void)\r\n{\r\ndebugfs_remove(csio_debugfs_root);\r\n}\r\nstatic int\r\ncsio_pci_init(struct pci_dev *pdev, int *bars)\r\n{\r\nint rv = -ENODEV;\r\n*bars = pci_select_bars(pdev, IORESOURCE_MEM);\r\nif (pci_enable_device_mem(pdev))\r\ngoto err;\r\nif (pci_request_selected_regions(pdev, *bars, KBUILD_MODNAME))\r\ngoto err_disable_device;\r\npci_set_master(pdev);\r\npci_try_set_mwi(pdev);\r\nif (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\npci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));\r\n} else if (!pci_set_dma_mask(pdev, DMA_BIT_MASK(32))) {\r\npci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));\r\n} else {\r\ndev_err(&pdev->dev, "No suitable DMA available.\n");\r\ngoto err_release_regions;\r\n}\r\nreturn 0;\r\nerr_release_regions:\r\npci_release_selected_regions(pdev, *bars);\r\nerr_disable_device:\r\npci_disable_device(pdev);\r\nerr:\r\nreturn rv;\r\n}\r\nstatic void\r\ncsio_pci_exit(struct pci_dev *pdev, int *bars)\r\n{\r\npci_release_selected_regions(pdev, *bars);\r\npci_disable_device(pdev);\r\n}\r\nstatic void\r\ncsio_hw_init_workers(struct csio_hw *hw)\r\n{\r\nINIT_WORK(&hw->evtq_work, csio_evtq_worker);\r\n}\r\nstatic void\r\ncsio_hw_exit_workers(struct csio_hw *hw)\r\n{\r\ncancel_work_sync(&hw->evtq_work);\r\nflush_scheduled_work();\r\n}\r\nstatic int\r\ncsio_create_queues(struct csio_hw *hw)\r\n{\r\nint i, j;\r\nstruct csio_mgmtm *mgmtm = csio_hw_to_mgmtm(hw);\r\nint rv;\r\nstruct csio_scsi_cpu_info *info;\r\nif (hw->flags & CSIO_HWF_Q_FW_ALLOCED)\r\nreturn 0;\r\nif (hw->intr_mode != CSIO_IM_MSIX) {\r\nrv = csio_wr_iq_create(hw, NULL, hw->intr_iq_idx,\r\n0, hw->pport[0].portid, false, NULL);\r\nif (rv != 0) {\r\ncsio_err(hw, " Forward Interrupt IQ failed!: %d\n", rv);\r\nreturn rv;\r\n}\r\n}\r\nrv = csio_wr_iq_create(hw, NULL, hw->fwevt_iq_idx,\r\ncsio_get_fwevt_intr_idx(hw),\r\nhw->pport[0].portid, true, NULL);\r\nif (rv != 0) {\r\ncsio_err(hw, "FW event IQ config failed!: %d\n", rv);\r\nreturn rv;\r\n}\r\nrv = csio_wr_eq_create(hw, NULL, mgmtm->eq_idx,\r\nmgmtm->iq_idx, hw->pport[0].portid, NULL);\r\nif (rv != 0) {\r\ncsio_err(hw, "Mgmt EQ create failed!: %d\n", rv);\r\ngoto err;\r\n}\r\nfor (i = 0; i < hw->num_pports; i++) {\r\ninfo = &hw->scsi_cpu_info[i];\r\nfor (j = 0; j < info->max_cpus; j++) {\r\nstruct csio_scsi_qset *sqset = &hw->sqset[i][j];\r\nrv = csio_wr_iq_create(hw, NULL, sqset->iq_idx,\r\nsqset->intr_idx, i, false, NULL);\r\nif (rv != 0) {\r\ncsio_err(hw,\r\n"SCSI module IQ config failed [%d][%d]:%d\n",\r\ni, j, rv);\r\ngoto err;\r\n}\r\nrv = csio_wr_eq_create(hw, NULL, sqset->eq_idx,\r\nsqset->iq_idx, i, NULL);\r\nif (rv != 0) {\r\ncsio_err(hw,\r\n"SCSI module EQ config failed [%d][%d]:%d\n",\r\ni, j, rv);\r\ngoto err;\r\n}\r\n}\r\n}\r\nhw->flags |= CSIO_HWF_Q_FW_ALLOCED;\r\nreturn 0;\r\nerr:\r\ncsio_wr_destroy_queues(hw, true);\r\nreturn -EINVAL;\r\n}\r\nint\r\ncsio_config_queues(struct csio_hw *hw)\r\n{\r\nint i, j, idx, k = 0;\r\nint rv;\r\nstruct csio_scsi_qset *sqset;\r\nstruct csio_mgmtm *mgmtm = csio_hw_to_mgmtm(hw);\r\nstruct csio_scsi_qset *orig;\r\nstruct csio_scsi_cpu_info *info;\r\nif (hw->flags & CSIO_HWF_Q_MEM_ALLOCED)\r\nreturn csio_create_queues(hw);\r\nhw->num_scsi_msix_cpus = num_online_cpus();\r\nhw->num_sqsets = num_online_cpus() * hw->num_pports;\r\nif (hw->num_sqsets > CSIO_MAX_SCSI_QSETS) {\r\nhw->num_sqsets = CSIO_MAX_SCSI_QSETS;\r\nhw->num_scsi_msix_cpus = CSIO_MAX_SCSI_CPU;\r\n}\r\nfor (i = 0; i < hw->num_pports; i++)\r\nhw->scsi_cpu_info[i].max_cpus = hw->num_scsi_msix_cpus;\r\ncsio_dbg(hw, "nsqsets:%d scpus:%d\n",\r\nhw->num_sqsets, hw->num_scsi_msix_cpus);\r\ncsio_intr_enable(hw);\r\nif (hw->intr_mode != CSIO_IM_MSIX) {\r\nhw->intr_iq_idx = csio_wr_alloc_q(hw, CSIO_INTR_IQSIZE,\r\nCSIO_INTR_WRSIZE, CSIO_INGRESS,\r\n(void *)hw, 0, 0, NULL);\r\nif (hw->intr_iq_idx == -1) {\r\ncsio_err(hw,\r\n"Forward interrupt queue creation failed\n");\r\ngoto intr_disable;\r\n}\r\n}\r\nhw->fwevt_iq_idx = csio_wr_alloc_q(hw, CSIO_FWEVT_IQSIZE,\r\nCSIO_FWEVT_WRSIZE,\r\nCSIO_INGRESS, (void *)hw,\r\nCSIO_FWEVT_FLBUFS, 0,\r\ncsio_fwevt_intx_handler);\r\nif (hw->fwevt_iq_idx == -1) {\r\ncsio_err(hw, "FW evt queue creation failed\n");\r\ngoto intr_disable;\r\n}\r\nmgmtm->eq_idx = csio_wr_alloc_q(hw, CSIO_MGMT_EQSIZE,\r\nCSIO_MGMT_EQ_WRSIZE,\r\nCSIO_EGRESS, (void *)hw, 0, 0, NULL);\r\nif (mgmtm->eq_idx == -1) {\r\ncsio_err(hw, "Failed to alloc egress queue for mgmt module\n");\r\ngoto intr_disable;\r\n}\r\nmgmtm->iq_idx = hw->fwevt_iq_idx;\r\nfor (i = 0; i < hw->num_pports; i++) {\r\ninfo = &hw->scsi_cpu_info[i];\r\nfor (j = 0; j < hw->num_scsi_msix_cpus; j++) {\r\nsqset = &hw->sqset[i][j];\r\nif (j >= info->max_cpus) {\r\nk = j % info->max_cpus;\r\norig = &hw->sqset[i][k];\r\nsqset->eq_idx = orig->eq_idx;\r\nsqset->iq_idx = orig->iq_idx;\r\ncontinue;\r\n}\r\nidx = csio_wr_alloc_q(hw, csio_scsi_eqsize, 0,\r\nCSIO_EGRESS, (void *)hw, 0, 0,\r\nNULL);\r\nif (idx == -1) {\r\ncsio_err(hw, "EQ creation failed for idx:%d\n",\r\nidx);\r\ngoto intr_disable;\r\n}\r\nsqset->eq_idx = idx;\r\nidx = csio_wr_alloc_q(hw, CSIO_SCSI_IQSIZE,\r\nCSIO_SCSI_IQ_WRSZ, CSIO_INGRESS,\r\n(void *)hw, 0, 0,\r\ncsio_scsi_intx_handler);\r\nif (idx == -1) {\r\ncsio_err(hw, "IQ creation failed for idx:%d\n",\r\nidx);\r\ngoto intr_disable;\r\n}\r\nsqset->iq_idx = idx;\r\n}\r\n}\r\nhw->flags |= CSIO_HWF_Q_MEM_ALLOCED;\r\nrv = csio_create_queues(hw);\r\nif (rv != 0)\r\ngoto intr_disable;\r\nrv = csio_request_irqs(hw);\r\nif (rv != 0)\r\nreturn -EINVAL;\r\nreturn 0;\r\nintr_disable:\r\ncsio_intr_disable(hw, false);\r\nreturn -EINVAL;\r\n}\r\nstatic int\r\ncsio_resource_alloc(struct csio_hw *hw)\r\n{\r\nstruct csio_wrm *wrm = csio_hw_to_wrm(hw);\r\nint rv = -ENOMEM;\r\nwrm->num_q = ((CSIO_MAX_SCSI_QSETS * 2) + CSIO_HW_NIQ +\r\nCSIO_HW_NEQ + CSIO_HW_NFLQ + CSIO_HW_NINTXQ);\r\nhw->mb_mempool = mempool_create_kmalloc_pool(CSIO_MIN_MEMPOOL_SZ,\r\nsizeof(struct csio_mb));\r\nif (!hw->mb_mempool)\r\ngoto err;\r\nhw->rnode_mempool = mempool_create_kmalloc_pool(CSIO_MIN_MEMPOOL_SZ,\r\nsizeof(struct csio_rnode));\r\nif (!hw->rnode_mempool)\r\ngoto err_free_mb_mempool;\r\nhw->scsi_pci_pool = pci_pool_create("csio_scsi_pci_pool", hw->pdev,\r\nCSIO_SCSI_RSP_LEN, 8, 0);\r\nif (!hw->scsi_pci_pool)\r\ngoto err_free_rn_pool;\r\nreturn 0;\r\nerr_free_rn_pool:\r\nmempool_destroy(hw->rnode_mempool);\r\nhw->rnode_mempool = NULL;\r\nerr_free_mb_mempool:\r\nmempool_destroy(hw->mb_mempool);\r\nhw->mb_mempool = NULL;\r\nerr:\r\nreturn rv;\r\n}\r\nstatic void\r\ncsio_resource_free(struct csio_hw *hw)\r\n{\r\npci_pool_destroy(hw->scsi_pci_pool);\r\nhw->scsi_pci_pool = NULL;\r\nmempool_destroy(hw->rnode_mempool);\r\nhw->rnode_mempool = NULL;\r\nmempool_destroy(hw->mb_mempool);\r\nhw->mb_mempool = NULL;\r\n}\r\nstatic struct csio_hw *csio_hw_alloc(struct pci_dev *pdev)\r\n{\r\nstruct csio_hw *hw;\r\nhw = kzalloc(sizeof(struct csio_hw), GFP_KERNEL);\r\nif (!hw)\r\ngoto err;\r\nhw->pdev = pdev;\r\nstrncpy(hw->drv_version, CSIO_DRV_VERSION, 32);\r\nif (csio_resource_alloc(hw))\r\ngoto err_free_hw;\r\nhw->regstart = ioremap_nocache(pci_resource_start(pdev, 0),\r\npci_resource_len(pdev, 0));\r\nif (!hw->regstart) {\r\ncsio_err(hw, "Could not map BAR 0, regstart = %p\n",\r\nhw->regstart);\r\ngoto err_resource_free;\r\n}\r\ncsio_hw_init_workers(hw);\r\nif (csio_hw_init(hw))\r\ngoto err_unmap_bar;\r\ncsio_dfs_create(hw);\r\ncsio_dbg(hw, "hw:%p\n", hw);\r\nreturn hw;\r\nerr_unmap_bar:\r\ncsio_hw_exit_workers(hw);\r\niounmap(hw->regstart);\r\nerr_resource_free:\r\ncsio_resource_free(hw);\r\nerr_free_hw:\r\nkfree(hw);\r\nerr:\r\nreturn NULL;\r\n}\r\nstatic void\r\ncsio_hw_free(struct csio_hw *hw)\r\n{\r\ncsio_intr_disable(hw, true);\r\ncsio_hw_exit_workers(hw);\r\ncsio_hw_exit(hw);\r\niounmap(hw->regstart);\r\ncsio_dfs_destroy(hw);\r\ncsio_resource_free(hw);\r\nkfree(hw);\r\n}\r\nstruct csio_lnode *\r\ncsio_shost_init(struct csio_hw *hw, struct device *dev,\r\nbool probe, struct csio_lnode *pln)\r\n{\r\nstruct Scsi_Host *shost = NULL;\r\nstruct csio_lnode *ln;\r\ncsio_fcoe_shost_template.cmd_per_lun = csio_lun_qdepth;\r\ncsio_fcoe_shost_vport_template.cmd_per_lun = csio_lun_qdepth;\r\nif (dev == &hw->pdev->dev)\r\nshost = scsi_host_alloc(\r\n&csio_fcoe_shost_template,\r\nsizeof(struct csio_lnode));\r\nelse\r\nshost = scsi_host_alloc(\r\n&csio_fcoe_shost_vport_template,\r\nsizeof(struct csio_lnode));\r\nif (!shost)\r\ngoto err;\r\nln = shost_priv(shost);\r\nmemset(ln, 0, sizeof(struct csio_lnode));\r\nln->dev_num = (shost->host_no << 16);\r\nshost->can_queue = CSIO_MAX_QUEUE;\r\nshost->this_id = -1;\r\nshost->unique_id = shost->host_no;\r\nshost->max_cmd_len = 16;\r\nshost->max_id = min_t(uint32_t, csio_fcoe_rnodes,\r\nhw->fres_info.max_ssns);\r\nshost->max_lun = CSIO_MAX_LUN;\r\nif (dev == &hw->pdev->dev)\r\nshost->transportt = csio_fcoe_transport;\r\nelse\r\nshost->transportt = csio_fcoe_transport_vport;\r\nif (!hw->rln)\r\nhw->rln = ln;\r\nif (csio_lnode_init(ln, hw, pln))\r\ngoto err_shost_put;\r\nif (scsi_add_host(shost, dev))\r\ngoto err_lnode_exit;\r\nreturn ln;\r\nerr_lnode_exit:\r\ncsio_lnode_exit(ln);\r\nerr_shost_put:\r\nscsi_host_put(shost);\r\nerr:\r\nreturn NULL;\r\n}\r\nvoid\r\ncsio_shost_exit(struct csio_lnode *ln)\r\n{\r\nstruct Scsi_Host *shost = csio_ln_to_shost(ln);\r\nstruct csio_hw *hw = csio_lnode_to_hw(ln);\r\nfc_remove_host(shost);\r\nscsi_remove_host(shost);\r\nspin_lock_irq(&hw->lock);\r\ncsio_evtq_flush(hw);\r\nspin_unlock_irq(&hw->lock);\r\ncsio_lnode_exit(ln);\r\nscsi_host_put(shost);\r\n}\r\nstruct csio_lnode *\r\ncsio_lnode_alloc(struct csio_hw *hw)\r\n{\r\nreturn csio_shost_init(hw, &hw->pdev->dev, false, NULL);\r\n}\r\nvoid\r\ncsio_lnodes_block_request(struct csio_hw *hw)\r\n{\r\nstruct Scsi_Host *shost;\r\nstruct csio_lnode *sln;\r\nstruct csio_lnode *ln;\r\nstruct list_head *cur_ln, *cur_cln;\r\nstruct csio_lnode **lnode_list;\r\nint cur_cnt = 0, ii;\r\nlnode_list = kzalloc((sizeof(struct csio_lnode *) * hw->num_lns),\r\nGFP_KERNEL);\r\nif (!lnode_list) {\r\ncsio_err(hw, "Failed to allocate lnodes_list");\r\nreturn;\r\n}\r\nspin_lock_irq(&hw->lock);\r\nlist_for_each(cur_ln, &hw->sln_head) {\r\nsln = (struct csio_lnode *) cur_ln;\r\nlnode_list[cur_cnt++] = sln;\r\nlist_for_each(cur_cln, &sln->cln_head)\r\nlnode_list[cur_cnt++] = (struct csio_lnode *) cur_cln;\r\n}\r\nspin_unlock_irq(&hw->lock);\r\nfor (ii = 0; ii < cur_cnt; ii++) {\r\ncsio_dbg(hw, "Blocking IOs on lnode: %p\n", lnode_list[ii]);\r\nln = lnode_list[ii];\r\nshost = csio_ln_to_shost(ln);\r\nscsi_block_requests(shost);\r\n}\r\nkfree(lnode_list);\r\n}\r\nvoid\r\ncsio_lnodes_unblock_request(struct csio_hw *hw)\r\n{\r\nstruct csio_lnode *ln;\r\nstruct Scsi_Host *shost;\r\nstruct csio_lnode *sln;\r\nstruct list_head *cur_ln, *cur_cln;\r\nstruct csio_lnode **lnode_list;\r\nint cur_cnt = 0, ii;\r\nlnode_list = kzalloc((sizeof(struct csio_lnode *) * hw->num_lns),\r\nGFP_KERNEL);\r\nif (!lnode_list) {\r\ncsio_err(hw, "Failed to allocate lnodes_list");\r\nreturn;\r\n}\r\nspin_lock_irq(&hw->lock);\r\nlist_for_each(cur_ln, &hw->sln_head) {\r\nsln = (struct csio_lnode *) cur_ln;\r\nlnode_list[cur_cnt++] = sln;\r\nlist_for_each(cur_cln, &sln->cln_head)\r\nlnode_list[cur_cnt++] = (struct csio_lnode *) cur_cln;\r\n}\r\nspin_unlock_irq(&hw->lock);\r\nfor (ii = 0; ii < cur_cnt; ii++) {\r\ncsio_dbg(hw, "unblocking IOs on lnode: %p\n", lnode_list[ii]);\r\nln = lnode_list[ii];\r\nshost = csio_ln_to_shost(ln);\r\nscsi_unblock_requests(shost);\r\n}\r\nkfree(lnode_list);\r\n}\r\nvoid\r\ncsio_lnodes_block_by_port(struct csio_hw *hw, uint8_t portid)\r\n{\r\nstruct csio_lnode *ln;\r\nstruct Scsi_Host *shost;\r\nstruct csio_lnode *sln;\r\nstruct list_head *cur_ln, *cur_cln;\r\nstruct csio_lnode **lnode_list;\r\nint cur_cnt = 0, ii;\r\nlnode_list = kzalloc((sizeof(struct csio_lnode *) * hw->num_lns),\r\nGFP_KERNEL);\r\nif (!lnode_list) {\r\ncsio_err(hw, "Failed to allocate lnodes_list");\r\nreturn;\r\n}\r\nspin_lock_irq(&hw->lock);\r\nlist_for_each(cur_ln, &hw->sln_head) {\r\nsln = (struct csio_lnode *) cur_ln;\r\nif (sln->portid != portid)\r\ncontinue;\r\nlnode_list[cur_cnt++] = sln;\r\nlist_for_each(cur_cln, &sln->cln_head)\r\nlnode_list[cur_cnt++] = (struct csio_lnode *) cur_cln;\r\n}\r\nspin_unlock_irq(&hw->lock);\r\nfor (ii = 0; ii < cur_cnt; ii++) {\r\ncsio_dbg(hw, "Blocking IOs on lnode: %p\n", lnode_list[ii]);\r\nln = lnode_list[ii];\r\nshost = csio_ln_to_shost(ln);\r\nscsi_block_requests(shost);\r\n}\r\nkfree(lnode_list);\r\n}\r\nvoid\r\ncsio_lnodes_unblock_by_port(struct csio_hw *hw, uint8_t portid)\r\n{\r\nstruct csio_lnode *ln;\r\nstruct Scsi_Host *shost;\r\nstruct csio_lnode *sln;\r\nstruct list_head *cur_ln, *cur_cln;\r\nstruct csio_lnode **lnode_list;\r\nint cur_cnt = 0, ii;\r\nlnode_list = kzalloc((sizeof(struct csio_lnode *) * hw->num_lns),\r\nGFP_KERNEL);\r\nif (!lnode_list) {\r\ncsio_err(hw, "Failed to allocate lnodes_list");\r\nreturn;\r\n}\r\nspin_lock_irq(&hw->lock);\r\nlist_for_each(cur_ln, &hw->sln_head) {\r\nsln = (struct csio_lnode *) cur_ln;\r\nif (sln->portid != portid)\r\ncontinue;\r\nlnode_list[cur_cnt++] = sln;\r\nlist_for_each(cur_cln, &sln->cln_head)\r\nlnode_list[cur_cnt++] = (struct csio_lnode *) cur_cln;\r\n}\r\nspin_unlock_irq(&hw->lock);\r\nfor (ii = 0; ii < cur_cnt; ii++) {\r\ncsio_dbg(hw, "unblocking IOs on lnode: %p\n", lnode_list[ii]);\r\nln = lnode_list[ii];\r\nshost = csio_ln_to_shost(ln);\r\nscsi_unblock_requests(shost);\r\n}\r\nkfree(lnode_list);\r\n}\r\nvoid\r\ncsio_lnodes_exit(struct csio_hw *hw, bool npiv)\r\n{\r\nstruct csio_lnode *sln;\r\nstruct csio_lnode *ln;\r\nstruct list_head *cur_ln, *cur_cln;\r\nstruct csio_lnode **lnode_list;\r\nint cur_cnt = 0, ii;\r\nlnode_list = kzalloc((sizeof(struct csio_lnode *) * hw->num_lns),\r\nGFP_KERNEL);\r\nif (!lnode_list) {\r\ncsio_err(hw, "lnodes_exit: Failed to allocate lnodes_list.\n");\r\nreturn;\r\n}\r\nspin_lock_irq(&hw->lock);\r\nlist_for_each(cur_ln, &hw->sln_head) {\r\nsln = (struct csio_lnode *) cur_ln;\r\nlist_for_each(cur_cln, &sln->cln_head)\r\nlnode_list[cur_cnt++] = (struct csio_lnode *) cur_cln;\r\n}\r\nspin_unlock_irq(&hw->lock);\r\nfor (ii = 0; ii < cur_cnt; ii++) {\r\ncsio_dbg(hw, "Deleting child lnode: %p\n", lnode_list[ii]);\r\nln = lnode_list[ii];\r\nfc_vport_terminate(ln->fc_vport);\r\n}\r\nif (npiv)\r\ngoto free_lnodes;\r\ncur_cnt = 0;\r\nspin_lock_irq(&hw->lock);\r\nlist_for_each(cur_ln, &hw->sln_head) {\r\nsln = (struct csio_lnode *) cur_ln;\r\nlnode_list[cur_cnt++] = sln;\r\n}\r\nspin_unlock_irq(&hw->lock);\r\nfor (ii = 0; ii < cur_cnt; ii++) {\r\ncsio_dbg(hw, "Deleting parent lnode: %p\n", lnode_list[ii]);\r\ncsio_shost_exit(lnode_list[ii]);\r\n}\r\nfree_lnodes:\r\nkfree(lnode_list);\r\n}\r\nstatic void\r\ncsio_lnode_init_post(struct csio_lnode *ln)\r\n{\r\nstruct Scsi_Host *shost = csio_ln_to_shost(ln);\r\ncsio_fchost_attr_init(ln);\r\nscsi_scan_host(shost);\r\n}\r\nstatic int csio_probe_one(struct pci_dev *pdev, const struct pci_device_id *id)\r\n{\r\nint rv;\r\nint bars;\r\nint i;\r\nstruct csio_hw *hw;\r\nstruct csio_lnode *ln;\r\nif (!csio_is_t5((pdev->device & CSIO_HW_CHIP_MASK)))\r\nreturn -ENODEV;\r\nrv = csio_pci_init(pdev, &bars);\r\nif (rv)\r\ngoto err;\r\nhw = csio_hw_alloc(pdev);\r\nif (!hw) {\r\nrv = -ENODEV;\r\ngoto err_pci_exit;\r\n}\r\npci_set_drvdata(pdev, hw);\r\nif (csio_hw_start(hw) != 0) {\r\ndev_err(&pdev->dev,\r\n"Failed to start FW, continuing in debug mode.\n");\r\nreturn 0;\r\n}\r\nsprintf(hw->fwrev_str, "%u.%u.%u.%u\n",\r\nFW_HDR_FW_VER_MAJOR_G(hw->fwrev),\r\nFW_HDR_FW_VER_MINOR_G(hw->fwrev),\r\nFW_HDR_FW_VER_MICRO_G(hw->fwrev),\r\nFW_HDR_FW_VER_BUILD_G(hw->fwrev));\r\nfor (i = 0; i < hw->num_pports; i++) {\r\nln = csio_shost_init(hw, &pdev->dev, true, NULL);\r\nif (!ln) {\r\nrv = -ENODEV;\r\nbreak;\r\n}\r\nln->portid = hw->pport[i].portid;\r\nspin_lock_irq(&hw->lock);\r\nif (csio_lnode_start(ln) != 0)\r\nrv = -ENODEV;\r\nspin_unlock_irq(&hw->lock);\r\nif (rv)\r\nbreak;\r\ncsio_lnode_init_post(ln);\r\n}\r\nif (rv)\r\ngoto err_lnode_exit;\r\nreturn 0;\r\nerr_lnode_exit:\r\ncsio_lnodes_block_request(hw);\r\nspin_lock_irq(&hw->lock);\r\ncsio_hw_stop(hw);\r\nspin_unlock_irq(&hw->lock);\r\ncsio_lnodes_unblock_request(hw);\r\ncsio_lnodes_exit(hw, 0);\r\ncsio_hw_free(hw);\r\nerr_pci_exit:\r\ncsio_pci_exit(pdev, &bars);\r\nerr:\r\ndev_err(&pdev->dev, "probe of device failed: %d\n", rv);\r\nreturn rv;\r\n}\r\nstatic void csio_remove_one(struct pci_dev *pdev)\r\n{\r\nstruct csio_hw *hw = pci_get_drvdata(pdev);\r\nint bars = pci_select_bars(pdev, IORESOURCE_MEM);\r\ncsio_lnodes_block_request(hw);\r\nspin_lock_irq(&hw->lock);\r\ncsio_hw_stop(hw);\r\nspin_unlock_irq(&hw->lock);\r\ncsio_lnodes_unblock_request(hw);\r\ncsio_lnodes_exit(hw, 0);\r\ncsio_hw_free(hw);\r\ncsio_pci_exit(pdev, &bars);\r\n}\r\nstatic pci_ers_result_t\r\ncsio_pci_error_detected(struct pci_dev *pdev, pci_channel_state_t state)\r\n{\r\nstruct csio_hw *hw = pci_get_drvdata(pdev);\r\ncsio_lnodes_block_request(hw);\r\nspin_lock_irq(&hw->lock);\r\ncsio_post_event(&hw->sm, CSIO_HWE_PCIERR_DETECTED);\r\nspin_unlock_irq(&hw->lock);\r\ncsio_lnodes_unblock_request(hw);\r\ncsio_lnodes_exit(hw, 0);\r\ncsio_intr_disable(hw, true);\r\npci_disable_device(pdev);\r\nreturn state == pci_channel_io_perm_failure ?\r\nPCI_ERS_RESULT_DISCONNECT : PCI_ERS_RESULT_NEED_RESET;\r\n}\r\nstatic pci_ers_result_t\r\ncsio_pci_slot_reset(struct pci_dev *pdev)\r\n{\r\nstruct csio_hw *hw = pci_get_drvdata(pdev);\r\nint ready;\r\nif (pci_enable_device(pdev)) {\r\ndev_err(&pdev->dev, "cannot re-enable device in slot reset\n");\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\npci_set_master(pdev);\r\npci_restore_state(pdev);\r\npci_save_state(pdev);\r\npci_cleanup_aer_uncorrect_error_status(pdev);\r\nspin_lock_irq(&hw->lock);\r\ncsio_post_event(&hw->sm, CSIO_HWE_PCIERR_SLOT_RESET);\r\nready = csio_is_hw_ready(hw);\r\nspin_unlock_irq(&hw->lock);\r\nif (ready) {\r\nreturn PCI_ERS_RESULT_RECOVERED;\r\n} else {\r\ndev_err(&pdev->dev, "Can't initialize HW when in slot reset\n");\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\n}\r\nstatic void\r\ncsio_pci_resume(struct pci_dev *pdev)\r\n{\r\nstruct csio_hw *hw = pci_get_drvdata(pdev);\r\nstruct csio_lnode *ln;\r\nint rv = 0;\r\nint i;\r\nfor (i = 0; i < hw->num_pports; i++) {\r\nln = csio_shost_init(hw, &pdev->dev, true, NULL);\r\nif (!ln) {\r\nrv = -ENODEV;\r\nbreak;\r\n}\r\nln->portid = hw->pport[i].portid;\r\nspin_lock_irq(&hw->lock);\r\nif (csio_lnode_start(ln) != 0)\r\nrv = -ENODEV;\r\nspin_unlock_irq(&hw->lock);\r\nif (rv)\r\nbreak;\r\ncsio_lnode_init_post(ln);\r\n}\r\nif (rv)\r\ngoto err_resume_exit;\r\nreturn;\r\nerr_resume_exit:\r\ncsio_lnodes_block_request(hw);\r\nspin_lock_irq(&hw->lock);\r\ncsio_hw_stop(hw);\r\nspin_unlock_irq(&hw->lock);\r\ncsio_lnodes_unblock_request(hw);\r\ncsio_lnodes_exit(hw, 0);\r\ncsio_hw_free(hw);\r\ndev_err(&pdev->dev, "resume of device failed: %d\n", rv);\r\n}\r\nstatic int __init\r\ncsio_init(void)\r\n{\r\nint rv = -ENOMEM;\r\npr_info("%s %s\n", CSIO_DRV_DESC, CSIO_DRV_VERSION);\r\ncsio_dfs_init();\r\ncsio_fcoe_transport = fc_attach_transport(&csio_fc_transport_funcs);\r\nif (!csio_fcoe_transport)\r\ngoto err;\r\ncsio_fcoe_transport_vport =\r\nfc_attach_transport(&csio_fc_transport_vport_funcs);\r\nif (!csio_fcoe_transport_vport)\r\ngoto err_vport;\r\nrv = pci_register_driver(&csio_pci_driver);\r\nif (rv)\r\ngoto err_pci;\r\nreturn 0;\r\nerr_pci:\r\nfc_release_transport(csio_fcoe_transport_vport);\r\nerr_vport:\r\nfc_release_transport(csio_fcoe_transport);\r\nerr:\r\ncsio_dfs_exit();\r\nreturn rv;\r\n}\r\nstatic void __exit\r\ncsio_exit(void)\r\n{\r\npci_unregister_driver(&csio_pci_driver);\r\ncsio_dfs_exit();\r\nfc_release_transport(csio_fcoe_transport_vport);\r\nfc_release_transport(csio_fcoe_transport);\r\n}
