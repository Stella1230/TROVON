static void chacha20_doneon(u32 *state, u8 *dst, const u8 *src,\r\nunsigned int bytes)\r\n{\r\nu8 buf[CHACHA20_BLOCK_SIZE];\r\nwhile (bytes >= CHACHA20_BLOCK_SIZE * 4) {\r\nchacha20_4block_xor_neon(state, dst, src);\r\nbytes -= CHACHA20_BLOCK_SIZE * 4;\r\nsrc += CHACHA20_BLOCK_SIZE * 4;\r\ndst += CHACHA20_BLOCK_SIZE * 4;\r\nstate[12] += 4;\r\n}\r\nwhile (bytes >= CHACHA20_BLOCK_SIZE) {\r\nchacha20_block_xor_neon(state, dst, src);\r\nbytes -= CHACHA20_BLOCK_SIZE;\r\nsrc += CHACHA20_BLOCK_SIZE;\r\ndst += CHACHA20_BLOCK_SIZE;\r\nstate[12]++;\r\n}\r\nif (bytes) {\r\nmemcpy(buf, src, bytes);\r\nchacha20_block_xor_neon(state, buf, buf);\r\nmemcpy(dst, buf, bytes);\r\n}\r\n}\r\nstatic int chacha20_neon(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct chacha20_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nstruct skcipher_walk walk;\r\nu32 state[16];\r\nint err;\r\nif (req->cryptlen <= CHACHA20_BLOCK_SIZE || !may_use_simd())\r\nreturn crypto_chacha20_crypt(req);\r\nerr = skcipher_walk_virt(&walk, req, true);\r\ncrypto_chacha20_init(state, ctx, walk.iv);\r\nkernel_neon_begin();\r\nwhile (walk.nbytes > 0) {\r\nunsigned int nbytes = walk.nbytes;\r\nif (nbytes < walk.total)\r\nnbytes = round_down(nbytes, walk.stride);\r\nchacha20_doneon(state, walk.dst.virt.addr, walk.src.virt.addr,\r\nnbytes);\r\nerr = skcipher_walk_done(&walk, walk.nbytes - nbytes);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int __init chacha20_simd_mod_init(void)\r\n{\r\nif (!(elf_hwcap & HWCAP_NEON))\r\nreturn -ENODEV;\r\nreturn crypto_register_skcipher(&alg);\r\n}\r\nstatic void __exit chacha20_simd_mod_fini(void)\r\n{\r\ncrypto_unregister_skcipher(&alg);\r\n}
