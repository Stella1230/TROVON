u32 mega_mod64(u64 dividend, u32 divisor)\r\n{\r\nu64 d;\r\nu32 remainder;\r\nif (!divisor)\r\nprintk(KERN_ERR "megasas : DIVISOR is zero, in div fn\n");\r\nd = dividend;\r\nremainder = do_div(d, divisor);\r\nreturn remainder;\r\n}\r\nu64 mega_div64_32(uint64_t dividend, uint32_t divisor)\r\n{\r\nu32 remainder;\r\nu64 d;\r\nif (!divisor)\r\nprintk(KERN_ERR "megasas : DIVISOR is zero in mod fn\n");\r\nd = dividend;\r\nremainder = do_div(d, divisor);\r\nreturn d;\r\n}\r\nstruct MR_LD_RAID *MR_LdRaidGet(u32 ld, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn &map->raidMap.ldSpanMap[ld].ldRaid;\r\n}\r\nstatic struct MR_SPAN_BLOCK_INFO *MR_LdSpanInfoGet(u32 ld,\r\nstruct MR_DRV_RAID_MAP_ALL\r\n*map)\r\n{\r\nreturn &map->raidMap.ldSpanMap[ld].spanBlock[0];\r\n}\r\nstatic u8 MR_LdDataArmGet(u32 ld, u32 armIdx, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn map->raidMap.ldSpanMap[ld].dataArmMap[armIdx];\r\n}\r\nu16 MR_ArPdGet(u32 ar, u32 arm, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn le16_to_cpu(map->raidMap.arMapInfo[ar].pd[arm]);\r\n}\r\nu16 MR_LdSpanArrayGet(u32 ld, u32 span, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn le16_to_cpu(map->raidMap.ldSpanMap[ld].spanBlock[span].span.arrayRef);\r\n}\r\n__le16 MR_PdDevHandleGet(u32 pd, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn map->raidMap.devHndlInfo[pd].curDevHdl;\r\n}\r\nstatic u8 MR_PdInterfaceTypeGet(u32 pd, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn map->raidMap.devHndlInfo[pd].interfaceType;\r\n}\r\nu16 MR_GetLDTgtId(u32 ld, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn le16_to_cpu(map->raidMap.ldSpanMap[ld].ldRaid.targetId);\r\n}\r\nu16 MR_TargetIdToLdGet(u32 ldTgtId, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn map->raidMap.ldTgtIdToLd[ldTgtId];\r\n}\r\nstatic struct MR_LD_SPAN *MR_LdSpanPtrGet(u32 ld, u32 span,\r\nstruct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nreturn &map->raidMap.ldSpanMap[ld].spanBlock[span].span;\r\n}\r\nvoid MR_PopulateDrvRaidMap(struct megasas_instance *instance)\r\n{\r\nstruct fusion_context *fusion = instance->ctrl_context;\r\nstruct MR_FW_RAID_MAP_ALL *fw_map_old = NULL;\r\nstruct MR_FW_RAID_MAP *pFwRaidMap = NULL;\r\nint i, j;\r\nu16 ld_count;\r\nstruct MR_FW_RAID_MAP_DYNAMIC *fw_map_dyn;\r\nstruct MR_FW_RAID_MAP_EXT *fw_map_ext;\r\nstruct MR_RAID_MAP_DESC_TABLE *desc_table;\r\nstruct MR_DRV_RAID_MAP_ALL *drv_map =\r\nfusion->ld_drv_map[(instance->map_id & 1)];\r\nstruct MR_DRV_RAID_MAP *pDrvRaidMap = &drv_map->raidMap;\r\nvoid *raid_map_data = NULL;\r\nmemset(drv_map, 0, fusion->drv_map_sz);\r\nmemset(pDrvRaidMap->ldTgtIdToLd,\r\n0xff, (sizeof(u16) * MAX_LOGICAL_DRIVES_DYN));\r\nif (instance->max_raid_mapsize) {\r\nfw_map_dyn = fusion->ld_map[(instance->map_id & 1)];\r\ndesc_table =\r\n(struct MR_RAID_MAP_DESC_TABLE *)((void *)fw_map_dyn + le32_to_cpu(fw_map_dyn->desc_table_offset));\r\nif (desc_table != fw_map_dyn->raid_map_desc_table)\r\ndev_dbg(&instance->pdev->dev, "offsets of desc table are not matching desc %p original %p\n",\r\ndesc_table, fw_map_dyn->raid_map_desc_table);\r\nld_count = (u16)le16_to_cpu(fw_map_dyn->ld_count);\r\npDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);\r\npDrvRaidMap->fpPdIoTimeoutSec =\r\nfw_map_dyn->fp_pd_io_timeout_sec;\r\npDrvRaidMap->totalSize =\r\ncpu_to_le32(sizeof(struct MR_DRV_RAID_MAP_ALL));\r\nraid_map_data = (void *)fw_map_dyn +\r\nle32_to_cpu(fw_map_dyn->desc_table_offset) +\r\nle32_to_cpu(fw_map_dyn->desc_table_size);\r\nfor (i = 0; i < le32_to_cpu(fw_map_dyn->desc_table_num_elements); ++i) {\r\nswitch (le32_to_cpu(desc_table->raid_map_desc_type)) {\r\ncase RAID_MAP_DESC_TYPE_DEVHDL_INFO:\r\nfw_map_dyn->dev_hndl_info =\r\n(struct MR_DEV_HANDLE_INFO *)(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));\r\nmemcpy(pDrvRaidMap->devHndlInfo,\r\nfw_map_dyn->dev_hndl_info,\r\nsizeof(struct MR_DEV_HANDLE_INFO) *\r\nle32_to_cpu(desc_table->raid_map_desc_elements));\r\nbreak;\r\ncase RAID_MAP_DESC_TYPE_TGTID_INFO:\r\nfw_map_dyn->ld_tgt_id_to_ld =\r\n(u16 *)(raid_map_data +\r\nle32_to_cpu(desc_table->raid_map_desc_offset));\r\nfor (j = 0; j < le32_to_cpu(desc_table->raid_map_desc_elements); j++) {\r\npDrvRaidMap->ldTgtIdToLd[j] =\r\nle16_to_cpu(fw_map_dyn->ld_tgt_id_to_ld[j]);\r\n}\r\nbreak;\r\ncase RAID_MAP_DESC_TYPE_ARRAY_INFO:\r\nfw_map_dyn->ar_map_info =\r\n(struct MR_ARRAY_INFO *)\r\n(raid_map_data + le32_to_cpu(desc_table->raid_map_desc_offset));\r\nmemcpy(pDrvRaidMap->arMapInfo,\r\nfw_map_dyn->ar_map_info,\r\nsizeof(struct MR_ARRAY_INFO) *\r\nle32_to_cpu(desc_table->raid_map_desc_elements));\r\nbreak;\r\ncase RAID_MAP_DESC_TYPE_SPAN_INFO:\r\nfw_map_dyn->ld_span_map =\r\n(struct MR_LD_SPAN_MAP *)\r\n(raid_map_data +\r\nle32_to_cpu(desc_table->raid_map_desc_offset));\r\nmemcpy(pDrvRaidMap->ldSpanMap,\r\nfw_map_dyn->ld_span_map,\r\nsizeof(struct MR_LD_SPAN_MAP) *\r\nle32_to_cpu(desc_table->raid_map_desc_elements));\r\nbreak;\r\ndefault:\r\ndev_dbg(&instance->pdev->dev, "wrong number of desctableElements %d\n",\r\nfw_map_dyn->desc_table_num_elements);\r\n}\r\n++desc_table;\r\n}\r\n} else if (instance->supportmax256vd) {\r\nfw_map_ext =\r\n(struct MR_FW_RAID_MAP_EXT *)fusion->ld_map[(instance->map_id & 1)];\r\nld_count = (u16)le16_to_cpu(fw_map_ext->ldCount);\r\nif (ld_count > MAX_LOGICAL_DRIVES_EXT) {\r\ndev_dbg(&instance->pdev->dev, "megaraid_sas: LD count exposed in RAID map in not valid\n");\r\nreturn;\r\n}\r\npDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);\r\npDrvRaidMap->fpPdIoTimeoutSec = fw_map_ext->fpPdIoTimeoutSec;\r\nfor (i = 0; i < (MAX_LOGICAL_DRIVES_EXT); i++)\r\npDrvRaidMap->ldTgtIdToLd[i] =\r\n(u16)fw_map_ext->ldTgtIdToLd[i];\r\nmemcpy(pDrvRaidMap->ldSpanMap, fw_map_ext->ldSpanMap,\r\nsizeof(struct MR_LD_SPAN_MAP) * ld_count);\r\nmemcpy(pDrvRaidMap->arMapInfo, fw_map_ext->arMapInfo,\r\nsizeof(struct MR_ARRAY_INFO) * MAX_API_ARRAYS_EXT);\r\nmemcpy(pDrvRaidMap->devHndlInfo, fw_map_ext->devHndlInfo,\r\nsizeof(struct MR_DEV_HANDLE_INFO) *\r\nMAX_RAIDMAP_PHYSICAL_DEVICES);\r\npDrvRaidMap->totalSize =\r\ncpu_to_le32(sizeof(struct MR_FW_RAID_MAP_EXT));\r\n} else {\r\nfw_map_old = (struct MR_FW_RAID_MAP_ALL *)\r\nfusion->ld_map[(instance->map_id & 1)];\r\npFwRaidMap = &fw_map_old->raidMap;\r\nld_count = (u16)le32_to_cpu(pFwRaidMap->ldCount);\r\npDrvRaidMap->totalSize = pFwRaidMap->totalSize;\r\npDrvRaidMap->ldCount = (__le16)cpu_to_le16(ld_count);\r\npDrvRaidMap->fpPdIoTimeoutSec = pFwRaidMap->fpPdIoTimeoutSec;\r\nfor (i = 0; i < MAX_RAIDMAP_LOGICAL_DRIVES + MAX_RAIDMAP_VIEWS; i++)\r\npDrvRaidMap->ldTgtIdToLd[i] =\r\n(u8)pFwRaidMap->ldTgtIdToLd[i];\r\nfor (i = 0; i < ld_count; i++) {\r\npDrvRaidMap->ldSpanMap[i] = pFwRaidMap->ldSpanMap[i];\r\n}\r\nmemcpy(pDrvRaidMap->arMapInfo, pFwRaidMap->arMapInfo,\r\nsizeof(struct MR_ARRAY_INFO) * MAX_RAIDMAP_ARRAYS);\r\nmemcpy(pDrvRaidMap->devHndlInfo, pFwRaidMap->devHndlInfo,\r\nsizeof(struct MR_DEV_HANDLE_INFO) *\r\nMAX_RAIDMAP_PHYSICAL_DEVICES);\r\n}\r\n}\r\nu8 MR_ValidateMapInfo(struct megasas_instance *instance)\r\n{\r\nstruct fusion_context *fusion;\r\nstruct MR_DRV_RAID_MAP_ALL *drv_map;\r\nstruct MR_DRV_RAID_MAP *pDrvRaidMap;\r\nstruct LD_LOAD_BALANCE_INFO *lbInfo;\r\nPLD_SPAN_INFO ldSpanInfo;\r\nstruct MR_LD_RAID *raid;\r\nu16 num_lds, i;\r\nu16 ld;\r\nu32 expected_size;\r\nMR_PopulateDrvRaidMap(instance);\r\nfusion = instance->ctrl_context;\r\ndrv_map = fusion->ld_drv_map[(instance->map_id & 1)];\r\npDrvRaidMap = &drv_map->raidMap;\r\nlbInfo = fusion->load_balance_info;\r\nldSpanInfo = fusion->log_to_span;\r\nif (instance->max_raid_mapsize)\r\nexpected_size = sizeof(struct MR_DRV_RAID_MAP_ALL);\r\nelse if (instance->supportmax256vd)\r\nexpected_size = sizeof(struct MR_FW_RAID_MAP_EXT);\r\nelse\r\nexpected_size =\r\n(sizeof(struct MR_FW_RAID_MAP) - sizeof(struct MR_LD_SPAN_MAP) +\r\n(sizeof(struct MR_LD_SPAN_MAP) * le16_to_cpu(pDrvRaidMap->ldCount)));\r\nif (le32_to_cpu(pDrvRaidMap->totalSize) != expected_size) {\r\ndev_dbg(&instance->pdev->dev, "megasas: map info structure size 0x%x",\r\nle32_to_cpu(pDrvRaidMap->totalSize));\r\ndev_dbg(&instance->pdev->dev, "is not matching expected size 0x%x\n",\r\n(unsigned int)expected_size);\r\ndev_err(&instance->pdev->dev, "megasas: span map %x, pDrvRaidMap->totalSize : %x\n",\r\n(unsigned int)sizeof(struct MR_LD_SPAN_MAP),\r\nle32_to_cpu(pDrvRaidMap->totalSize));\r\nreturn 0;\r\n}\r\nif (instance->UnevenSpanSupport)\r\nmr_update_span_set(drv_map, ldSpanInfo);\r\nif (lbInfo)\r\nmr_update_load_balance_params(drv_map, lbInfo);\r\nnum_lds = le16_to_cpu(drv_map->raidMap.ldCount);\r\nfor (i = 0; (num_lds > 0) && (i < MAX_LOGICAL_DRIVES_EXT); i++) {\r\nld = MR_TargetIdToLdGet(i, drv_map);\r\nif (ld >= (MAX_LOGICAL_DRIVES_EXT - 1))\r\ncontinue;\r\nraid = MR_LdRaidGet(ld, drv_map);\r\nle32_to_cpus((u32 *)&raid->capability);\r\nnum_lds--;\r\n}\r\nreturn 1;\r\n}\r\nu32 MR_GetSpanBlock(u32 ld, u64 row, u64 *span_blk,\r\nstruct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nstruct MR_SPAN_BLOCK_INFO *pSpanBlock = MR_LdSpanInfoGet(ld, map);\r\nstruct MR_QUAD_ELEMENT *quad;\r\nstruct MR_LD_RAID *raid = MR_LdRaidGet(ld, map);\r\nu32 span, j;\r\nfor (span = 0; span < raid->spanDepth; span++, pSpanBlock++) {\r\nfor (j = 0; j < le32_to_cpu(pSpanBlock->block_span_info.noElements); j++) {\r\nquad = &pSpanBlock->block_span_info.quad[j];\r\nif (le32_to_cpu(quad->diff) == 0)\r\nreturn SPAN_INVALID;\r\nif (le64_to_cpu(quad->logStart) <= row && row <=\r\nle64_to_cpu(quad->logEnd) && (mega_mod64(row - le64_to_cpu(quad->logStart),\r\nle32_to_cpu(quad->diff))) == 0) {\r\nif (span_blk != NULL) {\r\nu64 blk, debugBlk;\r\nblk = mega_div64_32((row-le64_to_cpu(quad->logStart)), le32_to_cpu(quad->diff));\r\ndebugBlk = blk;\r\nblk = (blk + le64_to_cpu(quad->offsetInSpan)) << raid->stripeShift;\r\n*span_blk = blk;\r\n}\r\nreturn span;\r\n}\r\n}\r\n}\r\nreturn SPAN_INVALID;\r\n}\r\nu32 mr_spanset_get_span_block(struct megasas_instance *instance,\r\nu32 ld, u64 row, u64 *span_blk, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nstruct fusion_context *fusion = instance->ctrl_context;\r\nstruct MR_LD_RAID *raid = MR_LdRaidGet(ld, map);\r\nLD_SPAN_SET *span_set;\r\nstruct MR_QUAD_ELEMENT *quad;\r\nu32 span, info;\r\nPLD_SPAN_INFO ldSpanInfo = fusion->log_to_span;\r\nfor (info = 0; info < MAX_QUAD_DEPTH; info++) {\r\nspan_set = &(ldSpanInfo[ld].span_set[info]);\r\nif (span_set->span_row_data_width == 0)\r\nbreak;\r\nif (row > span_set->data_row_end)\r\ncontinue;\r\nfor (span = 0; span < raid->spanDepth; span++)\r\nif (le32_to_cpu(map->raidMap.ldSpanMap[ld].spanBlock[span].\r\nblock_span_info.noElements) >= info+1) {\r\nquad = &map->raidMap.ldSpanMap[ld].\r\nspanBlock[span].\r\nblock_span_info.quad[info];\r\nif (le32_to_cpu(quad->diff) == 0)\r\nreturn SPAN_INVALID;\r\nif (le64_to_cpu(quad->logStart) <= row &&\r\nrow <= le64_to_cpu(quad->logEnd) &&\r\n(mega_mod64(row - le64_to_cpu(quad->logStart),\r\nle32_to_cpu(quad->diff))) == 0) {\r\nif (span_blk != NULL) {\r\nu64 blk;\r\nblk = mega_div64_32\r\n((row - le64_to_cpu(quad->logStart)),\r\nle32_to_cpu(quad->diff));\r\nblk = (blk + le64_to_cpu(quad->offsetInSpan))\r\n<< raid->stripeShift;\r\n*span_blk = blk;\r\n}\r\nreturn span;\r\n}\r\n}\r\n}\r\nreturn SPAN_INVALID;\r\n}\r\nstatic u64 get_row_from_strip(struct megasas_instance *instance,\r\nu32 ld, u64 strip, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nstruct fusion_context *fusion = instance->ctrl_context;\r\nstruct MR_LD_RAID *raid = MR_LdRaidGet(ld, map);\r\nLD_SPAN_SET *span_set;\r\nPLD_SPAN_INFO ldSpanInfo = fusion->log_to_span;\r\nu32 info, strip_offset, span, span_offset;\r\nu64 span_set_Strip, span_set_Row, retval;\r\nfor (info = 0; info < MAX_QUAD_DEPTH; info++) {\r\nspan_set = &(ldSpanInfo[ld].span_set[info]);\r\nif (span_set->span_row_data_width == 0)\r\nbreak;\r\nif (strip > span_set->data_strip_end)\r\ncontinue;\r\nspan_set_Strip = strip - span_set->data_strip_start;\r\nstrip_offset = mega_mod64(span_set_Strip,\r\nspan_set->span_row_data_width);\r\nspan_set_Row = mega_div64_32(span_set_Strip,\r\nspan_set->span_row_data_width) * span_set->diff;\r\nfor (span = 0, span_offset = 0; span < raid->spanDepth; span++)\r\nif (le32_to_cpu(map->raidMap.ldSpanMap[ld].spanBlock[span].\r\nblock_span_info.noElements) >= info+1) {\r\nif (strip_offset >=\r\nspan_set->strip_offset[span])\r\nspan_offset++;\r\nelse\r\nbreak;\r\n}\r\nretval = (span_set->data_row_start + span_set_Row +\r\n(span_offset - 1));\r\nreturn retval;\r\n}\r\nreturn -1LLU;\r\n}\r\nstatic u64 get_strip_from_row(struct megasas_instance *instance,\r\nu32 ld, u64 row, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nstruct fusion_context *fusion = instance->ctrl_context;\r\nstruct MR_LD_RAID *raid = MR_LdRaidGet(ld, map);\r\nLD_SPAN_SET *span_set;\r\nstruct MR_QUAD_ELEMENT *quad;\r\nPLD_SPAN_INFO ldSpanInfo = fusion->log_to_span;\r\nu32 span, info;\r\nu64 strip;\r\nfor (info = 0; info < MAX_QUAD_DEPTH; info++) {\r\nspan_set = &(ldSpanInfo[ld].span_set[info]);\r\nif (span_set->span_row_data_width == 0)\r\nbreak;\r\nif (row > span_set->data_row_end)\r\ncontinue;\r\nfor (span = 0; span < raid->spanDepth; span++)\r\nif (le32_to_cpu(map->raidMap.ldSpanMap[ld].spanBlock[span].\r\nblock_span_info.noElements) >= info+1) {\r\nquad = &map->raidMap.ldSpanMap[ld].\r\nspanBlock[span].block_span_info.quad[info];\r\nif (le64_to_cpu(quad->logStart) <= row &&\r\nrow <= le64_to_cpu(quad->logEnd) &&\r\nmega_mod64((row - le64_to_cpu(quad->logStart)),\r\nle32_to_cpu(quad->diff)) == 0) {\r\nstrip = mega_div64_32\r\n(((row - span_set->data_row_start)\r\n- le64_to_cpu(quad->logStart)),\r\nle32_to_cpu(quad->diff));\r\nstrip *= span_set->span_row_data_width;\r\nstrip += span_set->data_strip_start;\r\nstrip += span_set->strip_offset[span];\r\nreturn strip;\r\n}\r\n}\r\n}\r\ndev_err(&instance->pdev->dev, "get_strip_from_row"\r\n"returns invalid strip for ld=%x, row=%lx\n",\r\nld, (long unsigned int)row);\r\nreturn -1;\r\n}\r\nstatic u32 get_arm_from_strip(struct megasas_instance *instance,\r\nu32 ld, u64 strip, struct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nstruct fusion_context *fusion = instance->ctrl_context;\r\nstruct MR_LD_RAID *raid = MR_LdRaidGet(ld, map);\r\nLD_SPAN_SET *span_set;\r\nPLD_SPAN_INFO ldSpanInfo = fusion->log_to_span;\r\nu32 info, strip_offset, span, span_offset, retval;\r\nfor (info = 0 ; info < MAX_QUAD_DEPTH; info++) {\r\nspan_set = &(ldSpanInfo[ld].span_set[info]);\r\nif (span_set->span_row_data_width == 0)\r\nbreak;\r\nif (strip > span_set->data_strip_end)\r\ncontinue;\r\nstrip_offset = (uint)mega_mod64\r\n((strip - span_set->data_strip_start),\r\nspan_set->span_row_data_width);\r\nfor (span = 0, span_offset = 0; span < raid->spanDepth; span++)\r\nif (le32_to_cpu(map->raidMap.ldSpanMap[ld].spanBlock[span].\r\nblock_span_info.noElements) >= info+1) {\r\nif (strip_offset >=\r\nspan_set->strip_offset[span])\r\nspan_offset =\r\nspan_set->strip_offset[span];\r\nelse\r\nbreak;\r\n}\r\nretval = (strip_offset - span_offset);\r\nreturn retval;\r\n}\r\ndev_err(&instance->pdev->dev, "get_arm_from_strip"\r\n"returns invalid arm for ld=%x strip=%lx\n",\r\nld, (long unsigned int)strip);\r\nreturn -1;\r\n}\r\nu8 get_arm(struct megasas_instance *instance, u32 ld, u8 span, u64 stripe,\r\nstruct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nstruct MR_LD_RAID *raid = MR_LdRaidGet(ld, map);\r\nu32 arm = 0;\r\nswitch (raid->level) {\r\ncase 0:\r\ncase 5:\r\ncase 6:\r\narm = mega_mod64(stripe, SPAN_ROW_SIZE(map, ld, span));\r\nbreak;\r\ncase 1:\r\narm = get_arm_from_strip(instance, ld, stripe, map);\r\nif (arm != -1U)\r\narm *= 2;\r\nbreak;\r\n}\r\nreturn arm;\r\n}\r\nstatic u8 mr_spanset_get_phy_params(struct megasas_instance *instance, u32 ld,\r\nu64 stripRow, u16 stripRef, struct IO_REQUEST_INFO *io_info,\r\nstruct RAID_CONTEXT *pRAID_Context,\r\nstruct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nstruct MR_LD_RAID *raid = MR_LdRaidGet(ld, map);\r\nu32 pd, arRef, r1_alt_pd;\r\nu8 physArm, span;\r\nu64 row;\r\nu8 retval = TRUE;\r\nu64 *pdBlock = &io_info->pdBlock;\r\n__le16 *pDevHandle = &io_info->devHandle;\r\nu8 *pPdInterface = &io_info->pd_interface;\r\nu32 logArm, rowMod, armQ, arm;\r\nstruct fusion_context *fusion;\r\nfusion = instance->ctrl_context;\r\n*pDevHandle = cpu_to_le16(MR_DEVHANDLE_INVALID);\r\nrow = io_info->start_row;\r\nspan = io_info->start_span;\r\nif (raid->level == 6) {\r\nlogArm = get_arm_from_strip(instance, ld, stripRow, map);\r\nif (logArm == -1U)\r\nreturn FALSE;\r\nrowMod = mega_mod64(row, SPAN_ROW_SIZE(map, ld, span));\r\narmQ = SPAN_ROW_SIZE(map, ld, span) - 1 - rowMod;\r\narm = armQ + 1 + logArm;\r\nif (arm >= SPAN_ROW_SIZE(map, ld, span))\r\narm -= SPAN_ROW_SIZE(map, ld, span);\r\nphysArm = (u8)arm;\r\n} else\r\nphysArm = get_arm(instance, ld, span, stripRow, map);\r\nif (physArm == 0xFF)\r\nreturn FALSE;\r\narRef = MR_LdSpanArrayGet(ld, span, map);\r\npd = MR_ArPdGet(arRef, physArm, map);\r\nif (pd != MR_PD_INVALID) {\r\n*pDevHandle = MR_PdDevHandleGet(pd, map);\r\n*pPdInterface = MR_PdInterfaceTypeGet(pd, map);\r\nif (instance->is_ventura &&\r\n(raid->level == 1) &&\r\n!io_info->isRead) {\r\nr1_alt_pd = MR_ArPdGet(arRef, physArm + 1, map);\r\nif (r1_alt_pd != MR_PD_INVALID)\r\nio_info->r1_alt_dev_handle =\r\nMR_PdDevHandleGet(r1_alt_pd, map);\r\n}\r\n} else {\r\nif ((raid->level >= 5) &&\r\n((fusion->adapter_type == THUNDERBOLT_SERIES) ||\r\n((fusion->adapter_type == INVADER_SERIES) &&\r\n(raid->regTypeReqOnRead != REGION_TYPE_UNUSED))))\r\npRAID_Context->reg_lock_flags = REGION_TYPE_EXCLUSIVE;\r\nelse if (raid->level == 1) {\r\nphysArm = physArm + 1;\r\npd = MR_ArPdGet(arRef, physArm, map);\r\nif (pd != MR_PD_INVALID) {\r\n*pDevHandle = MR_PdDevHandleGet(pd, map);\r\n*pPdInterface = MR_PdInterfaceTypeGet(pd, map);\r\n}\r\n}\r\n}\r\n*pdBlock += stripRef + le64_to_cpu(MR_LdSpanPtrGet(ld, span, map)->startBlk);\r\nif (instance->is_ventura) {\r\n((struct RAID_CONTEXT_G35 *)pRAID_Context)->span_arm =\r\n(span << RAID_CTX_SPANARM_SPAN_SHIFT) | physArm;\r\nio_info->span_arm =\r\n(span << RAID_CTX_SPANARM_SPAN_SHIFT) | physArm;\r\n} else {\r\npRAID_Context->span_arm =\r\n(span << RAID_CTX_SPANARM_SPAN_SHIFT) | physArm;\r\nio_info->span_arm = pRAID_Context->span_arm;\r\n}\r\nio_info->pd_after_lb = pd;\r\nreturn retval;\r\n}\r\nu8 MR_GetPhyParams(struct megasas_instance *instance, u32 ld, u64 stripRow,\r\nu16 stripRef, struct IO_REQUEST_INFO *io_info,\r\nstruct RAID_CONTEXT *pRAID_Context,\r\nstruct MR_DRV_RAID_MAP_ALL *map)\r\n{\r\nstruct MR_LD_RAID *raid = MR_LdRaidGet(ld, map);\r\nu32 pd, arRef, r1_alt_pd;\r\nu8 physArm, span;\r\nu64 row;\r\nu8 retval = TRUE;\r\nu64 *pdBlock = &io_info->pdBlock;\r\n__le16 *pDevHandle = &io_info->devHandle;\r\nu8 *pPdInterface = &io_info->pd_interface;\r\nstruct fusion_context *fusion;\r\nfusion = instance->ctrl_context;\r\n*pDevHandle = cpu_to_le16(MR_DEVHANDLE_INVALID);\r\nrow = mega_div64_32(stripRow, raid->rowDataSize);\r\nif (raid->level == 6) {\r\nu32 logArm = mega_mod64(stripRow, raid->rowDataSize);\r\nu32 rowMod, armQ, arm;\r\nif (raid->rowSize == 0)\r\nreturn FALSE;\r\nrowMod = mega_mod64(row, raid->rowSize);\r\narmQ = raid->rowSize-1-rowMod;\r\narm = armQ+1+logArm;\r\nif (arm >= raid->rowSize)\r\narm -= raid->rowSize;\r\nphysArm = (u8)arm;\r\n} else {\r\nif (raid->modFactor == 0)\r\nreturn FALSE;\r\nphysArm = MR_LdDataArmGet(ld, mega_mod64(stripRow,\r\nraid->modFactor),\r\nmap);\r\n}\r\nif (raid->spanDepth == 1) {\r\nspan = 0;\r\n*pdBlock = row << raid->stripeShift;\r\n} else {\r\nspan = (u8)MR_GetSpanBlock(ld, row, pdBlock, map);\r\nif (span == SPAN_INVALID)\r\nreturn FALSE;\r\n}\r\narRef = MR_LdSpanArrayGet(ld, span, map);\r\npd = MR_ArPdGet(arRef, physArm, map);\r\nif (pd != MR_PD_INVALID) {\r\n*pDevHandle = MR_PdDevHandleGet(pd, map);\r\n*pPdInterface = MR_PdInterfaceTypeGet(pd, map);\r\nif (instance->is_ventura &&\r\n(raid->level == 1) &&\r\n!io_info->isRead) {\r\nr1_alt_pd = MR_ArPdGet(arRef, physArm + 1, map);\r\nif (r1_alt_pd != MR_PD_INVALID)\r\nio_info->r1_alt_dev_handle =\r\nMR_PdDevHandleGet(r1_alt_pd, map);\r\n}\r\n} else {\r\nif ((raid->level >= 5) &&\r\n((fusion->adapter_type == THUNDERBOLT_SERIES) ||\r\n((fusion->adapter_type == INVADER_SERIES) &&\r\n(raid->regTypeReqOnRead != REGION_TYPE_UNUSED))))\r\npRAID_Context->reg_lock_flags = REGION_TYPE_EXCLUSIVE;\r\nelse if (raid->level == 1) {\r\nphysArm = physArm + 1;\r\npd = MR_ArPdGet(arRef, physArm, map);\r\nif (pd != MR_PD_INVALID) {\r\n*pDevHandle = MR_PdDevHandleGet(pd, map);\r\n*pPdInterface = MR_PdInterfaceTypeGet(pd, map);\r\n}\r\n}\r\n}\r\n*pdBlock += stripRef + le64_to_cpu(MR_LdSpanPtrGet(ld, span, map)->startBlk);\r\nif (instance->is_ventura) {\r\n((struct RAID_CONTEXT_G35 *)pRAID_Context)->span_arm =\r\n(span << RAID_CTX_SPANARM_SPAN_SHIFT) | physArm;\r\nio_info->span_arm =\r\n(span << RAID_CTX_SPANARM_SPAN_SHIFT) | physArm;\r\n} else {\r\npRAID_Context->span_arm =\r\n(span << RAID_CTX_SPANARM_SPAN_SHIFT) | physArm;\r\nio_info->span_arm = pRAID_Context->span_arm;\r\n}\r\nio_info->pd_after_lb = pd;\r\nreturn retval;\r\n}\r\nu8\r\nMR_BuildRaidContext(struct megasas_instance *instance,\r\nstruct IO_REQUEST_INFO *io_info,\r\nstruct RAID_CONTEXT *pRAID_Context,\r\nstruct MR_DRV_RAID_MAP_ALL *map, u8 **raidLUN)\r\n{\r\nstruct fusion_context *fusion;\r\nstruct MR_LD_RAID *raid;\r\nu32 stripSize, stripe_mask;\r\nu64 endLba, endStrip, endRow, start_row, start_strip;\r\nu64 regStart;\r\nu32 regSize;\r\nu8 num_strips, numRows;\r\nu16 ref_in_start_stripe, ref_in_end_stripe;\r\nu64 ldStartBlock;\r\nu32 numBlocks, ldTgtId;\r\nu8 isRead;\r\nu8 retval = 0;\r\nu8 startlba_span = SPAN_INVALID;\r\nu64 *pdBlock = &io_info->pdBlock;\r\nu16 ld;\r\nldStartBlock = io_info->ldStartBlock;\r\nnumBlocks = io_info->numBlocks;\r\nldTgtId = io_info->ldTgtId;\r\nisRead = io_info->isRead;\r\nio_info->IoforUnevenSpan = 0;\r\nio_info->start_span = SPAN_INVALID;\r\nfusion = instance->ctrl_context;\r\nld = MR_TargetIdToLdGet(ldTgtId, map);\r\nraid = MR_LdRaidGet(ld, map);\r\nio_info->ra_capable = raid->capability.ra_capable;\r\nif (raid->rowDataSize == 0) {\r\nif (MR_LdSpanPtrGet(ld, 0, map)->spanRowDataSize == 0)\r\nreturn FALSE;\r\nelse if (instance->UnevenSpanSupport) {\r\nio_info->IoforUnevenSpan = 1;\r\n} else {\r\ndev_info(&instance->pdev->dev,\r\n"raid->rowDataSize is 0, but has SPAN[0]"\r\n"rowDataSize = 0x%0x,"\r\n"but there is _NO_ UnevenSpanSupport\n",\r\nMR_LdSpanPtrGet(ld, 0, map)->spanRowDataSize);\r\nreturn FALSE;\r\n}\r\n}\r\nstripSize = 1 << raid->stripeShift;\r\nstripe_mask = stripSize-1;\r\nstart_strip = ldStartBlock >> raid->stripeShift;\r\nref_in_start_stripe = (u16)(ldStartBlock & stripe_mask);\r\nendLba = ldStartBlock + numBlocks - 1;\r\nref_in_end_stripe = (u16)(endLba & stripe_mask);\r\nendStrip = endLba >> raid->stripeShift;\r\nnum_strips = (u8)(endStrip - start_strip + 1);\r\nif (io_info->IoforUnevenSpan) {\r\nstart_row = get_row_from_strip(instance, ld, start_strip, map);\r\nendRow = get_row_from_strip(instance, ld, endStrip, map);\r\nif (start_row == -1ULL || endRow == -1ULL) {\r\ndev_info(&instance->pdev->dev, "return from %s %d."\r\n"Send IO w/o region lock.\n",\r\n__func__, __LINE__);\r\nreturn FALSE;\r\n}\r\nif (raid->spanDepth == 1) {\r\nstartlba_span = 0;\r\n*pdBlock = start_row << raid->stripeShift;\r\n} else\r\nstartlba_span = (u8)mr_spanset_get_span_block(instance,\r\nld, start_row, pdBlock, map);\r\nif (startlba_span == SPAN_INVALID) {\r\ndev_info(&instance->pdev->dev, "return from %s %d"\r\n"for row 0x%llx,start strip %llx"\r\n"endSrip %llx\n", __func__, __LINE__,\r\n(unsigned long long)start_row,\r\n(unsigned long long)start_strip,\r\n(unsigned long long)endStrip);\r\nreturn FALSE;\r\n}\r\nio_info->start_span = startlba_span;\r\nio_info->start_row = start_row;\r\n} else {\r\nstart_row = mega_div64_32(start_strip, raid->rowDataSize);\r\nendRow = mega_div64_32(endStrip, raid->rowDataSize);\r\n}\r\nnumRows = (u8)(endRow - start_row + 1);\r\nregStart = start_row << raid->stripeShift;\r\nregSize = stripSize;\r\nio_info->do_fp_rlbypass = raid->capability.fpBypassRegionLock;\r\nif (raid->capability.fpCapable) {\r\nif (isRead)\r\nio_info->fpOkForIo = (raid->capability.fpReadCapable &&\r\n((num_strips == 1) ||\r\nraid->capability.\r\nfpReadAcrossStripe));\r\nelse\r\nio_info->fpOkForIo = (raid->capability.fpWriteCapable &&\r\n((num_strips == 1) ||\r\nraid->capability.\r\nfpWriteAcrossStripe));\r\n} else\r\nio_info->fpOkForIo = FALSE;\r\nif (numRows == 1) {\r\nif (num_strips == 1) {\r\nregStart += ref_in_start_stripe;\r\nregSize = numBlocks;\r\n}\r\n} else if (io_info->IoforUnevenSpan == 0) {\r\nif (start_strip == (start_row + 1) * raid->rowDataSize - 1) {\r\nregStart += ref_in_start_stripe;\r\nregSize = stripSize - ref_in_start_stripe;\r\n}\r\nif (numRows > 2)\r\nregSize += (numRows-2) << raid->stripeShift;\r\nif (endStrip == endRow*raid->rowDataSize)\r\nregSize += ref_in_end_stripe+1;\r\nelse\r\nregSize += stripSize;\r\n} else {\r\nif (start_strip == (get_strip_from_row(instance, ld, start_row, map) +\r\nSPAN_ROW_DATA_SIZE(map, ld, startlba_span) - 1)) {\r\nregStart += ref_in_start_stripe;\r\nregSize = stripSize - ref_in_start_stripe;\r\n}\r\nif (numRows > 2)\r\nregSize += (numRows-2) << raid->stripeShift;\r\nif (endStrip == get_strip_from_row(instance, ld, endRow, map))\r\nregSize += ref_in_end_stripe + 1;\r\nelse\r\nregSize += stripSize;\r\n}\r\npRAID_Context->timeout_value =\r\ncpu_to_le16(raid->fpIoTimeoutForLd ?\r\nraid->fpIoTimeoutForLd :\r\nmap->raidMap.fpPdIoTimeoutSec);\r\nif (fusion->adapter_type == INVADER_SERIES)\r\npRAID_Context->reg_lock_flags = (isRead) ?\r\nraid->regTypeReqOnRead : raid->regTypeReqOnWrite;\r\nelse if (!instance->is_ventura)\r\npRAID_Context->reg_lock_flags = (isRead) ?\r\nREGION_TYPE_SHARED_READ : raid->regTypeReqOnWrite;\r\npRAID_Context->virtual_disk_tgt_id = raid->targetId;\r\npRAID_Context->reg_lock_row_lba = cpu_to_le64(regStart);\r\npRAID_Context->reg_lock_length = cpu_to_le32(regSize);\r\npRAID_Context->config_seq_num = raid->seqNum;\r\n*raidLUN = raid->LUN;\r\nif (io_info->fpOkForIo) {\r\nretval = io_info->IoforUnevenSpan ?\r\nmr_spanset_get_phy_params(instance, ld,\r\nstart_strip, ref_in_start_stripe,\r\nio_info, pRAID_Context, map) :\r\nMR_GetPhyParams(instance, ld, start_strip,\r\nref_in_start_stripe, io_info,\r\npRAID_Context, map);\r\nif (io_info->devHandle == MR_DEVHANDLE_INVALID)\r\nio_info->fpOkForIo = FALSE;\r\nreturn retval;\r\n} else if (isRead) {\r\nuint stripIdx;\r\nfor (stripIdx = 0; stripIdx < num_strips; stripIdx++) {\r\nretval = io_info->IoforUnevenSpan ?\r\nmr_spanset_get_phy_params(instance, ld,\r\nstart_strip + stripIdx,\r\nref_in_start_stripe, io_info,\r\npRAID_Context, map) :\r\nMR_GetPhyParams(instance, ld,\r\nstart_strip + stripIdx, ref_in_start_stripe,\r\nio_info, pRAID_Context, map);\r\nif (!retval)\r\nreturn TRUE;\r\n}\r\n}\r\nreturn TRUE;\r\n}\r\nvoid mr_update_span_set(struct MR_DRV_RAID_MAP_ALL *map,\r\nPLD_SPAN_INFO ldSpanInfo)\r\n{\r\nu8 span, count;\r\nu32 element, span_row_width;\r\nu64 span_row;\r\nstruct MR_LD_RAID *raid;\r\nLD_SPAN_SET *span_set, *span_set_prev;\r\nstruct MR_QUAD_ELEMENT *quad;\r\nint ldCount;\r\nu16 ld;\r\nfor (ldCount = 0; ldCount < MAX_LOGICAL_DRIVES_EXT; ldCount++) {\r\nld = MR_TargetIdToLdGet(ldCount, map);\r\nif (ld >= (MAX_LOGICAL_DRIVES_EXT - 1))\r\ncontinue;\r\nraid = MR_LdRaidGet(ld, map);\r\nfor (element = 0; element < MAX_QUAD_DEPTH; element++) {\r\nfor (span = 0; span < raid->spanDepth; span++) {\r\nif (le32_to_cpu(map->raidMap.ldSpanMap[ld].spanBlock[span].\r\nblock_span_info.noElements) <\r\nelement + 1)\r\ncontinue;\r\nspan_set = &(ldSpanInfo[ld].span_set[element]);\r\nquad = &map->raidMap.ldSpanMap[ld].\r\nspanBlock[span].block_span_info.\r\nquad[element];\r\nspan_set->diff = le32_to_cpu(quad->diff);\r\nfor (count = 0, span_row_width = 0;\r\ncount < raid->spanDepth; count++) {\r\nif (le32_to_cpu(map->raidMap.ldSpanMap[ld].\r\nspanBlock[count].\r\nblock_span_info.\r\nnoElements) >= element + 1) {\r\nspan_set->strip_offset[count] =\r\nspan_row_width;\r\nspan_row_width +=\r\nMR_LdSpanPtrGet\r\n(ld, count, map)->spanRowDataSize;\r\n}\r\n}\r\nspan_set->span_row_data_width = span_row_width;\r\nspan_row = mega_div64_32(((le64_to_cpu(quad->logEnd) -\r\nle64_to_cpu(quad->logStart)) + le32_to_cpu(quad->diff)),\r\nle32_to_cpu(quad->diff));\r\nif (element == 0) {\r\nspan_set->log_start_lba = 0;\r\nspan_set->log_end_lba =\r\n((span_row << raid->stripeShift)\r\n* span_row_width) - 1;\r\nspan_set->span_row_start = 0;\r\nspan_set->span_row_end = span_row - 1;\r\nspan_set->data_strip_start = 0;\r\nspan_set->data_strip_end =\r\n(span_row * span_row_width) - 1;\r\nspan_set->data_row_start = 0;\r\nspan_set->data_row_end =\r\n(span_row * le32_to_cpu(quad->diff)) - 1;\r\n} else {\r\nspan_set_prev = &(ldSpanInfo[ld].\r\nspan_set[element - 1]);\r\nspan_set->log_start_lba =\r\nspan_set_prev->log_end_lba + 1;\r\nspan_set->log_end_lba =\r\nspan_set->log_start_lba +\r\n((span_row << raid->stripeShift)\r\n* span_row_width) - 1;\r\nspan_set->span_row_start =\r\nspan_set_prev->span_row_end + 1;\r\nspan_set->span_row_end =\r\nspan_set->span_row_start + span_row - 1;\r\nspan_set->data_strip_start =\r\nspan_set_prev->data_strip_end + 1;\r\nspan_set->data_strip_end =\r\nspan_set->data_strip_start +\r\n(span_row * span_row_width) - 1;\r\nspan_set->data_row_start =\r\nspan_set_prev->data_row_end + 1;\r\nspan_set->data_row_end =\r\nspan_set->data_row_start +\r\n(span_row * le32_to_cpu(quad->diff)) - 1;\r\n}\r\nbreak;\r\n}\r\nif (span == raid->spanDepth)\r\nbreak;\r\n}\r\n}\r\n}\r\nvoid mr_update_load_balance_params(struct MR_DRV_RAID_MAP_ALL *drv_map,\r\nstruct LD_LOAD_BALANCE_INFO *lbInfo)\r\n{\r\nint ldCount;\r\nu16 ld;\r\nstruct MR_LD_RAID *raid;\r\nif (lb_pending_cmds > 128 || lb_pending_cmds < 1)\r\nlb_pending_cmds = LB_PENDING_CMDS_DEFAULT;\r\nfor (ldCount = 0; ldCount < MAX_LOGICAL_DRIVES_EXT; ldCount++) {\r\nld = MR_TargetIdToLdGet(ldCount, drv_map);\r\nif (ld >= MAX_LOGICAL_DRIVES_EXT) {\r\nlbInfo[ldCount].loadBalanceFlag = 0;\r\ncontinue;\r\n}\r\nraid = MR_LdRaidGet(ld, drv_map);\r\nif ((raid->level != 1) ||\r\n(raid->ldState != MR_LD_STATE_OPTIMAL)) {\r\nlbInfo[ldCount].loadBalanceFlag = 0;\r\ncontinue;\r\n}\r\nlbInfo[ldCount].loadBalanceFlag = 1;\r\n}\r\n}\r\nu8 megasas_get_best_arm_pd(struct megasas_instance *instance,\r\nstruct LD_LOAD_BALANCE_INFO *lbInfo,\r\nstruct IO_REQUEST_INFO *io_info,\r\nstruct MR_DRV_RAID_MAP_ALL *drv_map)\r\n{\r\nstruct MR_LD_RAID *raid;\r\nu16 pd1_dev_handle;\r\nu16 pend0, pend1, ld;\r\nu64 diff0, diff1;\r\nu8 bestArm, pd0, pd1, span, arm;\r\nu32 arRef, span_row_size;\r\nu64 block = io_info->ldStartBlock;\r\nu32 count = io_info->numBlocks;\r\nspan = ((io_info->span_arm & RAID_CTX_SPANARM_SPAN_MASK)\r\n>> RAID_CTX_SPANARM_SPAN_SHIFT);\r\narm = (io_info->span_arm & RAID_CTX_SPANARM_ARM_MASK);\r\nld = MR_TargetIdToLdGet(io_info->ldTgtId, drv_map);\r\nraid = MR_LdRaidGet(ld, drv_map);\r\nspan_row_size = instance->UnevenSpanSupport ?\r\nSPAN_ROW_SIZE(drv_map, ld, span) : raid->rowSize;\r\narRef = MR_LdSpanArrayGet(ld, span, drv_map);\r\npd0 = MR_ArPdGet(arRef, arm, drv_map);\r\npd1 = MR_ArPdGet(arRef, (arm + 1) >= span_row_size ?\r\n(arm + 1 - span_row_size) : arm + 1, drv_map);\r\npd1_dev_handle = MR_PdDevHandleGet(pd1, drv_map);\r\nif (pd1_dev_handle == MR_DEVHANDLE_INVALID) {\r\nbestArm = arm;\r\n} else {\r\npend0 = atomic_read(&lbInfo->scsi_pending_cmds[pd0]);\r\npend1 = atomic_read(&lbInfo->scsi_pending_cmds[pd1]);\r\ndiff0 = ABS_DIFF(block, lbInfo->last_accessed_block[pd0]);\r\ndiff1 = ABS_DIFF(block, lbInfo->last_accessed_block[pd1]);\r\nbestArm = (diff0 <= diff1 ? arm : arm ^ 1);\r\nif ((bestArm == arm && pend0 > pend1 + lb_pending_cmds) ||\r\n(bestArm != arm && pend1 > pend0 + lb_pending_cmds))\r\nbestArm ^= 1;\r\nio_info->span_arm =\r\n(span << RAID_CTX_SPANARM_SPAN_SHIFT) | bestArm;\r\nio_info->pd_after_lb = (bestArm == arm) ? pd0 : pd1;\r\n}\r\nlbInfo->last_accessed_block[io_info->pd_after_lb] = block + count - 1;\r\nreturn io_info->pd_after_lb;\r\n}\r\n__le16 get_updated_dev_handle(struct megasas_instance *instance,\r\nstruct LD_LOAD_BALANCE_INFO *lbInfo,\r\nstruct IO_REQUEST_INFO *io_info,\r\nstruct MR_DRV_RAID_MAP_ALL *drv_map)\r\n{\r\nu8 arm_pd;\r\n__le16 devHandle;\r\narm_pd = megasas_get_best_arm_pd(instance, lbInfo, io_info, drv_map);\r\ndevHandle = MR_PdDevHandleGet(arm_pd, drv_map);\r\nio_info->pd_interface = MR_PdInterfaceTypeGet(arm_pd, drv_map);\r\natomic_inc(&lbInfo->scsi_pending_cmds[arm_pd]);\r\nreturn devHandle;\r\n}
