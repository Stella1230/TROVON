static inline void switch_buf(struct caam_hash_state *state)\r\n{\r\nstate->current_buf ^= 1;\r\n}\r\nstatic inline u8 *current_buf(struct caam_hash_state *state)\r\n{\r\nreturn state->current_buf ? state->buf_1 : state->buf_0;\r\n}\r\nstatic inline u8 *alt_buf(struct caam_hash_state *state)\r\n{\r\nreturn state->current_buf ? state->buf_0 : state->buf_1;\r\n}\r\nstatic inline int *current_buflen(struct caam_hash_state *state)\r\n{\r\nreturn state->current_buf ? &state->buflen_1 : &state->buflen_0;\r\n}\r\nstatic inline int *alt_buflen(struct caam_hash_state *state)\r\n{\r\nreturn state->current_buf ? &state->buflen_0 : &state->buflen_1;\r\n}\r\nstatic inline int map_seq_out_ptr_ctx(u32 *desc, struct device *jrdev,\r\nstruct caam_hash_state *state,\r\nint ctx_len)\r\n{\r\nstate->ctx_dma = dma_map_single(jrdev, state->caam_ctx,\r\nctx_len, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(jrdev, state->ctx_dma)) {\r\ndev_err(jrdev, "unable to map ctx\n");\r\nstate->ctx_dma = 0;\r\nreturn -ENOMEM;\r\n}\r\nappend_seq_out_ptr(desc, state->ctx_dma, ctx_len, 0);\r\nreturn 0;\r\n}\r\nstatic inline dma_addr_t map_seq_out_ptr_result(u32 *desc, struct device *jrdev,\r\nu8 *result, int digestsize)\r\n{\r\ndma_addr_t dst_dma;\r\ndst_dma = dma_map_single(jrdev, result, digestsize, DMA_FROM_DEVICE);\r\nappend_seq_out_ptr(desc, dst_dma, digestsize, 0);\r\nreturn dst_dma;\r\n}\r\nstatic inline int buf_map_to_sec4_sg(struct device *jrdev,\r\nstruct sec4_sg_entry *sec4_sg,\r\nstruct caam_hash_state *state)\r\n{\r\nint buflen = *current_buflen(state);\r\nif (!buflen)\r\nreturn 0;\r\nstate->buf_dma = dma_map_single(jrdev, current_buf(state), buflen,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, state->buf_dma)) {\r\ndev_err(jrdev, "unable to map buf\n");\r\nstate->buf_dma = 0;\r\nreturn -ENOMEM;\r\n}\r\ndma_to_sec4_sg_one(sec4_sg, state->buf_dma, buflen, 0);\r\nreturn 0;\r\n}\r\nstatic inline int ctx_map_to_sec4_sg(u32 *desc, struct device *jrdev,\r\nstruct caam_hash_state *state, int ctx_len,\r\nstruct sec4_sg_entry *sec4_sg, u32 flag)\r\n{\r\nstate->ctx_dma = dma_map_single(jrdev, state->caam_ctx, ctx_len, flag);\r\nif (dma_mapping_error(jrdev, state->ctx_dma)) {\r\ndev_err(jrdev, "unable to map ctx\n");\r\nstate->ctx_dma = 0;\r\nreturn -ENOMEM;\r\n}\r\ndma_to_sec4_sg_one(sec4_sg, state->ctx_dma, ctx_len, 0);\r\nreturn 0;\r\n}\r\nstatic inline void ahash_gen_sh_desc(u32 *desc, u32 state, int digestsize,\r\nstruct caam_hash_ctx *ctx, bool import_ctx)\r\n{\r\nu32 op = ctx->adata.algtype;\r\nu32 *skip_key_load;\r\ninit_sh_desc(desc, HDR_SHARE_SERIAL);\r\nif ((state != OP_ALG_AS_UPDATE) && (ctx->adata.keylen)) {\r\nskip_key_load = append_jump(desc, JUMP_JSL | JUMP_TEST_ALL |\r\nJUMP_COND_SHRD);\r\nappend_key_as_imm(desc, ctx->key, ctx->adata.keylen_pad,\r\nctx->adata.keylen, CLASS_2 |\r\nKEY_DEST_MDHA_SPLIT | KEY_ENC);\r\nset_jump_tgt_here(desc, skip_key_load);\r\nop |= OP_ALG_AAI_HMAC_PRECOMP;\r\n}\r\nif (import_ctx)\r\nappend_seq_load(desc, ctx->ctx_len, LDST_CLASS_2_CCB |\r\nLDST_SRCDST_BYTE_CONTEXT);\r\nappend_operation(desc, op | state | OP_ALG_ENCRYPT);\r\nappend_math_add(desc, VARSEQINLEN, SEQINLEN, REG0, CAAM_CMD_SZ);\r\nappend_seq_fifo_load(desc, 0, FIFOLD_CLASS_CLASS2 | FIFOLD_TYPE_LAST2 |\r\nFIFOLD_TYPE_MSG | KEY_VLF);\r\nappend_seq_store(desc, digestsize, LDST_CLASS_2_CCB |\r\nLDST_SRCDST_BYTE_CONTEXT);\r\n}\r\nstatic int ahash_set_sh_desc(struct crypto_ahash *ahash)\r\n{\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct device *jrdev = ctx->jrdev;\r\nu32 *desc;\r\ndesc = ctx->sh_desc_update;\r\nahash_gen_sh_desc(desc, OP_ALG_AS_UPDATE, ctx->ctx_len, ctx, true);\r\ndma_sync_single_for_device(jrdev, ctx->sh_desc_update_dma,\r\ndesc_bytes(desc), DMA_TO_DEVICE);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR,\r\n"ahash update shdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\ndesc = ctx->sh_desc_update_first;\r\nahash_gen_sh_desc(desc, OP_ALG_AS_INIT, ctx->ctx_len, ctx, false);\r\ndma_sync_single_for_device(jrdev, ctx->sh_desc_update_first_dma,\r\ndesc_bytes(desc), DMA_TO_DEVICE);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR,\r\n"ahash update first shdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\ndesc = ctx->sh_desc_fin;\r\nahash_gen_sh_desc(desc, OP_ALG_AS_FINALIZE, digestsize, ctx, true);\r\ndma_sync_single_for_device(jrdev, ctx->sh_desc_fin_dma,\r\ndesc_bytes(desc), DMA_TO_DEVICE);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ahash final shdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\ndesc = ctx->sh_desc_digest;\r\nahash_gen_sh_desc(desc, OP_ALG_AS_INITFINAL, digestsize, ctx, false);\r\ndma_sync_single_for_device(jrdev, ctx->sh_desc_digest_dma,\r\ndesc_bytes(desc), DMA_TO_DEVICE);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR,\r\n"ahash digest shdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\nreturn 0;\r\n}\r\nstatic int hash_digest_key(struct caam_hash_ctx *ctx, const u8 *key_in,\r\nu32 *keylen, u8 *key_out, u32 digestsize)\r\n{\r\nstruct device *jrdev = ctx->jrdev;\r\nu32 *desc;\r\nstruct split_key_result result;\r\ndma_addr_t src_dma, dst_dma;\r\nint ret;\r\ndesc = kmalloc(CAAM_CMD_SZ * 8 + CAAM_PTR_SZ * 2, GFP_KERNEL | GFP_DMA);\r\nif (!desc) {\r\ndev_err(jrdev, "unable to allocate key input memory\n");\r\nreturn -ENOMEM;\r\n}\r\ninit_job_desc(desc, 0);\r\nsrc_dma = dma_map_single(jrdev, (void *)key_in, *keylen,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, src_dma)) {\r\ndev_err(jrdev, "unable to map key input memory\n");\r\nkfree(desc);\r\nreturn -ENOMEM;\r\n}\r\ndst_dma = dma_map_single(jrdev, (void *)key_out, digestsize,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(jrdev, dst_dma)) {\r\ndev_err(jrdev, "unable to map key output memory\n");\r\ndma_unmap_single(jrdev, src_dma, *keylen, DMA_TO_DEVICE);\r\nkfree(desc);\r\nreturn -ENOMEM;\r\n}\r\nappend_operation(desc, ctx->adata.algtype | OP_ALG_ENCRYPT |\r\nOP_ALG_AS_INITFINAL);\r\nappend_seq_in_ptr(desc, src_dma, *keylen, 0);\r\nappend_seq_fifo_load(desc, *keylen, FIFOLD_CLASS_CLASS2 |\r\nFIFOLD_TYPE_LAST2 | FIFOLD_TYPE_MSG);\r\nappend_seq_out_ptr(desc, dst_dma, digestsize, 0);\r\nappend_seq_store(desc, digestsize, LDST_CLASS_2_CCB |\r\nLDST_SRCDST_BYTE_CONTEXT);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "key_in@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, key_in, *keylen, 1);\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nresult.err = 0;\r\ninit_completion(&result.completion);\r\nret = caam_jr_enqueue(jrdev, desc, split_key_done, &result);\r\nif (!ret) {\r\nwait_for_completion_interruptible(&result.completion);\r\nret = result.err;\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR,\r\n"digested key@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, key_in,\r\ndigestsize, 1);\r\n#endif\r\n}\r\ndma_unmap_single(jrdev, src_dma, *keylen, DMA_TO_DEVICE);\r\ndma_unmap_single(jrdev, dst_dma, digestsize, DMA_FROM_DEVICE);\r\n*keylen = digestsize;\r\nkfree(desc);\r\nreturn ret;\r\n}\r\nstatic int ahash_setkey(struct crypto_ahash *ahash,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nint blocksize = crypto_tfm_alg_blocksize(&ahash->base);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nint ret;\r\nu8 *hashed_key = NULL;\r\n#ifdef DEBUG\r\nprintk(KERN_ERR "keylen %d\n", keylen);\r\n#endif\r\nif (keylen > blocksize) {\r\nhashed_key = kmalloc_array(digestsize,\r\nsizeof(*hashed_key),\r\nGFP_KERNEL | GFP_DMA);\r\nif (!hashed_key)\r\nreturn -ENOMEM;\r\nret = hash_digest_key(ctx, key, &keylen, hashed_key,\r\ndigestsize);\r\nif (ret)\r\ngoto bad_free_key;\r\nkey = hashed_key;\r\n}\r\nret = gen_split_key(ctx->jrdev, ctx->key, &ctx->adata, key, keylen,\r\nCAAM_MAX_HASH_KEY_SIZE);\r\nif (ret)\r\ngoto bad_free_key;\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx.key@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, ctx->key,\r\nctx->adata.keylen_pad, 1);\r\n#endif\r\nkfree(hashed_key);\r\nreturn ahash_set_sh_desc(ahash);\r\nbad_free_key:\r\nkfree(hashed_key);\r\ncrypto_ahash_set_flags(ahash, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic inline void ahash_unmap(struct device *dev,\r\nstruct ahash_edesc *edesc,\r\nstruct ahash_request *req, int dst_len)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nif (edesc->src_nents)\r\ndma_unmap_sg(dev, req->src, edesc->src_nents, DMA_TO_DEVICE);\r\nif (edesc->dst_dma)\r\ndma_unmap_single(dev, edesc->dst_dma, dst_len, DMA_FROM_DEVICE);\r\nif (edesc->sec4_sg_bytes)\r\ndma_unmap_single(dev, edesc->sec4_sg_dma,\r\nedesc->sec4_sg_bytes, DMA_TO_DEVICE);\r\nif (state->buf_dma) {\r\ndma_unmap_single(dev, state->buf_dma, *current_buflen(state),\r\nDMA_TO_DEVICE);\r\nstate->buf_dma = 0;\r\n}\r\n}\r\nstatic inline void ahash_unmap_ctx(struct device *dev,\r\nstruct ahash_edesc *edesc,\r\nstruct ahash_request *req, int dst_len, u32 flag)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nif (state->ctx_dma) {\r\ndma_unmap_single(dev, state->ctx_dma, ctx->ctx_len, flag);\r\nstate->ctx_dma = 0;\r\n}\r\nahash_unmap(dev, edesc, req, dst_len);\r\n}\r\nstatic void ahash_done(struct device *jrdev, u32 *desc, u32 err,\r\nvoid *context)\r\n{\r\nstruct ahash_request *req = context;\r\nstruct ahash_edesc *edesc;\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\n#ifdef DEBUG\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\ndev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);\r\n#endif\r\nedesc = container_of(desc, struct ahash_edesc, hw_desc[0]);\r\nif (err)\r\ncaam_jr_strstatus(jrdev, err);\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\r\nctx->ctx_len, 1);\r\nif (req->result)\r\nprint_hex_dump(KERN_ERR, "result@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, req->result,\r\ndigestsize, 1);\r\n#endif\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic void ahash_done_bi(struct device *jrdev, u32 *desc, u32 err,\r\nvoid *context)\r\n{\r\nstruct ahash_request *req = context;\r\nstruct ahash_edesc *edesc;\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\n#ifdef DEBUG\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\ndev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);\r\n#endif\r\nedesc = container_of(desc, struct ahash_edesc, hw_desc[0]);\r\nif (err)\r\ncaam_jr_strstatus(jrdev, err);\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_BIDIRECTIONAL);\r\nswitch_buf(state);\r\nkfree(edesc);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\r\nctx->ctx_len, 1);\r\nif (req->result)\r\nprint_hex_dump(KERN_ERR, "result@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, req->result,\r\ndigestsize, 1);\r\n#endif\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic void ahash_done_ctx_src(struct device *jrdev, u32 *desc, u32 err,\r\nvoid *context)\r\n{\r\nstruct ahash_request *req = context;\r\nstruct ahash_edesc *edesc;\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\n#ifdef DEBUG\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\ndev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);\r\n#endif\r\nedesc = container_of(desc, struct ahash_edesc, hw_desc[0]);\r\nif (err)\r\ncaam_jr_strstatus(jrdev, err);\r\nahash_unmap_ctx(jrdev, edesc, req, digestsize, DMA_TO_DEVICE);\r\nkfree(edesc);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\r\nctx->ctx_len, 1);\r\nif (req->result)\r\nprint_hex_dump(KERN_ERR, "result@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, req->result,\r\ndigestsize, 1);\r\n#endif\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic void ahash_done_ctx_dst(struct device *jrdev, u32 *desc, u32 err,\r\nvoid *context)\r\n{\r\nstruct ahash_request *req = context;\r\nstruct ahash_edesc *edesc;\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\n#ifdef DEBUG\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\ndev_err(jrdev, "%s %d: err 0x%x\n", __func__, __LINE__, err);\r\n#endif\r\nedesc = container_of(desc, struct ahash_edesc, hw_desc[0]);\r\nif (err)\r\ncaam_jr_strstatus(jrdev, err);\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_FROM_DEVICE);\r\nswitch_buf(state);\r\nkfree(edesc);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "ctx@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, state->caam_ctx,\r\nctx->ctx_len, 1);\r\nif (req->result)\r\nprint_hex_dump(KERN_ERR, "result@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, req->result,\r\ndigestsize, 1);\r\n#endif\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic struct ahash_edesc *ahash_edesc_alloc(struct caam_hash_ctx *ctx,\r\nint sg_num, u32 *sh_desc,\r\ndma_addr_t sh_desc_dma,\r\ngfp_t flags)\r\n{\r\nstruct ahash_edesc *edesc;\r\nunsigned int sg_size = sg_num * sizeof(struct sec4_sg_entry);\r\nedesc = kzalloc(sizeof(*edesc) + sg_size, GFP_DMA | flags);\r\nif (!edesc) {\r\ndev_err(ctx->jrdev, "could not allocate extended descriptor\n");\r\nreturn NULL;\r\n}\r\ninit_job_desc_shared(edesc->hw_desc, sh_desc_dma, desc_len(sh_desc),\r\nHDR_SHARE_DEFER | HDR_REVERSE);\r\nreturn edesc;\r\n}\r\nstatic int ahash_edesc_add_src(struct caam_hash_ctx *ctx,\r\nstruct ahash_edesc *edesc,\r\nstruct ahash_request *req, int nents,\r\nunsigned int first_sg,\r\nunsigned int first_bytes, size_t to_hash)\r\n{\r\ndma_addr_t src_dma;\r\nu32 options;\r\nif (nents > 1 || first_sg) {\r\nstruct sec4_sg_entry *sg = edesc->sec4_sg;\r\nunsigned int sgsize = sizeof(*sg) * (first_sg + nents);\r\nsg_to_sec4_sg_last(req->src, nents, sg + first_sg, 0);\r\nsrc_dma = dma_map_single(ctx->jrdev, sg, sgsize, DMA_TO_DEVICE);\r\nif (dma_mapping_error(ctx->jrdev, src_dma)) {\r\ndev_err(ctx->jrdev, "unable to map S/G table\n");\r\nreturn -ENOMEM;\r\n}\r\nedesc->sec4_sg_bytes = sgsize;\r\nedesc->sec4_sg_dma = src_dma;\r\noptions = LDST_SGF;\r\n} else {\r\nsrc_dma = sg_dma_address(req->src);\r\noptions = 0;\r\n}\r\nappend_seq_in_ptr(edesc->hw_desc, src_dma, first_bytes + to_hash,\r\noptions);\r\nreturn 0;\r\n}\r\nstatic int ahash_update_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = current_buf(state);\r\nint *buflen = current_buflen(state);\r\nu8 *next_buf = alt_buf(state);\r\nint *next_buflen = alt_buflen(state), last_buflen;\r\nint in_len = *buflen + req->nbytes, to_hash;\r\nu32 *desc;\r\nint src_nents, mapped_nents, sec4_sg_bytes, sec4_sg_src_index;\r\nstruct ahash_edesc *edesc;\r\nint ret = 0;\r\nlast_buflen = *next_buflen;\r\n*next_buflen = in_len & (crypto_tfm_alg_blocksize(&ahash->base) - 1);\r\nto_hash = in_len - *next_buflen;\r\nif (to_hash) {\r\nsrc_nents = sg_nents_for_len(req->src,\r\nreq->nbytes - (*next_buflen));\r\nif (src_nents < 0) {\r\ndev_err(jrdev, "Invalid number of src SG.\n");\r\nreturn src_nents;\r\n}\r\nif (src_nents) {\r\nmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\r\nDMA_TO_DEVICE);\r\nif (!mapped_nents) {\r\ndev_err(jrdev, "unable to DMA map source\n");\r\nreturn -ENOMEM;\r\n}\r\n} else {\r\nmapped_nents = 0;\r\n}\r\nsec4_sg_src_index = 1 + (*buflen ? 1 : 0);\r\nsec4_sg_bytes = (sec4_sg_src_index + mapped_nents) *\r\nsizeof(struct sec4_sg_entry);\r\nedesc = ahash_edesc_alloc(ctx, sec4_sg_src_index + mapped_nents,\r\nctx->sh_desc_update,\r\nctx->sh_desc_update_dma, flags);\r\nif (!edesc) {\r\ndma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\r\nreturn -ENOMEM;\r\n}\r\nedesc->src_nents = src_nents;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nret = ctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len,\r\nedesc->sec4_sg, DMA_BIDIRECTIONAL);\r\nif (ret)\r\ngoto unmap_ctx;\r\nret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg + 1, state);\r\nif (ret)\r\ngoto unmap_ctx;\r\nif (mapped_nents) {\r\nsg_to_sec4_sg_last(req->src, mapped_nents,\r\nedesc->sec4_sg + sec4_sg_src_index,\r\n0);\r\nif (*next_buflen)\r\nscatterwalk_map_and_copy(next_buf, req->src,\r\nto_hash - *buflen,\r\n*next_buflen, 0);\r\n} else {\r\n(edesc->sec4_sg + sec4_sg_src_index - 1)->len |=\r\ncpu_to_caam32(SEC4_SG_LEN_FIN);\r\n}\r\ndesc = edesc->hw_desc;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {\r\ndev_err(jrdev, "unable to map S/G table\n");\r\nret = -ENOMEM;\r\ngoto unmap_ctx;\r\n}\r\nappend_seq_in_ptr(desc, edesc->sec4_sg_dma, ctx->ctx_len +\r\nto_hash, LDST_SGF);\r\nappend_seq_out_ptr(desc, state->ctx_dma, ctx->ctx_len, 0);\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_bi, req);\r\nif (ret)\r\ngoto unmap_ctx;\r\nret = -EINPROGRESS;\r\n} else if (*next_buflen) {\r\nscatterwalk_map_and_copy(buf + *buflen, req->src, 0,\r\nreq->nbytes, 0);\r\n*buflen = *next_buflen;\r\n*next_buflen = last_buflen;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "buf@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, buf, *buflen, 1);\r\nprint_hex_dump(KERN_ERR, "next buf@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, next_buf,\r\n*next_buflen, 1);\r\n#endif\r\nreturn ret;\r\nunmap_ctx:\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_BIDIRECTIONAL);\r\nkfree(edesc);\r\nreturn ret;\r\n}\r\nstatic int ahash_final_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nint buflen = *current_buflen(state);\r\nu32 *desc;\r\nint sec4_sg_bytes, sec4_sg_src_index;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct ahash_edesc *edesc;\r\nint ret;\r\nsec4_sg_src_index = 1 + (buflen ? 1 : 0);\r\nsec4_sg_bytes = sec4_sg_src_index * sizeof(struct sec4_sg_entry);\r\nedesc = ahash_edesc_alloc(ctx, sec4_sg_src_index,\r\nctx->sh_desc_fin, ctx->sh_desc_fin_dma,\r\nflags);\r\nif (!edesc)\r\nreturn -ENOMEM;\r\ndesc = edesc->hw_desc;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nedesc->src_nents = 0;\r\nret = ctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len,\r\nedesc->sec4_sg, DMA_TO_DEVICE);\r\nif (ret)\r\ngoto unmap_ctx;\r\nret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg + 1, state);\r\nif (ret)\r\ngoto unmap_ctx;\r\n(edesc->sec4_sg + sec4_sg_src_index - 1)->len |=\r\ncpu_to_caam32(SEC4_SG_LEN_FIN);\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes, DMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {\r\ndev_err(jrdev, "unable to map S/G table\n");\r\nret = -ENOMEM;\r\ngoto unmap_ctx;\r\n}\r\nappend_seq_in_ptr(desc, edesc->sec4_sg_dma, ctx->ctx_len + buflen,\r\nLDST_SGF);\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\nif (dma_mapping_error(jrdev, edesc->dst_dma)) {\r\ndev_err(jrdev, "unable to map dst\n");\r\nret = -ENOMEM;\r\ngoto unmap_ctx;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_ctx_src, req);\r\nif (ret)\r\ngoto unmap_ctx;\r\nreturn -EINPROGRESS;\r\nunmap_ctx:\r\nahash_unmap_ctx(jrdev, edesc, req, digestsize, DMA_FROM_DEVICE);\r\nkfree(edesc);\r\nreturn ret;\r\n}\r\nstatic int ahash_finup_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nint buflen = *current_buflen(state);\r\nu32 *desc;\r\nint sec4_sg_src_index;\r\nint src_nents, mapped_nents;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct ahash_edesc *edesc;\r\nint ret;\r\nsrc_nents = sg_nents_for_len(req->src, req->nbytes);\r\nif (src_nents < 0) {\r\ndev_err(jrdev, "Invalid number of src SG.\n");\r\nreturn src_nents;\r\n}\r\nif (src_nents) {\r\nmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\r\nDMA_TO_DEVICE);\r\nif (!mapped_nents) {\r\ndev_err(jrdev, "unable to DMA map source\n");\r\nreturn -ENOMEM;\r\n}\r\n} else {\r\nmapped_nents = 0;\r\n}\r\nsec4_sg_src_index = 1 + (buflen ? 1 : 0);\r\nedesc = ahash_edesc_alloc(ctx, sec4_sg_src_index + mapped_nents,\r\nctx->sh_desc_fin, ctx->sh_desc_fin_dma,\r\nflags);\r\nif (!edesc) {\r\ndma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\r\nreturn -ENOMEM;\r\n}\r\ndesc = edesc->hw_desc;\r\nedesc->src_nents = src_nents;\r\nret = ctx_map_to_sec4_sg(desc, jrdev, state, ctx->ctx_len,\r\nedesc->sec4_sg, DMA_TO_DEVICE);\r\nif (ret)\r\ngoto unmap_ctx;\r\nret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg + 1, state);\r\nif (ret)\r\ngoto unmap_ctx;\r\nret = ahash_edesc_add_src(ctx, edesc, req, mapped_nents,\r\nsec4_sg_src_index, ctx->ctx_len + buflen,\r\nreq->nbytes);\r\nif (ret)\r\ngoto unmap_ctx;\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\nif (dma_mapping_error(jrdev, edesc->dst_dma)) {\r\ndev_err(jrdev, "unable to map dst\n");\r\nret = -ENOMEM;\r\ngoto unmap_ctx;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_ctx_src, req);\r\nif (ret)\r\ngoto unmap_ctx;\r\nreturn -EINPROGRESS;\r\nunmap_ctx:\r\nahash_unmap_ctx(jrdev, edesc, req, digestsize, DMA_FROM_DEVICE);\r\nkfree(edesc);\r\nreturn ret;\r\n}\r\nstatic int ahash_digest(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu32 *desc;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nint src_nents, mapped_nents;\r\nstruct ahash_edesc *edesc;\r\nint ret;\r\nstate->buf_dma = 0;\r\nsrc_nents = sg_nents_for_len(req->src, req->nbytes);\r\nif (src_nents < 0) {\r\ndev_err(jrdev, "Invalid number of src SG.\n");\r\nreturn src_nents;\r\n}\r\nif (src_nents) {\r\nmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\r\nDMA_TO_DEVICE);\r\nif (!mapped_nents) {\r\ndev_err(jrdev, "unable to map source for DMA\n");\r\nreturn -ENOMEM;\r\n}\r\n} else {\r\nmapped_nents = 0;\r\n}\r\nedesc = ahash_edesc_alloc(ctx, mapped_nents > 1 ? mapped_nents : 0,\r\nctx->sh_desc_digest, ctx->sh_desc_digest_dma,\r\nflags);\r\nif (!edesc) {\r\ndma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\r\nreturn -ENOMEM;\r\n}\r\nedesc->src_nents = src_nents;\r\nret = ahash_edesc_add_src(ctx, edesc, req, mapped_nents, 0, 0,\r\nreq->nbytes);\r\nif (ret) {\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\nreturn ret;\r\n}\r\ndesc = edesc->hw_desc;\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\nif (dma_mapping_error(jrdev, edesc->dst_dma)) {\r\ndev_err(jrdev, "unable to map dst\n");\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\nreturn -ENOMEM;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\n}\r\nstatic int ahash_final_no_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = current_buf(state);\r\nint buflen = *current_buflen(state);\r\nu32 *desc;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct ahash_edesc *edesc;\r\nint ret;\r\nedesc = ahash_edesc_alloc(ctx, 0, ctx->sh_desc_digest,\r\nctx->sh_desc_digest_dma, flags);\r\nif (!edesc)\r\nreturn -ENOMEM;\r\ndesc = edesc->hw_desc;\r\nstate->buf_dma = dma_map_single(jrdev, buf, buflen, DMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, state->buf_dma)) {\r\ndev_err(jrdev, "unable to map src\n");\r\ngoto unmap;\r\n}\r\nappend_seq_in_ptr(desc, state->buf_dma, buflen, 0);\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\nif (dma_mapping_error(jrdev, edesc->dst_dma)) {\r\ndev_err(jrdev, "unable to map dst\n");\r\ngoto unmap;\r\n}\r\nedesc->src_nents = 0;\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\nunmap:\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\nreturn -ENOMEM;\r\n}\r\nstatic int ahash_update_no_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *buf = current_buf(state);\r\nint *buflen = current_buflen(state);\r\nu8 *next_buf = alt_buf(state);\r\nint *next_buflen = alt_buflen(state);\r\nint in_len = *buflen + req->nbytes, to_hash;\r\nint sec4_sg_bytes, src_nents, mapped_nents;\r\nstruct ahash_edesc *edesc;\r\nu32 *desc;\r\nint ret = 0;\r\n*next_buflen = in_len & (crypto_tfm_alg_blocksize(&ahash->base) - 1);\r\nto_hash = in_len - *next_buflen;\r\nif (to_hash) {\r\nsrc_nents = sg_nents_for_len(req->src,\r\nreq->nbytes - *next_buflen);\r\nif (src_nents < 0) {\r\ndev_err(jrdev, "Invalid number of src SG.\n");\r\nreturn src_nents;\r\n}\r\nif (src_nents) {\r\nmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\r\nDMA_TO_DEVICE);\r\nif (!mapped_nents) {\r\ndev_err(jrdev, "unable to DMA map source\n");\r\nreturn -ENOMEM;\r\n}\r\n} else {\r\nmapped_nents = 0;\r\n}\r\nsec4_sg_bytes = (1 + mapped_nents) *\r\nsizeof(struct sec4_sg_entry);\r\nedesc = ahash_edesc_alloc(ctx, 1 + mapped_nents,\r\nctx->sh_desc_update_first,\r\nctx->sh_desc_update_first_dma,\r\nflags);\r\nif (!edesc) {\r\ndma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\r\nreturn -ENOMEM;\r\n}\r\nedesc->src_nents = src_nents;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nedesc->dst_dma = 0;\r\nret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg, state);\r\nif (ret)\r\ngoto unmap_ctx;\r\nsg_to_sec4_sg_last(req->src, mapped_nents,\r\nedesc->sec4_sg + 1, 0);\r\nif (*next_buflen) {\r\nscatterwalk_map_and_copy(next_buf, req->src,\r\nto_hash - *buflen,\r\n*next_buflen, 0);\r\n}\r\ndesc = edesc->hw_desc;\r\nedesc->sec4_sg_dma = dma_map_single(jrdev, edesc->sec4_sg,\r\nsec4_sg_bytes,\r\nDMA_TO_DEVICE);\r\nif (dma_mapping_error(jrdev, edesc->sec4_sg_dma)) {\r\ndev_err(jrdev, "unable to map S/G table\n");\r\nret = -ENOMEM;\r\ngoto unmap_ctx;\r\n}\r\nappend_seq_in_ptr(desc, edesc->sec4_sg_dma, to_hash, LDST_SGF);\r\nret = map_seq_out_ptr_ctx(desc, jrdev, state, ctx->ctx_len);\r\nif (ret)\r\ngoto unmap_ctx;\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_ctx_dst, req);\r\nif (ret)\r\ngoto unmap_ctx;\r\nret = -EINPROGRESS;\r\nstate->update = ahash_update_ctx;\r\nstate->finup = ahash_finup_ctx;\r\nstate->final = ahash_final_ctx;\r\n} else if (*next_buflen) {\r\nscatterwalk_map_and_copy(buf + *buflen, req->src, 0,\r\nreq->nbytes, 0);\r\n*buflen = *next_buflen;\r\n*next_buflen = 0;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "buf@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, buf, *buflen, 1);\r\nprint_hex_dump(KERN_ERR, "next buf@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, next_buf,\r\n*next_buflen, 1);\r\n#endif\r\nreturn ret;\r\nunmap_ctx:\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_TO_DEVICE);\r\nkfree(edesc);\r\nreturn ret;\r\n}\r\nstatic int ahash_finup_no_ctx(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nint buflen = *current_buflen(state);\r\nu32 *desc;\r\nint sec4_sg_bytes, sec4_sg_src_index, src_nents, mapped_nents;\r\nint digestsize = crypto_ahash_digestsize(ahash);\r\nstruct ahash_edesc *edesc;\r\nint ret;\r\nsrc_nents = sg_nents_for_len(req->src, req->nbytes);\r\nif (src_nents < 0) {\r\ndev_err(jrdev, "Invalid number of src SG.\n");\r\nreturn src_nents;\r\n}\r\nif (src_nents) {\r\nmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\r\nDMA_TO_DEVICE);\r\nif (!mapped_nents) {\r\ndev_err(jrdev, "unable to DMA map source\n");\r\nreturn -ENOMEM;\r\n}\r\n} else {\r\nmapped_nents = 0;\r\n}\r\nsec4_sg_src_index = 2;\r\nsec4_sg_bytes = (sec4_sg_src_index + mapped_nents) *\r\nsizeof(struct sec4_sg_entry);\r\nedesc = ahash_edesc_alloc(ctx, sec4_sg_src_index + mapped_nents,\r\nctx->sh_desc_digest, ctx->sh_desc_digest_dma,\r\nflags);\r\nif (!edesc) {\r\ndma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\r\nreturn -ENOMEM;\r\n}\r\ndesc = edesc->hw_desc;\r\nedesc->src_nents = src_nents;\r\nedesc->sec4_sg_bytes = sec4_sg_bytes;\r\nret = buf_map_to_sec4_sg(jrdev, edesc->sec4_sg, state);\r\nif (ret)\r\ngoto unmap;\r\nret = ahash_edesc_add_src(ctx, edesc, req, mapped_nents, 1, buflen,\r\nreq->nbytes);\r\nif (ret) {\r\ndev_err(jrdev, "unable to map S/G table\n");\r\ngoto unmap;\r\n}\r\nedesc->dst_dma = map_seq_out_ptr_result(desc, jrdev, req->result,\r\ndigestsize);\r\nif (dma_mapping_error(jrdev, edesc->dst_dma)) {\r\ndev_err(jrdev, "unable to map dst\n");\r\ngoto unmap;\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc, desc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done, req);\r\nif (!ret) {\r\nret = -EINPROGRESS;\r\n} else {\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\n}\r\nreturn ret;\r\nunmap:\r\nahash_unmap(jrdev, edesc, req, digestsize);\r\nkfree(edesc);\r\nreturn -ENOMEM;\r\n}\r\nstatic int ahash_update_first(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *ahash = crypto_ahash_reqtfm(req);\r\nstruct caam_hash_ctx *ctx = crypto_ahash_ctx(ahash);\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct device *jrdev = ctx->jrdev;\r\ngfp_t flags = (req->base.flags & (CRYPTO_TFM_REQ_MAY_BACKLOG |\r\nCRYPTO_TFM_REQ_MAY_SLEEP)) ? GFP_KERNEL : GFP_ATOMIC;\r\nu8 *next_buf = alt_buf(state);\r\nint *next_buflen = alt_buflen(state);\r\nint to_hash;\r\nu32 *desc;\r\nint src_nents, mapped_nents;\r\nstruct ahash_edesc *edesc;\r\nint ret = 0;\r\n*next_buflen = req->nbytes & (crypto_tfm_alg_blocksize(&ahash->base) -\r\n1);\r\nto_hash = req->nbytes - *next_buflen;\r\nif (to_hash) {\r\nsrc_nents = sg_nents_for_len(req->src,\r\nreq->nbytes - *next_buflen);\r\nif (src_nents < 0) {\r\ndev_err(jrdev, "Invalid number of src SG.\n");\r\nreturn src_nents;\r\n}\r\nif (src_nents) {\r\nmapped_nents = dma_map_sg(jrdev, req->src, src_nents,\r\nDMA_TO_DEVICE);\r\nif (!mapped_nents) {\r\ndev_err(jrdev, "unable to map source for DMA\n");\r\nreturn -ENOMEM;\r\n}\r\n} else {\r\nmapped_nents = 0;\r\n}\r\nedesc = ahash_edesc_alloc(ctx, mapped_nents > 1 ?\r\nmapped_nents : 0,\r\nctx->sh_desc_update_first,\r\nctx->sh_desc_update_first_dma,\r\nflags);\r\nif (!edesc) {\r\ndma_unmap_sg(jrdev, req->src, src_nents, DMA_TO_DEVICE);\r\nreturn -ENOMEM;\r\n}\r\nedesc->src_nents = src_nents;\r\nedesc->dst_dma = 0;\r\nret = ahash_edesc_add_src(ctx, edesc, req, mapped_nents, 0, 0,\r\nto_hash);\r\nif (ret)\r\ngoto unmap_ctx;\r\nif (*next_buflen)\r\nscatterwalk_map_and_copy(next_buf, req->src, to_hash,\r\n*next_buflen, 0);\r\ndesc = edesc->hw_desc;\r\nret = map_seq_out_ptr_ctx(desc, jrdev, state, ctx->ctx_len);\r\nif (ret)\r\ngoto unmap_ctx;\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "jobdesc@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, desc,\r\ndesc_bytes(desc), 1);\r\n#endif\r\nret = caam_jr_enqueue(jrdev, desc, ahash_done_ctx_dst, req);\r\nif (ret)\r\ngoto unmap_ctx;\r\nret = -EINPROGRESS;\r\nstate->update = ahash_update_ctx;\r\nstate->finup = ahash_finup_ctx;\r\nstate->final = ahash_final_ctx;\r\n} else if (*next_buflen) {\r\nstate->update = ahash_update_no_ctx;\r\nstate->finup = ahash_finup_no_ctx;\r\nstate->final = ahash_final_no_ctx;\r\nscatterwalk_map_and_copy(next_buf, req->src, 0,\r\nreq->nbytes, 0);\r\nswitch_buf(state);\r\n}\r\n#ifdef DEBUG\r\nprint_hex_dump(KERN_ERR, "next buf@"__stringify(__LINE__)": ",\r\nDUMP_PREFIX_ADDRESS, 16, 4, next_buf,\r\n*next_buflen, 1);\r\n#endif\r\nreturn ret;\r\nunmap_ctx:\r\nahash_unmap_ctx(jrdev, edesc, req, ctx->ctx_len, DMA_TO_DEVICE);\r\nkfree(edesc);\r\nreturn ret;\r\n}\r\nstatic int ahash_finup_first(struct ahash_request *req)\r\n{\r\nreturn ahash_digest(req);\r\n}\r\nstatic int ahash_init(struct ahash_request *req)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstate->update = ahash_update_first;\r\nstate->finup = ahash_finup_first;\r\nstate->final = ahash_final_no_ctx;\r\nstate->ctx_dma = 0;\r\nstate->current_buf = 0;\r\nstate->buf_dma = 0;\r\nstate->buflen_0 = 0;\r\nstate->buflen_1 = 0;\r\nreturn 0;\r\n}\r\nstatic int ahash_update(struct ahash_request *req)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nreturn state->update(req);\r\n}\r\nstatic int ahash_finup(struct ahash_request *req)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nreturn state->finup(req);\r\n}\r\nstatic int ahash_final(struct ahash_request *req)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nreturn state->final(req);\r\n}\r\nstatic int ahash_export(struct ahash_request *req, void *out)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nstruct caam_export_state *export = out;\r\nint len;\r\nu8 *buf;\r\nif (state->current_buf) {\r\nbuf = state->buf_1;\r\nlen = state->buflen_1;\r\n} else {\r\nbuf = state->buf_0;\r\nlen = state->buflen_0;\r\n}\r\nmemcpy(export->buf, buf, len);\r\nmemcpy(export->caam_ctx, state->caam_ctx, sizeof(export->caam_ctx));\r\nexport->buflen = len;\r\nexport->update = state->update;\r\nexport->final = state->final;\r\nexport->finup = state->finup;\r\nreturn 0;\r\n}\r\nstatic int ahash_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct caam_hash_state *state = ahash_request_ctx(req);\r\nconst struct caam_export_state *export = in;\r\nmemset(state, 0, sizeof(*state));\r\nmemcpy(state->buf_0, export->buf, export->buflen);\r\nmemcpy(state->caam_ctx, export->caam_ctx, sizeof(state->caam_ctx));\r\nstate->buflen_0 = export->buflen;\r\nstate->update = export->update;\r\nstate->final = export->final;\r\nstate->finup = export->finup;\r\nreturn 0;\r\n}\r\nstatic int caam_hash_cra_init(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_ahash *ahash = __crypto_ahash_cast(tfm);\r\nstruct crypto_alg *base = tfm->__crt_alg;\r\nstruct hash_alg_common *halg =\r\ncontainer_of(base, struct hash_alg_common, base);\r\nstruct ahash_alg *alg =\r\ncontainer_of(halg, struct ahash_alg, halg);\r\nstruct caam_hash_alg *caam_hash =\r\ncontainer_of(alg, struct caam_hash_alg, ahash_alg);\r\nstruct caam_hash_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstatic const u8 runninglen[] = { HASH_MSG_LEN + MD5_DIGEST_SIZE,\r\nHASH_MSG_LEN + SHA1_DIGEST_SIZE,\r\nHASH_MSG_LEN + 32,\r\nHASH_MSG_LEN + SHA256_DIGEST_SIZE,\r\nHASH_MSG_LEN + 64,\r\nHASH_MSG_LEN + SHA512_DIGEST_SIZE };\r\ndma_addr_t dma_addr;\r\nctx->jrdev = caam_jr_alloc();\r\nif (IS_ERR(ctx->jrdev)) {\r\npr_err("Job Ring Device allocation for transform failed\n");\r\nreturn PTR_ERR(ctx->jrdev);\r\n}\r\ndma_addr = dma_map_single_attrs(ctx->jrdev, ctx->sh_desc_update,\r\noffsetof(struct caam_hash_ctx,\r\nsh_desc_update_dma),\r\nDMA_TO_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\r\nif (dma_mapping_error(ctx->jrdev, dma_addr)) {\r\ndev_err(ctx->jrdev, "unable to map shared descriptors\n");\r\ncaam_jr_free(ctx->jrdev);\r\nreturn -ENOMEM;\r\n}\r\nctx->sh_desc_update_dma = dma_addr;\r\nctx->sh_desc_update_first_dma = dma_addr +\r\noffsetof(struct caam_hash_ctx,\r\nsh_desc_update_first);\r\nctx->sh_desc_fin_dma = dma_addr + offsetof(struct caam_hash_ctx,\r\nsh_desc_fin);\r\nctx->sh_desc_digest_dma = dma_addr + offsetof(struct caam_hash_ctx,\r\nsh_desc_digest);\r\nctx->adata.algtype = OP_TYPE_CLASS2_ALG | caam_hash->alg_type;\r\nctx->ctx_len = runninglen[(ctx->adata.algtype &\r\nOP_ALG_ALGSEL_SUBMASK) >>\r\nOP_ALG_ALGSEL_SHIFT];\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct caam_hash_state));\r\nreturn ahash_set_sh_desc(ahash);\r\n}\r\nstatic void caam_hash_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct caam_hash_ctx *ctx = crypto_tfm_ctx(tfm);\r\ndma_unmap_single_attrs(ctx->jrdev, ctx->sh_desc_update_dma,\r\noffsetof(struct caam_hash_ctx,\r\nsh_desc_update_dma),\r\nDMA_TO_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\r\ncaam_jr_free(ctx->jrdev);\r\n}\r\nstatic void __exit caam_algapi_hash_exit(void)\r\n{\r\nstruct caam_hash_alg *t_alg, *n;\r\nif (!hash_list.next)\r\nreturn;\r\nlist_for_each_entry_safe(t_alg, n, &hash_list, entry) {\r\ncrypto_unregister_ahash(&t_alg->ahash_alg);\r\nlist_del(&t_alg->entry);\r\nkfree(t_alg);\r\n}\r\n}\r\nstatic struct caam_hash_alg *\r\ncaam_hash_alloc(struct caam_hash_template *template,\r\nbool keyed)\r\n{\r\nstruct caam_hash_alg *t_alg;\r\nstruct ahash_alg *halg;\r\nstruct crypto_alg *alg;\r\nt_alg = kzalloc(sizeof(*t_alg), GFP_KERNEL);\r\nif (!t_alg) {\r\npr_err("failed to allocate t_alg\n");\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nt_alg->ahash_alg = template->template_ahash;\r\nhalg = &t_alg->ahash_alg;\r\nalg = &halg->halg.base;\r\nif (keyed) {\r\nsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ntemplate->hmac_name);\r\nsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ntemplate->hmac_driver_name);\r\n} else {\r\nsnprintf(alg->cra_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ntemplate->name);\r\nsnprintf(alg->cra_driver_name, CRYPTO_MAX_ALG_NAME, "%s",\r\ntemplate->driver_name);\r\nt_alg->ahash_alg.setkey = NULL;\r\n}\r\nalg->cra_module = THIS_MODULE;\r\nalg->cra_init = caam_hash_cra_init;\r\nalg->cra_exit = caam_hash_cra_exit;\r\nalg->cra_ctxsize = sizeof(struct caam_hash_ctx);\r\nalg->cra_priority = CAAM_CRA_PRIORITY;\r\nalg->cra_blocksize = template->blocksize;\r\nalg->cra_alignmask = 0;\r\nalg->cra_flags = CRYPTO_ALG_ASYNC | CRYPTO_ALG_TYPE_AHASH;\r\nalg->cra_type = &crypto_ahash_type;\r\nt_alg->alg_type = template->alg_type;\r\nreturn t_alg;\r\n}\r\nstatic int __init caam_algapi_hash_init(void)\r\n{\r\nstruct device_node *dev_node;\r\nstruct platform_device *pdev;\r\nstruct device *ctrldev;\r\nint i = 0, err = 0;\r\nstruct caam_drv_private *priv;\r\nunsigned int md_limit = SHA512_DIGEST_SIZE;\r\nu32 cha_inst, cha_vid;\r\ndev_node = of_find_compatible_node(NULL, NULL, "fsl,sec-v4.0");\r\nif (!dev_node) {\r\ndev_node = of_find_compatible_node(NULL, NULL, "fsl,sec4.0");\r\nif (!dev_node)\r\nreturn -ENODEV;\r\n}\r\npdev = of_find_device_by_node(dev_node);\r\nif (!pdev) {\r\nof_node_put(dev_node);\r\nreturn -ENODEV;\r\n}\r\nctrldev = &pdev->dev;\r\npriv = dev_get_drvdata(ctrldev);\r\nof_node_put(dev_node);\r\nif (!priv)\r\nreturn -ENODEV;\r\ncha_vid = rd_reg32(&priv->ctrl->perfmon.cha_id_ls);\r\ncha_inst = rd_reg32(&priv->ctrl->perfmon.cha_num_ls);\r\nif (!((cha_inst & CHA_ID_LS_MD_MASK) >> CHA_ID_LS_MD_SHIFT))\r\nreturn -ENODEV;\r\nif ((cha_vid & CHA_ID_LS_MD_MASK) == CHA_ID_LS_MD_LP256)\r\nmd_limit = SHA256_DIGEST_SIZE;\r\nINIT_LIST_HEAD(&hash_list);\r\nfor (i = 0; i < ARRAY_SIZE(driver_hash); i++) {\r\nstruct caam_hash_alg *t_alg;\r\nstruct caam_hash_template *alg = driver_hash + i;\r\nif (alg->template_ahash.halg.digestsize > md_limit)\r\ncontinue;\r\nt_alg = caam_hash_alloc(alg, true);\r\nif (IS_ERR(t_alg)) {\r\nerr = PTR_ERR(t_alg);\r\npr_warn("%s alg allocation failed\n", alg->driver_name);\r\ncontinue;\r\n}\r\nerr = crypto_register_ahash(&t_alg->ahash_alg);\r\nif (err) {\r\npr_warn("%s alg registration failed: %d\n",\r\nt_alg->ahash_alg.halg.base.cra_driver_name,\r\nerr);\r\nkfree(t_alg);\r\n} else\r\nlist_add_tail(&t_alg->entry, &hash_list);\r\nt_alg = caam_hash_alloc(alg, false);\r\nif (IS_ERR(t_alg)) {\r\nerr = PTR_ERR(t_alg);\r\npr_warn("%s alg allocation failed\n", alg->driver_name);\r\ncontinue;\r\n}\r\nerr = crypto_register_ahash(&t_alg->ahash_alg);\r\nif (err) {\r\npr_warn("%s alg registration failed: %d\n",\r\nt_alg->ahash_alg.halg.base.cra_driver_name,\r\nerr);\r\nkfree(t_alg);\r\n} else\r\nlist_add_tail(&t_alg->entry, &hash_list);\r\n}\r\nreturn err;\r\n}
