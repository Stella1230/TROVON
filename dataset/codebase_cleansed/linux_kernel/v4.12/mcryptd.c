void mcryptd_arm_flusher(struct mcryptd_alg_cstate *cstate, unsigned long delay)\r\n{\r\nstruct mcryptd_flush_list *flist;\r\nif (!cstate->flusher_engaged) {\r\nflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\r\nmutex_lock(&flist->lock);\r\nlist_add_tail(&cstate->flush_list, &flist->list);\r\ncstate->flusher_engaged = true;\r\ncstate->next_flush = jiffies + delay;\r\nqueue_delayed_work_on(smp_processor_id(), kcrypto_wq,\r\n&cstate->flush, delay);\r\nmutex_unlock(&flist->lock);\r\n}\r\n}\r\nstatic int mcryptd_init_queue(struct mcryptd_queue *queue,\r\nunsigned int max_cpu_qlen)\r\n{\r\nint cpu;\r\nstruct mcryptd_cpu_queue *cpu_queue;\r\nqueue->cpu_queue = alloc_percpu(struct mcryptd_cpu_queue);\r\npr_debug("mqueue:%p mcryptd_cpu_queue %p\n", queue, queue->cpu_queue);\r\nif (!queue->cpu_queue)\r\nreturn -ENOMEM;\r\nfor_each_possible_cpu(cpu) {\r\ncpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\r\npr_debug("cpu_queue #%d %p\n", cpu, queue->cpu_queue);\r\ncrypto_init_queue(&cpu_queue->queue, max_cpu_qlen);\r\nINIT_WORK(&cpu_queue->work, mcryptd_queue_worker);\r\n}\r\nreturn 0;\r\n}\r\nstatic void mcryptd_fini_queue(struct mcryptd_queue *queue)\r\n{\r\nint cpu;\r\nstruct mcryptd_cpu_queue *cpu_queue;\r\nfor_each_possible_cpu(cpu) {\r\ncpu_queue = per_cpu_ptr(queue->cpu_queue, cpu);\r\nBUG_ON(cpu_queue->queue.qlen);\r\n}\r\nfree_percpu(queue->cpu_queue);\r\n}\r\nstatic int mcryptd_enqueue_request(struct mcryptd_queue *queue,\r\nstruct crypto_async_request *request,\r\nstruct mcryptd_hash_request_ctx *rctx)\r\n{\r\nint cpu, err;\r\nstruct mcryptd_cpu_queue *cpu_queue;\r\ncpu = get_cpu();\r\ncpu_queue = this_cpu_ptr(queue->cpu_queue);\r\nrctx->tag.cpu = cpu;\r\nerr = crypto_enqueue_request(&cpu_queue->queue, request);\r\npr_debug("enqueue request: cpu %d cpu_queue %p request %p\n",\r\ncpu, cpu_queue, request);\r\nqueue_work_on(cpu, kcrypto_wq, &cpu_queue->work);\r\nput_cpu();\r\nreturn err;\r\n}\r\nstatic void mcryptd_opportunistic_flush(void)\r\n{\r\nstruct mcryptd_flush_list *flist;\r\nstruct mcryptd_alg_cstate *cstate;\r\nflist = per_cpu_ptr(mcryptd_flist, smp_processor_id());\r\nwhile (single_task_running()) {\r\nmutex_lock(&flist->lock);\r\ncstate = list_first_entry_or_null(&flist->list,\r\nstruct mcryptd_alg_cstate, flush_list);\r\nif (!cstate || !cstate->flusher_engaged) {\r\nmutex_unlock(&flist->lock);\r\nreturn;\r\n}\r\nlist_del(&cstate->flush_list);\r\ncstate->flusher_engaged = false;\r\nmutex_unlock(&flist->lock);\r\ncstate->alg_state->flusher(cstate);\r\n}\r\n}\r\nstatic void mcryptd_queue_worker(struct work_struct *work)\r\n{\r\nstruct mcryptd_cpu_queue *cpu_queue;\r\nstruct crypto_async_request *req, *backlog;\r\nint i;\r\ncpu_queue = container_of(work, struct mcryptd_cpu_queue, work);\r\ni = 0;\r\nwhile (i < MCRYPTD_BATCH || single_task_running()) {\r\nlocal_bh_disable();\r\npreempt_disable();\r\nbacklog = crypto_get_backlog(&cpu_queue->queue);\r\nreq = crypto_dequeue_request(&cpu_queue->queue);\r\npreempt_enable();\r\nlocal_bh_enable();\r\nif (!req) {\r\nmcryptd_opportunistic_flush();\r\nreturn;\r\n}\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq->complete(req, 0);\r\nif (!cpu_queue->queue.qlen)\r\nreturn;\r\n++i;\r\n}\r\nif (cpu_queue->queue.qlen)\r\nqueue_work(kcrypto_wq, &cpu_queue->work);\r\n}\r\nvoid mcryptd_flusher(struct work_struct *__work)\r\n{\r\nstruct mcryptd_alg_cstate *alg_cpu_state;\r\nstruct mcryptd_alg_state *alg_state;\r\nstruct mcryptd_flush_list *flist;\r\nint cpu;\r\ncpu = smp_processor_id();\r\nalg_cpu_state = container_of(to_delayed_work(__work),\r\nstruct mcryptd_alg_cstate, flush);\r\nalg_state = alg_cpu_state->alg_state;\r\nif (alg_cpu_state->cpu != cpu)\r\npr_debug("mcryptd error: work on cpu %d, should be cpu %d\n",\r\ncpu, alg_cpu_state->cpu);\r\nif (alg_cpu_state->flusher_engaged) {\r\nflist = per_cpu_ptr(mcryptd_flist, cpu);\r\nmutex_lock(&flist->lock);\r\nlist_del(&alg_cpu_state->flush_list);\r\nalg_cpu_state->flusher_engaged = false;\r\nmutex_unlock(&flist->lock);\r\nalg_state->flusher(alg_cpu_state);\r\n}\r\n}\r\nstatic inline struct mcryptd_queue *mcryptd_get_queue(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\r\nstruct mcryptd_instance_ctx *ictx = crypto_instance_ctx(inst);\r\nreturn ictx->queue;\r\n}\r\nstatic void *mcryptd_alloc_instance(struct crypto_alg *alg, unsigned int head,\r\nunsigned int tail)\r\n{\r\nchar *p;\r\nstruct crypto_instance *inst;\r\nint err;\r\np = kzalloc(head + sizeof(*inst) + tail, GFP_KERNEL);\r\nif (!p)\r\nreturn ERR_PTR(-ENOMEM);\r\ninst = (void *)(p + head);\r\nerr = -ENAMETOOLONG;\r\nif (snprintf(inst->alg.cra_driver_name, CRYPTO_MAX_ALG_NAME,\r\n"mcryptd(%s)", alg->cra_driver_name) >= CRYPTO_MAX_ALG_NAME)\r\ngoto out_free_inst;\r\nmemcpy(inst->alg.cra_name, alg->cra_name, CRYPTO_MAX_ALG_NAME);\r\ninst->alg.cra_priority = alg->cra_priority + 50;\r\ninst->alg.cra_blocksize = alg->cra_blocksize;\r\ninst->alg.cra_alignmask = alg->cra_alignmask;\r\nout:\r\nreturn p;\r\nout_free_inst:\r\nkfree(p);\r\np = ERR_PTR(err);\r\ngoto out;\r\n}\r\nstatic inline bool mcryptd_check_internal(struct rtattr **tb, u32 *type,\r\nu32 *mask)\r\n{\r\nstruct crypto_attr_type *algt;\r\nalgt = crypto_get_attr_type(tb);\r\nif (IS_ERR(algt))\r\nreturn false;\r\n*type |= algt->type & CRYPTO_ALG_INTERNAL;\r\n*mask |= algt->mask & CRYPTO_ALG_INTERNAL;\r\nif (*type & *mask & CRYPTO_ALG_INTERNAL)\r\nreturn true;\r\nelse\r\nreturn false;\r\n}\r\nstatic int mcryptd_hash_init_tfm(struct crypto_tfm *tfm)\r\n{\r\nstruct crypto_instance *inst = crypto_tfm_alg_instance(tfm);\r\nstruct hashd_instance_ctx *ictx = crypto_instance_ctx(inst);\r\nstruct crypto_ahash_spawn *spawn = &ictx->spawn;\r\nstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\r\nstruct crypto_ahash *hash;\r\nhash = crypto_spawn_ahash(spawn);\r\nif (IS_ERR(hash))\r\nreturn PTR_ERR(hash);\r\nctx->child = hash;\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct mcryptd_hash_request_ctx) +\r\ncrypto_ahash_reqsize(hash));\r\nreturn 0;\r\n}\r\nstatic void mcryptd_hash_exit_tfm(struct crypto_tfm *tfm)\r\n{\r\nstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_ahash(ctx->child);\r\n}\r\nstatic int mcryptd_hash_setkey(struct crypto_ahash *parent,\r\nconst u8 *key, unsigned int keylen)\r\n{\r\nstruct mcryptd_hash_ctx *ctx = crypto_ahash_ctx(parent);\r\nstruct crypto_ahash *child = ctx->child;\r\nint err;\r\ncrypto_ahash_clear_flags(child, CRYPTO_TFM_REQ_MASK);\r\ncrypto_ahash_set_flags(child, crypto_ahash_get_flags(parent) &\r\nCRYPTO_TFM_REQ_MASK);\r\nerr = crypto_ahash_setkey(child, key, keylen);\r\ncrypto_ahash_set_flags(parent, crypto_ahash_get_flags(child) &\r\nCRYPTO_TFM_RES_MASK);\r\nreturn err;\r\n}\r\nstatic int mcryptd_hash_enqueue(struct ahash_request *req,\r\ncrypto_completion_t complete)\r\n{\r\nint ret;\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct mcryptd_queue *queue =\r\nmcryptd_get_queue(crypto_ahash_tfm(tfm));\r\nrctx->complete = req->base.complete;\r\nreq->base.complete = complete;\r\nret = mcryptd_enqueue_request(queue, &req->base, rctx);\r\nreturn ret;\r\n}\r\nstatic void mcryptd_hash_init(struct crypto_async_request *req_async, int err)\r\n{\r\nstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\r\nstruct crypto_ahash *child = ctx->child;\r\nstruct ahash_request *req = ahash_request_cast(req_async);\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct ahash_request *desc = &rctx->areq;\r\nif (unlikely(err == -EINPROGRESS))\r\ngoto out;\r\nahash_request_set_tfm(desc, child);\r\nahash_request_set_callback(desc, CRYPTO_TFM_REQ_MAY_SLEEP,\r\nrctx->complete, req_async);\r\nrctx->out = req->result;\r\nerr = crypto_ahash_init(desc);\r\nout:\r\nlocal_bh_disable();\r\nrctx->complete(&req->base, err);\r\nlocal_bh_enable();\r\n}\r\nstatic int mcryptd_hash_init_enqueue(struct ahash_request *req)\r\n{\r\nreturn mcryptd_hash_enqueue(req, mcryptd_hash_init);\r\n}\r\nstatic void mcryptd_hash_update(struct crypto_async_request *req_async, int err)\r\n{\r\nstruct ahash_request *req = ahash_request_cast(req_async);\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nif (unlikely(err == -EINPROGRESS))\r\ngoto out;\r\nrctx->out = req->result;\r\nerr = ahash_mcryptd_update(&rctx->areq);\r\nif (err) {\r\nreq->base.complete = rctx->complete;\r\ngoto out;\r\n}\r\nreturn;\r\nout:\r\nlocal_bh_disable();\r\nrctx->complete(&req->base, err);\r\nlocal_bh_enable();\r\n}\r\nstatic int mcryptd_hash_update_enqueue(struct ahash_request *req)\r\n{\r\nreturn mcryptd_hash_enqueue(req, mcryptd_hash_update);\r\n}\r\nstatic void mcryptd_hash_final(struct crypto_async_request *req_async, int err)\r\n{\r\nstruct ahash_request *req = ahash_request_cast(req_async);\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nif (unlikely(err == -EINPROGRESS))\r\ngoto out;\r\nrctx->out = req->result;\r\nerr = ahash_mcryptd_final(&rctx->areq);\r\nif (err) {\r\nreq->base.complete = rctx->complete;\r\ngoto out;\r\n}\r\nreturn;\r\nout:\r\nlocal_bh_disable();\r\nrctx->complete(&req->base, err);\r\nlocal_bh_enable();\r\n}\r\nstatic int mcryptd_hash_final_enqueue(struct ahash_request *req)\r\n{\r\nreturn mcryptd_hash_enqueue(req, mcryptd_hash_final);\r\n}\r\nstatic void mcryptd_hash_finup(struct crypto_async_request *req_async, int err)\r\n{\r\nstruct ahash_request *req = ahash_request_cast(req_async);\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nif (unlikely(err == -EINPROGRESS))\r\ngoto out;\r\nrctx->out = req->result;\r\nerr = ahash_mcryptd_finup(&rctx->areq);\r\nif (err) {\r\nreq->base.complete = rctx->complete;\r\ngoto out;\r\n}\r\nreturn;\r\nout:\r\nlocal_bh_disable();\r\nrctx->complete(&req->base, err);\r\nlocal_bh_enable();\r\n}\r\nstatic int mcryptd_hash_finup_enqueue(struct ahash_request *req)\r\n{\r\nreturn mcryptd_hash_enqueue(req, mcryptd_hash_finup);\r\n}\r\nstatic void mcryptd_hash_digest(struct crypto_async_request *req_async, int err)\r\n{\r\nstruct mcryptd_hash_ctx *ctx = crypto_tfm_ctx(req_async->tfm);\r\nstruct crypto_ahash *child = ctx->child;\r\nstruct ahash_request *req = ahash_request_cast(req_async);\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct ahash_request *desc = &rctx->areq;\r\nif (unlikely(err == -EINPROGRESS))\r\ngoto out;\r\nahash_request_set_tfm(desc, child);\r\nahash_request_set_callback(desc, CRYPTO_TFM_REQ_MAY_SLEEP,\r\nrctx->complete, req_async);\r\nrctx->out = req->result;\r\nerr = ahash_mcryptd_digest(desc);\r\nout:\r\nlocal_bh_disable();\r\nrctx->complete(&req->base, err);\r\nlocal_bh_enable();\r\n}\r\nstatic int mcryptd_hash_digest_enqueue(struct ahash_request *req)\r\n{\r\nreturn mcryptd_hash_enqueue(req, mcryptd_hash_digest);\r\n}\r\nstatic int mcryptd_hash_export(struct ahash_request *req, void *out)\r\n{\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nreturn crypto_ahash_export(&rctx->areq, out);\r\n}\r\nstatic int mcryptd_hash_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nreturn crypto_ahash_import(&rctx->areq, in);\r\n}\r\nstatic int mcryptd_create_hash(struct crypto_template *tmpl, struct rtattr **tb,\r\nstruct mcryptd_queue *queue)\r\n{\r\nstruct hashd_instance_ctx *ctx;\r\nstruct ahash_instance *inst;\r\nstruct hash_alg_common *halg;\r\nstruct crypto_alg *alg;\r\nu32 type = 0;\r\nu32 mask = 0;\r\nint err;\r\nif (!mcryptd_check_internal(tb, &type, &mask))\r\nreturn -EINVAL;\r\nhalg = ahash_attr_alg(tb[1], type, mask);\r\nif (IS_ERR(halg))\r\nreturn PTR_ERR(halg);\r\nalg = &halg->base;\r\npr_debug("crypto: mcryptd hash alg: %s\n", alg->cra_name);\r\ninst = mcryptd_alloc_instance(alg, ahash_instance_headroom(),\r\nsizeof(*ctx));\r\nerr = PTR_ERR(inst);\r\nif (IS_ERR(inst))\r\ngoto out_put_alg;\r\nctx = ahash_instance_ctx(inst);\r\nctx->queue = queue;\r\nerr = crypto_init_ahash_spawn(&ctx->spawn, halg,\r\nahash_crypto_instance(inst));\r\nif (err)\r\ngoto out_free_inst;\r\ntype = CRYPTO_ALG_ASYNC;\r\nif (alg->cra_flags & CRYPTO_ALG_INTERNAL)\r\ntype |= CRYPTO_ALG_INTERNAL;\r\ninst->alg.halg.base.cra_flags = type;\r\ninst->alg.halg.digestsize = halg->digestsize;\r\ninst->alg.halg.statesize = halg->statesize;\r\ninst->alg.halg.base.cra_ctxsize = sizeof(struct mcryptd_hash_ctx);\r\ninst->alg.halg.base.cra_init = mcryptd_hash_init_tfm;\r\ninst->alg.halg.base.cra_exit = mcryptd_hash_exit_tfm;\r\ninst->alg.init = mcryptd_hash_init_enqueue;\r\ninst->alg.update = mcryptd_hash_update_enqueue;\r\ninst->alg.final = mcryptd_hash_final_enqueue;\r\ninst->alg.finup = mcryptd_hash_finup_enqueue;\r\ninst->alg.export = mcryptd_hash_export;\r\ninst->alg.import = mcryptd_hash_import;\r\ninst->alg.setkey = mcryptd_hash_setkey;\r\ninst->alg.digest = mcryptd_hash_digest_enqueue;\r\nerr = ahash_register_instance(tmpl, inst);\r\nif (err) {\r\ncrypto_drop_ahash(&ctx->spawn);\r\nout_free_inst:\r\nkfree(inst);\r\n}\r\nout_put_alg:\r\ncrypto_mod_put(alg);\r\nreturn err;\r\n}\r\nstatic int mcryptd_create(struct crypto_template *tmpl, struct rtattr **tb)\r\n{\r\nstruct crypto_attr_type *algt;\r\nalgt = crypto_get_attr_type(tb);\r\nif (IS_ERR(algt))\r\nreturn PTR_ERR(algt);\r\nswitch (algt->type & algt->mask & CRYPTO_ALG_TYPE_MASK) {\r\ncase CRYPTO_ALG_TYPE_DIGEST:\r\nreturn mcryptd_create_hash(tmpl, tb, &mqueue);\r\nbreak;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic void mcryptd_free(struct crypto_instance *inst)\r\n{\r\nstruct mcryptd_instance_ctx *ctx = crypto_instance_ctx(inst);\r\nstruct hashd_instance_ctx *hctx = crypto_instance_ctx(inst);\r\nswitch (inst->alg.cra_flags & CRYPTO_ALG_TYPE_MASK) {\r\ncase CRYPTO_ALG_TYPE_AHASH:\r\ncrypto_drop_ahash(&hctx->spawn);\r\nkfree(ahash_instance(inst));\r\nreturn;\r\ndefault:\r\ncrypto_drop_spawn(&ctx->spawn);\r\nkfree(inst);\r\n}\r\n}\r\nstruct mcryptd_ahash *mcryptd_alloc_ahash(const char *alg_name,\r\nu32 type, u32 mask)\r\n{\r\nchar mcryptd_alg_name[CRYPTO_MAX_ALG_NAME];\r\nstruct crypto_ahash *tfm;\r\nif (snprintf(mcryptd_alg_name, CRYPTO_MAX_ALG_NAME,\r\n"mcryptd(%s)", alg_name) >= CRYPTO_MAX_ALG_NAME)\r\nreturn ERR_PTR(-EINVAL);\r\ntfm = crypto_alloc_ahash(mcryptd_alg_name, type, mask);\r\nif (IS_ERR(tfm))\r\nreturn ERR_CAST(tfm);\r\nif (tfm->base.__crt_alg->cra_module != THIS_MODULE) {\r\ncrypto_free_ahash(tfm);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nreturn __mcryptd_ahash_cast(tfm);\r\n}\r\nint ahash_mcryptd_digest(struct ahash_request *desc)\r\n{\r\nreturn crypto_ahash_init(desc) ?: ahash_mcryptd_finup(desc);\r\n}\r\nint ahash_mcryptd_update(struct ahash_request *desc)\r\n{\r\nreturn crypto_ahash_update(desc);\r\n}\r\nint ahash_mcryptd_finup(struct ahash_request *desc)\r\n{\r\nreturn crypto_ahash_finup(desc);\r\n}\r\nint ahash_mcryptd_final(struct ahash_request *desc)\r\n{\r\nreturn crypto_ahash_final(desc);\r\n}\r\nstruct crypto_ahash *mcryptd_ahash_child(struct mcryptd_ahash *tfm)\r\n{\r\nstruct mcryptd_hash_ctx *ctx = crypto_ahash_ctx(&tfm->base);\r\nreturn ctx->child;\r\n}\r\nstruct ahash_request *mcryptd_ahash_desc(struct ahash_request *req)\r\n{\r\nstruct mcryptd_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nreturn &rctx->areq;\r\n}\r\nvoid mcryptd_free_ahash(struct mcryptd_ahash *tfm)\r\n{\r\ncrypto_free_ahash(&tfm->base);\r\n}\r\nstatic int __init mcryptd_init(void)\r\n{\r\nint err, cpu;\r\nstruct mcryptd_flush_list *flist;\r\nmcryptd_flist = alloc_percpu(struct mcryptd_flush_list);\r\nfor_each_possible_cpu(cpu) {\r\nflist = per_cpu_ptr(mcryptd_flist, cpu);\r\nINIT_LIST_HEAD(&flist->list);\r\nmutex_init(&flist->lock);\r\n}\r\nerr = mcryptd_init_queue(&mqueue, MCRYPTD_MAX_CPU_QLEN);\r\nif (err) {\r\nfree_percpu(mcryptd_flist);\r\nreturn err;\r\n}\r\nerr = crypto_register_template(&mcryptd_tmpl);\r\nif (err) {\r\nmcryptd_fini_queue(&mqueue);\r\nfree_percpu(mcryptd_flist);\r\n}\r\nreturn err;\r\n}\r\nstatic void __exit mcryptd_exit(void)\r\n{\r\nmcryptd_fini_queue(&mqueue);\r\ncrypto_unregister_template(&mcryptd_tmpl);\r\nfree_percpu(mcryptd_flist);\r\n}
