static inline struct task_struct *dl_task_of(struct sched_dl_entity *dl_se)\r\n{\r\nreturn container_of(dl_se, struct task_struct, dl);\r\n}\r\nstatic inline struct rq *rq_of_dl_rq(struct dl_rq *dl_rq)\r\n{\r\nreturn container_of(dl_rq, struct rq, dl);\r\n}\r\nstatic inline struct dl_rq *dl_rq_of_se(struct sched_dl_entity *dl_se)\r\n{\r\nstruct task_struct *p = dl_task_of(dl_se);\r\nstruct rq *rq = task_rq(p);\r\nreturn &rq->dl;\r\n}\r\nstatic inline int on_dl_rq(struct sched_dl_entity *dl_se)\r\n{\r\nreturn !RB_EMPTY_NODE(&dl_se->rb_node);\r\n}\r\nstatic inline int is_leftmost(struct task_struct *p, struct dl_rq *dl_rq)\r\n{\r\nstruct sched_dl_entity *dl_se = &p->dl;\r\nreturn dl_rq->rb_leftmost == &dl_se->rb_node;\r\n}\r\nvoid init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime)\r\n{\r\nraw_spin_lock_init(&dl_b->dl_runtime_lock);\r\ndl_b->dl_period = period;\r\ndl_b->dl_runtime = runtime;\r\n}\r\nvoid init_dl_bw(struct dl_bw *dl_b)\r\n{\r\nraw_spin_lock_init(&dl_b->lock);\r\nraw_spin_lock(&def_dl_bandwidth.dl_runtime_lock);\r\nif (global_rt_runtime() == RUNTIME_INF)\r\ndl_b->bw = -1;\r\nelse\r\ndl_b->bw = to_ratio(global_rt_period(), global_rt_runtime());\r\nraw_spin_unlock(&def_dl_bandwidth.dl_runtime_lock);\r\ndl_b->total_bw = 0;\r\n}\r\nvoid init_dl_rq(struct dl_rq *dl_rq)\r\n{\r\ndl_rq->rb_root = RB_ROOT;\r\n#ifdef CONFIG_SMP\r\ndl_rq->earliest_dl.curr = dl_rq->earliest_dl.next = 0;\r\ndl_rq->dl_nr_migratory = 0;\r\ndl_rq->overloaded = 0;\r\ndl_rq->pushable_dl_tasks_root = RB_ROOT;\r\n#else\r\ninit_dl_bw(&dl_rq->dl_bw);\r\n#endif\r\n}\r\nstatic inline int dl_overloaded(struct rq *rq)\r\n{\r\nreturn atomic_read(&rq->rd->dlo_count);\r\n}\r\nstatic inline void dl_set_overload(struct rq *rq)\r\n{\r\nif (!rq->online)\r\nreturn;\r\ncpumask_set_cpu(rq->cpu, rq->rd->dlo_mask);\r\nsmp_wmb();\r\natomic_inc(&rq->rd->dlo_count);\r\n}\r\nstatic inline void dl_clear_overload(struct rq *rq)\r\n{\r\nif (!rq->online)\r\nreturn;\r\natomic_dec(&rq->rd->dlo_count);\r\ncpumask_clear_cpu(rq->cpu, rq->rd->dlo_mask);\r\n}\r\nstatic void update_dl_migration(struct dl_rq *dl_rq)\r\n{\r\nif (dl_rq->dl_nr_migratory && dl_rq->dl_nr_running > 1) {\r\nif (!dl_rq->overloaded) {\r\ndl_set_overload(rq_of_dl_rq(dl_rq));\r\ndl_rq->overloaded = 1;\r\n}\r\n} else if (dl_rq->overloaded) {\r\ndl_clear_overload(rq_of_dl_rq(dl_rq));\r\ndl_rq->overloaded = 0;\r\n}\r\n}\r\nstatic void inc_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)\r\n{\r\nstruct task_struct *p = dl_task_of(dl_se);\r\nif (p->nr_cpus_allowed > 1)\r\ndl_rq->dl_nr_migratory++;\r\nupdate_dl_migration(dl_rq);\r\n}\r\nstatic void dec_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)\r\n{\r\nstruct task_struct *p = dl_task_of(dl_se);\r\nif (p->nr_cpus_allowed > 1)\r\ndl_rq->dl_nr_migratory--;\r\nupdate_dl_migration(dl_rq);\r\n}\r\nstatic void enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)\r\n{\r\nstruct dl_rq *dl_rq = &rq->dl;\r\nstruct rb_node **link = &dl_rq->pushable_dl_tasks_root.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct task_struct *entry;\r\nint leftmost = 1;\r\nBUG_ON(!RB_EMPTY_NODE(&p->pushable_dl_tasks));\r\nwhile (*link) {\r\nparent = *link;\r\nentry = rb_entry(parent, struct task_struct,\r\npushable_dl_tasks);\r\nif (dl_entity_preempt(&p->dl, &entry->dl))\r\nlink = &parent->rb_left;\r\nelse {\r\nlink = &parent->rb_right;\r\nleftmost = 0;\r\n}\r\n}\r\nif (leftmost) {\r\ndl_rq->pushable_dl_tasks_leftmost = &p->pushable_dl_tasks;\r\ndl_rq->earliest_dl.next = p->dl.deadline;\r\n}\r\nrb_link_node(&p->pushable_dl_tasks, parent, link);\r\nrb_insert_color(&p->pushable_dl_tasks, &dl_rq->pushable_dl_tasks_root);\r\n}\r\nstatic void dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)\r\n{\r\nstruct dl_rq *dl_rq = &rq->dl;\r\nif (RB_EMPTY_NODE(&p->pushable_dl_tasks))\r\nreturn;\r\nif (dl_rq->pushable_dl_tasks_leftmost == &p->pushable_dl_tasks) {\r\nstruct rb_node *next_node;\r\nnext_node = rb_next(&p->pushable_dl_tasks);\r\ndl_rq->pushable_dl_tasks_leftmost = next_node;\r\nif (next_node) {\r\ndl_rq->earliest_dl.next = rb_entry(next_node,\r\nstruct task_struct, pushable_dl_tasks)->dl.deadline;\r\n}\r\n}\r\nrb_erase(&p->pushable_dl_tasks, &dl_rq->pushable_dl_tasks_root);\r\nRB_CLEAR_NODE(&p->pushable_dl_tasks);\r\n}\r\nstatic inline int has_pushable_dl_tasks(struct rq *rq)\r\n{\r\nreturn !RB_EMPTY_ROOT(&rq->dl.pushable_dl_tasks_root);\r\n}\r\nstatic inline bool need_pull_dl_task(struct rq *rq, struct task_struct *prev)\r\n{\r\nreturn dl_task(prev);\r\n}\r\nstatic inline void queue_push_tasks(struct rq *rq)\r\n{\r\nif (!has_pushable_dl_tasks(rq))\r\nreturn;\r\nqueue_balance_callback(rq, &per_cpu(dl_push_head, rq->cpu), push_dl_tasks);\r\n}\r\nstatic inline void queue_pull_task(struct rq *rq)\r\n{\r\nqueue_balance_callback(rq, &per_cpu(dl_pull_head, rq->cpu), pull_dl_task);\r\n}\r\nstatic struct rq *dl_task_offline_migration(struct rq *rq, struct task_struct *p)\r\n{\r\nstruct rq *later_rq = NULL;\r\nlater_rq = find_lock_later_rq(p, rq);\r\nif (!later_rq) {\r\nint cpu;\r\ncpu = cpumask_any_and(cpu_active_mask, &p->cpus_allowed);\r\nif (cpu >= nr_cpu_ids) {\r\nBUG_ON(dl_bandwidth_enabled());\r\ncpu = cpumask_any(cpu_active_mask);\r\n}\r\nlater_rq = cpu_rq(cpu);\r\ndouble_lock_balance(rq, later_rq);\r\n}\r\nset_task_cpu(p, later_rq->cpu);\r\ndouble_unlock_balance(later_rq, rq);\r\nreturn later_rq;\r\n}\r\nstatic inline\r\nvoid enqueue_pushable_dl_task(struct rq *rq, struct task_struct *p)\r\n{\r\n}\r\nstatic inline\r\nvoid dequeue_pushable_dl_task(struct rq *rq, struct task_struct *p)\r\n{\r\n}\r\nstatic inline\r\nvoid inc_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)\r\n{\r\n}\r\nstatic inline\r\nvoid dec_dl_migration(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)\r\n{\r\n}\r\nstatic inline bool need_pull_dl_task(struct rq *rq, struct task_struct *prev)\r\n{\r\nreturn false;\r\n}\r\nstatic inline void pull_dl_task(struct rq *rq)\r\n{\r\n}\r\nstatic inline void queue_push_tasks(struct rq *rq)\r\n{\r\n}\r\nstatic inline void queue_pull_task(struct rq *rq)\r\n{\r\n}\r\nstatic inline void setup_new_dl_entity(struct sched_dl_entity *dl_se)\r\n{\r\nstruct dl_rq *dl_rq = dl_rq_of_se(dl_se);\r\nstruct rq *rq = rq_of_dl_rq(dl_rq);\r\nWARN_ON(dl_se->dl_boosted);\r\nWARN_ON(dl_time_before(rq_clock(rq), dl_se->deadline));\r\nif (dl_se->dl_throttled)\r\nreturn;\r\ndl_se->deadline = rq_clock(rq) + dl_se->dl_deadline;\r\ndl_se->runtime = dl_se->dl_runtime;\r\n}\r\nstatic void replenish_dl_entity(struct sched_dl_entity *dl_se,\r\nstruct sched_dl_entity *pi_se)\r\n{\r\nstruct dl_rq *dl_rq = dl_rq_of_se(dl_se);\r\nstruct rq *rq = rq_of_dl_rq(dl_rq);\r\nBUG_ON(pi_se->dl_runtime <= 0);\r\nif (dl_se->dl_deadline == 0) {\r\ndl_se->deadline = rq_clock(rq) + pi_se->dl_deadline;\r\ndl_se->runtime = pi_se->dl_runtime;\r\n}\r\nif (dl_se->dl_yielded && dl_se->runtime > 0)\r\ndl_se->runtime = 0;\r\nwhile (dl_se->runtime <= 0) {\r\ndl_se->deadline += pi_se->dl_period;\r\ndl_se->runtime += pi_se->dl_runtime;\r\n}\r\nif (dl_time_before(dl_se->deadline, rq_clock(rq))) {\r\nprintk_deferred_once("sched: DL replenish lagged too much\n");\r\ndl_se->deadline = rq_clock(rq) + pi_se->dl_deadline;\r\ndl_se->runtime = pi_se->dl_runtime;\r\n}\r\nif (dl_se->dl_yielded)\r\ndl_se->dl_yielded = 0;\r\nif (dl_se->dl_throttled)\r\ndl_se->dl_throttled = 0;\r\n}\r\nstatic bool dl_entity_overflow(struct sched_dl_entity *dl_se,\r\nstruct sched_dl_entity *pi_se, u64 t)\r\n{\r\nu64 left, right;\r\nleft = (pi_se->dl_deadline >> DL_SCALE) * (dl_se->runtime >> DL_SCALE);\r\nright = ((dl_se->deadline - t) >> DL_SCALE) *\r\n(pi_se->dl_runtime >> DL_SCALE);\r\nreturn dl_time_before(right, left);\r\n}\r\nstatic void update_dl_entity(struct sched_dl_entity *dl_se,\r\nstruct sched_dl_entity *pi_se)\r\n{\r\nstruct dl_rq *dl_rq = dl_rq_of_se(dl_se);\r\nstruct rq *rq = rq_of_dl_rq(dl_rq);\r\nif (dl_time_before(dl_se->deadline, rq_clock(rq)) ||\r\ndl_entity_overflow(dl_se, pi_se, rq_clock(rq))) {\r\ndl_se->deadline = rq_clock(rq) + pi_se->dl_deadline;\r\ndl_se->runtime = pi_se->dl_runtime;\r\n}\r\n}\r\nstatic inline u64 dl_next_period(struct sched_dl_entity *dl_se)\r\n{\r\nreturn dl_se->deadline - dl_se->dl_deadline + dl_se->dl_period;\r\n}\r\nstatic int start_dl_timer(struct task_struct *p)\r\n{\r\nstruct sched_dl_entity *dl_se = &p->dl;\r\nstruct hrtimer *timer = &dl_se->dl_timer;\r\nstruct rq *rq = task_rq(p);\r\nktime_t now, act;\r\ns64 delta;\r\nlockdep_assert_held(&rq->lock);\r\nact = ns_to_ktime(dl_next_period(dl_se));\r\nnow = hrtimer_cb_get_time(timer);\r\ndelta = ktime_to_ns(now) - rq_clock(rq);\r\nact = ktime_add_ns(act, delta);\r\nif (ktime_us_delta(act, now) < 0)\r\nreturn 0;\r\nif (!hrtimer_is_queued(timer)) {\r\nget_task_struct(p);\r\nhrtimer_start(timer, act, HRTIMER_MODE_ABS);\r\n}\r\nreturn 1;\r\n}\r\nstatic enum hrtimer_restart dl_task_timer(struct hrtimer *timer)\r\n{\r\nstruct sched_dl_entity *dl_se = container_of(timer,\r\nstruct sched_dl_entity,\r\ndl_timer);\r\nstruct task_struct *p = dl_task_of(dl_se);\r\nstruct rq_flags rf;\r\nstruct rq *rq;\r\nrq = task_rq_lock(p, &rf);\r\nif (!dl_task(p)) {\r\n__dl_clear_params(p);\r\ngoto unlock;\r\n}\r\nif (dl_se->dl_boosted)\r\ngoto unlock;\r\nif (!dl_se->dl_throttled)\r\ngoto unlock;\r\nsched_clock_tick();\r\nupdate_rq_clock(rq);\r\nif (!task_on_rq_queued(p)) {\r\nreplenish_dl_entity(dl_se, dl_se);\r\ngoto unlock;\r\n}\r\n#ifdef CONFIG_SMP\r\nif (unlikely(!rq->online)) {\r\nlockdep_unpin_lock(&rq->lock, rf.cookie);\r\nrq = dl_task_offline_migration(rq, p);\r\nrf.cookie = lockdep_pin_lock(&rq->lock);\r\nupdate_rq_clock(rq);\r\n}\r\n#endif\r\nenqueue_task_dl(rq, p, ENQUEUE_REPLENISH);\r\nif (dl_task(rq->curr))\r\ncheck_preempt_curr_dl(rq, p, 0);\r\nelse\r\nresched_curr(rq);\r\n#ifdef CONFIG_SMP\r\nif (has_pushable_dl_tasks(rq)) {\r\nrq_unpin_lock(rq, &rf);\r\npush_dl_task(rq);\r\nrq_repin_lock(rq, &rf);\r\n}\r\n#endif\r\nunlock:\r\ntask_rq_unlock(rq, p, &rf);\r\nput_task_struct(p);\r\nreturn HRTIMER_NORESTART;\r\n}\r\nvoid init_dl_task_timer(struct sched_dl_entity *dl_se)\r\n{\r\nstruct hrtimer *timer = &dl_se->dl_timer;\r\nhrtimer_init(timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\ntimer->function = dl_task_timer;\r\n}\r\nstatic inline void dl_check_constrained_dl(struct sched_dl_entity *dl_se)\r\n{\r\nstruct task_struct *p = dl_task_of(dl_se);\r\nstruct rq *rq = rq_of_dl_rq(dl_rq_of_se(dl_se));\r\nif (dl_time_before(dl_se->deadline, rq_clock(rq)) &&\r\ndl_time_before(rq_clock(rq), dl_next_period(dl_se))) {\r\nif (unlikely(dl_se->dl_boosted || !start_dl_timer(p)))\r\nreturn;\r\ndl_se->dl_throttled = 1;\r\n}\r\n}\r\nstatic\r\nint dl_runtime_exceeded(struct sched_dl_entity *dl_se)\r\n{\r\nreturn (dl_se->runtime <= 0);\r\n}\r\nstatic void update_curr_dl(struct rq *rq)\r\n{\r\nstruct task_struct *curr = rq->curr;\r\nstruct sched_dl_entity *dl_se = &curr->dl;\r\nu64 delta_exec;\r\nif (!dl_task(curr) || !on_dl_rq(dl_se))\r\nreturn;\r\ndelta_exec = rq_clock_task(rq) - curr->se.exec_start;\r\nif (unlikely((s64)delta_exec <= 0)) {\r\nif (unlikely(dl_se->dl_yielded))\r\ngoto throttle;\r\nreturn;\r\n}\r\ncpufreq_update_this_cpu(rq, SCHED_CPUFREQ_DL);\r\nschedstat_set(curr->se.statistics.exec_max,\r\nmax(curr->se.statistics.exec_max, delta_exec));\r\ncurr->se.sum_exec_runtime += delta_exec;\r\naccount_group_exec_runtime(curr, delta_exec);\r\ncurr->se.exec_start = rq_clock_task(rq);\r\ncpuacct_charge(curr, delta_exec);\r\nsched_rt_avg_update(rq, delta_exec);\r\ndl_se->runtime -= delta_exec;\r\nthrottle:\r\nif (dl_runtime_exceeded(dl_se) || dl_se->dl_yielded) {\r\ndl_se->dl_throttled = 1;\r\n__dequeue_task_dl(rq, curr, 0);\r\nif (unlikely(dl_se->dl_boosted || !start_dl_timer(curr)))\r\nenqueue_task_dl(rq, curr, ENQUEUE_REPLENISH);\r\nif (!is_leftmost(curr, &rq->dl))\r\nresched_curr(rq);\r\n}\r\nif (rt_bandwidth_enabled()) {\r\nstruct rt_rq *rt_rq = &rq->rt;\r\nraw_spin_lock(&rt_rq->rt_runtime_lock);\r\nif (sched_rt_bandwidth_account(rt_rq))\r\nrt_rq->rt_time += delta_exec;\r\nraw_spin_unlock(&rt_rq->rt_runtime_lock);\r\n}\r\n}\r\nstatic void inc_dl_deadline(struct dl_rq *dl_rq, u64 deadline)\r\n{\r\nstruct rq *rq = rq_of_dl_rq(dl_rq);\r\nif (dl_rq->earliest_dl.curr == 0 ||\r\ndl_time_before(deadline, dl_rq->earliest_dl.curr)) {\r\ndl_rq->earliest_dl.curr = deadline;\r\ncpudl_set(&rq->rd->cpudl, rq->cpu, deadline);\r\n}\r\n}\r\nstatic void dec_dl_deadline(struct dl_rq *dl_rq, u64 deadline)\r\n{\r\nstruct rq *rq = rq_of_dl_rq(dl_rq);\r\nif (!dl_rq->dl_nr_running) {\r\ndl_rq->earliest_dl.curr = 0;\r\ndl_rq->earliest_dl.next = 0;\r\ncpudl_clear(&rq->rd->cpudl, rq->cpu);\r\n} else {\r\nstruct rb_node *leftmost = dl_rq->rb_leftmost;\r\nstruct sched_dl_entity *entry;\r\nentry = rb_entry(leftmost, struct sched_dl_entity, rb_node);\r\ndl_rq->earliest_dl.curr = entry->deadline;\r\ncpudl_set(&rq->rd->cpudl, rq->cpu, entry->deadline);\r\n}\r\n}\r\nstatic inline void inc_dl_deadline(struct dl_rq *dl_rq, u64 deadline) {}\r\nstatic inline void dec_dl_deadline(struct dl_rq *dl_rq, u64 deadline) {}\r\nstatic inline\r\nvoid inc_dl_tasks(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)\r\n{\r\nint prio = dl_task_of(dl_se)->prio;\r\nu64 deadline = dl_se->deadline;\r\nWARN_ON(!dl_prio(prio));\r\ndl_rq->dl_nr_running++;\r\nadd_nr_running(rq_of_dl_rq(dl_rq), 1);\r\ninc_dl_deadline(dl_rq, deadline);\r\ninc_dl_migration(dl_se, dl_rq);\r\n}\r\nstatic inline\r\nvoid dec_dl_tasks(struct sched_dl_entity *dl_se, struct dl_rq *dl_rq)\r\n{\r\nint prio = dl_task_of(dl_se)->prio;\r\nWARN_ON(!dl_prio(prio));\r\nWARN_ON(!dl_rq->dl_nr_running);\r\ndl_rq->dl_nr_running--;\r\nsub_nr_running(rq_of_dl_rq(dl_rq), 1);\r\ndec_dl_deadline(dl_rq, dl_se->deadline);\r\ndec_dl_migration(dl_se, dl_rq);\r\n}\r\nstatic void __enqueue_dl_entity(struct sched_dl_entity *dl_se)\r\n{\r\nstruct dl_rq *dl_rq = dl_rq_of_se(dl_se);\r\nstruct rb_node **link = &dl_rq->rb_root.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct sched_dl_entity *entry;\r\nint leftmost = 1;\r\nBUG_ON(!RB_EMPTY_NODE(&dl_se->rb_node));\r\nwhile (*link) {\r\nparent = *link;\r\nentry = rb_entry(parent, struct sched_dl_entity, rb_node);\r\nif (dl_time_before(dl_se->deadline, entry->deadline))\r\nlink = &parent->rb_left;\r\nelse {\r\nlink = &parent->rb_right;\r\nleftmost = 0;\r\n}\r\n}\r\nif (leftmost)\r\ndl_rq->rb_leftmost = &dl_se->rb_node;\r\nrb_link_node(&dl_se->rb_node, parent, link);\r\nrb_insert_color(&dl_se->rb_node, &dl_rq->rb_root);\r\ninc_dl_tasks(dl_se, dl_rq);\r\n}\r\nstatic void __dequeue_dl_entity(struct sched_dl_entity *dl_se)\r\n{\r\nstruct dl_rq *dl_rq = dl_rq_of_se(dl_se);\r\nif (RB_EMPTY_NODE(&dl_se->rb_node))\r\nreturn;\r\nif (dl_rq->rb_leftmost == &dl_se->rb_node) {\r\nstruct rb_node *next_node;\r\nnext_node = rb_next(&dl_se->rb_node);\r\ndl_rq->rb_leftmost = next_node;\r\n}\r\nrb_erase(&dl_se->rb_node, &dl_rq->rb_root);\r\nRB_CLEAR_NODE(&dl_se->rb_node);\r\ndec_dl_tasks(dl_se, dl_rq);\r\n}\r\nstatic void\r\nenqueue_dl_entity(struct sched_dl_entity *dl_se,\r\nstruct sched_dl_entity *pi_se, int flags)\r\n{\r\nBUG_ON(on_dl_rq(dl_se));\r\nif (flags & ENQUEUE_WAKEUP)\r\nupdate_dl_entity(dl_se, pi_se);\r\nelse if (flags & ENQUEUE_REPLENISH)\r\nreplenish_dl_entity(dl_se, pi_se);\r\n__enqueue_dl_entity(dl_se);\r\n}\r\nstatic void dequeue_dl_entity(struct sched_dl_entity *dl_se)\r\n{\r\n__dequeue_dl_entity(dl_se);\r\n}\r\nstatic inline bool dl_is_constrained(struct sched_dl_entity *dl_se)\r\n{\r\nreturn dl_se->dl_deadline < dl_se->dl_period;\r\n}\r\nstatic void enqueue_task_dl(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nstruct task_struct *pi_task = rt_mutex_get_top_task(p);\r\nstruct sched_dl_entity *pi_se = &p->dl;\r\nif (pi_task && p->dl.dl_boosted && dl_prio(pi_task->normal_prio)) {\r\npi_se = &pi_task->dl;\r\n} else if (!dl_prio(p->normal_prio)) {\r\nBUG_ON(!p->dl.dl_boosted || flags != ENQUEUE_REPLENISH);\r\nreturn;\r\n}\r\nif (!p->dl.dl_throttled && dl_is_constrained(&p->dl))\r\ndl_check_constrained_dl(&p->dl);\r\nif (p->dl.dl_throttled && !(flags & ENQUEUE_REPLENISH))\r\nreturn;\r\nenqueue_dl_entity(&p->dl, pi_se, flags);\r\nif (!task_current(rq, p) && p->nr_cpus_allowed > 1)\r\nenqueue_pushable_dl_task(rq, p);\r\n}\r\nstatic void __dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\ndequeue_dl_entity(&p->dl);\r\ndequeue_pushable_dl_task(rq, p);\r\n}\r\nstatic void dequeue_task_dl(struct rq *rq, struct task_struct *p, int flags)\r\n{\r\nupdate_curr_dl(rq);\r\n__dequeue_task_dl(rq, p, flags);\r\n}\r\nstatic void yield_task_dl(struct rq *rq)\r\n{\r\nrq->curr->dl.dl_yielded = 1;\r\nupdate_rq_clock(rq);\r\nupdate_curr_dl(rq);\r\nrq_clock_skip_update(rq, true);\r\n}\r\nstatic int\r\nselect_task_rq_dl(struct task_struct *p, int cpu, int sd_flag, int flags)\r\n{\r\nstruct task_struct *curr;\r\nstruct rq *rq;\r\nif (sd_flag != SD_BALANCE_WAKE)\r\ngoto out;\r\nrq = cpu_rq(cpu);\r\nrcu_read_lock();\r\ncurr = READ_ONCE(rq->curr);\r\nif (unlikely(dl_task(curr)) &&\r\n(curr->nr_cpus_allowed < 2 ||\r\n!dl_entity_preempt(&p->dl, &curr->dl)) &&\r\n(p->nr_cpus_allowed > 1)) {\r\nint target = find_later_rq(p);\r\nif (target != -1 &&\r\n(dl_time_before(p->dl.deadline,\r\ncpu_rq(target)->dl.earliest_dl.curr) ||\r\n(cpu_rq(target)->dl.dl_nr_running == 0)))\r\ncpu = target;\r\n}\r\nrcu_read_unlock();\r\nout:\r\nreturn cpu;\r\n}\r\nstatic void check_preempt_equal_dl(struct rq *rq, struct task_struct *p)\r\n{\r\nif (rq->curr->nr_cpus_allowed == 1 ||\r\ncpudl_find(&rq->rd->cpudl, rq->curr, NULL) == -1)\r\nreturn;\r\nif (p->nr_cpus_allowed != 1 &&\r\ncpudl_find(&rq->rd->cpudl, p, NULL) != -1)\r\nreturn;\r\nresched_curr(rq);\r\n}\r\nstatic void check_preempt_curr_dl(struct rq *rq, struct task_struct *p,\r\nint flags)\r\n{\r\nif (dl_entity_preempt(&p->dl, &rq->curr->dl)) {\r\nresched_curr(rq);\r\nreturn;\r\n}\r\n#ifdef CONFIG_SMP\r\nif ((p->dl.deadline == rq->curr->dl.deadline) &&\r\n!test_tsk_need_resched(rq->curr))\r\ncheck_preempt_equal_dl(rq, p);\r\n#endif\r\n}\r\nstatic void start_hrtick_dl(struct rq *rq, struct task_struct *p)\r\n{\r\nhrtick_start(rq, p->dl.runtime);\r\n}\r\nstatic void start_hrtick_dl(struct rq *rq, struct task_struct *p)\r\n{\r\n}\r\nstatic struct sched_dl_entity *pick_next_dl_entity(struct rq *rq,\r\nstruct dl_rq *dl_rq)\r\n{\r\nstruct rb_node *left = dl_rq->rb_leftmost;\r\nif (!left)\r\nreturn NULL;\r\nreturn rb_entry(left, struct sched_dl_entity, rb_node);\r\n}\r\nstruct task_struct *\r\npick_next_task_dl(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\r\n{\r\nstruct sched_dl_entity *dl_se;\r\nstruct task_struct *p;\r\nstruct dl_rq *dl_rq;\r\ndl_rq = &rq->dl;\r\nif (need_pull_dl_task(rq, prev)) {\r\nrq_unpin_lock(rq, rf);\r\npull_dl_task(rq);\r\nrq_repin_lock(rq, rf);\r\nif (rq->stop && task_on_rq_queued(rq->stop))\r\nreturn RETRY_TASK;\r\n}\r\nif (prev->sched_class == &dl_sched_class)\r\nupdate_curr_dl(rq);\r\nif (unlikely(!dl_rq->dl_nr_running))\r\nreturn NULL;\r\nput_prev_task(rq, prev);\r\ndl_se = pick_next_dl_entity(rq, dl_rq);\r\nBUG_ON(!dl_se);\r\np = dl_task_of(dl_se);\r\np->se.exec_start = rq_clock_task(rq);\r\ndequeue_pushable_dl_task(rq, p);\r\nif (hrtick_enabled(rq))\r\nstart_hrtick_dl(rq, p);\r\nqueue_push_tasks(rq);\r\nreturn p;\r\n}\r\nstatic void put_prev_task_dl(struct rq *rq, struct task_struct *p)\r\n{\r\nupdate_curr_dl(rq);\r\nif (on_dl_rq(&p->dl) && p->nr_cpus_allowed > 1)\r\nenqueue_pushable_dl_task(rq, p);\r\n}\r\nstatic void task_tick_dl(struct rq *rq, struct task_struct *p, int queued)\r\n{\r\nupdate_curr_dl(rq);\r\nif (hrtick_enabled(rq) && queued && p->dl.runtime > 0 &&\r\nis_leftmost(p, &rq->dl))\r\nstart_hrtick_dl(rq, p);\r\n}\r\nstatic void task_fork_dl(struct task_struct *p)\r\n{\r\n}\r\nstatic void task_dead_dl(struct task_struct *p)\r\n{\r\nstruct dl_bw *dl_b = dl_bw_of(task_cpu(p));\r\nraw_spin_lock_irq(&dl_b->lock);\r\ndl_b->total_bw -= p->dl.dl_bw;\r\nraw_spin_unlock_irq(&dl_b->lock);\r\n}\r\nstatic void set_curr_task_dl(struct rq *rq)\r\n{\r\nstruct task_struct *p = rq->curr;\r\np->se.exec_start = rq_clock_task(rq);\r\ndequeue_pushable_dl_task(rq, p);\r\n}\r\nstatic int pick_dl_task(struct rq *rq, struct task_struct *p, int cpu)\r\n{\r\nif (!task_running(rq, p) &&\r\ncpumask_test_cpu(cpu, &p->cpus_allowed))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic struct task_struct *pick_earliest_pushable_dl_task(struct rq *rq, int cpu)\r\n{\r\nstruct rb_node *next_node = rq->dl.pushable_dl_tasks_leftmost;\r\nstruct task_struct *p = NULL;\r\nif (!has_pushable_dl_tasks(rq))\r\nreturn NULL;\r\nnext_node:\r\nif (next_node) {\r\np = rb_entry(next_node, struct task_struct, pushable_dl_tasks);\r\nif (pick_dl_task(rq, p, cpu))\r\nreturn p;\r\nnext_node = rb_next(next_node);\r\ngoto next_node;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int find_later_rq(struct task_struct *task)\r\n{\r\nstruct sched_domain *sd;\r\nstruct cpumask *later_mask = this_cpu_cpumask_var_ptr(local_cpu_mask_dl);\r\nint this_cpu = smp_processor_id();\r\nint best_cpu, cpu = task_cpu(task);\r\nif (unlikely(!later_mask))\r\nreturn -1;\r\nif (task->nr_cpus_allowed == 1)\r\nreturn -1;\r\nbest_cpu = cpudl_find(&task_rq(task)->rd->cpudl,\r\ntask, later_mask);\r\nif (best_cpu == -1)\r\nreturn -1;\r\nif (cpumask_test_cpu(cpu, later_mask))\r\nreturn cpu;\r\nif (!cpumask_test_cpu(this_cpu, later_mask))\r\nthis_cpu = -1;\r\nrcu_read_lock();\r\nfor_each_domain(cpu, sd) {\r\nif (sd->flags & SD_WAKE_AFFINE) {\r\nif (this_cpu != -1 &&\r\ncpumask_test_cpu(this_cpu, sched_domain_span(sd))) {\r\nrcu_read_unlock();\r\nreturn this_cpu;\r\n}\r\nif (best_cpu < nr_cpu_ids &&\r\ncpumask_test_cpu(best_cpu, sched_domain_span(sd))) {\r\nrcu_read_unlock();\r\nreturn best_cpu;\r\n}\r\n}\r\n}\r\nrcu_read_unlock();\r\nif (this_cpu != -1)\r\nreturn this_cpu;\r\ncpu = cpumask_any(later_mask);\r\nif (cpu < nr_cpu_ids)\r\nreturn cpu;\r\nreturn -1;\r\n}\r\nstatic struct rq *find_lock_later_rq(struct task_struct *task, struct rq *rq)\r\n{\r\nstruct rq *later_rq = NULL;\r\nint tries;\r\nint cpu;\r\nfor (tries = 0; tries < DL_MAX_TRIES; tries++) {\r\ncpu = find_later_rq(task);\r\nif ((cpu == -1) || (cpu == rq->cpu))\r\nbreak;\r\nlater_rq = cpu_rq(cpu);\r\nif (later_rq->dl.dl_nr_running &&\r\n!dl_time_before(task->dl.deadline,\r\nlater_rq->dl.earliest_dl.curr)) {\r\nlater_rq = NULL;\r\nbreak;\r\n}\r\nif (double_lock_balance(rq, later_rq)) {\r\nif (unlikely(task_rq(task) != rq ||\r\n!cpumask_test_cpu(later_rq->cpu, &task->cpus_allowed) ||\r\ntask_running(rq, task) ||\r\n!dl_task(task) ||\r\n!task_on_rq_queued(task))) {\r\ndouble_unlock_balance(rq, later_rq);\r\nlater_rq = NULL;\r\nbreak;\r\n}\r\n}\r\nif (!later_rq->dl.dl_nr_running ||\r\ndl_time_before(task->dl.deadline,\r\nlater_rq->dl.earliest_dl.curr))\r\nbreak;\r\ndouble_unlock_balance(rq, later_rq);\r\nlater_rq = NULL;\r\n}\r\nreturn later_rq;\r\n}\r\nstatic struct task_struct *pick_next_pushable_dl_task(struct rq *rq)\r\n{\r\nstruct task_struct *p;\r\nif (!has_pushable_dl_tasks(rq))\r\nreturn NULL;\r\np = rb_entry(rq->dl.pushable_dl_tasks_leftmost,\r\nstruct task_struct, pushable_dl_tasks);\r\nBUG_ON(rq->cpu != task_cpu(p));\r\nBUG_ON(task_current(rq, p));\r\nBUG_ON(p->nr_cpus_allowed <= 1);\r\nBUG_ON(!task_on_rq_queued(p));\r\nBUG_ON(!dl_task(p));\r\nreturn p;\r\n}\r\nstatic int push_dl_task(struct rq *rq)\r\n{\r\nstruct task_struct *next_task;\r\nstruct rq *later_rq;\r\nint ret = 0;\r\nif (!rq->dl.overloaded)\r\nreturn 0;\r\nnext_task = pick_next_pushable_dl_task(rq);\r\nif (!next_task)\r\nreturn 0;\r\nretry:\r\nif (unlikely(next_task == rq->curr)) {\r\nWARN_ON(1);\r\nreturn 0;\r\n}\r\nif (dl_task(rq->curr) &&\r\ndl_time_before(next_task->dl.deadline, rq->curr->dl.deadline) &&\r\nrq->curr->nr_cpus_allowed > 1) {\r\nresched_curr(rq);\r\nreturn 0;\r\n}\r\nget_task_struct(next_task);\r\nlater_rq = find_lock_later_rq(next_task, rq);\r\nif (!later_rq) {\r\nstruct task_struct *task;\r\ntask = pick_next_pushable_dl_task(rq);\r\nif (task_cpu(next_task) == rq->cpu && task == next_task) {\r\ngoto out;\r\n}\r\nif (!task)\r\ngoto out;\r\nput_task_struct(next_task);\r\nnext_task = task;\r\ngoto retry;\r\n}\r\ndeactivate_task(rq, next_task, 0);\r\nset_task_cpu(next_task, later_rq->cpu);\r\nactivate_task(later_rq, next_task, 0);\r\nret = 1;\r\nresched_curr(later_rq);\r\ndouble_unlock_balance(rq, later_rq);\r\nout:\r\nput_task_struct(next_task);\r\nreturn ret;\r\n}\r\nstatic void push_dl_tasks(struct rq *rq)\r\n{\r\nwhile (push_dl_task(rq))\r\n;\r\n}\r\nstatic void pull_dl_task(struct rq *this_rq)\r\n{\r\nint this_cpu = this_rq->cpu, cpu;\r\nstruct task_struct *p;\r\nbool resched = false;\r\nstruct rq *src_rq;\r\nu64 dmin = LONG_MAX;\r\nif (likely(!dl_overloaded(this_rq)))\r\nreturn;\r\nsmp_rmb();\r\nfor_each_cpu(cpu, this_rq->rd->dlo_mask) {\r\nif (this_cpu == cpu)\r\ncontinue;\r\nsrc_rq = cpu_rq(cpu);\r\nif (this_rq->dl.dl_nr_running &&\r\ndl_time_before(this_rq->dl.earliest_dl.curr,\r\nsrc_rq->dl.earliest_dl.next))\r\ncontinue;\r\ndouble_lock_balance(this_rq, src_rq);\r\nif (src_rq->dl.dl_nr_running <= 1)\r\ngoto skip;\r\np = pick_earliest_pushable_dl_task(src_rq, this_cpu);\r\nif (p && dl_time_before(p->dl.deadline, dmin) &&\r\n(!this_rq->dl.dl_nr_running ||\r\ndl_time_before(p->dl.deadline,\r\nthis_rq->dl.earliest_dl.curr))) {\r\nWARN_ON(p == src_rq->curr);\r\nWARN_ON(!task_on_rq_queued(p));\r\nif (dl_time_before(p->dl.deadline,\r\nsrc_rq->curr->dl.deadline))\r\ngoto skip;\r\nresched = true;\r\ndeactivate_task(src_rq, p, 0);\r\nset_task_cpu(p, this_cpu);\r\nactivate_task(this_rq, p, 0);\r\ndmin = p->dl.deadline;\r\n}\r\nskip:\r\ndouble_unlock_balance(this_rq, src_rq);\r\n}\r\nif (resched)\r\nresched_curr(this_rq);\r\n}\r\nstatic void task_woken_dl(struct rq *rq, struct task_struct *p)\r\n{\r\nif (!task_running(rq, p) &&\r\n!test_tsk_need_resched(rq->curr) &&\r\np->nr_cpus_allowed > 1 &&\r\ndl_task(rq->curr) &&\r\n(rq->curr->nr_cpus_allowed < 2 ||\r\n!dl_entity_preempt(&p->dl, &rq->curr->dl))) {\r\npush_dl_tasks(rq);\r\n}\r\n}\r\nstatic void set_cpus_allowed_dl(struct task_struct *p,\r\nconst struct cpumask *new_mask)\r\n{\r\nstruct root_domain *src_rd;\r\nstruct rq *rq;\r\nBUG_ON(!dl_task(p));\r\nrq = task_rq(p);\r\nsrc_rd = rq->rd;\r\nif (!cpumask_intersects(src_rd->span, new_mask)) {\r\nstruct dl_bw *src_dl_b;\r\nsrc_dl_b = dl_bw_of(cpu_of(rq));\r\nraw_spin_lock(&src_dl_b->lock);\r\n__dl_clear(src_dl_b, p->dl.dl_bw);\r\nraw_spin_unlock(&src_dl_b->lock);\r\n}\r\nset_cpus_allowed_common(p, new_mask);\r\n}\r\nstatic void rq_online_dl(struct rq *rq)\r\n{\r\nif (rq->dl.overloaded)\r\ndl_set_overload(rq);\r\ncpudl_set_freecpu(&rq->rd->cpudl, rq->cpu);\r\nif (rq->dl.dl_nr_running > 0)\r\ncpudl_set(&rq->rd->cpudl, rq->cpu, rq->dl.earliest_dl.curr);\r\n}\r\nstatic void rq_offline_dl(struct rq *rq)\r\n{\r\nif (rq->dl.overloaded)\r\ndl_clear_overload(rq);\r\ncpudl_clear(&rq->rd->cpudl, rq->cpu);\r\ncpudl_clear_freecpu(&rq->rd->cpudl, rq->cpu);\r\n}\r\nvoid __init init_sched_dl_class(void)\r\n{\r\nunsigned int i;\r\nfor_each_possible_cpu(i)\r\nzalloc_cpumask_var_node(&per_cpu(local_cpu_mask_dl, i),\r\nGFP_KERNEL, cpu_to_node(i));\r\n}\r\nstatic void switched_from_dl(struct rq *rq, struct task_struct *p)\r\n{\r\nif (!start_dl_timer(p))\r\n__dl_clear_params(p);\r\nif (!task_on_rq_queued(p) || rq->dl.dl_nr_running)\r\nreturn;\r\nqueue_pull_task(rq);\r\n}\r\nstatic void switched_to_dl(struct rq *rq, struct task_struct *p)\r\n{\r\nif (!task_on_rq_queued(p))\r\nreturn;\r\nif (dl_time_before(p->dl.deadline, rq_clock(rq)))\r\nsetup_new_dl_entity(&p->dl);\r\nif (rq->curr != p) {\r\n#ifdef CONFIG_SMP\r\nif (p->nr_cpus_allowed > 1 && rq->dl.overloaded)\r\nqueue_push_tasks(rq);\r\n#endif\r\nif (dl_task(rq->curr))\r\ncheck_preempt_curr_dl(rq, p, 0);\r\nelse\r\nresched_curr(rq);\r\n}\r\n}\r\nstatic void prio_changed_dl(struct rq *rq, struct task_struct *p,\r\nint oldprio)\r\n{\r\nif (task_on_rq_queued(p) || rq->curr == p) {\r\n#ifdef CONFIG_SMP\r\nif (!rq->dl.overloaded)\r\nqueue_pull_task(rq);\r\nif (dl_time_before(rq->dl.earliest_dl.curr, p->dl.deadline))\r\nresched_curr(rq);\r\n#else\r\nresched_curr(rq);\r\n#endif\r\n}\r\n}\r\nvoid print_dl_stats(struct seq_file *m, int cpu)\r\n{\r\nprint_dl_rq(m, cpu, &cpu_rq(cpu)->dl);\r\n}
