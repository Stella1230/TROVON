void\r\nkclist_add(struct kcore_list *new, void *addr, size_t size, int type)\r\n{\r\nnew->addr = (unsigned long)addr;\r\nnew->size = size;\r\nnew->type = type;\r\nwrite_lock(&kclist_lock);\r\nlist_add_tail(&new->list, &kclist_head);\r\nwrite_unlock(&kclist_lock);\r\n}\r\nstatic size_t get_kcore_size(int *nphdr, size_t *elf_buflen)\r\n{\r\nsize_t try, size;\r\nstruct kcore_list *m;\r\n*nphdr = 1;\r\nsize = 0;\r\nlist_for_each_entry(m, &kclist_head, list) {\r\ntry = kc_vaddr_to_offset((size_t)m->addr + m->size);\r\nif (try > size)\r\nsize = try;\r\n*nphdr = *nphdr + 1;\r\n}\r\n*elf_buflen = sizeof(struct elfhdr) +\r\n(*nphdr + 2)*sizeof(struct elf_phdr) +\r\n3 * ((sizeof(struct elf_note)) +\r\nroundup(sizeof(CORE_STR), 4)) +\r\nroundup(sizeof(struct elf_prstatus), 4) +\r\nroundup(sizeof(struct elf_prpsinfo), 4) +\r\nroundup(arch_task_struct_size, 4);\r\n*elf_buflen = PAGE_ALIGN(*elf_buflen);\r\nreturn size + *elf_buflen;\r\n}\r\nstatic void free_kclist_ents(struct list_head *head)\r\n{\r\nstruct kcore_list *tmp, *pos;\r\nlist_for_each_entry_safe(pos, tmp, head, list) {\r\nlist_del(&pos->list);\r\nkfree(pos);\r\n}\r\n}\r\nstatic void __kcore_update_ram(struct list_head *list)\r\n{\r\nint nphdr;\r\nsize_t size;\r\nstruct kcore_list *tmp, *pos;\r\nLIST_HEAD(garbage);\r\nwrite_lock(&kclist_lock);\r\nif (kcore_need_update) {\r\nlist_for_each_entry_safe(pos, tmp, &kclist_head, list) {\r\nif (pos->type == KCORE_RAM\r\n|| pos->type == KCORE_VMEMMAP)\r\nlist_move(&pos->list, &garbage);\r\n}\r\nlist_splice_tail(list, &kclist_head);\r\n} else\r\nlist_splice(list, &garbage);\r\nkcore_need_update = 0;\r\nproc_root_kcore->size = get_kcore_size(&nphdr, &size);\r\nwrite_unlock(&kclist_lock);\r\nfree_kclist_ents(&garbage);\r\n}\r\nstatic int kcore_update_ram(void)\r\n{\r\nLIST_HEAD(head);\r\nstruct kcore_list *ent;\r\nint ret = 0;\r\nent = kmalloc(sizeof(*ent), GFP_KERNEL);\r\nif (!ent)\r\nreturn -ENOMEM;\r\nent->addr = (unsigned long)__va(0);\r\nent->size = max_low_pfn << PAGE_SHIFT;\r\nent->type = KCORE_RAM;\r\nlist_add(&ent->list, &head);\r\n__kcore_update_ram(&head);\r\nreturn ret;\r\n}\r\nstatic int\r\nget_sparsemem_vmemmap_info(struct kcore_list *ent, struct list_head *head)\r\n{\r\nunsigned long pfn = __pa(ent->addr) >> PAGE_SHIFT;\r\nunsigned long nr_pages = ent->size >> PAGE_SHIFT;\r\nunsigned long start, end;\r\nstruct kcore_list *vmm, *tmp;\r\nstart = ((unsigned long)pfn_to_page(pfn)) & PAGE_MASK;\r\nend = ((unsigned long)pfn_to_page(pfn + nr_pages)) - 1;\r\nend = PAGE_ALIGN(end);\r\nlist_for_each_entry(tmp, head, list) {\r\nif (tmp->type != KCORE_VMEMMAP)\r\ncontinue;\r\nif (start < tmp->addr + tmp->size)\r\nif (end > tmp->addr)\r\nend = tmp->addr;\r\n}\r\nif (start < end) {\r\nvmm = kmalloc(sizeof(*vmm), GFP_KERNEL);\r\nif (!vmm)\r\nreturn 0;\r\nvmm->addr = start;\r\nvmm->size = end - start;\r\nvmm->type = KCORE_VMEMMAP;\r\nlist_add_tail(&vmm->list, head);\r\n}\r\nreturn 1;\r\n}\r\nstatic int\r\nget_sparsemem_vmemmap_info(struct kcore_list *ent, struct list_head *head)\r\n{\r\nreturn 1;\r\n}\r\nstatic int\r\nkclist_add_private(unsigned long pfn, unsigned long nr_pages, void *arg)\r\n{\r\nstruct list_head *head = (struct list_head *)arg;\r\nstruct kcore_list *ent;\r\nent = kmalloc(sizeof(*ent), GFP_KERNEL);\r\nif (!ent)\r\nreturn -ENOMEM;\r\nent->addr = (unsigned long)__va((pfn << PAGE_SHIFT));\r\nent->size = nr_pages << PAGE_SHIFT;\r\nif (ent->addr < (unsigned long) __va(0))\r\ngoto free_out;\r\nif (ULONG_MAX - ent->addr < ent->size)\r\nent->size = ULONG_MAX - ent->addr;\r\nif (VMALLOC_START > (unsigned long)__va(0)) {\r\nif (ent->addr > VMALLOC_START)\r\ngoto free_out;\r\nif (VMALLOC_START - ent->addr < ent->size)\r\nent->size = VMALLOC_START - ent->addr;\r\n}\r\nent->type = KCORE_RAM;\r\nlist_add_tail(&ent->list, head);\r\nif (!get_sparsemem_vmemmap_info(ent, head)) {\r\nlist_del(&ent->list);\r\ngoto free_out;\r\n}\r\nreturn 0;\r\nfree_out:\r\nkfree(ent);\r\nreturn 1;\r\n}\r\nstatic int kcore_update_ram(void)\r\n{\r\nint nid, ret;\r\nunsigned long end_pfn;\r\nLIST_HEAD(head);\r\nend_pfn = 0;\r\nfor_each_node_state(nid, N_MEMORY) {\r\nunsigned long node_end;\r\nnode_end = node_end_pfn(nid);\r\nif (end_pfn < node_end)\r\nend_pfn = node_end;\r\n}\r\nret = walk_system_ram_range(0, end_pfn, &head, kclist_add_private);\r\nif (ret) {\r\nfree_kclist_ents(&head);\r\nreturn -ENOMEM;\r\n}\r\n__kcore_update_ram(&head);\r\nreturn ret;\r\n}\r\nstatic int notesize(struct memelfnote *en)\r\n{\r\nint sz;\r\nsz = sizeof(struct elf_note);\r\nsz += roundup((strlen(en->name) + 1), 4);\r\nsz += roundup(en->datasz, 4);\r\nreturn sz;\r\n}\r\nstatic char *storenote(struct memelfnote *men, char *bufp)\r\n{\r\nstruct elf_note en;\r\n#define DUMP_WRITE(addr,nr) do { memcpy(bufp,addr,nr); bufp += nr; } while(0)\r\nen.n_namesz = strlen(men->name) + 1;\r\nen.n_descsz = men->datasz;\r\nen.n_type = men->type;\r\nDUMP_WRITE(&en, sizeof(en));\r\nDUMP_WRITE(men->name, en.n_namesz);\r\nbufp = (char*) roundup((unsigned long)bufp,4);\r\nDUMP_WRITE(men->data, men->datasz);\r\nbufp = (char*) roundup((unsigned long)bufp,4);\r\n#undef DUMP_WRITE\r\nreturn bufp;\r\n}\r\nstatic void elf_kcore_store_hdr(char *bufp, int nphdr, int dataoff)\r\n{\r\nstruct elf_prstatus prstatus;\r\nstruct elf_prpsinfo prpsinfo;\r\nstruct elf_phdr *nhdr, *phdr;\r\nstruct elfhdr *elf;\r\nstruct memelfnote notes[3];\r\noff_t offset = 0;\r\nstruct kcore_list *m;\r\nelf = (struct elfhdr *) bufp;\r\nbufp += sizeof(struct elfhdr);\r\noffset += sizeof(struct elfhdr);\r\nmemcpy(elf->e_ident, ELFMAG, SELFMAG);\r\nelf->e_ident[EI_CLASS] = ELF_CLASS;\r\nelf->e_ident[EI_DATA] = ELF_DATA;\r\nelf->e_ident[EI_VERSION]= EV_CURRENT;\r\nelf->e_ident[EI_OSABI] = ELF_OSABI;\r\nmemset(elf->e_ident+EI_PAD, 0, EI_NIDENT-EI_PAD);\r\nelf->e_type = ET_CORE;\r\nelf->e_machine = ELF_ARCH;\r\nelf->e_version = EV_CURRENT;\r\nelf->e_entry = 0;\r\nelf->e_phoff = sizeof(struct elfhdr);\r\nelf->e_shoff = 0;\r\nelf->e_flags = ELF_CORE_EFLAGS;\r\nelf->e_ehsize = sizeof(struct elfhdr);\r\nelf->e_phentsize= sizeof(struct elf_phdr);\r\nelf->e_phnum = nphdr;\r\nelf->e_shentsize= 0;\r\nelf->e_shnum = 0;\r\nelf->e_shstrndx = 0;\r\nnhdr = (struct elf_phdr *) bufp;\r\nbufp += sizeof(struct elf_phdr);\r\noffset += sizeof(struct elf_phdr);\r\nnhdr->p_type = PT_NOTE;\r\nnhdr->p_offset = 0;\r\nnhdr->p_vaddr = 0;\r\nnhdr->p_paddr = 0;\r\nnhdr->p_filesz = 0;\r\nnhdr->p_memsz = 0;\r\nnhdr->p_flags = 0;\r\nnhdr->p_align = 0;\r\nlist_for_each_entry(m, &kclist_head, list) {\r\nphdr = (struct elf_phdr *) bufp;\r\nbufp += sizeof(struct elf_phdr);\r\noffset += sizeof(struct elf_phdr);\r\nphdr->p_type = PT_LOAD;\r\nphdr->p_flags = PF_R|PF_W|PF_X;\r\nphdr->p_offset = kc_vaddr_to_offset(m->addr) + dataoff;\r\nphdr->p_vaddr = (size_t)m->addr;\r\nif (m->type == KCORE_RAM || m->type == KCORE_TEXT)\r\nphdr->p_paddr = __pa(m->addr);\r\nelse\r\nphdr->p_paddr = (elf_addr_t)-1;\r\nphdr->p_filesz = phdr->p_memsz = m->size;\r\nphdr->p_align = PAGE_SIZE;\r\n}\r\nnhdr->p_offset = offset;\r\nnotes[0].name = CORE_STR;\r\nnotes[0].type = NT_PRSTATUS;\r\nnotes[0].datasz = sizeof(struct elf_prstatus);\r\nnotes[0].data = &prstatus;\r\nmemset(&prstatus, 0, sizeof(struct elf_prstatus));\r\nnhdr->p_filesz = notesize(&notes[0]);\r\nbufp = storenote(&notes[0], bufp);\r\nnotes[1].name = CORE_STR;\r\nnotes[1].type = NT_PRPSINFO;\r\nnotes[1].datasz = sizeof(struct elf_prpsinfo);\r\nnotes[1].data = &prpsinfo;\r\nmemset(&prpsinfo, 0, sizeof(struct elf_prpsinfo));\r\nprpsinfo.pr_state = 0;\r\nprpsinfo.pr_sname = 'R';\r\nprpsinfo.pr_zomb = 0;\r\nstrcpy(prpsinfo.pr_fname, "vmlinux");\r\nstrlcpy(prpsinfo.pr_psargs, saved_command_line, sizeof(prpsinfo.pr_psargs));\r\nnhdr->p_filesz += notesize(&notes[1]);\r\nbufp = storenote(&notes[1], bufp);\r\nnotes[2].name = CORE_STR;\r\nnotes[2].type = NT_TASKSTRUCT;\r\nnotes[2].datasz = arch_task_struct_size;\r\nnotes[2].data = current;\r\nnhdr->p_filesz += notesize(&notes[2]);\r\nbufp = storenote(&notes[2], bufp);\r\n}\r\nstatic ssize_t\r\nread_kcore(struct file *file, char __user *buffer, size_t buflen, loff_t *fpos)\r\n{\r\nchar *buf = file->private_data;\r\nssize_t acc = 0;\r\nsize_t size, tsz;\r\nsize_t elf_buflen;\r\nint nphdr;\r\nunsigned long start;\r\nread_lock(&kclist_lock);\r\nsize = get_kcore_size(&nphdr, &elf_buflen);\r\nif (buflen == 0 || *fpos >= size) {\r\nread_unlock(&kclist_lock);\r\nreturn 0;\r\n}\r\nif (buflen > size - *fpos)\r\nbuflen = size - *fpos;\r\nif (*fpos < elf_buflen) {\r\nchar * elf_buf;\r\ntsz = elf_buflen - *fpos;\r\nif (buflen < tsz)\r\ntsz = buflen;\r\nelf_buf = kzalloc(elf_buflen, GFP_ATOMIC);\r\nif (!elf_buf) {\r\nread_unlock(&kclist_lock);\r\nreturn -ENOMEM;\r\n}\r\nelf_kcore_store_hdr(elf_buf, nphdr, elf_buflen);\r\nread_unlock(&kclist_lock);\r\nif (copy_to_user(buffer, elf_buf + *fpos, tsz)) {\r\nkfree(elf_buf);\r\nreturn -EFAULT;\r\n}\r\nkfree(elf_buf);\r\nbuflen -= tsz;\r\n*fpos += tsz;\r\nbuffer += tsz;\r\nacc += tsz;\r\nif (buflen == 0)\r\nreturn acc;\r\n} else\r\nread_unlock(&kclist_lock);\r\nstart = kc_offset_to_vaddr(*fpos - elf_buflen);\r\nif ((tsz = (PAGE_SIZE - (start & ~PAGE_MASK))) > buflen)\r\ntsz = buflen;\r\nwhile (buflen) {\r\nstruct kcore_list *m;\r\nread_lock(&kclist_lock);\r\nlist_for_each_entry(m, &kclist_head, list) {\r\nif (start >= m->addr && start < (m->addr+m->size))\r\nbreak;\r\n}\r\nread_unlock(&kclist_lock);\r\nif (&m->list == &kclist_head) {\r\nif (clear_user(buffer, tsz))\r\nreturn -EFAULT;\r\n} else if (is_vmalloc_or_module_addr((void *)start)) {\r\nvread(buf, (char *)start, tsz);\r\nif (copy_to_user(buffer, buf, tsz))\r\nreturn -EFAULT;\r\n} else {\r\nif (kern_addr_valid(start)) {\r\nunsigned long n;\r\nmemcpy(buf, (char *) start, tsz);\r\nn = copy_to_user(buffer, buf, tsz);\r\nif (n) {\r\nif (clear_user(buffer + tsz - n,\r\nn))\r\nreturn -EFAULT;\r\n}\r\n} else {\r\nif (clear_user(buffer, tsz))\r\nreturn -EFAULT;\r\n}\r\n}\r\nbuflen -= tsz;\r\n*fpos += tsz;\r\nbuffer += tsz;\r\nacc += tsz;\r\nstart += tsz;\r\ntsz = (buflen > PAGE_SIZE ? PAGE_SIZE : buflen);\r\n}\r\nreturn acc;\r\n}\r\nstatic int open_kcore(struct inode *inode, struct file *filp)\r\n{\r\nif (!capable(CAP_SYS_RAWIO))\r\nreturn -EPERM;\r\nfilp->private_data = kmalloc(PAGE_SIZE, GFP_KERNEL);\r\nif (!filp->private_data)\r\nreturn -ENOMEM;\r\nif (kcore_need_update)\r\nkcore_update_ram();\r\nif (i_size_read(inode) != proc_root_kcore->size) {\r\ninode_lock(inode);\r\ni_size_write(inode, proc_root_kcore->size);\r\ninode_unlock(inode);\r\n}\r\nreturn 0;\r\n}\r\nstatic int release_kcore(struct inode *inode, struct file *file)\r\n{\r\nkfree(file->private_data);\r\nreturn 0;\r\n}\r\nstatic int __meminit kcore_callback(struct notifier_block *self,\r\nunsigned long action, void *arg)\r\n{\r\nswitch (action) {\r\ncase MEM_ONLINE:\r\ncase MEM_OFFLINE:\r\nwrite_lock(&kclist_lock);\r\nkcore_need_update = 1;\r\nwrite_unlock(&kclist_lock);\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void __init proc_kcore_text_init(void)\r\n{\r\nkclist_add(&kcore_text, _text, _end - _text, KCORE_TEXT);\r\n}\r\nstatic void __init proc_kcore_text_init(void)\r\n{\r\n}\r\nstatic void __init add_modules_range(void)\r\n{\r\nif (MODULES_VADDR != VMALLOC_START && MODULES_END != VMALLOC_END) {\r\nkclist_add(&kcore_modules, (void *)MODULES_VADDR,\r\nMODULES_END - MODULES_VADDR, KCORE_VMALLOC);\r\n}\r\n}\r\nstatic void __init add_modules_range(void)\r\n{\r\n}\r\nstatic int __init proc_kcore_init(void)\r\n{\r\nproc_root_kcore = proc_create("kcore", S_IRUSR, NULL,\r\n&proc_kcore_operations);\r\nif (!proc_root_kcore) {\r\npr_err("couldn't create /proc/kcore\n");\r\nreturn 0;\r\n}\r\nproc_kcore_text_init();\r\nkclist_add(&kcore_vmalloc, (void *)VMALLOC_START,\r\nVMALLOC_END - VMALLOC_START, KCORE_VMALLOC);\r\nadd_modules_range();\r\nkcore_update_ram();\r\nregister_hotmemory_notifier(&kcore_callback_nb);\r\nreturn 0;\r\n}
