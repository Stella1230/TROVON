static inline u32 img_hash_read(struct img_hash_dev *hdev, u32 offset)\r\n{\r\nreturn readl_relaxed(hdev->io_base + offset);\r\n}\r\nstatic inline void img_hash_write(struct img_hash_dev *hdev,\r\nu32 offset, u32 value)\r\n{\r\nwritel_relaxed(value, hdev->io_base + offset);\r\n}\r\nstatic inline u32 img_hash_read_result_queue(struct img_hash_dev *hdev)\r\n{\r\nreturn be32_to_cpu(img_hash_read(hdev, CR_RESULT_QUEUE));\r\n}\r\nstatic void img_hash_start(struct img_hash_dev *hdev, bool dma)\r\n{\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\r\nu32 cr = IMG_HASH_BYTE_ORDER << CR_CONTROL_BYTE_ORDER_SHIFT;\r\nif (ctx->flags & DRIVER_FLAGS_MD5)\r\ncr |= CR_CONTROL_ALGO_MD5;\r\nelse if (ctx->flags & DRIVER_FLAGS_SHA1)\r\ncr |= CR_CONTROL_ALGO_SHA1;\r\nelse if (ctx->flags & DRIVER_FLAGS_SHA224)\r\ncr |= CR_CONTROL_ALGO_SHA224;\r\nelse if (ctx->flags & DRIVER_FLAGS_SHA256)\r\ncr |= CR_CONTROL_ALGO_SHA256;\r\ndev_dbg(hdev->dev, "Starting hash process\n");\r\nimg_hash_write(hdev, CR_CONTROL, cr);\r\nif (!dma)\r\nimg_hash_read(hdev, CR_CONTROL);\r\n}\r\nstatic int img_hash_xmit_cpu(struct img_hash_dev *hdev, const u8 *buf,\r\nsize_t length, int final)\r\n{\r\nu32 count, len32;\r\nconst u32 *buffer = (const u32 *)buf;\r\ndev_dbg(hdev->dev, "xmit_cpu: length: %zu bytes\n", length);\r\nif (final)\r\nhdev->flags |= DRIVER_FLAGS_FINAL;\r\nlen32 = DIV_ROUND_UP(length, sizeof(u32));\r\nfor (count = 0; count < len32; count++)\r\nwritel_relaxed(buffer[count], hdev->cpu_addr);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic void img_hash_dma_callback(void *data)\r\n{\r\nstruct img_hash_dev *hdev = (struct img_hash_dev *)data;\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\r\nif (ctx->bufcnt) {\r\nimg_hash_xmit_cpu(hdev, ctx->buffer, ctx->bufcnt, 0);\r\nctx->bufcnt = 0;\r\n}\r\nif (ctx->sg)\r\ntasklet_schedule(&hdev->dma_task);\r\n}\r\nstatic int img_hash_xmit_dma(struct img_hash_dev *hdev, struct scatterlist *sg)\r\n{\r\nstruct dma_async_tx_descriptor *desc;\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\r\nctx->dma_ct = dma_map_sg(hdev->dev, sg, 1, DMA_TO_DEVICE);\r\nif (ctx->dma_ct == 0) {\r\ndev_err(hdev->dev, "Invalid DMA sg\n");\r\nhdev->err = -EINVAL;\r\nreturn -EINVAL;\r\n}\r\ndesc = dmaengine_prep_slave_sg(hdev->dma_lch,\r\nsg,\r\nctx->dma_ct,\r\nDMA_MEM_TO_DEV,\r\nDMA_PREP_INTERRUPT | DMA_CTRL_ACK);\r\nif (!desc) {\r\ndev_err(hdev->dev, "Null DMA descriptor\n");\r\nhdev->err = -EINVAL;\r\ndma_unmap_sg(hdev->dev, sg, 1, DMA_TO_DEVICE);\r\nreturn -EINVAL;\r\n}\r\ndesc->callback = img_hash_dma_callback;\r\ndesc->callback_param = hdev;\r\ndmaengine_submit(desc);\r\ndma_async_issue_pending(hdev->dma_lch);\r\nreturn 0;\r\n}\r\nstatic int img_hash_write_via_cpu(struct img_hash_dev *hdev)\r\n{\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\r\nctx->bufcnt = sg_copy_to_buffer(hdev->req->src, sg_nents(ctx->sg),\r\nctx->buffer, hdev->req->nbytes);\r\nctx->total = hdev->req->nbytes;\r\nctx->bufcnt = 0;\r\nhdev->flags |= (DRIVER_FLAGS_CPU | DRIVER_FLAGS_FINAL);\r\nimg_hash_start(hdev, false);\r\nreturn img_hash_xmit_cpu(hdev, ctx->buffer, ctx->total, 1);\r\n}\r\nstatic int img_hash_finish(struct ahash_request *req)\r\n{\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\r\nif (!req->result)\r\nreturn -EINVAL;\r\nmemcpy(req->result, ctx->digest, ctx->digsize);\r\nreturn 0;\r\n}\r\nstatic void img_hash_copy_hash(struct ahash_request *req)\r\n{\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\r\nu32 *hash = (u32 *)ctx->digest;\r\nint i;\r\nfor (i = (ctx->digsize / sizeof(u32)) - 1; i >= 0; i--)\r\nhash[i] = img_hash_read_result_queue(ctx->hdev);\r\n}\r\nstatic void img_hash_finish_req(struct ahash_request *req, int err)\r\n{\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\r\nstruct img_hash_dev *hdev = ctx->hdev;\r\nif (!err) {\r\nimg_hash_copy_hash(req);\r\nif (DRIVER_FLAGS_FINAL & hdev->flags)\r\nerr = img_hash_finish(req);\r\n} else {\r\ndev_warn(hdev->dev, "Hash failed with error %d\n", err);\r\nctx->flags |= DRIVER_FLAGS_ERROR;\r\n}\r\nhdev->flags &= ~(DRIVER_FLAGS_DMA_READY | DRIVER_FLAGS_OUTPUT_READY |\r\nDRIVER_FLAGS_CPU | DRIVER_FLAGS_BUSY | DRIVER_FLAGS_FINAL);\r\nif (req->base.complete)\r\nreq->base.complete(&req->base, err);\r\n}\r\nstatic int img_hash_write_via_dma(struct img_hash_dev *hdev)\r\n{\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\r\nimg_hash_start(hdev, true);\r\ndev_dbg(hdev->dev, "xmit dma size: %d\n", ctx->total);\r\nif (!ctx->total)\r\nhdev->flags |= DRIVER_FLAGS_FINAL;\r\nhdev->flags |= DRIVER_FLAGS_DMA_ACTIVE | DRIVER_FLAGS_FINAL;\r\ntasklet_schedule(&hdev->dma_task);\r\nreturn -EINPROGRESS;\r\n}\r\nstatic int img_hash_dma_init(struct img_hash_dev *hdev)\r\n{\r\nstruct dma_slave_config dma_conf;\r\nint err = -EINVAL;\r\nhdev->dma_lch = dma_request_slave_channel(hdev->dev, "tx");\r\nif (!hdev->dma_lch) {\r\ndev_err(hdev->dev, "Couldn't acquire a slave DMA channel.\n");\r\nreturn -EBUSY;\r\n}\r\ndma_conf.direction = DMA_MEM_TO_DEV;\r\ndma_conf.dst_addr = hdev->bus_addr;\r\ndma_conf.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES;\r\ndma_conf.dst_maxburst = IMG_HASH_DMA_BURST;\r\ndma_conf.device_fc = false;\r\nerr = dmaengine_slave_config(hdev->dma_lch, &dma_conf);\r\nif (err) {\r\ndev_err(hdev->dev, "Couldn't configure DMA slave.\n");\r\ndma_release_channel(hdev->dma_lch);\r\nreturn err;\r\n}\r\nreturn 0;\r\n}\r\nstatic void img_hash_dma_task(unsigned long d)\r\n{\r\nstruct img_hash_dev *hdev = (struct img_hash_dev *)d;\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\r\nu8 *addr;\r\nsize_t nbytes, bleft, wsend, len, tbc;\r\nstruct scatterlist tsg;\r\nif (!hdev->req || !ctx->sg)\r\nreturn;\r\naddr = sg_virt(ctx->sg);\r\nnbytes = ctx->sg->length - ctx->offset;\r\nbleft = nbytes % 4;\r\nwsend = (nbytes / 4);\r\nif (wsend) {\r\nsg_init_one(&tsg, addr + ctx->offset, wsend * 4);\r\nif (img_hash_xmit_dma(hdev, &tsg)) {\r\ndev_err(hdev->dev, "DMA failed, falling back to CPU");\r\nctx->flags |= DRIVER_FLAGS_CPU;\r\nhdev->err = 0;\r\nimg_hash_xmit_cpu(hdev, addr + ctx->offset,\r\nwsend * 4, 0);\r\nctx->sent += wsend * 4;\r\nwsend = 0;\r\n} else {\r\nctx->sent += wsend * 4;\r\n}\r\n}\r\nif (bleft) {\r\nctx->bufcnt = sg_pcopy_to_buffer(ctx->sgfirst, ctx->nents,\r\nctx->buffer, bleft, ctx->sent);\r\ntbc = 0;\r\nctx->sg = sg_next(ctx->sg);\r\nwhile (ctx->sg && (ctx->bufcnt < 4)) {\r\nlen = ctx->sg->length;\r\nif (likely(len > (4 - ctx->bufcnt)))\r\nlen = 4 - ctx->bufcnt;\r\ntbc = sg_pcopy_to_buffer(ctx->sgfirst, ctx->nents,\r\nctx->buffer + ctx->bufcnt, len,\r\nctx->sent + ctx->bufcnt);\r\nctx->bufcnt += tbc;\r\nif (tbc >= ctx->sg->length) {\r\nctx->sg = sg_next(ctx->sg);\r\ntbc = 0;\r\n}\r\n}\r\nctx->sent += ctx->bufcnt;\r\nctx->offset = tbc;\r\nif (!wsend)\r\nimg_hash_dma_callback(hdev);\r\n} else {\r\nctx->offset = 0;\r\nctx->sg = sg_next(ctx->sg);\r\n}\r\n}\r\nstatic int img_hash_write_via_dma_stop(struct img_hash_dev *hdev)\r\n{\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(hdev->req);\r\nif (ctx->flags & DRIVER_FLAGS_SG)\r\ndma_unmap_sg(hdev->dev, ctx->sg, ctx->dma_ct, DMA_TO_DEVICE);\r\nreturn 0;\r\n}\r\nstatic int img_hash_process_data(struct img_hash_dev *hdev)\r\n{\r\nstruct ahash_request *req = hdev->req;\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\r\nint err = 0;\r\nctx->bufcnt = 0;\r\nif (req->nbytes >= IMG_HASH_DMA_THRESHOLD) {\r\ndev_dbg(hdev->dev, "process data request(%d bytes) using DMA\n",\r\nreq->nbytes);\r\nerr = img_hash_write_via_dma(hdev);\r\n} else {\r\ndev_dbg(hdev->dev, "process data request(%d bytes) using CPU\n",\r\nreq->nbytes);\r\nerr = img_hash_write_via_cpu(hdev);\r\n}\r\nreturn err;\r\n}\r\nstatic int img_hash_hw_init(struct img_hash_dev *hdev)\r\n{\r\nunsigned long long nbits;\r\nu32 u, l;\r\nimg_hash_write(hdev, CR_RESET, CR_RESET_SET);\r\nimg_hash_write(hdev, CR_RESET, CR_RESET_UNSET);\r\nimg_hash_write(hdev, CR_INTENAB, CR_INT_NEW_RESULTS_SET);\r\nnbits = (u64)hdev->req->nbytes << 3;\r\nu = nbits >> 32;\r\nl = nbits;\r\nimg_hash_write(hdev, CR_MESSAGE_LENGTH_H, u);\r\nimg_hash_write(hdev, CR_MESSAGE_LENGTH_L, l);\r\nif (!(DRIVER_FLAGS_INIT & hdev->flags)) {\r\nhdev->flags |= DRIVER_FLAGS_INIT;\r\nhdev->err = 0;\r\n}\r\ndev_dbg(hdev->dev, "hw initialized, nbits: %llx\n", nbits);\r\nreturn 0;\r\n}\r\nstatic int img_hash_init(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\r\nrctx->fallback_req.base.flags = req->base.flags\r\n& CRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_ahash_init(&rctx->fallback_req);\r\n}\r\nstatic int img_hash_handle_queue(struct img_hash_dev *hdev,\r\nstruct ahash_request *req)\r\n{\r\nstruct crypto_async_request *async_req, *backlog;\r\nstruct img_hash_request_ctx *ctx;\r\nunsigned long flags;\r\nint err = 0, res = 0;\r\nspin_lock_irqsave(&hdev->lock, flags);\r\nif (req)\r\nres = ahash_enqueue_request(&hdev->queue, req);\r\nif (DRIVER_FLAGS_BUSY & hdev->flags) {\r\nspin_unlock_irqrestore(&hdev->lock, flags);\r\nreturn res;\r\n}\r\nbacklog = crypto_get_backlog(&hdev->queue);\r\nasync_req = crypto_dequeue_request(&hdev->queue);\r\nif (async_req)\r\nhdev->flags |= DRIVER_FLAGS_BUSY;\r\nspin_unlock_irqrestore(&hdev->lock, flags);\r\nif (!async_req)\r\nreturn res;\r\nif (backlog)\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nreq = ahash_request_cast(async_req);\r\nhdev->req = req;\r\nctx = ahash_request_ctx(req);\r\ndev_info(hdev->dev, "processing req, op: %lu, bytes: %d\n",\r\nctx->op, req->nbytes);\r\nerr = img_hash_hw_init(hdev);\r\nif (!err)\r\nerr = img_hash_process_data(hdev);\r\nif (err != -EINPROGRESS) {\r\nimg_hash_finish_req(req, err);\r\n}\r\nreturn res;\r\n}\r\nstatic int img_hash_update(struct ahash_request *req)\r\n{\r\nstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\r\nrctx->fallback_req.base.flags = req->base.flags\r\n& CRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.nbytes = req->nbytes;\r\nrctx->fallback_req.src = req->src;\r\nreturn crypto_ahash_update(&rctx->fallback_req);\r\n}\r\nstatic int img_hash_final(struct ahash_request *req)\r\n{\r\nstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\r\nrctx->fallback_req.base.flags = req->base.flags\r\n& CRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.result = req->result;\r\nreturn crypto_ahash_final(&rctx->fallback_req);\r\n}\r\nstatic int img_hash_finup(struct ahash_request *req)\r\n{\r\nstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\r\nrctx->fallback_req.base.flags = req->base.flags\r\n& CRYPTO_TFM_REQ_MAY_SLEEP;\r\nrctx->fallback_req.nbytes = req->nbytes;\r\nrctx->fallback_req.src = req->src;\r\nrctx->fallback_req.result = req->result;\r\nreturn crypto_ahash_finup(&rctx->fallback_req);\r\n}\r\nstatic int img_hash_import(struct ahash_request *req, const void *in)\r\n{\r\nstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\r\nrctx->fallback_req.base.flags = req->base.flags\r\n& CRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_ahash_import(&rctx->fallback_req, in);\r\n}\r\nstatic int img_hash_export(struct ahash_request *req, void *out)\r\n{\r\nstruct img_hash_request_ctx *rctx = ahash_request_ctx(req);\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct img_hash_ctx *ctx = crypto_ahash_ctx(tfm);\r\nahash_request_set_tfm(&rctx->fallback_req, ctx->fallback);\r\nrctx->fallback_req.base.flags = req->base.flags\r\n& CRYPTO_TFM_REQ_MAY_SLEEP;\r\nreturn crypto_ahash_export(&rctx->fallback_req, out);\r\n}\r\nstatic int img_hash_digest(struct ahash_request *req)\r\n{\r\nstruct crypto_ahash *tfm = crypto_ahash_reqtfm(req);\r\nstruct img_hash_ctx *tctx = crypto_ahash_ctx(tfm);\r\nstruct img_hash_request_ctx *ctx = ahash_request_ctx(req);\r\nstruct img_hash_dev *hdev = NULL;\r\nstruct img_hash_dev *tmp;\r\nint err;\r\nspin_lock(&img_hash.lock);\r\nif (!tctx->hdev) {\r\nlist_for_each_entry(tmp, &img_hash.dev_list, list) {\r\nhdev = tmp;\r\nbreak;\r\n}\r\ntctx->hdev = hdev;\r\n} else {\r\nhdev = tctx->hdev;\r\n}\r\nspin_unlock(&img_hash.lock);\r\nctx->hdev = hdev;\r\nctx->flags = 0;\r\nctx->digsize = crypto_ahash_digestsize(tfm);\r\nswitch (ctx->digsize) {\r\ncase SHA1_DIGEST_SIZE:\r\nctx->flags |= DRIVER_FLAGS_SHA1;\r\nbreak;\r\ncase SHA256_DIGEST_SIZE:\r\nctx->flags |= DRIVER_FLAGS_SHA256;\r\nbreak;\r\ncase SHA224_DIGEST_SIZE:\r\nctx->flags |= DRIVER_FLAGS_SHA224;\r\nbreak;\r\ncase MD5_DIGEST_SIZE:\r\nctx->flags |= DRIVER_FLAGS_MD5;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nctx->bufcnt = 0;\r\nctx->offset = 0;\r\nctx->sent = 0;\r\nctx->total = req->nbytes;\r\nctx->sg = req->src;\r\nctx->sgfirst = req->src;\r\nctx->nents = sg_nents(ctx->sg);\r\nerr = img_hash_handle_queue(tctx->hdev, req);\r\nreturn err;\r\n}\r\nstatic int img_hash_cra_init(struct crypto_tfm *tfm, const char *alg_name)\r\n{\r\nstruct img_hash_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint err = -ENOMEM;\r\nctx->fallback = crypto_alloc_ahash(alg_name, 0,\r\nCRYPTO_ALG_NEED_FALLBACK);\r\nif (IS_ERR(ctx->fallback)) {\r\npr_err("img_hash: Could not load fallback driver.\n");\r\nerr = PTR_ERR(ctx->fallback);\r\ngoto err;\r\n}\r\ncrypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),\r\nsizeof(struct img_hash_request_ctx) +\r\ncrypto_ahash_reqsize(ctx->fallback) +\r\nIMG_HASH_DMA_THRESHOLD);\r\nreturn 0;\r\nerr:\r\nreturn err;\r\n}\r\nstatic int img_hash_cra_md5_init(struct crypto_tfm *tfm)\r\n{\r\nreturn img_hash_cra_init(tfm, "md5-generic");\r\n}\r\nstatic int img_hash_cra_sha1_init(struct crypto_tfm *tfm)\r\n{\r\nreturn img_hash_cra_init(tfm, "sha1-generic");\r\n}\r\nstatic int img_hash_cra_sha224_init(struct crypto_tfm *tfm)\r\n{\r\nreturn img_hash_cra_init(tfm, "sha224-generic");\r\n}\r\nstatic int img_hash_cra_sha256_init(struct crypto_tfm *tfm)\r\n{\r\nreturn img_hash_cra_init(tfm, "sha256-generic");\r\n}\r\nstatic void img_hash_cra_exit(struct crypto_tfm *tfm)\r\n{\r\nstruct img_hash_ctx *tctx = crypto_tfm_ctx(tfm);\r\ncrypto_free_ahash(tctx->fallback);\r\n}\r\nstatic irqreturn_t img_irq_handler(int irq, void *dev_id)\r\n{\r\nstruct img_hash_dev *hdev = dev_id;\r\nu32 reg;\r\nreg = img_hash_read(hdev, CR_INTSTAT);\r\nimg_hash_write(hdev, CR_INTCLEAR, reg);\r\nif (reg & CR_INT_NEW_RESULTS_SET) {\r\ndev_dbg(hdev->dev, "IRQ CR_INT_NEW_RESULTS_SET\n");\r\nif (DRIVER_FLAGS_BUSY & hdev->flags) {\r\nhdev->flags |= DRIVER_FLAGS_OUTPUT_READY;\r\nif (!(DRIVER_FLAGS_CPU & hdev->flags))\r\nhdev->flags |= DRIVER_FLAGS_DMA_READY;\r\ntasklet_schedule(&hdev->done_task);\r\n} else {\r\ndev_warn(hdev->dev,\r\n"HASH interrupt when no active requests.\n");\r\n}\r\n} else if (reg & CR_INT_RESULTS_AVAILABLE) {\r\ndev_warn(hdev->dev,\r\n"IRQ triggered before the hash had completed\n");\r\n} else if (reg & CR_INT_RESULT_READ_ERR) {\r\ndev_warn(hdev->dev,\r\n"Attempt to read from an empty result queue\n");\r\n} else if (reg & CR_INT_MESSAGE_WRITE_ERROR) {\r\ndev_warn(hdev->dev,\r\n"Data written before the hardware was configured\n");\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int img_register_algs(struct img_hash_dev *hdev)\r\n{\r\nint i, err;\r\nfor (i = 0; i < ARRAY_SIZE(img_algs); i++) {\r\nerr = crypto_register_ahash(&img_algs[i]);\r\nif (err)\r\ngoto err_reg;\r\n}\r\nreturn 0;\r\nerr_reg:\r\nfor (; i--; )\r\ncrypto_unregister_ahash(&img_algs[i]);\r\nreturn err;\r\n}\r\nstatic int img_unregister_algs(struct img_hash_dev *hdev)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(img_algs); i++)\r\ncrypto_unregister_ahash(&img_algs[i]);\r\nreturn 0;\r\n}\r\nstatic void img_hash_done_task(unsigned long data)\r\n{\r\nstruct img_hash_dev *hdev = (struct img_hash_dev *)data;\r\nint err = 0;\r\nif (hdev->err == -EINVAL) {\r\nerr = hdev->err;\r\ngoto finish;\r\n}\r\nif (!(DRIVER_FLAGS_BUSY & hdev->flags)) {\r\nimg_hash_handle_queue(hdev, NULL);\r\nreturn;\r\n}\r\nif (DRIVER_FLAGS_CPU & hdev->flags) {\r\nif (DRIVER_FLAGS_OUTPUT_READY & hdev->flags) {\r\nhdev->flags &= ~DRIVER_FLAGS_OUTPUT_READY;\r\ngoto finish;\r\n}\r\n} else if (DRIVER_FLAGS_DMA_READY & hdev->flags) {\r\nif (DRIVER_FLAGS_DMA_ACTIVE & hdev->flags) {\r\nhdev->flags &= ~DRIVER_FLAGS_DMA_ACTIVE;\r\nimg_hash_write_via_dma_stop(hdev);\r\nif (hdev->err) {\r\nerr = hdev->err;\r\ngoto finish;\r\n}\r\n}\r\nif (DRIVER_FLAGS_OUTPUT_READY & hdev->flags) {\r\nhdev->flags &= ~(DRIVER_FLAGS_DMA_READY |\r\nDRIVER_FLAGS_OUTPUT_READY);\r\ngoto finish;\r\n}\r\n}\r\nreturn;\r\nfinish:\r\nimg_hash_finish_req(hdev->req, err);\r\n}\r\nstatic int img_hash_probe(struct platform_device *pdev)\r\n{\r\nstruct img_hash_dev *hdev;\r\nstruct device *dev = &pdev->dev;\r\nstruct resource *hash_res;\r\nint irq;\r\nint err;\r\nhdev = devm_kzalloc(dev, sizeof(*hdev), GFP_KERNEL);\r\nif (hdev == NULL)\r\nreturn -ENOMEM;\r\nspin_lock_init(&hdev->lock);\r\nhdev->dev = dev;\r\nplatform_set_drvdata(pdev, hdev);\r\nINIT_LIST_HEAD(&hdev->list);\r\ntasklet_init(&hdev->done_task, img_hash_done_task, (unsigned long)hdev);\r\ntasklet_init(&hdev->dma_task, img_hash_dma_task, (unsigned long)hdev);\r\ncrypto_init_queue(&hdev->queue, IMG_HASH_QUEUE_LENGTH);\r\nhash_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nhdev->io_base = devm_ioremap_resource(dev, hash_res);\r\nif (IS_ERR(hdev->io_base)) {\r\nerr = PTR_ERR(hdev->io_base);\r\ndev_err(dev, "can't ioremap, returned %d\n", err);\r\ngoto res_err;\r\n}\r\nhash_res = platform_get_resource(pdev, IORESOURCE_MEM, 1);\r\nhdev->cpu_addr = devm_ioremap_resource(dev, hash_res);\r\nif (IS_ERR(hdev->cpu_addr)) {\r\ndev_err(dev, "can't ioremap write port\n");\r\nerr = PTR_ERR(hdev->cpu_addr);\r\ngoto res_err;\r\n}\r\nhdev->bus_addr = hash_res->start;\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0) {\r\ndev_err(dev, "no IRQ resource info\n");\r\nerr = irq;\r\ngoto res_err;\r\n}\r\nerr = devm_request_irq(dev, irq, img_irq_handler, 0,\r\ndev_name(dev), hdev);\r\nif (err) {\r\ndev_err(dev, "unable to request irq\n");\r\ngoto res_err;\r\n}\r\ndev_dbg(dev, "using IRQ channel %d\n", irq);\r\nhdev->hash_clk = devm_clk_get(&pdev->dev, "hash");\r\nif (IS_ERR(hdev->hash_clk)) {\r\ndev_err(dev, "clock initialization failed.\n");\r\nerr = PTR_ERR(hdev->hash_clk);\r\ngoto res_err;\r\n}\r\nhdev->sys_clk = devm_clk_get(&pdev->dev, "sys");\r\nif (IS_ERR(hdev->sys_clk)) {\r\ndev_err(dev, "clock initialization failed.\n");\r\nerr = PTR_ERR(hdev->sys_clk);\r\ngoto res_err;\r\n}\r\nerr = clk_prepare_enable(hdev->hash_clk);\r\nif (err)\r\ngoto res_err;\r\nerr = clk_prepare_enable(hdev->sys_clk);\r\nif (err)\r\ngoto clk_err;\r\nerr = img_hash_dma_init(hdev);\r\nif (err)\r\ngoto dma_err;\r\ndev_dbg(dev, "using %s for DMA transfers\n",\r\ndma_chan_name(hdev->dma_lch));\r\nspin_lock(&img_hash.lock);\r\nlist_add_tail(&hdev->list, &img_hash.dev_list);\r\nspin_unlock(&img_hash.lock);\r\nerr = img_register_algs(hdev);\r\nif (err)\r\ngoto err_algs;\r\ndev_info(dev, "Img MD5/SHA1/SHA224/SHA256 Hardware accelerator initialized\n");\r\nreturn 0;\r\nerr_algs:\r\nspin_lock(&img_hash.lock);\r\nlist_del(&hdev->list);\r\nspin_unlock(&img_hash.lock);\r\ndma_release_channel(hdev->dma_lch);\r\ndma_err:\r\nclk_disable_unprepare(hdev->sys_clk);\r\nclk_err:\r\nclk_disable_unprepare(hdev->hash_clk);\r\nres_err:\r\ntasklet_kill(&hdev->done_task);\r\ntasklet_kill(&hdev->dma_task);\r\nreturn err;\r\n}\r\nstatic int img_hash_remove(struct platform_device *pdev)\r\n{\r\nstatic struct img_hash_dev *hdev;\r\nhdev = platform_get_drvdata(pdev);\r\nspin_lock(&img_hash.lock);\r\nlist_del(&hdev->list);\r\nspin_unlock(&img_hash.lock);\r\nimg_unregister_algs(hdev);\r\ntasklet_kill(&hdev->done_task);\r\ntasklet_kill(&hdev->dma_task);\r\ndma_release_channel(hdev->dma_lch);\r\nclk_disable_unprepare(hdev->hash_clk);\r\nclk_disable_unprepare(hdev->sys_clk);\r\nreturn 0;\r\n}\r\nstatic int img_hash_suspend(struct device *dev)\r\n{\r\nstruct img_hash_dev *hdev = dev_get_drvdata(dev);\r\nclk_disable_unprepare(hdev->hash_clk);\r\nclk_disable_unprepare(hdev->sys_clk);\r\nreturn 0;\r\n}\r\nstatic int img_hash_resume(struct device *dev)\r\n{\r\nstruct img_hash_dev *hdev = dev_get_drvdata(dev);\r\nclk_prepare_enable(hdev->hash_clk);\r\nclk_prepare_enable(hdev->sys_clk);\r\nreturn 0;\r\n}
