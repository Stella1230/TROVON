int do_mmap_info(struct rxe_dev *rxe,\r\nstruct ib_udata *udata,\r\nbool is_req,\r\nstruct ib_ucontext *context,\r\nstruct rxe_queue_buf *buf,\r\nsize_t buf_size,\r\nstruct rxe_mmap_info **ip_p)\r\n{\r\nint err;\r\nu32 len, offset;\r\nstruct rxe_mmap_info *ip = NULL;\r\nif (udata) {\r\nif (is_req) {\r\nlen = udata->outlen - sizeof(struct mminfo);\r\noffset = sizeof(struct mminfo);\r\n} else {\r\nlen = udata->outlen;\r\noffset = 0;\r\n}\r\nif (len < sizeof(ip->info))\r\ngoto err1;\r\nip = rxe_create_mmap_info(rxe, buf_size, context, buf);\r\nif (!ip)\r\ngoto err1;\r\nerr = copy_to_user(udata->outbuf + offset, &ip->info,\r\nsizeof(ip->info));\r\nif (err)\r\ngoto err2;\r\nspin_lock_bh(&rxe->pending_lock);\r\nlist_add(&ip->pending_mmaps, &rxe->pending_mmaps);\r\nspin_unlock_bh(&rxe->pending_lock);\r\n}\r\n*ip_p = ip;\r\nreturn 0;\r\nerr2:\r\nkfree(ip);\r\nerr1:\r\nreturn -EINVAL;\r\n}\r\ninline void rxe_queue_reset(struct rxe_queue *q)\r\n{\r\nmemset(q->buf->data, 0, q->buf_size - sizeof(struct rxe_queue_buf));\r\n}\r\nstruct rxe_queue *rxe_queue_init(struct rxe_dev *rxe,\r\nint *num_elem,\r\nunsigned int elem_size)\r\n{\r\nstruct rxe_queue *q;\r\nsize_t buf_size;\r\nunsigned int num_slots;\r\nif (*num_elem < 0)\r\ngoto err1;\r\nq = kmalloc(sizeof(*q), GFP_KERNEL);\r\nif (!q)\r\ngoto err1;\r\nq->rxe = rxe;\r\nq->elem_size = elem_size;\r\nif (elem_size < cache_line_size())\r\nelem_size = cache_line_size();\r\nelem_size = roundup_pow_of_two(elem_size);\r\nq->log2_elem_size = order_base_2(elem_size);\r\nnum_slots = *num_elem + 1;\r\nnum_slots = roundup_pow_of_two(num_slots);\r\nq->index_mask = num_slots - 1;\r\nbuf_size = sizeof(struct rxe_queue_buf) + num_slots * elem_size;\r\nq->buf = vmalloc_user(buf_size);\r\nif (!q->buf)\r\ngoto err2;\r\nq->buf->log2_elem_size = q->log2_elem_size;\r\nq->buf->index_mask = q->index_mask;\r\nq->buf_size = buf_size;\r\n*num_elem = num_slots - 1;\r\nreturn q;\r\nerr2:\r\nkfree(q);\r\nerr1:\r\nreturn NULL;\r\n}\r\nstatic int resize_finish(struct rxe_queue *q, struct rxe_queue *new_q,\r\nunsigned int num_elem)\r\n{\r\nif (!queue_empty(q) && (num_elem < queue_count(q)))\r\nreturn -EINVAL;\r\nwhile (!queue_empty(q)) {\r\nmemcpy(producer_addr(new_q), consumer_addr(q),\r\nnew_q->elem_size);\r\nadvance_producer(new_q);\r\nadvance_consumer(q);\r\n}\r\nswap(*q, *new_q);\r\nreturn 0;\r\n}\r\nint rxe_queue_resize(struct rxe_queue *q,\r\nunsigned int *num_elem_p,\r\nunsigned int elem_size,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata,\r\nspinlock_t *producer_lock,\r\nspinlock_t *consumer_lock)\r\n{\r\nstruct rxe_queue *new_q;\r\nunsigned int num_elem = *num_elem_p;\r\nint err;\r\nunsigned long flags = 0, flags1;\r\nnew_q = rxe_queue_init(q->rxe, &num_elem, elem_size);\r\nif (!new_q)\r\nreturn -ENOMEM;\r\nerr = do_mmap_info(new_q->rxe, udata, false, context, new_q->buf,\r\nnew_q->buf_size, &new_q->ip);\r\nif (err) {\r\nvfree(new_q->buf);\r\nkfree(new_q);\r\ngoto err1;\r\n}\r\nspin_lock_irqsave(consumer_lock, flags1);\r\nif (producer_lock) {\r\nspin_lock_irqsave(producer_lock, flags);\r\nerr = resize_finish(q, new_q, num_elem);\r\nspin_unlock_irqrestore(producer_lock, flags);\r\n} else {\r\nerr = resize_finish(q, new_q, num_elem);\r\n}\r\nspin_unlock_irqrestore(consumer_lock, flags1);\r\nrxe_queue_cleanup(new_q);\r\nif (err)\r\ngoto err1;\r\n*num_elem_p = num_elem;\r\nreturn 0;\r\nerr1:\r\nreturn err;\r\n}\r\nvoid rxe_queue_cleanup(struct rxe_queue *q)\r\n{\r\nif (q->ip)\r\nkref_put(&q->ip->ref, rxe_mmap_release);\r\nelse\r\nvfree(q->buf);\r\nkfree(q);\r\n}
