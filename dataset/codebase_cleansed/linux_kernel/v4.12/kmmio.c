static struct list_head *kmmio_page_list(unsigned long addr)\r\n{\r\nunsigned int l;\r\npte_t *pte = lookup_address(addr, &l);\r\nif (!pte)\r\nreturn NULL;\r\naddr &= page_level_mask(l);\r\nreturn &kmmio_page_table[hash_long(addr, KMMIO_PAGE_HASH_BITS)];\r\n}\r\nstatic struct kmmio_probe *get_kmmio_probe(unsigned long addr)\r\n{\r\nstruct kmmio_probe *p;\r\nlist_for_each_entry_rcu(p, &kmmio_probes, list) {\r\nif (addr >= p->addr && addr < (p->addr + p->len))\r\nreturn p;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct kmmio_fault_page *get_kmmio_fault_page(unsigned long addr)\r\n{\r\nstruct list_head *head;\r\nstruct kmmio_fault_page *f;\r\nunsigned int l;\r\npte_t *pte = lookup_address(addr, &l);\r\nif (!pte)\r\nreturn NULL;\r\naddr &= page_level_mask(l);\r\nhead = kmmio_page_list(addr);\r\nlist_for_each_entry_rcu(f, head, list) {\r\nif (f->addr == addr)\r\nreturn f;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void clear_pmd_presence(pmd_t *pmd, bool clear, pmdval_t *old)\r\n{\r\npmdval_t v = pmd_val(*pmd);\r\nif (clear) {\r\n*old = v & _PAGE_PRESENT;\r\nv &= ~_PAGE_PRESENT;\r\n} else\r\nv |= *old;\r\nset_pmd(pmd, __pmd(v));\r\n}\r\nstatic void clear_pte_presence(pte_t *pte, bool clear, pteval_t *old)\r\n{\r\npteval_t v = pte_val(*pte);\r\nif (clear) {\r\n*old = v & _PAGE_PRESENT;\r\nv &= ~_PAGE_PRESENT;\r\n} else\r\nv |= *old;\r\nset_pte_atomic(pte, __pte(v));\r\n}\r\nstatic int clear_page_presence(struct kmmio_fault_page *f, bool clear)\r\n{\r\nunsigned int level;\r\npte_t *pte = lookup_address(f->addr, &level);\r\nif (!pte) {\r\npr_err("no pte for addr 0x%08lx\n", f->addr);\r\nreturn -1;\r\n}\r\nswitch (level) {\r\ncase PG_LEVEL_2M:\r\nclear_pmd_presence((pmd_t *)pte, clear, &f->old_presence);\r\nbreak;\r\ncase PG_LEVEL_4K:\r\nclear_pte_presence(pte, clear, &f->old_presence);\r\nbreak;\r\ndefault:\r\npr_err("unexpected page level 0x%x.\n", level);\r\nreturn -1;\r\n}\r\n__flush_tlb_one(f->addr);\r\nreturn 0;\r\n}\r\nstatic int arm_kmmio_fault_page(struct kmmio_fault_page *f)\r\n{\r\nint ret;\r\nWARN_ONCE(f->armed, KERN_ERR pr_fmt("kmmio page already armed.\n"));\r\nif (f->armed) {\r\npr_warning("double-arm: addr 0x%08lx, ref %d, old %d\n",\r\nf->addr, f->count, !!f->old_presence);\r\n}\r\nret = clear_page_presence(f, true);\r\nWARN_ONCE(ret < 0, KERN_ERR pr_fmt("arming at 0x%08lx failed.\n"),\r\nf->addr);\r\nf->armed = true;\r\nreturn ret;\r\n}\r\nstatic void disarm_kmmio_fault_page(struct kmmio_fault_page *f)\r\n{\r\nint ret = clear_page_presence(f, false);\r\nWARN_ONCE(ret < 0,\r\nKERN_ERR "kmmio disarming at 0x%08lx failed.\n", f->addr);\r\nf->armed = false;\r\n}\r\nint kmmio_handler(struct pt_regs *regs, unsigned long addr)\r\n{\r\nstruct kmmio_context *ctx;\r\nstruct kmmio_fault_page *faultpage;\r\nint ret = 0;\r\nunsigned long page_base = addr;\r\nunsigned int l;\r\npte_t *pte = lookup_address(addr, &l);\r\nif (!pte)\r\nreturn -EINVAL;\r\npage_base &= page_level_mask(l);\r\npreempt_disable();\r\nrcu_read_lock();\r\nfaultpage = get_kmmio_fault_page(page_base);\r\nif (!faultpage) {\r\ngoto no_kmmio;\r\n}\r\nctx = &get_cpu_var(kmmio_ctx);\r\nif (ctx->active) {\r\nif (page_base == ctx->addr) {\r\npr_debug("secondary hit for 0x%08lx CPU %d.\n",\r\naddr, smp_processor_id());\r\nif (!faultpage->old_presence)\r\npr_info("unexpected secondary hit for address 0x%08lx on CPU %d.\n",\r\naddr, smp_processor_id());\r\n} else {\r\npr_emerg("recursive probe hit on CPU %d, for address 0x%08lx. Ignoring.\n",\r\nsmp_processor_id(), addr);\r\npr_emerg("previous hit was at 0x%08lx.\n", ctx->addr);\r\ndisarm_kmmio_fault_page(faultpage);\r\n}\r\ngoto no_kmmio_ctx;\r\n}\r\nctx->active++;\r\nctx->fpage = faultpage;\r\nctx->probe = get_kmmio_probe(page_base);\r\nctx->saved_flags = (regs->flags & (X86_EFLAGS_TF | X86_EFLAGS_IF));\r\nctx->addr = page_base;\r\nif (ctx->probe && ctx->probe->pre_handler)\r\nctx->probe->pre_handler(ctx->probe, regs, addr);\r\nregs->flags |= X86_EFLAGS_TF;\r\nregs->flags &= ~X86_EFLAGS_IF;\r\ndisarm_kmmio_fault_page(ctx->fpage);\r\nput_cpu_var(kmmio_ctx);\r\nreturn 1;\r\nno_kmmio_ctx:\r\nput_cpu_var(kmmio_ctx);\r\nno_kmmio:\r\nrcu_read_unlock();\r\npreempt_enable_no_resched();\r\nreturn ret;\r\n}\r\nstatic int post_kmmio_handler(unsigned long condition, struct pt_regs *regs)\r\n{\r\nint ret = 0;\r\nstruct kmmio_context *ctx = &get_cpu_var(kmmio_ctx);\r\nif (!ctx->active) {\r\npr_warning("unexpected debug trap on CPU %d.\n",\r\nsmp_processor_id());\r\ngoto out;\r\n}\r\nif (ctx->probe && ctx->probe->post_handler)\r\nctx->probe->post_handler(ctx->probe, condition, regs);\r\nspin_lock(&kmmio_lock);\r\nif (ctx->fpage->count)\r\narm_kmmio_fault_page(ctx->fpage);\r\nspin_unlock(&kmmio_lock);\r\nregs->flags &= ~X86_EFLAGS_TF;\r\nregs->flags |= ctx->saved_flags;\r\nctx->active--;\r\nBUG_ON(ctx->active);\r\nrcu_read_unlock();\r\npreempt_enable_no_resched();\r\nif (!(regs->flags & X86_EFLAGS_TF))\r\nret = 1;\r\nout:\r\nput_cpu_var(kmmio_ctx);\r\nreturn ret;\r\n}\r\nstatic int add_kmmio_fault_page(unsigned long addr)\r\n{\r\nstruct kmmio_fault_page *f;\r\nf = get_kmmio_fault_page(addr);\r\nif (f) {\r\nif (!f->count)\r\narm_kmmio_fault_page(f);\r\nf->count++;\r\nreturn 0;\r\n}\r\nf = kzalloc(sizeof(*f), GFP_ATOMIC);\r\nif (!f)\r\nreturn -1;\r\nf->count = 1;\r\nf->addr = addr;\r\nif (arm_kmmio_fault_page(f)) {\r\nkfree(f);\r\nreturn -1;\r\n}\r\nlist_add_rcu(&f->list, kmmio_page_list(f->addr));\r\nreturn 0;\r\n}\r\nstatic void release_kmmio_fault_page(unsigned long addr,\r\nstruct kmmio_fault_page **release_list)\r\n{\r\nstruct kmmio_fault_page *f;\r\nf = get_kmmio_fault_page(addr);\r\nif (!f)\r\nreturn;\r\nf->count--;\r\nBUG_ON(f->count < 0);\r\nif (!f->count) {\r\ndisarm_kmmio_fault_page(f);\r\nif (!f->scheduled_for_release) {\r\nf->release_next = *release_list;\r\n*release_list = f;\r\nf->scheduled_for_release = true;\r\n}\r\n}\r\n}\r\nint register_kmmio_probe(struct kmmio_probe *p)\r\n{\r\nunsigned long flags;\r\nint ret = 0;\r\nunsigned long size = 0;\r\nconst unsigned long size_lim = p->len + (p->addr & ~PAGE_MASK);\r\nunsigned int l;\r\npte_t *pte;\r\nspin_lock_irqsave(&kmmio_lock, flags);\r\nif (get_kmmio_probe(p->addr)) {\r\nret = -EEXIST;\r\ngoto out;\r\n}\r\npte = lookup_address(p->addr, &l);\r\nif (!pte) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nkmmio_count++;\r\nlist_add_rcu(&p->list, &kmmio_probes);\r\nwhile (size < size_lim) {\r\nif (add_kmmio_fault_page(p->addr + size))\r\npr_err("Unable to set page fault.\n");\r\nsize += page_level_size(l);\r\n}\r\nout:\r\nspin_unlock_irqrestore(&kmmio_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void rcu_free_kmmio_fault_pages(struct rcu_head *head)\r\n{\r\nstruct kmmio_delayed_release *dr = container_of(\r\nhead,\r\nstruct kmmio_delayed_release,\r\nrcu);\r\nstruct kmmio_fault_page *f = dr->release_list;\r\nwhile (f) {\r\nstruct kmmio_fault_page *next = f->release_next;\r\nBUG_ON(f->count);\r\nkfree(f);\r\nf = next;\r\n}\r\nkfree(dr);\r\n}\r\nstatic void remove_kmmio_fault_pages(struct rcu_head *head)\r\n{\r\nstruct kmmio_delayed_release *dr =\r\ncontainer_of(head, struct kmmio_delayed_release, rcu);\r\nstruct kmmio_fault_page *f = dr->release_list;\r\nstruct kmmio_fault_page **prevp = &dr->release_list;\r\nunsigned long flags;\r\nspin_lock_irqsave(&kmmio_lock, flags);\r\nwhile (f) {\r\nif (!f->count) {\r\nlist_del_rcu(&f->list);\r\nprevp = &f->release_next;\r\n} else {\r\n*prevp = f->release_next;\r\nf->release_next = NULL;\r\nf->scheduled_for_release = false;\r\n}\r\nf = *prevp;\r\n}\r\nspin_unlock_irqrestore(&kmmio_lock, flags);\r\ncall_rcu(&dr->rcu, rcu_free_kmmio_fault_pages);\r\n}\r\nvoid unregister_kmmio_probe(struct kmmio_probe *p)\r\n{\r\nunsigned long flags;\r\nunsigned long size = 0;\r\nconst unsigned long size_lim = p->len + (p->addr & ~PAGE_MASK);\r\nstruct kmmio_fault_page *release_list = NULL;\r\nstruct kmmio_delayed_release *drelease;\r\nunsigned int l;\r\npte_t *pte;\r\npte = lookup_address(p->addr, &l);\r\nif (!pte)\r\nreturn;\r\nspin_lock_irqsave(&kmmio_lock, flags);\r\nwhile (size < size_lim) {\r\nrelease_kmmio_fault_page(p->addr + size, &release_list);\r\nsize += page_level_size(l);\r\n}\r\nlist_del_rcu(&p->list);\r\nkmmio_count--;\r\nspin_unlock_irqrestore(&kmmio_lock, flags);\r\nif (!release_list)\r\nreturn;\r\ndrelease = kmalloc(sizeof(*drelease), GFP_ATOMIC);\r\nif (!drelease) {\r\npr_crit("leaking kmmio_fault_page objects.\n");\r\nreturn;\r\n}\r\ndrelease->release_list = release_list;\r\ncall_rcu(&drelease->rcu, remove_kmmio_fault_pages);\r\n}\r\nstatic int\r\nkmmio_die_notifier(struct notifier_block *nb, unsigned long val, void *args)\r\n{\r\nstruct die_args *arg = args;\r\nunsigned long* dr6_p = (unsigned long *)ERR_PTR(arg->err);\r\nif (val == DIE_DEBUG && (*dr6_p & DR_STEP))\r\nif (post_kmmio_handler(*dr6_p, arg->regs) == 1) {\r\n*dr6_p &= ~DR_STEP;\r\nreturn NOTIFY_STOP;\r\n}\r\nreturn NOTIFY_DONE;\r\n}\r\nint kmmio_init(void)\r\n{\r\nint i;\r\nfor (i = 0; i < KMMIO_PAGE_TABLE_SIZE; i++)\r\nINIT_LIST_HEAD(&kmmio_page_table[i]);\r\nreturn register_die_notifier(&nb_die);\r\n}\r\nvoid kmmio_cleanup(void)\r\n{\r\nint i;\r\nunregister_die_notifier(&nb_die);\r\nfor (i = 0; i < KMMIO_PAGE_TABLE_SIZE; i++) {\r\nWARN_ONCE(!list_empty(&kmmio_page_table[i]),\r\nKERN_ERR "kmmio_page_table not empty at cleanup, any further tracing will leak memory.\n");\r\n}\r\n}
