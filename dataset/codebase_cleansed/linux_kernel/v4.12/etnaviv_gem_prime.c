struct sg_table *etnaviv_gem_prime_get_sg_table(struct drm_gem_object *obj)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nint npages = obj->size >> PAGE_SHIFT;\r\nif (WARN_ON(!etnaviv_obj->pages))\r\nreturn NULL;\r\nreturn drm_prime_pages_to_sg(etnaviv_obj->pages, npages);\r\n}\r\nvoid *etnaviv_gem_prime_vmap(struct drm_gem_object *obj)\r\n{\r\nreturn etnaviv_gem_vmap(obj);\r\n}\r\nvoid etnaviv_gem_prime_vunmap(struct drm_gem_object *obj, void *vaddr)\r\n{\r\n}\r\nint etnaviv_gem_prime_mmap(struct drm_gem_object *obj,\r\nstruct vm_area_struct *vma)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nint ret;\r\nret = drm_gem_mmap_obj(obj, obj->size, vma);\r\nif (ret < 0)\r\nreturn ret;\r\nreturn etnaviv_obj->ops->mmap(etnaviv_obj, vma);\r\n}\r\nint etnaviv_gem_prime_pin(struct drm_gem_object *obj)\r\n{\r\nif (!obj->import_attach) {\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nmutex_lock(&etnaviv_obj->lock);\r\netnaviv_gem_get_pages(etnaviv_obj);\r\nmutex_unlock(&etnaviv_obj->lock);\r\n}\r\nreturn 0;\r\n}\r\nvoid etnaviv_gem_prime_unpin(struct drm_gem_object *obj)\r\n{\r\nif (!obj->import_attach) {\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nmutex_lock(&etnaviv_obj->lock);\r\netnaviv_gem_put_pages(to_etnaviv_bo(obj));\r\nmutex_unlock(&etnaviv_obj->lock);\r\n}\r\n}\r\nstatic void etnaviv_gem_prime_release(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nif (etnaviv_obj->vaddr)\r\ndma_buf_vunmap(etnaviv_obj->base.import_attach->dmabuf,\r\netnaviv_obj->vaddr);\r\nif (etnaviv_obj->pages)\r\ndrm_free_large(etnaviv_obj->pages);\r\ndrm_prime_gem_destroy(&etnaviv_obj->base, etnaviv_obj->sgt);\r\n}\r\nstatic void *etnaviv_gem_prime_vmap_impl(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nlockdep_assert_held(&etnaviv_obj->lock);\r\nreturn dma_buf_vmap(etnaviv_obj->base.import_attach->dmabuf);\r\n}\r\nstatic int etnaviv_gem_prime_mmap_obj(struct etnaviv_gem_object *etnaviv_obj,\r\nstruct vm_area_struct *vma)\r\n{\r\nreturn dma_buf_mmap(etnaviv_obj->base.dma_buf, vma, 0);\r\n}\r\nstruct drm_gem_object *etnaviv_gem_prime_import_sg_table(struct drm_device *dev,\r\nstruct dma_buf_attachment *attach, struct sg_table *sgt)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj;\r\nsize_t size = PAGE_ALIGN(attach->dmabuf->size);\r\nint ret, npages;\r\nret = etnaviv_gem_new_private(dev, size, ETNA_BO_WC,\r\nattach->dmabuf->resv,\r\n&etnaviv_gem_prime_ops, &etnaviv_obj);\r\nif (ret < 0)\r\nreturn ERR_PTR(ret);\r\nnpages = size / PAGE_SIZE;\r\netnaviv_obj->sgt = sgt;\r\netnaviv_obj->pages = drm_malloc_ab(npages, sizeof(struct page *));\r\nif (!etnaviv_obj->pages) {\r\nret = -ENOMEM;\r\ngoto fail;\r\n}\r\nret = drm_prime_sg_to_page_addr_arrays(sgt, etnaviv_obj->pages,\r\nNULL, npages);\r\nif (ret)\r\ngoto fail;\r\nret = etnaviv_gem_obj_add(dev, &etnaviv_obj->base);\r\nif (ret)\r\ngoto fail;\r\nreturn &etnaviv_obj->base;\r\nfail:\r\ndrm_gem_object_unreference_unlocked(&etnaviv_obj->base);\r\nreturn ERR_PTR(ret);\r\n}
