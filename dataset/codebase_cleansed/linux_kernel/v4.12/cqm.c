static inline bool __rmid_valid(u32 rmid)\r\n{\r\nif (!rmid || rmid == INVALID_RMID)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic u64 __rmid_read(u32 rmid)\r\n{\r\nu64 val;\r\nwrmsr(MSR_IA32_QM_EVTSEL, QOS_L3_OCCUP_EVENT_ID, rmid);\r\nrdmsrl(MSR_IA32_QM_CTR, val);\r\nreturn val;\r\n}\r\nstatic inline struct cqm_rmid_entry *__rmid_entry(u32 rmid)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nentry = cqm_rmid_ptrs[rmid];\r\nWARN_ON(entry->rmid != rmid);\r\nreturn entry;\r\n}\r\nstatic u32 __get_rmid(void)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nlockdep_assert_held(&cache_mutex);\r\nif (list_empty(&cqm_rmid_free_lru))\r\nreturn INVALID_RMID;\r\nentry = list_first_entry(&cqm_rmid_free_lru, struct cqm_rmid_entry, list);\r\nlist_del(&entry->list);\r\nreturn entry->rmid;\r\n}\r\nstatic void __put_rmid(u32 rmid)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nlockdep_assert_held(&cache_mutex);\r\nWARN_ON(!__rmid_valid(rmid));\r\nentry = __rmid_entry(rmid);\r\nentry->queue_time = jiffies;\r\nentry->state = RMID_YOUNG;\r\nlist_add_tail(&entry->list, &cqm_rmid_limbo_lru);\r\n}\r\nstatic void cqm_cleanup(void)\r\n{\r\nint i;\r\nif (!cqm_rmid_ptrs)\r\nreturn;\r\nfor (i = 0; i < cqm_max_rmid; i++)\r\nkfree(cqm_rmid_ptrs[i]);\r\nkfree(cqm_rmid_ptrs);\r\ncqm_rmid_ptrs = NULL;\r\ncqm_enabled = false;\r\n}\r\nstatic int intel_cqm_setup_rmid_cache(void)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nunsigned int nr_rmids;\r\nint r = 0;\r\nnr_rmids = cqm_max_rmid + 1;\r\ncqm_rmid_ptrs = kzalloc(sizeof(struct cqm_rmid_entry *) *\r\nnr_rmids, GFP_KERNEL);\r\nif (!cqm_rmid_ptrs)\r\nreturn -ENOMEM;\r\nfor (; r <= cqm_max_rmid; r++) {\r\nstruct cqm_rmid_entry *entry;\r\nentry = kmalloc(sizeof(*entry), GFP_KERNEL);\r\nif (!entry)\r\ngoto fail;\r\nINIT_LIST_HEAD(&entry->list);\r\nentry->rmid = r;\r\ncqm_rmid_ptrs[r] = entry;\r\nlist_add_tail(&entry->list, &cqm_rmid_free_lru);\r\n}\r\nentry = __rmid_entry(0);\r\nlist_del(&entry->list);\r\nmutex_lock(&cache_mutex);\r\nintel_cqm_rotation_rmid = __get_rmid();\r\nmutex_unlock(&cache_mutex);\r\nreturn 0;\r\nfail:\r\ncqm_cleanup();\r\nreturn -ENOMEM;\r\n}\r\nstatic bool __match_event(struct perf_event *a, struct perf_event *b)\r\n{\r\nif ((a->attach_state & PERF_ATTACH_TASK) !=\r\n(b->attach_state & PERF_ATTACH_TASK))\r\nreturn false;\r\n#ifdef CONFIG_CGROUP_PERF\r\nif (a->cgrp != b->cgrp)\r\nreturn false;\r\n#endif\r\nif (!(b->attach_state & PERF_ATTACH_TASK))\r\nreturn true;\r\nif (a->hw.target == b->hw.target) {\r\nb->hw.is_group_event = true;\r\nreturn true;\r\n}\r\nif (b->parent == a)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline struct perf_cgroup *event_to_cgroup(struct perf_event *event)\r\n{\r\nif (event->attach_state & PERF_ATTACH_TASK)\r\nreturn perf_cgroup_from_task(event->hw.target, event->ctx);\r\nreturn event->cgrp;\r\n}\r\nstatic bool __conflict_event(struct perf_event *a, struct perf_event *b)\r\n{\r\n#ifdef CONFIG_CGROUP_PERF\r\nif (a->cgrp && b->cgrp) {\r\nstruct perf_cgroup *ac = a->cgrp;\r\nstruct perf_cgroup *bc = b->cgrp;\r\nWARN_ON_ONCE(ac == bc);\r\nif (cgroup_is_descendant(ac->css.cgroup, bc->css.cgroup) ||\r\ncgroup_is_descendant(bc->css.cgroup, ac->css.cgroup))\r\nreturn true;\r\nreturn false;\r\n}\r\nif (a->cgrp || b->cgrp) {\r\nstruct perf_cgroup *ac, *bc;\r\nif ((a->cgrp && !(b->attach_state & PERF_ATTACH_TASK)) ||\r\n(b->cgrp && !(a->attach_state & PERF_ATTACH_TASK)))\r\nreturn true;\r\nac = event_to_cgroup(a);\r\nbc = event_to_cgroup(b);\r\nif (ac == bc)\r\nreturn true;\r\nif (!ac || !bc)\r\nreturn false;\r\nif (cgroup_is_descendant(ac->css.cgroup, bc->css.cgroup) ||\r\ncgroup_is_descendant(bc->css.cgroup, ac->css.cgroup))\r\nreturn true;\r\nreturn false;\r\n}\r\n#endif\r\nif (!(a->attach_state & PERF_ATTACH_TASK) ||\r\n!(b->attach_state & PERF_ATTACH_TASK))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool is_cqm_event(int e)\r\n{\r\nreturn (e == QOS_L3_OCCUP_EVENT_ID);\r\n}\r\nstatic bool is_mbm_event(int e)\r\n{\r\nreturn (e >= QOS_MBM_TOTAL_EVENT_ID && e <= QOS_MBM_LOCAL_EVENT_ID);\r\n}\r\nstatic void cqm_mask_call(struct rmid_read *rr)\r\n{\r\nif (is_mbm_event(rr->evt_type))\r\non_each_cpu_mask(&cqm_cpumask, __intel_mbm_event_count, rr, 1);\r\nelse\r\non_each_cpu_mask(&cqm_cpumask, __intel_cqm_event_count, rr, 1);\r\n}\r\nstatic u32 intel_cqm_xchg_rmid(struct perf_event *group, u32 rmid)\r\n{\r\nstruct perf_event *event;\r\nstruct list_head *head = &group->hw.cqm_group_entry;\r\nu32 old_rmid = group->hw.cqm_rmid;\r\nlockdep_assert_held(&cache_mutex);\r\nif (__rmid_valid(old_rmid) && !__rmid_valid(rmid)) {\r\nstruct rmid_read rr = {\r\n.rmid = old_rmid,\r\n.evt_type = group->attr.config,\r\n.value = ATOMIC64_INIT(0),\r\n};\r\ncqm_mask_call(&rr);\r\nlocal64_set(&group->count, atomic64_read(&rr.value));\r\n}\r\nraw_spin_lock_irq(&cache_lock);\r\ngroup->hw.cqm_rmid = rmid;\r\nlist_for_each_entry(event, head, hw.cqm_group_entry)\r\nevent->hw.cqm_rmid = rmid;\r\nraw_spin_unlock_irq(&cache_lock);\r\nif (__rmid_valid(rmid)) {\r\nevent = group;\r\nif (is_mbm_event(event->attr.config))\r\ninit_mbm_sample(rmid, event->attr.config);\r\nlist_for_each_entry(event, head, hw.cqm_group_entry) {\r\nif (is_mbm_event(event->attr.config))\r\ninit_mbm_sample(rmid, event->attr.config);\r\n}\r\n}\r\nreturn old_rmid;\r\n}\r\nstatic void intel_cqm_stable(void *arg)\r\n{\r\nstruct cqm_rmid_entry *entry;\r\nlist_for_each_entry(entry, &cqm_rmid_limbo_lru, list) {\r\nif (entry->state != RMID_AVAILABLE)\r\nbreak;\r\nif (__rmid_read(entry->rmid) > __intel_cqm_threshold)\r\nentry->state = RMID_DIRTY;\r\n}\r\n}\r\nstatic bool intel_cqm_sched_in_event(u32 rmid)\r\n{\r\nstruct perf_event *leader, *event;\r\nlockdep_assert_held(&cache_mutex);\r\nleader = list_first_entry(&cache_groups, struct perf_event,\r\nhw.cqm_groups_entry);\r\nevent = leader;\r\nlist_for_each_entry_continue(event, &cache_groups,\r\nhw.cqm_groups_entry) {\r\nif (__rmid_valid(event->hw.cqm_rmid))\r\ncontinue;\r\nif (__conflict_event(event, leader))\r\ncontinue;\r\nintel_cqm_xchg_rmid(event, rmid);\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic bool intel_cqm_rmid_stabilize(unsigned int *available)\r\n{\r\nstruct cqm_rmid_entry *entry, *tmp;\r\nlockdep_assert_held(&cache_mutex);\r\n*available = 0;\r\nlist_for_each_entry(entry, &cqm_rmid_limbo_lru, list) {\r\nunsigned long min_queue_time;\r\nunsigned long now = jiffies;\r\nmin_queue_time = entry->queue_time +\r\nmsecs_to_jiffies(__rmid_queue_time_ms);\r\nif (time_after(min_queue_time, now))\r\nbreak;\r\nentry->state = RMID_AVAILABLE;\r\n(*available)++;\r\n}\r\nif (!*available)\r\nreturn false;\r\non_each_cpu_mask(&cqm_cpumask, intel_cqm_stable, NULL, true);\r\nlist_for_each_entry_safe(entry, tmp, &cqm_rmid_limbo_lru, list) {\r\nif (entry->state == RMID_YOUNG)\r\nbreak;\r\nif (entry->state == RMID_DIRTY)\r\ncontinue;\r\nlist_del(&entry->list);\r\nif (!__rmid_valid(intel_cqm_rotation_rmid)) {\r\nintel_cqm_rotation_rmid = entry->rmid;\r\ncontinue;\r\n}\r\nif (intel_cqm_sched_in_event(entry->rmid))\r\ncontinue;\r\nlist_add_tail(&entry->list, &cqm_rmid_free_lru);\r\n}\r\nreturn __rmid_valid(intel_cqm_rotation_rmid);\r\n}\r\nstatic void __intel_cqm_pick_and_rotate(struct perf_event *next)\r\n{\r\nstruct perf_event *rotor;\r\nu32 rmid;\r\nlockdep_assert_held(&cache_mutex);\r\nrotor = list_first_entry(&cache_groups, struct perf_event,\r\nhw.cqm_groups_entry);\r\nif (next == rotor)\r\nreturn;\r\nrmid = intel_cqm_xchg_rmid(rotor, INVALID_RMID);\r\n__put_rmid(rmid);\r\nlist_rotate_left(&cache_groups);\r\n}\r\nstatic void intel_cqm_sched_out_conflicting_events(struct perf_event *event)\r\n{\r\nstruct perf_event *group, *g;\r\nu32 rmid;\r\nlockdep_assert_held(&cache_mutex);\r\nlist_for_each_entry_safe(group, g, &cache_groups, hw.cqm_groups_entry) {\r\nif (group == event)\r\ncontinue;\r\nrmid = group->hw.cqm_rmid;\r\nif (!__rmid_valid(rmid))\r\ncontinue;\r\nif (!__conflict_event(group, event))\r\ncontinue;\r\nintel_cqm_xchg_rmid(group, INVALID_RMID);\r\n__put_rmid(rmid);\r\n}\r\n}\r\nstatic bool __intel_cqm_rmid_rotate(void)\r\n{\r\nstruct perf_event *group, *start = NULL;\r\nunsigned int threshold_limit;\r\nunsigned int nr_needed = 0;\r\nunsigned int nr_available;\r\nbool rotated = false;\r\nmutex_lock(&cache_mutex);\r\nagain:\r\nif (list_empty(&cache_groups) && list_empty(&cqm_rmid_limbo_lru))\r\ngoto out;\r\nlist_for_each_entry(group, &cache_groups, hw.cqm_groups_entry) {\r\nif (!__rmid_valid(group->hw.cqm_rmid)) {\r\nif (!start)\r\nstart = group;\r\nnr_needed++;\r\n}\r\n}\r\nif (!nr_needed && list_empty(&cqm_rmid_limbo_lru))\r\ngoto out;\r\nif (!nr_needed)\r\ngoto stabilize;\r\n__intel_cqm_pick_and_rotate(start);\r\nif (__rmid_valid(intel_cqm_rotation_rmid)) {\r\nintel_cqm_xchg_rmid(start, intel_cqm_rotation_rmid);\r\nintel_cqm_rotation_rmid = __get_rmid();\r\nintel_cqm_sched_out_conflicting_events(start);\r\nif (__intel_cqm_threshold)\r\n__intel_cqm_threshold--;\r\n}\r\nrotated = true;\r\nstabilize:\r\nthreshold_limit = __intel_cqm_max_threshold / cqm_l3_scale;\r\nwhile (intel_cqm_rmid_stabilize(&nr_available) &&\r\n__intel_cqm_threshold < threshold_limit) {\r\nunsigned int steal_limit;\r\nif (!nr_needed)\r\nbreak;\r\nsteal_limit = (cqm_max_rmid + 1) / 4;\r\nif (nr_available < steal_limit)\r\ngoto again;\r\n__intel_cqm_threshold++;\r\n}\r\nout:\r\nmutex_unlock(&cache_mutex);\r\nreturn rotated;\r\n}\r\nstatic void intel_cqm_rmid_rotate(struct work_struct *work)\r\n{\r\nunsigned long delay;\r\n__intel_cqm_rmid_rotate();\r\ndelay = msecs_to_jiffies(intel_cqm_pmu.hrtimer_interval_ms);\r\nschedule_delayed_work(&intel_cqm_rmid_work, delay);\r\n}\r\nstatic u64 update_sample(unsigned int rmid, u32 evt_type, int first)\r\n{\r\nstruct sample *mbm_current;\r\nu32 vrmid = rmid_2_index(rmid);\r\nu64 val, bytes, shift;\r\nu32 eventid;\r\nif (evt_type == QOS_MBM_LOCAL_EVENT_ID) {\r\nmbm_current = &mbm_local[vrmid];\r\neventid = QOS_MBM_LOCAL_EVENT_ID;\r\n} else {\r\nmbm_current = &mbm_total[vrmid];\r\neventid = QOS_MBM_TOTAL_EVENT_ID;\r\n}\r\nwrmsr(MSR_IA32_QM_EVTSEL, eventid, rmid);\r\nrdmsrl(MSR_IA32_QM_CTR, val);\r\nif (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))\r\nreturn mbm_current->total_bytes;\r\nif (first) {\r\nmbm_current->prev_msr = val;\r\nmbm_current->total_bytes = 0;\r\nreturn mbm_current->total_bytes;\r\n}\r\nshift = 64 - MBM_CNTR_WIDTH;\r\nbytes = (val << shift) - (mbm_current->prev_msr << shift);\r\nbytes >>= shift;\r\nbytes *= cqm_l3_scale;\r\nmbm_current->total_bytes += bytes;\r\nmbm_current->prev_msr = val;\r\nreturn mbm_current->total_bytes;\r\n}\r\nstatic u64 rmid_read_mbm(unsigned int rmid, u32 evt_type)\r\n{\r\nreturn update_sample(rmid, evt_type, 0);\r\n}\r\nstatic void __intel_mbm_event_init(void *info)\r\n{\r\nstruct rmid_read *rr = info;\r\nupdate_sample(rr->rmid, rr->evt_type, 1);\r\n}\r\nstatic void init_mbm_sample(u32 rmid, u32 evt_type)\r\n{\r\nstruct rmid_read rr = {\r\n.rmid = rmid,\r\n.evt_type = evt_type,\r\n.value = ATOMIC64_INIT(0),\r\n};\r\non_each_cpu_mask(&cqm_cpumask, __intel_mbm_event_init, &rr, 1);\r\n}\r\nstatic void intel_cqm_setup_event(struct perf_event *event,\r\nstruct perf_event **group)\r\n{\r\nstruct perf_event *iter;\r\nbool conflict = false;\r\nu32 rmid;\r\nevent->hw.is_group_event = false;\r\nlist_for_each_entry(iter, &cache_groups, hw.cqm_groups_entry) {\r\nrmid = iter->hw.cqm_rmid;\r\nif (__match_event(iter, event)) {\r\nevent->hw.cqm_rmid = rmid;\r\n*group = iter;\r\nif (is_mbm_event(event->attr.config) && __rmid_valid(rmid))\r\ninit_mbm_sample(rmid, event->attr.config);\r\nreturn;\r\n}\r\nif (__conflict_event(iter, event) && __rmid_valid(rmid))\r\nconflict = true;\r\n}\r\nif (conflict)\r\nrmid = INVALID_RMID;\r\nelse\r\nrmid = __get_rmid();\r\nif (is_mbm_event(event->attr.config) && __rmid_valid(rmid))\r\ninit_mbm_sample(rmid, event->attr.config);\r\nevent->hw.cqm_rmid = rmid;\r\n}\r\nstatic void intel_cqm_event_read(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nu32 rmid;\r\nu64 val;\r\nif (event->cpu == -1)\r\nreturn;\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nrmid = event->hw.cqm_rmid;\r\nif (!__rmid_valid(rmid))\r\ngoto out;\r\nif (is_mbm_event(event->attr.config))\r\nval = rmid_read_mbm(rmid, event->attr.config);\r\nelse\r\nval = __rmid_read(rmid);\r\nif (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))\r\ngoto out;\r\nlocal64_set(&event->count, val);\r\nout:\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\n}\r\nstatic void __intel_cqm_event_count(void *info)\r\n{\r\nstruct rmid_read *rr = info;\r\nu64 val;\r\nval = __rmid_read(rr->rmid);\r\nif (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))\r\nreturn;\r\natomic64_add(val, &rr->value);\r\n}\r\nstatic inline bool cqm_group_leader(struct perf_event *event)\r\n{\r\nreturn !list_empty(&event->hw.cqm_groups_entry);\r\n}\r\nstatic void __intel_mbm_event_count(void *info)\r\n{\r\nstruct rmid_read *rr = info;\r\nu64 val;\r\nval = rmid_read_mbm(rr->rmid, rr->evt_type);\r\nif (val & (RMID_VAL_ERROR | RMID_VAL_UNAVAIL))\r\nreturn;\r\natomic64_add(val, &rr->value);\r\n}\r\nstatic enum hrtimer_restart mbm_hrtimer_handle(struct hrtimer *hrtimer)\r\n{\r\nstruct perf_event *iter, *iter1;\r\nint ret = HRTIMER_RESTART;\r\nstruct list_head *head;\r\nunsigned long flags;\r\nu32 grp_rmid;\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nif (list_empty(&cache_groups)) {\r\nret = HRTIMER_NORESTART;\r\ngoto out;\r\n}\r\nlist_for_each_entry(iter, &cache_groups, hw.cqm_groups_entry) {\r\ngrp_rmid = iter->hw.cqm_rmid;\r\nif (!__rmid_valid(grp_rmid))\r\ncontinue;\r\nif (is_mbm_event(iter->attr.config))\r\nupdate_sample(grp_rmid, iter->attr.config, 0);\r\nhead = &iter->hw.cqm_group_entry;\r\nif (list_empty(head))\r\ncontinue;\r\nlist_for_each_entry(iter1, head, hw.cqm_group_entry) {\r\nif (!iter1->hw.is_group_event)\r\nbreak;\r\nif (is_mbm_event(iter1->attr.config))\r\nupdate_sample(iter1->hw.cqm_rmid,\r\niter1->attr.config, 0);\r\n}\r\n}\r\nhrtimer_forward_now(hrtimer, ms_to_ktime(MBM_CTR_OVERFLOW_TIME));\r\nout:\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void __mbm_start_timer(void *info)\r\n{\r\nhrtimer_start(&mbm_timers[pkg_id], ms_to_ktime(MBM_CTR_OVERFLOW_TIME),\r\nHRTIMER_MODE_REL_PINNED);\r\n}\r\nstatic void __mbm_stop_timer(void *info)\r\n{\r\nhrtimer_cancel(&mbm_timers[pkg_id]);\r\n}\r\nstatic void mbm_start_timers(void)\r\n{\r\non_each_cpu_mask(&cqm_cpumask, __mbm_start_timer, NULL, 1);\r\n}\r\nstatic void mbm_stop_timers(void)\r\n{\r\non_each_cpu_mask(&cqm_cpumask, __mbm_stop_timer, NULL, 1);\r\n}\r\nstatic void mbm_hrtimer_init(void)\r\n{\r\nstruct hrtimer *hr;\r\nint i;\r\nfor (i = 0; i < mbm_socket_max; i++) {\r\nhr = &mbm_timers[i];\r\nhrtimer_init(hr, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\r\nhr->function = mbm_hrtimer_handle;\r\n}\r\n}\r\nstatic u64 intel_cqm_event_count(struct perf_event *event)\r\n{\r\nunsigned long flags;\r\nstruct rmid_read rr = {\r\n.evt_type = event->attr.config,\r\n.value = ATOMIC64_INIT(0),\r\n};\r\nif (event->cpu != -1)\r\nreturn __perf_event_count(event);\r\nif (!cqm_group_leader(event) && !event->hw.is_group_event)\r\nreturn 0;\r\nif (unlikely(in_interrupt()))\r\ngoto out;\r\nrr.rmid = ACCESS_ONCE(event->hw.cqm_rmid);\r\nif (!__rmid_valid(rr.rmid))\r\ngoto out;\r\ncqm_mask_call(&rr);\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nif (event->hw.cqm_rmid == rr.rmid)\r\nlocal64_set(&event->count, atomic64_read(&rr.value));\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\nout:\r\nreturn __perf_event_count(event);\r\n}\r\nstatic void intel_cqm_event_start(struct perf_event *event, int mode)\r\n{\r\nstruct intel_pqr_state *state = this_cpu_ptr(&pqr_state);\r\nu32 rmid = event->hw.cqm_rmid;\r\nif (!(event->hw.cqm_state & PERF_HES_STOPPED))\r\nreturn;\r\nevent->hw.cqm_state &= ~PERF_HES_STOPPED;\r\nif (state->rmid_usecnt++) {\r\nif (!WARN_ON_ONCE(state->rmid != rmid))\r\nreturn;\r\n} else {\r\nWARN_ON_ONCE(state->rmid);\r\n}\r\nstate->rmid = rmid;\r\nwrmsr(MSR_IA32_PQR_ASSOC, rmid, state->closid);\r\n}\r\nstatic void intel_cqm_event_stop(struct perf_event *event, int mode)\r\n{\r\nstruct intel_pqr_state *state = this_cpu_ptr(&pqr_state);\r\nif (event->hw.cqm_state & PERF_HES_STOPPED)\r\nreturn;\r\nevent->hw.cqm_state |= PERF_HES_STOPPED;\r\nintel_cqm_event_read(event);\r\nif (!--state->rmid_usecnt) {\r\nstate->rmid = 0;\r\nwrmsr(MSR_IA32_PQR_ASSOC, 0, state->closid);\r\n} else {\r\nWARN_ON_ONCE(!state->rmid);\r\n}\r\n}\r\nstatic int intel_cqm_event_add(struct perf_event *event, int mode)\r\n{\r\nunsigned long flags;\r\nu32 rmid;\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nevent->hw.cqm_state = PERF_HES_STOPPED;\r\nrmid = event->hw.cqm_rmid;\r\nif (__rmid_valid(rmid) && (mode & PERF_EF_START))\r\nintel_cqm_event_start(event, mode);\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\nreturn 0;\r\n}\r\nstatic void intel_cqm_event_destroy(struct perf_event *event)\r\n{\r\nstruct perf_event *group_other = NULL;\r\nunsigned long flags;\r\nmutex_lock(&cache_mutex);\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nif (!list_empty(&event->hw.cqm_group_entry)) {\r\ngroup_other = list_first_entry(&event->hw.cqm_group_entry,\r\nstruct perf_event,\r\nhw.cqm_group_entry);\r\nlist_del(&event->hw.cqm_group_entry);\r\n}\r\nif (cqm_group_leader(event)) {\r\nif (group_other) {\r\nlist_replace(&event->hw.cqm_groups_entry,\r\n&group_other->hw.cqm_groups_entry);\r\n} else {\r\nu32 rmid = event->hw.cqm_rmid;\r\nif (__rmid_valid(rmid))\r\n__put_rmid(rmid);\r\nlist_del(&event->hw.cqm_groups_entry);\r\n}\r\n}\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\nif (mbm_enabled && list_empty(&cache_groups))\r\nmbm_stop_timers();\r\nmutex_unlock(&cache_mutex);\r\n}\r\nstatic int intel_cqm_event_init(struct perf_event *event)\r\n{\r\nstruct perf_event *group = NULL;\r\nbool rotate = false;\r\nunsigned long flags;\r\nif (event->attr.type != intel_cqm_pmu.type)\r\nreturn -ENOENT;\r\nif ((event->attr.config < QOS_L3_OCCUP_EVENT_ID) ||\r\n(event->attr.config > QOS_MBM_LOCAL_EVENT_ID))\r\nreturn -EINVAL;\r\nif ((is_cqm_event(event->attr.config) && !cqm_enabled) ||\r\n(is_mbm_event(event->attr.config) && !mbm_enabled))\r\nreturn -EINVAL;\r\nif (event->attr.exclude_user ||\r\nevent->attr.exclude_kernel ||\r\nevent->attr.exclude_hv ||\r\nevent->attr.exclude_idle ||\r\nevent->attr.exclude_host ||\r\nevent->attr.exclude_guest ||\r\nevent->attr.sample_period)\r\nreturn -EINVAL;\r\nINIT_LIST_HEAD(&event->hw.cqm_group_entry);\r\nINIT_LIST_HEAD(&event->hw.cqm_groups_entry);\r\nevent->destroy = intel_cqm_event_destroy;\r\nmutex_lock(&cache_mutex);\r\nif (mbm_enabled && list_empty(&cache_groups))\r\nmbm_start_timers();\r\nintel_cqm_setup_event(event, &group);\r\nraw_spin_lock_irqsave(&cache_lock, flags);\r\nif (group) {\r\nlist_add_tail(&event->hw.cqm_group_entry,\r\n&group->hw.cqm_group_entry);\r\n} else {\r\nlist_add_tail(&event->hw.cqm_groups_entry,\r\n&cache_groups);\r\nif (!__rmid_valid(event->hw.cqm_rmid))\r\nrotate = true;\r\n}\r\nraw_spin_unlock_irqrestore(&cache_lock, flags);\r\nmutex_unlock(&cache_mutex);\r\nif (rotate)\r\nschedule_delayed_work(&intel_cqm_rmid_work, 0);\r\nreturn 0;\r\n}\r\nstatic ssize_t\r\nmax_recycle_threshold_show(struct device *dev, struct device_attribute *attr,\r\nchar *page)\r\n{\r\nssize_t rv;\r\nmutex_lock(&cache_mutex);\r\nrv = snprintf(page, PAGE_SIZE-1, "%u\n", __intel_cqm_max_threshold);\r\nmutex_unlock(&cache_mutex);\r\nreturn rv;\r\n}\r\nstatic ssize_t\r\nmax_recycle_threshold_store(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nunsigned int bytes, cachelines;\r\nint ret;\r\nret = kstrtouint(buf, 0, &bytes);\r\nif (ret)\r\nreturn ret;\r\nmutex_lock(&cache_mutex);\r\n__intel_cqm_max_threshold = bytes;\r\ncachelines = bytes / cqm_l3_scale;\r\nif (__intel_cqm_threshold > cachelines)\r\n__intel_cqm_threshold = cachelines;\r\nmutex_unlock(&cache_mutex);\r\nreturn count;\r\n}\r\nstatic inline void cqm_pick_event_reader(int cpu)\r\n{\r\nint reader;\r\nreader = cpumask_any_and(&cqm_cpumask, topology_core_cpumask(cpu));\r\nif (reader >= nr_cpu_ids)\r\ncpumask_set_cpu(cpu, &cqm_cpumask);\r\n}\r\nstatic int intel_cqm_cpu_starting(unsigned int cpu)\r\n{\r\nstruct intel_pqr_state *state = &per_cpu(pqr_state, cpu);\r\nstruct cpuinfo_x86 *c = &cpu_data(cpu);\r\nstate->rmid = 0;\r\nstate->closid = 0;\r\nstate->rmid_usecnt = 0;\r\nWARN_ON(c->x86_cache_max_rmid != cqm_max_rmid);\r\nWARN_ON(c->x86_cache_occ_scale != cqm_l3_scale);\r\ncqm_pick_event_reader(cpu);\r\nreturn 0;\r\n}\r\nstatic int intel_cqm_cpu_exit(unsigned int cpu)\r\n{\r\nint target;\r\nif (!cpumask_test_and_clear_cpu(cpu, &cqm_cpumask))\r\nreturn 0;\r\ntarget = cpumask_any_but(topology_core_cpumask(cpu), cpu);\r\nif (target < nr_cpu_ids)\r\ncpumask_set_cpu(target, &cqm_cpumask);\r\nreturn 0;\r\n}\r\nstatic void mbm_cleanup(void)\r\n{\r\nif (!mbm_enabled)\r\nreturn;\r\nkfree(mbm_local);\r\nkfree(mbm_total);\r\nmbm_enabled = false;\r\n}\r\nstatic int intel_mbm_init(void)\r\n{\r\nint ret = 0, array_size, maxid = cqm_max_rmid + 1;\r\nmbm_socket_max = topology_max_packages();\r\narray_size = sizeof(struct sample) * maxid * mbm_socket_max;\r\nmbm_local = kmalloc(array_size, GFP_KERNEL);\r\nif (!mbm_local)\r\nreturn -ENOMEM;\r\nmbm_total = kmalloc(array_size, GFP_KERNEL);\r\nif (!mbm_total) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\narray_size = sizeof(struct hrtimer) * mbm_socket_max;\r\nmbm_timers = kmalloc(array_size, GFP_KERNEL);\r\nif (!mbm_timers) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nmbm_hrtimer_init();\r\nout:\r\nif (ret)\r\nmbm_cleanup();\r\nreturn ret;\r\n}\r\nstatic int __init intel_cqm_init(void)\r\n{\r\nchar *str = NULL, scale[20];\r\nint cpu, ret;\r\nif (x86_match_cpu(intel_cqm_match))\r\ncqm_enabled = true;\r\nif (x86_match_cpu(intel_mbm_local_match) &&\r\nx86_match_cpu(intel_mbm_total_match))\r\nmbm_enabled = true;\r\nif (!cqm_enabled && !mbm_enabled)\r\nreturn -ENODEV;\r\ncqm_l3_scale = boot_cpu_data.x86_cache_occ_scale;\r\nget_online_cpus();\r\nfor_each_online_cpu(cpu) {\r\nstruct cpuinfo_x86 *c = &cpu_data(cpu);\r\nif (c->x86_cache_max_rmid < cqm_max_rmid)\r\ncqm_max_rmid = c->x86_cache_max_rmid;\r\nif (c->x86_cache_occ_scale != cqm_l3_scale) {\r\npr_err("Multiple LLC scale values, disabling\n");\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\n__intel_cqm_max_threshold =\r\nboot_cpu_data.x86_cache_size * 1024 / (cqm_max_rmid + 1);\r\nsnprintf(scale, sizeof(scale), "%u", cqm_l3_scale);\r\nstr = kstrdup(scale, GFP_KERNEL);\r\nif (!str) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nevent_attr_intel_cqm_llc_scale.event_str = str;\r\nret = intel_cqm_setup_rmid_cache();\r\nif (ret)\r\ngoto out;\r\nif (mbm_enabled)\r\nret = intel_mbm_init();\r\nif (ret && !cqm_enabled)\r\ngoto out;\r\nif (cqm_enabled && mbm_enabled)\r\nintel_cqm_events_group.attrs = intel_cmt_mbm_events_attr;\r\nelse if (!cqm_enabled && mbm_enabled)\r\nintel_cqm_events_group.attrs = intel_mbm_events_attr;\r\nelse if (cqm_enabled && !mbm_enabled)\r\nintel_cqm_events_group.attrs = intel_cqm_events_attr;\r\nret = perf_pmu_register(&intel_cqm_pmu, "intel_cqm", -1);\r\nif (ret) {\r\npr_err("Intel CQM perf registration failed: %d\n", ret);\r\ngoto out;\r\n}\r\nif (cqm_enabled)\r\npr_info("Intel CQM monitoring enabled\n");\r\nif (mbm_enabled)\r\npr_info("Intel MBM enabled\n");\r\ncpuhp_setup_state(CPUHP_AP_PERF_X86_CQM_STARTING,\r\n"perf/x86/cqm:starting",\r\nintel_cqm_cpu_starting, NULL);\r\ncpuhp_setup_state(CPUHP_AP_PERF_X86_CQM_ONLINE, "perf/x86/cqm:online",\r\nNULL, intel_cqm_cpu_exit);\r\nout:\r\nput_online_cpus();\r\nif (ret) {\r\nkfree(str);\r\ncqm_cleanup();\r\nmbm_cleanup();\r\n}\r\nreturn ret;\r\n}
