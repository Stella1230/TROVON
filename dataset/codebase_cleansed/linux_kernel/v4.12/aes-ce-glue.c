static int num_rounds(struct crypto_aes_ctx *ctx)\r\n{\r\nreturn 6 + ctx->key_length / 4;\r\n}\r\nstatic int ce_aes_expandkey(struct crypto_aes_ctx *ctx, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstatic u8 const rcon[] = {\r\n0x01, 0x02, 0x04, 0x08, 0x10, 0x20, 0x40, 0x80, 0x1b, 0x36,\r\n};\r\nu32 kwords = key_len / sizeof(u32);\r\nstruct aes_block *key_enc, *key_dec;\r\nint i, j;\r\nif (key_len != AES_KEYSIZE_128 &&\r\nkey_len != AES_KEYSIZE_192 &&\r\nkey_len != AES_KEYSIZE_256)\r\nreturn -EINVAL;\r\nmemcpy(ctx->key_enc, in_key, key_len);\r\nctx->key_length = key_len;\r\nkernel_neon_begin();\r\nfor (i = 0; i < sizeof(rcon); i++) {\r\nu32 *rki = ctx->key_enc + (i * kwords);\r\nu32 *rko = rki + kwords;\r\n#ifndef CONFIG_CPU_BIG_ENDIAN\r\nrko[0] = ror32(ce_aes_sub(rki[kwords - 1]), 8);\r\nrko[0] = rko[0] ^ rki[0] ^ rcon[i];\r\n#else\r\nrko[0] = rol32(ce_aes_sub(rki[kwords - 1]), 8);\r\nrko[0] = rko[0] ^ rki[0] ^ (rcon[i] << 24);\r\n#endif\r\nrko[1] = rko[0] ^ rki[1];\r\nrko[2] = rko[1] ^ rki[2];\r\nrko[3] = rko[2] ^ rki[3];\r\nif (key_len == AES_KEYSIZE_192) {\r\nif (i >= 7)\r\nbreak;\r\nrko[4] = rko[3] ^ rki[4];\r\nrko[5] = rko[4] ^ rki[5];\r\n} else if (key_len == AES_KEYSIZE_256) {\r\nif (i >= 6)\r\nbreak;\r\nrko[4] = ce_aes_sub(rko[3]) ^ rki[4];\r\nrko[5] = rko[4] ^ rki[5];\r\nrko[6] = rko[5] ^ rki[6];\r\nrko[7] = rko[6] ^ rki[7];\r\n}\r\n}\r\nkey_enc = (struct aes_block *)ctx->key_enc;\r\nkey_dec = (struct aes_block *)ctx->key_dec;\r\nj = num_rounds(ctx);\r\nkey_dec[0] = key_enc[j];\r\nfor (i = 1, j--; j > 0; i++, j--)\r\nce_aes_invert(key_dec + i, key_enc + j);\r\nkey_dec[i] = key_enc[0];\r\nkernel_neon_end();\r\nreturn 0;\r\n}\r\nstatic int ce_aes_setkey(struct crypto_skcipher *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint ret;\r\nret = ce_aes_expandkey(ctx, in_key, key_len);\r\nif (!ret)\r\nreturn 0;\r\ncrypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic int xts_set_key(struct crypto_skcipher *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct crypto_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint ret;\r\nret = xts_verify_key(tfm, in_key, key_len);\r\nif (ret)\r\nreturn ret;\r\nret = ce_aes_expandkey(&ctx->key1, in_key, key_len / 2);\r\nif (!ret)\r\nret = ce_aes_expandkey(&ctx->key2, &in_key[key_len / 2],\r\nkey_len / 2);\r\nif (!ret)\r\nreturn 0;\r\ncrypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic int ecb_encrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nint err;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_ecb_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, num_rounds(ctx), blocks);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int ecb_decrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nint err;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_ecb_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_dec, num_rounds(ctx), blocks);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int cbc_encrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nint err;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_cbc_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, num_rounds(ctx), blocks,\r\nwalk.iv);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int cbc_decrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nint err;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_cbc_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_dec, num_rounds(ctx), blocks,\r\nwalk.iv);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int ctr_encrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nstruct skcipher_walk walk;\r\nint err, blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\nce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, num_rounds(ctx), blocks,\r\nwalk.iv);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nif (walk.nbytes) {\r\nu8 __aligned(8) tail[AES_BLOCK_SIZE];\r\nunsigned int nbytes = walk.nbytes;\r\nu8 *tdst = walk.dst.virt.addr;\r\nu8 *tsrc = walk.src.virt.addr;\r\nblocks = -1;\r\nce_aes_ctr_encrypt(tail, NULL, (u8 *)ctx->key_enc,\r\nnum_rounds(ctx), blocks, walk.iv);\r\nif (tdst != tsrc)\r\nmemcpy(tdst, tsrc, nbytes);\r\ncrypto_xor(tdst, tail, nbytes);\r\nerr = skcipher_walk_done(&walk, 0);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int xts_encrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = num_rounds(&ctx->key1);\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\nce_aes_xts_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key1.key_enc, rounds, blocks,\r\nwalk.iv, (u8 *)ctx->key2.key_enc, first);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int xts_decrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = num_rounds(&ctx->key1);\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\nce_aes_xts_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key1.key_dec, rounds, blocks,\r\nwalk.iv, (u8 *)ctx->key2.key_enc, first);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic void aes_exit(void)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(aes_simd_algs) && aes_simd_algs[i]; i++)\r\nsimd_skcipher_free(aes_simd_algs[i]);\r\ncrypto_unregister_skciphers(aes_algs, ARRAY_SIZE(aes_algs));\r\n}\r\nstatic int __init aes_init(void)\r\n{\r\nstruct simd_skcipher_alg *simd;\r\nconst char *basename;\r\nconst char *algname;\r\nconst char *drvname;\r\nint err;\r\nint i;\r\nif (!(elf_hwcap2 & HWCAP2_AES))\r\nreturn -ENODEV;\r\nerr = crypto_register_skciphers(aes_algs, ARRAY_SIZE(aes_algs));\r\nif (err)\r\nreturn err;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++) {\r\nalgname = aes_algs[i].base.cra_name + 2;\r\ndrvname = aes_algs[i].base.cra_driver_name + 2;\r\nbasename = aes_algs[i].base.cra_driver_name;\r\nsimd = simd_skcipher_create_compat(algname, drvname, basename);\r\nerr = PTR_ERR(simd);\r\nif (IS_ERR(simd))\r\ngoto unregister_simds;\r\naes_simd_algs[i] = simd;\r\n}\r\nreturn 0;\r\nunregister_simds:\r\naes_exit();\r\nreturn err;\r\n}
