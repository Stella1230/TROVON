static inline void wa_seg_init(struct wa_seg *seg)\r\n{\r\nusb_init_urb(&seg->tr_urb);\r\nmemset(((void *)seg) + sizeof(seg->tr_urb), 0,\r\nsizeof(*seg) - sizeof(seg->tr_urb));\r\n}\r\nstatic inline void wa_xfer_init(struct wa_xfer *xfer)\r\n{\r\nkref_init(&xfer->refcnt);\r\nINIT_LIST_HEAD(&xfer->list_node);\r\nspin_lock_init(&xfer->lock);\r\n}\r\nstatic void wa_xfer_destroy(struct kref *_xfer)\r\n{\r\nstruct wa_xfer *xfer = container_of(_xfer, struct wa_xfer, refcnt);\r\nif (xfer->seg) {\r\nunsigned cnt;\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nstruct wa_seg *seg = xfer->seg[cnt];\r\nif (seg) {\r\nusb_free_urb(seg->isoc_pack_desc_urb);\r\nif (seg->dto_urb) {\r\nkfree(seg->dto_urb->sg);\r\nusb_free_urb(seg->dto_urb);\r\n}\r\nusb_free_urb(&seg->tr_urb);\r\n}\r\n}\r\nkfree(xfer->seg);\r\n}\r\nkfree(xfer);\r\n}\r\nstatic void wa_xfer_get(struct wa_xfer *xfer)\r\n{\r\nkref_get(&xfer->refcnt);\r\n}\r\nstatic void wa_xfer_put(struct wa_xfer *xfer)\r\n{\r\nkref_put(&xfer->refcnt, wa_xfer_destroy);\r\n}\r\nstatic inline int __wa_dto_try_get(struct wahc *wa)\r\n{\r\nreturn (test_and_set_bit(0, &wa->dto_in_use) == 0);\r\n}\r\nstatic inline void __wa_dto_put(struct wahc *wa)\r\n{\r\nclear_bit_unlock(0, &wa->dto_in_use);\r\n}\r\nstatic void wa_check_for_delayed_rpipes(struct wahc *wa)\r\n{\r\nunsigned long flags;\r\nint dto_waiting = 0;\r\nstruct wa_rpipe *rpipe;\r\nspin_lock_irqsave(&wa->rpipe_lock, flags);\r\nwhile (!list_empty(&wa->rpipe_delayed_list) && !dto_waiting) {\r\nrpipe = list_first_entry(&wa->rpipe_delayed_list,\r\nstruct wa_rpipe, list_node);\r\n__wa_xfer_delayed_run(rpipe, &dto_waiting);\r\nif (!dto_waiting) {\r\npr_debug("%s: RPIPE %d serviced and removed from delayed list.\n",\r\n__func__,\r\nle16_to_cpu(rpipe->descr.wRPipeIndex));\r\nlist_del_init(&rpipe->list_node);\r\n}\r\n}\r\nspin_unlock_irqrestore(&wa->rpipe_lock, flags);\r\n}\r\nstatic void wa_add_delayed_rpipe(struct wahc *wa, struct wa_rpipe *rpipe)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&wa->rpipe_lock, flags);\r\nif (list_empty(&rpipe->list_node)) {\r\npr_debug("%s: adding RPIPE %d to the delayed list.\n",\r\n__func__, le16_to_cpu(rpipe->descr.wRPipeIndex));\r\nlist_add_tail(&rpipe->list_node, &wa->rpipe_delayed_list);\r\n}\r\nspin_unlock_irqrestore(&wa->rpipe_lock, flags);\r\n}\r\nstatic void wa_xfer_giveback(struct wa_xfer *xfer)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&xfer->wa->xfer_list_lock, flags);\r\nlist_del_init(&xfer->list_node);\r\nusb_hcd_unlink_urb_from_ep(&(xfer->wa->wusb->usb_hcd), xfer->urb);\r\nspin_unlock_irqrestore(&xfer->wa->xfer_list_lock, flags);\r\nwusbhc_giveback_urb(xfer->wa->wusb, xfer->urb, xfer->result);\r\nwa_put(xfer->wa);\r\nwa_xfer_put(xfer);\r\n}\r\nstatic void wa_xfer_completion(struct wa_xfer *xfer)\r\n{\r\nif (xfer->wusb_dev)\r\nwusb_dev_put(xfer->wusb_dev);\r\nrpipe_put(xfer->ep->hcpriv);\r\nwa_xfer_giveback(xfer);\r\n}\r\nstatic void wa_xfer_id_init(struct wa_xfer *xfer)\r\n{\r\nxfer->id = atomic_add_return(1, &xfer->wa->xfer_id_count);\r\n}\r\nstatic inline u32 wa_xfer_id(struct wa_xfer *xfer)\r\n{\r\nreturn xfer->id;\r\n}\r\nstatic inline __le32 wa_xfer_id_le32(struct wa_xfer *xfer)\r\n{\r\nreturn cpu_to_le32(xfer->id);\r\n}\r\nstatic unsigned __wa_xfer_is_done(struct wa_xfer *xfer)\r\n{\r\nstruct device *dev = &xfer->wa->usb_iface->dev;\r\nunsigned result, cnt;\r\nstruct wa_seg *seg;\r\nstruct urb *urb = xfer->urb;\r\nunsigned found_short = 0;\r\nresult = xfer->segs_done == xfer->segs_submitted;\r\nif (result == 0)\r\ngoto out;\r\nurb->actual_length = 0;\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nseg = xfer->seg[cnt];\r\nswitch (seg->status) {\r\ncase WA_SEG_DONE:\r\nif (found_short && seg->result > 0) {\r\ndev_dbg(dev, "xfer %p ID %08X#%u: bad short segments (%zu)\n",\r\nxfer, wa_xfer_id(xfer), cnt,\r\nseg->result);\r\nurb->status = -EINVAL;\r\ngoto out;\r\n}\r\nurb->actual_length += seg->result;\r\nif (!(usb_pipeisoc(xfer->urb->pipe))\r\n&& seg->result < xfer->seg_size\r\n&& cnt != xfer->segs-1)\r\nfound_short = 1;\r\ndev_dbg(dev, "xfer %p ID %08X#%u: DONE short %d "\r\n"result %zu urb->actual_length %d\n",\r\nxfer, wa_xfer_id(xfer), seg->index, found_short,\r\nseg->result, urb->actual_length);\r\nbreak;\r\ncase WA_SEG_ERROR:\r\nxfer->result = seg->result;\r\ndev_dbg(dev, "xfer %p ID %08X#%u: ERROR result %zi(0x%08zX)\n",\r\nxfer, wa_xfer_id(xfer), seg->index, seg->result,\r\nseg->result);\r\ngoto out;\r\ncase WA_SEG_ABORTED:\r\nxfer->result = seg->result;\r\ndev_dbg(dev, "xfer %p ID %08X#%u: ABORTED result %zi(0x%08zX)\n",\r\nxfer, wa_xfer_id(xfer), seg->index, seg->result,\r\nseg->result);\r\ngoto out;\r\ndefault:\r\ndev_warn(dev, "xfer %p ID %08X#%u: is_done bad state %d\n",\r\nxfer, wa_xfer_id(xfer), cnt, seg->status);\r\nxfer->result = -EINVAL;\r\ngoto out;\r\n}\r\n}\r\nxfer->result = 0;\r\nout:\r\nreturn result;\r\n}\r\nstatic unsigned __wa_xfer_mark_seg_as_done(struct wa_xfer *xfer,\r\nstruct wa_seg *seg, enum wa_seg_status status)\r\n{\r\nseg->status = status;\r\nxfer->segs_done++;\r\nreturn __wa_xfer_is_done(xfer);\r\n}\r\nstatic struct wa_xfer *wa_xfer_get_by_id(struct wahc *wa, u32 id)\r\n{\r\nunsigned long flags;\r\nstruct wa_xfer *xfer_itr;\r\nspin_lock_irqsave(&wa->xfer_list_lock, flags);\r\nlist_for_each_entry(xfer_itr, &wa->xfer_list, list_node) {\r\nif (id == xfer_itr->id) {\r\nwa_xfer_get(xfer_itr);\r\ngoto out;\r\n}\r\n}\r\nxfer_itr = NULL;\r\nout:\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags);\r\nreturn xfer_itr;\r\n}\r\nstatic void __wa_xfer_abort_cb(struct urb *urb)\r\n{\r\nstruct wa_xfer_abort_buffer *b = urb->context;\r\nstruct wahc *wa = b->wa;\r\nif (urb->status < 0) {\r\nstruct wa_xfer *xfer;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nxfer = wa_xfer_get_by_id(wa, le32_to_cpu(b->cmd.dwTransferID));\r\ndev_err(dev, "%s: Transfer Abort request failed. result: %d\n",\r\n__func__, urb->status);\r\nif (xfer) {\r\nunsigned long flags;\r\nint done, seg_index = 0;\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\ndev_err(dev, "%s: cleaning up xfer %p ID 0x%08X.\n",\r\n__func__, xfer, wa_xfer_id(xfer));\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwhile (seg_index < xfer->segs) {\r\nstruct wa_seg *seg = xfer->seg[seg_index];\r\nif ((seg->status == WA_SEG_DONE) ||\r\n(seg->status == WA_SEG_ERROR)) {\r\n++seg_index;\r\n} else {\r\nbreak;\r\n}\r\n}\r\nwa_complete_remaining_xfer_segs(xfer, seg_index,\r\nWA_SEG_ABORTED);\r\ndone = __wa_xfer_is_done(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nwa_xfer_delayed_run(rpipe);\r\nwa_xfer_put(xfer);\r\n} else {\r\ndev_err(dev, "%s: xfer ID 0x%08X already gone.\n",\r\n__func__, le32_to_cpu(b->cmd.dwTransferID));\r\n}\r\n}\r\nwa_put(wa);\r\nusb_put_urb(&b->urb);\r\n}\r\nstatic int __wa_xfer_abort(struct wa_xfer *xfer)\r\n{\r\nint result = -ENOMEM;\r\nstruct device *dev = &xfer->wa->usb_iface->dev;\r\nstruct wa_xfer_abort_buffer *b;\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nb = kmalloc(sizeof(*b), GFP_ATOMIC);\r\nif (b == NULL)\r\ngoto error_kmalloc;\r\nb->cmd.bLength = sizeof(b->cmd);\r\nb->cmd.bRequestType = WA_XFER_ABORT;\r\nb->cmd.wRPipe = rpipe->descr.wRPipeIndex;\r\nb->cmd.dwTransferID = wa_xfer_id_le32(xfer);\r\nb->wa = wa_get(xfer->wa);\r\nusb_init_urb(&b->urb);\r\nusb_fill_bulk_urb(&b->urb, xfer->wa->usb_dev,\r\nusb_sndbulkpipe(xfer->wa->usb_dev,\r\nxfer->wa->dto_epd->bEndpointAddress),\r\n&b->cmd, sizeof(b->cmd), __wa_xfer_abort_cb, b);\r\nresult = usb_submit_urb(&b->urb, GFP_ATOMIC);\r\nif (result < 0)\r\ngoto error_submit;\r\nreturn result;\r\nerror_submit:\r\nwa_put(xfer->wa);\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p: Can't submit abort request: %d\n",\r\nxfer, result);\r\nkfree(b);\r\nerror_kmalloc:\r\nreturn result;\r\n}\r\nstatic int __wa_seg_calculate_isoc_frame_count(struct wa_xfer *xfer,\r\nint isoc_frame_offset, int *total_size)\r\n{\r\nint segment_size = 0, frame_count = 0;\r\nint index = isoc_frame_offset;\r\nstruct usb_iso_packet_descriptor *iso_frame_desc =\r\nxfer->urb->iso_frame_desc;\r\nwhile ((index < xfer->urb->number_of_packets)\r\n&& ((segment_size + iso_frame_desc[index].length)\r\n<= xfer->seg_size)) {\r\nif ((xfer->wa->quirks & WUSB_QUIRK_ALEREON_HWA_CONCAT_ISOC)\r\n&& (xfer->is_inbound == 0)\r\n&& (index > isoc_frame_offset)\r\n&& ((iso_frame_desc[index - 1].offset +\r\niso_frame_desc[index - 1].length) !=\r\niso_frame_desc[index].offset))\r\nbreak;\r\n++frame_count;\r\nsegment_size += iso_frame_desc[index].length;\r\n++index;\r\n}\r\n*total_size = segment_size;\r\nreturn frame_count;\r\n}\r\nstatic ssize_t __wa_xfer_setup_sizes(struct wa_xfer *xfer,\r\nenum wa_xfer_type *pxfer_type)\r\n{\r\nssize_t result;\r\nstruct device *dev = &xfer->wa->usb_iface->dev;\r\nsize_t maxpktsize;\r\nstruct urb *urb = xfer->urb;\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nswitch (rpipe->descr.bmAttribute & 0x3) {\r\ncase USB_ENDPOINT_XFER_CONTROL:\r\n*pxfer_type = WA_XFER_TYPE_CTL;\r\nresult = sizeof(struct wa_xfer_ctl);\r\nbreak;\r\ncase USB_ENDPOINT_XFER_INT:\r\ncase USB_ENDPOINT_XFER_BULK:\r\n*pxfer_type = WA_XFER_TYPE_BI;\r\nresult = sizeof(struct wa_xfer_bi);\r\nbreak;\r\ncase USB_ENDPOINT_XFER_ISOC:\r\n*pxfer_type = WA_XFER_TYPE_ISO;\r\nresult = sizeof(struct wa_xfer_hwaiso);\r\nbreak;\r\ndefault:\r\nBUG();\r\nresult = -EINVAL;\r\n}\r\nxfer->is_inbound = urb->pipe & USB_DIR_IN ? 1 : 0;\r\nxfer->is_dma = urb->transfer_flags & URB_NO_TRANSFER_DMA_MAP ? 1 : 0;\r\nmaxpktsize = le16_to_cpu(rpipe->descr.wMaxPacketSize);\r\nxfer->seg_size = le16_to_cpu(rpipe->descr.wBlocks)\r\n* 1 << (xfer->wa->wa_descr->bRPipeBlockSize - 1);\r\nif (xfer->seg_size < maxpktsize) {\r\ndev_err(dev,\r\n"HW BUG? seg_size %zu smaller than maxpktsize %zu\n",\r\nxfer->seg_size, maxpktsize);\r\nresult = -EINVAL;\r\ngoto error;\r\n}\r\nxfer->seg_size = (xfer->seg_size / maxpktsize) * maxpktsize;\r\nif ((rpipe->descr.bmAttribute & 0x3) == USB_ENDPOINT_XFER_ISOC) {\r\nint index = 0;\r\nxfer->segs = 0;\r\nwhile (index < urb->number_of_packets) {\r\nint seg_size;\r\nindex += __wa_seg_calculate_isoc_frame_count(xfer,\r\nindex, &seg_size);\r\n++xfer->segs;\r\n}\r\n} else {\r\nxfer->segs = DIV_ROUND_UP(urb->transfer_buffer_length,\r\nxfer->seg_size);\r\nif (xfer->segs == 0 && *pxfer_type == WA_XFER_TYPE_CTL)\r\nxfer->segs = 1;\r\n}\r\nif (xfer->segs > WA_SEGS_MAX) {\r\ndev_err(dev, "BUG? oops, number of segments %zu bigger than %d\n",\r\n(urb->transfer_buffer_length/xfer->seg_size),\r\nWA_SEGS_MAX);\r\nresult = -EINVAL;\r\ngoto error;\r\n}\r\nerror:\r\nreturn result;\r\n}\r\nstatic void __wa_setup_isoc_packet_descr(\r\nstruct wa_xfer_packet_info_hwaiso *packet_desc,\r\nstruct wa_xfer *xfer,\r\nstruct wa_seg *seg) {\r\nstruct usb_iso_packet_descriptor *iso_frame_desc =\r\nxfer->urb->iso_frame_desc;\r\nint frame_index;\r\npacket_desc->bPacketType = WA_XFER_ISO_PACKET_INFO;\r\npacket_desc->wLength = cpu_to_le16(sizeof(*packet_desc) +\r\n(sizeof(packet_desc->PacketLength[0]) *\r\nseg->isoc_frame_count));\r\nfor (frame_index = 0; frame_index < seg->isoc_frame_count;\r\n++frame_index) {\r\nint offset_index = frame_index + seg->isoc_frame_offset;\r\npacket_desc->PacketLength[frame_index] =\r\ncpu_to_le16(iso_frame_desc[offset_index].length);\r\n}\r\n}\r\nstatic void __wa_xfer_setup_hdr0(struct wa_xfer *xfer,\r\nstruct wa_xfer_hdr *xfer_hdr0,\r\nenum wa_xfer_type xfer_type,\r\nsize_t xfer_hdr_size)\r\n{\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nstruct wa_seg *seg = xfer->seg[0];\r\nxfer_hdr0 = &seg->xfer_hdr;\r\nxfer_hdr0->bLength = xfer_hdr_size;\r\nxfer_hdr0->bRequestType = xfer_type;\r\nxfer_hdr0->wRPipe = rpipe->descr.wRPipeIndex;\r\nxfer_hdr0->dwTransferID = wa_xfer_id_le32(xfer);\r\nxfer_hdr0->bTransferSegment = 0;\r\nswitch (xfer_type) {\r\ncase WA_XFER_TYPE_CTL: {\r\nstruct wa_xfer_ctl *xfer_ctl =\r\ncontainer_of(xfer_hdr0, struct wa_xfer_ctl, hdr);\r\nxfer_ctl->bmAttribute = xfer->is_inbound ? 1 : 0;\r\nmemcpy(&xfer_ctl->baSetupData, xfer->urb->setup_packet,\r\nsizeof(xfer_ctl->baSetupData));\r\nbreak;\r\n}\r\ncase WA_XFER_TYPE_BI:\r\nbreak;\r\ncase WA_XFER_TYPE_ISO: {\r\nstruct wa_xfer_hwaiso *xfer_iso =\r\ncontainer_of(xfer_hdr0, struct wa_xfer_hwaiso, hdr);\r\nstruct wa_xfer_packet_info_hwaiso *packet_desc =\r\n((void *)xfer_iso) + xfer_hdr_size;\r\nxfer_iso->dwNumOfPackets = cpu_to_le32(seg->isoc_frame_count);\r\n__wa_setup_isoc_packet_descr(packet_desc, xfer, seg);\r\nbreak;\r\n}\r\ndefault:\r\nBUG();\r\n};\r\n}\r\nstatic void wa_seg_dto_cb(struct urb *urb)\r\n{\r\nstruct wa_seg *seg = urb->context;\r\nstruct wa_xfer *xfer = seg->xfer;\r\nstruct wahc *wa;\r\nstruct device *dev;\r\nstruct wa_rpipe *rpipe;\r\nunsigned long flags;\r\nunsigned rpipe_ready = 0;\r\nint data_send_done = 1, release_dto = 0, holding_dto = 0;\r\nu8 done = 0;\r\nint result;\r\nkfree(urb->sg);\r\nurb->sg = NULL;\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\nif (usb_pipeisoc(xfer->urb->pipe)) {\r\nif (wa->quirks & WUSB_QUIRK_ALEREON_HWA_CONCAT_ISOC)\r\nseg->isoc_frame_index += seg->isoc_frame_count;\r\nelse\r\nseg->isoc_frame_index += 1;\r\nif (seg->isoc_frame_index < seg->isoc_frame_count) {\r\ndata_send_done = 0;\r\nholding_dto = 1;\r\nif ((seg->isoc_frame_index + 1) >=\r\nseg->isoc_frame_count)\r\nrelease_dto = 1;\r\n}\r\ndev_dbg(dev, "xfer 0x%08X#%u: isoc frame = %d, holding_dto = %d, release_dto = %d.\n",\r\nwa_xfer_id(xfer), seg->index, seg->isoc_frame_index,\r\nholding_dto, release_dto);\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nswitch (urb->status) {\r\ncase 0:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nseg->result += urb->actual_length;\r\nif (data_send_done) {\r\ndev_dbg(dev, "xfer 0x%08X#%u: data out done (%zu bytes)\n",\r\nwa_xfer_id(xfer), seg->index, seg->result);\r\nif (seg->status < WA_SEG_PENDING)\r\nseg->status = WA_SEG_PENDING;\r\n} else {\r\n__wa_populate_dto_urb_isoc(xfer, seg,\r\nseg->isoc_frame_offset + seg->isoc_frame_index);\r\nwa_xfer_get(xfer);\r\nresult = usb_submit_urb(seg->dto_urb, GFP_ATOMIC);\r\nif (result < 0) {\r\ndev_err(dev, "xfer 0x%08X#%u: DTO submit failed: %d\n",\r\nwa_xfer_id(xfer), seg->index, result);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\ngoto error_dto_submit;\r\n}\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (release_dto) {\r\n__wa_dto_put(wa);\r\nwa_check_for_delayed_rpipes(wa);\r\n}\r\nbreak;\r\ncase -ECONNRESET:\r\ncase -ENOENT:\r\nif (holding_dto) {\r\n__wa_dto_put(wa);\r\nwa_check_for_delayed_rpipes(wa);\r\n}\r\nbreak;\r\ndefault:\r\ndev_err(dev, "xfer 0x%08X#%u: data out error %d\n",\r\nwa_xfer_id(xfer), seg->index, urb->status);\r\ngoto error_default;\r\n}\r\nwa_xfer_put(xfer);\r\nreturn;\r\nerror_dto_submit:\r\nwa_xfer_put(xfer);\r\nerror_default:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nrpipe = xfer->ep->hcpriv;\r\nif (edc_inc(&wa->nep_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)){\r\ndev_err(dev, "DTO: URB max acceptable errors exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nif (seg->status != WA_SEG_ERROR) {\r\nseg->result = urb->status;\r\n__wa_xfer_abort(xfer);\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_mark_seg_as_done(xfer, seg, WA_SEG_ERROR);\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (holding_dto) {\r\n__wa_dto_put(wa);\r\nwa_check_for_delayed_rpipes(wa);\r\n}\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nwa_xfer_put(xfer);\r\n}\r\nstatic void wa_seg_iso_pack_desc_cb(struct urb *urb)\r\n{\r\nstruct wa_seg *seg = urb->context;\r\nstruct wa_xfer *xfer = seg->xfer;\r\nstruct wahc *wa;\r\nstruct device *dev;\r\nstruct wa_rpipe *rpipe;\r\nunsigned long flags;\r\nunsigned rpipe_ready = 0;\r\nu8 done = 0;\r\nswitch (urb->status) {\r\ncase 0:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\ndev_dbg(dev, "iso xfer %08X#%u: packet descriptor done\n",\r\nwa_xfer_id(xfer), seg->index);\r\nif (xfer->is_inbound && seg->status < WA_SEG_PENDING)\r\nseg->status = WA_SEG_PENDING;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nbreak;\r\ncase -ECONNRESET:\r\ncase -ENOENT:\r\nbreak;\r\ndefault:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\nrpipe = xfer->ep->hcpriv;\r\npr_err_ratelimited("iso xfer %08X#%u: packet descriptor error %d\n",\r\nwa_xfer_id(xfer), seg->index, urb->status);\r\nif (edc_inc(&wa->nep_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)){\r\ndev_err(dev, "iso xfer: URB max acceptable errors exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nif (seg->status != WA_SEG_ERROR) {\r\nusb_unlink_urb(seg->dto_urb);\r\nseg->result = urb->status;\r\n__wa_xfer_abort(xfer);\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_mark_seg_as_done(xfer, seg,\r\nWA_SEG_ERROR);\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\n}\r\nwa_xfer_put(xfer);\r\n}\r\nstatic void wa_seg_tr_cb(struct urb *urb)\r\n{\r\nstruct wa_seg *seg = urb->context;\r\nstruct wa_xfer *xfer = seg->xfer;\r\nstruct wahc *wa;\r\nstruct device *dev;\r\nstruct wa_rpipe *rpipe;\r\nunsigned long flags;\r\nunsigned rpipe_ready;\r\nu8 done = 0;\r\nswitch (urb->status) {\r\ncase 0:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\ndev_dbg(dev, "xfer %p ID 0x%08X#%u: request done\n",\r\nxfer, wa_xfer_id(xfer), seg->index);\r\nif (xfer->is_inbound &&\r\nseg->status < WA_SEG_PENDING &&\r\n!(usb_pipeisoc(xfer->urb->pipe)))\r\nseg->status = WA_SEG_PENDING;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nbreak;\r\ncase -ECONNRESET:\r\ncase -ENOENT:\r\nbreak;\r\ndefault:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\nrpipe = xfer->ep->hcpriv;\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p ID 0x%08X#%u: request error %d\n",\r\nxfer, wa_xfer_id(xfer), seg->index,\r\nurb->status);\r\nif (edc_inc(&wa->nep_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)){\r\ndev_err(dev, "DTO: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nusb_unlink_urb(seg->isoc_pack_desc_urb);\r\nusb_unlink_urb(seg->dto_urb);\r\nseg->result = urb->status;\r\n__wa_xfer_abort(xfer);\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_mark_seg_as_done(xfer, seg, WA_SEG_ERROR);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\n}\r\nwa_xfer_put(xfer);\r\n}\r\nstatic struct scatterlist *wa_xfer_create_subset_sg(struct scatterlist *in_sg,\r\nconst unsigned int bytes_transferred,\r\nconst unsigned int bytes_to_transfer, int *out_num_sgs)\r\n{\r\nstruct scatterlist *out_sg;\r\nunsigned int bytes_processed = 0, offset_into_current_page_data = 0,\r\nnents;\r\nstruct scatterlist *current_xfer_sg = in_sg;\r\nstruct scatterlist *current_seg_sg, *last_seg_sg;\r\nwhile ((current_xfer_sg) &&\r\n(bytes_processed < bytes_transferred)) {\r\nbytes_processed += current_xfer_sg->length;\r\nif (bytes_processed <= bytes_transferred)\r\ncurrent_xfer_sg = sg_next(current_xfer_sg);\r\n}\r\nif (bytes_processed > bytes_transferred) {\r\noffset_into_current_page_data = current_xfer_sg->length -\r\n(bytes_processed - bytes_transferred);\r\n}\r\nnents = DIV_ROUND_UP((bytes_to_transfer +\r\noffset_into_current_page_data +\r\ncurrent_xfer_sg->offset),\r\nPAGE_SIZE);\r\nout_sg = kmalloc((sizeof(struct scatterlist) * nents), GFP_ATOMIC);\r\nif (out_sg) {\r\nsg_init_table(out_sg, nents);\r\nlast_seg_sg = current_seg_sg = out_sg;\r\nbytes_processed = 0;\r\nnents = 0;\r\nwhile ((bytes_processed < bytes_to_transfer) &&\r\ncurrent_seg_sg && current_xfer_sg) {\r\nunsigned int page_len = min((current_xfer_sg->length -\r\noffset_into_current_page_data),\r\n(bytes_to_transfer - bytes_processed));\r\nsg_set_page(current_seg_sg, sg_page(current_xfer_sg),\r\npage_len,\r\ncurrent_xfer_sg->offset +\r\noffset_into_current_page_data);\r\nbytes_processed += page_len;\r\nlast_seg_sg = current_seg_sg;\r\ncurrent_seg_sg = sg_next(current_seg_sg);\r\ncurrent_xfer_sg = sg_next(current_xfer_sg);\r\noffset_into_current_page_data = 0;\r\nnents++;\r\n}\r\nsg_mark_end(last_seg_sg);\r\n*out_num_sgs = nents;\r\n}\r\nreturn out_sg;\r\n}\r\nstatic void __wa_populate_dto_urb_isoc(struct wa_xfer *xfer,\r\nstruct wa_seg *seg, int curr_iso_frame)\r\n{\r\nseg->dto_urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\r\nseg->dto_urb->sg = NULL;\r\nseg->dto_urb->num_sgs = 0;\r\nseg->dto_urb->transfer_dma = xfer->urb->transfer_dma +\r\nxfer->urb->iso_frame_desc[curr_iso_frame].offset;\r\nif (xfer->wa->quirks & WUSB_QUIRK_ALEREON_HWA_CONCAT_ISOC)\r\nseg->dto_urb->transfer_buffer_length = seg->isoc_size;\r\nelse\r\nseg->dto_urb->transfer_buffer_length =\r\nxfer->urb->iso_frame_desc[curr_iso_frame].length;\r\n}\r\nstatic int __wa_populate_dto_urb(struct wa_xfer *xfer,\r\nstruct wa_seg *seg, size_t buf_itr_offset, size_t buf_itr_size)\r\n{\r\nint result = 0;\r\nif (xfer->is_dma) {\r\nseg->dto_urb->transfer_dma =\r\nxfer->urb->transfer_dma + buf_itr_offset;\r\nseg->dto_urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\r\nseg->dto_urb->sg = NULL;\r\nseg->dto_urb->num_sgs = 0;\r\n} else {\r\nseg->dto_urb->transfer_flags &=\r\n~URB_NO_TRANSFER_DMA_MAP;\r\nseg->dto_urb->num_mapped_sgs = 0;\r\nif (xfer->urb->transfer_buffer) {\r\nseg->dto_urb->transfer_buffer =\r\nxfer->urb->transfer_buffer +\r\nbuf_itr_offset;\r\nseg->dto_urb->sg = NULL;\r\nseg->dto_urb->num_sgs = 0;\r\n} else {\r\nseg->dto_urb->transfer_buffer = NULL;\r\nseg->dto_urb->sg = wa_xfer_create_subset_sg(\r\nxfer->urb->sg,\r\nbuf_itr_offset, buf_itr_size,\r\n&(seg->dto_urb->num_sgs));\r\nif (!(seg->dto_urb->sg))\r\nresult = -ENOMEM;\r\n}\r\n}\r\nseg->dto_urb->transfer_buffer_length = buf_itr_size;\r\nreturn result;\r\n}\r\nstatic int __wa_xfer_setup_segs(struct wa_xfer *xfer, size_t xfer_hdr_size)\r\n{\r\nint result, cnt, isoc_frame_offset = 0;\r\nsize_t alloc_size = sizeof(*xfer->seg[0])\r\n- sizeof(xfer->seg[0]->xfer_hdr) + xfer_hdr_size;\r\nstruct usb_device *usb_dev = xfer->wa->usb_dev;\r\nconst struct usb_endpoint_descriptor *dto_epd = xfer->wa->dto_epd;\r\nstruct wa_seg *seg;\r\nsize_t buf_itr, buf_size, buf_itr_size;\r\nresult = -ENOMEM;\r\nxfer->seg = kcalloc(xfer->segs, sizeof(xfer->seg[0]), GFP_ATOMIC);\r\nif (xfer->seg == NULL)\r\ngoto error_segs_kzalloc;\r\nbuf_itr = 0;\r\nbuf_size = xfer->urb->transfer_buffer_length;\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nsize_t iso_pkt_descr_size = 0;\r\nint seg_isoc_frame_count = 0, seg_isoc_size = 0;\r\nif (usb_pipeisoc(xfer->urb->pipe)) {\r\nseg_isoc_frame_count =\r\n__wa_seg_calculate_isoc_frame_count(xfer,\r\nisoc_frame_offset, &seg_isoc_size);\r\niso_pkt_descr_size =\r\nsizeof(struct wa_xfer_packet_info_hwaiso) +\r\n(seg_isoc_frame_count * sizeof(__le16));\r\n}\r\nresult = -ENOMEM;\r\nseg = xfer->seg[cnt] = kmalloc(alloc_size + iso_pkt_descr_size,\r\nGFP_ATOMIC);\r\nif (seg == NULL)\r\ngoto error_seg_kmalloc;\r\nwa_seg_init(seg);\r\nseg->xfer = xfer;\r\nseg->index = cnt;\r\nusb_fill_bulk_urb(&seg->tr_urb, usb_dev,\r\nusb_sndbulkpipe(usb_dev,\r\ndto_epd->bEndpointAddress),\r\n&seg->xfer_hdr, xfer_hdr_size,\r\nwa_seg_tr_cb, seg);\r\nbuf_itr_size = min(buf_size, xfer->seg_size);\r\nif (usb_pipeisoc(xfer->urb->pipe)) {\r\nseg->isoc_frame_count = seg_isoc_frame_count;\r\nseg->isoc_frame_offset = isoc_frame_offset;\r\nseg->isoc_size = seg_isoc_size;\r\nseg->isoc_pack_desc_urb =\r\nusb_alloc_urb(0, GFP_ATOMIC);\r\nif (seg->isoc_pack_desc_urb == NULL)\r\ngoto error_iso_pack_desc_alloc;\r\nusb_fill_bulk_urb(\r\nseg->isoc_pack_desc_urb, usb_dev,\r\nusb_sndbulkpipe(usb_dev,\r\ndto_epd->bEndpointAddress),\r\n(void *)(&seg->xfer_hdr) +\r\nxfer_hdr_size,\r\niso_pkt_descr_size,\r\nwa_seg_iso_pack_desc_cb, seg);\r\nisoc_frame_offset += seg_isoc_frame_count;\r\n}\r\nif (xfer->is_inbound == 0 && buf_size > 0) {\r\nseg->dto_urb = usb_alloc_urb(0, GFP_ATOMIC);\r\nif (seg->dto_urb == NULL)\r\ngoto error_dto_alloc;\r\nusb_fill_bulk_urb(\r\nseg->dto_urb, usb_dev,\r\nusb_sndbulkpipe(usb_dev,\r\ndto_epd->bEndpointAddress),\r\nNULL, 0, wa_seg_dto_cb, seg);\r\nif (usb_pipeisoc(xfer->urb->pipe)) {\r\n__wa_populate_dto_urb_isoc(xfer, seg,\r\nseg->isoc_frame_offset);\r\n} else {\r\nresult = __wa_populate_dto_urb(xfer, seg,\r\nbuf_itr, buf_itr_size);\r\nif (result < 0)\r\ngoto error_seg_outbound_populate;\r\nbuf_itr += buf_itr_size;\r\nbuf_size -= buf_itr_size;\r\n}\r\n}\r\nseg->status = WA_SEG_READY;\r\n}\r\nreturn 0;\r\nerror_seg_outbound_populate:\r\nusb_free_urb(xfer->seg[cnt]->dto_urb);\r\nerror_dto_alloc:\r\nusb_free_urb(xfer->seg[cnt]->isoc_pack_desc_urb);\r\nerror_iso_pack_desc_alloc:\r\nkfree(xfer->seg[cnt]);\r\nxfer->seg[cnt] = NULL;\r\nerror_seg_kmalloc:\r\nerror_segs_kzalloc:\r\nreturn result;\r\n}\r\nstatic int __wa_xfer_setup(struct wa_xfer *xfer, struct urb *urb)\r\n{\r\nint result;\r\nstruct device *dev = &xfer->wa->usb_iface->dev;\r\nenum wa_xfer_type xfer_type = 0;\r\nsize_t xfer_hdr_size, cnt, transfer_size;\r\nstruct wa_xfer_hdr *xfer_hdr0, *xfer_hdr;\r\nresult = __wa_xfer_setup_sizes(xfer, &xfer_type);\r\nif (result < 0)\r\ngoto error_setup_sizes;\r\nxfer_hdr_size = result;\r\nresult = __wa_xfer_setup_segs(xfer, xfer_hdr_size);\r\nif (result < 0) {\r\ndev_err(dev, "xfer %p: Failed to allocate %d segments: %d\n",\r\nxfer, xfer->segs, result);\r\ngoto error_setup_segs;\r\n}\r\nxfer_hdr0 = &xfer->seg[0]->xfer_hdr;\r\nwa_xfer_id_init(xfer);\r\n__wa_xfer_setup_hdr0(xfer, xfer_hdr0, xfer_type, xfer_hdr_size);\r\nxfer_hdr = xfer_hdr0;\r\nif (xfer_type == WA_XFER_TYPE_ISO) {\r\nxfer_hdr0->dwTransferLength =\r\ncpu_to_le32(xfer->seg[0]->isoc_size);\r\nfor (cnt = 1; cnt < xfer->segs; cnt++) {\r\nstruct wa_xfer_packet_info_hwaiso *packet_desc;\r\nstruct wa_seg *seg = xfer->seg[cnt];\r\nstruct wa_xfer_hwaiso *xfer_iso;\r\nxfer_hdr = &seg->xfer_hdr;\r\nxfer_iso = container_of(xfer_hdr,\r\nstruct wa_xfer_hwaiso, hdr);\r\npacket_desc = ((void *)xfer_hdr) + xfer_hdr_size;\r\nmemcpy(xfer_hdr, xfer_hdr0, xfer_hdr_size);\r\nxfer_hdr->bTransferSegment = cnt;\r\nxfer_hdr->dwTransferLength =\r\ncpu_to_le32(seg->isoc_size);\r\nxfer_iso->dwNumOfPackets =\r\ncpu_to_le32(seg->isoc_frame_count);\r\n__wa_setup_isoc_packet_descr(packet_desc, xfer, seg);\r\nseg->status = WA_SEG_READY;\r\n}\r\n} else {\r\ntransfer_size = urb->transfer_buffer_length;\r\nxfer_hdr0->dwTransferLength = transfer_size > xfer->seg_size ?\r\ncpu_to_le32(xfer->seg_size) :\r\ncpu_to_le32(transfer_size);\r\ntransfer_size -= xfer->seg_size;\r\nfor (cnt = 1; cnt < xfer->segs; cnt++) {\r\nxfer_hdr = &xfer->seg[cnt]->xfer_hdr;\r\nmemcpy(xfer_hdr, xfer_hdr0, xfer_hdr_size);\r\nxfer_hdr->bTransferSegment = cnt;\r\nxfer_hdr->dwTransferLength =\r\ntransfer_size > xfer->seg_size ?\r\ncpu_to_le32(xfer->seg_size)\r\n: cpu_to_le32(transfer_size);\r\nxfer->seg[cnt]->status = WA_SEG_READY;\r\ntransfer_size -= xfer->seg_size;\r\n}\r\n}\r\nxfer_hdr->bTransferSegment |= 0x80;\r\nresult = 0;\r\nerror_setup_segs:\r\nerror_setup_sizes:\r\nreturn result;\r\n}\r\nstatic int __wa_seg_submit(struct wa_rpipe *rpipe, struct wa_xfer *xfer,\r\nstruct wa_seg *seg, int *dto_done)\r\n{\r\nint result;\r\n*dto_done = 1;\r\nwa_xfer_get(xfer);\r\nseg->status = WA_SEG_SUBMITTED;\r\nresult = usb_submit_urb(&seg->tr_urb, GFP_ATOMIC);\r\nif (result < 0) {\r\npr_err("%s: xfer %p#%u: REQ submit failed: %d\n",\r\n__func__, xfer, seg->index, result);\r\nwa_xfer_put(xfer);\r\ngoto error_tr_submit;\r\n}\r\nif (seg->isoc_pack_desc_urb) {\r\nwa_xfer_get(xfer);\r\nresult = usb_submit_urb(seg->isoc_pack_desc_urb, GFP_ATOMIC);\r\nseg->isoc_frame_index = 0;\r\nif (result < 0) {\r\npr_err("%s: xfer %p#%u: ISO packet descriptor submit failed: %d\n",\r\n__func__, xfer, seg->index, result);\r\nwa_xfer_put(xfer);\r\ngoto error_iso_pack_desc_submit;\r\n}\r\n}\r\nif (seg->dto_urb) {\r\nstruct wahc *wa = xfer->wa;\r\nwa_xfer_get(xfer);\r\nresult = usb_submit_urb(seg->dto_urb, GFP_ATOMIC);\r\nif (result < 0) {\r\npr_err("%s: xfer %p#%u: DTO submit failed: %d\n",\r\n__func__, xfer, seg->index, result);\r\nwa_xfer_put(xfer);\r\ngoto error_dto_submit;\r\n}\r\nif (((wa->quirks & WUSB_QUIRK_ALEREON_HWA_CONCAT_ISOC) == 0)\r\n&& (seg->isoc_frame_count > 1))\r\n*dto_done = 0;\r\n}\r\nrpipe_avail_dec(rpipe);\r\nreturn 0;\r\nerror_dto_submit:\r\nusb_unlink_urb(seg->isoc_pack_desc_urb);\r\nerror_iso_pack_desc_submit:\r\nusb_unlink_urb(&seg->tr_urb);\r\nerror_tr_submit:\r\nseg->status = WA_SEG_ERROR;\r\nseg->result = result;\r\n*dto_done = 1;\r\nreturn result;\r\n}\r\nstatic int __wa_xfer_delayed_run(struct wa_rpipe *rpipe, int *dto_waiting)\r\n{\r\nint result, dto_acquired = 0, dto_done = 0;\r\nstruct device *dev = &rpipe->wa->usb_iface->dev;\r\nstruct wa_seg *seg;\r\nstruct wa_xfer *xfer;\r\nunsigned long flags;\r\n*dto_waiting = 0;\r\nspin_lock_irqsave(&rpipe->seg_lock, flags);\r\nwhile (atomic_read(&rpipe->segs_available) > 0\r\n&& !list_empty(&rpipe->seg_list)\r\n&& (dto_acquired = __wa_dto_try_get(rpipe->wa))) {\r\nseg = list_first_entry(&(rpipe->seg_list), struct wa_seg,\r\nlist_node);\r\nlist_del(&seg->list_node);\r\nxfer = seg->xfer;\r\nwa_xfer_get(xfer);\r\nresult = __wa_seg_submit(rpipe, xfer, seg, &dto_done);\r\nif (dto_done)\r\n__wa_dto_put(rpipe->wa);\r\ndev_dbg(dev, "xfer %p ID %08X#%u submitted from delayed [%d segments available] %d\n",\r\nxfer, wa_xfer_id(xfer), seg->index,\r\natomic_read(&rpipe->segs_available), result);\r\nif (unlikely(result < 0)) {\r\nint done;\r\nspin_unlock_irqrestore(&rpipe->seg_lock, flags);\r\nspin_lock_irqsave(&xfer->lock, flags);\r\n__wa_xfer_abort(xfer);\r\nxfer->segs_done++;\r\ndone = __wa_xfer_is_done(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nspin_lock_irqsave(&rpipe->seg_lock, flags);\r\n}\r\nwa_xfer_put(xfer);\r\n}\r\nif (!dto_acquired && !list_empty(&rpipe->seg_list)\r\n&& (atomic_read(&rpipe->segs_available) ==\r\nle16_to_cpu(rpipe->descr.wRequests)))\r\n*dto_waiting = 1;\r\nspin_unlock_irqrestore(&rpipe->seg_lock, flags);\r\nreturn dto_done;\r\n}\r\nstatic void wa_xfer_delayed_run(struct wa_rpipe *rpipe)\r\n{\r\nint dto_waiting;\r\nint dto_done = __wa_xfer_delayed_run(rpipe, &dto_waiting);\r\nif (dto_waiting)\r\nwa_add_delayed_rpipe(rpipe->wa, rpipe);\r\nelse if (dto_done)\r\nwa_check_for_delayed_rpipes(rpipe->wa);\r\n}\r\nstatic int __wa_xfer_submit(struct wa_xfer *xfer)\r\n{\r\nint result, dto_acquired = 0, dto_done = 0, dto_waiting = 0;\r\nstruct wahc *wa = xfer->wa;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nunsigned cnt;\r\nstruct wa_seg *seg;\r\nunsigned long flags;\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nsize_t maxrequests = le16_to_cpu(rpipe->descr.wRequests);\r\nu8 available;\r\nu8 empty;\r\nspin_lock_irqsave(&wa->xfer_list_lock, flags);\r\nlist_add_tail(&xfer->list_node, &wa->xfer_list);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags);\r\nBUG_ON(atomic_read(&rpipe->segs_available) > maxrequests);\r\nresult = 0;\r\nspin_lock_irqsave(&rpipe->seg_lock, flags);\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nint delay_seg = 1;\r\navailable = atomic_read(&rpipe->segs_available);\r\nempty = list_empty(&rpipe->seg_list);\r\nseg = xfer->seg[cnt];\r\nif (available && empty) {\r\ndto_acquired = __wa_dto_try_get(rpipe->wa);\r\nif (dto_acquired) {\r\ndelay_seg = 0;\r\nresult = __wa_seg_submit(rpipe, xfer, seg,\r\n&dto_done);\r\ndev_dbg(dev, "xfer %p ID 0x%08X#%u: available %u empty %u submitted\n",\r\nxfer, wa_xfer_id(xfer), cnt, available,\r\nempty);\r\nif (dto_done)\r\n__wa_dto_put(rpipe->wa);\r\nif (result < 0) {\r\n__wa_xfer_abort(xfer);\r\ngoto error_seg_submit;\r\n}\r\n}\r\n}\r\nif (delay_seg) {\r\ndev_dbg(dev, "xfer %p ID 0x%08X#%u: available %u empty %u delayed\n",\r\nxfer, wa_xfer_id(xfer), cnt, available, empty);\r\nseg->status = WA_SEG_DELAYED;\r\nlist_add_tail(&seg->list_node, &rpipe->seg_list);\r\n}\r\nxfer->segs_submitted++;\r\n}\r\nerror_seg_submit:\r\nif (!dto_acquired && !list_empty(&rpipe->seg_list)\r\n&& (atomic_read(&rpipe->segs_available) ==\r\nle16_to_cpu(rpipe->descr.wRequests)))\r\ndto_waiting = 1;\r\nspin_unlock_irqrestore(&rpipe->seg_lock, flags);\r\nif (dto_waiting)\r\nwa_add_delayed_rpipe(rpipe->wa, rpipe);\r\nelse if (dto_done)\r\nwa_check_for_delayed_rpipes(rpipe->wa);\r\nreturn result;\r\n}\r\nstatic int wa_urb_enqueue_b(struct wa_xfer *xfer)\r\n{\r\nint result;\r\nunsigned long flags;\r\nstruct urb *urb = xfer->urb;\r\nstruct wahc *wa = xfer->wa;\r\nstruct wusbhc *wusbhc = wa->wusb;\r\nstruct wusb_dev *wusb_dev;\r\nunsigned done;\r\nresult = rpipe_get_by_ep(wa, xfer->ep, urb, xfer->gfp);\r\nif (result < 0) {\r\npr_err("%s: error_rpipe_get\n", __func__);\r\ngoto error_rpipe_get;\r\n}\r\nresult = -ENODEV;\r\nmutex_lock(&wusbhc->mutex);\r\nif (urb->dev == NULL) {\r\nmutex_unlock(&wusbhc->mutex);\r\npr_err("%s: error usb dev gone\n", __func__);\r\ngoto error_dev_gone;\r\n}\r\nwusb_dev = __wusb_dev_get_by_usb_dev(wusbhc, urb->dev);\r\nif (wusb_dev == NULL) {\r\nmutex_unlock(&wusbhc->mutex);\r\ndev_err(&(urb->dev->dev), "%s: error wusb dev gone\n",\r\n__func__);\r\ngoto error_dev_gone;\r\n}\r\nmutex_unlock(&wusbhc->mutex);\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nxfer->wusb_dev = wusb_dev;\r\nresult = urb->status;\r\nif (urb->status != -EINPROGRESS) {\r\ndev_err(&(urb->dev->dev), "%s: error_dequeued\n", __func__);\r\ngoto error_dequeued;\r\n}\r\nresult = __wa_xfer_setup(xfer, urb);\r\nif (result < 0) {\r\ndev_err(&(urb->dev->dev), "%s: error_xfer_setup\n", __func__);\r\ngoto error_xfer_setup;\r\n}\r\nwa_xfer_get(xfer);\r\nresult = __wa_xfer_submit(xfer);\r\nif (result < 0) {\r\ndev_err(&(urb->dev->dev), "%s: error_xfer_submit\n", __func__);\r\ngoto error_xfer_submit;\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nwa_xfer_put(xfer);\r\nreturn 0;\r\nerror_xfer_setup:\r\nerror_dequeued:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (wusb_dev)\r\nwusb_dev_put(wusb_dev);\r\nerror_dev_gone:\r\nrpipe_put(xfer->ep->hcpriv);\r\nerror_rpipe_get:\r\nxfer->result = result;\r\nreturn result;\r\nerror_xfer_submit:\r\ndone = __wa_xfer_is_done(xfer);\r\nxfer->result = result;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nwa_xfer_put(xfer);\r\nreturn 0;\r\n}\r\nvoid wa_urb_enqueue_run(struct work_struct *ws)\r\n{\r\nstruct wahc *wa = container_of(ws, struct wahc, xfer_enqueue_work);\r\nstruct wa_xfer *xfer, *next;\r\nstruct urb *urb;\r\nLIST_HEAD(tmp_list);\r\nspin_lock_irq(&wa->xfer_list_lock);\r\nlist_cut_position(&tmp_list, &wa->xfer_delayed_list,\r\nwa->xfer_delayed_list.prev);\r\nspin_unlock_irq(&wa->xfer_list_lock);\r\nlist_for_each_entry_safe(xfer, next, &tmp_list, list_node) {\r\nlist_del_init(&xfer->list_node);\r\nurb = xfer->urb;\r\nif (wa_urb_enqueue_b(xfer) < 0)\r\nwa_xfer_giveback(xfer);\r\nusb_put_urb(urb);\r\n}\r\n}\r\nvoid wa_process_errored_transfers_run(struct work_struct *ws)\r\n{\r\nstruct wahc *wa = container_of(ws, struct wahc, xfer_error_work);\r\nstruct wa_xfer *xfer, *next;\r\nLIST_HEAD(tmp_list);\r\npr_info("%s: Run delayed STALL processing.\n", __func__);\r\nspin_lock_irq(&wa->xfer_list_lock);\r\nlist_cut_position(&tmp_list, &wa->xfer_errored_list,\r\nwa->xfer_errored_list.prev);\r\nspin_unlock_irq(&wa->xfer_list_lock);\r\nlist_for_each_entry_safe(xfer, next, &tmp_list, list_node) {\r\nstruct usb_host_endpoint *ep;\r\nunsigned long flags;\r\nstruct wa_rpipe *rpipe;\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nep = xfer->ep;\r\nrpipe = ep->hcpriv;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nrpipe_clear_feature_stalled(wa, ep);\r\nwa_xfer_completion(xfer);\r\nwa_xfer_delayed_run(rpipe);\r\n}\r\n}\r\nint wa_urb_enqueue(struct wahc *wa, struct usb_host_endpoint *ep,\r\nstruct urb *urb, gfp_t gfp)\r\n{\r\nint result;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nstruct wa_xfer *xfer;\r\nunsigned long my_flags;\r\nunsigned cant_sleep = irqs_disabled() | in_atomic();\r\nif ((urb->transfer_buffer == NULL)\r\n&& (urb->sg == NULL)\r\n&& !(urb->transfer_flags & URB_NO_TRANSFER_DMA_MAP)\r\n&& urb->transfer_buffer_length != 0) {\r\ndev_err(dev, "BUG? urb %p: NULL xfer buffer & NODMA\n", urb);\r\ndump_stack();\r\n}\r\nspin_lock_irqsave(&wa->xfer_list_lock, my_flags);\r\nresult = usb_hcd_link_urb_to_ep(&(wa->wusb->usb_hcd), urb);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, my_flags);\r\nif (result < 0)\r\ngoto error_link_urb;\r\nresult = -ENOMEM;\r\nxfer = kzalloc(sizeof(*xfer), gfp);\r\nif (xfer == NULL)\r\ngoto error_kmalloc;\r\nresult = -ENOENT;\r\nif (urb->status != -EINPROGRESS)\r\ngoto error_dequeued;\r\nwa_xfer_init(xfer);\r\nxfer->wa = wa_get(wa);\r\nxfer->urb = urb;\r\nxfer->gfp = gfp;\r\nxfer->ep = ep;\r\nurb->hcpriv = xfer;\r\ndev_dbg(dev, "xfer %p urb %p pipe 0x%02x [%d bytes] %s %s %s\n",\r\nxfer, urb, urb->pipe, urb->transfer_buffer_length,\r\nurb->transfer_flags & URB_NO_TRANSFER_DMA_MAP ? "dma" : "nodma",\r\nurb->pipe & USB_DIR_IN ? "inbound" : "outbound",\r\ncant_sleep ? "deferred" : "inline");\r\nif (cant_sleep) {\r\nusb_get_urb(urb);\r\nspin_lock_irqsave(&wa->xfer_list_lock, my_flags);\r\nlist_add_tail(&xfer->list_node, &wa->xfer_delayed_list);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, my_flags);\r\nqueue_work(wusbd, &wa->xfer_enqueue_work);\r\n} else {\r\nresult = wa_urb_enqueue_b(xfer);\r\nif (result < 0) {\r\ndev_err(dev, "%s: URB enqueue failed: %d\n",\r\n__func__, result);\r\nwa_put(xfer->wa);\r\nwa_xfer_put(xfer);\r\nspin_lock_irqsave(&wa->xfer_list_lock, my_flags);\r\nusb_hcd_unlink_urb_from_ep(&(wa->wusb->usb_hcd), urb);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, my_flags);\r\nreturn result;\r\n}\r\n}\r\nreturn 0;\r\nerror_dequeued:\r\nkfree(xfer);\r\nerror_kmalloc:\r\nspin_lock_irqsave(&wa->xfer_list_lock, my_flags);\r\nusb_hcd_unlink_urb_from_ep(&(wa->wusb->usb_hcd), urb);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, my_flags);\r\nerror_link_urb:\r\nreturn result;\r\n}\r\nint wa_urb_dequeue(struct wahc *wa, struct urb *urb, int status)\r\n{\r\nunsigned long flags, flags2;\r\nstruct wa_xfer *xfer;\r\nstruct wa_seg *seg;\r\nstruct wa_rpipe *rpipe;\r\nunsigned cnt, done = 0, xfer_abort_pending;\r\nunsigned rpipe_ready = 0;\r\nint result;\r\nspin_lock_irqsave(&wa->xfer_list_lock, flags);\r\nresult = usb_hcd_check_unlink_urb(&(wa->wusb->usb_hcd), urb, status);\r\nif ((result == 0) && urb->hcpriv) {\r\nwa_xfer_get(urb->hcpriv);\r\n}\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags);\r\nif (result)\r\nreturn result;\r\nxfer = urb->hcpriv;\r\nif (xfer == NULL)\r\nreturn -ENOENT;\r\nspin_lock_irqsave(&xfer->lock, flags);\r\npr_debug("%s: DEQUEUE xfer id 0x%08X\n", __func__, wa_xfer_id(xfer));\r\nrpipe = xfer->ep->hcpriv;\r\nif (rpipe == NULL) {\r\npr_debug("%s: xfer %p id 0x%08X has no RPIPE. %s",\r\n__func__, xfer, wa_xfer_id(xfer),\r\n"Probably already aborted.\n" );\r\nresult = -ENOENT;\r\ngoto out_unlock;\r\n}\r\nif (__wa_xfer_is_done(xfer)) {\r\npr_debug("%s: xfer %p id 0x%08X already done.\n", __func__,\r\nxfer, wa_xfer_id(xfer));\r\nresult = -ENOENT;\r\ngoto out_unlock;\r\n}\r\nspin_lock_irqsave(&wa->xfer_list_lock, flags2);\r\nif (!list_empty(&xfer->list_node) && xfer->seg == NULL)\r\ngoto dequeue_delayed;\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags2);\r\nif (xfer->seg == NULL)\r\ngoto out_unlock;\r\nxfer_abort_pending = __wa_xfer_abort(xfer) >= 0;\r\nspin_lock(&rpipe->seg_lock);\r\nfor (cnt = 0; cnt < xfer->segs; cnt++) {\r\nseg = xfer->seg[cnt];\r\npr_debug("%s: xfer id 0x%08X#%d status = %d\n",\r\n__func__, wa_xfer_id(xfer), cnt, seg->status);\r\nswitch (seg->status) {\r\ncase WA_SEG_NOTREADY:\r\ncase WA_SEG_READY:\r\nprintk(KERN_ERR "xfer %p#%u: dequeue bad state %u\n",\r\nxfer, cnt, seg->status);\r\nWARN_ON(1);\r\nbreak;\r\ncase WA_SEG_DELAYED:\r\nseg->status = WA_SEG_ABORTED;\r\nseg->result = -ENOENT;\r\nlist_del(&seg->list_node);\r\nxfer->segs_done++;\r\nbreak;\r\ncase WA_SEG_DONE:\r\ncase WA_SEG_ERROR:\r\ncase WA_SEG_ABORTED:\r\nbreak;\r\ncase WA_SEG_DTI_PENDING:\r\nbreak;\r\ncase WA_SEG_SUBMITTED:\r\ncase WA_SEG_PENDING:\r\nif (!xfer_abort_pending) {\r\nseg->status = WA_SEG_ABORTED;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\nxfer->segs_done++;\r\n}\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&rpipe->seg_lock);\r\nxfer->result = urb->status;\r\ndone = __wa_xfer_is_done(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nwa_xfer_put(xfer);\r\nreturn result;\r\nout_unlock:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nwa_xfer_put(xfer);\r\nreturn result;\r\ndequeue_delayed:\r\nlist_del_init(&xfer->list_node);\r\nspin_unlock_irqrestore(&wa->xfer_list_lock, flags2);\r\nxfer->result = urb->status;\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nwa_xfer_giveback(xfer);\r\nwa_xfer_put(xfer);\r\nusb_put_urb(urb);\r\nreturn 0;\r\n}\r\nstatic int wa_xfer_status_to_errno(u8 status)\r\n{\r\nint errno;\r\nu8 real_status = status;\r\nstatic int xlat[] = {\r\n[WA_XFER_STATUS_SUCCESS] = 0,\r\n[WA_XFER_STATUS_HALTED] = -EPIPE,\r\n[WA_XFER_STATUS_DATA_BUFFER_ERROR] = -ENOBUFS,\r\n[WA_XFER_STATUS_BABBLE] = -EOVERFLOW,\r\n[WA_XFER_RESERVED] = EINVAL,\r\n[WA_XFER_STATUS_NOT_FOUND] = 0,\r\n[WA_XFER_STATUS_INSUFFICIENT_RESOURCE] = -ENOMEM,\r\n[WA_XFER_STATUS_TRANSACTION_ERROR] = -EILSEQ,\r\n[WA_XFER_STATUS_ABORTED] = -ENOENT,\r\n[WA_XFER_STATUS_RPIPE_NOT_READY] = EINVAL,\r\n[WA_XFER_INVALID_FORMAT] = EINVAL,\r\n[WA_XFER_UNEXPECTED_SEGMENT_NUMBER] = EINVAL,\r\n[WA_XFER_STATUS_RPIPE_TYPE_MISMATCH] = EINVAL,\r\n};\r\nstatus &= 0x3f;\r\nif (status == 0)\r\nreturn 0;\r\nif (status >= ARRAY_SIZE(xlat)) {\r\nprintk_ratelimited(KERN_ERR "%s(): BUG? "\r\n"Unknown WA transfer status 0x%02x\n",\r\n__func__, real_status);\r\nreturn -EINVAL;\r\n}\r\nerrno = xlat[status];\r\nif (unlikely(errno > 0)) {\r\nprintk_ratelimited(KERN_ERR "%s(): BUG? "\r\n"Inconsistent WA status: 0x%02x\n",\r\n__func__, real_status);\r\nerrno = -errno;\r\n}\r\nreturn errno;\r\n}\r\nstatic void wa_complete_remaining_xfer_segs(struct wa_xfer *xfer,\r\nint starting_index, enum wa_seg_status status)\r\n{\r\nint index;\r\nstruct wa_rpipe *rpipe = xfer->ep->hcpriv;\r\nfor (index = starting_index; index < xfer->segs_submitted; index++) {\r\nstruct wa_seg *current_seg = xfer->seg[index];\r\nBUG_ON(current_seg == NULL);\r\nswitch (current_seg->status) {\r\ncase WA_SEG_SUBMITTED:\r\ncase WA_SEG_PENDING:\r\ncase WA_SEG_DTI_PENDING:\r\nrpipe_avail_inc(rpipe);\r\ncase WA_SEG_DELAYED:\r\nxfer->segs_done++;\r\ncurrent_seg->status = status;\r\nbreak;\r\ncase WA_SEG_ABORTED:\r\nbreak;\r\ndefault:\r\nWARN(1, "%s: xfer 0x%08X#%d. bad seg status = %d\n",\r\n__func__, wa_xfer_id(xfer), index,\r\ncurrent_seg->status);\r\nbreak;\r\n}\r\n}\r\n}\r\nstatic int __wa_populate_buf_in_urb_isoc(struct wahc *wa,\r\nstruct urb *buf_in_urb, struct wa_xfer *xfer, struct wa_seg *seg)\r\n{\r\nint urb_start_frame = seg->isoc_frame_index + seg->isoc_frame_offset;\r\nint seg_index, total_len = 0, urb_frame_index = urb_start_frame;\r\nstruct usb_iso_packet_descriptor *iso_frame_desc =\r\nxfer->urb->iso_frame_desc;\r\nconst int dti_packet_size = usb_endpoint_maxp(wa->dti_epd);\r\nint next_frame_contiguous;\r\nstruct usb_iso_packet_descriptor *iso_frame;\r\nBUG_ON(buf_in_urb->status == -EINPROGRESS);\r\nseg_index = seg->isoc_frame_index;\r\ndo {\r\nnext_frame_contiguous = 0;\r\niso_frame = &iso_frame_desc[urb_frame_index];\r\ntotal_len += iso_frame->actual_length;\r\n++urb_frame_index;\r\n++seg_index;\r\nif (seg_index < seg->isoc_frame_count) {\r\nstruct usb_iso_packet_descriptor *next_iso_frame;\r\nnext_iso_frame = &iso_frame_desc[urb_frame_index];\r\nif ((iso_frame->offset + iso_frame->actual_length) ==\r\nnext_iso_frame->offset)\r\nnext_frame_contiguous = 1;\r\n}\r\n} while (next_frame_contiguous\r\n&& ((iso_frame->actual_length % dti_packet_size) == 0));\r\nbuf_in_urb->num_mapped_sgs = 0;\r\nbuf_in_urb->transfer_dma = xfer->urb->transfer_dma +\r\niso_frame_desc[urb_start_frame].offset;\r\nbuf_in_urb->transfer_buffer_length = total_len;\r\nbuf_in_urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\r\nbuf_in_urb->transfer_buffer = NULL;\r\nbuf_in_urb->sg = NULL;\r\nbuf_in_urb->num_sgs = 0;\r\nbuf_in_urb->context = seg;\r\nreturn seg_index - seg->isoc_frame_index;\r\n}\r\nstatic int wa_populate_buf_in_urb(struct urb *buf_in_urb, struct wa_xfer *xfer,\r\nunsigned int seg_idx, unsigned int bytes_transferred)\r\n{\r\nint result = 0;\r\nstruct wa_seg *seg = xfer->seg[seg_idx];\r\nBUG_ON(buf_in_urb->status == -EINPROGRESS);\r\nbuf_in_urb->num_mapped_sgs = 0;\r\nif (xfer->is_dma) {\r\nbuf_in_urb->transfer_dma = xfer->urb->transfer_dma\r\n+ (seg_idx * xfer->seg_size);\r\nbuf_in_urb->transfer_flags |= URB_NO_TRANSFER_DMA_MAP;\r\nbuf_in_urb->transfer_buffer = NULL;\r\nbuf_in_urb->sg = NULL;\r\nbuf_in_urb->num_sgs = 0;\r\n} else {\r\nbuf_in_urb->transfer_flags &= ~URB_NO_TRANSFER_DMA_MAP;\r\nif (xfer->urb->transfer_buffer) {\r\nbuf_in_urb->transfer_buffer =\r\nxfer->urb->transfer_buffer\r\n+ (seg_idx * xfer->seg_size);\r\nbuf_in_urb->sg = NULL;\r\nbuf_in_urb->num_sgs = 0;\r\n} else {\r\nbuf_in_urb->sg = wa_xfer_create_subset_sg(\r\nxfer->urb->sg,\r\nseg_idx * xfer->seg_size,\r\nbytes_transferred,\r\n&(buf_in_urb->num_sgs));\r\nif (!(buf_in_urb->sg)) {\r\nbuf_in_urb->num_sgs = 0;\r\nresult = -ENOMEM;\r\n}\r\nbuf_in_urb->transfer_buffer = NULL;\r\n}\r\n}\r\nbuf_in_urb->transfer_buffer_length = bytes_transferred;\r\nbuf_in_urb->context = seg;\r\nreturn result;\r\n}\r\nstatic void wa_xfer_result_chew(struct wahc *wa, struct wa_xfer *xfer,\r\nstruct wa_xfer_result *xfer_result)\r\n{\r\nint result;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nunsigned long flags;\r\nunsigned int seg_idx;\r\nstruct wa_seg *seg;\r\nstruct wa_rpipe *rpipe;\r\nunsigned done = 0;\r\nu8 usb_status;\r\nunsigned rpipe_ready = 0;\r\nunsigned bytes_transferred = le32_to_cpu(xfer_result->dwTransferLength);\r\nstruct urb *buf_in_urb = &(wa->buf_in_urbs[0]);\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nseg_idx = xfer_result->bTransferSegment & 0x7f;\r\nif (unlikely(seg_idx >= xfer->segs))\r\ngoto error_bad_seg;\r\nseg = xfer->seg[seg_idx];\r\nrpipe = xfer->ep->hcpriv;\r\nusb_status = xfer_result->bTransferStatus;\r\ndev_dbg(dev, "xfer %p ID 0x%08X#%u: bTransferStatus 0x%02x (seg status %u)\n",\r\nxfer, wa_xfer_id(xfer), seg_idx, usb_status, seg->status);\r\nif (seg->status == WA_SEG_ABORTED\r\n|| seg->status == WA_SEG_ERROR)\r\ngoto segment_aborted;\r\nif (seg->status == WA_SEG_SUBMITTED)\r\nseg->status = WA_SEG_PENDING;\r\nif (seg->status != WA_SEG_PENDING) {\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p#%u: Bad segment state %u\n",\r\nxfer, seg_idx, seg->status);\r\nseg->status = WA_SEG_PENDING;\r\n}\r\nif (usb_status & 0x80) {\r\nseg->result = wa_xfer_status_to_errno(usb_status);\r\ndev_err(dev, "DTI: xfer %p 0x%08X:#%u failed (0x%02x)\n",\r\nxfer, xfer->id, seg->index, usb_status);\r\nseg->status = ((usb_status & 0x7F) == WA_XFER_STATUS_ABORTED) ?\r\nWA_SEG_ABORTED : WA_SEG_ERROR;\r\ngoto error_complete;\r\n}\r\nif (usb_status & 0x40)\r\nusb_status = 0;\r\nif (xfer_result->bTransferSegment & 0x80)\r\nwa_complete_remaining_xfer_segs(xfer, seg->index + 1,\r\nWA_SEG_DONE);\r\nif (usb_pipeisoc(xfer->urb->pipe)\r\n&& (le32_to_cpu(xfer_result->dwNumOfPackets) > 0)) {\r\nwa->dti_isoc_xfer_in_progress = wa_xfer_id(xfer);\r\nwa->dti_isoc_xfer_seg = seg_idx;\r\nwa->dti_state = WA_DTI_ISOC_PACKET_STATUS_PENDING;\r\n} else if (xfer->is_inbound && !usb_pipeisoc(xfer->urb->pipe)\r\n&& (bytes_transferred > 0)) {\r\nseg->status = WA_SEG_DTI_PENDING;\r\nresult = wa_populate_buf_in_urb(buf_in_urb, xfer, seg_idx,\r\nbytes_transferred);\r\nif (result < 0)\r\ngoto error_buf_in_populate;\r\n++(wa->active_buf_in_urbs);\r\nresult = usb_submit_urb(buf_in_urb, GFP_ATOMIC);\r\nif (result < 0) {\r\n--(wa->active_buf_in_urbs);\r\ngoto error_submit_buf_in;\r\n}\r\n} else {\r\nseg->result = bytes_transferred;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_mark_seg_as_done(xfer, seg, WA_SEG_DONE);\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nreturn;\r\nerror_submit_buf_in:\r\nif (edc_inc(&wa->dti_edc, EDC_MAX_ERRORS, EDC_ERROR_TIMEFRAME)) {\r\ndev_err(dev, "DTI: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p#%u: can't submit DTI data phase: %d\n",\r\nxfer, seg_idx, result);\r\nseg->result = result;\r\nkfree(buf_in_urb->sg);\r\nbuf_in_urb->sg = NULL;\r\nerror_buf_in_populate:\r\n__wa_xfer_abort(xfer);\r\nseg->status = WA_SEG_ERROR;\r\nerror_complete:\r\nxfer->segs_done++;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\nwa_complete_remaining_xfer_segs(xfer, seg->index + 1, seg->status);\r\ndone = __wa_xfer_is_done(xfer);\r\nif (((usb_status & 0x3f) == WA_XFER_STATUS_HALTED) &&\r\nusb_endpoint_xfer_control(&xfer->ep->desc) &&\r\ndone) {\r\ndev_info(dev, "Control EP stall. Queue delayed work.\n");\r\nspin_lock(&wa->xfer_list_lock);\r\nlist_move_tail(&xfer->list_node, &wa->xfer_errored_list);\r\nspin_unlock(&wa->xfer_list_lock);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nqueue_work(wusbd, &wa->xfer_error_work);\r\n} else {\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\n}\r\nreturn;\r\nerror_bad_seg:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nwa_urb_dequeue(wa, xfer->urb, -ENOENT);\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p#%u: bad segment\n", xfer, seg_idx);\r\nif (edc_inc(&wa->dti_edc, EDC_MAX_ERRORS, EDC_ERROR_TIMEFRAME)) {\r\ndev_err(dev, "DTI: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nreturn;\r\nsegment_aborted:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\n}\r\nstatic int wa_process_iso_packet_status(struct wahc *wa, struct urb *urb)\r\n{\r\nstruct device *dev = &wa->usb_iface->dev;\r\nstruct wa_xfer_packet_status_hwaiso *packet_status;\r\nstruct wa_xfer_packet_status_len_hwaiso *status_array;\r\nstruct wa_xfer *xfer;\r\nunsigned long flags;\r\nstruct wa_seg *seg;\r\nstruct wa_rpipe *rpipe;\r\nunsigned done = 0, dti_busy = 0, data_frame_count = 0, seg_index;\r\nunsigned first_frame_index = 0, rpipe_ready = 0;\r\nint expected_size;\r\ndev_dbg(dev, "DTI: isoc packet status %d bytes at %p\n",\r\nurb->actual_length, urb->transfer_buffer);\r\npacket_status = (struct wa_xfer_packet_status_hwaiso *)(wa->dti_buf);\r\nif (packet_status->bPacketType != WA_XFER_ISO_PACKET_STATUS) {\r\ndev_err(dev, "DTI Error: isoc packet status--bad type 0x%02x\n",\r\npacket_status->bPacketType);\r\ngoto error_parse_buffer;\r\n}\r\nxfer = wa_xfer_get_by_id(wa, wa->dti_isoc_xfer_in_progress);\r\nif (xfer == NULL) {\r\ndev_err(dev, "DTI Error: isoc packet status--unknown xfer 0x%08x\n",\r\nwa->dti_isoc_xfer_in_progress);\r\ngoto error_parse_buffer;\r\n}\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nif (unlikely(wa->dti_isoc_xfer_seg >= xfer->segs))\r\ngoto error_bad_seg;\r\nseg = xfer->seg[wa->dti_isoc_xfer_seg];\r\nrpipe = xfer->ep->hcpriv;\r\nexpected_size = sizeof(*packet_status) +\r\n(sizeof(packet_status->PacketStatus[0]) *\r\nseg->isoc_frame_count);\r\nif (urb->actual_length != expected_size) {\r\ndev_err(dev, "DTI Error: isoc packet status--bad urb length (%d bytes vs %d needed)\n",\r\nurb->actual_length, expected_size);\r\ngoto error_bad_seg;\r\n}\r\nif (le16_to_cpu(packet_status->wLength) != expected_size) {\r\ndev_err(dev, "DTI Error: isoc packet status--bad length %u\n",\r\nle16_to_cpu(packet_status->wLength));\r\ngoto error_bad_seg;\r\n}\r\nstatus_array = packet_status->PacketStatus;\r\nxfer->urb->start_frame =\r\nwa->wusb->usb_hcd.driver->get_frame_number(&wa->wusb->usb_hcd);\r\nfor (seg_index = 0; seg_index < seg->isoc_frame_count; ++seg_index) {\r\nstruct usb_iso_packet_descriptor *iso_frame_desc =\r\nxfer->urb->iso_frame_desc;\r\nconst int xfer_frame_index =\r\nseg->isoc_frame_offset + seg_index;\r\niso_frame_desc[xfer_frame_index].status =\r\nwa_xfer_status_to_errno(\r\nle16_to_cpu(status_array[seg_index].PacketStatus));\r\niso_frame_desc[xfer_frame_index].actual_length =\r\nle16_to_cpu(status_array[seg_index].PacketLength);\r\nif (iso_frame_desc[xfer_frame_index].actual_length > 0) {\r\nif (!data_frame_count)\r\nfirst_frame_index = seg_index;\r\n++data_frame_count;\r\n}\r\n}\r\nif (xfer->is_inbound && data_frame_count) {\r\nint result, total_frames_read = 0, urb_index = 0;\r\nstruct urb *buf_in_urb;\r\nseg->status = WA_SEG_DTI_PENDING;\r\nseg->isoc_frame_index = first_frame_index;\r\ndo {\r\nint urb_frame_index, urb_frame_count;\r\nstruct usb_iso_packet_descriptor *iso_frame_desc;\r\nbuf_in_urb = &(wa->buf_in_urbs[urb_index]);\r\nurb_frame_count = __wa_populate_buf_in_urb_isoc(wa,\r\nbuf_in_urb, xfer, seg);\r\nseg->isoc_frame_index += urb_frame_count;\r\ntotal_frames_read += urb_frame_count;\r\n++(wa->active_buf_in_urbs);\r\nresult = usb_submit_urb(buf_in_urb, GFP_ATOMIC);\r\nurb_frame_index =\r\nseg->isoc_frame_offset + seg->isoc_frame_index;\r\niso_frame_desc =\r\n&(xfer->urb->iso_frame_desc[urb_frame_index]);\r\nwhile ((seg->isoc_frame_index <\r\nseg->isoc_frame_count) &&\r\n(iso_frame_desc->actual_length == 0)) {\r\n++(seg->isoc_frame_index);\r\n++iso_frame_desc;\r\n}\r\n++urb_index;\r\n} while ((result == 0) && (urb_index < WA_MAX_BUF_IN_URBS)\r\n&& (seg->isoc_frame_index <\r\nseg->isoc_frame_count));\r\nif (result < 0) {\r\n--(wa->active_buf_in_urbs);\r\ndev_err(dev, "DTI Error: Could not submit buf in URB (%d)",\r\nresult);\r\nwa_reset_all(wa);\r\n} else if (data_frame_count > total_frames_read)\r\ndti_busy = 1;\r\n} else {\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_mark_seg_as_done(xfer, seg, WA_SEG_DONE);\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (dti_busy)\r\nwa->dti_state = WA_DTI_BUF_IN_DATA_PENDING;\r\nelse\r\nwa->dti_state = WA_DTI_TRANSFER_RESULT_PENDING;\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nwa_xfer_put(xfer);\r\nreturn dti_busy;\r\nerror_bad_seg:\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nwa_xfer_put(xfer);\r\nerror_parse_buffer:\r\nreturn dti_busy;\r\n}\r\nstatic void wa_buf_in_cb(struct urb *urb)\r\n{\r\nstruct wa_seg *seg = urb->context;\r\nstruct wa_xfer *xfer = seg->xfer;\r\nstruct wahc *wa;\r\nstruct device *dev;\r\nstruct wa_rpipe *rpipe;\r\nunsigned rpipe_ready = 0, isoc_data_frame_count = 0;\r\nunsigned long flags;\r\nint resubmit_dti = 0, active_buf_in_urbs;\r\nu8 done = 0;\r\nkfree(urb->sg);\r\nurb->sg = NULL;\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nwa = xfer->wa;\r\ndev = &wa->usb_iface->dev;\r\n--(wa->active_buf_in_urbs);\r\nactive_buf_in_urbs = wa->active_buf_in_urbs;\r\nrpipe = xfer->ep->hcpriv;\r\nif (usb_pipeisoc(xfer->urb->pipe)) {\r\nstruct usb_iso_packet_descriptor *iso_frame_desc =\r\nxfer->urb->iso_frame_desc;\r\nint seg_index;\r\nseg_index = seg->isoc_frame_index;\r\nwhile (seg_index < seg->isoc_frame_count) {\r\nconst int urb_frame_index =\r\nseg->isoc_frame_offset + seg_index;\r\nif (iso_frame_desc[urb_frame_index].actual_length > 0) {\r\nif (!isoc_data_frame_count)\r\nseg->isoc_frame_index = seg_index;\r\n++isoc_data_frame_count;\r\n}\r\n++seg_index;\r\n}\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nswitch (urb->status) {\r\ncase 0:\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nseg->result += urb->actual_length;\r\nif (isoc_data_frame_count > 0) {\r\nint result, urb_frame_count;\r\nurb_frame_count = __wa_populate_buf_in_urb_isoc(wa, urb,\r\nxfer, seg);\r\nseg->isoc_frame_index += urb_frame_count;\r\n++(wa->active_buf_in_urbs);\r\nresult = usb_submit_urb(urb, GFP_ATOMIC);\r\nif (result < 0) {\r\n--(wa->active_buf_in_urbs);\r\ndev_err(dev, "DTI Error: Could not submit buf in URB (%d)",\r\nresult);\r\nwa_reset_all(wa);\r\n}\r\nresubmit_dti = (isoc_data_frame_count ==\r\nurb_frame_count);\r\n} else if (active_buf_in_urbs == 0) {\r\ndev_dbg(dev,\r\n"xfer %p 0x%08X#%u: data in done (%zu bytes)\n",\r\nxfer, wa_xfer_id(xfer), seg->index,\r\nseg->result);\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\ndone = __wa_xfer_mark_seg_as_done(xfer, seg,\r\nWA_SEG_DONE);\r\n}\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\nbreak;\r\ncase -ECONNRESET:\r\ncase -ENOENT:\r\nbreak;\r\ndefault:\r\nresubmit_dti = wa->dti_state != WA_DTI_TRANSFER_RESULT_PENDING;\r\nspin_lock_irqsave(&xfer->lock, flags);\r\nif (printk_ratelimit())\r\ndev_err(dev, "xfer %p 0x%08X#%u: data in error %d\n",\r\nxfer, wa_xfer_id(xfer), seg->index,\r\nurb->status);\r\nif (edc_inc(&wa->nep_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)){\r\ndev_err(dev, "DTO: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\n}\r\nseg->result = urb->status;\r\nrpipe_ready = rpipe_avail_inc(rpipe);\r\nif (active_buf_in_urbs == 0)\r\ndone = __wa_xfer_mark_seg_as_done(xfer, seg,\r\nWA_SEG_ERROR);\r\nelse\r\n__wa_xfer_abort(xfer);\r\nspin_unlock_irqrestore(&xfer->lock, flags);\r\nif (done)\r\nwa_xfer_completion(xfer);\r\nif (rpipe_ready)\r\nwa_xfer_delayed_run(rpipe);\r\n}\r\nif (resubmit_dti) {\r\nint result;\r\nwa->dti_state = WA_DTI_TRANSFER_RESULT_PENDING;\r\nresult = usb_submit_urb(wa->dti_urb, GFP_ATOMIC);\r\nif (result < 0) {\r\ndev_err(dev, "DTI Error: Could not submit DTI URB (%d)\n",\r\nresult);\r\nwa_reset_all(wa);\r\n}\r\n}\r\n}\r\nstatic void wa_dti_cb(struct urb *urb)\r\n{\r\nint result, dti_busy = 0;\r\nstruct wahc *wa = urb->context;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nu32 xfer_id;\r\nu8 usb_status;\r\nBUG_ON(wa->dti_urb != urb);\r\nswitch (wa->dti_urb->status) {\r\ncase 0:\r\nif (wa->dti_state == WA_DTI_TRANSFER_RESULT_PENDING) {\r\nstruct wa_xfer_result *xfer_result;\r\nstruct wa_xfer *xfer;\r\ndev_dbg(dev, "DTI: xfer result %d bytes at %p\n",\r\nurb->actual_length, urb->transfer_buffer);\r\nif (urb->actual_length != sizeof(*xfer_result)) {\r\ndev_err(dev, "DTI Error: xfer result--bad size xfer result (%d bytes vs %zu needed)\n",\r\nurb->actual_length,\r\nsizeof(*xfer_result));\r\nbreak;\r\n}\r\nxfer_result = (struct wa_xfer_result *)(wa->dti_buf);\r\nif (xfer_result->hdr.bLength != sizeof(*xfer_result)) {\r\ndev_err(dev, "DTI Error: xfer result--bad header length %u\n",\r\nxfer_result->hdr.bLength);\r\nbreak;\r\n}\r\nif (xfer_result->hdr.bNotifyType != WA_XFER_RESULT) {\r\ndev_err(dev, "DTI Error: xfer result--bad header type 0x%02x\n",\r\nxfer_result->hdr.bNotifyType);\r\nbreak;\r\n}\r\nxfer_id = le32_to_cpu(xfer_result->dwTransferID);\r\nusb_status = xfer_result->bTransferStatus & 0x3f;\r\nif (usb_status == WA_XFER_STATUS_NOT_FOUND) {\r\ndev_dbg(dev, "%s: xfer 0x%08X#%u not found.\n",\r\n__func__, xfer_id,\r\nxfer_result->bTransferSegment & 0x7f);\r\nbreak;\r\n}\r\nxfer = wa_xfer_get_by_id(wa, xfer_id);\r\nif (xfer == NULL) {\r\ndev_err(dev, "DTI Error: xfer result--unknown xfer 0x%08x (status 0x%02x)\n",\r\nxfer_id, usb_status);\r\nbreak;\r\n}\r\nwa_xfer_result_chew(wa, xfer, xfer_result);\r\nwa_xfer_put(xfer);\r\n} else if (wa->dti_state == WA_DTI_ISOC_PACKET_STATUS_PENDING) {\r\ndti_busy = wa_process_iso_packet_status(wa, urb);\r\n} else {\r\ndev_err(dev, "DTI Error: unexpected EP state = %d\n",\r\nwa->dti_state);\r\n}\r\nbreak;\r\ncase -ENOENT:\r\ncase -ESHUTDOWN:\r\ndev_dbg(dev, "DTI: going down! %d\n", urb->status);\r\ngoto out;\r\ndefault:\r\nif (edc_inc(&wa->dti_edc, EDC_MAX_ERRORS,\r\nEDC_ERROR_TIMEFRAME)) {\r\ndev_err(dev, "DTI: URB max acceptable errors "\r\n"exceeded, resetting device\n");\r\nwa_reset_all(wa);\r\ngoto out;\r\n}\r\nif (printk_ratelimit())\r\ndev_err(dev, "DTI: URB error %d\n", urb->status);\r\nbreak;\r\n}\r\nif (!dti_busy) {\r\nresult = usb_submit_urb(wa->dti_urb, GFP_ATOMIC);\r\nif (result < 0) {\r\ndev_err(dev, "DTI Error: Could not submit DTI URB (%d)\n",\r\nresult);\r\nwa_reset_all(wa);\r\n}\r\n}\r\nout:\r\nreturn;\r\n}\r\nint wa_dti_start(struct wahc *wa)\r\n{\r\nconst struct usb_endpoint_descriptor *dti_epd = wa->dti_epd;\r\nstruct device *dev = &wa->usb_iface->dev;\r\nint result = -ENOMEM, index;\r\nif (wa->dti_urb != NULL)\r\ngoto out;\r\nwa->dti_urb = usb_alloc_urb(0, GFP_KERNEL);\r\nif (wa->dti_urb == NULL)\r\ngoto error_dti_urb_alloc;\r\nusb_fill_bulk_urb(\r\nwa->dti_urb, wa->usb_dev,\r\nusb_rcvbulkpipe(wa->usb_dev, 0x80 | dti_epd->bEndpointAddress),\r\nwa->dti_buf, wa->dti_buf_size,\r\nwa_dti_cb, wa);\r\nfor (index = 0; index < WA_MAX_BUF_IN_URBS; ++index) {\r\nusb_fill_bulk_urb(\r\n&(wa->buf_in_urbs[index]), wa->usb_dev,\r\nusb_rcvbulkpipe(wa->usb_dev,\r\n0x80 | dti_epd->bEndpointAddress),\r\nNULL, 0, wa_buf_in_cb, wa);\r\n}\r\nresult = usb_submit_urb(wa->dti_urb, GFP_KERNEL);\r\nif (result < 0) {\r\ndev_err(dev, "DTI Error: Could not submit DTI URB (%d) resetting\n",\r\nresult);\r\ngoto error_dti_urb_submit;\r\n}\r\nout:\r\nreturn 0;\r\nerror_dti_urb_submit:\r\nusb_put_urb(wa->dti_urb);\r\nwa->dti_urb = NULL;\r\nerror_dti_urb_alloc:\r\nreturn result;\r\n}\r\nvoid wa_handle_notif_xfer(struct wahc *wa, struct wa_notif_hdr *notif_hdr)\r\n{\r\nstruct device *dev = &wa->usb_iface->dev;\r\nstruct wa_notif_xfer *notif_xfer;\r\nconst struct usb_endpoint_descriptor *dti_epd = wa->dti_epd;\r\nnotif_xfer = container_of(notif_hdr, struct wa_notif_xfer, hdr);\r\nBUG_ON(notif_hdr->bNotifyType != WA_NOTIF_TRANSFER);\r\nif ((0x80 | notif_xfer->bEndpoint) != dti_epd->bEndpointAddress) {\r\ndev_err(dev, "BUG: DTI ep is %u, not %u (hack me)\n",\r\nnotif_xfer->bEndpoint, dti_epd->bEndpointAddress);\r\ngoto error;\r\n}\r\nif (wa_dti_start(wa) < 0)\r\ngoto error;\r\nreturn;\r\nerror:\r\nwa_reset_all(wa);\r\n}
