static inline u32 iproc_msi_read_reg(struct iproc_msi *msi,\r\nenum iproc_msi_reg reg,\r\nunsigned int eq)\r\n{\r\nstruct iproc_pcie *pcie = msi->pcie;\r\nreturn readl_relaxed(pcie->base + msi->reg_offsets[eq][reg]);\r\n}\r\nstatic inline void iproc_msi_write_reg(struct iproc_msi *msi,\r\nenum iproc_msi_reg reg,\r\nint eq, u32 val)\r\n{\r\nstruct iproc_pcie *pcie = msi->pcie;\r\nwritel_relaxed(val, pcie->base + msi->reg_offsets[eq][reg]);\r\n}\r\nstatic inline u32 hwirq_to_group(struct iproc_msi *msi, unsigned long hwirq)\r\n{\r\nreturn (hwirq % msi->nr_irqs);\r\n}\r\nstatic inline unsigned int iproc_msi_addr_offset(struct iproc_msi *msi,\r\nunsigned long hwirq)\r\n{\r\nif (msi->nr_msi_region > 1)\r\nreturn hwirq_to_group(msi, hwirq) * MSI_MEM_REGION_SIZE;\r\nelse\r\nreturn hwirq_to_group(msi, hwirq) * sizeof(u32);\r\n}\r\nstatic inline unsigned int iproc_msi_eq_offset(struct iproc_msi *msi, u32 eq)\r\n{\r\nif (msi->nr_eq_region > 1)\r\nreturn eq * EQ_MEM_REGION_SIZE;\r\nelse\r\nreturn eq * EQ_LEN * sizeof(u32);\r\n}\r\nstatic inline int hwirq_to_cpu(struct iproc_msi *msi, unsigned long hwirq)\r\n{\r\nreturn (hwirq % msi->nr_cpus);\r\n}\r\nstatic inline unsigned long hwirq_to_canonical_hwirq(struct iproc_msi *msi,\r\nunsigned long hwirq)\r\n{\r\nreturn (hwirq - hwirq_to_cpu(msi, hwirq));\r\n}\r\nstatic int iproc_msi_irq_set_affinity(struct irq_data *data,\r\nconst struct cpumask *mask, bool force)\r\n{\r\nstruct iproc_msi *msi = irq_data_get_irq_chip_data(data);\r\nint target_cpu = cpumask_first(mask);\r\nint curr_cpu;\r\ncurr_cpu = hwirq_to_cpu(msi, data->hwirq);\r\nif (curr_cpu == target_cpu)\r\nreturn IRQ_SET_MASK_OK_DONE;\r\ndata->hwirq = hwirq_to_canonical_hwirq(msi, data->hwirq) + target_cpu;\r\nreturn IRQ_SET_MASK_OK;\r\n}\r\nstatic void iproc_msi_irq_compose_msi_msg(struct irq_data *data,\r\nstruct msi_msg *msg)\r\n{\r\nstruct iproc_msi *msi = irq_data_get_irq_chip_data(data);\r\ndma_addr_t addr;\r\naddr = msi->msi_addr + iproc_msi_addr_offset(msi, data->hwirq);\r\nmsg->address_lo = lower_32_bits(addr);\r\nmsg->address_hi = upper_32_bits(addr);\r\nmsg->data = data->hwirq;\r\n}\r\nstatic int iproc_msi_irq_domain_alloc(struct irq_domain *domain,\r\nunsigned int virq, unsigned int nr_irqs,\r\nvoid *args)\r\n{\r\nstruct iproc_msi *msi = domain->host_data;\r\nint hwirq;\r\nmutex_lock(&msi->bitmap_lock);\r\nhwirq = bitmap_find_next_zero_area(msi->bitmap, msi->nr_msi_vecs, 0,\r\nmsi->nr_cpus, 0);\r\nif (hwirq < msi->nr_msi_vecs) {\r\nbitmap_set(msi->bitmap, hwirq, msi->nr_cpus);\r\n} else {\r\nmutex_unlock(&msi->bitmap_lock);\r\nreturn -ENOSPC;\r\n}\r\nmutex_unlock(&msi->bitmap_lock);\r\nirq_domain_set_info(domain, virq, hwirq, &iproc_msi_bottom_irq_chip,\r\ndomain->host_data, handle_simple_irq, NULL, NULL);\r\nreturn 0;\r\n}\r\nstatic void iproc_msi_irq_domain_free(struct irq_domain *domain,\r\nunsigned int virq, unsigned int nr_irqs)\r\n{\r\nstruct irq_data *data = irq_domain_get_irq_data(domain, virq);\r\nstruct iproc_msi *msi = irq_data_get_irq_chip_data(data);\r\nunsigned int hwirq;\r\nmutex_lock(&msi->bitmap_lock);\r\nhwirq = hwirq_to_canonical_hwirq(msi, data->hwirq);\r\nbitmap_clear(msi->bitmap, hwirq, msi->nr_cpus);\r\nmutex_unlock(&msi->bitmap_lock);\r\nirq_domain_free_irqs_parent(domain, virq, nr_irqs);\r\n}\r\nstatic inline u32 decode_msi_hwirq(struct iproc_msi *msi, u32 eq, u32 head)\r\n{\r\nu32 *msg, hwirq;\r\nunsigned int offs;\r\noffs = iproc_msi_eq_offset(msi, eq) + head * sizeof(u32);\r\nmsg = (u32 *)(msi->eq_cpu + offs);\r\nhwirq = *msg & IPROC_MSI_EQ_MASK;\r\nreturn hwirq_to_canonical_hwirq(msi, hwirq);\r\n}\r\nstatic void iproc_msi_handler(struct irq_desc *desc)\r\n{\r\nstruct irq_chip *chip = irq_desc_get_chip(desc);\r\nstruct iproc_msi_grp *grp;\r\nstruct iproc_msi *msi;\r\nstruct iproc_pcie *pcie;\r\nu32 eq, head, tail, nr_events;\r\nunsigned long hwirq;\r\nint virq;\r\nchained_irq_enter(chip, desc);\r\ngrp = irq_desc_get_handler_data(desc);\r\nmsi = grp->msi;\r\npcie = msi->pcie;\r\neq = grp->eq;\r\nhead = iproc_msi_read_reg(msi, IPROC_MSI_EQ_HEAD,\r\neq) & IPROC_MSI_EQ_MASK;\r\ndo {\r\ntail = iproc_msi_read_reg(msi, IPROC_MSI_EQ_TAIL,\r\neq) & IPROC_MSI_EQ_MASK;\r\nnr_events = (tail < head) ?\r\n(EQ_LEN - (head - tail)) : (tail - head);\r\nif (!nr_events)\r\nbreak;\r\nwhile (nr_events--) {\r\nhwirq = decode_msi_hwirq(msi, eq, head);\r\nvirq = irq_find_mapping(msi->inner_domain, hwirq);\r\ngeneric_handle_irq(virq);\r\nhead++;\r\nhead %= EQ_LEN;\r\n}\r\niproc_msi_write_reg(msi, IPROC_MSI_EQ_HEAD, eq, head);\r\n} while (true);\r\nchained_irq_exit(chip, desc);\r\n}\r\nstatic void iproc_msi_enable(struct iproc_msi *msi)\r\n{\r\nint i, eq;\r\nu32 val;\r\nfor (i = 0; i < msi->nr_eq_region; i++) {\r\ndma_addr_t addr = msi->eq_dma + (i * EQ_MEM_REGION_SIZE);\r\niproc_msi_write_reg(msi, IPROC_MSI_EQ_PAGE, i,\r\nlower_32_bits(addr));\r\niproc_msi_write_reg(msi, IPROC_MSI_EQ_PAGE_UPPER, i,\r\nupper_32_bits(addr));\r\n}\r\nfor (i = 0; i < msi->nr_msi_region; i++) {\r\nphys_addr_t addr = msi->msi_addr + (i * MSI_MEM_REGION_SIZE);\r\niproc_msi_write_reg(msi, IPROC_MSI_PAGE, i,\r\nlower_32_bits(addr));\r\niproc_msi_write_reg(msi, IPROC_MSI_PAGE_UPPER, i,\r\nupper_32_bits(addr));\r\n}\r\nfor (eq = 0; eq < msi->nr_irqs; eq++) {\r\nval = IPROC_MSI_INTR_EN | IPROC_MSI_INT_N_EVENT |\r\nIPROC_MSI_EQ_EN;\r\niproc_msi_write_reg(msi, IPROC_MSI_CTRL, eq, val);\r\nif (msi->has_inten_reg) {\r\nval = iproc_msi_read_reg(msi, IPROC_MSI_INTS_EN, eq);\r\nval |= BIT(eq);\r\niproc_msi_write_reg(msi, IPROC_MSI_INTS_EN, eq, val);\r\n}\r\n}\r\n}\r\nstatic void iproc_msi_disable(struct iproc_msi *msi)\r\n{\r\nu32 eq, val;\r\nfor (eq = 0; eq < msi->nr_irqs; eq++) {\r\nif (msi->has_inten_reg) {\r\nval = iproc_msi_read_reg(msi, IPROC_MSI_INTS_EN, eq);\r\nval &= ~BIT(eq);\r\niproc_msi_write_reg(msi, IPROC_MSI_INTS_EN, eq, val);\r\n}\r\nval = iproc_msi_read_reg(msi, IPROC_MSI_CTRL, eq);\r\nval &= ~(IPROC_MSI_INTR_EN | IPROC_MSI_INT_N_EVENT |\r\nIPROC_MSI_EQ_EN);\r\niproc_msi_write_reg(msi, IPROC_MSI_CTRL, eq, val);\r\n}\r\n}\r\nstatic int iproc_msi_alloc_domains(struct device_node *node,\r\nstruct iproc_msi *msi)\r\n{\r\nmsi->inner_domain = irq_domain_add_linear(NULL, msi->nr_msi_vecs,\r\n&msi_domain_ops, msi);\r\nif (!msi->inner_domain)\r\nreturn -ENOMEM;\r\nmsi->msi_domain = pci_msi_create_irq_domain(of_node_to_fwnode(node),\r\n&iproc_msi_domain_info,\r\nmsi->inner_domain);\r\nif (!msi->msi_domain) {\r\nirq_domain_remove(msi->inner_domain);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void iproc_msi_free_domains(struct iproc_msi *msi)\r\n{\r\nif (msi->msi_domain)\r\nirq_domain_remove(msi->msi_domain);\r\nif (msi->inner_domain)\r\nirq_domain_remove(msi->inner_domain);\r\n}\r\nstatic void iproc_msi_irq_free(struct iproc_msi *msi, unsigned int cpu)\r\n{\r\nint i;\r\nfor (i = cpu; i < msi->nr_irqs; i += msi->nr_cpus) {\r\nirq_set_chained_handler_and_data(msi->grps[i].gic_irq,\r\nNULL, NULL);\r\n}\r\n}\r\nstatic int iproc_msi_irq_setup(struct iproc_msi *msi, unsigned int cpu)\r\n{\r\nint i, ret;\r\ncpumask_var_t mask;\r\nstruct iproc_pcie *pcie = msi->pcie;\r\nfor (i = cpu; i < msi->nr_irqs; i += msi->nr_cpus) {\r\nirq_set_chained_handler_and_data(msi->grps[i].gic_irq,\r\niproc_msi_handler,\r\n&msi->grps[i]);\r\nif (alloc_cpumask_var(&mask, GFP_KERNEL)) {\r\ncpumask_clear(mask);\r\ncpumask_set_cpu(cpu, mask);\r\nret = irq_set_affinity(msi->grps[i].gic_irq, mask);\r\nif (ret)\r\ndev_err(pcie->dev,\r\n"failed to set affinity for IRQ%d\n",\r\nmsi->grps[i].gic_irq);\r\nfree_cpumask_var(mask);\r\n} else {\r\ndev_err(pcie->dev, "failed to alloc CPU mask\n");\r\nret = -EINVAL;\r\n}\r\nif (ret) {\r\niproc_msi_irq_free(msi, cpu);\r\nreturn ret;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint iproc_msi_init(struct iproc_pcie *pcie, struct device_node *node)\r\n{\r\nstruct iproc_msi *msi;\r\nint i, ret;\r\nunsigned int cpu;\r\nif (!of_device_is_compatible(node, "brcm,iproc-msi"))\r\nreturn -ENODEV;\r\nif (!of_find_property(node, "msi-controller", NULL))\r\nreturn -ENODEV;\r\nif (pcie->msi)\r\nreturn -EBUSY;\r\nmsi = devm_kzalloc(pcie->dev, sizeof(*msi), GFP_KERNEL);\r\nif (!msi)\r\nreturn -ENOMEM;\r\nmsi->pcie = pcie;\r\npcie->msi = msi;\r\nmsi->msi_addr = pcie->base_addr;\r\nmutex_init(&msi->bitmap_lock);\r\nmsi->nr_cpus = num_possible_cpus();\r\nmsi->nr_irqs = of_irq_count(node);\r\nif (!msi->nr_irqs) {\r\ndev_err(pcie->dev, "found no MSI GIC interrupt\n");\r\nreturn -ENODEV;\r\n}\r\nif (msi->nr_irqs > NR_HW_IRQS) {\r\ndev_warn(pcie->dev, "too many MSI GIC interrupts defined %d\n",\r\nmsi->nr_irqs);\r\nmsi->nr_irqs = NR_HW_IRQS;\r\n}\r\nif (msi->nr_irqs < msi->nr_cpus) {\r\ndev_err(pcie->dev,\r\n"not enough GIC interrupts for MSI affinity\n");\r\nreturn -EINVAL;\r\n}\r\nif (msi->nr_irqs % msi->nr_cpus != 0) {\r\nmsi->nr_irqs -= msi->nr_irqs % msi->nr_cpus;\r\ndev_warn(pcie->dev, "Reducing number of interrupts to %d\n",\r\nmsi->nr_irqs);\r\n}\r\nswitch (pcie->type) {\r\ncase IPROC_PCIE_PAXB_BCMA:\r\ncase IPROC_PCIE_PAXB:\r\nmsi->reg_offsets = iproc_msi_reg_paxb;\r\nmsi->nr_eq_region = 1;\r\nmsi->nr_msi_region = 1;\r\nbreak;\r\ncase IPROC_PCIE_PAXC:\r\nmsi->reg_offsets = iproc_msi_reg_paxc;\r\nmsi->nr_eq_region = msi->nr_irqs;\r\nmsi->nr_msi_region = msi->nr_irqs;\r\nbreak;\r\ndefault:\r\ndev_err(pcie->dev, "incompatible iProc PCIe interface\n");\r\nreturn -EINVAL;\r\n}\r\nif (of_find_property(node, "brcm,pcie-msi-inten", NULL))\r\nmsi->has_inten_reg = true;\r\nmsi->nr_msi_vecs = msi->nr_irqs * EQ_LEN;\r\nmsi->bitmap = devm_kcalloc(pcie->dev, BITS_TO_LONGS(msi->nr_msi_vecs),\r\nsizeof(*msi->bitmap), GFP_KERNEL);\r\nif (!msi->bitmap)\r\nreturn -ENOMEM;\r\nmsi->grps = devm_kcalloc(pcie->dev, msi->nr_irqs, sizeof(*msi->grps),\r\nGFP_KERNEL);\r\nif (!msi->grps)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < msi->nr_irqs; i++) {\r\nunsigned int irq = irq_of_parse_and_map(node, i);\r\nif (!irq) {\r\ndev_err(pcie->dev, "unable to parse/map interrupt\n");\r\nret = -ENODEV;\r\ngoto free_irqs;\r\n}\r\nmsi->grps[i].gic_irq = irq;\r\nmsi->grps[i].msi = msi;\r\nmsi->grps[i].eq = i;\r\n}\r\nmsi->eq_cpu = dma_zalloc_coherent(pcie->dev,\r\nmsi->nr_eq_region * EQ_MEM_REGION_SIZE,\r\n&msi->eq_dma, GFP_KERNEL);\r\nif (!msi->eq_cpu) {\r\nret = -ENOMEM;\r\ngoto free_irqs;\r\n}\r\nret = iproc_msi_alloc_domains(node, msi);\r\nif (ret) {\r\ndev_err(pcie->dev, "failed to create MSI domains\n");\r\ngoto free_eq_dma;\r\n}\r\nfor_each_online_cpu(cpu) {\r\nret = iproc_msi_irq_setup(msi, cpu);\r\nif (ret)\r\ngoto free_msi_irq;\r\n}\r\niproc_msi_enable(msi);\r\nreturn 0;\r\nfree_msi_irq:\r\nfor_each_online_cpu(cpu)\r\niproc_msi_irq_free(msi, cpu);\r\niproc_msi_free_domains(msi);\r\nfree_eq_dma:\r\ndma_free_coherent(pcie->dev, msi->nr_eq_region * EQ_MEM_REGION_SIZE,\r\nmsi->eq_cpu, msi->eq_dma);\r\nfree_irqs:\r\nfor (i = 0; i < msi->nr_irqs; i++) {\r\nif (msi->grps[i].gic_irq)\r\nirq_dispose_mapping(msi->grps[i].gic_irq);\r\n}\r\npcie->msi = NULL;\r\nreturn ret;\r\n}\r\nvoid iproc_msi_exit(struct iproc_pcie *pcie)\r\n{\r\nstruct iproc_msi *msi = pcie->msi;\r\nunsigned int i, cpu;\r\nif (!msi)\r\nreturn;\r\niproc_msi_disable(msi);\r\nfor_each_online_cpu(cpu)\r\niproc_msi_irq_free(msi, cpu);\r\niproc_msi_free_domains(msi);\r\ndma_free_coherent(pcie->dev, msi->nr_eq_region * EQ_MEM_REGION_SIZE,\r\nmsi->eq_cpu, msi->eq_dma);\r\nfor (i = 0; i < msi->nr_irqs; i++) {\r\nif (msi->grps[i].gic_irq)\r\nirq_dispose_mapping(msi->grps[i].gic_irq);\r\n}\r\n}
