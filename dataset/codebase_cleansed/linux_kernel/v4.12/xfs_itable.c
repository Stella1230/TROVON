STATIC int\r\nxfs_internal_inum(\r\nxfs_mount_t *mp,\r\nxfs_ino_t ino)\r\n{\r\nreturn (ino == mp->m_sb.sb_rbmino || ino == mp->m_sb.sb_rsumino ||\r\n(xfs_sb_version_hasquota(&mp->m_sb) &&\r\nxfs_is_quota_inode(&mp->m_sb, ino)));\r\n}\r\nint\r\nxfs_bulkstat_one_int(\r\nstruct xfs_mount *mp,\r\nxfs_ino_t ino,\r\nvoid __user *buffer,\r\nint ubsize,\r\nbulkstat_one_fmt_pf formatter,\r\nint *ubused,\r\nint *stat)\r\n{\r\nstruct xfs_icdinode *dic;\r\nstruct xfs_inode *ip;\r\nstruct inode *inode;\r\nstruct xfs_bstat *buf;\r\nint error = 0;\r\n*stat = BULKSTAT_RV_NOTHING;\r\nif (!buffer || xfs_internal_inum(mp, ino))\r\nreturn -EINVAL;\r\nbuf = kmem_zalloc(sizeof(*buf), KM_SLEEP | KM_MAYFAIL);\r\nif (!buf)\r\nreturn -ENOMEM;\r\nerror = xfs_iget(mp, NULL, ino,\r\n(XFS_IGET_DONTCACHE | XFS_IGET_UNTRUSTED),\r\nXFS_ILOCK_SHARED, &ip);\r\nif (error)\r\ngoto out_free;\r\nASSERT(ip != NULL);\r\nASSERT(ip->i_imap.im_blkno != 0);\r\ninode = VFS_I(ip);\r\ndic = &ip->i_d;\r\nbuf->bs_projid_lo = dic->di_projid_lo;\r\nbuf->bs_projid_hi = dic->di_projid_hi;\r\nbuf->bs_ino = ino;\r\nbuf->bs_uid = dic->di_uid;\r\nbuf->bs_gid = dic->di_gid;\r\nbuf->bs_size = dic->di_size;\r\nbuf->bs_nlink = inode->i_nlink;\r\nbuf->bs_atime.tv_sec = inode->i_atime.tv_sec;\r\nbuf->bs_atime.tv_nsec = inode->i_atime.tv_nsec;\r\nbuf->bs_mtime.tv_sec = inode->i_mtime.tv_sec;\r\nbuf->bs_mtime.tv_nsec = inode->i_mtime.tv_nsec;\r\nbuf->bs_ctime.tv_sec = inode->i_ctime.tv_sec;\r\nbuf->bs_ctime.tv_nsec = inode->i_ctime.tv_nsec;\r\nbuf->bs_gen = inode->i_generation;\r\nbuf->bs_mode = inode->i_mode;\r\nbuf->bs_xflags = xfs_ip2xflags(ip);\r\nbuf->bs_extsize = dic->di_extsize << mp->m_sb.sb_blocklog;\r\nbuf->bs_extents = dic->di_nextents;\r\nmemset(buf->bs_pad, 0, sizeof(buf->bs_pad));\r\nbuf->bs_dmevmask = dic->di_dmevmask;\r\nbuf->bs_dmstate = dic->di_dmstate;\r\nbuf->bs_aextents = dic->di_anextents;\r\nbuf->bs_forkoff = XFS_IFORK_BOFF(ip);\r\nif (dic->di_version == 3) {\r\nif (dic->di_flags2 & XFS_DIFLAG2_COWEXTSIZE)\r\nbuf->bs_cowextsize = dic->di_cowextsize <<\r\nmp->m_sb.sb_blocklog;\r\n}\r\nswitch (dic->di_format) {\r\ncase XFS_DINODE_FMT_DEV:\r\nbuf->bs_rdev = ip->i_df.if_u2.if_rdev;\r\nbuf->bs_blksize = BLKDEV_IOSIZE;\r\nbuf->bs_blocks = 0;\r\nbreak;\r\ncase XFS_DINODE_FMT_LOCAL:\r\ncase XFS_DINODE_FMT_UUID:\r\nbuf->bs_rdev = 0;\r\nbuf->bs_blksize = mp->m_sb.sb_blocksize;\r\nbuf->bs_blocks = 0;\r\nbreak;\r\ncase XFS_DINODE_FMT_EXTENTS:\r\ncase XFS_DINODE_FMT_BTREE:\r\nbuf->bs_rdev = 0;\r\nbuf->bs_blksize = mp->m_sb.sb_blocksize;\r\nbuf->bs_blocks = dic->di_nblocks + ip->i_delayed_blks;\r\nbreak;\r\n}\r\nxfs_iunlock(ip, XFS_ILOCK_SHARED);\r\nIRELE(ip);\r\nerror = formatter(buffer, ubsize, ubused, buf);\r\nif (!error)\r\n*stat = BULKSTAT_RV_DIDONE;\r\nout_free:\r\nkmem_free(buf);\r\nreturn error;\r\n}\r\nSTATIC int\r\nxfs_bulkstat_one_fmt(\r\nvoid __user *ubuffer,\r\nint ubsize,\r\nint *ubused,\r\nconst xfs_bstat_t *buffer)\r\n{\r\nif (ubsize < sizeof(*buffer))\r\nreturn -ENOMEM;\r\nif (copy_to_user(ubuffer, buffer, sizeof(*buffer)))\r\nreturn -EFAULT;\r\nif (ubused)\r\n*ubused = sizeof(*buffer);\r\nreturn 0;\r\n}\r\nint\r\nxfs_bulkstat_one(\r\nxfs_mount_t *mp,\r\nxfs_ino_t ino,\r\nvoid __user *buffer,\r\nint ubsize,\r\nint *ubused,\r\nint *stat)\r\n{\r\nreturn xfs_bulkstat_one_int(mp, ino, buffer, ubsize,\r\nxfs_bulkstat_one_fmt, ubused, stat);\r\n}\r\nSTATIC void\r\nxfs_bulkstat_ichunk_ra(\r\nstruct xfs_mount *mp,\r\nxfs_agnumber_t agno,\r\nstruct xfs_inobt_rec_incore *irec)\r\n{\r\nxfs_agblock_t agbno;\r\nstruct blk_plug plug;\r\nint blks_per_cluster;\r\nint inodes_per_cluster;\r\nint i;\r\nagbno = XFS_AGINO_TO_AGBNO(mp, irec->ir_startino);\r\nblks_per_cluster = xfs_icluster_size_fsb(mp);\r\ninodes_per_cluster = blks_per_cluster << mp->m_sb.sb_inopblog;\r\nblk_start_plug(&plug);\r\nfor (i = 0; i < XFS_INODES_PER_CHUNK;\r\ni += inodes_per_cluster, agbno += blks_per_cluster) {\r\nif (xfs_inobt_maskn(i, inodes_per_cluster) & ~irec->ir_free) {\r\nxfs_btree_reada_bufs(mp, agno, agbno, blks_per_cluster,\r\n&xfs_inode_buf_ops);\r\n}\r\n}\r\nblk_finish_plug(&plug);\r\n}\r\nSTATIC int\r\nxfs_bulkstat_grab_ichunk(\r\nstruct xfs_btree_cur *cur,\r\nxfs_agino_t agino,\r\nint *icount,\r\nstruct xfs_inobt_rec_incore *irec)\r\n{\r\nint idx;\r\nint stat;\r\nint error = 0;\r\nerror = xfs_inobt_lookup(cur, agino, XFS_LOOKUP_LE, &stat);\r\nif (error)\r\nreturn error;\r\nif (!stat) {\r\n*icount = 0;\r\nreturn error;\r\n}\r\nerror = xfs_inobt_get_rec(cur, irec, &stat);\r\nif (error)\r\nreturn error;\r\nXFS_WANT_CORRUPTED_RETURN(cur->bc_mp, stat == 1);\r\nif (irec->ir_startino + XFS_INODES_PER_CHUNK <= agino) {\r\n*icount = 0;\r\nreturn 0;\r\n}\r\nidx = agino - irec->ir_startino + 1;\r\nif (idx < XFS_INODES_PER_CHUNK &&\r\n(xfs_inobt_maskn(idx, XFS_INODES_PER_CHUNK - idx) & ~irec->ir_free)) {\r\nint i;\r\nfor (i = 0; i < idx; i++) {\r\nif (XFS_INOBT_MASK(i) & ~irec->ir_free)\r\nirec->ir_freecount++;\r\n}\r\nirec->ir_free |= xfs_inobt_maskn(0, idx);\r\n*icount = irec->ir_count - irec->ir_freecount;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nxfs_bulkstat_ag_ichunk(\r\nstruct xfs_mount *mp,\r\nxfs_agnumber_t agno,\r\nstruct xfs_inobt_rec_incore *irbp,\r\nbulkstat_one_pf formatter,\r\nsize_t statstruct_size,\r\nstruct xfs_bulkstat_agichunk *acp,\r\nxfs_agino_t *last_agino)\r\n{\r\nchar __user **ubufp = acp->ac_ubuffer;\r\nint chunkidx;\r\nint error = 0;\r\nxfs_agino_t agino = irbp->ir_startino;\r\nfor (chunkidx = 0; chunkidx < XFS_INODES_PER_CHUNK;\r\nchunkidx++, agino++) {\r\nint fmterror;\r\nint ubused;\r\nif (acp->ac_ubleft < statstruct_size)\r\nbreak;\r\nif (XFS_INOBT_MASK(chunkidx) & irbp->ir_free)\r\ncontinue;\r\nubused = statstruct_size;\r\nerror = formatter(mp, XFS_AGINO_TO_INO(mp, agno, agino),\r\n*ubufp, acp->ac_ubleft, &ubused, &fmterror);\r\nif (fmterror == BULKSTAT_RV_GIVEUP ||\r\n(error && error != -ENOENT && error != -EINVAL)) {\r\nacp->ac_ubleft = 0;\r\nASSERT(error);\r\nbreak;\r\n}\r\nif (fmterror == BULKSTAT_RV_NOTHING || error) {\r\nerror = 0;\r\ncontinue;\r\n}\r\n*ubufp += ubused;\r\nacp->ac_ubleft -= ubused;\r\nacp->ac_ubelem++;\r\n}\r\n*last_agino = agino - 1;\r\nreturn error;\r\n}\r\nint\r\nxfs_bulkstat(\r\nxfs_mount_t *mp,\r\nxfs_ino_t *lastinop,\r\nint *ubcountp,\r\nbulkstat_one_pf formatter,\r\nsize_t statstruct_size,\r\nchar __user *ubuffer,\r\nint *done)\r\n{\r\nxfs_buf_t *agbp;\r\nxfs_agino_t agino;\r\nxfs_agnumber_t agno;\r\nxfs_btree_cur_t *cur;\r\nxfs_inobt_rec_incore_t *irbuf;\r\nint nirbuf;\r\nint ubcount;\r\nstruct xfs_bulkstat_agichunk ac;\r\nint error = 0;\r\nagno = XFS_INO_TO_AGNO(mp, *lastinop);\r\nagino = XFS_INO_TO_AGINO(mp, *lastinop);\r\nif (agno >= mp->m_sb.sb_agcount ||\r\n*lastinop != XFS_AGINO_TO_INO(mp, agno, agino)) {\r\n*done = 1;\r\n*ubcountp = 0;\r\nreturn 0;\r\n}\r\nubcount = *ubcountp;\r\nac.ac_ubuffer = &ubuffer;\r\nac.ac_ubleft = ubcount * statstruct_size; ;\r\nac.ac_ubelem = 0;\r\n*ubcountp = 0;\r\n*done = 0;\r\nirbuf = kmem_zalloc_large(PAGE_SIZE * 4, KM_SLEEP);\r\nif (!irbuf)\r\nreturn -ENOMEM;\r\nnirbuf = (PAGE_SIZE * 4) / sizeof(*irbuf);\r\nwhile (agno < mp->m_sb.sb_agcount) {\r\nstruct xfs_inobt_rec_incore *irbp = irbuf;\r\nstruct xfs_inobt_rec_incore *irbufend = irbuf + nirbuf;\r\nbool end_of_ag = false;\r\nint icount = 0;\r\nint stat;\r\nerror = xfs_ialloc_read_agi(mp, NULL, agno, &agbp);\r\nif (error)\r\nbreak;\r\ncur = xfs_inobt_init_cursor(mp, NULL, agbp, agno,\r\nXFS_BTNUM_INO);\r\nif (agino > 0) {\r\nstruct xfs_inobt_rec_incore r;\r\nerror = xfs_bulkstat_grab_ichunk(cur, agino, &icount, &r);\r\nif (error)\r\ngoto del_cursor;\r\nif (icount) {\r\nirbp->ir_startino = r.ir_startino;\r\nirbp->ir_holemask = r.ir_holemask;\r\nirbp->ir_count = r.ir_count;\r\nirbp->ir_freecount = r.ir_freecount;\r\nirbp->ir_free = r.ir_free;\r\nirbp++;\r\n}\r\nerror = xfs_btree_increment(cur, 0, &stat);\r\n} else {\r\nerror = xfs_inobt_lookup(cur, 0, XFS_LOOKUP_GE, &stat);\r\n}\r\nif (error || stat == 0) {\r\nend_of_ag = true;\r\ngoto del_cursor;\r\n}\r\nwhile (irbp < irbufend && icount < ubcount) {\r\nstruct xfs_inobt_rec_incore r;\r\nerror = xfs_inobt_get_rec(cur, &r, &stat);\r\nif (error || stat == 0) {\r\nend_of_ag = true;\r\ngoto del_cursor;\r\n}\r\nif (r.ir_freecount < r.ir_count) {\r\nxfs_bulkstat_ichunk_ra(mp, agno, &r);\r\nirbp->ir_startino = r.ir_startino;\r\nirbp->ir_holemask = r.ir_holemask;\r\nirbp->ir_count = r.ir_count;\r\nirbp->ir_freecount = r.ir_freecount;\r\nirbp->ir_free = r.ir_free;\r\nirbp++;\r\nicount += r.ir_count - r.ir_freecount;\r\n}\r\nerror = xfs_btree_increment(cur, 0, &stat);\r\nif (error || stat == 0) {\r\nend_of_ag = true;\r\ngoto del_cursor;\r\n}\r\ncond_resched();\r\n}\r\ndel_cursor:\r\nxfs_btree_del_cursor(cur, error ?\r\nXFS_BTREE_ERROR : XFS_BTREE_NOERROR);\r\nxfs_buf_relse(agbp);\r\nif (error)\r\nbreak;\r\nirbufend = irbp;\r\nfor (irbp = irbuf;\r\nirbp < irbufend && ac.ac_ubleft >= statstruct_size;\r\nirbp++) {\r\nerror = xfs_bulkstat_ag_ichunk(mp, agno, irbp,\r\nformatter, statstruct_size, &ac,\r\n&agino);\r\nif (error)\r\nbreak;\r\ncond_resched();\r\n}\r\nif (ac.ac_ubleft < statstruct_size || error)\r\nbreak;\r\nif (end_of_ag) {\r\nagno++;\r\nagino = 0;\r\n}\r\n}\r\nkmem_free(irbuf);\r\n*ubcountp = ac.ac_ubelem;\r\nif (ac.ac_ubelem)\r\nerror = 0;\r\n*lastinop = XFS_AGINO_TO_INO(mp, agno, agino);\r\nif (agno >= mp->m_sb.sb_agcount)\r\n*done = 1;\r\nreturn error;\r\n}\r\nint\r\nxfs_inumbers_fmt(\r\nvoid __user *ubuffer,\r\nconst struct xfs_inogrp *buffer,\r\nlong count,\r\nlong *written)\r\n{\r\nif (copy_to_user(ubuffer, buffer, count * sizeof(*buffer)))\r\nreturn -EFAULT;\r\n*written = count * sizeof(*buffer);\r\nreturn 0;\r\n}\r\nint\r\nxfs_inumbers(\r\nstruct xfs_mount *mp,\r\nxfs_ino_t *lastino,\r\nint *count,\r\nvoid __user *ubuffer,\r\ninumbers_fmt_pf formatter)\r\n{\r\nxfs_agnumber_t agno = XFS_INO_TO_AGNO(mp, *lastino);\r\nxfs_agino_t agino = XFS_INO_TO_AGINO(mp, *lastino);\r\nstruct xfs_btree_cur *cur = NULL;\r\nstruct xfs_buf *agbp = NULL;\r\nstruct xfs_inogrp *buffer;\r\nint bcount;\r\nint left = *count;\r\nint bufidx = 0;\r\nint error = 0;\r\n*count = 0;\r\nif (agno >= mp->m_sb.sb_agcount ||\r\n*lastino != XFS_AGINO_TO_INO(mp, agno, agino))\r\nreturn error;\r\nbcount = MIN(left, (int)(PAGE_SIZE / sizeof(*buffer)));\r\nbuffer = kmem_zalloc(bcount * sizeof(*buffer), KM_SLEEP);\r\ndo {\r\nstruct xfs_inobt_rec_incore r;\r\nint stat;\r\nif (!agbp) {\r\nerror = xfs_ialloc_read_agi(mp, NULL, agno, &agbp);\r\nif (error)\r\nbreak;\r\ncur = xfs_inobt_init_cursor(mp, NULL, agbp, agno,\r\nXFS_BTNUM_INO);\r\nerror = xfs_inobt_lookup(cur, agino, XFS_LOOKUP_GE,\r\n&stat);\r\nif (error)\r\nbreak;\r\nif (!stat)\r\ngoto next_ag;\r\n}\r\nerror = xfs_inobt_get_rec(cur, &r, &stat);\r\nif (error)\r\nbreak;\r\nif (!stat)\r\ngoto next_ag;\r\nagino = r.ir_startino + XFS_INODES_PER_CHUNK - 1;\r\nbuffer[bufidx].xi_startino =\r\nXFS_AGINO_TO_INO(mp, agno, r.ir_startino);\r\nbuffer[bufidx].xi_alloccount = r.ir_count - r.ir_freecount;\r\nbuffer[bufidx].xi_allocmask = ~r.ir_free;\r\nif (++bufidx == bcount) {\r\nlong written;\r\nerror = formatter(ubuffer, buffer, bufidx, &written);\r\nif (error)\r\nbreak;\r\nubuffer += written;\r\n*count += bufidx;\r\nbufidx = 0;\r\n}\r\nif (!--left)\r\nbreak;\r\nerror = xfs_btree_increment(cur, 0, &stat);\r\nif (error)\r\nbreak;\r\nif (stat)\r\ncontinue;\r\nnext_ag:\r\nxfs_btree_del_cursor(cur, XFS_BTREE_ERROR);\r\ncur = NULL;\r\nxfs_buf_relse(agbp);\r\nagbp = NULL;\r\nagino = 0;\r\nagno++;\r\n} while (agno < mp->m_sb.sb_agcount);\r\nif (!error) {\r\nif (bufidx) {\r\nlong written;\r\nerror = formatter(ubuffer, buffer, bufidx, &written);\r\nif (!error)\r\n*count += bufidx;\r\n}\r\n*lastino = XFS_AGINO_TO_INO(mp, agno, agino);\r\n}\r\nkmem_free(buffer);\r\nif (cur)\r\nxfs_btree_del_cursor(cur, (error ? XFS_BTREE_ERROR :\r\nXFS_BTREE_NOERROR));\r\nif (agbp)\r\nxfs_buf_relse(agbp);\r\nreturn error;\r\n}
