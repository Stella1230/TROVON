static void gfar_init_rxbdp(struct gfar_priv_rx_q *rx_queue, struct rxbd8 *bdp,\r\ndma_addr_t buf)\r\n{\r\nu32 lstatus;\r\nbdp->bufPtr = cpu_to_be32(buf);\r\nlstatus = BD_LFLAG(RXBD_EMPTY | RXBD_INTERRUPT);\r\nif (bdp == rx_queue->rx_bd_base + rx_queue->rx_ring_size - 1)\r\nlstatus |= BD_LFLAG(RXBD_WRAP);\r\ngfar_wmb();\r\nbdp->lstatus = cpu_to_be32(lstatus);\r\n}\r\nstatic void gfar_init_bds(struct net_device *ndev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(ndev);\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nstruct gfar_priv_tx_q *tx_queue = NULL;\r\nstruct gfar_priv_rx_q *rx_queue = NULL;\r\nstruct txbd8 *txbdp;\r\nu32 __iomem *rfbptr;\r\nint i, j;\r\nfor (i = 0; i < priv->num_tx_queues; i++) {\r\ntx_queue = priv->tx_queue[i];\r\ntx_queue->num_txbdfree = tx_queue->tx_ring_size;\r\ntx_queue->dirty_tx = tx_queue->tx_bd_base;\r\ntx_queue->cur_tx = tx_queue->tx_bd_base;\r\ntx_queue->skb_curtx = 0;\r\ntx_queue->skb_dirtytx = 0;\r\ntxbdp = tx_queue->tx_bd_base;\r\nfor (j = 0; j < tx_queue->tx_ring_size; j++) {\r\ntxbdp->lstatus = 0;\r\ntxbdp->bufPtr = 0;\r\ntxbdp++;\r\n}\r\ntxbdp--;\r\ntxbdp->status = cpu_to_be16(be16_to_cpu(txbdp->status) |\r\nTXBD_WRAP);\r\n}\r\nrfbptr = &regs->rfbptr0;\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\nrx_queue = priv->rx_queue[i];\r\nrx_queue->next_to_clean = 0;\r\nrx_queue->next_to_use = 0;\r\nrx_queue->next_to_alloc = 0;\r\ngfar_alloc_rx_buffs(rx_queue, gfar_rxbd_unused(rx_queue));\r\nrx_queue->rfbptr = rfbptr;\r\nrfbptr += 2;\r\n}\r\n}\r\nstatic int gfar_alloc_skb_resources(struct net_device *ndev)\r\n{\r\nvoid *vaddr;\r\ndma_addr_t addr;\r\nint i, j;\r\nstruct gfar_private *priv = netdev_priv(ndev);\r\nstruct device *dev = priv->dev;\r\nstruct gfar_priv_tx_q *tx_queue = NULL;\r\nstruct gfar_priv_rx_q *rx_queue = NULL;\r\npriv->total_tx_ring_size = 0;\r\nfor (i = 0; i < priv->num_tx_queues; i++)\r\npriv->total_tx_ring_size += priv->tx_queue[i]->tx_ring_size;\r\npriv->total_rx_ring_size = 0;\r\nfor (i = 0; i < priv->num_rx_queues; i++)\r\npriv->total_rx_ring_size += priv->rx_queue[i]->rx_ring_size;\r\nvaddr = dma_alloc_coherent(dev,\r\n(priv->total_tx_ring_size *\r\nsizeof(struct txbd8)) +\r\n(priv->total_rx_ring_size *\r\nsizeof(struct rxbd8)),\r\n&addr, GFP_KERNEL);\r\nif (!vaddr)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < priv->num_tx_queues; i++) {\r\ntx_queue = priv->tx_queue[i];\r\ntx_queue->tx_bd_base = vaddr;\r\ntx_queue->tx_bd_dma_base = addr;\r\ntx_queue->dev = ndev;\r\naddr += sizeof(struct txbd8) * tx_queue->tx_ring_size;\r\nvaddr += sizeof(struct txbd8) * tx_queue->tx_ring_size;\r\n}\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\nrx_queue = priv->rx_queue[i];\r\nrx_queue->rx_bd_base = vaddr;\r\nrx_queue->rx_bd_dma_base = addr;\r\nrx_queue->ndev = ndev;\r\nrx_queue->dev = dev;\r\naddr += sizeof(struct rxbd8) * rx_queue->rx_ring_size;\r\nvaddr += sizeof(struct rxbd8) * rx_queue->rx_ring_size;\r\n}\r\nfor (i = 0; i < priv->num_tx_queues; i++) {\r\ntx_queue = priv->tx_queue[i];\r\ntx_queue->tx_skbuff =\r\nkmalloc_array(tx_queue->tx_ring_size,\r\nsizeof(*tx_queue->tx_skbuff),\r\nGFP_KERNEL);\r\nif (!tx_queue->tx_skbuff)\r\ngoto cleanup;\r\nfor (j = 0; j < tx_queue->tx_ring_size; j++)\r\ntx_queue->tx_skbuff[j] = NULL;\r\n}\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\nrx_queue = priv->rx_queue[i];\r\nrx_queue->rx_buff = kcalloc(rx_queue->rx_ring_size,\r\nsizeof(*rx_queue->rx_buff),\r\nGFP_KERNEL);\r\nif (!rx_queue->rx_buff)\r\ngoto cleanup;\r\n}\r\ngfar_init_bds(ndev);\r\nreturn 0;\r\ncleanup:\r\nfree_skb_resources(priv);\r\nreturn -ENOMEM;\r\n}\r\nstatic void gfar_init_tx_rx_base(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 __iomem *baddr;\r\nint i;\r\nbaddr = &regs->tbase0;\r\nfor (i = 0; i < priv->num_tx_queues; i++) {\r\ngfar_write(baddr, priv->tx_queue[i]->tx_bd_dma_base);\r\nbaddr += 2;\r\n}\r\nbaddr = &regs->rbase0;\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\ngfar_write(baddr, priv->rx_queue[i]->rx_bd_dma_base);\r\nbaddr += 2;\r\n}\r\n}\r\nstatic void gfar_init_rqprm(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 __iomem *baddr;\r\nint i;\r\nbaddr = &regs->rqprm0;\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\ngfar_write(baddr, priv->rx_queue[i]->rx_ring_size |\r\n(DEFAULT_RX_LFC_THR << FBTHR_SHIFT));\r\nbaddr++;\r\n}\r\n}\r\nstatic void gfar_rx_offload_en(struct gfar_private *priv)\r\n{\r\npriv->uses_rxfcb = 0;\r\nif (priv->ndev->features & (NETIF_F_RXCSUM | NETIF_F_HW_VLAN_CTAG_RX))\r\npriv->uses_rxfcb = 1;\r\nif (priv->hwts_rx_en || priv->rx_filer_enable)\r\npriv->uses_rxfcb = 1;\r\n}\r\nstatic void gfar_mac_rx_config(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 rctrl = 0;\r\nif (priv->rx_filer_enable) {\r\nrctrl |= RCTRL_FILREN | RCTRL_PRSDEP_INIT;\r\nif (priv->poll_mode == GFAR_SQ_POLLING)\r\ngfar_write(&regs->rir0, DEFAULT_2RXQ_RIR0);\r\nelse\r\ngfar_write(&regs->rir0, DEFAULT_8RXQ_RIR0);\r\n}\r\nif (priv->ndev->flags & IFF_PROMISC)\r\nrctrl |= RCTRL_PROM;\r\nif (priv->ndev->features & NETIF_F_RXCSUM)\r\nrctrl |= RCTRL_CHECKSUMMING;\r\nif (priv->extended_hash)\r\nrctrl |= RCTRL_EXTHASH | RCTRL_EMEN;\r\nif (priv->padding) {\r\nrctrl &= ~RCTRL_PAL_MASK;\r\nrctrl |= RCTRL_PADDING(priv->padding);\r\n}\r\nif (priv->hwts_rx_en)\r\nrctrl |= RCTRL_PRSDEP_INIT | RCTRL_TS_ENABLE;\r\nif (priv->ndev->features & NETIF_F_HW_VLAN_CTAG_RX)\r\nrctrl |= RCTRL_VLEX | RCTRL_PRSDEP_INIT;\r\ngfar_write(&regs->rctrl, rctrl);\r\ngfar_init_rqprm(priv);\r\ngfar_write(&regs->ptv, DEFAULT_LFC_PTVVAL);\r\nrctrl |= RCTRL_LFC;\r\ngfar_write(&regs->rctrl, rctrl);\r\n}\r\nstatic void gfar_mac_tx_config(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tctrl = 0;\r\nif (priv->ndev->features & NETIF_F_IP_CSUM)\r\ntctrl |= TCTRL_INIT_CSUM;\r\nif (priv->prio_sched_en)\r\ntctrl |= TCTRL_TXSCHED_PRIO;\r\nelse {\r\ntctrl |= TCTRL_TXSCHED_WRRS;\r\ngfar_write(&regs->tr03wt, DEFAULT_WRRS_WEIGHT);\r\ngfar_write(&regs->tr47wt, DEFAULT_WRRS_WEIGHT);\r\n}\r\nif (priv->ndev->features & NETIF_F_HW_VLAN_CTAG_TX)\r\ntctrl |= TCTRL_VLINS;\r\ngfar_write(&regs->tctrl, tctrl);\r\n}\r\nstatic void gfar_configure_coalescing(struct gfar_private *priv,\r\nunsigned long tx_mask, unsigned long rx_mask)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 __iomem *baddr;\r\nif (priv->mode == MQ_MG_MODE) {\r\nint i = 0;\r\nbaddr = &regs->txic0;\r\nfor_each_set_bit(i, &tx_mask, priv->num_tx_queues) {\r\ngfar_write(baddr + i, 0);\r\nif (likely(priv->tx_queue[i]->txcoalescing))\r\ngfar_write(baddr + i, priv->tx_queue[i]->txic);\r\n}\r\nbaddr = &regs->rxic0;\r\nfor_each_set_bit(i, &rx_mask, priv->num_rx_queues) {\r\ngfar_write(baddr + i, 0);\r\nif (likely(priv->rx_queue[i]->rxcoalescing))\r\ngfar_write(baddr + i, priv->rx_queue[i]->rxic);\r\n}\r\n} else {\r\ngfar_write(&regs->txic, 0);\r\nif (likely(priv->tx_queue[0]->txcoalescing))\r\ngfar_write(&regs->txic, priv->tx_queue[0]->txic);\r\ngfar_write(&regs->rxic, 0);\r\nif (unlikely(priv->rx_queue[0]->rxcoalescing))\r\ngfar_write(&regs->rxic, priv->rx_queue[0]->rxic);\r\n}\r\n}\r\nvoid gfar_configure_coalescing_all(struct gfar_private *priv)\r\n{\r\ngfar_configure_coalescing(priv, 0xFF, 0xFF);\r\n}\r\nstatic struct net_device_stats *gfar_get_stats(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nunsigned long rx_packets = 0, rx_bytes = 0, rx_dropped = 0;\r\nunsigned long tx_packets = 0, tx_bytes = 0;\r\nint i;\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\nrx_packets += priv->rx_queue[i]->stats.rx_packets;\r\nrx_bytes += priv->rx_queue[i]->stats.rx_bytes;\r\nrx_dropped += priv->rx_queue[i]->stats.rx_dropped;\r\n}\r\ndev->stats.rx_packets = rx_packets;\r\ndev->stats.rx_bytes = rx_bytes;\r\ndev->stats.rx_dropped = rx_dropped;\r\nfor (i = 0; i < priv->num_tx_queues; i++) {\r\ntx_bytes += priv->tx_queue[i]->stats.tx_bytes;\r\ntx_packets += priv->tx_queue[i]->stats.tx_packets;\r\n}\r\ndev->stats.tx_bytes = tx_bytes;\r\ndev->stats.tx_packets = tx_packets;\r\nreturn &dev->stats;\r\n}\r\nstatic int gfar_set_mac_addr(struct net_device *dev, void *p)\r\n{\r\neth_mac_addr(dev, p);\r\ngfar_set_mac_for_addr(dev, 0, dev->dev_addr);\r\nreturn 0;\r\n}\r\nstatic void gfar_ints_disable(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nstruct gfar __iomem *regs = priv->gfargrp[i].regs;\r\ngfar_write(&regs->ievent, IEVENT_INIT_CLEAR);\r\ngfar_write(&regs->imask, IMASK_INIT_CLEAR);\r\n}\r\n}\r\nstatic void gfar_ints_enable(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nstruct gfar __iomem *regs = priv->gfargrp[i].regs;\r\ngfar_write(&regs->imask, IMASK_DEFAULT);\r\n}\r\n}\r\nstatic int gfar_alloc_tx_queues(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_tx_queues; i++) {\r\npriv->tx_queue[i] = kzalloc(sizeof(struct gfar_priv_tx_q),\r\nGFP_KERNEL);\r\nif (!priv->tx_queue[i])\r\nreturn -ENOMEM;\r\npriv->tx_queue[i]->tx_skbuff = NULL;\r\npriv->tx_queue[i]->qindex = i;\r\npriv->tx_queue[i]->dev = priv->ndev;\r\nspin_lock_init(&(priv->tx_queue[i]->txlock));\r\n}\r\nreturn 0;\r\n}\r\nstatic int gfar_alloc_rx_queues(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\npriv->rx_queue[i] = kzalloc(sizeof(struct gfar_priv_rx_q),\r\nGFP_KERNEL);\r\nif (!priv->rx_queue[i])\r\nreturn -ENOMEM;\r\npriv->rx_queue[i]->qindex = i;\r\npriv->rx_queue[i]->ndev = priv->ndev;\r\n}\r\nreturn 0;\r\n}\r\nstatic void gfar_free_tx_queues(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_tx_queues; i++)\r\nkfree(priv->tx_queue[i]);\r\n}\r\nstatic void gfar_free_rx_queues(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_rx_queues; i++)\r\nkfree(priv->rx_queue[i]);\r\n}\r\nstatic void unmap_group_regs(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < MAXGROUPS; i++)\r\nif (priv->gfargrp[i].regs)\r\niounmap(priv->gfargrp[i].regs);\r\n}\r\nstatic void free_gfar_dev(struct gfar_private *priv)\r\n{\r\nint i, j;\r\nfor (i = 0; i < priv->num_grps; i++)\r\nfor (j = 0; j < GFAR_NUM_IRQS; j++) {\r\nkfree(priv->gfargrp[i].irqinfo[j]);\r\npriv->gfargrp[i].irqinfo[j] = NULL;\r\n}\r\nfree_netdev(priv->ndev);\r\n}\r\nstatic void disable_napi(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nnapi_disable(&priv->gfargrp[i].napi_rx);\r\nnapi_disable(&priv->gfargrp[i].napi_tx);\r\n}\r\n}\r\nstatic void enable_napi(struct gfar_private *priv)\r\n{\r\nint i;\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nnapi_enable(&priv->gfargrp[i].napi_rx);\r\nnapi_enable(&priv->gfargrp[i].napi_tx);\r\n}\r\n}\r\nstatic int gfar_parse_group(struct device_node *np,\r\nstruct gfar_private *priv, const char *model)\r\n{\r\nstruct gfar_priv_grp *grp = &priv->gfargrp[priv->num_grps];\r\nint i;\r\nfor (i = 0; i < GFAR_NUM_IRQS; i++) {\r\ngrp->irqinfo[i] = kzalloc(sizeof(struct gfar_irqinfo),\r\nGFP_KERNEL);\r\nif (!grp->irqinfo[i])\r\nreturn -ENOMEM;\r\n}\r\ngrp->regs = of_iomap(np, 0);\r\nif (!grp->regs)\r\nreturn -ENOMEM;\r\ngfar_irq(grp, TX)->irq = irq_of_parse_and_map(np, 0);\r\nif (model && strcasecmp(model, "FEC")) {\r\ngfar_irq(grp, RX)->irq = irq_of_parse_and_map(np, 1);\r\ngfar_irq(grp, ER)->irq = irq_of_parse_and_map(np, 2);\r\nif (!gfar_irq(grp, TX)->irq ||\r\n!gfar_irq(grp, RX)->irq ||\r\n!gfar_irq(grp, ER)->irq)\r\nreturn -EINVAL;\r\n}\r\ngrp->priv = priv;\r\nspin_lock_init(&grp->grplock);\r\nif (priv->mode == MQ_MG_MODE) {\r\nu32 rxq_mask, txq_mask;\r\nint ret;\r\ngrp->rx_bit_map = (DEFAULT_MAPPING >> priv->num_grps);\r\ngrp->tx_bit_map = (DEFAULT_MAPPING >> priv->num_grps);\r\nret = of_property_read_u32(np, "fsl,rx-bit-map", &rxq_mask);\r\nif (!ret) {\r\ngrp->rx_bit_map = rxq_mask ?\r\nrxq_mask : (DEFAULT_MAPPING >> priv->num_grps);\r\n}\r\nret = of_property_read_u32(np, "fsl,tx-bit-map", &txq_mask);\r\nif (!ret) {\r\ngrp->tx_bit_map = txq_mask ?\r\ntxq_mask : (DEFAULT_MAPPING >> priv->num_grps);\r\n}\r\nif (priv->poll_mode == GFAR_SQ_POLLING) {\r\ngrp->rx_bit_map = (DEFAULT_MAPPING >> priv->num_grps);\r\ngrp->tx_bit_map = (DEFAULT_MAPPING >> priv->num_grps);\r\n}\r\n} else {\r\ngrp->rx_bit_map = 0xFF;\r\ngrp->tx_bit_map = 0xFF;\r\n}\r\ngrp->rx_bit_map = bitrev8(grp->rx_bit_map);\r\ngrp->tx_bit_map = bitrev8(grp->tx_bit_map);\r\nfor_each_set_bit(i, &grp->rx_bit_map, priv->num_rx_queues) {\r\nif (!grp->rx_queue)\r\ngrp->rx_queue = priv->rx_queue[i];\r\ngrp->num_rx_queues++;\r\ngrp->rstat |= (RSTAT_CLEAR_RHALT >> i);\r\npriv->rqueue |= ((RQUEUE_EN0 | RQUEUE_EX0) >> i);\r\npriv->rx_queue[i]->grp = grp;\r\n}\r\nfor_each_set_bit(i, &grp->tx_bit_map, priv->num_tx_queues) {\r\nif (!grp->tx_queue)\r\ngrp->tx_queue = priv->tx_queue[i];\r\ngrp->num_tx_queues++;\r\ngrp->tstat |= (TSTAT_CLEAR_THALT >> i);\r\npriv->tqueue |= (TQUEUE_EN0 >> i);\r\npriv->tx_queue[i]->grp = grp;\r\n}\r\npriv->num_grps++;\r\nreturn 0;\r\n}\r\nstatic int gfar_of_group_count(struct device_node *np)\r\n{\r\nstruct device_node *child;\r\nint num = 0;\r\nfor_each_available_child_of_node(np, child)\r\nif (!of_node_cmp(child->name, "queue-group"))\r\nnum++;\r\nreturn num;\r\n}\r\nstatic int gfar_of_init(struct platform_device *ofdev, struct net_device **pdev)\r\n{\r\nconst char *model;\r\nconst char *ctype;\r\nconst void *mac_addr;\r\nint err = 0, i;\r\nstruct net_device *dev = NULL;\r\nstruct gfar_private *priv = NULL;\r\nstruct device_node *np = ofdev->dev.of_node;\r\nstruct device_node *child = NULL;\r\nu32 stash_len = 0;\r\nu32 stash_idx = 0;\r\nunsigned int num_tx_qs, num_rx_qs;\r\nunsigned short mode, poll_mode;\r\nif (!np)\r\nreturn -ENODEV;\r\nif (of_device_is_compatible(np, "fsl,etsec2")) {\r\nmode = MQ_MG_MODE;\r\npoll_mode = GFAR_SQ_POLLING;\r\n} else {\r\nmode = SQ_SG_MODE;\r\npoll_mode = GFAR_SQ_POLLING;\r\n}\r\nif (mode == SQ_SG_MODE) {\r\nnum_tx_qs = 1;\r\nnum_rx_qs = 1;\r\n} else {\r\nunsigned int num_grps = gfar_of_group_count(np);\r\nif (num_grps == 0 || num_grps > MAXGROUPS) {\r\ndev_err(&ofdev->dev, "Invalid # of int groups(%d)\n",\r\nnum_grps);\r\npr_err("Cannot do alloc_etherdev, aborting\n");\r\nreturn -EINVAL;\r\n}\r\nif (poll_mode == GFAR_SQ_POLLING) {\r\nnum_tx_qs = num_grps;\r\nnum_rx_qs = num_grps;\r\n} else {\r\nu32 tx_queues, rx_queues;\r\nint ret;\r\nret = of_property_read_u32(np, "fsl,num_tx_queues",\r\n&tx_queues);\r\nnum_tx_qs = ret ? 1 : tx_queues;\r\nret = of_property_read_u32(np, "fsl,num_rx_queues",\r\n&rx_queues);\r\nnum_rx_qs = ret ? 1 : rx_queues;\r\n}\r\n}\r\nif (num_tx_qs > MAX_TX_QS) {\r\npr_err("num_tx_qs(=%d) greater than MAX_TX_QS(=%d)\n",\r\nnum_tx_qs, MAX_TX_QS);\r\npr_err("Cannot do alloc_etherdev, aborting\n");\r\nreturn -EINVAL;\r\n}\r\nif (num_rx_qs > MAX_RX_QS) {\r\npr_err("num_rx_qs(=%d) greater than MAX_RX_QS(=%d)\n",\r\nnum_rx_qs, MAX_RX_QS);\r\npr_err("Cannot do alloc_etherdev, aborting\n");\r\nreturn -EINVAL;\r\n}\r\n*pdev = alloc_etherdev_mq(sizeof(*priv), num_tx_qs);\r\ndev = *pdev;\r\nif (NULL == dev)\r\nreturn -ENOMEM;\r\npriv = netdev_priv(dev);\r\npriv->ndev = dev;\r\npriv->mode = mode;\r\npriv->poll_mode = poll_mode;\r\npriv->num_tx_queues = num_tx_qs;\r\nnetif_set_real_num_rx_queues(dev, num_rx_qs);\r\npriv->num_rx_queues = num_rx_qs;\r\nerr = gfar_alloc_tx_queues(priv);\r\nif (err)\r\ngoto tx_alloc_failed;\r\nerr = gfar_alloc_rx_queues(priv);\r\nif (err)\r\ngoto rx_alloc_failed;\r\nerr = of_property_read_string(np, "model", &model);\r\nif (err) {\r\npr_err("Device model property missing, aborting\n");\r\ngoto rx_alloc_failed;\r\n}\r\nINIT_LIST_HEAD(&priv->rx_list.list);\r\npriv->rx_list.count = 0;\r\nmutex_init(&priv->rx_queue_access);\r\nfor (i = 0; i < MAXGROUPS; i++)\r\npriv->gfargrp[i].regs = NULL;\r\nif (priv->mode == MQ_MG_MODE) {\r\nfor_each_available_child_of_node(np, child) {\r\nif (of_node_cmp(child->name, "queue-group"))\r\ncontinue;\r\nerr = gfar_parse_group(child, priv, model);\r\nif (err)\r\ngoto err_grp_init;\r\n}\r\n} else {\r\nerr = gfar_parse_group(np, priv, model);\r\nif (err)\r\ngoto err_grp_init;\r\n}\r\nif (of_property_read_bool(np, "bd-stash")) {\r\npriv->device_flags |= FSL_GIANFAR_DEV_HAS_BD_STASHING;\r\npriv->bd_stash_en = 1;\r\n}\r\nerr = of_property_read_u32(np, "rx-stash-len", &stash_len);\r\nif (err == 0)\r\npriv->rx_stash_size = stash_len;\r\nerr = of_property_read_u32(np, "rx-stash-idx", &stash_idx);\r\nif (err == 0)\r\npriv->rx_stash_index = stash_idx;\r\nif (stash_len || stash_idx)\r\npriv->device_flags |= FSL_GIANFAR_DEV_HAS_BUF_STASHING;\r\nmac_addr = of_get_mac_address(np);\r\nif (mac_addr)\r\nmemcpy(dev->dev_addr, mac_addr, ETH_ALEN);\r\nif (model && !strcasecmp(model, "TSEC"))\r\npriv->device_flags |= FSL_GIANFAR_DEV_HAS_GIGABIT |\r\nFSL_GIANFAR_DEV_HAS_COALESCE |\r\nFSL_GIANFAR_DEV_HAS_RMON |\r\nFSL_GIANFAR_DEV_HAS_MULTI_INTR;\r\nif (model && !strcasecmp(model, "eTSEC"))\r\npriv->device_flags |= FSL_GIANFAR_DEV_HAS_GIGABIT |\r\nFSL_GIANFAR_DEV_HAS_COALESCE |\r\nFSL_GIANFAR_DEV_HAS_RMON |\r\nFSL_GIANFAR_DEV_HAS_MULTI_INTR |\r\nFSL_GIANFAR_DEV_HAS_CSUM |\r\nFSL_GIANFAR_DEV_HAS_VLAN |\r\nFSL_GIANFAR_DEV_HAS_MAGIC_PACKET |\r\nFSL_GIANFAR_DEV_HAS_EXTENDED_HASH |\r\nFSL_GIANFAR_DEV_HAS_TIMER |\r\nFSL_GIANFAR_DEV_HAS_RX_FILER;\r\nerr = of_property_read_string(np, "phy-connection-type", &ctype);\r\nif (err == 0 && !strcmp(ctype, "rgmii-id"))\r\npriv->interface = PHY_INTERFACE_MODE_RGMII_ID;\r\nelse\r\npriv->interface = PHY_INTERFACE_MODE_MII;\r\nif (of_find_property(np, "fsl,magic-packet", NULL))\r\npriv->device_flags |= FSL_GIANFAR_DEV_HAS_MAGIC_PACKET;\r\nif (of_get_property(np, "fsl,wake-on-filer", NULL))\r\npriv->device_flags |= FSL_GIANFAR_DEV_HAS_WAKE_ON_FILER;\r\npriv->phy_node = of_parse_phandle(np, "phy-handle", 0);\r\nif (!priv->phy_node && of_phy_is_fixed_link(np)) {\r\nerr = of_phy_register_fixed_link(np);\r\nif (err)\r\ngoto err_grp_init;\r\npriv->phy_node = of_node_get(np);\r\n}\r\npriv->tbi_node = of_parse_phandle(np, "tbi-handle", 0);\r\nreturn 0;\r\nerr_grp_init:\r\nunmap_group_regs(priv);\r\nrx_alloc_failed:\r\ngfar_free_rx_queues(priv);\r\ntx_alloc_failed:\r\ngfar_free_tx_queues(priv);\r\nfree_gfar_dev(priv);\r\nreturn err;\r\n}\r\nstatic int gfar_hwtstamp_set(struct net_device *netdev, struct ifreq *ifr)\r\n{\r\nstruct hwtstamp_config config;\r\nstruct gfar_private *priv = netdev_priv(netdev);\r\nif (copy_from_user(&config, ifr->ifr_data, sizeof(config)))\r\nreturn -EFAULT;\r\nif (config.flags)\r\nreturn -EINVAL;\r\nswitch (config.tx_type) {\r\ncase HWTSTAMP_TX_OFF:\r\npriv->hwts_tx_en = 0;\r\nbreak;\r\ncase HWTSTAMP_TX_ON:\r\nif (!(priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER))\r\nreturn -ERANGE;\r\npriv->hwts_tx_en = 1;\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nswitch (config.rx_filter) {\r\ncase HWTSTAMP_FILTER_NONE:\r\nif (priv->hwts_rx_en) {\r\npriv->hwts_rx_en = 0;\r\nreset_gfar(netdev);\r\n}\r\nbreak;\r\ndefault:\r\nif (!(priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER))\r\nreturn -ERANGE;\r\nif (!priv->hwts_rx_en) {\r\npriv->hwts_rx_en = 1;\r\nreset_gfar(netdev);\r\n}\r\nconfig.rx_filter = HWTSTAMP_FILTER_ALL;\r\nbreak;\r\n}\r\nreturn copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?\r\n-EFAULT : 0;\r\n}\r\nstatic int gfar_hwtstamp_get(struct net_device *netdev, struct ifreq *ifr)\r\n{\r\nstruct hwtstamp_config config;\r\nstruct gfar_private *priv = netdev_priv(netdev);\r\nconfig.flags = 0;\r\nconfig.tx_type = priv->hwts_tx_en ? HWTSTAMP_TX_ON : HWTSTAMP_TX_OFF;\r\nconfig.rx_filter = (priv->hwts_rx_en ?\r\nHWTSTAMP_FILTER_ALL : HWTSTAMP_FILTER_NONE);\r\nreturn copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?\r\n-EFAULT : 0;\r\n}\r\nstatic int gfar_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct phy_device *phydev = dev->phydev;\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nif (cmd == SIOCSHWTSTAMP)\r\nreturn gfar_hwtstamp_set(dev, rq);\r\nif (cmd == SIOCGHWTSTAMP)\r\nreturn gfar_hwtstamp_get(dev, rq);\r\nif (!phydev)\r\nreturn -ENODEV;\r\nreturn phy_mii_ioctl(phydev, rq, cmd);\r\n}\r\nstatic u32 cluster_entry_per_class(struct gfar_private *priv, u32 rqfar,\r\nu32 class)\r\n{\r\nu32 rqfpr = FPR_FILER_MASK;\r\nu32 rqfcr = 0x0;\r\nrqfar--;\r\nrqfcr = RQFCR_CLE | RQFCR_PID_MASK | RQFCR_CMP_EXACT;\r\npriv->ftp_rqfpr[rqfar] = rqfpr;\r\npriv->ftp_rqfcr[rqfar] = rqfcr;\r\ngfar_write_filer(priv, rqfar, rqfcr, rqfpr);\r\nrqfar--;\r\nrqfcr = RQFCR_CMP_NOMATCH;\r\npriv->ftp_rqfpr[rqfar] = rqfpr;\r\npriv->ftp_rqfcr[rqfar] = rqfcr;\r\ngfar_write_filer(priv, rqfar, rqfcr, rqfpr);\r\nrqfar--;\r\nrqfcr = RQFCR_CMP_EXACT | RQFCR_PID_PARSE | RQFCR_CLE | RQFCR_AND;\r\nrqfpr = class;\r\npriv->ftp_rqfcr[rqfar] = rqfcr;\r\npriv->ftp_rqfpr[rqfar] = rqfpr;\r\ngfar_write_filer(priv, rqfar, rqfcr, rqfpr);\r\nrqfar--;\r\nrqfcr = RQFCR_CMP_EXACT | RQFCR_PID_MASK | RQFCR_AND;\r\nrqfpr = class;\r\npriv->ftp_rqfcr[rqfar] = rqfcr;\r\npriv->ftp_rqfpr[rqfar] = rqfpr;\r\ngfar_write_filer(priv, rqfar, rqfcr, rqfpr);\r\nreturn rqfar;\r\n}\r\nstatic void gfar_init_filer_table(struct gfar_private *priv)\r\n{\r\nint i = 0x0;\r\nu32 rqfar = MAX_FILER_IDX;\r\nu32 rqfcr = 0x0;\r\nu32 rqfpr = FPR_FILER_MASK;\r\nrqfcr = RQFCR_CMP_MATCH;\r\npriv->ftp_rqfcr[rqfar] = rqfcr;\r\npriv->ftp_rqfpr[rqfar] = rqfpr;\r\ngfar_write_filer(priv, rqfar, rqfcr, rqfpr);\r\nrqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV6);\r\nrqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV6 | RQFPR_UDP);\r\nrqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV6 | RQFPR_TCP);\r\nrqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV4);\r\nrqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV4 | RQFPR_UDP);\r\nrqfar = cluster_entry_per_class(priv, rqfar, RQFPR_IPV4 | RQFPR_TCP);\r\npriv->cur_filer_idx = rqfar;\r\nrqfcr = RQFCR_CMP_NOMATCH;\r\nfor (i = 0; i < rqfar; i++) {\r\npriv->ftp_rqfcr[i] = rqfcr;\r\npriv->ftp_rqfpr[i] = rqfpr;\r\ngfar_write_filer(priv, i, rqfcr, rqfpr);\r\n}\r\n}\r\nstatic void __gfar_detect_errata_83xx(struct gfar_private *priv)\r\n{\r\nunsigned int pvr = mfspr(SPRN_PVR);\r\nunsigned int svr = mfspr(SPRN_SVR);\r\nunsigned int mod = (svr >> 16) & 0xfff6;\r\nunsigned int rev = svr & 0xffff;\r\nif ((pvr == 0x80850010 && mod == 0x80b0 && rev >= 0x0020) ||\r\n(pvr == 0x80861010 && (mod & 0xfff9) == 0x80c0))\r\npriv->errata |= GFAR_ERRATA_74;\r\nif ((pvr == 0x80850010 && mod == 0x80b0) ||\r\n(pvr == 0x80861010 && (mod & 0xfff9) == 0x80c0))\r\npriv->errata |= GFAR_ERRATA_76;\r\nif (pvr == 0x80850010 && mod == 0x80b0 && rev < 0x0020)\r\npriv->errata |= GFAR_ERRATA_12;\r\n}\r\nstatic void __gfar_detect_errata_85xx(struct gfar_private *priv)\r\n{\r\nunsigned int svr = mfspr(SPRN_SVR);\r\nif ((SVR_SOC_VER(svr) == SVR_8548) && (SVR_REV(svr) == 0x20))\r\npriv->errata |= GFAR_ERRATA_12;\r\nif (((SVR_SOC_VER(svr) == SVR_P2020) && (SVR_REV(svr) < 0x20)) ||\r\n((SVR_SOC_VER(svr) == SVR_P2010) && (SVR_REV(svr) < 0x20)) ||\r\n((SVR_SOC_VER(svr) == SVR_8548) && (SVR_REV(svr) < 0x31)))\r\npriv->errata |= GFAR_ERRATA_76;\r\n}\r\nstatic void gfar_detect_errata(struct gfar_private *priv)\r\n{\r\nstruct device *dev = &priv->ofdev->dev;\r\npriv->errata |= GFAR_ERRATA_A002;\r\n#ifdef CONFIG_PPC\r\nif (pvr_version_is(PVR_VER_E500V1) || pvr_version_is(PVR_VER_E500V2))\r\n__gfar_detect_errata_85xx(priv);\r\nelse\r\n__gfar_detect_errata_83xx(priv);\r\n#endif\r\nif (priv->errata)\r\ndev_info(dev, "enabled errata workarounds, flags: 0x%x\n",\r\npriv->errata);\r\n}\r\nvoid gfar_mac_reset(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\ngfar_write(&regs->maccfg1, MACCFG1_SOFT_RESET);\r\nudelay(3);\r\ngfar_write(&regs->maccfg1, 0);\r\nudelay(3);\r\ngfar_rx_offload_en(priv);\r\ngfar_write(&regs->maxfrm, GFAR_JUMBO_FRAME_SIZE);\r\ngfar_write(&regs->mrblr, GFAR_RXB_SIZE);\r\ngfar_write(&regs->minflr, MINFLR_INIT_SETTINGS);\r\ntempval = MACCFG2_INIT_SETTINGS;\r\nif (gfar_has_errata(priv, GFAR_ERRATA_74))\r\ntempval |= MACCFG2_HUGEFRAME | MACCFG2_LENGTHCHECK;\r\ngfar_write(&regs->maccfg2, tempval);\r\ngfar_write(&regs->igaddr0, 0);\r\ngfar_write(&regs->igaddr1, 0);\r\ngfar_write(&regs->igaddr2, 0);\r\ngfar_write(&regs->igaddr3, 0);\r\ngfar_write(&regs->igaddr4, 0);\r\ngfar_write(&regs->igaddr5, 0);\r\ngfar_write(&regs->igaddr6, 0);\r\ngfar_write(&regs->igaddr7, 0);\r\ngfar_write(&regs->gaddr0, 0);\r\ngfar_write(&regs->gaddr1, 0);\r\ngfar_write(&regs->gaddr2, 0);\r\ngfar_write(&regs->gaddr3, 0);\r\ngfar_write(&regs->gaddr4, 0);\r\ngfar_write(&regs->gaddr5, 0);\r\ngfar_write(&regs->gaddr6, 0);\r\ngfar_write(&regs->gaddr7, 0);\r\nif (priv->extended_hash)\r\ngfar_clear_exact_match(priv->ndev);\r\ngfar_mac_rx_config(priv);\r\ngfar_mac_tx_config(priv);\r\ngfar_set_mac_address(priv->ndev);\r\ngfar_set_multi(priv->ndev);\r\ngfar_ints_disable(priv);\r\ngfar_configure_coalescing_all(priv);\r\n}\r\nstatic void gfar_hw_init(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 attrs;\r\ngfar_halt(priv);\r\ngfar_mac_reset(priv);\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_RMON) {\r\nmemset_io(&(regs->rmon), 0, sizeof(struct rmon_mib));\r\ngfar_write(&regs->rmon.cam1, 0xffffffff);\r\ngfar_write(&regs->rmon.cam2, 0xffffffff);\r\n}\r\ngfar_write(&regs->ecntrl, ECNTRL_INIT_SETTINGS);\r\nattrs = ATTRELI_EL(priv->rx_stash_size) |\r\nATTRELI_EI(priv->rx_stash_index);\r\ngfar_write(&regs->attreli, attrs);\r\nattrs = ATTR_INIT_SETTINGS;\r\nif (priv->bd_stash_en)\r\nattrs |= ATTR_BDSTASH;\r\nif (priv->rx_stash_size != 0)\r\nattrs |= ATTR_BUFSTASH;\r\ngfar_write(&regs->attr, attrs);\r\ngfar_write(&regs->fifo_tx_thr, DEFAULT_FIFO_TX_THR);\r\ngfar_write(&regs->fifo_tx_starve, DEFAULT_FIFO_TX_STARVE);\r\ngfar_write(&regs->fifo_tx_starve_shutoff, DEFAULT_FIFO_TX_STARVE_OFF);\r\nif (priv->num_grps > 1)\r\ngfar_write_isrg(priv);\r\n}\r\nstatic void gfar_init_addr_hash_table(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_EXTENDED_HASH) {\r\npriv->extended_hash = 1;\r\npriv->hash_width = 9;\r\npriv->hash_regs[0] = &regs->igaddr0;\r\npriv->hash_regs[1] = &regs->igaddr1;\r\npriv->hash_regs[2] = &regs->igaddr2;\r\npriv->hash_regs[3] = &regs->igaddr3;\r\npriv->hash_regs[4] = &regs->igaddr4;\r\npriv->hash_regs[5] = &regs->igaddr5;\r\npriv->hash_regs[6] = &regs->igaddr6;\r\npriv->hash_regs[7] = &regs->igaddr7;\r\npriv->hash_regs[8] = &regs->gaddr0;\r\npriv->hash_regs[9] = &regs->gaddr1;\r\npriv->hash_regs[10] = &regs->gaddr2;\r\npriv->hash_regs[11] = &regs->gaddr3;\r\npriv->hash_regs[12] = &regs->gaddr4;\r\npriv->hash_regs[13] = &regs->gaddr5;\r\npriv->hash_regs[14] = &regs->gaddr6;\r\npriv->hash_regs[15] = &regs->gaddr7;\r\n} else {\r\npriv->extended_hash = 0;\r\npriv->hash_width = 8;\r\npriv->hash_regs[0] = &regs->gaddr0;\r\npriv->hash_regs[1] = &regs->gaddr1;\r\npriv->hash_regs[2] = &regs->gaddr2;\r\npriv->hash_regs[3] = &regs->gaddr3;\r\npriv->hash_regs[4] = &regs->gaddr4;\r\npriv->hash_regs[5] = &regs->gaddr5;\r\npriv->hash_regs[6] = &regs->gaddr6;\r\npriv->hash_regs[7] = &regs->gaddr7;\r\n}\r\n}\r\nstatic int gfar_probe(struct platform_device *ofdev)\r\n{\r\nstruct device_node *np = ofdev->dev.of_node;\r\nstruct net_device *dev = NULL;\r\nstruct gfar_private *priv = NULL;\r\nint err = 0, i;\r\nerr = gfar_of_init(ofdev, &dev);\r\nif (err)\r\nreturn err;\r\npriv = netdev_priv(dev);\r\npriv->ndev = dev;\r\npriv->ofdev = ofdev;\r\npriv->dev = &ofdev->dev;\r\nSET_NETDEV_DEV(dev, &ofdev->dev);\r\nINIT_WORK(&priv->reset_task, gfar_reset_task);\r\nplatform_set_drvdata(ofdev, priv);\r\ngfar_detect_errata(priv);\r\ndev->base_addr = (unsigned long) priv->gfargrp[0].regs;\r\ndev->watchdog_timeo = TX_TIMEOUT;\r\ndev->mtu = 1500;\r\ndev->min_mtu = 50;\r\ndev->max_mtu = GFAR_JUMBO_FRAME_SIZE - ETH_HLEN;\r\ndev->netdev_ops = &gfar_netdev_ops;\r\ndev->ethtool_ops = &gfar_ethtool_ops;\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nif (priv->poll_mode == GFAR_SQ_POLLING) {\r\nnetif_napi_add(dev, &priv->gfargrp[i].napi_rx,\r\ngfar_poll_rx_sq, GFAR_DEV_WEIGHT);\r\nnetif_tx_napi_add(dev, &priv->gfargrp[i].napi_tx,\r\ngfar_poll_tx_sq, 2);\r\n} else {\r\nnetif_napi_add(dev, &priv->gfargrp[i].napi_rx,\r\ngfar_poll_rx, GFAR_DEV_WEIGHT);\r\nnetif_tx_napi_add(dev, &priv->gfargrp[i].napi_tx,\r\ngfar_poll_tx, 2);\r\n}\r\n}\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_CSUM) {\r\ndev->hw_features = NETIF_F_IP_CSUM | NETIF_F_SG |\r\nNETIF_F_RXCSUM;\r\ndev->features |= NETIF_F_IP_CSUM | NETIF_F_SG |\r\nNETIF_F_RXCSUM | NETIF_F_HIGHDMA;\r\n}\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_VLAN) {\r\ndev->hw_features |= NETIF_F_HW_VLAN_CTAG_TX |\r\nNETIF_F_HW_VLAN_CTAG_RX;\r\ndev->features |= NETIF_F_HW_VLAN_CTAG_RX;\r\n}\r\ndev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\r\ngfar_init_addr_hash_table(priv);\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER)\r\npriv->padding = 8;\r\nif (dev->features & NETIF_F_IP_CSUM ||\r\npriv->device_flags & FSL_GIANFAR_DEV_HAS_TIMER)\r\ndev->needed_headroom = GMAC_FCB_LEN;\r\nfor (i = 0; i < priv->num_tx_queues; i++) {\r\npriv->tx_queue[i]->tx_ring_size = DEFAULT_TX_RING_SIZE;\r\npriv->tx_queue[i]->num_txbdfree = DEFAULT_TX_RING_SIZE;\r\npriv->tx_queue[i]->txcoalescing = DEFAULT_TX_COALESCE;\r\npriv->tx_queue[i]->txic = DEFAULT_TXIC;\r\n}\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\npriv->rx_queue[i]->rx_ring_size = DEFAULT_RX_RING_SIZE;\r\npriv->rx_queue[i]->rxcoalescing = DEFAULT_RX_COALESCE;\r\npriv->rx_queue[i]->rxic = DEFAULT_RXIC;\r\n}\r\npriv->rx_filer_enable =\r\n(priv->device_flags & FSL_GIANFAR_DEV_HAS_RX_FILER) ? 1 : 0;\r\npriv->msg_enable = (NETIF_MSG_IFUP << 1 ) - 1;\r\nif (priv->num_tx_queues == 1)\r\npriv->prio_sched_en = 1;\r\nset_bit(GFAR_DOWN, &priv->state);\r\ngfar_hw_init(priv);\r\nnetif_carrier_off(dev);\r\nerr = register_netdev(dev);\r\nif (err) {\r\npr_err("%s: Cannot register net device, aborting\n", dev->name);\r\ngoto register_fail;\r\n}\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MAGIC_PACKET)\r\npriv->wol_supported |= GFAR_WOL_MAGIC;\r\nif ((priv->device_flags & FSL_GIANFAR_DEV_HAS_WAKE_ON_FILER) &&\r\npriv->rx_filer_enable)\r\npriv->wol_supported |= GFAR_WOL_FILER_UCAST;\r\ndevice_set_wakeup_capable(&ofdev->dev, priv->wol_supported);\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nstruct gfar_priv_grp *grp = &priv->gfargrp[i];\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) {\r\nsprintf(gfar_irq(grp, TX)->name, "%s%s%c%s",\r\ndev->name, "_g", '0' + i, "_tx");\r\nsprintf(gfar_irq(grp, RX)->name, "%s%s%c%s",\r\ndev->name, "_g", '0' + i, "_rx");\r\nsprintf(gfar_irq(grp, ER)->name, "%s%s%c%s",\r\ndev->name, "_g", '0' + i, "_er");\r\n} else\r\nstrcpy(gfar_irq(grp, TX)->name, dev->name);\r\n}\r\ngfar_init_filer_table(priv);\r\nnetdev_info(dev, "mac: %pM\n", dev->dev_addr);\r\nnetdev_info(dev, "Running with NAPI enabled\n");\r\nfor (i = 0; i < priv->num_rx_queues; i++)\r\nnetdev_info(dev, "RX BD ring size for Q[%d]: %d\n",\r\ni, priv->rx_queue[i]->rx_ring_size);\r\nfor (i = 0; i < priv->num_tx_queues; i++)\r\nnetdev_info(dev, "TX BD ring size for Q[%d]: %d\n",\r\ni, priv->tx_queue[i]->tx_ring_size);\r\nreturn 0;\r\nregister_fail:\r\nif (of_phy_is_fixed_link(np))\r\nof_phy_deregister_fixed_link(np);\r\nunmap_group_regs(priv);\r\ngfar_free_rx_queues(priv);\r\ngfar_free_tx_queues(priv);\r\nof_node_put(priv->phy_node);\r\nof_node_put(priv->tbi_node);\r\nfree_gfar_dev(priv);\r\nreturn err;\r\n}\r\nstatic int gfar_remove(struct platform_device *ofdev)\r\n{\r\nstruct gfar_private *priv = platform_get_drvdata(ofdev);\r\nstruct device_node *np = ofdev->dev.of_node;\r\nof_node_put(priv->phy_node);\r\nof_node_put(priv->tbi_node);\r\nunregister_netdev(priv->ndev);\r\nif (of_phy_is_fixed_link(np))\r\nof_phy_deregister_fixed_link(np);\r\nunmap_group_regs(priv);\r\ngfar_free_rx_queues(priv);\r\ngfar_free_tx_queues(priv);\r\nfree_gfar_dev(priv);\r\nreturn 0;\r\n}\r\nstatic void __gfar_filer_disable(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 temp;\r\ntemp = gfar_read(&regs->rctrl);\r\ntemp &= ~(RCTRL_FILREN | RCTRL_PRSDEP_INIT);\r\ngfar_write(&regs->rctrl, temp);\r\n}\r\nstatic void __gfar_filer_enable(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 temp;\r\ntemp = gfar_read(&regs->rctrl);\r\ntemp |= RCTRL_FILREN | RCTRL_PRSDEP_INIT;\r\ngfar_write(&regs->rctrl, temp);\r\n}\r\nstatic void gfar_filer_config_wol(struct gfar_private *priv)\r\n{\r\nunsigned int i;\r\nu32 rqfcr;\r\n__gfar_filer_disable(priv);\r\nrqfcr = RQFCR_RJE | RQFCR_CMP_MATCH;\r\nfor (i = 0; i <= MAX_FILER_IDX; i++)\r\ngfar_write_filer(priv, i, rqfcr, 0);\r\ni = 0;\r\nif (priv->wol_opts & GFAR_WOL_FILER_UCAST) {\r\nstruct net_device *ndev = priv->ndev;\r\nu8 qindex = (u8)priv->gfargrp[0].rx_queue->qindex;\r\nu32 dest_mac_addr = (ndev->dev_addr[0] << 16) |\r\n(ndev->dev_addr[1] << 8) |\r\nndev->dev_addr[2];\r\nrqfcr = (qindex << 10) | RQFCR_AND |\r\nRQFCR_CMP_EXACT | RQFCR_PID_DAH;\r\ngfar_write_filer(priv, i++, rqfcr, dest_mac_addr);\r\ndest_mac_addr = (ndev->dev_addr[3] << 16) |\r\n(ndev->dev_addr[4] << 8) |\r\nndev->dev_addr[5];\r\nrqfcr = (qindex << 10) | RQFCR_GPI |\r\nRQFCR_CMP_EXACT | RQFCR_PID_DAL;\r\ngfar_write_filer(priv, i++, rqfcr, dest_mac_addr);\r\n}\r\n__gfar_filer_enable(priv);\r\n}\r\nstatic void gfar_filer_restore_table(struct gfar_private *priv)\r\n{\r\nu32 rqfcr, rqfpr;\r\nunsigned int i;\r\n__gfar_filer_disable(priv);\r\nfor (i = 0; i <= MAX_FILER_IDX; i++) {\r\nrqfcr = priv->ftp_rqfcr[i];\r\nrqfpr = priv->ftp_rqfpr[i];\r\ngfar_write_filer(priv, i, rqfcr, rqfpr);\r\n}\r\n__gfar_filer_enable(priv);\r\n}\r\nstatic void gfar_start_wol_filer(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\nint i = 0;\r\ngfar_write(&regs->rqueue, priv->rqueue);\r\ntempval = gfar_read(&regs->dmactrl);\r\ntempval |= DMACTRL_INIT_SETTINGS;\r\ngfar_write(&regs->dmactrl, tempval);\r\ntempval = gfar_read(&regs->dmactrl);\r\ntempval &= ~DMACTRL_GRS;\r\ngfar_write(&regs->dmactrl, tempval);\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nregs = priv->gfargrp[i].regs;\r\ngfar_write(&regs->rstat, priv->gfargrp[i].rstat);\r\ngfar_write(&regs->imask, IMASK_FGPI);\r\n}\r\ntempval = gfar_read(&regs->maccfg1);\r\ntempval |= MACCFG1_RX_EN;\r\ngfar_write(&regs->maccfg1, tempval);\r\n}\r\nstatic int gfar_suspend(struct device *dev)\r\n{\r\nstruct gfar_private *priv = dev_get_drvdata(dev);\r\nstruct net_device *ndev = priv->ndev;\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\nu16 wol = priv->wol_opts;\r\nif (!netif_running(ndev))\r\nreturn 0;\r\ndisable_napi(priv);\r\nnetif_tx_lock(ndev);\r\nnetif_device_detach(ndev);\r\nnetif_tx_unlock(ndev);\r\ngfar_halt(priv);\r\nif (wol & GFAR_WOL_MAGIC) {\r\ngfar_write(&regs->imask, IMASK_MAG);\r\ntempval = gfar_read(&regs->maccfg2);\r\ntempval |= MACCFG2_MPEN;\r\ngfar_write(&regs->maccfg2, tempval);\r\ntempval = gfar_read(&regs->maccfg1);\r\ntempval |= MACCFG1_RX_EN;\r\ngfar_write(&regs->maccfg1, tempval);\r\n} else if (wol & GFAR_WOL_FILER_UCAST) {\r\ngfar_filer_config_wol(priv);\r\ngfar_start_wol_filer(priv);\r\n} else {\r\nphy_stop(ndev->phydev);\r\n}\r\nreturn 0;\r\n}\r\nstatic int gfar_resume(struct device *dev)\r\n{\r\nstruct gfar_private *priv = dev_get_drvdata(dev);\r\nstruct net_device *ndev = priv->ndev;\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\nu16 wol = priv->wol_opts;\r\nif (!netif_running(ndev))\r\nreturn 0;\r\nif (wol & GFAR_WOL_MAGIC) {\r\ntempval = gfar_read(&regs->maccfg2);\r\ntempval &= ~MACCFG2_MPEN;\r\ngfar_write(&regs->maccfg2, tempval);\r\n} else if (wol & GFAR_WOL_FILER_UCAST) {\r\ngfar_halt(priv);\r\ngfar_filer_restore_table(priv);\r\n} else {\r\nphy_start(ndev->phydev);\r\n}\r\ngfar_start(priv);\r\nnetif_device_attach(ndev);\r\nenable_napi(priv);\r\nreturn 0;\r\n}\r\nstatic int gfar_restore(struct device *dev)\r\n{\r\nstruct gfar_private *priv = dev_get_drvdata(dev);\r\nstruct net_device *ndev = priv->ndev;\r\nif (!netif_running(ndev)) {\r\nnetif_device_attach(ndev);\r\nreturn 0;\r\n}\r\ngfar_init_bds(ndev);\r\ngfar_mac_reset(priv);\r\ngfar_init_tx_rx_base(priv);\r\ngfar_start(priv);\r\npriv->oldlink = 0;\r\npriv->oldspeed = 0;\r\npriv->oldduplex = -1;\r\nif (ndev->phydev)\r\nphy_start(ndev->phydev);\r\nnetif_device_attach(ndev);\r\nenable_napi(priv);\r\nreturn 0;\r\n}\r\nstatic phy_interface_t gfar_get_interface(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 ecntrl;\r\necntrl = gfar_read(&regs->ecntrl);\r\nif (ecntrl & ECNTRL_SGMII_MODE)\r\nreturn PHY_INTERFACE_MODE_SGMII;\r\nif (ecntrl & ECNTRL_TBI_MODE) {\r\nif (ecntrl & ECNTRL_REDUCED_MODE)\r\nreturn PHY_INTERFACE_MODE_RTBI;\r\nelse\r\nreturn PHY_INTERFACE_MODE_TBI;\r\n}\r\nif (ecntrl & ECNTRL_REDUCED_MODE) {\r\nif (ecntrl & ECNTRL_REDUCED_MII_MODE) {\r\nreturn PHY_INTERFACE_MODE_RMII;\r\n}\r\nelse {\r\nphy_interface_t interface = priv->interface;\r\nif (interface == PHY_INTERFACE_MODE_RGMII_ID)\r\nreturn PHY_INTERFACE_MODE_RGMII_ID;\r\nreturn PHY_INTERFACE_MODE_RGMII;\r\n}\r\n}\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_GIGABIT)\r\nreturn PHY_INTERFACE_MODE_GMII;\r\nreturn PHY_INTERFACE_MODE_MII;\r\n}\r\nstatic int init_phy(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nuint gigabit_support =\r\npriv->device_flags & FSL_GIANFAR_DEV_HAS_GIGABIT ?\r\nGFAR_SUPPORTED_GBIT : 0;\r\nphy_interface_t interface;\r\nstruct phy_device *phydev;\r\npriv->oldlink = 0;\r\npriv->oldspeed = 0;\r\npriv->oldduplex = -1;\r\ninterface = gfar_get_interface(dev);\r\nphydev = of_phy_connect(dev, priv->phy_node, &adjust_link, 0,\r\ninterface);\r\nif (!phydev) {\r\ndev_err(&dev->dev, "could not attach to PHY\n");\r\nreturn -ENODEV;\r\n}\r\nif (interface == PHY_INTERFACE_MODE_SGMII)\r\ngfar_configure_serdes(dev);\r\nphydev->supported &= (GFAR_SUPPORTED | gigabit_support);\r\nphydev->advertising = phydev->supported;\r\nphydev->supported |= (SUPPORTED_Pause | SUPPORTED_Asym_Pause);\r\nreturn 0;\r\n}\r\nstatic void gfar_configure_serdes(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nstruct phy_device *tbiphy;\r\nif (!priv->tbi_node) {\r\ndev_warn(&dev->dev, "error: SGMII mode requires that the "\r\n"device tree specify a tbi-handle\n");\r\nreturn;\r\n}\r\ntbiphy = of_phy_find_device(priv->tbi_node);\r\nif (!tbiphy) {\r\ndev_err(&dev->dev, "error: Could not get TBI device\n");\r\nreturn;\r\n}\r\nif (phy_read(tbiphy, MII_BMSR) & BMSR_LSTATUS) {\r\nput_device(&tbiphy->mdio.dev);\r\nreturn;\r\n}\r\nphy_write(tbiphy, MII_TBICON, TBICON_CLK_SELECT);\r\nphy_write(tbiphy, MII_ADVERTISE,\r\nADVERTISE_1000XFULL | ADVERTISE_1000XPAUSE |\r\nADVERTISE_1000XPSE_ASYM);\r\nphy_write(tbiphy, MII_BMCR,\r\nBMCR_ANENABLE | BMCR_ANRESTART | BMCR_FULLDPLX |\r\nBMCR_SPEED1000);\r\nput_device(&tbiphy->mdio.dev);\r\n}\r\nstatic int __gfar_is_rx_idle(struct gfar_private *priv)\r\n{\r\nu32 res;\r\nif (!gfar_has_errata(priv, GFAR_ERRATA_A002))\r\nreturn 0;\r\nres = gfar_read((void __iomem *)priv->gfargrp[0].regs + 0xd1c);\r\nres &= 0x7f807f80;\r\nif ((res & 0xffff) == (res >> 16))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void gfar_halt_nodisable(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\nunsigned int timeout;\r\nint stopped;\r\ngfar_ints_disable(priv);\r\nif (gfar_is_dma_stopped(priv))\r\nreturn;\r\ntempval = gfar_read(&regs->dmactrl);\r\ntempval |= (DMACTRL_GRS | DMACTRL_GTS);\r\ngfar_write(&regs->dmactrl, tempval);\r\nretry:\r\ntimeout = 1000;\r\nwhile (!(stopped = gfar_is_dma_stopped(priv)) && timeout) {\r\ncpu_relax();\r\ntimeout--;\r\n}\r\nif (!timeout)\r\nstopped = gfar_is_dma_stopped(priv);\r\nif (!stopped && !gfar_is_rx_dma_stopped(priv) &&\r\n!__gfar_is_rx_idle(priv))\r\ngoto retry;\r\n}\r\nvoid gfar_halt(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\ngfar_write(&regs->rqueue, 0);\r\ngfar_write(&regs->tqueue, 0);\r\nmdelay(10);\r\ngfar_halt_nodisable(priv);\r\ntempval = gfar_read(&regs->maccfg1);\r\ntempval &= ~(MACCFG1_RX_EN | MACCFG1_TX_EN);\r\ngfar_write(&regs->maccfg1, tempval);\r\n}\r\nvoid stop_gfar(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nnetif_tx_stop_all_queues(dev);\r\nsmp_mb__before_atomic();\r\nset_bit(GFAR_DOWN, &priv->state);\r\nsmp_mb__after_atomic();\r\ndisable_napi(priv);\r\ngfar_halt(priv);\r\nphy_stop(dev->phydev);\r\nfree_skb_resources(priv);\r\n}\r\nstatic void free_skb_tx_queue(struct gfar_priv_tx_q *tx_queue)\r\n{\r\nstruct txbd8 *txbdp;\r\nstruct gfar_private *priv = netdev_priv(tx_queue->dev);\r\nint i, j;\r\ntxbdp = tx_queue->tx_bd_base;\r\nfor (i = 0; i < tx_queue->tx_ring_size; i++) {\r\nif (!tx_queue->tx_skbuff[i])\r\ncontinue;\r\ndma_unmap_single(priv->dev, be32_to_cpu(txbdp->bufPtr),\r\nbe16_to_cpu(txbdp->length), DMA_TO_DEVICE);\r\ntxbdp->lstatus = 0;\r\nfor (j = 0; j < skb_shinfo(tx_queue->tx_skbuff[i])->nr_frags;\r\nj++) {\r\ntxbdp++;\r\ndma_unmap_page(priv->dev, be32_to_cpu(txbdp->bufPtr),\r\nbe16_to_cpu(txbdp->length),\r\nDMA_TO_DEVICE);\r\n}\r\ntxbdp++;\r\ndev_kfree_skb_any(tx_queue->tx_skbuff[i]);\r\ntx_queue->tx_skbuff[i] = NULL;\r\n}\r\nkfree(tx_queue->tx_skbuff);\r\ntx_queue->tx_skbuff = NULL;\r\n}\r\nstatic void free_skb_rx_queue(struct gfar_priv_rx_q *rx_queue)\r\n{\r\nint i;\r\nstruct rxbd8 *rxbdp = rx_queue->rx_bd_base;\r\nif (rx_queue->skb)\r\ndev_kfree_skb(rx_queue->skb);\r\nfor (i = 0; i < rx_queue->rx_ring_size; i++) {\r\nstruct gfar_rx_buff *rxb = &rx_queue->rx_buff[i];\r\nrxbdp->lstatus = 0;\r\nrxbdp->bufPtr = 0;\r\nrxbdp++;\r\nif (!rxb->page)\r\ncontinue;\r\ndma_unmap_page(rx_queue->dev, rxb->dma,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\n__free_page(rxb->page);\r\nrxb->page = NULL;\r\n}\r\nkfree(rx_queue->rx_buff);\r\nrx_queue->rx_buff = NULL;\r\n}\r\nstatic void free_skb_resources(struct gfar_private *priv)\r\n{\r\nstruct gfar_priv_tx_q *tx_queue = NULL;\r\nstruct gfar_priv_rx_q *rx_queue = NULL;\r\nint i;\r\nfor (i = 0; i < priv->num_tx_queues; i++) {\r\nstruct netdev_queue *txq;\r\ntx_queue = priv->tx_queue[i];\r\ntxq = netdev_get_tx_queue(tx_queue->dev, tx_queue->qindex);\r\nif (tx_queue->tx_skbuff)\r\nfree_skb_tx_queue(tx_queue);\r\nnetdev_tx_reset_queue(txq);\r\n}\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\nrx_queue = priv->rx_queue[i];\r\nif (rx_queue->rx_buff)\r\nfree_skb_rx_queue(rx_queue);\r\n}\r\ndma_free_coherent(priv->dev,\r\nsizeof(struct txbd8) * priv->total_tx_ring_size +\r\nsizeof(struct rxbd8) * priv->total_rx_ring_size,\r\npriv->tx_queue[0]->tx_bd_base,\r\npriv->tx_queue[0]->tx_bd_dma_base);\r\n}\r\nvoid gfar_start(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\nint i = 0;\r\ngfar_write(&regs->rqueue, priv->rqueue);\r\ngfar_write(&regs->tqueue, priv->tqueue);\r\ntempval = gfar_read(&regs->dmactrl);\r\ntempval |= DMACTRL_INIT_SETTINGS;\r\ngfar_write(&regs->dmactrl, tempval);\r\ntempval = gfar_read(&regs->dmactrl);\r\ntempval &= ~(DMACTRL_GRS | DMACTRL_GTS);\r\ngfar_write(&regs->dmactrl, tempval);\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nregs = priv->gfargrp[i].regs;\r\ngfar_write(&regs->tstat, priv->gfargrp[i].tstat);\r\ngfar_write(&regs->rstat, priv->gfargrp[i].rstat);\r\n}\r\ntempval = gfar_read(&regs->maccfg1);\r\ntempval |= (MACCFG1_RX_EN | MACCFG1_TX_EN);\r\ngfar_write(&regs->maccfg1, tempval);\r\ngfar_ints_enable(priv);\r\nnetif_trans_update(priv->ndev);\r\n}\r\nstatic void free_grp_irqs(struct gfar_priv_grp *grp)\r\n{\r\nfree_irq(gfar_irq(grp, TX)->irq, grp);\r\nfree_irq(gfar_irq(grp, RX)->irq, grp);\r\nfree_irq(gfar_irq(grp, ER)->irq, grp);\r\n}\r\nstatic int register_grp_irqs(struct gfar_priv_grp *grp)\r\n{\r\nstruct gfar_private *priv = grp->priv;\r\nstruct net_device *dev = priv->ndev;\r\nint err;\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) {\r\nerr = request_irq(gfar_irq(grp, ER)->irq, gfar_error, 0,\r\ngfar_irq(grp, ER)->name, grp);\r\nif (err < 0) {\r\nnetif_err(priv, intr, dev, "Can't get IRQ %d\n",\r\ngfar_irq(grp, ER)->irq);\r\ngoto err_irq_fail;\r\n}\r\nenable_irq_wake(gfar_irq(grp, ER)->irq);\r\nerr = request_irq(gfar_irq(grp, TX)->irq, gfar_transmit, 0,\r\ngfar_irq(grp, TX)->name, grp);\r\nif (err < 0) {\r\nnetif_err(priv, intr, dev, "Can't get IRQ %d\n",\r\ngfar_irq(grp, TX)->irq);\r\ngoto tx_irq_fail;\r\n}\r\nerr = request_irq(gfar_irq(grp, RX)->irq, gfar_receive, 0,\r\ngfar_irq(grp, RX)->name, grp);\r\nif (err < 0) {\r\nnetif_err(priv, intr, dev, "Can't get IRQ %d\n",\r\ngfar_irq(grp, RX)->irq);\r\ngoto rx_irq_fail;\r\n}\r\nenable_irq_wake(gfar_irq(grp, RX)->irq);\r\n} else {\r\nerr = request_irq(gfar_irq(grp, TX)->irq, gfar_interrupt, 0,\r\ngfar_irq(grp, TX)->name, grp);\r\nif (err < 0) {\r\nnetif_err(priv, intr, dev, "Can't get IRQ %d\n",\r\ngfar_irq(grp, TX)->irq);\r\ngoto err_irq_fail;\r\n}\r\nenable_irq_wake(gfar_irq(grp, TX)->irq);\r\n}\r\nreturn 0;\r\nrx_irq_fail:\r\nfree_irq(gfar_irq(grp, TX)->irq, grp);\r\ntx_irq_fail:\r\nfree_irq(gfar_irq(grp, ER)->irq, grp);\r\nerr_irq_fail:\r\nreturn err;\r\n}\r\nstatic void gfar_free_irq(struct gfar_private *priv)\r\n{\r\nint i;\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) {\r\nfor (i = 0; i < priv->num_grps; i++)\r\nfree_grp_irqs(&priv->gfargrp[i]);\r\n} else {\r\nfor (i = 0; i < priv->num_grps; i++)\r\nfree_irq(gfar_irq(&priv->gfargrp[i], TX)->irq,\r\n&priv->gfargrp[i]);\r\n}\r\n}\r\nstatic int gfar_request_irq(struct gfar_private *priv)\r\n{\r\nint err, i, j;\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nerr = register_grp_irqs(&priv->gfargrp[i]);\r\nif (err) {\r\nfor (j = 0; j < i; j++)\r\nfree_grp_irqs(&priv->gfargrp[j]);\r\nreturn err;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint startup_gfar(struct net_device *ndev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(ndev);\r\nint err;\r\ngfar_mac_reset(priv);\r\nerr = gfar_alloc_skb_resources(ndev);\r\nif (err)\r\nreturn err;\r\ngfar_init_tx_rx_base(priv);\r\nsmp_mb__before_atomic();\r\nclear_bit(GFAR_DOWN, &priv->state);\r\nsmp_mb__after_atomic();\r\ngfar_start(priv);\r\npriv->oldlink = 0;\r\npriv->oldspeed = 0;\r\npriv->oldduplex = -1;\r\nphy_start(ndev->phydev);\r\nenable_napi(priv);\r\nnetif_tx_wake_all_queues(ndev);\r\nreturn 0;\r\n}\r\nstatic int gfar_enet_open(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nint err;\r\nerr = init_phy(dev);\r\nif (err)\r\nreturn err;\r\nerr = gfar_request_irq(priv);\r\nif (err)\r\nreturn err;\r\nerr = startup_gfar(dev);\r\nif (err)\r\nreturn err;\r\nreturn err;\r\n}\r\nstatic inline struct txfcb *gfar_add_fcb(struct sk_buff *skb)\r\n{\r\nstruct txfcb *fcb = (struct txfcb *)skb_push(skb, GMAC_FCB_LEN);\r\nmemset(fcb, 0, GMAC_FCB_LEN);\r\nreturn fcb;\r\n}\r\nstatic inline void gfar_tx_checksum(struct sk_buff *skb, struct txfcb *fcb,\r\nint fcb_length)\r\n{\r\nu8 flags = TXFCB_DEFAULT;\r\nif (ip_hdr(skb)->protocol == IPPROTO_UDP) {\r\nflags |= TXFCB_UDP;\r\nfcb->phcs = (__force __be16)(udp_hdr(skb)->check);\r\n} else\r\nfcb->phcs = (__force __be16)(tcp_hdr(skb)->check);\r\nfcb->l3os = (u8)(skb_network_offset(skb) - fcb_length);\r\nfcb->l4os = skb_network_header_len(skb);\r\nfcb->flags = flags;\r\n}\r\nstatic inline void gfar_tx_vlan(struct sk_buff *skb, struct txfcb *fcb)\r\n{\r\nfcb->flags |= TXFCB_VLN;\r\nfcb->vlctl = cpu_to_be16(skb_vlan_tag_get(skb));\r\n}\r\nstatic inline struct txbd8 *skip_txbd(struct txbd8 *bdp, int stride,\r\nstruct txbd8 *base, int ring_size)\r\n{\r\nstruct txbd8 *new_bd = bdp + stride;\r\nreturn (new_bd >= (base + ring_size)) ? (new_bd - ring_size) : new_bd;\r\n}\r\nstatic inline struct txbd8 *next_txbd(struct txbd8 *bdp, struct txbd8 *base,\r\nint ring_size)\r\n{\r\nreturn skip_txbd(bdp, 1, base, ring_size);\r\n}\r\nstatic inline bool gfar_csum_errata_12(struct gfar_private *priv,\r\nunsigned long fcb_addr)\r\n{\r\nreturn (gfar_has_errata(priv, GFAR_ERRATA_12) &&\r\n(fcb_addr % 0x20) > 0x18);\r\n}\r\nstatic inline bool gfar_csum_errata_76(struct gfar_private *priv,\r\nunsigned int len)\r\n{\r\nreturn (gfar_has_errata(priv, GFAR_ERRATA_76) &&\r\n(len > 2500));\r\n}\r\nstatic int gfar_start_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nstruct gfar_priv_tx_q *tx_queue = NULL;\r\nstruct netdev_queue *txq;\r\nstruct gfar __iomem *regs = NULL;\r\nstruct txfcb *fcb = NULL;\r\nstruct txbd8 *txbdp, *txbdp_start, *base, *txbdp_tstamp = NULL;\r\nu32 lstatus;\r\nskb_frag_t *frag;\r\nint i, rq = 0;\r\nint do_tstamp, do_csum, do_vlan;\r\nu32 bufaddr;\r\nunsigned int nr_frags, nr_txbds, bytes_sent, fcb_len = 0;\r\nrq = skb->queue_mapping;\r\ntx_queue = priv->tx_queue[rq];\r\ntxq = netdev_get_tx_queue(dev, rq);\r\nbase = tx_queue->tx_bd_base;\r\nregs = tx_queue->grp->regs;\r\ndo_csum = (CHECKSUM_PARTIAL == skb->ip_summed);\r\ndo_vlan = skb_vlan_tag_present(skb);\r\ndo_tstamp = (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\r\npriv->hwts_tx_en;\r\nif (do_csum || do_vlan)\r\nfcb_len = GMAC_FCB_LEN;\r\nif (unlikely(do_tstamp))\r\nfcb_len = GMAC_FCB_LEN + GMAC_TXPAL_LEN;\r\nif (fcb_len && unlikely(skb_headroom(skb) < fcb_len)) {\r\nstruct sk_buff *skb_new;\r\nskb_new = skb_realloc_headroom(skb, fcb_len);\r\nif (!skb_new) {\r\ndev->stats.tx_errors++;\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nif (skb->sk)\r\nskb_set_owner_w(skb_new, skb->sk);\r\ndev_consume_skb_any(skb);\r\nskb = skb_new;\r\n}\r\nnr_frags = skb_shinfo(skb)->nr_frags;\r\nif (unlikely(do_tstamp))\r\nnr_txbds = nr_frags + 2;\r\nelse\r\nnr_txbds = nr_frags + 1;\r\nif (nr_txbds > tx_queue->num_txbdfree) {\r\nnetif_tx_stop_queue(txq);\r\ndev->stats.tx_fifo_errors++;\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nbytes_sent = skb->len;\r\ntx_queue->stats.tx_bytes += bytes_sent;\r\nGFAR_CB(skb)->bytes_sent = bytes_sent;\r\ntx_queue->stats.tx_packets++;\r\ntxbdp = txbdp_start = tx_queue->cur_tx;\r\nlstatus = be32_to_cpu(txbdp->lstatus);\r\nif (unlikely(do_tstamp)) {\r\nskb_push(skb, GMAC_TXPAL_LEN);\r\nmemset(skb->data, 0, GMAC_TXPAL_LEN);\r\n}\r\nif (fcb_len) {\r\nfcb = gfar_add_fcb(skb);\r\nlstatus |= BD_LFLAG(TXBD_TOE);\r\n}\r\nif (do_csum) {\r\ngfar_tx_checksum(skb, fcb, fcb_len);\r\nif (unlikely(gfar_csum_errata_12(priv, (unsigned long)fcb)) ||\r\nunlikely(gfar_csum_errata_76(priv, skb->len))) {\r\n__skb_pull(skb, GMAC_FCB_LEN);\r\nskb_checksum_help(skb);\r\nif (do_vlan || do_tstamp) {\r\nfcb = gfar_add_fcb(skb);\r\n} else {\r\nlstatus &= ~(BD_LFLAG(TXBD_TOE));\r\nfcb = NULL;\r\n}\r\n}\r\n}\r\nif (do_vlan)\r\ngfar_tx_vlan(skb, fcb);\r\nbufaddr = dma_map_single(priv->dev, skb->data, skb_headlen(skb),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(priv->dev, bufaddr)))\r\ngoto dma_map_err;\r\ntxbdp_start->bufPtr = cpu_to_be32(bufaddr);\r\nif (unlikely(do_tstamp))\r\ntxbdp_tstamp = txbdp = next_txbd(txbdp, base,\r\ntx_queue->tx_ring_size);\r\nif (likely(!nr_frags)) {\r\nif (likely(!do_tstamp))\r\nlstatus |= BD_LFLAG(TXBD_LAST | TXBD_INTERRUPT);\r\n} else {\r\nu32 lstatus_start = lstatus;\r\nfrag = &skb_shinfo(skb)->frags[0];\r\nfor (i = 0; i < nr_frags; i++, frag++) {\r\nunsigned int size;\r\ntxbdp = next_txbd(txbdp, base, tx_queue->tx_ring_size);\r\nsize = skb_frag_size(frag);\r\nlstatus = be32_to_cpu(txbdp->lstatus) | size |\r\nBD_LFLAG(TXBD_READY);\r\nif (i == nr_frags - 1)\r\nlstatus |= BD_LFLAG(TXBD_LAST | TXBD_INTERRUPT);\r\nbufaddr = skb_frag_dma_map(priv->dev, frag, 0,\r\nsize, DMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(priv->dev, bufaddr)))\r\ngoto dma_map_err;\r\ntxbdp->bufPtr = cpu_to_be32(bufaddr);\r\ntxbdp->lstatus = cpu_to_be32(lstatus);\r\n}\r\nlstatus = lstatus_start;\r\n}\r\nif (unlikely(do_tstamp)) {\r\nu32 lstatus_ts = be32_to_cpu(txbdp_tstamp->lstatus);\r\nbufaddr = be32_to_cpu(txbdp_start->bufPtr);\r\nbufaddr += fcb_len;\r\nlstatus_ts |= BD_LFLAG(TXBD_READY) |\r\n(skb_headlen(skb) - fcb_len);\r\nif (!nr_frags)\r\nlstatus_ts |= BD_LFLAG(TXBD_LAST | TXBD_INTERRUPT);\r\ntxbdp_tstamp->bufPtr = cpu_to_be32(bufaddr);\r\ntxbdp_tstamp->lstatus = cpu_to_be32(lstatus_ts);\r\nlstatus |= BD_LFLAG(TXBD_CRC | TXBD_READY) | GMAC_FCB_LEN;\r\nskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\r\nfcb->ptp = 1;\r\n} else {\r\nlstatus |= BD_LFLAG(TXBD_CRC | TXBD_READY) | skb_headlen(skb);\r\n}\r\nnetdev_tx_sent_queue(txq, bytes_sent);\r\ngfar_wmb();\r\ntxbdp_start->lstatus = cpu_to_be32(lstatus);\r\ngfar_wmb();\r\ntx_queue->tx_skbuff[tx_queue->skb_curtx] = skb;\r\ntx_queue->skb_curtx = (tx_queue->skb_curtx + 1) &\r\nTX_RING_MOD_MASK(tx_queue->tx_ring_size);\r\ntx_queue->cur_tx = next_txbd(txbdp, base, tx_queue->tx_ring_size);\r\nspin_lock_bh(&tx_queue->txlock);\r\ntx_queue->num_txbdfree -= (nr_txbds);\r\nspin_unlock_bh(&tx_queue->txlock);\r\nif (!tx_queue->num_txbdfree) {\r\nnetif_tx_stop_queue(txq);\r\ndev->stats.tx_fifo_errors++;\r\n}\r\ngfar_write(&regs->tstat, TSTAT_CLEAR_THALT >> tx_queue->qindex);\r\nreturn NETDEV_TX_OK;\r\ndma_map_err:\r\ntxbdp = next_txbd(txbdp_start, base, tx_queue->tx_ring_size);\r\nif (do_tstamp)\r\ntxbdp = next_txbd(txbdp, base, tx_queue->tx_ring_size);\r\nfor (i = 0; i < nr_frags; i++) {\r\nlstatus = be32_to_cpu(txbdp->lstatus);\r\nif (!(lstatus & BD_LFLAG(TXBD_READY)))\r\nbreak;\r\nlstatus &= ~BD_LFLAG(TXBD_READY);\r\ntxbdp->lstatus = cpu_to_be32(lstatus);\r\nbufaddr = be32_to_cpu(txbdp->bufPtr);\r\ndma_unmap_page(priv->dev, bufaddr, be16_to_cpu(txbdp->length),\r\nDMA_TO_DEVICE);\r\ntxbdp = next_txbd(txbdp, base, tx_queue->tx_ring_size);\r\n}\r\ngfar_wmb();\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic int gfar_close(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\ncancel_work_sync(&priv->reset_task);\r\nstop_gfar(dev);\r\nphy_disconnect(dev->phydev);\r\ngfar_free_irq(priv);\r\nreturn 0;\r\n}\r\nstatic int gfar_set_mac_address(struct net_device *dev)\r\n{\r\ngfar_set_mac_for_addr(dev, 0, dev->dev_addr);\r\nreturn 0;\r\n}\r\nstatic int gfar_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nwhile (test_and_set_bit_lock(GFAR_RESETTING, &priv->state))\r\ncpu_relax();\r\nif (dev->flags & IFF_UP)\r\nstop_gfar(dev);\r\ndev->mtu = new_mtu;\r\nif (dev->flags & IFF_UP)\r\nstartup_gfar(dev);\r\nclear_bit_unlock(GFAR_RESETTING, &priv->state);\r\nreturn 0;\r\n}\r\nvoid reset_gfar(struct net_device *ndev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(ndev);\r\nwhile (test_and_set_bit_lock(GFAR_RESETTING, &priv->state))\r\ncpu_relax();\r\nstop_gfar(ndev);\r\nstartup_gfar(ndev);\r\nclear_bit_unlock(GFAR_RESETTING, &priv->state);\r\n}\r\nstatic void gfar_reset_task(struct work_struct *work)\r\n{\r\nstruct gfar_private *priv = container_of(work, struct gfar_private,\r\nreset_task);\r\nreset_gfar(priv->ndev);\r\n}\r\nstatic void gfar_timeout(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\ndev->stats.tx_errors++;\r\nschedule_work(&priv->reset_task);\r\n}\r\nstatic void gfar_clean_tx_ring(struct gfar_priv_tx_q *tx_queue)\r\n{\r\nstruct net_device *dev = tx_queue->dev;\r\nstruct netdev_queue *txq;\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nstruct txbd8 *bdp, *next = NULL;\r\nstruct txbd8 *lbdp = NULL;\r\nstruct txbd8 *base = tx_queue->tx_bd_base;\r\nstruct sk_buff *skb;\r\nint skb_dirtytx;\r\nint tx_ring_size = tx_queue->tx_ring_size;\r\nint frags = 0, nr_txbds = 0;\r\nint i;\r\nint howmany = 0;\r\nint tqi = tx_queue->qindex;\r\nunsigned int bytes_sent = 0;\r\nu32 lstatus;\r\nsize_t buflen;\r\ntxq = netdev_get_tx_queue(dev, tqi);\r\nbdp = tx_queue->dirty_tx;\r\nskb_dirtytx = tx_queue->skb_dirtytx;\r\nwhile ((skb = tx_queue->tx_skbuff[skb_dirtytx])) {\r\nfrags = skb_shinfo(skb)->nr_frags;\r\nif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS))\r\nnr_txbds = frags + 2;\r\nelse\r\nnr_txbds = frags + 1;\r\nlbdp = skip_txbd(bdp, nr_txbds - 1, base, tx_ring_size);\r\nlstatus = be32_to_cpu(lbdp->lstatus);\r\nif ((lstatus & BD_LFLAG(TXBD_READY)) &&\r\n(lstatus & BD_LENGTH_MASK))\r\nbreak;\r\nif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS)) {\r\nnext = next_txbd(bdp, base, tx_ring_size);\r\nbuflen = be16_to_cpu(next->length) +\r\nGMAC_FCB_LEN + GMAC_TXPAL_LEN;\r\n} else\r\nbuflen = be16_to_cpu(bdp->length);\r\ndma_unmap_single(priv->dev, be32_to_cpu(bdp->bufPtr),\r\nbuflen, DMA_TO_DEVICE);\r\nif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS)) {\r\nstruct skb_shared_hwtstamps shhwtstamps;\r\nu64 *ns = (u64 *)(((uintptr_t)skb->data + 0x10) &\r\n~0x7UL);\r\nmemset(&shhwtstamps, 0, sizeof(shhwtstamps));\r\nshhwtstamps.hwtstamp = ns_to_ktime(be64_to_cpu(*ns));\r\nskb_pull(skb, GMAC_FCB_LEN + GMAC_TXPAL_LEN);\r\nskb_tstamp_tx(skb, &shhwtstamps);\r\ngfar_clear_txbd_status(bdp);\r\nbdp = next;\r\n}\r\ngfar_clear_txbd_status(bdp);\r\nbdp = next_txbd(bdp, base, tx_ring_size);\r\nfor (i = 0; i < frags; i++) {\r\ndma_unmap_page(priv->dev, be32_to_cpu(bdp->bufPtr),\r\nbe16_to_cpu(bdp->length),\r\nDMA_TO_DEVICE);\r\ngfar_clear_txbd_status(bdp);\r\nbdp = next_txbd(bdp, base, tx_ring_size);\r\n}\r\nbytes_sent += GFAR_CB(skb)->bytes_sent;\r\ndev_kfree_skb_any(skb);\r\ntx_queue->tx_skbuff[skb_dirtytx] = NULL;\r\nskb_dirtytx = (skb_dirtytx + 1) &\r\nTX_RING_MOD_MASK(tx_ring_size);\r\nhowmany++;\r\nspin_lock(&tx_queue->txlock);\r\ntx_queue->num_txbdfree += nr_txbds;\r\nspin_unlock(&tx_queue->txlock);\r\n}\r\nif (tx_queue->num_txbdfree &&\r\nnetif_tx_queue_stopped(txq) &&\r\n!(test_bit(GFAR_DOWN, &priv->state)))\r\nnetif_wake_subqueue(priv->ndev, tqi);\r\ntx_queue->skb_dirtytx = skb_dirtytx;\r\ntx_queue->dirty_tx = bdp;\r\nnetdev_tx_completed_queue(txq, howmany, bytes_sent);\r\n}\r\nstatic bool gfar_new_page(struct gfar_priv_rx_q *rxq, struct gfar_rx_buff *rxb)\r\n{\r\nstruct page *page;\r\ndma_addr_t addr;\r\npage = dev_alloc_page();\r\nif (unlikely(!page))\r\nreturn false;\r\naddr = dma_map_page(rxq->dev, page, 0, PAGE_SIZE, DMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(rxq->dev, addr))) {\r\n__free_page(page);\r\nreturn false;\r\n}\r\nrxb->dma = addr;\r\nrxb->page = page;\r\nrxb->page_offset = 0;\r\nreturn true;\r\n}\r\nstatic void gfar_rx_alloc_err(struct gfar_priv_rx_q *rx_queue)\r\n{\r\nstruct gfar_private *priv = netdev_priv(rx_queue->ndev);\r\nstruct gfar_extra_stats *estats = &priv->extra_stats;\r\nnetdev_err(rx_queue->ndev, "Can't alloc RX buffers\n");\r\natomic64_inc(&estats->rx_alloc_err);\r\n}\r\nstatic void gfar_alloc_rx_buffs(struct gfar_priv_rx_q *rx_queue,\r\nint alloc_cnt)\r\n{\r\nstruct rxbd8 *bdp;\r\nstruct gfar_rx_buff *rxb;\r\nint i;\r\ni = rx_queue->next_to_use;\r\nbdp = &rx_queue->rx_bd_base[i];\r\nrxb = &rx_queue->rx_buff[i];\r\nwhile (alloc_cnt--) {\r\nif (unlikely(!rxb->page)) {\r\nif (unlikely(!gfar_new_page(rx_queue, rxb))) {\r\ngfar_rx_alloc_err(rx_queue);\r\nbreak;\r\n}\r\n}\r\ngfar_init_rxbdp(rx_queue, bdp,\r\nrxb->dma + rxb->page_offset + RXBUF_ALIGNMENT);\r\nbdp++;\r\nrxb++;\r\nif (unlikely(++i == rx_queue->rx_ring_size)) {\r\ni = 0;\r\nbdp = rx_queue->rx_bd_base;\r\nrxb = rx_queue->rx_buff;\r\n}\r\n}\r\nrx_queue->next_to_use = i;\r\nrx_queue->next_to_alloc = i;\r\n}\r\nstatic void count_errors(u32 lstatus, struct net_device *ndev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(ndev);\r\nstruct net_device_stats *stats = &ndev->stats;\r\nstruct gfar_extra_stats *estats = &priv->extra_stats;\r\nif (lstatus & BD_LFLAG(RXBD_TRUNCATED)) {\r\nstats->rx_length_errors++;\r\natomic64_inc(&estats->rx_trunc);\r\nreturn;\r\n}\r\nif (lstatus & BD_LFLAG(RXBD_LARGE | RXBD_SHORT)) {\r\nstats->rx_length_errors++;\r\nif (lstatus & BD_LFLAG(RXBD_LARGE))\r\natomic64_inc(&estats->rx_large);\r\nelse\r\natomic64_inc(&estats->rx_short);\r\n}\r\nif (lstatus & BD_LFLAG(RXBD_NONOCTET)) {\r\nstats->rx_frame_errors++;\r\natomic64_inc(&estats->rx_nonoctet);\r\n}\r\nif (lstatus & BD_LFLAG(RXBD_CRCERR)) {\r\natomic64_inc(&estats->rx_crcerr);\r\nstats->rx_crc_errors++;\r\n}\r\nif (lstatus & BD_LFLAG(RXBD_OVERRUN)) {\r\natomic64_inc(&estats->rx_overrun);\r\nstats->rx_over_errors++;\r\n}\r\n}\r\nirqreturn_t gfar_receive(int irq, void *grp_id)\r\n{\r\nstruct gfar_priv_grp *grp = (struct gfar_priv_grp *)grp_id;\r\nunsigned long flags;\r\nu32 imask, ievent;\r\nievent = gfar_read(&grp->regs->ievent);\r\nif (unlikely(ievent & IEVENT_FGPI)) {\r\ngfar_write(&grp->regs->ievent, IEVENT_FGPI);\r\nreturn IRQ_HANDLED;\r\n}\r\nif (likely(napi_schedule_prep(&grp->napi_rx))) {\r\nspin_lock_irqsave(&grp->grplock, flags);\r\nimask = gfar_read(&grp->regs->imask);\r\nimask &= IMASK_RX_DISABLED;\r\ngfar_write(&grp->regs->imask, imask);\r\nspin_unlock_irqrestore(&grp->grplock, flags);\r\n__napi_schedule(&grp->napi_rx);\r\n} else {\r\ngfar_write(&grp->regs->ievent, IEVENT_RX_MASK);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t gfar_transmit(int irq, void *grp_id)\r\n{\r\nstruct gfar_priv_grp *grp = (struct gfar_priv_grp *)grp_id;\r\nunsigned long flags;\r\nu32 imask;\r\nif (likely(napi_schedule_prep(&grp->napi_tx))) {\r\nspin_lock_irqsave(&grp->grplock, flags);\r\nimask = gfar_read(&grp->regs->imask);\r\nimask &= IMASK_TX_DISABLED;\r\ngfar_write(&grp->regs->imask, imask);\r\nspin_unlock_irqrestore(&grp->grplock, flags);\r\n__napi_schedule(&grp->napi_tx);\r\n} else {\r\ngfar_write(&grp->regs->ievent, IEVENT_TX_MASK);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic bool gfar_add_rx_frag(struct gfar_rx_buff *rxb, u32 lstatus,\r\nstruct sk_buff *skb, bool first)\r\n{\r\nunsigned int size = lstatus & BD_LENGTH_MASK;\r\nstruct page *page = rxb->page;\r\nbool last = !!(lstatus & BD_LFLAG(RXBD_LAST));\r\nif (last)\r\nsize -= ETH_FCS_LEN;\r\nif (likely(first)) {\r\nskb_put(skb, size);\r\n} else {\r\nif (last)\r\nsize -= skb->len;\r\nif (size > 0)\r\nskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page,\r\nrxb->page_offset + RXBUF_ALIGNMENT,\r\nsize, GFAR_RXB_TRUESIZE);\r\n}\r\nif (unlikely(page_count(page) != 1 || page_is_pfmemalloc(page)))\r\nreturn false;\r\nrxb->page_offset ^= GFAR_RXB_TRUESIZE;\r\npage_ref_inc(page);\r\nreturn true;\r\n}\r\nstatic void gfar_reuse_rx_page(struct gfar_priv_rx_q *rxq,\r\nstruct gfar_rx_buff *old_rxb)\r\n{\r\nstruct gfar_rx_buff *new_rxb;\r\nu16 nta = rxq->next_to_alloc;\r\nnew_rxb = &rxq->rx_buff[nta];\r\nnta++;\r\nrxq->next_to_alloc = (nta < rxq->rx_ring_size) ? nta : 0;\r\n*new_rxb = *old_rxb;\r\ndma_sync_single_range_for_device(rxq->dev, old_rxb->dma,\r\nold_rxb->page_offset,\r\nGFAR_RXB_TRUESIZE, DMA_FROM_DEVICE);\r\n}\r\nstatic struct sk_buff *gfar_get_next_rxbuff(struct gfar_priv_rx_q *rx_queue,\r\nu32 lstatus, struct sk_buff *skb)\r\n{\r\nstruct gfar_rx_buff *rxb = &rx_queue->rx_buff[rx_queue->next_to_clean];\r\nstruct page *page = rxb->page;\r\nbool first = false;\r\nif (likely(!skb)) {\r\nvoid *buff_addr = page_address(page) + rxb->page_offset;\r\nskb = build_skb(buff_addr, GFAR_SKBFRAG_SIZE);\r\nif (unlikely(!skb)) {\r\ngfar_rx_alloc_err(rx_queue);\r\nreturn NULL;\r\n}\r\nskb_reserve(skb, RXBUF_ALIGNMENT);\r\nfirst = true;\r\n}\r\ndma_sync_single_range_for_cpu(rx_queue->dev, rxb->dma, rxb->page_offset,\r\nGFAR_RXB_TRUESIZE, DMA_FROM_DEVICE);\r\nif (gfar_add_rx_frag(rxb, lstatus, skb, first)) {\r\ngfar_reuse_rx_page(rx_queue, rxb);\r\n} else {\r\ndma_unmap_page(rx_queue->dev, rxb->dma,\r\nPAGE_SIZE, DMA_FROM_DEVICE);\r\n}\r\nrxb->page = NULL;\r\nreturn skb;\r\n}\r\nstatic inline void gfar_rx_checksum(struct sk_buff *skb, struct rxfcb *fcb)\r\n{\r\nif ((be16_to_cpu(fcb->flags) & RXFCB_CSUM_MASK) ==\r\n(RXFCB_CIP | RXFCB_CTU))\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nelse\r\nskb_checksum_none_assert(skb);\r\n}\r\nstatic void gfar_process_frame(struct net_device *ndev, struct sk_buff *skb)\r\n{\r\nstruct gfar_private *priv = netdev_priv(ndev);\r\nstruct rxfcb *fcb = NULL;\r\nfcb = (struct rxfcb *)skb->data;\r\nif (priv->uses_rxfcb)\r\nskb_pull(skb, GMAC_FCB_LEN);\r\nif (priv->hwts_rx_en) {\r\nstruct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);\r\nu64 *ns = (u64 *) skb->data;\r\nmemset(shhwtstamps, 0, sizeof(*shhwtstamps));\r\nshhwtstamps->hwtstamp = ns_to_ktime(be64_to_cpu(*ns));\r\n}\r\nif (priv->padding)\r\nskb_pull(skb, priv->padding);\r\nif (ndev->features & NETIF_F_RXCSUM)\r\ngfar_rx_checksum(skb, fcb);\r\nskb->protocol = eth_type_trans(skb, ndev);\r\nif (ndev->features & NETIF_F_HW_VLAN_CTAG_RX &&\r\nbe16_to_cpu(fcb->flags) & RXFCB_VLN)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\r\nbe16_to_cpu(fcb->vlctl));\r\n}\r\nint gfar_clean_rx_ring(struct gfar_priv_rx_q *rx_queue, int rx_work_limit)\r\n{\r\nstruct net_device *ndev = rx_queue->ndev;\r\nstruct gfar_private *priv = netdev_priv(ndev);\r\nstruct rxbd8 *bdp;\r\nint i, howmany = 0;\r\nstruct sk_buff *skb = rx_queue->skb;\r\nint cleaned_cnt = gfar_rxbd_unused(rx_queue);\r\nunsigned int total_bytes = 0, total_pkts = 0;\r\ni = rx_queue->next_to_clean;\r\nwhile (rx_work_limit--) {\r\nu32 lstatus;\r\nif (cleaned_cnt >= GFAR_RX_BUFF_ALLOC) {\r\ngfar_alloc_rx_buffs(rx_queue, cleaned_cnt);\r\ncleaned_cnt = 0;\r\n}\r\nbdp = &rx_queue->rx_bd_base[i];\r\nlstatus = be32_to_cpu(bdp->lstatus);\r\nif (lstatus & BD_LFLAG(RXBD_EMPTY))\r\nbreak;\r\nrmb();\r\nskb = gfar_get_next_rxbuff(rx_queue, lstatus, skb);\r\nif (unlikely(!skb))\r\nbreak;\r\ncleaned_cnt++;\r\nhowmany++;\r\nif (unlikely(++i == rx_queue->rx_ring_size))\r\ni = 0;\r\nrx_queue->next_to_clean = i;\r\nif (!(lstatus & BD_LFLAG(RXBD_LAST)))\r\ncontinue;\r\nif (unlikely(lstatus & BD_LFLAG(RXBD_ERR))) {\r\ncount_errors(lstatus, ndev);\r\ndev_kfree_skb(skb);\r\nskb = NULL;\r\nrx_queue->stats.rx_dropped++;\r\ncontinue;\r\n}\r\ntotal_pkts++;\r\ntotal_bytes += skb->len;\r\nskb_record_rx_queue(skb, rx_queue->qindex);\r\ngfar_process_frame(ndev, skb);\r\nnapi_gro_receive(&rx_queue->grp->napi_rx, skb);\r\nskb = NULL;\r\n}\r\nrx_queue->skb = skb;\r\nrx_queue->stats.rx_packets += total_pkts;\r\nrx_queue->stats.rx_bytes += total_bytes;\r\nif (cleaned_cnt)\r\ngfar_alloc_rx_buffs(rx_queue, cleaned_cnt);\r\nif (unlikely(priv->tx_actual_en)) {\r\nu32 bdp_dma = gfar_rxbd_dma_lastfree(rx_queue);\r\ngfar_write(rx_queue->rfbptr, bdp_dma);\r\n}\r\nreturn howmany;\r\n}\r\nstatic int gfar_poll_rx_sq(struct napi_struct *napi, int budget)\r\n{\r\nstruct gfar_priv_grp *gfargrp =\r\ncontainer_of(napi, struct gfar_priv_grp, napi_rx);\r\nstruct gfar __iomem *regs = gfargrp->regs;\r\nstruct gfar_priv_rx_q *rx_queue = gfargrp->rx_queue;\r\nint work_done = 0;\r\ngfar_write(&regs->ievent, IEVENT_RX_MASK);\r\nwork_done = gfar_clean_rx_ring(rx_queue, budget);\r\nif (work_done < budget) {\r\nu32 imask;\r\nnapi_complete_done(napi, work_done);\r\ngfar_write(&regs->rstat, gfargrp->rstat);\r\nspin_lock_irq(&gfargrp->grplock);\r\nimask = gfar_read(&regs->imask);\r\nimask |= IMASK_RX_DEFAULT;\r\ngfar_write(&regs->imask, imask);\r\nspin_unlock_irq(&gfargrp->grplock);\r\n}\r\nreturn work_done;\r\n}\r\nstatic int gfar_poll_tx_sq(struct napi_struct *napi, int budget)\r\n{\r\nstruct gfar_priv_grp *gfargrp =\r\ncontainer_of(napi, struct gfar_priv_grp, napi_tx);\r\nstruct gfar __iomem *regs = gfargrp->regs;\r\nstruct gfar_priv_tx_q *tx_queue = gfargrp->tx_queue;\r\nu32 imask;\r\ngfar_write(&regs->ievent, IEVENT_TX_MASK);\r\nif (tx_queue->tx_skbuff[tx_queue->skb_dirtytx])\r\ngfar_clean_tx_ring(tx_queue);\r\nnapi_complete(napi);\r\nspin_lock_irq(&gfargrp->grplock);\r\nimask = gfar_read(&regs->imask);\r\nimask |= IMASK_TX_DEFAULT;\r\ngfar_write(&regs->imask, imask);\r\nspin_unlock_irq(&gfargrp->grplock);\r\nreturn 0;\r\n}\r\nstatic int gfar_poll_rx(struct napi_struct *napi, int budget)\r\n{\r\nstruct gfar_priv_grp *gfargrp =\r\ncontainer_of(napi, struct gfar_priv_grp, napi_rx);\r\nstruct gfar_private *priv = gfargrp->priv;\r\nstruct gfar __iomem *regs = gfargrp->regs;\r\nstruct gfar_priv_rx_q *rx_queue = NULL;\r\nint work_done = 0, work_done_per_q = 0;\r\nint i, budget_per_q = 0;\r\nunsigned long rstat_rxf;\r\nint num_act_queues;\r\ngfar_write(&regs->ievent, IEVENT_RX_MASK);\r\nrstat_rxf = gfar_read(&regs->rstat) & RSTAT_RXF_MASK;\r\nnum_act_queues = bitmap_weight(&rstat_rxf, MAX_RX_QS);\r\nif (num_act_queues)\r\nbudget_per_q = budget/num_act_queues;\r\nfor_each_set_bit(i, &gfargrp->rx_bit_map, priv->num_rx_queues) {\r\nif (!(rstat_rxf & (RSTAT_CLEAR_RXF0 >> i)))\r\ncontinue;\r\nrx_queue = priv->rx_queue[i];\r\nwork_done_per_q =\r\ngfar_clean_rx_ring(rx_queue, budget_per_q);\r\nwork_done += work_done_per_q;\r\nif (work_done_per_q < budget_per_q) {\r\ngfar_write(&regs->rstat,\r\nRSTAT_CLEAR_RXF0 >> i);\r\nnum_act_queues--;\r\nif (!num_act_queues)\r\nbreak;\r\n}\r\n}\r\nif (!num_act_queues) {\r\nu32 imask;\r\nnapi_complete_done(napi, work_done);\r\ngfar_write(&regs->rstat, gfargrp->rstat);\r\nspin_lock_irq(&gfargrp->grplock);\r\nimask = gfar_read(&regs->imask);\r\nimask |= IMASK_RX_DEFAULT;\r\ngfar_write(&regs->imask, imask);\r\nspin_unlock_irq(&gfargrp->grplock);\r\n}\r\nreturn work_done;\r\n}\r\nstatic int gfar_poll_tx(struct napi_struct *napi, int budget)\r\n{\r\nstruct gfar_priv_grp *gfargrp =\r\ncontainer_of(napi, struct gfar_priv_grp, napi_tx);\r\nstruct gfar_private *priv = gfargrp->priv;\r\nstruct gfar __iomem *regs = gfargrp->regs;\r\nstruct gfar_priv_tx_q *tx_queue = NULL;\r\nint has_tx_work = 0;\r\nint i;\r\ngfar_write(&regs->ievent, IEVENT_TX_MASK);\r\nfor_each_set_bit(i, &gfargrp->tx_bit_map, priv->num_tx_queues) {\r\ntx_queue = priv->tx_queue[i];\r\nif (tx_queue->tx_skbuff[tx_queue->skb_dirtytx]) {\r\ngfar_clean_tx_ring(tx_queue);\r\nhas_tx_work = 1;\r\n}\r\n}\r\nif (!has_tx_work) {\r\nu32 imask;\r\nnapi_complete(napi);\r\nspin_lock_irq(&gfargrp->grplock);\r\nimask = gfar_read(&regs->imask);\r\nimask |= IMASK_TX_DEFAULT;\r\ngfar_write(&regs->imask, imask);\r\nspin_unlock_irq(&gfargrp->grplock);\r\n}\r\nreturn 0;\r\n}\r\nstatic void gfar_netpoll(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nint i;\r\nif (priv->device_flags & FSL_GIANFAR_DEV_HAS_MULTI_INTR) {\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nstruct gfar_priv_grp *grp = &priv->gfargrp[i];\r\ndisable_irq(gfar_irq(grp, TX)->irq);\r\ndisable_irq(gfar_irq(grp, RX)->irq);\r\ndisable_irq(gfar_irq(grp, ER)->irq);\r\ngfar_interrupt(gfar_irq(grp, TX)->irq, grp);\r\nenable_irq(gfar_irq(grp, ER)->irq);\r\nenable_irq(gfar_irq(grp, RX)->irq);\r\nenable_irq(gfar_irq(grp, TX)->irq);\r\n}\r\n} else {\r\nfor (i = 0; i < priv->num_grps; i++) {\r\nstruct gfar_priv_grp *grp = &priv->gfargrp[i];\r\ndisable_irq(gfar_irq(grp, TX)->irq);\r\ngfar_interrupt(gfar_irq(grp, TX)->irq, grp);\r\nenable_irq(gfar_irq(grp, TX)->irq);\r\n}\r\n}\r\n}\r\nstatic irqreturn_t gfar_interrupt(int irq, void *grp_id)\r\n{\r\nstruct gfar_priv_grp *gfargrp = grp_id;\r\nu32 events = gfar_read(&gfargrp->regs->ievent);\r\nif (events & IEVENT_RX_MASK)\r\ngfar_receive(irq, grp_id);\r\nif (events & IEVENT_TX_MASK)\r\ngfar_transmit(irq, grp_id);\r\nif (events & IEVENT_ERR_MASK)\r\ngfar_error(irq, grp_id);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void adjust_link(struct net_device *dev)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nstruct phy_device *phydev = dev->phydev;\r\nif (unlikely(phydev->link != priv->oldlink ||\r\n(phydev->link && (phydev->duplex != priv->oldduplex ||\r\nphydev->speed != priv->oldspeed))))\r\ngfar_update_link_state(priv);\r\n}\r\nstatic void gfar_set_multi(struct net_device *dev)\r\n{\r\nstruct netdev_hw_addr *ha;\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\nif (dev->flags & IFF_PROMISC) {\r\ntempval = gfar_read(&regs->rctrl);\r\ntempval |= RCTRL_PROM;\r\ngfar_write(&regs->rctrl, tempval);\r\n} else {\r\ntempval = gfar_read(&regs->rctrl);\r\ntempval &= ~(RCTRL_PROM);\r\ngfar_write(&regs->rctrl, tempval);\r\n}\r\nif (dev->flags & IFF_ALLMULTI) {\r\ngfar_write(&regs->igaddr0, 0xffffffff);\r\ngfar_write(&regs->igaddr1, 0xffffffff);\r\ngfar_write(&regs->igaddr2, 0xffffffff);\r\ngfar_write(&regs->igaddr3, 0xffffffff);\r\ngfar_write(&regs->igaddr4, 0xffffffff);\r\ngfar_write(&regs->igaddr5, 0xffffffff);\r\ngfar_write(&regs->igaddr6, 0xffffffff);\r\ngfar_write(&regs->igaddr7, 0xffffffff);\r\ngfar_write(&regs->gaddr0, 0xffffffff);\r\ngfar_write(&regs->gaddr1, 0xffffffff);\r\ngfar_write(&regs->gaddr2, 0xffffffff);\r\ngfar_write(&regs->gaddr3, 0xffffffff);\r\ngfar_write(&regs->gaddr4, 0xffffffff);\r\ngfar_write(&regs->gaddr5, 0xffffffff);\r\ngfar_write(&regs->gaddr6, 0xffffffff);\r\ngfar_write(&regs->gaddr7, 0xffffffff);\r\n} else {\r\nint em_num;\r\nint idx;\r\ngfar_write(&regs->igaddr0, 0x0);\r\ngfar_write(&regs->igaddr1, 0x0);\r\ngfar_write(&regs->igaddr2, 0x0);\r\ngfar_write(&regs->igaddr3, 0x0);\r\ngfar_write(&regs->igaddr4, 0x0);\r\ngfar_write(&regs->igaddr5, 0x0);\r\ngfar_write(&regs->igaddr6, 0x0);\r\ngfar_write(&regs->igaddr7, 0x0);\r\ngfar_write(&regs->gaddr0, 0x0);\r\ngfar_write(&regs->gaddr1, 0x0);\r\ngfar_write(&regs->gaddr2, 0x0);\r\ngfar_write(&regs->gaddr3, 0x0);\r\ngfar_write(&regs->gaddr4, 0x0);\r\ngfar_write(&regs->gaddr5, 0x0);\r\ngfar_write(&regs->gaddr6, 0x0);\r\ngfar_write(&regs->gaddr7, 0x0);\r\nif (priv->extended_hash) {\r\nem_num = GFAR_EM_NUM + 1;\r\ngfar_clear_exact_match(dev);\r\nidx = 1;\r\n} else {\r\nidx = 0;\r\nem_num = 0;\r\n}\r\nif (netdev_mc_empty(dev))\r\nreturn;\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nif (idx < em_num) {\r\ngfar_set_mac_for_addr(dev, idx, ha->addr);\r\nidx++;\r\n} else\r\ngfar_set_hash_for_addr(dev, ha->addr);\r\n}\r\n}\r\n}\r\nstatic void gfar_clear_exact_match(struct net_device *dev)\r\n{\r\nint idx;\r\nstatic const u8 zero_arr[ETH_ALEN] = {0, 0, 0, 0, 0, 0};\r\nfor (idx = 1; idx < GFAR_EM_NUM + 1; idx++)\r\ngfar_set_mac_for_addr(dev, idx, zero_arr);\r\n}\r\nstatic void gfar_set_hash_for_addr(struct net_device *dev, u8 *addr)\r\n{\r\nu32 tempval;\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nu32 result = ether_crc(ETH_ALEN, addr);\r\nint width = priv->hash_width;\r\nu8 whichbit = (result >> (32 - width)) & 0x1f;\r\nu8 whichreg = result >> (32 - width + 5);\r\nu32 value = (1 << (31-whichbit));\r\ntempval = gfar_read(priv->hash_regs[whichreg]);\r\ntempval |= value;\r\ngfar_write(priv->hash_regs[whichreg], tempval);\r\n}\r\nstatic void gfar_set_mac_for_addr(struct net_device *dev, int num,\r\nconst u8 *addr)\r\n{\r\nstruct gfar_private *priv = netdev_priv(dev);\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nu32 tempval;\r\nu32 __iomem *macptr = &regs->macstnaddr1;\r\nmacptr += num*2;\r\ntempval = (addr[5] << 24) | (addr[4] << 16) |\r\n(addr[3] << 8) | addr[2];\r\ngfar_write(macptr, tempval);\r\ntempval = (addr[1] << 24) | (addr[0] << 16);\r\ngfar_write(macptr+1, tempval);\r\n}\r\nstatic irqreturn_t gfar_error(int irq, void *grp_id)\r\n{\r\nstruct gfar_priv_grp *gfargrp = grp_id;\r\nstruct gfar __iomem *regs = gfargrp->regs;\r\nstruct gfar_private *priv= gfargrp->priv;\r\nstruct net_device *dev = priv->ndev;\r\nu32 events = gfar_read(&regs->ievent);\r\ngfar_write(&regs->ievent, events & IEVENT_ERR_MASK);\r\nif ((priv->device_flags & FSL_GIANFAR_DEV_HAS_MAGIC_PACKET) &&\r\n(events & IEVENT_MAG))\r\nevents &= ~IEVENT_MAG;\r\nif (netif_msg_rx_err(priv) || netif_msg_tx_err(priv))\r\nnetdev_dbg(dev,\r\n"error interrupt (ievent=0x%08x imask=0x%08x)\n",\r\nevents, gfar_read(&regs->imask));\r\nif (events & IEVENT_TXE) {\r\ndev->stats.tx_errors++;\r\nif (events & IEVENT_LC)\r\ndev->stats.tx_window_errors++;\r\nif (events & IEVENT_CRL)\r\ndev->stats.tx_aborted_errors++;\r\nif (events & IEVENT_XFUN) {\r\nnetif_dbg(priv, tx_err, dev,\r\n"TX FIFO underrun, packet dropped\n");\r\ndev->stats.tx_dropped++;\r\natomic64_inc(&priv->extra_stats.tx_underrun);\r\nschedule_work(&priv->reset_task);\r\n}\r\nnetif_dbg(priv, tx_err, dev, "Transmit Error\n");\r\n}\r\nif (events & IEVENT_BSY) {\r\ndev->stats.rx_over_errors++;\r\natomic64_inc(&priv->extra_stats.rx_bsy);\r\nnetif_dbg(priv, rx_err, dev, "busy error (rstat: %x)\n",\r\ngfar_read(&regs->rstat));\r\n}\r\nif (events & IEVENT_BABR) {\r\ndev->stats.rx_errors++;\r\natomic64_inc(&priv->extra_stats.rx_babr);\r\nnetif_dbg(priv, rx_err, dev, "babbling RX error\n");\r\n}\r\nif (events & IEVENT_EBERR) {\r\natomic64_inc(&priv->extra_stats.eberr);\r\nnetif_dbg(priv, rx_err, dev, "bus error\n");\r\n}\r\nif (events & IEVENT_RXC)\r\nnetif_dbg(priv, rx_status, dev, "control frame\n");\r\nif (events & IEVENT_BABT) {\r\natomic64_inc(&priv->extra_stats.tx_babt);\r\nnetif_dbg(priv, tx_err, dev, "babbling TX error\n");\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic u32 gfar_get_flowctrl_cfg(struct gfar_private *priv)\r\n{\r\nstruct net_device *ndev = priv->ndev;\r\nstruct phy_device *phydev = ndev->phydev;\r\nu32 val = 0;\r\nif (!phydev->duplex)\r\nreturn val;\r\nif (!priv->pause_aneg_en) {\r\nif (priv->tx_pause_en)\r\nval |= MACCFG1_TX_FLOW;\r\nif (priv->rx_pause_en)\r\nval |= MACCFG1_RX_FLOW;\r\n} else {\r\nu16 lcl_adv, rmt_adv;\r\nu8 flowctrl;\r\nrmt_adv = 0;\r\nif (phydev->pause)\r\nrmt_adv = LPA_PAUSE_CAP;\r\nif (phydev->asym_pause)\r\nrmt_adv |= LPA_PAUSE_ASYM;\r\nlcl_adv = 0;\r\nif (phydev->advertising & ADVERTISED_Pause)\r\nlcl_adv |= ADVERTISE_PAUSE_CAP;\r\nif (phydev->advertising & ADVERTISED_Asym_Pause)\r\nlcl_adv |= ADVERTISE_PAUSE_ASYM;\r\nflowctrl = mii_resolve_flowctrl_fdx(lcl_adv, rmt_adv);\r\nif (flowctrl & FLOW_CTRL_TX)\r\nval |= MACCFG1_TX_FLOW;\r\nif (flowctrl & FLOW_CTRL_RX)\r\nval |= MACCFG1_RX_FLOW;\r\n}\r\nreturn val;\r\n}\r\nstatic noinline void gfar_update_link_state(struct gfar_private *priv)\r\n{\r\nstruct gfar __iomem *regs = priv->gfargrp[0].regs;\r\nstruct net_device *ndev = priv->ndev;\r\nstruct phy_device *phydev = ndev->phydev;\r\nstruct gfar_priv_rx_q *rx_queue = NULL;\r\nint i;\r\nif (unlikely(test_bit(GFAR_RESETTING, &priv->state)))\r\nreturn;\r\nif (phydev->link) {\r\nu32 tempval1 = gfar_read(&regs->maccfg1);\r\nu32 tempval = gfar_read(&regs->maccfg2);\r\nu32 ecntrl = gfar_read(&regs->ecntrl);\r\nu32 tx_flow_oldval = (tempval & MACCFG1_TX_FLOW);\r\nif (phydev->duplex != priv->oldduplex) {\r\nif (!(phydev->duplex))\r\ntempval &= ~(MACCFG2_FULL_DUPLEX);\r\nelse\r\ntempval |= MACCFG2_FULL_DUPLEX;\r\npriv->oldduplex = phydev->duplex;\r\n}\r\nif (phydev->speed != priv->oldspeed) {\r\nswitch (phydev->speed) {\r\ncase 1000:\r\ntempval =\r\n((tempval & ~(MACCFG2_IF)) | MACCFG2_GMII);\r\necntrl &= ~(ECNTRL_R100);\r\nbreak;\r\ncase 100:\r\ncase 10:\r\ntempval =\r\n((tempval & ~(MACCFG2_IF)) | MACCFG2_MII);\r\nif (phydev->speed == SPEED_100)\r\necntrl |= ECNTRL_R100;\r\nelse\r\necntrl &= ~(ECNTRL_R100);\r\nbreak;\r\ndefault:\r\nnetif_warn(priv, link, priv->ndev,\r\n"Ack! Speed (%d) is not 10/100/1000!\n",\r\nphydev->speed);\r\nbreak;\r\n}\r\npriv->oldspeed = phydev->speed;\r\n}\r\ntempval1 &= ~(MACCFG1_TX_FLOW | MACCFG1_RX_FLOW);\r\ntempval1 |= gfar_get_flowctrl_cfg(priv);\r\nif ((tempval1 & MACCFG1_TX_FLOW) && !tx_flow_oldval) {\r\nfor (i = 0; i < priv->num_rx_queues; i++) {\r\nu32 bdp_dma;\r\nrx_queue = priv->rx_queue[i];\r\nbdp_dma = gfar_rxbd_dma_lastfree(rx_queue);\r\ngfar_write(rx_queue->rfbptr, bdp_dma);\r\n}\r\npriv->tx_actual_en = 1;\r\n}\r\nif (unlikely(!(tempval1 & MACCFG1_TX_FLOW) && tx_flow_oldval))\r\npriv->tx_actual_en = 0;\r\ngfar_write(&regs->maccfg1, tempval1);\r\ngfar_write(&regs->maccfg2, tempval);\r\ngfar_write(&regs->ecntrl, ecntrl);\r\nif (!priv->oldlink)\r\npriv->oldlink = 1;\r\n} else if (priv->oldlink) {\r\npriv->oldlink = 0;\r\npriv->oldspeed = 0;\r\npriv->oldduplex = -1;\r\n}\r\nif (netif_msg_link(priv))\r\nphy_print_status(phydev);\r\n}
