struct scatterlist *sg_next(struct scatterlist *sg)\r\n{\r\n#ifdef CONFIG_DEBUG_SG\r\nBUG_ON(sg->sg_magic != SG_MAGIC);\r\n#endif\r\nif (sg_is_last(sg))\r\nreturn NULL;\r\nsg++;\r\nif (unlikely(sg_is_chain(sg)))\r\nsg = sg_chain_ptr(sg);\r\nreturn sg;\r\n}\r\nint sg_nents(struct scatterlist *sg)\r\n{\r\nint nents;\r\nfor (nents = 0; sg; sg = sg_next(sg))\r\nnents++;\r\nreturn nents;\r\n}\r\nint sg_nents_for_len(struct scatterlist *sg, u64 len)\r\n{\r\nint nents;\r\nu64 total;\r\nif (!len)\r\nreturn 0;\r\nfor (nents = 0, total = 0; sg; sg = sg_next(sg)) {\r\nnents++;\r\ntotal += sg->length;\r\nif (total >= len)\r\nreturn nents;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstruct scatterlist *sg_last(struct scatterlist *sgl, unsigned int nents)\r\n{\r\nstruct scatterlist *sg, *ret = NULL;\r\nunsigned int i;\r\nfor_each_sg(sgl, sg, nents, i)\r\nret = sg;\r\n#ifdef CONFIG_DEBUG_SG\r\nBUG_ON(sgl[0].sg_magic != SG_MAGIC);\r\nBUG_ON(!sg_is_last(ret));\r\n#endif\r\nreturn ret;\r\n}\r\nvoid sg_init_table(struct scatterlist *sgl, unsigned int nents)\r\n{\r\nmemset(sgl, 0, sizeof(*sgl) * nents);\r\n#ifdef CONFIG_DEBUG_SG\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < nents; i++)\r\nsgl[i].sg_magic = SG_MAGIC;\r\n}\r\n#endif\r\nsg_mark_end(&sgl[nents - 1]);\r\n}\r\nvoid sg_init_one(struct scatterlist *sg, const void *buf, unsigned int buflen)\r\n{\r\nsg_init_table(sg, 1);\r\nsg_set_buf(sg, buf, buflen);\r\n}\r\nstatic struct scatterlist *sg_kmalloc(unsigned int nents, gfp_t gfp_mask)\r\n{\r\nif (nents == SG_MAX_SINGLE_ALLOC) {\r\nvoid *ptr = (void *) __get_free_page(gfp_mask);\r\nkmemleak_alloc(ptr, PAGE_SIZE, 1, gfp_mask);\r\nreturn ptr;\r\n} else\r\nreturn kmalloc(nents * sizeof(struct scatterlist), gfp_mask);\r\n}\r\nstatic void sg_kfree(struct scatterlist *sg, unsigned int nents)\r\n{\r\nif (nents == SG_MAX_SINGLE_ALLOC) {\r\nkmemleak_free(sg);\r\nfree_page((unsigned long) sg);\r\n} else\r\nkfree(sg);\r\n}\r\nvoid __sg_free_table(struct sg_table *table, unsigned int max_ents,\r\nbool skip_first_chunk, sg_free_fn *free_fn)\r\n{\r\nstruct scatterlist *sgl, *next;\r\nif (unlikely(!table->sgl))\r\nreturn;\r\nsgl = table->sgl;\r\nwhile (table->orig_nents) {\r\nunsigned int alloc_size = table->orig_nents;\r\nunsigned int sg_size;\r\nif (alloc_size > max_ents) {\r\nnext = sg_chain_ptr(&sgl[max_ents - 1]);\r\nalloc_size = max_ents;\r\nsg_size = alloc_size - 1;\r\n} else {\r\nsg_size = alloc_size;\r\nnext = NULL;\r\n}\r\ntable->orig_nents -= sg_size;\r\nif (skip_first_chunk)\r\nskip_first_chunk = false;\r\nelse\r\nfree_fn(sgl, alloc_size);\r\nsgl = next;\r\n}\r\ntable->sgl = NULL;\r\n}\r\nvoid sg_free_table(struct sg_table *table)\r\n{\r\n__sg_free_table(table, SG_MAX_SINGLE_ALLOC, false, sg_kfree);\r\n}\r\nint __sg_alloc_table(struct sg_table *table, unsigned int nents,\r\nunsigned int max_ents, struct scatterlist *first_chunk,\r\ngfp_t gfp_mask, sg_alloc_fn *alloc_fn)\r\n{\r\nstruct scatterlist *sg, *prv;\r\nunsigned int left;\r\nmemset(table, 0, sizeof(*table));\r\nif (nents == 0)\r\nreturn -EINVAL;\r\n#ifndef CONFIG_ARCH_HAS_SG_CHAIN\r\nif (WARN_ON_ONCE(nents > max_ents))\r\nreturn -EINVAL;\r\n#endif\r\nleft = nents;\r\nprv = NULL;\r\ndo {\r\nunsigned int sg_size, alloc_size = left;\r\nif (alloc_size > max_ents) {\r\nalloc_size = max_ents;\r\nsg_size = alloc_size - 1;\r\n} else\r\nsg_size = alloc_size;\r\nleft -= sg_size;\r\nif (first_chunk) {\r\nsg = first_chunk;\r\nfirst_chunk = NULL;\r\n} else {\r\nsg = alloc_fn(alloc_size, gfp_mask);\r\n}\r\nif (unlikely(!sg)) {\r\nif (prv)\r\ntable->nents = ++table->orig_nents;\r\nreturn -ENOMEM;\r\n}\r\nsg_init_table(sg, alloc_size);\r\ntable->nents = table->orig_nents += sg_size;\r\nif (prv)\r\nsg_chain(prv, max_ents, sg);\r\nelse\r\ntable->sgl = sg;\r\nif (!left)\r\nsg_mark_end(&sg[sg_size - 1]);\r\nprv = sg;\r\n} while (left);\r\nreturn 0;\r\n}\r\nint sg_alloc_table(struct sg_table *table, unsigned int nents, gfp_t gfp_mask)\r\n{\r\nint ret;\r\nret = __sg_alloc_table(table, nents, SG_MAX_SINGLE_ALLOC,\r\nNULL, gfp_mask, sg_kmalloc);\r\nif (unlikely(ret))\r\n__sg_free_table(table, SG_MAX_SINGLE_ALLOC, false, sg_kfree);\r\nreturn ret;\r\n}\r\nint sg_alloc_table_from_pages(struct sg_table *sgt,\r\nstruct page **pages, unsigned int n_pages,\r\nunsigned long offset, unsigned long size,\r\ngfp_t gfp_mask)\r\n{\r\nunsigned int chunks;\r\nunsigned int i;\r\nunsigned int cur_page;\r\nint ret;\r\nstruct scatterlist *s;\r\nchunks = 1;\r\nfor (i = 1; i < n_pages; ++i)\r\nif (page_to_pfn(pages[i]) != page_to_pfn(pages[i - 1]) + 1)\r\n++chunks;\r\nret = sg_alloc_table(sgt, chunks, gfp_mask);\r\nif (unlikely(ret))\r\nreturn ret;\r\ncur_page = 0;\r\nfor_each_sg(sgt->sgl, s, sgt->orig_nents, i) {\r\nunsigned long chunk_size;\r\nunsigned int j;\r\nfor (j = cur_page + 1; j < n_pages; ++j)\r\nif (page_to_pfn(pages[j]) !=\r\npage_to_pfn(pages[j - 1]) + 1)\r\nbreak;\r\nchunk_size = ((j - cur_page) << PAGE_SHIFT) - offset;\r\nsg_set_page(s, pages[cur_page], min(size, chunk_size), offset);\r\nsize -= chunk_size;\r\noffset = 0;\r\ncur_page = j;\r\n}\r\nreturn 0;\r\n}\r\nvoid __sg_page_iter_start(struct sg_page_iter *piter,\r\nstruct scatterlist *sglist, unsigned int nents,\r\nunsigned long pgoffset)\r\n{\r\npiter->__pg_advance = 0;\r\npiter->__nents = nents;\r\npiter->sg = sglist;\r\npiter->sg_pgoffset = pgoffset;\r\n}\r\nstatic int sg_page_count(struct scatterlist *sg)\r\n{\r\nreturn PAGE_ALIGN(sg->offset + sg->length) >> PAGE_SHIFT;\r\n}\r\nbool __sg_page_iter_next(struct sg_page_iter *piter)\r\n{\r\nif (!piter->__nents || !piter->sg)\r\nreturn false;\r\npiter->sg_pgoffset += piter->__pg_advance;\r\npiter->__pg_advance = 1;\r\nwhile (piter->sg_pgoffset >= sg_page_count(piter->sg)) {\r\npiter->sg_pgoffset -= sg_page_count(piter->sg);\r\npiter->sg = sg_next(piter->sg);\r\nif (!--piter->__nents || !piter->sg)\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nvoid sg_miter_start(struct sg_mapping_iter *miter, struct scatterlist *sgl,\r\nunsigned int nents, unsigned int flags)\r\n{\r\nmemset(miter, 0, sizeof(struct sg_mapping_iter));\r\n__sg_page_iter_start(&miter->piter, sgl, nents, 0);\r\nWARN_ON(!(flags & (SG_MITER_TO_SG | SG_MITER_FROM_SG)));\r\nmiter->__flags = flags;\r\n}\r\nstatic bool sg_miter_get_next_page(struct sg_mapping_iter *miter)\r\n{\r\nif (!miter->__remaining) {\r\nstruct scatterlist *sg;\r\nunsigned long pgoffset;\r\nif (!__sg_page_iter_next(&miter->piter))\r\nreturn false;\r\nsg = miter->piter.sg;\r\npgoffset = miter->piter.sg_pgoffset;\r\nmiter->__offset = pgoffset ? 0 : sg->offset;\r\nmiter->__remaining = sg->offset + sg->length -\r\n(pgoffset << PAGE_SHIFT) - miter->__offset;\r\nmiter->__remaining = min_t(unsigned long, miter->__remaining,\r\nPAGE_SIZE - miter->__offset);\r\n}\r\nreturn true;\r\n}\r\nbool sg_miter_skip(struct sg_mapping_iter *miter, off_t offset)\r\n{\r\nsg_miter_stop(miter);\r\nwhile (offset) {\r\noff_t consumed;\r\nif (!sg_miter_get_next_page(miter))\r\nreturn false;\r\nconsumed = min_t(off_t, offset, miter->__remaining);\r\nmiter->__offset += consumed;\r\nmiter->__remaining -= consumed;\r\noffset -= consumed;\r\n}\r\nreturn true;\r\n}\r\nbool sg_miter_next(struct sg_mapping_iter *miter)\r\n{\r\nsg_miter_stop(miter);\r\nif (!sg_miter_get_next_page(miter))\r\nreturn false;\r\nmiter->page = sg_page_iter_page(&miter->piter);\r\nmiter->consumed = miter->length = miter->__remaining;\r\nif (miter->__flags & SG_MITER_ATOMIC)\r\nmiter->addr = kmap_atomic(miter->page) + miter->__offset;\r\nelse\r\nmiter->addr = kmap(miter->page) + miter->__offset;\r\nreturn true;\r\n}\r\nvoid sg_miter_stop(struct sg_mapping_iter *miter)\r\n{\r\nWARN_ON(miter->consumed > miter->length);\r\nif (miter->addr) {\r\nmiter->__offset += miter->consumed;\r\nmiter->__remaining -= miter->consumed;\r\nif ((miter->__flags & SG_MITER_TO_SG) &&\r\n!PageSlab(miter->page))\r\nflush_kernel_dcache_page(miter->page);\r\nif (miter->__flags & SG_MITER_ATOMIC) {\r\nWARN_ON_ONCE(preemptible());\r\nkunmap_atomic(miter->addr);\r\n} else\r\nkunmap(miter->page);\r\nmiter->page = NULL;\r\nmiter->addr = NULL;\r\nmiter->length = 0;\r\nmiter->consumed = 0;\r\n}\r\n}\r\nsize_t sg_copy_buffer(struct scatterlist *sgl, unsigned int nents, void *buf,\r\nsize_t buflen, off_t skip, bool to_buffer)\r\n{\r\nunsigned int offset = 0;\r\nstruct sg_mapping_iter miter;\r\nunsigned int sg_flags = SG_MITER_ATOMIC;\r\nif (to_buffer)\r\nsg_flags |= SG_MITER_FROM_SG;\r\nelse\r\nsg_flags |= SG_MITER_TO_SG;\r\nsg_miter_start(&miter, sgl, nents, sg_flags);\r\nif (!sg_miter_skip(&miter, skip))\r\nreturn false;\r\nwhile ((offset < buflen) && sg_miter_next(&miter)) {\r\nunsigned int len;\r\nlen = min(miter.length, buflen - offset);\r\nif (to_buffer)\r\nmemcpy(buf + offset, miter.addr, len);\r\nelse\r\nmemcpy(miter.addr, buf + offset, len);\r\noffset += len;\r\n}\r\nsg_miter_stop(&miter);\r\nreturn offset;\r\n}\r\nsize_t sg_copy_from_buffer(struct scatterlist *sgl, unsigned int nents,\r\nconst void *buf, size_t buflen)\r\n{\r\nreturn sg_copy_buffer(sgl, nents, (void *)buf, buflen, 0, false);\r\n}\r\nsize_t sg_copy_to_buffer(struct scatterlist *sgl, unsigned int nents,\r\nvoid *buf, size_t buflen)\r\n{\r\nreturn sg_copy_buffer(sgl, nents, buf, buflen, 0, true);\r\n}\r\nsize_t sg_pcopy_from_buffer(struct scatterlist *sgl, unsigned int nents,\r\nconst void *buf, size_t buflen, off_t skip)\r\n{\r\nreturn sg_copy_buffer(sgl, nents, (void *)buf, buflen, skip, false);\r\n}\r\nsize_t sg_pcopy_to_buffer(struct scatterlist *sgl, unsigned int nents,\r\nvoid *buf, size_t buflen, off_t skip)\r\n{\r\nreturn sg_copy_buffer(sgl, nents, buf, buflen, skip, true);\r\n}
