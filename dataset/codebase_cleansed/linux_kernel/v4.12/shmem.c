static unsigned long shmem_default_max_blocks(void)\r\n{\r\nreturn totalram_pages / 2;\r\n}\r\nstatic unsigned long shmem_default_max_inodes(void)\r\n{\r\nreturn min(totalram_pages - totalhigh_pages, totalram_pages / 2);\r\n}\r\nint shmem_getpage(struct inode *inode, pgoff_t index,\r\nstruct page **pagep, enum sgp_type sgp)\r\n{\r\nreturn shmem_getpage_gfp(inode, index, pagep, sgp,\r\nmapping_gfp_mask(inode->i_mapping), NULL, NULL, NULL);\r\n}\r\nstatic inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)\r\n{\r\nreturn sb->s_fs_info;\r\n}\r\nstatic inline int shmem_acct_size(unsigned long flags, loff_t size)\r\n{\r\nreturn (flags & VM_NORESERVE) ?\r\n0 : security_vm_enough_memory_mm(current->mm, VM_ACCT(size));\r\n}\r\nstatic inline void shmem_unacct_size(unsigned long flags, loff_t size)\r\n{\r\nif (!(flags & VM_NORESERVE))\r\nvm_unacct_memory(VM_ACCT(size));\r\n}\r\nstatic inline int shmem_reacct_size(unsigned long flags,\r\nloff_t oldsize, loff_t newsize)\r\n{\r\nif (!(flags & VM_NORESERVE)) {\r\nif (VM_ACCT(newsize) > VM_ACCT(oldsize))\r\nreturn security_vm_enough_memory_mm(current->mm,\r\nVM_ACCT(newsize) - VM_ACCT(oldsize));\r\nelse if (VM_ACCT(newsize) < VM_ACCT(oldsize))\r\nvm_unacct_memory(VM_ACCT(oldsize) - VM_ACCT(newsize));\r\n}\r\nreturn 0;\r\n}\r\nstatic inline int shmem_acct_block(unsigned long flags, long pages)\r\n{\r\nif (!(flags & VM_NORESERVE))\r\nreturn 0;\r\nreturn security_vm_enough_memory_mm(current->mm,\r\npages * VM_ACCT(PAGE_SIZE));\r\n}\r\nstatic inline void shmem_unacct_blocks(unsigned long flags, long pages)\r\n{\r\nif (flags & VM_NORESERVE)\r\nvm_unacct_memory(pages * VM_ACCT(PAGE_SIZE));\r\n}\r\nbool vma_is_shmem(struct vm_area_struct *vma)\r\n{\r\nreturn vma->vm_ops == &shmem_vm_ops;\r\n}\r\nstatic int shmem_reserve_inode(struct super_block *sb)\r\n{\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\r\nif (sbinfo->max_inodes) {\r\nspin_lock(&sbinfo->stat_lock);\r\nif (!sbinfo->free_inodes) {\r\nspin_unlock(&sbinfo->stat_lock);\r\nreturn -ENOSPC;\r\n}\r\nsbinfo->free_inodes--;\r\nspin_unlock(&sbinfo->stat_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic void shmem_free_inode(struct super_block *sb)\r\n{\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\r\nif (sbinfo->max_inodes) {\r\nspin_lock(&sbinfo->stat_lock);\r\nsbinfo->free_inodes++;\r\nspin_unlock(&sbinfo->stat_lock);\r\n}\r\n}\r\nstatic void shmem_recalc_inode(struct inode *inode)\r\n{\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nlong freed;\r\nfreed = info->alloced - info->swapped - inode->i_mapping->nrpages;\r\nif (freed > 0) {\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\r\nif (sbinfo->max_blocks)\r\npercpu_counter_add(&sbinfo->used_blocks, -freed);\r\ninfo->alloced -= freed;\r\ninode->i_blocks -= freed * BLOCKS_PER_PAGE;\r\nshmem_unacct_blocks(info->flags, freed);\r\n}\r\n}\r\nbool shmem_charge(struct inode *inode, long pages)\r\n{\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\r\nunsigned long flags;\r\nif (shmem_acct_block(info->flags, pages))\r\nreturn false;\r\nspin_lock_irqsave(&info->lock, flags);\r\ninfo->alloced += pages;\r\ninode->i_blocks += pages * BLOCKS_PER_PAGE;\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irqrestore(&info->lock, flags);\r\ninode->i_mapping->nrpages += pages;\r\nif (!sbinfo->max_blocks)\r\nreturn true;\r\nif (percpu_counter_compare(&sbinfo->used_blocks,\r\nsbinfo->max_blocks - pages) > 0) {\r\ninode->i_mapping->nrpages -= pages;\r\nspin_lock_irqsave(&info->lock, flags);\r\ninfo->alloced -= pages;\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irqrestore(&info->lock, flags);\r\nshmem_unacct_blocks(info->flags, pages);\r\nreturn false;\r\n}\r\npercpu_counter_add(&sbinfo->used_blocks, pages);\r\nreturn true;\r\n}\r\nvoid shmem_uncharge(struct inode *inode, long pages)\r\n{\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\r\nunsigned long flags;\r\nspin_lock_irqsave(&info->lock, flags);\r\ninfo->alloced -= pages;\r\ninode->i_blocks -= pages * BLOCKS_PER_PAGE;\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irqrestore(&info->lock, flags);\r\nif (sbinfo->max_blocks)\r\npercpu_counter_sub(&sbinfo->used_blocks, pages);\r\nshmem_unacct_blocks(info->flags, pages);\r\n}\r\nstatic int shmem_radix_tree_replace(struct address_space *mapping,\r\npgoff_t index, void *expected, void *replacement)\r\n{\r\nstruct radix_tree_node *node;\r\nvoid **pslot;\r\nvoid *item;\r\nVM_BUG_ON(!expected);\r\nVM_BUG_ON(!replacement);\r\nitem = __radix_tree_lookup(&mapping->page_tree, index, &node, &pslot);\r\nif (!item)\r\nreturn -ENOENT;\r\nif (item != expected)\r\nreturn -ENOENT;\r\n__radix_tree_replace(&mapping->page_tree, node, pslot,\r\nreplacement, NULL, NULL);\r\nreturn 0;\r\n}\r\nstatic bool shmem_confirm_swap(struct address_space *mapping,\r\npgoff_t index, swp_entry_t swap)\r\n{\r\nvoid *item;\r\nrcu_read_lock();\r\nitem = radix_tree_lookup(&mapping->page_tree, index);\r\nrcu_read_unlock();\r\nreturn item == swp_to_radix_entry(swap);\r\n}\r\nstatic int shmem_parse_huge(const char *str)\r\n{\r\nif (!strcmp(str, "never"))\r\nreturn SHMEM_HUGE_NEVER;\r\nif (!strcmp(str, "always"))\r\nreturn SHMEM_HUGE_ALWAYS;\r\nif (!strcmp(str, "within_size"))\r\nreturn SHMEM_HUGE_WITHIN_SIZE;\r\nif (!strcmp(str, "advise"))\r\nreturn SHMEM_HUGE_ADVISE;\r\nif (!strcmp(str, "deny"))\r\nreturn SHMEM_HUGE_DENY;\r\nif (!strcmp(str, "force"))\r\nreturn SHMEM_HUGE_FORCE;\r\nreturn -EINVAL;\r\n}\r\nstatic const char *shmem_format_huge(int huge)\r\n{\r\nswitch (huge) {\r\ncase SHMEM_HUGE_NEVER:\r\nreturn "never";\r\ncase SHMEM_HUGE_ALWAYS:\r\nreturn "always";\r\ncase SHMEM_HUGE_WITHIN_SIZE:\r\nreturn "within_size";\r\ncase SHMEM_HUGE_ADVISE:\r\nreturn "advise";\r\ncase SHMEM_HUGE_DENY:\r\nreturn "deny";\r\ncase SHMEM_HUGE_FORCE:\r\nreturn "force";\r\ndefault:\r\nVM_BUG_ON(1);\r\nreturn "bad_val";\r\n}\r\n}\r\nstatic unsigned long shmem_unused_huge_shrink(struct shmem_sb_info *sbinfo,\r\nstruct shrink_control *sc, unsigned long nr_to_split)\r\n{\r\nLIST_HEAD(list), *pos, *next;\r\nLIST_HEAD(to_remove);\r\nstruct inode *inode;\r\nstruct shmem_inode_info *info;\r\nstruct page *page;\r\nunsigned long batch = sc ? sc->nr_to_scan : 128;\r\nint removed = 0, split = 0;\r\nif (list_empty(&sbinfo->shrinklist))\r\nreturn SHRINK_STOP;\r\nspin_lock(&sbinfo->shrinklist_lock);\r\nlist_for_each_safe(pos, next, &sbinfo->shrinklist) {\r\ninfo = list_entry(pos, struct shmem_inode_info, shrinklist);\r\ninode = igrab(&info->vfs_inode);\r\nif (!inode) {\r\nlist_del_init(&info->shrinklist);\r\nremoved++;\r\ngoto next;\r\n}\r\nif (round_up(inode->i_size, PAGE_SIZE) ==\r\nround_up(inode->i_size, HPAGE_PMD_SIZE)) {\r\nlist_move(&info->shrinklist, &to_remove);\r\nremoved++;\r\ngoto next;\r\n}\r\nlist_move(&info->shrinklist, &list);\r\nnext:\r\nif (!--batch)\r\nbreak;\r\n}\r\nspin_unlock(&sbinfo->shrinklist_lock);\r\nlist_for_each_safe(pos, next, &to_remove) {\r\ninfo = list_entry(pos, struct shmem_inode_info, shrinklist);\r\ninode = &info->vfs_inode;\r\nlist_del_init(&info->shrinklist);\r\niput(inode);\r\n}\r\nlist_for_each_safe(pos, next, &list) {\r\nint ret;\r\ninfo = list_entry(pos, struct shmem_inode_info, shrinklist);\r\ninode = &info->vfs_inode;\r\nif (nr_to_split && split >= nr_to_split) {\r\niput(inode);\r\ncontinue;\r\n}\r\npage = find_lock_page(inode->i_mapping,\r\n(inode->i_size & HPAGE_PMD_MASK) >> PAGE_SHIFT);\r\nif (!page)\r\ngoto drop;\r\nif (!PageTransHuge(page)) {\r\nunlock_page(page);\r\nput_page(page);\r\ngoto drop;\r\n}\r\nret = split_huge_page(page);\r\nunlock_page(page);\r\nput_page(page);\r\nif (ret) {\r\niput(inode);\r\ncontinue;\r\n}\r\nsplit++;\r\ndrop:\r\nlist_del_init(&info->shrinklist);\r\nremoved++;\r\niput(inode);\r\n}\r\nspin_lock(&sbinfo->shrinklist_lock);\r\nlist_splice_tail(&list, &sbinfo->shrinklist);\r\nsbinfo->shrinklist_len -= removed;\r\nspin_unlock(&sbinfo->shrinklist_lock);\r\nreturn split;\r\n}\r\nstatic long shmem_unused_huge_scan(struct super_block *sb,\r\nstruct shrink_control *sc)\r\n{\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\r\nif (!READ_ONCE(sbinfo->shrinklist_len))\r\nreturn SHRINK_STOP;\r\nreturn shmem_unused_huge_shrink(sbinfo, sc, 0);\r\n}\r\nstatic long shmem_unused_huge_count(struct super_block *sb,\r\nstruct shrink_control *sc)\r\n{\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\r\nreturn READ_ONCE(sbinfo->shrinklist_len);\r\n}\r\nstatic unsigned long shmem_unused_huge_shrink(struct shmem_sb_info *sbinfo,\r\nstruct shrink_control *sc, unsigned long nr_to_split)\r\n{\r\nreturn 0;\r\n}\r\nstatic int shmem_add_to_page_cache(struct page *page,\r\nstruct address_space *mapping,\r\npgoff_t index, void *expected)\r\n{\r\nint error, nr = hpage_nr_pages(page);\r\nVM_BUG_ON_PAGE(PageTail(page), page);\r\nVM_BUG_ON_PAGE(index != round_down(index, nr), page);\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nVM_BUG_ON_PAGE(!PageSwapBacked(page), page);\r\nVM_BUG_ON(expected && PageTransHuge(page));\r\npage_ref_add(page, nr);\r\npage->mapping = mapping;\r\npage->index = index;\r\nspin_lock_irq(&mapping->tree_lock);\r\nif (PageTransHuge(page)) {\r\nvoid __rcu **results;\r\npgoff_t idx;\r\nint i;\r\nerror = 0;\r\nif (radix_tree_gang_lookup_slot(&mapping->page_tree,\r\n&results, &idx, index, 1) &&\r\nidx < index + HPAGE_PMD_NR) {\r\nerror = -EEXIST;\r\n}\r\nif (!error) {\r\nfor (i = 0; i < HPAGE_PMD_NR; i++) {\r\nerror = radix_tree_insert(&mapping->page_tree,\r\nindex + i, page + i);\r\nVM_BUG_ON(error);\r\n}\r\ncount_vm_event(THP_FILE_ALLOC);\r\n}\r\n} else if (!expected) {\r\nerror = radix_tree_insert(&mapping->page_tree, index, page);\r\n} else {\r\nerror = shmem_radix_tree_replace(mapping, index, expected,\r\npage);\r\n}\r\nif (!error) {\r\nmapping->nrpages += nr;\r\nif (PageTransHuge(page))\r\n__inc_node_page_state(page, NR_SHMEM_THPS);\r\n__mod_node_page_state(page_pgdat(page), NR_FILE_PAGES, nr);\r\n__mod_node_page_state(page_pgdat(page), NR_SHMEM, nr);\r\nspin_unlock_irq(&mapping->tree_lock);\r\n} else {\r\npage->mapping = NULL;\r\nspin_unlock_irq(&mapping->tree_lock);\r\npage_ref_sub(page, nr);\r\n}\r\nreturn error;\r\n}\r\nstatic void shmem_delete_from_page_cache(struct page *page, void *radswap)\r\n{\r\nstruct address_space *mapping = page->mapping;\r\nint error;\r\nVM_BUG_ON_PAGE(PageCompound(page), page);\r\nspin_lock_irq(&mapping->tree_lock);\r\nerror = shmem_radix_tree_replace(mapping, page->index, page, radswap);\r\npage->mapping = NULL;\r\nmapping->nrpages--;\r\n__dec_node_page_state(page, NR_FILE_PAGES);\r\n__dec_node_page_state(page, NR_SHMEM);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nput_page(page);\r\nBUG_ON(error);\r\n}\r\nstatic int shmem_free_swap(struct address_space *mapping,\r\npgoff_t index, void *radswap)\r\n{\r\nvoid *old;\r\nspin_lock_irq(&mapping->tree_lock);\r\nold = radix_tree_delete_item(&mapping->page_tree, index, radswap);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nif (old != radswap)\r\nreturn -ENOENT;\r\nfree_swap_and_cache(radix_to_swp_entry(radswap));\r\nreturn 0;\r\n}\r\nunsigned long shmem_partial_swap_usage(struct address_space *mapping,\r\npgoff_t start, pgoff_t end)\r\n{\r\nstruct radix_tree_iter iter;\r\nvoid **slot;\r\nstruct page *page;\r\nunsigned long swapped = 0;\r\nrcu_read_lock();\r\nradix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {\r\nif (iter.index >= end)\r\nbreak;\r\npage = radix_tree_deref_slot(slot);\r\nif (radix_tree_deref_retry(page)) {\r\nslot = radix_tree_iter_retry(&iter);\r\ncontinue;\r\n}\r\nif (radix_tree_exceptional_entry(page))\r\nswapped++;\r\nif (need_resched()) {\r\nslot = radix_tree_iter_resume(slot, &iter);\r\ncond_resched_rcu();\r\n}\r\n}\r\nrcu_read_unlock();\r\nreturn swapped << PAGE_SHIFT;\r\n}\r\nunsigned long shmem_swap_usage(struct vm_area_struct *vma)\r\n{\r\nstruct inode *inode = file_inode(vma->vm_file);\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nstruct address_space *mapping = inode->i_mapping;\r\nunsigned long swapped;\r\nswapped = READ_ONCE(info->swapped);\r\nif (!swapped)\r\nreturn 0;\r\nif (!vma->vm_pgoff && vma->vm_end - vma->vm_start >= inode->i_size)\r\nreturn swapped << PAGE_SHIFT;\r\nreturn shmem_partial_swap_usage(mapping,\r\nlinear_page_index(vma, vma->vm_start),\r\nlinear_page_index(vma, vma->vm_end));\r\n}\r\nvoid shmem_unlock_mapping(struct address_space *mapping)\r\n{\r\nstruct pagevec pvec;\r\npgoff_t indices[PAGEVEC_SIZE];\r\npgoff_t index = 0;\r\npagevec_init(&pvec, 0);\r\nwhile (!mapping_unevictable(mapping)) {\r\npvec.nr = find_get_entries(mapping, index,\r\nPAGEVEC_SIZE, pvec.pages, indices);\r\nif (!pvec.nr)\r\nbreak;\r\nindex = indices[pvec.nr - 1] + 1;\r\npagevec_remove_exceptionals(&pvec);\r\ncheck_move_unevictable_pages(pvec.pages, pvec.nr);\r\npagevec_release(&pvec);\r\ncond_resched();\r\n}\r\n}\r\nstatic void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,\r\nbool unfalloc)\r\n{\r\nstruct address_space *mapping = inode->i_mapping;\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\npgoff_t start = (lstart + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\npgoff_t end = (lend + 1) >> PAGE_SHIFT;\r\nunsigned int partial_start = lstart & (PAGE_SIZE - 1);\r\nunsigned int partial_end = (lend + 1) & (PAGE_SIZE - 1);\r\nstruct pagevec pvec;\r\npgoff_t indices[PAGEVEC_SIZE];\r\nlong nr_swaps_freed = 0;\r\npgoff_t index;\r\nint i;\r\nif (lend == -1)\r\nend = -1;\r\npagevec_init(&pvec, 0);\r\nindex = start;\r\nwhile (index < end) {\r\npvec.nr = find_get_entries(mapping, index,\r\nmin(end - index, (pgoff_t)PAGEVEC_SIZE),\r\npvec.pages, indices);\r\nif (!pvec.nr)\r\nbreak;\r\nfor (i = 0; i < pagevec_count(&pvec); i++) {\r\nstruct page *page = pvec.pages[i];\r\nindex = indices[i];\r\nif (index >= end)\r\nbreak;\r\nif (radix_tree_exceptional_entry(page)) {\r\nif (unfalloc)\r\ncontinue;\r\nnr_swaps_freed += !shmem_free_swap(mapping,\r\nindex, page);\r\ncontinue;\r\n}\r\nVM_BUG_ON_PAGE(page_to_pgoff(page) != index, page);\r\nif (!trylock_page(page))\r\ncontinue;\r\nif (PageTransTail(page)) {\r\nclear_highpage(page);\r\nunlock_page(page);\r\ncontinue;\r\n} else if (PageTransHuge(page)) {\r\nif (index == round_down(end, HPAGE_PMD_NR)) {\r\nclear_highpage(page);\r\nunlock_page(page);\r\ncontinue;\r\n}\r\nindex += HPAGE_PMD_NR - 1;\r\ni += HPAGE_PMD_NR - 1;\r\n}\r\nif (!unfalloc || !PageUptodate(page)) {\r\nVM_BUG_ON_PAGE(PageTail(page), page);\r\nif (page_mapping(page) == mapping) {\r\nVM_BUG_ON_PAGE(PageWriteback(page), page);\r\ntruncate_inode_page(mapping, page);\r\n}\r\n}\r\nunlock_page(page);\r\n}\r\npagevec_remove_exceptionals(&pvec);\r\npagevec_release(&pvec);\r\ncond_resched();\r\nindex++;\r\n}\r\nif (partial_start) {\r\nstruct page *page = NULL;\r\nshmem_getpage(inode, start - 1, &page, SGP_READ);\r\nif (page) {\r\nunsigned int top = PAGE_SIZE;\r\nif (start > end) {\r\ntop = partial_end;\r\npartial_end = 0;\r\n}\r\nzero_user_segment(page, partial_start, top);\r\nset_page_dirty(page);\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\n}\r\nif (partial_end) {\r\nstruct page *page = NULL;\r\nshmem_getpage(inode, end, &page, SGP_READ);\r\nif (page) {\r\nzero_user_segment(page, 0, partial_end);\r\nset_page_dirty(page);\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\n}\r\nif (start >= end)\r\nreturn;\r\nindex = start;\r\nwhile (index < end) {\r\ncond_resched();\r\npvec.nr = find_get_entries(mapping, index,\r\nmin(end - index, (pgoff_t)PAGEVEC_SIZE),\r\npvec.pages, indices);\r\nif (!pvec.nr) {\r\nif (index == start || end != -1)\r\nbreak;\r\nindex = start;\r\ncontinue;\r\n}\r\nfor (i = 0; i < pagevec_count(&pvec); i++) {\r\nstruct page *page = pvec.pages[i];\r\nindex = indices[i];\r\nif (index >= end)\r\nbreak;\r\nif (radix_tree_exceptional_entry(page)) {\r\nif (unfalloc)\r\ncontinue;\r\nif (shmem_free_swap(mapping, index, page)) {\r\nindex--;\r\nbreak;\r\n}\r\nnr_swaps_freed++;\r\ncontinue;\r\n}\r\nlock_page(page);\r\nif (PageTransTail(page)) {\r\nclear_highpage(page);\r\nunlock_page(page);\r\nif (index != round_down(end, HPAGE_PMD_NR))\r\nstart++;\r\ncontinue;\r\n} else if (PageTransHuge(page)) {\r\nif (index == round_down(end, HPAGE_PMD_NR)) {\r\nclear_highpage(page);\r\nunlock_page(page);\r\ncontinue;\r\n}\r\nindex += HPAGE_PMD_NR - 1;\r\ni += HPAGE_PMD_NR - 1;\r\n}\r\nif (!unfalloc || !PageUptodate(page)) {\r\nVM_BUG_ON_PAGE(PageTail(page), page);\r\nif (page_mapping(page) == mapping) {\r\nVM_BUG_ON_PAGE(PageWriteback(page), page);\r\ntruncate_inode_page(mapping, page);\r\n} else {\r\nunlock_page(page);\r\nindex--;\r\nbreak;\r\n}\r\n}\r\nunlock_page(page);\r\n}\r\npagevec_remove_exceptionals(&pvec);\r\npagevec_release(&pvec);\r\nindex++;\r\n}\r\nspin_lock_irq(&info->lock);\r\ninfo->swapped -= nr_swaps_freed;\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irq(&info->lock);\r\n}\r\nvoid shmem_truncate_range(struct inode *inode, loff_t lstart, loff_t lend)\r\n{\r\nshmem_undo_range(inode, lstart, lend, false);\r\ninode->i_ctime = inode->i_mtime = current_time(inode);\r\n}\r\nstatic int shmem_getattr(const struct path *path, struct kstat *stat,\r\nu32 request_mask, unsigned int query_flags)\r\n{\r\nstruct inode *inode = path->dentry->d_inode;\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nif (info->alloced - info->swapped != inode->i_mapping->nrpages) {\r\nspin_lock_irq(&info->lock);\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irq(&info->lock);\r\n}\r\ngeneric_fillattr(inode, stat);\r\nreturn 0;\r\n}\r\nstatic int shmem_setattr(struct dentry *dentry, struct iattr *attr)\r\n{\r\nstruct inode *inode = d_inode(dentry);\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\r\nint error;\r\nerror = setattr_prepare(dentry, attr);\r\nif (error)\r\nreturn error;\r\nif (S_ISREG(inode->i_mode) && (attr->ia_valid & ATTR_SIZE)) {\r\nloff_t oldsize = inode->i_size;\r\nloff_t newsize = attr->ia_size;\r\nif ((newsize < oldsize && (info->seals & F_SEAL_SHRINK)) ||\r\n(newsize > oldsize && (info->seals & F_SEAL_GROW)))\r\nreturn -EPERM;\r\nif (newsize != oldsize) {\r\nerror = shmem_reacct_size(SHMEM_I(inode)->flags,\r\noldsize, newsize);\r\nif (error)\r\nreturn error;\r\ni_size_write(inode, newsize);\r\ninode->i_ctime = inode->i_mtime = current_time(inode);\r\n}\r\nif (newsize <= oldsize) {\r\nloff_t holebegin = round_up(newsize, PAGE_SIZE);\r\nif (oldsize > holebegin)\r\nunmap_mapping_range(inode->i_mapping,\r\nholebegin, 0, 1);\r\nif (info->alloced)\r\nshmem_truncate_range(inode,\r\nnewsize, (loff_t)-1);\r\nif (oldsize > holebegin)\r\nunmap_mapping_range(inode->i_mapping,\r\nholebegin, 0, 1);\r\nif (IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE)) {\r\nspin_lock(&sbinfo->shrinklist_lock);\r\nif (list_empty(&info->shrinklist)) {\r\nlist_add_tail(&info->shrinklist,\r\n&sbinfo->shrinklist);\r\nsbinfo->shrinklist_len++;\r\n}\r\nspin_unlock(&sbinfo->shrinklist_lock);\r\n}\r\n}\r\n}\r\nsetattr_copy(inode, attr);\r\nif (attr->ia_valid & ATTR_MODE)\r\nerror = posix_acl_chmod(inode, inode->i_mode);\r\nreturn error;\r\n}\r\nstatic void shmem_evict_inode(struct inode *inode)\r\n{\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\r\nif (inode->i_mapping->a_ops == &shmem_aops) {\r\nshmem_unacct_size(info->flags, inode->i_size);\r\ninode->i_size = 0;\r\nshmem_truncate_range(inode, 0, (loff_t)-1);\r\nif (!list_empty(&info->shrinklist)) {\r\nspin_lock(&sbinfo->shrinklist_lock);\r\nif (!list_empty(&info->shrinklist)) {\r\nlist_del_init(&info->shrinklist);\r\nsbinfo->shrinklist_len--;\r\n}\r\nspin_unlock(&sbinfo->shrinklist_lock);\r\n}\r\nif (!list_empty(&info->swaplist)) {\r\nmutex_lock(&shmem_swaplist_mutex);\r\nlist_del_init(&info->swaplist);\r\nmutex_unlock(&shmem_swaplist_mutex);\r\n}\r\n}\r\nsimple_xattrs_free(&info->xattrs);\r\nWARN_ON(inode->i_blocks);\r\nshmem_free_inode(inode->i_sb);\r\nclear_inode(inode);\r\n}\r\nstatic unsigned long find_swap_entry(struct radix_tree_root *root, void *item)\r\n{\r\nstruct radix_tree_iter iter;\r\nvoid **slot;\r\nunsigned long found = -1;\r\nunsigned int checked = 0;\r\nrcu_read_lock();\r\nradix_tree_for_each_slot(slot, root, &iter, 0) {\r\nif (*slot == item) {\r\nfound = iter.index;\r\nbreak;\r\n}\r\nchecked++;\r\nif ((checked % 4096) != 0)\r\ncontinue;\r\nslot = radix_tree_iter_resume(slot, &iter);\r\ncond_resched_rcu();\r\n}\r\nrcu_read_unlock();\r\nreturn found;\r\n}\r\nstatic int shmem_unuse_inode(struct shmem_inode_info *info,\r\nswp_entry_t swap, struct page **pagep)\r\n{\r\nstruct address_space *mapping = info->vfs_inode.i_mapping;\r\nvoid *radswap;\r\npgoff_t index;\r\ngfp_t gfp;\r\nint error = 0;\r\nradswap = swp_to_radix_entry(swap);\r\nindex = find_swap_entry(&mapping->page_tree, radswap);\r\nif (index == -1)\r\nreturn -EAGAIN;\r\nif (shmem_swaplist.next != &info->swaplist)\r\nlist_move_tail(&shmem_swaplist, &info->swaplist);\r\ngfp = mapping_gfp_mask(mapping);\r\nif (shmem_should_replace_page(*pagep, gfp)) {\r\nmutex_unlock(&shmem_swaplist_mutex);\r\nerror = shmem_replace_page(pagep, gfp, info, index);\r\nmutex_lock(&shmem_swaplist_mutex);\r\nif (!page_swapcount(*pagep))\r\nerror = -ENOENT;\r\n}\r\nif (!error)\r\nerror = shmem_add_to_page_cache(*pagep, mapping, index,\r\nradswap);\r\nif (error != -ENOMEM) {\r\ndelete_from_swap_cache(*pagep);\r\nset_page_dirty(*pagep);\r\nif (!error) {\r\nspin_lock_irq(&info->lock);\r\ninfo->swapped--;\r\nspin_unlock_irq(&info->lock);\r\nswap_free(swap);\r\n}\r\n}\r\nreturn error;\r\n}\r\nint shmem_unuse(swp_entry_t swap, struct page *page)\r\n{\r\nstruct list_head *this, *next;\r\nstruct shmem_inode_info *info;\r\nstruct mem_cgroup *memcg;\r\nint error = 0;\r\nif (unlikely(!PageSwapCache(page) || page_private(page) != swap.val))\r\ngoto out;\r\nerror = mem_cgroup_try_charge(page, current->mm, GFP_KERNEL, &memcg,\r\nfalse);\r\nif (error)\r\ngoto out;\r\nerror = -EAGAIN;\r\nmutex_lock(&shmem_swaplist_mutex);\r\nlist_for_each_safe(this, next, &shmem_swaplist) {\r\ninfo = list_entry(this, struct shmem_inode_info, swaplist);\r\nif (info->swapped)\r\nerror = shmem_unuse_inode(info, swap, &page);\r\nelse\r\nlist_del_init(&info->swaplist);\r\ncond_resched();\r\nif (error != -EAGAIN)\r\nbreak;\r\n}\r\nmutex_unlock(&shmem_swaplist_mutex);\r\nif (error) {\r\nif (error != -ENOMEM)\r\nerror = 0;\r\nmem_cgroup_cancel_charge(page, memcg, false);\r\n} else\r\nmem_cgroup_commit_charge(page, memcg, true, false);\r\nout:\r\nunlock_page(page);\r\nput_page(page);\r\nreturn error;\r\n}\r\nstatic int shmem_writepage(struct page *page, struct writeback_control *wbc)\r\n{\r\nstruct shmem_inode_info *info;\r\nstruct address_space *mapping;\r\nstruct inode *inode;\r\nswp_entry_t swap;\r\npgoff_t index;\r\nVM_BUG_ON_PAGE(PageCompound(page), page);\r\nBUG_ON(!PageLocked(page));\r\nmapping = page->mapping;\r\nindex = page->index;\r\ninode = mapping->host;\r\ninfo = SHMEM_I(inode);\r\nif (info->flags & VM_LOCKED)\r\ngoto redirty;\r\nif (!total_swap_pages)\r\ngoto redirty;\r\nif (!wbc->for_reclaim) {\r\nWARN_ON_ONCE(1);\r\ngoto redirty;\r\n}\r\nif (!PageUptodate(page)) {\r\nif (inode->i_private) {\r\nstruct shmem_falloc *shmem_falloc;\r\nspin_lock(&inode->i_lock);\r\nshmem_falloc = inode->i_private;\r\nif (shmem_falloc &&\r\n!shmem_falloc->waitq &&\r\nindex >= shmem_falloc->start &&\r\nindex < shmem_falloc->next)\r\nshmem_falloc->nr_unswapped++;\r\nelse\r\nshmem_falloc = NULL;\r\nspin_unlock(&inode->i_lock);\r\nif (shmem_falloc)\r\ngoto redirty;\r\n}\r\nclear_highpage(page);\r\nflush_dcache_page(page);\r\nSetPageUptodate(page);\r\n}\r\nswap = get_swap_page();\r\nif (!swap.val)\r\ngoto redirty;\r\nif (mem_cgroup_try_charge_swap(page, swap))\r\ngoto free_swap;\r\nmutex_lock(&shmem_swaplist_mutex);\r\nif (list_empty(&info->swaplist))\r\nlist_add_tail(&info->swaplist, &shmem_swaplist);\r\nif (add_to_swap_cache(page, swap, GFP_ATOMIC) == 0) {\r\nspin_lock_irq(&info->lock);\r\nshmem_recalc_inode(inode);\r\ninfo->swapped++;\r\nspin_unlock_irq(&info->lock);\r\nswap_shmem_alloc(swap);\r\nshmem_delete_from_page_cache(page, swp_to_radix_entry(swap));\r\nmutex_unlock(&shmem_swaplist_mutex);\r\nBUG_ON(page_mapped(page));\r\nswap_writepage(page, wbc);\r\nreturn 0;\r\n}\r\nmutex_unlock(&shmem_swaplist_mutex);\r\nfree_swap:\r\nswapcache_free(swap);\r\nredirty:\r\nset_page_dirty(page);\r\nif (wbc->for_reclaim)\r\nreturn AOP_WRITEPAGE_ACTIVATE;\r\nunlock_page(page);\r\nreturn 0;\r\n}\r\nstatic void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)\r\n{\r\nchar buffer[64];\r\nif (!mpol || mpol->mode == MPOL_DEFAULT)\r\nreturn;\r\nmpol_to_str(buffer, sizeof(buffer), mpol);\r\nseq_printf(seq, ",mpol=%s", buffer);\r\n}\r\nstatic struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)\r\n{\r\nstruct mempolicy *mpol = NULL;\r\nif (sbinfo->mpol) {\r\nspin_lock(&sbinfo->stat_lock);\r\nmpol = sbinfo->mpol;\r\nmpol_get(mpol);\r\nspin_unlock(&sbinfo->stat_lock);\r\n}\r\nreturn mpol;\r\n}\r\nstatic inline void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)\r\n{\r\n}\r\nstatic inline struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)\r\n{\r\nreturn NULL;\r\n}\r\nstatic void shmem_pseudo_vma_init(struct vm_area_struct *vma,\r\nstruct shmem_inode_info *info, pgoff_t index)\r\n{\r\nvma->vm_start = 0;\r\nvma->vm_pgoff = index + info->vfs_inode.i_ino;\r\nvma->vm_ops = NULL;\r\nvma->vm_policy = mpol_shared_policy_lookup(&info->policy, index);\r\n}\r\nstatic void shmem_pseudo_vma_destroy(struct vm_area_struct *vma)\r\n{\r\nmpol_cond_put(vma->vm_policy);\r\n}\r\nstatic struct page *shmem_swapin(swp_entry_t swap, gfp_t gfp,\r\nstruct shmem_inode_info *info, pgoff_t index)\r\n{\r\nstruct vm_area_struct pvma;\r\nstruct page *page;\r\nshmem_pseudo_vma_init(&pvma, info, index);\r\npage = swapin_readahead(swap, gfp, &pvma, 0);\r\nshmem_pseudo_vma_destroy(&pvma);\r\nreturn page;\r\n}\r\nstatic struct page *shmem_alloc_hugepage(gfp_t gfp,\r\nstruct shmem_inode_info *info, pgoff_t index)\r\n{\r\nstruct vm_area_struct pvma;\r\nstruct inode *inode = &info->vfs_inode;\r\nstruct address_space *mapping = inode->i_mapping;\r\npgoff_t idx, hindex;\r\nvoid __rcu **results;\r\nstruct page *page;\r\nif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE))\r\nreturn NULL;\r\nhindex = round_down(index, HPAGE_PMD_NR);\r\nrcu_read_lock();\r\nif (radix_tree_gang_lookup_slot(&mapping->page_tree, &results, &idx,\r\nhindex, 1) && idx < hindex + HPAGE_PMD_NR) {\r\nrcu_read_unlock();\r\nreturn NULL;\r\n}\r\nrcu_read_unlock();\r\nshmem_pseudo_vma_init(&pvma, info, hindex);\r\npage = alloc_pages_vma(gfp | __GFP_COMP | __GFP_NORETRY | __GFP_NOWARN,\r\nHPAGE_PMD_ORDER, &pvma, 0, numa_node_id(), true);\r\nshmem_pseudo_vma_destroy(&pvma);\r\nif (page)\r\nprep_transhuge_page(page);\r\nreturn page;\r\n}\r\nstatic struct page *shmem_alloc_page(gfp_t gfp,\r\nstruct shmem_inode_info *info, pgoff_t index)\r\n{\r\nstruct vm_area_struct pvma;\r\nstruct page *page;\r\nshmem_pseudo_vma_init(&pvma, info, index);\r\npage = alloc_page_vma(gfp, &pvma, 0);\r\nshmem_pseudo_vma_destroy(&pvma);\r\nreturn page;\r\n}\r\nstatic struct page *shmem_alloc_and_acct_page(gfp_t gfp,\r\nstruct shmem_inode_info *info, struct shmem_sb_info *sbinfo,\r\npgoff_t index, bool huge)\r\n{\r\nstruct page *page;\r\nint nr;\r\nint err = -ENOSPC;\r\nif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE))\r\nhuge = false;\r\nnr = huge ? HPAGE_PMD_NR : 1;\r\nif (shmem_acct_block(info->flags, nr))\r\ngoto failed;\r\nif (sbinfo->max_blocks) {\r\nif (percpu_counter_compare(&sbinfo->used_blocks,\r\nsbinfo->max_blocks - nr) > 0)\r\ngoto unacct;\r\npercpu_counter_add(&sbinfo->used_blocks, nr);\r\n}\r\nif (huge)\r\npage = shmem_alloc_hugepage(gfp, info, index);\r\nelse\r\npage = shmem_alloc_page(gfp, info, index);\r\nif (page) {\r\n__SetPageLocked(page);\r\n__SetPageSwapBacked(page);\r\nreturn page;\r\n}\r\nerr = -ENOMEM;\r\nif (sbinfo->max_blocks)\r\npercpu_counter_add(&sbinfo->used_blocks, -nr);\r\nunacct:\r\nshmem_unacct_blocks(info->flags, nr);\r\nfailed:\r\nreturn ERR_PTR(err);\r\n}\r\nstatic bool shmem_should_replace_page(struct page *page, gfp_t gfp)\r\n{\r\nreturn page_zonenum(page) > gfp_zone(gfp);\r\n}\r\nstatic int shmem_replace_page(struct page **pagep, gfp_t gfp,\r\nstruct shmem_inode_info *info, pgoff_t index)\r\n{\r\nstruct page *oldpage, *newpage;\r\nstruct address_space *swap_mapping;\r\npgoff_t swap_index;\r\nint error;\r\noldpage = *pagep;\r\nswap_index = page_private(oldpage);\r\nswap_mapping = page_mapping(oldpage);\r\ngfp &= ~GFP_CONSTRAINT_MASK;\r\nnewpage = shmem_alloc_page(gfp, info, index);\r\nif (!newpage)\r\nreturn -ENOMEM;\r\nget_page(newpage);\r\ncopy_highpage(newpage, oldpage);\r\nflush_dcache_page(newpage);\r\n__SetPageLocked(newpage);\r\n__SetPageSwapBacked(newpage);\r\nSetPageUptodate(newpage);\r\nset_page_private(newpage, swap_index);\r\nSetPageSwapCache(newpage);\r\nspin_lock_irq(&swap_mapping->tree_lock);\r\nerror = shmem_radix_tree_replace(swap_mapping, swap_index, oldpage,\r\nnewpage);\r\nif (!error) {\r\n__inc_node_page_state(newpage, NR_FILE_PAGES);\r\n__dec_node_page_state(oldpage, NR_FILE_PAGES);\r\n}\r\nspin_unlock_irq(&swap_mapping->tree_lock);\r\nif (unlikely(error)) {\r\noldpage = newpage;\r\n} else {\r\nmem_cgroup_migrate(oldpage, newpage);\r\nlru_cache_add_anon(newpage);\r\n*pagep = newpage;\r\n}\r\nClearPageSwapCache(oldpage);\r\nset_page_private(oldpage, 0);\r\nunlock_page(oldpage);\r\nput_page(oldpage);\r\nput_page(oldpage);\r\nreturn error;\r\n}\r\nstatic int shmem_getpage_gfp(struct inode *inode, pgoff_t index,\r\nstruct page **pagep, enum sgp_type sgp, gfp_t gfp,\r\nstruct vm_area_struct *vma, struct vm_fault *vmf, int *fault_type)\r\n{\r\nstruct address_space *mapping = inode->i_mapping;\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nstruct shmem_sb_info *sbinfo;\r\nstruct mm_struct *charge_mm;\r\nstruct mem_cgroup *memcg;\r\nstruct page *page;\r\nswp_entry_t swap;\r\nenum sgp_type sgp_huge = sgp;\r\npgoff_t hindex = index;\r\nint error;\r\nint once = 0;\r\nint alloced = 0;\r\nif (index > (MAX_LFS_FILESIZE >> PAGE_SHIFT))\r\nreturn -EFBIG;\r\nif (sgp == SGP_NOHUGE || sgp == SGP_HUGE)\r\nsgp = SGP_CACHE;\r\nrepeat:\r\nswap.val = 0;\r\npage = find_lock_entry(mapping, index);\r\nif (radix_tree_exceptional_entry(page)) {\r\nswap = radix_to_swp_entry(page);\r\npage = NULL;\r\n}\r\nif (sgp <= SGP_CACHE &&\r\n((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {\r\nerror = -EINVAL;\r\ngoto unlock;\r\n}\r\nif (page && sgp == SGP_WRITE)\r\nmark_page_accessed(page);\r\nif (page && !PageUptodate(page)) {\r\nif (sgp != SGP_READ)\r\ngoto clear;\r\nunlock_page(page);\r\nput_page(page);\r\npage = NULL;\r\n}\r\nif (page || (sgp == SGP_READ && !swap.val)) {\r\n*pagep = page;\r\nreturn 0;\r\n}\r\nsbinfo = SHMEM_SB(inode->i_sb);\r\ncharge_mm = vma ? vma->vm_mm : current->mm;\r\nif (swap.val) {\r\npage = lookup_swap_cache(swap);\r\nif (!page) {\r\nif (fault_type) {\r\n*fault_type |= VM_FAULT_MAJOR;\r\ncount_vm_event(PGMAJFAULT);\r\nmem_cgroup_count_vm_event(charge_mm,\r\nPGMAJFAULT);\r\n}\r\npage = shmem_swapin(swap, gfp, info, index);\r\nif (!page) {\r\nerror = -ENOMEM;\r\ngoto failed;\r\n}\r\n}\r\nlock_page(page);\r\nif (!PageSwapCache(page) || page_private(page) != swap.val ||\r\n!shmem_confirm_swap(mapping, index, swap)) {\r\nerror = -EEXIST;\r\ngoto unlock;\r\n}\r\nif (!PageUptodate(page)) {\r\nerror = -EIO;\r\ngoto failed;\r\n}\r\nwait_on_page_writeback(page);\r\nif (shmem_should_replace_page(page, gfp)) {\r\nerror = shmem_replace_page(&page, gfp, info, index);\r\nif (error)\r\ngoto failed;\r\n}\r\nerror = mem_cgroup_try_charge(page, charge_mm, gfp, &memcg,\r\nfalse);\r\nif (!error) {\r\nerror = shmem_add_to_page_cache(page, mapping, index,\r\nswp_to_radix_entry(swap));\r\nif (error) {\r\nmem_cgroup_cancel_charge(page, memcg, false);\r\ndelete_from_swap_cache(page);\r\n}\r\n}\r\nif (error)\r\ngoto failed;\r\nmem_cgroup_commit_charge(page, memcg, true, false);\r\nspin_lock_irq(&info->lock);\r\ninfo->swapped--;\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irq(&info->lock);\r\nif (sgp == SGP_WRITE)\r\nmark_page_accessed(page);\r\ndelete_from_swap_cache(page);\r\nset_page_dirty(page);\r\nswap_free(swap);\r\n} else {\r\nif (vma && userfaultfd_missing(vma)) {\r\n*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);\r\nreturn 0;\r\n}\r\nif (mapping->a_ops != &shmem_aops)\r\ngoto alloc_nohuge;\r\nif (shmem_huge == SHMEM_HUGE_DENY || sgp_huge == SGP_NOHUGE)\r\ngoto alloc_nohuge;\r\nif (shmem_huge == SHMEM_HUGE_FORCE)\r\ngoto alloc_huge;\r\nswitch (sbinfo->huge) {\r\nloff_t i_size;\r\npgoff_t off;\r\ncase SHMEM_HUGE_NEVER:\r\ngoto alloc_nohuge;\r\ncase SHMEM_HUGE_WITHIN_SIZE:\r\noff = round_up(index, HPAGE_PMD_NR);\r\ni_size = round_up(i_size_read(inode), PAGE_SIZE);\r\nif (i_size >= HPAGE_PMD_SIZE &&\r\ni_size >> PAGE_SHIFT >= off)\r\ngoto alloc_huge;\r\ncase SHMEM_HUGE_ADVISE:\r\nif (sgp_huge == SGP_HUGE)\r\ngoto alloc_huge;\r\ngoto alloc_nohuge;\r\n}\r\nalloc_huge:\r\npage = shmem_alloc_and_acct_page(gfp, info, sbinfo,\r\nindex, true);\r\nif (IS_ERR(page)) {\r\nalloc_nohuge: page = shmem_alloc_and_acct_page(gfp, info, sbinfo,\r\nindex, false);\r\n}\r\nif (IS_ERR(page)) {\r\nint retry = 5;\r\nerror = PTR_ERR(page);\r\npage = NULL;\r\nif (error != -ENOSPC)\r\ngoto failed;\r\nwhile (retry--) {\r\nint ret;\r\nret = shmem_unused_huge_shrink(sbinfo, NULL, 1);\r\nif (ret == SHRINK_STOP)\r\nbreak;\r\nif (ret)\r\ngoto alloc_nohuge;\r\n}\r\ngoto failed;\r\n}\r\nif (PageTransHuge(page))\r\nhindex = round_down(index, HPAGE_PMD_NR);\r\nelse\r\nhindex = index;\r\nif (sgp == SGP_WRITE)\r\n__SetPageReferenced(page);\r\nerror = mem_cgroup_try_charge(page, charge_mm, gfp, &memcg,\r\nPageTransHuge(page));\r\nif (error)\r\ngoto unacct;\r\nerror = radix_tree_maybe_preload_order(gfp & GFP_RECLAIM_MASK,\r\ncompound_order(page));\r\nif (!error) {\r\nerror = shmem_add_to_page_cache(page, mapping, hindex,\r\nNULL);\r\nradix_tree_preload_end();\r\n}\r\nif (error) {\r\nmem_cgroup_cancel_charge(page, memcg,\r\nPageTransHuge(page));\r\ngoto unacct;\r\n}\r\nmem_cgroup_commit_charge(page, memcg, false,\r\nPageTransHuge(page));\r\nlru_cache_add_anon(page);\r\nspin_lock_irq(&info->lock);\r\ninfo->alloced += 1 << compound_order(page);\r\ninode->i_blocks += BLOCKS_PER_PAGE << compound_order(page);\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irq(&info->lock);\r\nalloced = true;\r\nif (PageTransHuge(page) &&\r\nDIV_ROUND_UP(i_size_read(inode), PAGE_SIZE) <\r\nhindex + HPAGE_PMD_NR - 1) {\r\nspin_lock(&sbinfo->shrinklist_lock);\r\nif (list_empty(&info->shrinklist)) {\r\nlist_add_tail(&info->shrinklist,\r\n&sbinfo->shrinklist);\r\nsbinfo->shrinklist_len++;\r\n}\r\nspin_unlock(&sbinfo->shrinklist_lock);\r\n}\r\nif (sgp == SGP_FALLOC)\r\nsgp = SGP_WRITE;\r\nclear:\r\nif (sgp != SGP_WRITE && !PageUptodate(page)) {\r\nstruct page *head = compound_head(page);\r\nint i;\r\nfor (i = 0; i < (1 << compound_order(head)); i++) {\r\nclear_highpage(head + i);\r\nflush_dcache_page(head + i);\r\n}\r\nSetPageUptodate(head);\r\n}\r\n}\r\nif (sgp <= SGP_CACHE &&\r\n((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {\r\nif (alloced) {\r\nClearPageDirty(page);\r\ndelete_from_page_cache(page);\r\nspin_lock_irq(&info->lock);\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irq(&info->lock);\r\n}\r\nerror = -EINVAL;\r\ngoto unlock;\r\n}\r\n*pagep = page + index - hindex;\r\nreturn 0;\r\nunacct:\r\nif (sbinfo->max_blocks)\r\npercpu_counter_sub(&sbinfo->used_blocks,\r\n1 << compound_order(page));\r\nshmem_unacct_blocks(info->flags, 1 << compound_order(page));\r\nif (PageTransHuge(page)) {\r\nunlock_page(page);\r\nput_page(page);\r\ngoto alloc_nohuge;\r\n}\r\nfailed:\r\nif (swap.val && !shmem_confirm_swap(mapping, index, swap))\r\nerror = -EEXIST;\r\nunlock:\r\nif (page) {\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\nif (error == -ENOSPC && !once++) {\r\nspin_lock_irq(&info->lock);\r\nshmem_recalc_inode(inode);\r\nspin_unlock_irq(&info->lock);\r\ngoto repeat;\r\n}\r\nif (error == -EEXIST)\r\ngoto repeat;\r\nreturn error;\r\n}\r\nstatic int synchronous_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key)\r\n{\r\nint ret = default_wake_function(wait, mode, sync, key);\r\nlist_del_init(&wait->task_list);\r\nreturn ret;\r\n}\r\nstatic int shmem_fault(struct vm_fault *vmf)\r\n{\r\nstruct vm_area_struct *vma = vmf->vma;\r\nstruct inode *inode = file_inode(vma->vm_file);\r\ngfp_t gfp = mapping_gfp_mask(inode->i_mapping);\r\nenum sgp_type sgp;\r\nint error;\r\nint ret = VM_FAULT_LOCKED;\r\nif (unlikely(inode->i_private)) {\r\nstruct shmem_falloc *shmem_falloc;\r\nspin_lock(&inode->i_lock);\r\nshmem_falloc = inode->i_private;\r\nif (shmem_falloc &&\r\nshmem_falloc->waitq &&\r\nvmf->pgoff >= shmem_falloc->start &&\r\nvmf->pgoff < shmem_falloc->next) {\r\nwait_queue_head_t *shmem_falloc_waitq;\r\nDEFINE_WAIT_FUNC(shmem_fault_wait, synchronous_wake_function);\r\nret = VM_FAULT_NOPAGE;\r\nif ((vmf->flags & FAULT_FLAG_ALLOW_RETRY) &&\r\n!(vmf->flags & FAULT_FLAG_RETRY_NOWAIT)) {\r\nup_read(&vma->vm_mm->mmap_sem);\r\nret = VM_FAULT_RETRY;\r\n}\r\nshmem_falloc_waitq = shmem_falloc->waitq;\r\nprepare_to_wait(shmem_falloc_waitq, &shmem_fault_wait,\r\nTASK_UNINTERRUPTIBLE);\r\nspin_unlock(&inode->i_lock);\r\nschedule();\r\nspin_lock(&inode->i_lock);\r\nfinish_wait(shmem_falloc_waitq, &shmem_fault_wait);\r\nspin_unlock(&inode->i_lock);\r\nreturn ret;\r\n}\r\nspin_unlock(&inode->i_lock);\r\n}\r\nsgp = SGP_CACHE;\r\nif (vma->vm_flags & VM_HUGEPAGE)\r\nsgp = SGP_HUGE;\r\nelse if (vma->vm_flags & VM_NOHUGEPAGE)\r\nsgp = SGP_NOHUGE;\r\nerror = shmem_getpage_gfp(inode, vmf->pgoff, &vmf->page, sgp,\r\ngfp, vma, vmf, &ret);\r\nif (error)\r\nreturn ((error == -ENOMEM) ? VM_FAULT_OOM : VM_FAULT_SIGBUS);\r\nreturn ret;\r\n}\r\nunsigned long shmem_get_unmapped_area(struct file *file,\r\nunsigned long uaddr, unsigned long len,\r\nunsigned long pgoff, unsigned long flags)\r\n{\r\nunsigned long (*get_area)(struct file *,\r\nunsigned long, unsigned long, unsigned long, unsigned long);\r\nunsigned long addr;\r\nunsigned long offset;\r\nunsigned long inflated_len;\r\nunsigned long inflated_addr;\r\nunsigned long inflated_offset;\r\nif (len > TASK_SIZE)\r\nreturn -ENOMEM;\r\nget_area = current->mm->get_unmapped_area;\r\naddr = get_area(file, uaddr, len, pgoff, flags);\r\nif (!IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE))\r\nreturn addr;\r\nif (IS_ERR_VALUE(addr))\r\nreturn addr;\r\nif (addr & ~PAGE_MASK)\r\nreturn addr;\r\nif (addr > TASK_SIZE - len)\r\nreturn addr;\r\nif (shmem_huge == SHMEM_HUGE_DENY)\r\nreturn addr;\r\nif (len < HPAGE_PMD_SIZE)\r\nreturn addr;\r\nif (flags & MAP_FIXED)\r\nreturn addr;\r\nif (uaddr)\r\nreturn addr;\r\nif (shmem_huge != SHMEM_HUGE_FORCE) {\r\nstruct super_block *sb;\r\nif (file) {\r\nVM_BUG_ON(file->f_op != &shmem_file_operations);\r\nsb = file_inode(file)->i_sb;\r\n} else {\r\nif (IS_ERR(shm_mnt))\r\nreturn addr;\r\nsb = shm_mnt->mnt_sb;\r\n}\r\nif (SHMEM_SB(sb)->huge == SHMEM_HUGE_NEVER)\r\nreturn addr;\r\n}\r\noffset = (pgoff << PAGE_SHIFT) & (HPAGE_PMD_SIZE-1);\r\nif (offset && offset + len < 2 * HPAGE_PMD_SIZE)\r\nreturn addr;\r\nif ((addr & (HPAGE_PMD_SIZE-1)) == offset)\r\nreturn addr;\r\ninflated_len = len + HPAGE_PMD_SIZE - PAGE_SIZE;\r\nif (inflated_len > TASK_SIZE)\r\nreturn addr;\r\nif (inflated_len < len)\r\nreturn addr;\r\ninflated_addr = get_area(NULL, 0, inflated_len, 0, flags);\r\nif (IS_ERR_VALUE(inflated_addr))\r\nreturn addr;\r\nif (inflated_addr & ~PAGE_MASK)\r\nreturn addr;\r\ninflated_offset = inflated_addr & (HPAGE_PMD_SIZE-1);\r\ninflated_addr += offset - inflated_offset;\r\nif (inflated_offset > offset)\r\ninflated_addr += HPAGE_PMD_SIZE;\r\nif (inflated_addr > TASK_SIZE - len)\r\nreturn addr;\r\nreturn inflated_addr;\r\n}\r\nstatic int shmem_set_policy(struct vm_area_struct *vma, struct mempolicy *mpol)\r\n{\r\nstruct inode *inode = file_inode(vma->vm_file);\r\nreturn mpol_set_shared_policy(&SHMEM_I(inode)->policy, vma, mpol);\r\n}\r\nstatic struct mempolicy *shmem_get_policy(struct vm_area_struct *vma,\r\nunsigned long addr)\r\n{\r\nstruct inode *inode = file_inode(vma->vm_file);\r\npgoff_t index;\r\nindex = ((addr - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\nreturn mpol_shared_policy_lookup(&SHMEM_I(inode)->policy, index);\r\n}\r\nint shmem_lock(struct file *file, int lock, struct user_struct *user)\r\n{\r\nstruct inode *inode = file_inode(file);\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nint retval = -ENOMEM;\r\nspin_lock_irq(&info->lock);\r\nif (lock && !(info->flags & VM_LOCKED)) {\r\nif (!user_shm_lock(inode->i_size, user))\r\ngoto out_nomem;\r\ninfo->flags |= VM_LOCKED;\r\nmapping_set_unevictable(file->f_mapping);\r\n}\r\nif (!lock && (info->flags & VM_LOCKED) && user) {\r\nuser_shm_unlock(inode->i_size, user);\r\ninfo->flags &= ~VM_LOCKED;\r\nmapping_clear_unevictable(file->f_mapping);\r\n}\r\nretval = 0;\r\nout_nomem:\r\nspin_unlock_irq(&info->lock);\r\nreturn retval;\r\n}\r\nstatic int shmem_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nfile_accessed(file);\r\nvma->vm_ops = &shmem_vm_ops;\r\nif (IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE) &&\r\n((vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK) <\r\n(vma->vm_end & HPAGE_PMD_MASK)) {\r\nkhugepaged_enter(vma, vma->vm_flags);\r\n}\r\nreturn 0;\r\n}\r\nstatic struct inode *shmem_get_inode(struct super_block *sb, const struct inode *dir,\r\numode_t mode, dev_t dev, unsigned long flags)\r\n{\r\nstruct inode *inode;\r\nstruct shmem_inode_info *info;\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\r\nif (shmem_reserve_inode(sb))\r\nreturn NULL;\r\ninode = new_inode(sb);\r\nif (inode) {\r\ninode->i_ino = get_next_ino();\r\ninode_init_owner(inode, dir, mode);\r\ninode->i_blocks = 0;\r\ninode->i_atime = inode->i_mtime = inode->i_ctime = current_time(inode);\r\ninode->i_generation = get_seconds();\r\ninfo = SHMEM_I(inode);\r\nmemset(info, 0, (char *)inode - (char *)info);\r\nspin_lock_init(&info->lock);\r\ninfo->seals = F_SEAL_SEAL;\r\ninfo->flags = flags & VM_NORESERVE;\r\nINIT_LIST_HEAD(&info->shrinklist);\r\nINIT_LIST_HEAD(&info->swaplist);\r\nsimple_xattrs_init(&info->xattrs);\r\ncache_no_acl(inode);\r\nswitch (mode & S_IFMT) {\r\ndefault:\r\ninode->i_op = &shmem_special_inode_operations;\r\ninit_special_inode(inode, mode, dev);\r\nbreak;\r\ncase S_IFREG:\r\ninode->i_mapping->a_ops = &shmem_aops;\r\ninode->i_op = &shmem_inode_operations;\r\ninode->i_fop = &shmem_file_operations;\r\nmpol_shared_policy_init(&info->policy,\r\nshmem_get_sbmpol(sbinfo));\r\nbreak;\r\ncase S_IFDIR:\r\ninc_nlink(inode);\r\ninode->i_size = 2 * BOGO_DIRENT_SIZE;\r\ninode->i_op = &shmem_dir_inode_operations;\r\ninode->i_fop = &simple_dir_operations;\r\nbreak;\r\ncase S_IFLNK:\r\nmpol_shared_policy_init(&info->policy, NULL);\r\nbreak;\r\n}\r\n} else\r\nshmem_free_inode(sb);\r\nreturn inode;\r\n}\r\nbool shmem_mapping(struct address_space *mapping)\r\n{\r\nreturn mapping->a_ops == &shmem_aops;\r\n}\r\nint shmem_mcopy_atomic_pte(struct mm_struct *dst_mm,\r\npmd_t *dst_pmd,\r\nstruct vm_area_struct *dst_vma,\r\nunsigned long dst_addr,\r\nunsigned long src_addr,\r\nstruct page **pagep)\r\n{\r\nstruct inode *inode = file_inode(dst_vma->vm_file);\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\r\nstruct address_space *mapping = inode->i_mapping;\r\ngfp_t gfp = mapping_gfp_mask(mapping);\r\npgoff_t pgoff = linear_page_index(dst_vma, dst_addr);\r\nstruct mem_cgroup *memcg;\r\nspinlock_t *ptl;\r\nvoid *page_kaddr;\r\nstruct page *page;\r\npte_t _dst_pte, *dst_pte;\r\nint ret;\r\nret = -ENOMEM;\r\nif (shmem_acct_block(info->flags, 1))\r\ngoto out;\r\nif (sbinfo->max_blocks) {\r\nif (percpu_counter_compare(&sbinfo->used_blocks,\r\nsbinfo->max_blocks) >= 0)\r\ngoto out_unacct_blocks;\r\npercpu_counter_inc(&sbinfo->used_blocks);\r\n}\r\nif (!*pagep) {\r\npage = shmem_alloc_page(gfp, info, pgoff);\r\nif (!page)\r\ngoto out_dec_used_blocks;\r\npage_kaddr = kmap_atomic(page);\r\nret = copy_from_user(page_kaddr, (const void __user *)src_addr,\r\nPAGE_SIZE);\r\nkunmap_atomic(page_kaddr);\r\nif (unlikely(ret)) {\r\n*pagep = page;\r\nif (sbinfo->max_blocks)\r\npercpu_counter_add(&sbinfo->used_blocks, -1);\r\nshmem_unacct_blocks(info->flags, 1);\r\nreturn -EFAULT;\r\n}\r\n} else {\r\npage = *pagep;\r\n*pagep = NULL;\r\n}\r\nVM_BUG_ON(PageLocked(page) || PageSwapBacked(page));\r\n__SetPageLocked(page);\r\n__SetPageSwapBacked(page);\r\n__SetPageUptodate(page);\r\nret = mem_cgroup_try_charge(page, dst_mm, gfp, &memcg, false);\r\nif (ret)\r\ngoto out_release;\r\nret = radix_tree_maybe_preload(gfp & GFP_RECLAIM_MASK);\r\nif (!ret) {\r\nret = shmem_add_to_page_cache(page, mapping, pgoff, NULL);\r\nradix_tree_preload_end();\r\n}\r\nif (ret)\r\ngoto out_release_uncharge;\r\nmem_cgroup_commit_charge(page, memcg, false, false);\r\n_dst_pte = mk_pte(page, dst_vma->vm_page_prot);\r\nif (dst_vma->vm_flags & VM_WRITE)\r\n_dst_pte = pte_mkwrite(pte_mkdirty(_dst_pte));\r\nret = -EEXIST;\r\ndst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\r\nif (!pte_none(*dst_pte))\r\ngoto out_release_uncharge_unlock;\r\nlru_cache_add_anon(page);\r\nspin_lock(&info->lock);\r\ninfo->alloced++;\r\ninode->i_blocks += BLOCKS_PER_PAGE;\r\nshmem_recalc_inode(inode);\r\nspin_unlock(&info->lock);\r\ninc_mm_counter(dst_mm, mm_counter_file(page));\r\npage_add_file_rmap(page, false);\r\nset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\r\nupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\r\nunlock_page(page);\r\npte_unmap_unlock(dst_pte, ptl);\r\nret = 0;\r\nout:\r\nreturn ret;\r\nout_release_uncharge_unlock:\r\npte_unmap_unlock(dst_pte, ptl);\r\nout_release_uncharge:\r\nmem_cgroup_cancel_charge(page, memcg, false);\r\nout_release:\r\nunlock_page(page);\r\nput_page(page);\r\nout_dec_used_blocks:\r\nif (sbinfo->max_blocks)\r\npercpu_counter_add(&sbinfo->used_blocks, -1);\r\nout_unacct_blocks:\r\nshmem_unacct_blocks(info->flags, 1);\r\ngoto out;\r\n}\r\nstatic int\r\nshmem_write_begin(struct file *file, struct address_space *mapping,\r\nloff_t pos, unsigned len, unsigned flags,\r\nstruct page **pagep, void **fsdata)\r\n{\r\nstruct inode *inode = mapping->host;\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\npgoff_t index = pos >> PAGE_SHIFT;\r\nif (unlikely(info->seals & (F_SEAL_WRITE | F_SEAL_GROW))) {\r\nif (info->seals & F_SEAL_WRITE)\r\nreturn -EPERM;\r\nif ((info->seals & F_SEAL_GROW) && pos + len > inode->i_size)\r\nreturn -EPERM;\r\n}\r\nreturn shmem_getpage(inode, index, pagep, SGP_WRITE);\r\n}\r\nstatic int\r\nshmem_write_end(struct file *file, struct address_space *mapping,\r\nloff_t pos, unsigned len, unsigned copied,\r\nstruct page *page, void *fsdata)\r\n{\r\nstruct inode *inode = mapping->host;\r\nif (pos + copied > inode->i_size)\r\ni_size_write(inode, pos + copied);\r\nif (!PageUptodate(page)) {\r\nstruct page *head = compound_head(page);\r\nif (PageTransCompound(page)) {\r\nint i;\r\nfor (i = 0; i < HPAGE_PMD_NR; i++) {\r\nif (head + i == page)\r\ncontinue;\r\nclear_highpage(head + i);\r\nflush_dcache_page(head + i);\r\n}\r\n}\r\nif (copied < PAGE_SIZE) {\r\nunsigned from = pos & (PAGE_SIZE - 1);\r\nzero_user_segments(page, 0, from,\r\nfrom + copied, PAGE_SIZE);\r\n}\r\nSetPageUptodate(head);\r\n}\r\nset_page_dirty(page);\r\nunlock_page(page);\r\nput_page(page);\r\nreturn copied;\r\n}\r\nstatic ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct inode *inode = file_inode(file);\r\nstruct address_space *mapping = inode->i_mapping;\r\npgoff_t index;\r\nunsigned long offset;\r\nenum sgp_type sgp = SGP_READ;\r\nint error = 0;\r\nssize_t retval = 0;\r\nloff_t *ppos = &iocb->ki_pos;\r\nif (!iter_is_iovec(to))\r\nsgp = SGP_CACHE;\r\nindex = *ppos >> PAGE_SHIFT;\r\noffset = *ppos & ~PAGE_MASK;\r\nfor (;;) {\r\nstruct page *page = NULL;\r\npgoff_t end_index;\r\nunsigned long nr, ret;\r\nloff_t i_size = i_size_read(inode);\r\nend_index = i_size >> PAGE_SHIFT;\r\nif (index > end_index)\r\nbreak;\r\nif (index == end_index) {\r\nnr = i_size & ~PAGE_MASK;\r\nif (nr <= offset)\r\nbreak;\r\n}\r\nerror = shmem_getpage(inode, index, &page, sgp);\r\nif (error) {\r\nif (error == -EINVAL)\r\nerror = 0;\r\nbreak;\r\n}\r\nif (page) {\r\nif (sgp == SGP_CACHE)\r\nset_page_dirty(page);\r\nunlock_page(page);\r\n}\r\nnr = PAGE_SIZE;\r\ni_size = i_size_read(inode);\r\nend_index = i_size >> PAGE_SHIFT;\r\nif (index == end_index) {\r\nnr = i_size & ~PAGE_MASK;\r\nif (nr <= offset) {\r\nif (page)\r\nput_page(page);\r\nbreak;\r\n}\r\n}\r\nnr -= offset;\r\nif (page) {\r\nif (mapping_writably_mapped(mapping))\r\nflush_dcache_page(page);\r\nif (!offset)\r\nmark_page_accessed(page);\r\n} else {\r\npage = ZERO_PAGE(0);\r\nget_page(page);\r\n}\r\nret = copy_page_to_iter(page, offset, nr, to);\r\nretval += ret;\r\noffset += ret;\r\nindex += offset >> PAGE_SHIFT;\r\noffset &= ~PAGE_MASK;\r\nput_page(page);\r\nif (!iov_iter_count(to))\r\nbreak;\r\nif (ret < nr) {\r\nerror = -EFAULT;\r\nbreak;\r\n}\r\ncond_resched();\r\n}\r\n*ppos = ((loff_t) index << PAGE_SHIFT) + offset;\r\nfile_accessed(file);\r\nreturn retval ? retval : error;\r\n}\r\nstatic pgoff_t shmem_seek_hole_data(struct address_space *mapping,\r\npgoff_t index, pgoff_t end, int whence)\r\n{\r\nstruct page *page;\r\nstruct pagevec pvec;\r\npgoff_t indices[PAGEVEC_SIZE];\r\nbool done = false;\r\nint i;\r\npagevec_init(&pvec, 0);\r\npvec.nr = 1;\r\nwhile (!done) {\r\npvec.nr = find_get_entries(mapping, index,\r\npvec.nr, pvec.pages, indices);\r\nif (!pvec.nr) {\r\nif (whence == SEEK_DATA)\r\nindex = end;\r\nbreak;\r\n}\r\nfor (i = 0; i < pvec.nr; i++, index++) {\r\nif (index < indices[i]) {\r\nif (whence == SEEK_HOLE) {\r\ndone = true;\r\nbreak;\r\n}\r\nindex = indices[i];\r\n}\r\npage = pvec.pages[i];\r\nif (page && !radix_tree_exceptional_entry(page)) {\r\nif (!PageUptodate(page))\r\npage = NULL;\r\n}\r\nif (index >= end ||\r\n(page && whence == SEEK_DATA) ||\r\n(!page && whence == SEEK_HOLE)) {\r\ndone = true;\r\nbreak;\r\n}\r\n}\r\npagevec_remove_exceptionals(&pvec);\r\npagevec_release(&pvec);\r\npvec.nr = PAGEVEC_SIZE;\r\ncond_resched();\r\n}\r\nreturn index;\r\n}\r\nstatic loff_t shmem_file_llseek(struct file *file, loff_t offset, int whence)\r\n{\r\nstruct address_space *mapping = file->f_mapping;\r\nstruct inode *inode = mapping->host;\r\npgoff_t start, end;\r\nloff_t new_offset;\r\nif (whence != SEEK_DATA && whence != SEEK_HOLE)\r\nreturn generic_file_llseek_size(file, offset, whence,\r\nMAX_LFS_FILESIZE, i_size_read(inode));\r\ninode_lock(inode);\r\nif (offset < 0)\r\noffset = -EINVAL;\r\nelse if (offset >= inode->i_size)\r\noffset = -ENXIO;\r\nelse {\r\nstart = offset >> PAGE_SHIFT;\r\nend = (inode->i_size + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nnew_offset = shmem_seek_hole_data(mapping, start, end, whence);\r\nnew_offset <<= PAGE_SHIFT;\r\nif (new_offset > offset) {\r\nif (new_offset < inode->i_size)\r\noffset = new_offset;\r\nelse if (whence == SEEK_DATA)\r\noffset = -ENXIO;\r\nelse\r\noffset = inode->i_size;\r\n}\r\n}\r\nif (offset >= 0)\r\noffset = vfs_setpos(file, offset, MAX_LFS_FILESIZE);\r\ninode_unlock(inode);\r\nreturn offset;\r\n}\r\nstatic void shmem_tag_pins(struct address_space *mapping)\r\n{\r\nstruct radix_tree_iter iter;\r\nvoid **slot;\r\npgoff_t start;\r\nstruct page *page;\r\nlru_add_drain();\r\nstart = 0;\r\nrcu_read_lock();\r\nradix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {\r\npage = radix_tree_deref_slot(slot);\r\nif (!page || radix_tree_exception(page)) {\r\nif (radix_tree_deref_retry(page)) {\r\nslot = radix_tree_iter_retry(&iter);\r\ncontinue;\r\n}\r\n} else if (page_count(page) - page_mapcount(page) > 1) {\r\nspin_lock_irq(&mapping->tree_lock);\r\nradix_tree_tag_set(&mapping->page_tree, iter.index,\r\nSHMEM_TAG_PINNED);\r\nspin_unlock_irq(&mapping->tree_lock);\r\n}\r\nif (need_resched()) {\r\nslot = radix_tree_iter_resume(slot, &iter);\r\ncond_resched_rcu();\r\n}\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic int shmem_wait_for_pins(struct address_space *mapping)\r\n{\r\nstruct radix_tree_iter iter;\r\nvoid **slot;\r\npgoff_t start;\r\nstruct page *page;\r\nint error, scan;\r\nshmem_tag_pins(mapping);\r\nerror = 0;\r\nfor (scan = 0; scan <= LAST_SCAN; scan++) {\r\nif (!radix_tree_tagged(&mapping->page_tree, SHMEM_TAG_PINNED))\r\nbreak;\r\nif (!scan)\r\nlru_add_drain_all();\r\nelse if (schedule_timeout_killable((HZ << scan) / 200))\r\nscan = LAST_SCAN;\r\nstart = 0;\r\nrcu_read_lock();\r\nradix_tree_for_each_tagged(slot, &mapping->page_tree, &iter,\r\nstart, SHMEM_TAG_PINNED) {\r\npage = radix_tree_deref_slot(slot);\r\nif (radix_tree_exception(page)) {\r\nif (radix_tree_deref_retry(page)) {\r\nslot = radix_tree_iter_retry(&iter);\r\ncontinue;\r\n}\r\npage = NULL;\r\n}\r\nif (page &&\r\npage_count(page) - page_mapcount(page) != 1) {\r\nif (scan < LAST_SCAN)\r\ngoto continue_resched;\r\nerror = -EBUSY;\r\n}\r\nspin_lock_irq(&mapping->tree_lock);\r\nradix_tree_tag_clear(&mapping->page_tree,\r\niter.index, SHMEM_TAG_PINNED);\r\nspin_unlock_irq(&mapping->tree_lock);\r\ncontinue_resched:\r\nif (need_resched()) {\r\nslot = radix_tree_iter_resume(slot, &iter);\r\ncond_resched_rcu();\r\n}\r\n}\r\nrcu_read_unlock();\r\n}\r\nreturn error;\r\n}\r\nint shmem_add_seals(struct file *file, unsigned int seals)\r\n{\r\nstruct inode *inode = file_inode(file);\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nint error;\r\nif (file->f_op != &shmem_file_operations)\r\nreturn -EINVAL;\r\nif (!(file->f_mode & FMODE_WRITE))\r\nreturn -EPERM;\r\nif (seals & ~(unsigned int)F_ALL_SEALS)\r\nreturn -EINVAL;\r\ninode_lock(inode);\r\nif (info->seals & F_SEAL_SEAL) {\r\nerror = -EPERM;\r\ngoto unlock;\r\n}\r\nif ((seals & F_SEAL_WRITE) && !(info->seals & F_SEAL_WRITE)) {\r\nerror = mapping_deny_writable(file->f_mapping);\r\nif (error)\r\ngoto unlock;\r\nerror = shmem_wait_for_pins(file->f_mapping);\r\nif (error) {\r\nmapping_allow_writable(file->f_mapping);\r\ngoto unlock;\r\n}\r\n}\r\ninfo->seals |= seals;\r\nerror = 0;\r\nunlock:\r\ninode_unlock(inode);\r\nreturn error;\r\n}\r\nint shmem_get_seals(struct file *file)\r\n{\r\nif (file->f_op != &shmem_file_operations)\r\nreturn -EINVAL;\r\nreturn SHMEM_I(file_inode(file))->seals;\r\n}\r\nlong shmem_fcntl(struct file *file, unsigned int cmd, unsigned long arg)\r\n{\r\nlong error;\r\nswitch (cmd) {\r\ncase F_ADD_SEALS:\r\nif (arg > UINT_MAX)\r\nreturn -EINVAL;\r\nerror = shmem_add_seals(file, arg);\r\nbreak;\r\ncase F_GET_SEALS:\r\nerror = shmem_get_seals(file);\r\nbreak;\r\ndefault:\r\nerror = -EINVAL;\r\nbreak;\r\n}\r\nreturn error;\r\n}\r\nstatic long shmem_fallocate(struct file *file, int mode, loff_t offset,\r\nloff_t len)\r\n{\r\nstruct inode *inode = file_inode(file);\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nstruct shmem_falloc shmem_falloc;\r\npgoff_t start, index, end;\r\nint error;\r\nif (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))\r\nreturn -EOPNOTSUPP;\r\ninode_lock(inode);\r\nif (mode & FALLOC_FL_PUNCH_HOLE) {\r\nstruct address_space *mapping = file->f_mapping;\r\nloff_t unmap_start = round_up(offset, PAGE_SIZE);\r\nloff_t unmap_end = round_down(offset + len, PAGE_SIZE) - 1;\r\nDECLARE_WAIT_QUEUE_HEAD_ONSTACK(shmem_falloc_waitq);\r\nif (info->seals & F_SEAL_WRITE) {\r\nerror = -EPERM;\r\ngoto out;\r\n}\r\nshmem_falloc.waitq = &shmem_falloc_waitq;\r\nshmem_falloc.start = unmap_start >> PAGE_SHIFT;\r\nshmem_falloc.next = (unmap_end + 1) >> PAGE_SHIFT;\r\nspin_lock(&inode->i_lock);\r\ninode->i_private = &shmem_falloc;\r\nspin_unlock(&inode->i_lock);\r\nif ((u64)unmap_end > (u64)unmap_start)\r\nunmap_mapping_range(mapping, unmap_start,\r\n1 + unmap_end - unmap_start, 0);\r\nshmem_truncate_range(inode, offset, offset + len - 1);\r\nspin_lock(&inode->i_lock);\r\ninode->i_private = NULL;\r\nwake_up_all(&shmem_falloc_waitq);\r\nWARN_ON_ONCE(!list_empty(&shmem_falloc_waitq.task_list));\r\nspin_unlock(&inode->i_lock);\r\nerror = 0;\r\ngoto out;\r\n}\r\nerror = inode_newsize_ok(inode, offset + len);\r\nif (error)\r\ngoto out;\r\nif ((info->seals & F_SEAL_GROW) && offset + len > inode->i_size) {\r\nerror = -EPERM;\r\ngoto out;\r\n}\r\nstart = offset >> PAGE_SHIFT;\r\nend = (offset + len + PAGE_SIZE - 1) >> PAGE_SHIFT;\r\nif (sbinfo->max_blocks && end - start > sbinfo->max_blocks) {\r\nerror = -ENOSPC;\r\ngoto out;\r\n}\r\nshmem_falloc.waitq = NULL;\r\nshmem_falloc.start = start;\r\nshmem_falloc.next = start;\r\nshmem_falloc.nr_falloced = 0;\r\nshmem_falloc.nr_unswapped = 0;\r\nspin_lock(&inode->i_lock);\r\ninode->i_private = &shmem_falloc;\r\nspin_unlock(&inode->i_lock);\r\nfor (index = start; index < end; index++) {\r\nstruct page *page;\r\nif (signal_pending(current))\r\nerror = -EINTR;\r\nelse if (shmem_falloc.nr_unswapped > shmem_falloc.nr_falloced)\r\nerror = -ENOMEM;\r\nelse\r\nerror = shmem_getpage(inode, index, &page, SGP_FALLOC);\r\nif (error) {\r\nif (index > start) {\r\nshmem_undo_range(inode,\r\n(loff_t)start << PAGE_SHIFT,\r\n((loff_t)index << PAGE_SHIFT) - 1, true);\r\n}\r\ngoto undone;\r\n}\r\nshmem_falloc.next++;\r\nif (!PageUptodate(page))\r\nshmem_falloc.nr_falloced++;\r\nset_page_dirty(page);\r\nunlock_page(page);\r\nput_page(page);\r\ncond_resched();\r\n}\r\nif (!(mode & FALLOC_FL_KEEP_SIZE) && offset + len > inode->i_size)\r\ni_size_write(inode, offset + len);\r\ninode->i_ctime = current_time(inode);\r\nundone:\r\nspin_lock(&inode->i_lock);\r\ninode->i_private = NULL;\r\nspin_unlock(&inode->i_lock);\r\nout:\r\ninode_unlock(inode);\r\nreturn error;\r\n}\r\nstatic int shmem_statfs(struct dentry *dentry, struct kstatfs *buf)\r\n{\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(dentry->d_sb);\r\nbuf->f_type = TMPFS_MAGIC;\r\nbuf->f_bsize = PAGE_SIZE;\r\nbuf->f_namelen = NAME_MAX;\r\nif (sbinfo->max_blocks) {\r\nbuf->f_blocks = sbinfo->max_blocks;\r\nbuf->f_bavail =\r\nbuf->f_bfree = sbinfo->max_blocks -\r\npercpu_counter_sum(&sbinfo->used_blocks);\r\n}\r\nif (sbinfo->max_inodes) {\r\nbuf->f_files = sbinfo->max_inodes;\r\nbuf->f_ffree = sbinfo->free_inodes;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nshmem_mknod(struct inode *dir, struct dentry *dentry, umode_t mode, dev_t dev)\r\n{\r\nstruct inode *inode;\r\nint error = -ENOSPC;\r\ninode = shmem_get_inode(dir->i_sb, dir, mode, dev, VM_NORESERVE);\r\nif (inode) {\r\nerror = simple_acl_create(dir, inode);\r\nif (error)\r\ngoto out_iput;\r\nerror = security_inode_init_security(inode, dir,\r\n&dentry->d_name,\r\nshmem_initxattrs, NULL);\r\nif (error && error != -EOPNOTSUPP)\r\ngoto out_iput;\r\nerror = 0;\r\ndir->i_size += BOGO_DIRENT_SIZE;\r\ndir->i_ctime = dir->i_mtime = current_time(dir);\r\nd_instantiate(dentry, inode);\r\ndget(dentry);\r\n}\r\nreturn error;\r\nout_iput:\r\niput(inode);\r\nreturn error;\r\n}\r\nstatic int\r\nshmem_tmpfile(struct inode *dir, struct dentry *dentry, umode_t mode)\r\n{\r\nstruct inode *inode;\r\nint error = -ENOSPC;\r\ninode = shmem_get_inode(dir->i_sb, dir, mode, 0, VM_NORESERVE);\r\nif (inode) {\r\nerror = security_inode_init_security(inode, dir,\r\nNULL,\r\nshmem_initxattrs, NULL);\r\nif (error && error != -EOPNOTSUPP)\r\ngoto out_iput;\r\nerror = simple_acl_create(dir, inode);\r\nif (error)\r\ngoto out_iput;\r\nd_tmpfile(dentry, inode);\r\n}\r\nreturn error;\r\nout_iput:\r\niput(inode);\r\nreturn error;\r\n}\r\nstatic int shmem_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)\r\n{\r\nint error;\r\nif ((error = shmem_mknod(dir, dentry, mode | S_IFDIR, 0)))\r\nreturn error;\r\ninc_nlink(dir);\r\nreturn 0;\r\n}\r\nstatic int shmem_create(struct inode *dir, struct dentry *dentry, umode_t mode,\r\nbool excl)\r\n{\r\nreturn shmem_mknod(dir, dentry, mode | S_IFREG, 0);\r\n}\r\nstatic int shmem_link(struct dentry *old_dentry, struct inode *dir, struct dentry *dentry)\r\n{\r\nstruct inode *inode = d_inode(old_dentry);\r\nint ret;\r\nret = shmem_reserve_inode(inode->i_sb);\r\nif (ret)\r\ngoto out;\r\ndir->i_size += BOGO_DIRENT_SIZE;\r\ninode->i_ctime = dir->i_ctime = dir->i_mtime = current_time(inode);\r\ninc_nlink(inode);\r\nihold(inode);\r\ndget(dentry);\r\nd_instantiate(dentry, inode);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int shmem_unlink(struct inode *dir, struct dentry *dentry)\r\n{\r\nstruct inode *inode = d_inode(dentry);\r\nif (inode->i_nlink > 1 && !S_ISDIR(inode->i_mode))\r\nshmem_free_inode(inode->i_sb);\r\ndir->i_size -= BOGO_DIRENT_SIZE;\r\ninode->i_ctime = dir->i_ctime = dir->i_mtime = current_time(inode);\r\ndrop_nlink(inode);\r\ndput(dentry);\r\nreturn 0;\r\n}\r\nstatic int shmem_rmdir(struct inode *dir, struct dentry *dentry)\r\n{\r\nif (!simple_empty(dentry))\r\nreturn -ENOTEMPTY;\r\ndrop_nlink(d_inode(dentry));\r\ndrop_nlink(dir);\r\nreturn shmem_unlink(dir, dentry);\r\n}\r\nstatic int shmem_exchange(struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry)\r\n{\r\nbool old_is_dir = d_is_dir(old_dentry);\r\nbool new_is_dir = d_is_dir(new_dentry);\r\nif (old_dir != new_dir && old_is_dir != new_is_dir) {\r\nif (old_is_dir) {\r\ndrop_nlink(old_dir);\r\ninc_nlink(new_dir);\r\n} else {\r\ndrop_nlink(new_dir);\r\ninc_nlink(old_dir);\r\n}\r\n}\r\nold_dir->i_ctime = old_dir->i_mtime =\r\nnew_dir->i_ctime = new_dir->i_mtime =\r\nd_inode(old_dentry)->i_ctime =\r\nd_inode(new_dentry)->i_ctime = current_time(old_dir);\r\nreturn 0;\r\n}\r\nstatic int shmem_whiteout(struct inode *old_dir, struct dentry *old_dentry)\r\n{\r\nstruct dentry *whiteout;\r\nint error;\r\nwhiteout = d_alloc(old_dentry->d_parent, &old_dentry->d_name);\r\nif (!whiteout)\r\nreturn -ENOMEM;\r\nerror = shmem_mknod(old_dir, whiteout,\r\nS_IFCHR | WHITEOUT_MODE, WHITEOUT_DEV);\r\ndput(whiteout);\r\nif (error)\r\nreturn error;\r\nd_rehash(whiteout);\r\nreturn 0;\r\n}\r\nstatic int shmem_rename2(struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry, unsigned int flags)\r\n{\r\nstruct inode *inode = d_inode(old_dentry);\r\nint they_are_dirs = S_ISDIR(inode->i_mode);\r\nif (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))\r\nreturn -EINVAL;\r\nif (flags & RENAME_EXCHANGE)\r\nreturn shmem_exchange(old_dir, old_dentry, new_dir, new_dentry);\r\nif (!simple_empty(new_dentry))\r\nreturn -ENOTEMPTY;\r\nif (flags & RENAME_WHITEOUT) {\r\nint error;\r\nerror = shmem_whiteout(old_dir, old_dentry);\r\nif (error)\r\nreturn error;\r\n}\r\nif (d_really_is_positive(new_dentry)) {\r\n(void) shmem_unlink(new_dir, new_dentry);\r\nif (they_are_dirs) {\r\ndrop_nlink(d_inode(new_dentry));\r\ndrop_nlink(old_dir);\r\n}\r\n} else if (they_are_dirs) {\r\ndrop_nlink(old_dir);\r\ninc_nlink(new_dir);\r\n}\r\nold_dir->i_size -= BOGO_DIRENT_SIZE;\r\nnew_dir->i_size += BOGO_DIRENT_SIZE;\r\nold_dir->i_ctime = old_dir->i_mtime =\r\nnew_dir->i_ctime = new_dir->i_mtime =\r\ninode->i_ctime = current_time(old_dir);\r\nreturn 0;\r\n}\r\nstatic int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *symname)\r\n{\r\nint error;\r\nint len;\r\nstruct inode *inode;\r\nstruct page *page;\r\nstruct shmem_inode_info *info;\r\nlen = strlen(symname) + 1;\r\nif (len > PAGE_SIZE)\r\nreturn -ENAMETOOLONG;\r\ninode = shmem_get_inode(dir->i_sb, dir, S_IFLNK|S_IRWXUGO, 0, VM_NORESERVE);\r\nif (!inode)\r\nreturn -ENOSPC;\r\nerror = security_inode_init_security(inode, dir, &dentry->d_name,\r\nshmem_initxattrs, NULL);\r\nif (error) {\r\nif (error != -EOPNOTSUPP) {\r\niput(inode);\r\nreturn error;\r\n}\r\nerror = 0;\r\n}\r\ninfo = SHMEM_I(inode);\r\ninode->i_size = len-1;\r\nif (len <= SHORT_SYMLINK_LEN) {\r\ninode->i_link = kmemdup(symname, len, GFP_KERNEL);\r\nif (!inode->i_link) {\r\niput(inode);\r\nreturn -ENOMEM;\r\n}\r\ninode->i_op = &shmem_short_symlink_operations;\r\n} else {\r\ninode_nohighmem(inode);\r\nerror = shmem_getpage(inode, 0, &page, SGP_WRITE);\r\nif (error) {\r\niput(inode);\r\nreturn error;\r\n}\r\ninode->i_mapping->a_ops = &shmem_aops;\r\ninode->i_op = &shmem_symlink_inode_operations;\r\nmemcpy(page_address(page), symname, len);\r\nSetPageUptodate(page);\r\nset_page_dirty(page);\r\nunlock_page(page);\r\nput_page(page);\r\n}\r\ndir->i_size += BOGO_DIRENT_SIZE;\r\ndir->i_ctime = dir->i_mtime = current_time(dir);\r\nd_instantiate(dentry, inode);\r\ndget(dentry);\r\nreturn 0;\r\n}\r\nstatic void shmem_put_link(void *arg)\r\n{\r\nmark_page_accessed(arg);\r\nput_page(arg);\r\n}\r\nstatic const char *shmem_get_link(struct dentry *dentry,\r\nstruct inode *inode,\r\nstruct delayed_call *done)\r\n{\r\nstruct page *page = NULL;\r\nint error;\r\nif (!dentry) {\r\npage = find_get_page(inode->i_mapping, 0);\r\nif (!page)\r\nreturn ERR_PTR(-ECHILD);\r\nif (!PageUptodate(page)) {\r\nput_page(page);\r\nreturn ERR_PTR(-ECHILD);\r\n}\r\n} else {\r\nerror = shmem_getpage(inode, 0, &page, SGP_READ);\r\nif (error)\r\nreturn ERR_PTR(error);\r\nunlock_page(page);\r\n}\r\nset_delayed_call(done, shmem_put_link, page);\r\nreturn page_address(page);\r\n}\r\nstatic int shmem_initxattrs(struct inode *inode,\r\nconst struct xattr *xattr_array,\r\nvoid *fs_info)\r\n{\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nconst struct xattr *xattr;\r\nstruct simple_xattr *new_xattr;\r\nsize_t len;\r\nfor (xattr = xattr_array; xattr->name != NULL; xattr++) {\r\nnew_xattr = simple_xattr_alloc(xattr->value, xattr->value_len);\r\nif (!new_xattr)\r\nreturn -ENOMEM;\r\nlen = strlen(xattr->name) + 1;\r\nnew_xattr->name = kmalloc(XATTR_SECURITY_PREFIX_LEN + len,\r\nGFP_KERNEL);\r\nif (!new_xattr->name) {\r\nkfree(new_xattr);\r\nreturn -ENOMEM;\r\n}\r\nmemcpy(new_xattr->name, XATTR_SECURITY_PREFIX,\r\nXATTR_SECURITY_PREFIX_LEN);\r\nmemcpy(new_xattr->name + XATTR_SECURITY_PREFIX_LEN,\r\nxattr->name, len);\r\nsimple_xattr_list_add(&info->xattrs, new_xattr);\r\n}\r\nreturn 0;\r\n}\r\nstatic int shmem_xattr_handler_get(const struct xattr_handler *handler,\r\nstruct dentry *unused, struct inode *inode,\r\nconst char *name, void *buffer, size_t size)\r\n{\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nname = xattr_full_name(handler, name);\r\nreturn simple_xattr_get(&info->xattrs, name, buffer, size);\r\n}\r\nstatic int shmem_xattr_handler_set(const struct xattr_handler *handler,\r\nstruct dentry *unused, struct inode *inode,\r\nconst char *name, const void *value,\r\nsize_t size, int flags)\r\n{\r\nstruct shmem_inode_info *info = SHMEM_I(inode);\r\nname = xattr_full_name(handler, name);\r\nreturn simple_xattr_set(&info->xattrs, name, value, size, flags);\r\n}\r\nstatic ssize_t shmem_listxattr(struct dentry *dentry, char *buffer, size_t size)\r\n{\r\nstruct shmem_inode_info *info = SHMEM_I(d_inode(dentry));\r\nreturn simple_xattr_list(d_inode(dentry), &info->xattrs, buffer, size);\r\n}\r\nstatic struct dentry *shmem_get_parent(struct dentry *child)\r\n{\r\nreturn ERR_PTR(-ESTALE);\r\n}\r\nstatic int shmem_match(struct inode *ino, void *vfh)\r\n{\r\n__u32 *fh = vfh;\r\n__u64 inum = fh[2];\r\ninum = (inum << 32) | fh[1];\r\nreturn ino->i_ino == inum && fh[0] == ino->i_generation;\r\n}\r\nstatic struct dentry *shmem_fh_to_dentry(struct super_block *sb,\r\nstruct fid *fid, int fh_len, int fh_type)\r\n{\r\nstruct inode *inode;\r\nstruct dentry *dentry = NULL;\r\nu64 inum;\r\nif (fh_len < 3)\r\nreturn NULL;\r\ninum = fid->raw[2];\r\ninum = (inum << 32) | fid->raw[1];\r\ninode = ilookup5(sb, (unsigned long)(inum + fid->raw[0]),\r\nshmem_match, fid->raw);\r\nif (inode) {\r\ndentry = d_find_alias(inode);\r\niput(inode);\r\n}\r\nreturn dentry;\r\n}\r\nstatic int shmem_encode_fh(struct inode *inode, __u32 *fh, int *len,\r\nstruct inode *parent)\r\n{\r\nif (*len < 3) {\r\n*len = 3;\r\nreturn FILEID_INVALID;\r\n}\r\nif (inode_unhashed(inode)) {\r\nstatic DEFINE_SPINLOCK(lock);\r\nspin_lock(&lock);\r\nif (inode_unhashed(inode))\r\n__insert_inode_hash(inode,\r\ninode->i_ino + inode->i_generation);\r\nspin_unlock(&lock);\r\n}\r\nfh[0] = inode->i_generation;\r\nfh[1] = inode->i_ino;\r\nfh[2] = ((__u64)inode->i_ino) >> 32;\r\n*len = 3;\r\nreturn 1;\r\n}\r\nstatic int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,\r\nbool remount)\r\n{\r\nchar *this_char, *value, *rest;\r\nstruct mempolicy *mpol = NULL;\r\nuid_t uid;\r\ngid_t gid;\r\nwhile (options != NULL) {\r\nthis_char = options;\r\nfor (;;) {\r\noptions = strchr(options, ',');\r\nif (options == NULL)\r\nbreak;\r\noptions++;\r\nif (!isdigit(*options)) {\r\noptions[-1] = '\0';\r\nbreak;\r\n}\r\n}\r\nif (!*this_char)\r\ncontinue;\r\nif ((value = strchr(this_char,'=')) != NULL) {\r\n*value++ = 0;\r\n} else {\r\npr_err("tmpfs: No value for mount option '%s'\n",\r\nthis_char);\r\ngoto error;\r\n}\r\nif (!strcmp(this_char,"size")) {\r\nunsigned long long size;\r\nsize = memparse(value,&rest);\r\nif (*rest == '%') {\r\nsize <<= PAGE_SHIFT;\r\nsize *= totalram_pages;\r\ndo_div(size, 100);\r\nrest++;\r\n}\r\nif (*rest)\r\ngoto bad_val;\r\nsbinfo->max_blocks =\r\nDIV_ROUND_UP(size, PAGE_SIZE);\r\n} else if (!strcmp(this_char,"nr_blocks")) {\r\nsbinfo->max_blocks = memparse(value, &rest);\r\nif (*rest)\r\ngoto bad_val;\r\n} else if (!strcmp(this_char,"nr_inodes")) {\r\nsbinfo->max_inodes = memparse(value, &rest);\r\nif (*rest)\r\ngoto bad_val;\r\n} else if (!strcmp(this_char,"mode")) {\r\nif (remount)\r\ncontinue;\r\nsbinfo->mode = simple_strtoul(value, &rest, 8) & 07777;\r\nif (*rest)\r\ngoto bad_val;\r\n} else if (!strcmp(this_char,"uid")) {\r\nif (remount)\r\ncontinue;\r\nuid = simple_strtoul(value, &rest, 0);\r\nif (*rest)\r\ngoto bad_val;\r\nsbinfo->uid = make_kuid(current_user_ns(), uid);\r\nif (!uid_valid(sbinfo->uid))\r\ngoto bad_val;\r\n} else if (!strcmp(this_char,"gid")) {\r\nif (remount)\r\ncontinue;\r\ngid = simple_strtoul(value, &rest, 0);\r\nif (*rest)\r\ngoto bad_val;\r\nsbinfo->gid = make_kgid(current_user_ns(), gid);\r\nif (!gid_valid(sbinfo->gid))\r\ngoto bad_val;\r\n#ifdef CONFIG_TRANSPARENT_HUGE_PAGECACHE\r\n} else if (!strcmp(this_char, "huge")) {\r\nint huge;\r\nhuge = shmem_parse_huge(value);\r\nif (huge < 0)\r\ngoto bad_val;\r\nif (!has_transparent_hugepage() &&\r\nhuge != SHMEM_HUGE_NEVER)\r\ngoto bad_val;\r\nsbinfo->huge = huge;\r\n#endif\r\n#ifdef CONFIG_NUMA\r\n} else if (!strcmp(this_char,"mpol")) {\r\nmpol_put(mpol);\r\nmpol = NULL;\r\nif (mpol_parse_str(value, &mpol))\r\ngoto bad_val;\r\n#endif\r\n} else {\r\npr_err("tmpfs: Bad mount option %s\n", this_char);\r\ngoto error;\r\n}\r\n}\r\nsbinfo->mpol = mpol;\r\nreturn 0;\r\nbad_val:\r\npr_err("tmpfs: Bad value '%s' for mount option '%s'\n",\r\nvalue, this_char);\r\nerror:\r\nmpol_put(mpol);\r\nreturn 1;\r\n}\r\nstatic int shmem_remount_fs(struct super_block *sb, int *flags, char *data)\r\n{\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\r\nstruct shmem_sb_info config = *sbinfo;\r\nunsigned long inodes;\r\nint error = -EINVAL;\r\nconfig.mpol = NULL;\r\nif (shmem_parse_options(data, &config, true))\r\nreturn error;\r\nspin_lock(&sbinfo->stat_lock);\r\ninodes = sbinfo->max_inodes - sbinfo->free_inodes;\r\nif (percpu_counter_compare(&sbinfo->used_blocks, config.max_blocks) > 0)\r\ngoto out;\r\nif (config.max_inodes < inodes)\r\ngoto out;\r\nif (config.max_blocks && !sbinfo->max_blocks)\r\ngoto out;\r\nif (config.max_inodes && !sbinfo->max_inodes)\r\ngoto out;\r\nerror = 0;\r\nsbinfo->huge = config.huge;\r\nsbinfo->max_blocks = config.max_blocks;\r\nsbinfo->max_inodes = config.max_inodes;\r\nsbinfo->free_inodes = config.max_inodes - inodes;\r\nif (config.mpol) {\r\nmpol_put(sbinfo->mpol);\r\nsbinfo->mpol = config.mpol;\r\n}\r\nout:\r\nspin_unlock(&sbinfo->stat_lock);\r\nreturn error;\r\n}\r\nstatic int shmem_show_options(struct seq_file *seq, struct dentry *root)\r\n{\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(root->d_sb);\r\nif (sbinfo->max_blocks != shmem_default_max_blocks())\r\nseq_printf(seq, ",size=%luk",\r\nsbinfo->max_blocks << (PAGE_SHIFT - 10));\r\nif (sbinfo->max_inodes != shmem_default_max_inodes())\r\nseq_printf(seq, ",nr_inodes=%lu", sbinfo->max_inodes);\r\nif (sbinfo->mode != (S_IRWXUGO | S_ISVTX))\r\nseq_printf(seq, ",mode=%03ho", sbinfo->mode);\r\nif (!uid_eq(sbinfo->uid, GLOBAL_ROOT_UID))\r\nseq_printf(seq, ",uid=%u",\r\nfrom_kuid_munged(&init_user_ns, sbinfo->uid));\r\nif (!gid_eq(sbinfo->gid, GLOBAL_ROOT_GID))\r\nseq_printf(seq, ",gid=%u",\r\nfrom_kgid_munged(&init_user_ns, sbinfo->gid));\r\n#ifdef CONFIG_TRANSPARENT_HUGE_PAGECACHE\r\nif (sbinfo->huge)\r\nseq_printf(seq, ",huge=%s", shmem_format_huge(sbinfo->huge));\r\n#endif\r\nshmem_show_mpol(seq, sbinfo->mpol);\r\nreturn 0;\r\n}\r\nstatic void shmem_put_super(struct super_block *sb)\r\n{\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(sb);\r\npercpu_counter_destroy(&sbinfo->used_blocks);\r\nmpol_put(sbinfo->mpol);\r\nkfree(sbinfo);\r\nsb->s_fs_info = NULL;\r\n}\r\nint shmem_fill_super(struct super_block *sb, void *data, int silent)\r\n{\r\nstruct inode *inode;\r\nstruct shmem_sb_info *sbinfo;\r\nint err = -ENOMEM;\r\nsbinfo = kzalloc(max((int)sizeof(struct shmem_sb_info),\r\nL1_CACHE_BYTES), GFP_KERNEL);\r\nif (!sbinfo)\r\nreturn -ENOMEM;\r\nsbinfo->mode = S_IRWXUGO | S_ISVTX;\r\nsbinfo->uid = current_fsuid();\r\nsbinfo->gid = current_fsgid();\r\nsb->s_fs_info = sbinfo;\r\n#ifdef CONFIG_TMPFS\r\nif (!(sb->s_flags & MS_KERNMOUNT)) {\r\nsbinfo->max_blocks = shmem_default_max_blocks();\r\nsbinfo->max_inodes = shmem_default_max_inodes();\r\nif (shmem_parse_options(data, sbinfo, false)) {\r\nerr = -EINVAL;\r\ngoto failed;\r\n}\r\n} else {\r\nsb->s_flags |= MS_NOUSER;\r\n}\r\nsb->s_export_op = &shmem_export_ops;\r\nsb->s_flags |= MS_NOSEC;\r\n#else\r\nsb->s_flags |= MS_NOUSER;\r\n#endif\r\nspin_lock_init(&sbinfo->stat_lock);\r\nif (percpu_counter_init(&sbinfo->used_blocks, 0, GFP_KERNEL))\r\ngoto failed;\r\nsbinfo->free_inodes = sbinfo->max_inodes;\r\nspin_lock_init(&sbinfo->shrinklist_lock);\r\nINIT_LIST_HEAD(&sbinfo->shrinklist);\r\nsb->s_maxbytes = MAX_LFS_FILESIZE;\r\nsb->s_blocksize = PAGE_SIZE;\r\nsb->s_blocksize_bits = PAGE_SHIFT;\r\nsb->s_magic = TMPFS_MAGIC;\r\nsb->s_op = &shmem_ops;\r\nsb->s_time_gran = 1;\r\n#ifdef CONFIG_TMPFS_XATTR\r\nsb->s_xattr = shmem_xattr_handlers;\r\n#endif\r\n#ifdef CONFIG_TMPFS_POSIX_ACL\r\nsb->s_flags |= MS_POSIXACL;\r\n#endif\r\ninode = shmem_get_inode(sb, NULL, S_IFDIR | sbinfo->mode, 0, VM_NORESERVE);\r\nif (!inode)\r\ngoto failed;\r\ninode->i_uid = sbinfo->uid;\r\ninode->i_gid = sbinfo->gid;\r\nsb->s_root = d_make_root(inode);\r\nif (!sb->s_root)\r\ngoto failed;\r\nreturn 0;\r\nfailed:\r\nshmem_put_super(sb);\r\nreturn err;\r\n}\r\nstatic struct inode *shmem_alloc_inode(struct super_block *sb)\r\n{\r\nstruct shmem_inode_info *info;\r\ninfo = kmem_cache_alloc(shmem_inode_cachep, GFP_KERNEL);\r\nif (!info)\r\nreturn NULL;\r\nreturn &info->vfs_inode;\r\n}\r\nstatic void shmem_destroy_callback(struct rcu_head *head)\r\n{\r\nstruct inode *inode = container_of(head, struct inode, i_rcu);\r\nif (S_ISLNK(inode->i_mode))\r\nkfree(inode->i_link);\r\nkmem_cache_free(shmem_inode_cachep, SHMEM_I(inode));\r\n}\r\nstatic void shmem_destroy_inode(struct inode *inode)\r\n{\r\nif (S_ISREG(inode->i_mode))\r\nmpol_free_shared_policy(&SHMEM_I(inode)->policy);\r\ncall_rcu(&inode->i_rcu, shmem_destroy_callback);\r\n}\r\nstatic void shmem_init_inode(void *foo)\r\n{\r\nstruct shmem_inode_info *info = foo;\r\ninode_init_once(&info->vfs_inode);\r\n}\r\nstatic int shmem_init_inodecache(void)\r\n{\r\nshmem_inode_cachep = kmem_cache_create("shmem_inode_cache",\r\nsizeof(struct shmem_inode_info),\r\n0, SLAB_PANIC|SLAB_ACCOUNT, shmem_init_inode);\r\nreturn 0;\r\n}\r\nstatic void shmem_destroy_inodecache(void)\r\n{\r\nkmem_cache_destroy(shmem_inode_cachep);\r\n}\r\nstatic struct dentry *shmem_mount(struct file_system_type *fs_type,\r\nint flags, const char *dev_name, void *data)\r\n{\r\nreturn mount_nodev(fs_type, flags, data, shmem_fill_super);\r\n}\r\nint __init shmem_init(void)\r\n{\r\nint error;\r\nif (shmem_inode_cachep)\r\nreturn 0;\r\nerror = shmem_init_inodecache();\r\nif (error)\r\ngoto out3;\r\nerror = register_filesystem(&shmem_fs_type);\r\nif (error) {\r\npr_err("Could not register tmpfs\n");\r\ngoto out2;\r\n}\r\nshm_mnt = kern_mount(&shmem_fs_type);\r\nif (IS_ERR(shm_mnt)) {\r\nerror = PTR_ERR(shm_mnt);\r\npr_err("Could not kern_mount tmpfs\n");\r\ngoto out1;\r\n}\r\n#ifdef CONFIG_TRANSPARENT_HUGE_PAGECACHE\r\nif (has_transparent_hugepage() && shmem_huge < SHMEM_HUGE_DENY)\r\nSHMEM_SB(shm_mnt->mnt_sb)->huge = shmem_huge;\r\nelse\r\nshmem_huge = 0;\r\n#endif\r\nreturn 0;\r\nout1:\r\nunregister_filesystem(&shmem_fs_type);\r\nout2:\r\nshmem_destroy_inodecache();\r\nout3:\r\nshm_mnt = ERR_PTR(error);\r\nreturn error;\r\n}\r\nstatic ssize_t shmem_enabled_show(struct kobject *kobj,\r\nstruct kobj_attribute *attr, char *buf)\r\n{\r\nint values[] = {\r\nSHMEM_HUGE_ALWAYS,\r\nSHMEM_HUGE_WITHIN_SIZE,\r\nSHMEM_HUGE_ADVISE,\r\nSHMEM_HUGE_NEVER,\r\nSHMEM_HUGE_DENY,\r\nSHMEM_HUGE_FORCE,\r\n};\r\nint i, count;\r\nfor (i = 0, count = 0; i < ARRAY_SIZE(values); i++) {\r\nconst char *fmt = shmem_huge == values[i] ? "[%s] " : "%s ";\r\ncount += sprintf(buf + count, fmt,\r\nshmem_format_huge(values[i]));\r\n}\r\nbuf[count - 1] = '\n';\r\nreturn count;\r\n}\r\nstatic ssize_t shmem_enabled_store(struct kobject *kobj,\r\nstruct kobj_attribute *attr, const char *buf, size_t count)\r\n{\r\nchar tmp[16];\r\nint huge;\r\nif (count + 1 > sizeof(tmp))\r\nreturn -EINVAL;\r\nmemcpy(tmp, buf, count);\r\ntmp[count] = '\0';\r\nif (count && tmp[count - 1] == '\n')\r\ntmp[count - 1] = '\0';\r\nhuge = shmem_parse_huge(tmp);\r\nif (huge == -EINVAL)\r\nreturn -EINVAL;\r\nif (!has_transparent_hugepage() &&\r\nhuge != SHMEM_HUGE_NEVER && huge != SHMEM_HUGE_DENY)\r\nreturn -EINVAL;\r\nshmem_huge = huge;\r\nif (shmem_huge < SHMEM_HUGE_DENY)\r\nSHMEM_SB(shm_mnt->mnt_sb)->huge = shmem_huge;\r\nreturn count;\r\n}\r\nbool shmem_huge_enabled(struct vm_area_struct *vma)\r\n{\r\nstruct inode *inode = file_inode(vma->vm_file);\r\nstruct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);\r\nloff_t i_size;\r\npgoff_t off;\r\nif (shmem_huge == SHMEM_HUGE_FORCE)\r\nreturn true;\r\nif (shmem_huge == SHMEM_HUGE_DENY)\r\nreturn false;\r\nswitch (sbinfo->huge) {\r\ncase SHMEM_HUGE_NEVER:\r\nreturn false;\r\ncase SHMEM_HUGE_ALWAYS:\r\nreturn true;\r\ncase SHMEM_HUGE_WITHIN_SIZE:\r\noff = round_up(vma->vm_pgoff, HPAGE_PMD_NR);\r\ni_size = round_up(i_size_read(inode), PAGE_SIZE);\r\nif (i_size >= HPAGE_PMD_SIZE &&\r\ni_size >> PAGE_SHIFT >= off)\r\nreturn true;\r\ncase SHMEM_HUGE_ADVISE:\r\nreturn (vma->vm_flags & VM_HUGEPAGE);\r\ndefault:\r\nVM_BUG_ON(1);\r\nreturn false;\r\n}\r\n}\r\nint __init shmem_init(void)\r\n{\r\nBUG_ON(register_filesystem(&shmem_fs_type) != 0);\r\nshm_mnt = kern_mount(&shmem_fs_type);\r\nBUG_ON(IS_ERR(shm_mnt));\r\nreturn 0;\r\n}\r\nint shmem_unuse(swp_entry_t swap, struct page *page)\r\n{\r\nreturn 0;\r\n}\r\nint shmem_lock(struct file *file, int lock, struct user_struct *user)\r\n{\r\nreturn 0;\r\n}\r\nvoid shmem_unlock_mapping(struct address_space *mapping)\r\n{\r\n}\r\nunsigned long shmem_get_unmapped_area(struct file *file,\r\nunsigned long addr, unsigned long len,\r\nunsigned long pgoff, unsigned long flags)\r\n{\r\nreturn current->mm->get_unmapped_area(file, addr, len, pgoff, flags);\r\n}\r\nvoid shmem_truncate_range(struct inode *inode, loff_t lstart, loff_t lend)\r\n{\r\ntruncate_inode_pages_range(inode->i_mapping, lstart, lend);\r\n}\r\nstatic struct file *__shmem_file_setup(const char *name, loff_t size,\r\nunsigned long flags, unsigned int i_flags)\r\n{\r\nstruct file *res;\r\nstruct inode *inode;\r\nstruct path path;\r\nstruct super_block *sb;\r\nstruct qstr this;\r\nif (IS_ERR(shm_mnt))\r\nreturn ERR_CAST(shm_mnt);\r\nif (size < 0 || size > MAX_LFS_FILESIZE)\r\nreturn ERR_PTR(-EINVAL);\r\nif (shmem_acct_size(flags, size))\r\nreturn ERR_PTR(-ENOMEM);\r\nres = ERR_PTR(-ENOMEM);\r\nthis.name = name;\r\nthis.len = strlen(name);\r\nthis.hash = 0;\r\nsb = shm_mnt->mnt_sb;\r\npath.mnt = mntget(shm_mnt);\r\npath.dentry = d_alloc_pseudo(sb, &this);\r\nif (!path.dentry)\r\ngoto put_memory;\r\nd_set_d_op(path.dentry, &anon_ops);\r\nres = ERR_PTR(-ENOSPC);\r\ninode = shmem_get_inode(sb, NULL, S_IFREG | S_IRWXUGO, 0, flags);\r\nif (!inode)\r\ngoto put_memory;\r\ninode->i_flags |= i_flags;\r\nd_instantiate(path.dentry, inode);\r\ninode->i_size = size;\r\nclear_nlink(inode);\r\nres = ERR_PTR(ramfs_nommu_expand_for_mapping(inode, size));\r\nif (IS_ERR(res))\r\ngoto put_path;\r\nres = alloc_file(&path, FMODE_WRITE | FMODE_READ,\r\n&shmem_file_operations);\r\nif (IS_ERR(res))\r\ngoto put_path;\r\nreturn res;\r\nput_memory:\r\nshmem_unacct_size(flags, size);\r\nput_path:\r\npath_put(&path);\r\nreturn res;\r\n}\r\nstruct file *shmem_kernel_file_setup(const char *name, loff_t size, unsigned long flags)\r\n{\r\nreturn __shmem_file_setup(name, size, flags, S_PRIVATE);\r\n}\r\nstruct file *shmem_file_setup(const char *name, loff_t size, unsigned long flags)\r\n{\r\nreturn __shmem_file_setup(name, size, flags, 0);\r\n}\r\nint shmem_zero_setup(struct vm_area_struct *vma)\r\n{\r\nstruct file *file;\r\nloff_t size = vma->vm_end - vma->vm_start;\r\nfile = __shmem_file_setup("dev/zero", size, vma->vm_flags, S_PRIVATE);\r\nif (IS_ERR(file))\r\nreturn PTR_ERR(file);\r\nif (vma->vm_file)\r\nfput(vma->vm_file);\r\nvma->vm_file = file;\r\nvma->vm_ops = &shmem_vm_ops;\r\nif (IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE) &&\r\n((vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK) <\r\n(vma->vm_end & HPAGE_PMD_MASK)) {\r\nkhugepaged_enter(vma, vma->vm_flags);\r\n}\r\nreturn 0;\r\n}\r\nstruct page *shmem_read_mapping_page_gfp(struct address_space *mapping,\r\npgoff_t index, gfp_t gfp)\r\n{\r\n#ifdef CONFIG_SHMEM\r\nstruct inode *inode = mapping->host;\r\nstruct page *page;\r\nint error;\r\nBUG_ON(mapping->a_ops != &shmem_aops);\r\nerror = shmem_getpage_gfp(inode, index, &page, SGP_CACHE,\r\ngfp, NULL, NULL, NULL);\r\nif (error)\r\npage = ERR_PTR(error);\r\nelse\r\nunlock_page(page);\r\nreturn page;\r\n#else\r\nreturn read_cache_page_gfp(mapping, index, gfp);\r\n#endif\r\n}
