static int rk_crypto_enable_clk(struct rk_crypto_info *dev)\r\n{\r\nint err;\r\nerr = clk_prepare_enable(dev->sclk);\r\nif (err) {\r\ndev_err(dev->dev, "[%s:%d], Couldn't enable clock sclk\n",\r\n__func__, __LINE__);\r\ngoto err_return;\r\n}\r\nerr = clk_prepare_enable(dev->aclk);\r\nif (err) {\r\ndev_err(dev->dev, "[%s:%d], Couldn't enable clock aclk\n",\r\n__func__, __LINE__);\r\ngoto err_aclk;\r\n}\r\nerr = clk_prepare_enable(dev->hclk);\r\nif (err) {\r\ndev_err(dev->dev, "[%s:%d], Couldn't enable clock hclk\n",\r\n__func__, __LINE__);\r\ngoto err_hclk;\r\n}\r\nerr = clk_prepare_enable(dev->dmaclk);\r\nif (err) {\r\ndev_err(dev->dev, "[%s:%d], Couldn't enable clock dmaclk\n",\r\n__func__, __LINE__);\r\ngoto err_dmaclk;\r\n}\r\nreturn err;\r\nerr_dmaclk:\r\nclk_disable_unprepare(dev->hclk);\r\nerr_hclk:\r\nclk_disable_unprepare(dev->aclk);\r\nerr_aclk:\r\nclk_disable_unprepare(dev->sclk);\r\nerr_return:\r\nreturn err;\r\n}\r\nstatic void rk_crypto_disable_clk(struct rk_crypto_info *dev)\r\n{\r\nclk_disable_unprepare(dev->dmaclk);\r\nclk_disable_unprepare(dev->hclk);\r\nclk_disable_unprepare(dev->aclk);\r\nclk_disable_unprepare(dev->sclk);\r\n}\r\nstatic int check_alignment(struct scatterlist *sg_src,\r\nstruct scatterlist *sg_dst,\r\nint align_mask)\r\n{\r\nint in, out, align;\r\nin = IS_ALIGNED((uint32_t)sg_src->offset, 4) &&\r\nIS_ALIGNED((uint32_t)sg_src->length, align_mask);\r\nif (!sg_dst)\r\nreturn in;\r\nout = IS_ALIGNED((uint32_t)sg_dst->offset, 4) &&\r\nIS_ALIGNED((uint32_t)sg_dst->length, align_mask);\r\nalign = in && out;\r\nreturn (align && (sg_src->length == sg_dst->length));\r\n}\r\nstatic int rk_load_data(struct rk_crypto_info *dev,\r\nstruct scatterlist *sg_src,\r\nstruct scatterlist *sg_dst)\r\n{\r\nunsigned int count;\r\ndev->aligned = dev->aligned ?\r\ncheck_alignment(sg_src, sg_dst, dev->align_size) :\r\ndev->aligned;\r\nif (dev->aligned) {\r\ncount = min(dev->left_bytes, sg_src->length);\r\ndev->left_bytes -= count;\r\nif (!dma_map_sg(dev->dev, sg_src, 1, DMA_TO_DEVICE)) {\r\ndev_err(dev->dev, "[%s:%d] dma_map_sg(src) error\n",\r\n__func__, __LINE__);\r\nreturn -EINVAL;\r\n}\r\ndev->addr_in = sg_dma_address(sg_src);\r\nif (sg_dst) {\r\nif (!dma_map_sg(dev->dev, sg_dst, 1, DMA_FROM_DEVICE)) {\r\ndev_err(dev->dev,\r\n"[%s:%d] dma_map_sg(dst) error\n",\r\n__func__, __LINE__);\r\ndma_unmap_sg(dev->dev, sg_src, 1,\r\nDMA_TO_DEVICE);\r\nreturn -EINVAL;\r\n}\r\ndev->addr_out = sg_dma_address(sg_dst);\r\n}\r\n} else {\r\ncount = (dev->left_bytes > PAGE_SIZE) ?\r\nPAGE_SIZE : dev->left_bytes;\r\nif (!sg_pcopy_to_buffer(dev->first, dev->nents,\r\ndev->addr_vir, count,\r\ndev->total - dev->left_bytes)) {\r\ndev_err(dev->dev, "[%s:%d] pcopy err\n",\r\n__func__, __LINE__);\r\nreturn -EINVAL;\r\n}\r\ndev->left_bytes -= count;\r\nsg_init_one(&dev->sg_tmp, dev->addr_vir, count);\r\nif (!dma_map_sg(dev->dev, &dev->sg_tmp, 1, DMA_TO_DEVICE)) {\r\ndev_err(dev->dev, "[%s:%d] dma_map_sg(sg_tmp) error\n",\r\n__func__, __LINE__);\r\nreturn -ENOMEM;\r\n}\r\ndev->addr_in = sg_dma_address(&dev->sg_tmp);\r\nif (sg_dst) {\r\nif (!dma_map_sg(dev->dev, &dev->sg_tmp, 1,\r\nDMA_FROM_DEVICE)) {\r\ndev_err(dev->dev,\r\n"[%s:%d] dma_map_sg(sg_tmp) error\n",\r\n__func__, __LINE__);\r\ndma_unmap_sg(dev->dev, &dev->sg_tmp, 1,\r\nDMA_TO_DEVICE);\r\nreturn -ENOMEM;\r\n}\r\ndev->addr_out = sg_dma_address(&dev->sg_tmp);\r\n}\r\n}\r\ndev->count = count;\r\nreturn 0;\r\n}\r\nstatic void rk_unload_data(struct rk_crypto_info *dev)\r\n{\r\nstruct scatterlist *sg_in, *sg_out;\r\nsg_in = dev->aligned ? dev->sg_src : &dev->sg_tmp;\r\ndma_unmap_sg(dev->dev, sg_in, 1, DMA_TO_DEVICE);\r\nif (dev->sg_dst) {\r\nsg_out = dev->aligned ? dev->sg_dst : &dev->sg_tmp;\r\ndma_unmap_sg(dev->dev, sg_out, 1, DMA_FROM_DEVICE);\r\n}\r\n}\r\nstatic irqreturn_t rk_crypto_irq_handle(int irq, void *dev_id)\r\n{\r\nstruct rk_crypto_info *dev = platform_get_drvdata(dev_id);\r\nu32 interrupt_status;\r\nint err = 0;\r\nspin_lock(&dev->lock);\r\ninterrupt_status = CRYPTO_READ(dev, RK_CRYPTO_INTSTS);\r\nCRYPTO_WRITE(dev, RK_CRYPTO_INTSTS, interrupt_status);\r\nif (interrupt_status & 0x0a) {\r\ndev_warn(dev->dev, "DMA Error\n");\r\nerr = -EFAULT;\r\n} else if (interrupt_status & 0x05) {\r\nerr = dev->update(dev);\r\n}\r\nif (err)\r\ndev->complete(dev, err);\r\nspin_unlock(&dev->lock);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void rk_crypto_tasklet_cb(unsigned long data)\r\n{\r\nstruct rk_crypto_info *dev = (struct rk_crypto_info *)data;\r\nstruct crypto_async_request *async_req, *backlog;\r\nunsigned long flags;\r\nint err = 0;\r\nspin_lock_irqsave(&dev->lock, flags);\r\nbacklog = crypto_get_backlog(&dev->queue);\r\nasync_req = crypto_dequeue_request(&dev->queue);\r\nspin_unlock_irqrestore(&dev->lock, flags);\r\nif (!async_req) {\r\ndev_err(dev->dev, "async_req is NULL !!\n");\r\nreturn;\r\n}\r\nif (backlog) {\r\nbacklog->complete(backlog, -EINPROGRESS);\r\nbacklog = NULL;\r\n}\r\nif (crypto_tfm_alg_type(async_req->tfm) == CRYPTO_ALG_TYPE_ABLKCIPHER)\r\ndev->ablk_req = ablkcipher_request_cast(async_req);\r\nelse\r\ndev->ahash_req = ahash_request_cast(async_req);\r\nerr = dev->start(dev);\r\nif (err)\r\ndev->complete(dev, err);\r\n}\r\nstatic int rk_crypto_register(struct rk_crypto_info *crypto_info)\r\n{\r\nunsigned int i, k;\r\nint err = 0;\r\nfor (i = 0; i < ARRAY_SIZE(rk_cipher_algs); i++) {\r\nrk_cipher_algs[i]->dev = crypto_info;\r\nif (rk_cipher_algs[i]->type == ALG_TYPE_CIPHER)\r\nerr = crypto_register_alg(\r\n&rk_cipher_algs[i]->alg.crypto);\r\nelse\r\nerr = crypto_register_ahash(\r\n&rk_cipher_algs[i]->alg.hash);\r\nif (err)\r\ngoto err_cipher_algs;\r\n}\r\nreturn 0;\r\nerr_cipher_algs:\r\nfor (k = 0; k < i; k++) {\r\nif (rk_cipher_algs[i]->type == ALG_TYPE_CIPHER)\r\ncrypto_unregister_alg(&rk_cipher_algs[k]->alg.crypto);\r\nelse\r\ncrypto_unregister_ahash(&rk_cipher_algs[i]->alg.hash);\r\n}\r\nreturn err;\r\n}\r\nstatic void rk_crypto_unregister(void)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < ARRAY_SIZE(rk_cipher_algs); i++) {\r\nif (rk_cipher_algs[i]->type == ALG_TYPE_CIPHER)\r\ncrypto_unregister_alg(&rk_cipher_algs[i]->alg.crypto);\r\nelse\r\ncrypto_unregister_ahash(&rk_cipher_algs[i]->alg.hash);\r\n}\r\n}\r\nstatic void rk_crypto_action(void *data)\r\n{\r\nstruct rk_crypto_info *crypto_info = data;\r\nreset_control_assert(crypto_info->rst);\r\n}\r\nstatic int rk_crypto_probe(struct platform_device *pdev)\r\n{\r\nstruct resource *res;\r\nstruct device *dev = &pdev->dev;\r\nstruct rk_crypto_info *crypto_info;\r\nint err = 0;\r\ncrypto_info = devm_kzalloc(&pdev->dev,\r\nsizeof(*crypto_info), GFP_KERNEL);\r\nif (!crypto_info) {\r\nerr = -ENOMEM;\r\ngoto err_crypto;\r\n}\r\ncrypto_info->rst = devm_reset_control_get(dev, "crypto-rst");\r\nif (IS_ERR(crypto_info->rst)) {\r\nerr = PTR_ERR(crypto_info->rst);\r\ngoto err_crypto;\r\n}\r\nreset_control_assert(crypto_info->rst);\r\nusleep_range(10, 20);\r\nreset_control_deassert(crypto_info->rst);\r\nerr = devm_add_action_or_reset(dev, rk_crypto_action, crypto_info);\r\nif (err)\r\ngoto err_crypto;\r\nspin_lock_init(&crypto_info->lock);\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\ncrypto_info->reg = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(crypto_info->reg)) {\r\nerr = PTR_ERR(crypto_info->reg);\r\ngoto err_crypto;\r\n}\r\ncrypto_info->aclk = devm_clk_get(&pdev->dev, "aclk");\r\nif (IS_ERR(crypto_info->aclk)) {\r\nerr = PTR_ERR(crypto_info->aclk);\r\ngoto err_crypto;\r\n}\r\ncrypto_info->hclk = devm_clk_get(&pdev->dev, "hclk");\r\nif (IS_ERR(crypto_info->hclk)) {\r\nerr = PTR_ERR(crypto_info->hclk);\r\ngoto err_crypto;\r\n}\r\ncrypto_info->sclk = devm_clk_get(&pdev->dev, "sclk");\r\nif (IS_ERR(crypto_info->sclk)) {\r\nerr = PTR_ERR(crypto_info->sclk);\r\ngoto err_crypto;\r\n}\r\ncrypto_info->dmaclk = devm_clk_get(&pdev->dev, "apb_pclk");\r\nif (IS_ERR(crypto_info->dmaclk)) {\r\nerr = PTR_ERR(crypto_info->dmaclk);\r\ngoto err_crypto;\r\n}\r\ncrypto_info->irq = platform_get_irq(pdev, 0);\r\nif (crypto_info->irq < 0) {\r\ndev_warn(crypto_info->dev,\r\n"control Interrupt is not available.\n");\r\nerr = crypto_info->irq;\r\ngoto err_crypto;\r\n}\r\nerr = devm_request_irq(&pdev->dev, crypto_info->irq,\r\nrk_crypto_irq_handle, IRQF_SHARED,\r\n"rk-crypto", pdev);\r\nif (err) {\r\ndev_err(crypto_info->dev, "irq request failed.\n");\r\ngoto err_crypto;\r\n}\r\ncrypto_info->dev = &pdev->dev;\r\nplatform_set_drvdata(pdev, crypto_info);\r\ntasklet_init(&crypto_info->crypto_tasklet,\r\nrk_crypto_tasklet_cb, (unsigned long)crypto_info);\r\ncrypto_init_queue(&crypto_info->queue, 50);\r\ncrypto_info->enable_clk = rk_crypto_enable_clk;\r\ncrypto_info->disable_clk = rk_crypto_disable_clk;\r\ncrypto_info->load_data = rk_load_data;\r\ncrypto_info->unload_data = rk_unload_data;\r\nerr = rk_crypto_register(crypto_info);\r\nif (err) {\r\ndev_err(dev, "err in register alg");\r\ngoto err_register_alg;\r\n}\r\ndev_info(dev, "Crypto Accelerator successfully registered\n");\r\nreturn 0;\r\nerr_register_alg:\r\ntasklet_kill(&crypto_info->crypto_tasklet);\r\nerr_crypto:\r\nreturn err;\r\n}\r\nstatic int rk_crypto_remove(struct platform_device *pdev)\r\n{\r\nstruct rk_crypto_info *crypto_tmp = platform_get_drvdata(pdev);\r\nrk_crypto_unregister();\r\ntasklet_kill(&crypto_tmp->crypto_tasklet);\r\nreturn 0;\r\n}
