static void *real_vmalloc_addr(void *x)\r\n{\r\nunsigned long addr = (unsigned long) x;\r\npte_t *p;\r\np = __find_linux_pte_or_hugepte(swapper_pg_dir, addr, NULL, NULL);\r\nif (!p || !pte_present(*p))\r\nreturn NULL;\r\naddr = (pte_pfn(*p) << PAGE_SHIFT) | (addr & ~PAGE_MASK);\r\nreturn __va(addr);\r\n}\r\nstatic int global_invalidates(struct kvm *kvm, unsigned long flags)\r\n{\r\nint global;\r\nint cpu;\r\nif (kvm->arch.online_vcores == 1 && local_paca->kvm_hstate.kvm_vcpu)\r\nglobal = 0;\r\nelse\r\nglobal = 1;\r\nif (!global) {\r\nsmp_wmb();\r\ncpumask_setall(&kvm->arch.need_tlb_flush);\r\ncpu = local_paca->kvm_hstate.kvm_vcore->pcpu;\r\nif (cpu_has_feature(CPU_FTR_ARCH_300))\r\ncpu = cpu_first_thread_sibling(cpu);\r\ncpumask_clear_cpu(cpu, &kvm->arch.need_tlb_flush);\r\n}\r\nreturn global;\r\n}\r\nvoid kvmppc_add_revmap_chain(struct kvm *kvm, struct revmap_entry *rev,\r\nunsigned long *rmap, long pte_index, int realmode)\r\n{\r\nstruct revmap_entry *head, *tail;\r\nunsigned long i;\r\nif (*rmap & KVMPPC_RMAP_PRESENT) {\r\ni = *rmap & KVMPPC_RMAP_INDEX;\r\nhead = &kvm->arch.hpt.rev[i];\r\nif (realmode)\r\nhead = real_vmalloc_addr(head);\r\ntail = &kvm->arch.hpt.rev[head->back];\r\nif (realmode)\r\ntail = real_vmalloc_addr(tail);\r\nrev->forw = i;\r\nrev->back = head->back;\r\ntail->forw = pte_index;\r\nhead->back = pte_index;\r\n} else {\r\nrev->forw = rev->back = pte_index;\r\n*rmap = (*rmap & ~KVMPPC_RMAP_INDEX) |\r\npte_index | KVMPPC_RMAP_PRESENT;\r\n}\r\nunlock_rmap(rmap);\r\n}\r\nvoid kvmppc_update_rmap_change(unsigned long *rmap, unsigned long psize)\r\n{\r\nunsigned long order;\r\nif (!psize)\r\nreturn;\r\norder = ilog2(psize);\r\norder <<= KVMPPC_RMAP_CHG_SHIFT;\r\nif (order > (*rmap & KVMPPC_RMAP_CHG_ORDER))\r\n*rmap = (*rmap & ~KVMPPC_RMAP_CHG_ORDER) | order;\r\n}\r\nstatic unsigned long *revmap_for_hpte(struct kvm *kvm, unsigned long hpte_v,\r\nunsigned long hpte_gr)\r\n{\r\nstruct kvm_memory_slot *memslot;\r\nunsigned long *rmap;\r\nunsigned long gfn;\r\ngfn = hpte_rpn(hpte_gr, hpte_page_size(hpte_v, hpte_gr));\r\nmemslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);\r\nif (!memslot)\r\nreturn NULL;\r\nrmap = real_vmalloc_addr(&memslot->arch.rmap[gfn - memslot->base_gfn]);\r\nreturn rmap;\r\n}\r\nstatic void remove_revmap_chain(struct kvm *kvm, long pte_index,\r\nstruct revmap_entry *rev,\r\nunsigned long hpte_v, unsigned long hpte_r)\r\n{\r\nstruct revmap_entry *next, *prev;\r\nunsigned long ptel, head;\r\nunsigned long *rmap;\r\nunsigned long rcbits;\r\nrcbits = hpte_r & (HPTE_R_R | HPTE_R_C);\r\nptel = rev->guest_rpte |= rcbits;\r\nrmap = revmap_for_hpte(kvm, hpte_v, ptel);\r\nif (!rmap)\r\nreturn;\r\nlock_rmap(rmap);\r\nhead = *rmap & KVMPPC_RMAP_INDEX;\r\nnext = real_vmalloc_addr(&kvm->arch.hpt.rev[rev->forw]);\r\nprev = real_vmalloc_addr(&kvm->arch.hpt.rev[rev->back]);\r\nnext->back = rev->back;\r\nprev->forw = rev->forw;\r\nif (head == pte_index) {\r\nhead = rev->forw;\r\nif (head == pte_index)\r\n*rmap &= ~(KVMPPC_RMAP_PRESENT | KVMPPC_RMAP_INDEX);\r\nelse\r\n*rmap = (*rmap & ~KVMPPC_RMAP_INDEX) | head;\r\n}\r\n*rmap |= rcbits << KVMPPC_RMAP_RC_SHIFT;\r\nif (rcbits & HPTE_R_C)\r\nkvmppc_update_rmap_change(rmap, hpte_page_size(hpte_v, hpte_r));\r\nunlock_rmap(rmap);\r\n}\r\nlong kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,\r\nlong pte_index, unsigned long pteh, unsigned long ptel,\r\npgd_t *pgdir, bool realmode, unsigned long *pte_idx_ret)\r\n{\r\nunsigned long i, pa, gpa, gfn, psize;\r\nunsigned long slot_fn, hva;\r\n__be64 *hpte;\r\nstruct revmap_entry *rev;\r\nunsigned long g_ptel;\r\nstruct kvm_memory_slot *memslot;\r\nunsigned hpage_shift;\r\nbool is_ci;\r\nunsigned long *rmap;\r\npte_t *ptep;\r\nunsigned int writing;\r\nunsigned long mmu_seq;\r\nunsigned long rcbits, irq_flags = 0;\r\nif (kvm_is_radix(kvm))\r\nreturn H_FUNCTION;\r\npsize = hpte_page_size(pteh, ptel);\r\nif (!psize)\r\nreturn H_PARAMETER;\r\nwriting = hpte_is_writable(ptel);\r\npteh &= ~(HPTE_V_HVLOCK | HPTE_V_ABSENT | HPTE_V_VALID);\r\nptel &= ~HPTE_GR_RESERVED;\r\ng_ptel = ptel;\r\nmmu_seq = kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\ngpa = (ptel & HPTE_R_RPN) & ~(psize - 1);\r\ngfn = gpa >> PAGE_SHIFT;\r\nmemslot = __gfn_to_memslot(kvm_memslots_raw(kvm), gfn);\r\npa = 0;\r\nis_ci = false;\r\nrmap = NULL;\r\nif (!(memslot && !(memslot->flags & KVM_MEMSLOT_INVALID))) {\r\npteh |= HPTE_V_ABSENT;\r\nptel |= HPTE_R_KEY_HI | HPTE_R_KEY_LO;\r\ngoto do_insert;\r\n}\r\nif (!slot_is_aligned(memslot, psize))\r\nreturn H_PARAMETER;\r\nslot_fn = gfn - memslot->base_gfn;\r\nrmap = &memslot->arch.rmap[slot_fn];\r\nhva = __gfn_to_hva_memslot(memslot, gfn);\r\nif (realmode)\r\nptep = __find_linux_pte_or_hugepte(pgdir, hva, NULL,\r\n&hpage_shift);\r\nelse {\r\nlocal_irq_save(irq_flags);\r\nptep = find_linux_pte_or_hugepte(pgdir, hva, NULL,\r\n&hpage_shift);\r\n}\r\nif (ptep) {\r\npte_t pte;\r\nunsigned int host_pte_size;\r\nif (hpage_shift)\r\nhost_pte_size = 1ul << hpage_shift;\r\nelse\r\nhost_pte_size = PAGE_SIZE;\r\nif (host_pte_size < psize) {\r\nif (!realmode)\r\nlocal_irq_restore(flags);\r\nreturn H_PARAMETER;\r\n}\r\npte = kvmppc_read_update_linux_pte(ptep, writing);\r\nif (pte_present(pte) && !pte_protnone(pte)) {\r\nif (writing && !__pte_write(pte))\r\nptel = hpte_make_readonly(ptel);\r\nis_ci = pte_ci(pte);\r\npa = pte_pfn(pte) << PAGE_SHIFT;\r\npa |= hva & (host_pte_size - 1);\r\npa |= gpa & ~PAGE_MASK;\r\n}\r\n}\r\nif (!realmode)\r\nlocal_irq_restore(irq_flags);\r\nptel &= ~(HPTE_R_PP0 - psize);\r\nptel |= pa;\r\nif (pa)\r\npteh |= HPTE_V_VALID;\r\nelse {\r\npteh |= HPTE_V_ABSENT;\r\nptel &= ~(HPTE_R_KEY_HI | HPTE_R_KEY_LO);\r\n}\r\nif (ptep && !hpte_cache_flags_ok(ptel, is_ci)) {\r\nif (is_ci)\r\nreturn H_PARAMETER;\r\nptel &= ~(HPTE_R_W|HPTE_R_I|HPTE_R_G);\r\nptel |= HPTE_R_M;\r\n}\r\ndo_insert:\r\nif (pte_index >= kvmppc_hpt_npte(&kvm->arch.hpt))\r\nreturn H_PARAMETER;\r\nif (likely((flags & H_EXACT) == 0)) {\r\npte_index &= ~7UL;\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (pte_index << 4));\r\nfor (i = 0; i < 8; ++i) {\r\nif ((be64_to_cpu(*hpte) & HPTE_V_VALID) == 0 &&\r\ntry_lock_hpte(hpte, HPTE_V_HVLOCK | HPTE_V_VALID |\r\nHPTE_V_ABSENT))\r\nbreak;\r\nhpte += 2;\r\n}\r\nif (i == 8) {\r\nhpte -= 16;\r\nfor (i = 0; i < 8; ++i) {\r\nu64 pte;\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\npte = be64_to_cpu(hpte[0]);\r\nif (!(pte & (HPTE_V_VALID | HPTE_V_ABSENT)))\r\nbreak;\r\n__unlock_hpte(hpte, pte);\r\nhpte += 2;\r\n}\r\nif (i == 8)\r\nreturn H_PTEG_FULL;\r\n}\r\npte_index += i;\r\n} else {\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (pte_index << 4));\r\nif (!try_lock_hpte(hpte, HPTE_V_HVLOCK | HPTE_V_VALID |\r\nHPTE_V_ABSENT)) {\r\nu64 pte;\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\npte = be64_to_cpu(hpte[0]);\r\nif (pte & (HPTE_V_VALID | HPTE_V_ABSENT)) {\r\n__unlock_hpte(hpte, pte);\r\nreturn H_PTEG_FULL;\r\n}\r\n}\r\n}\r\nrev = &kvm->arch.hpt.rev[pte_index];\r\nif (realmode)\r\nrev = real_vmalloc_addr(rev);\r\nif (rev) {\r\nrev->guest_rpte = g_ptel;\r\nnote_hpte_modification(kvm, rev);\r\n}\r\nif (pteh & HPTE_V_VALID) {\r\nif (realmode)\r\nrmap = real_vmalloc_addr(rmap);\r\nlock_rmap(rmap);\r\nif (mmu_notifier_retry(kvm, mmu_seq)) {\r\npteh |= HPTE_V_ABSENT;\r\npteh &= ~HPTE_V_VALID;\r\nptel &= ~(HPTE_R_KEY_HI | HPTE_R_KEY_LO);\r\nunlock_rmap(rmap);\r\n} else {\r\nkvmppc_add_revmap_chain(kvm, rev, rmap, pte_index,\r\nrealmode);\r\nrcbits = *rmap >> KVMPPC_RMAP_RC_SHIFT;\r\nptel &= rcbits | ~(HPTE_R_R | HPTE_R_C);\r\n}\r\n}\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nptel = hpte_old_to_new_r(pteh, ptel);\r\npteh = hpte_old_to_new_v(pteh);\r\n}\r\nhpte[1] = cpu_to_be64(ptel);\r\neieio();\r\n__unlock_hpte(hpte, pteh);\r\nasm volatile("ptesync" : : : "memory");\r\n*pte_idx_ret = pte_index;\r\nreturn H_SUCCESS;\r\n}\r\nlong kvmppc_h_enter(struct kvm_vcpu *vcpu, unsigned long flags,\r\nlong pte_index, unsigned long pteh, unsigned long ptel)\r\n{\r\nreturn kvmppc_do_h_enter(vcpu->kvm, flags, pte_index, pteh, ptel,\r\nvcpu->arch.pgdir, true, &vcpu->arch.gpr[4]);\r\n}\r\nstatic inline int is_mmio_hpte(unsigned long v, unsigned long r)\r\n{\r\nreturn ((v & HPTE_V_ABSENT) &&\r\n(r & (HPTE_R_KEY_HI | HPTE_R_KEY_LO)) ==\r\n(HPTE_R_KEY_HI | HPTE_R_KEY_LO));\r\n}\r\nstatic inline int try_lock_tlbie(unsigned int *lock)\r\n{\r\nunsigned int tmp, old;\r\nunsigned int token = LOCK_TOKEN;\r\nasm volatile("1:lwarx %1,0,%2\n"\r\n" cmpwi cr0,%1,0\n"\r\n" bne 2f\n"\r\n" stwcx. %3,0,%2\n"\r\n" bne- 1b\n"\r\n" isync\n"\r\n"2:"\r\n: "=&r" (tmp), "=&r" (old)\r\n: "r" (lock), "r" (token)\r\n: "cc", "memory");\r\nreturn old == 0;\r\n}\r\nstatic void do_tlbies(struct kvm *kvm, unsigned long *rbvalues,\r\nlong npages, int global, bool need_sync)\r\n{\r\nlong i;\r\nif (global) {\r\nwhile (!try_lock_tlbie(&kvm->arch.tlbie_lock))\r\ncpu_relax();\r\nif (need_sync)\r\nasm volatile("ptesync" : : : "memory");\r\nfor (i = 0; i < npages; ++i)\r\nasm volatile(PPC_TLBIE_5(%0,%1,0,0,0) : :\r\n"r" (rbvalues[i]), "r" (kvm->arch.lpid));\r\nasm volatile("eieio; tlbsync; ptesync" : : : "memory");\r\nkvm->arch.tlbie_lock = 0;\r\n} else {\r\nif (need_sync)\r\nasm volatile("ptesync" : : : "memory");\r\nfor (i = 0; i < npages; ++i)\r\nasm volatile(PPC_TLBIEL(%0,%1,0,0,0) : :\r\n"r" (rbvalues[i]), "r" (0));\r\nasm volatile("ptesync" : : : "memory");\r\n}\r\n}\r\nlong kvmppc_do_h_remove(struct kvm *kvm, unsigned long flags,\r\nunsigned long pte_index, unsigned long avpn,\r\nunsigned long *hpret)\r\n{\r\n__be64 *hpte;\r\nunsigned long v, r, rb;\r\nstruct revmap_entry *rev;\r\nu64 pte, orig_pte, pte_r;\r\nif (kvm_is_radix(kvm))\r\nreturn H_FUNCTION;\r\nif (pte_index >= kvmppc_hpt_npte(&kvm->arch.hpt))\r\nreturn H_PARAMETER;\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (pte_index << 4));\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\npte = orig_pte = be64_to_cpu(hpte[0]);\r\npte_r = be64_to_cpu(hpte[1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\npte = hpte_new_to_old_v(pte, pte_r);\r\npte_r = hpte_new_to_old_r(pte_r);\r\n}\r\nif ((pte & (HPTE_V_ABSENT | HPTE_V_VALID)) == 0 ||\r\n((flags & H_AVPN) && (pte & ~0x7fUL) != avpn) ||\r\n((flags & H_ANDCOND) && (pte & avpn) != 0)) {\r\n__unlock_hpte(hpte, orig_pte);\r\nreturn H_NOT_FOUND;\r\n}\r\nrev = real_vmalloc_addr(&kvm->arch.hpt.rev[pte_index]);\r\nv = pte & ~HPTE_V_HVLOCK;\r\nif (v & HPTE_V_VALID) {\r\nhpte[0] &= ~cpu_to_be64(HPTE_V_VALID);\r\nrb = compute_tlbie_rb(v, pte_r, pte_index);\r\ndo_tlbies(kvm, &rb, 1, global_invalidates(kvm, flags), true);\r\nremove_revmap_chain(kvm, pte_index, rev, v,\r\nbe64_to_cpu(hpte[1]));\r\n}\r\nr = rev->guest_rpte & ~HPTE_GR_RESERVED;\r\nnote_hpte_modification(kvm, rev);\r\nunlock_hpte(hpte, 0);\r\nif (is_mmio_hpte(v, pte_r))\r\natomic64_inc(&kvm->arch.mmio_update);\r\nif (v & HPTE_V_ABSENT)\r\nv = (v & ~HPTE_V_ABSENT) | HPTE_V_VALID;\r\nhpret[0] = v;\r\nhpret[1] = r;\r\nreturn H_SUCCESS;\r\n}\r\nlong kvmppc_h_remove(struct kvm_vcpu *vcpu, unsigned long flags,\r\nunsigned long pte_index, unsigned long avpn)\r\n{\r\nreturn kvmppc_do_h_remove(vcpu->kvm, flags, pte_index, avpn,\r\n&vcpu->arch.gpr[4]);\r\n}\r\nlong kvmppc_h_bulk_remove(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nunsigned long *args = &vcpu->arch.gpr[4];\r\n__be64 *hp, *hptes[4];\r\nunsigned long tlbrb[4];\r\nlong int i, j, k, n, found, indexes[4];\r\nunsigned long flags, req, pte_index, rcbits;\r\nint global;\r\nlong int ret = H_SUCCESS;\r\nstruct revmap_entry *rev, *revs[4];\r\nu64 hp0, hp1;\r\nif (kvm_is_radix(kvm))\r\nreturn H_FUNCTION;\r\nglobal = global_invalidates(kvm, 0);\r\nfor (i = 0; i < 4 && ret == H_SUCCESS; ) {\r\nn = 0;\r\nfor (; i < 4; ++i) {\r\nj = i * 2;\r\npte_index = args[j];\r\nflags = pte_index >> 56;\r\npte_index &= ((1ul << 56) - 1);\r\nreq = flags >> 6;\r\nflags &= 3;\r\nif (req == 3) {\r\ni = 4;\r\nbreak;\r\n}\r\nif (req != 1 || flags == 3 ||\r\npte_index >= kvmppc_hpt_npte(&kvm->arch.hpt)) {\r\nargs[j] = ((0xa0 | flags) << 56) + pte_index;\r\nret = H_PARAMETER;\r\nbreak;\r\n}\r\nhp = (__be64 *) (kvm->arch.hpt.virt + (pte_index << 4));\r\nif (!try_lock_hpte(hp, HPTE_V_HVLOCK)) {\r\nif (n)\r\nbreak;\r\nwhile (!try_lock_hpte(hp, HPTE_V_HVLOCK))\r\ncpu_relax();\r\n}\r\nfound = 0;\r\nhp0 = be64_to_cpu(hp[0]);\r\nhp1 = be64_to_cpu(hp[1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nhp0 = hpte_new_to_old_v(hp0, hp1);\r\nhp1 = hpte_new_to_old_r(hp1);\r\n}\r\nif (hp0 & (HPTE_V_ABSENT | HPTE_V_VALID)) {\r\nswitch (flags & 3) {\r\ncase 0:\r\nfound = 1;\r\nbreak;\r\ncase 1:\r\nif (!(hp0 & args[j + 1]))\r\nfound = 1;\r\nbreak;\r\ncase 2:\r\nif ((hp0 & ~0x7fUL) == args[j + 1])\r\nfound = 1;\r\nbreak;\r\n}\r\n}\r\nif (!found) {\r\nhp[0] &= ~cpu_to_be64(HPTE_V_HVLOCK);\r\nargs[j] = ((0x90 | flags) << 56) + pte_index;\r\ncontinue;\r\n}\r\nargs[j] = ((0x80 | flags) << 56) + pte_index;\r\nrev = real_vmalloc_addr(&kvm->arch.hpt.rev[pte_index]);\r\nnote_hpte_modification(kvm, rev);\r\nif (!(hp0 & HPTE_V_VALID)) {\r\nrcbits = rev->guest_rpte & (HPTE_R_R|HPTE_R_C);\r\nargs[j] |= rcbits << (56 - 5);\r\nhp[0] = 0;\r\nif (is_mmio_hpte(hp0, hp1))\r\natomic64_inc(&kvm->arch.mmio_update);\r\ncontinue;\r\n}\r\nhp[0] &= ~cpu_to_be64(HPTE_V_VALID);\r\ntlbrb[n] = compute_tlbie_rb(hp0, hp1, pte_index);\r\nindexes[n] = j;\r\nhptes[n] = hp;\r\nrevs[n] = rev;\r\n++n;\r\n}\r\nif (!n)\r\nbreak;\r\ndo_tlbies(kvm, tlbrb, n, global, true);\r\nfor (k = 0; k < n; ++k) {\r\nj = indexes[k];\r\npte_index = args[j] & ((1ul << 56) - 1);\r\nhp = hptes[k];\r\nrev = revs[k];\r\nremove_revmap_chain(kvm, pte_index, rev,\r\nbe64_to_cpu(hp[0]), be64_to_cpu(hp[1]));\r\nrcbits = rev->guest_rpte & (HPTE_R_R|HPTE_R_C);\r\nargs[j] |= rcbits << (56 - 5);\r\n__unlock_hpte(hp, 0);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nlong kvmppc_h_protect(struct kvm_vcpu *vcpu, unsigned long flags,\r\nunsigned long pte_index, unsigned long avpn,\r\nunsigned long va)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\n__be64 *hpte;\r\nstruct revmap_entry *rev;\r\nunsigned long v, r, rb, mask, bits;\r\nu64 pte_v, pte_r;\r\nif (kvm_is_radix(kvm))\r\nreturn H_FUNCTION;\r\nif (pte_index >= kvmppc_hpt_npte(&kvm->arch.hpt))\r\nreturn H_PARAMETER;\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (pte_index << 4));\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = pte_v = be64_to_cpu(hpte[0]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300))\r\nv = hpte_new_to_old_v(v, be64_to_cpu(hpte[1]));\r\nif ((v & (HPTE_V_ABSENT | HPTE_V_VALID)) == 0 ||\r\n((flags & H_AVPN) && (v & ~0x7fUL) != avpn)) {\r\n__unlock_hpte(hpte, pte_v);\r\nreturn H_NOT_FOUND;\r\n}\r\npte_r = be64_to_cpu(hpte[1]);\r\nbits = (flags << 55) & HPTE_R_PP0;\r\nbits |= (flags << 48) & HPTE_R_KEY_HI;\r\nbits |= flags & (HPTE_R_PP | HPTE_R_N | HPTE_R_KEY_LO);\r\nmask = HPTE_R_PP0 | HPTE_R_PP | HPTE_R_N |\r\nHPTE_R_KEY_HI | HPTE_R_KEY_LO;\r\nrev = real_vmalloc_addr(&kvm->arch.hpt.rev[pte_index]);\r\nif (rev) {\r\nr = (rev->guest_rpte & ~mask) | bits;\r\nrev->guest_rpte = r;\r\nnote_hpte_modification(kvm, rev);\r\n}\r\nif (v & HPTE_V_VALID) {\r\nr = (pte_r & ~mask) | bits;\r\nif (hpte_is_writable(r) && !hpte_is_writable(pte_r))\r\nr = hpte_make_readonly(r);\r\nif (r != pte_r) {\r\nrb = compute_tlbie_rb(v, r, pte_index);\r\nhpte[0] = cpu_to_be64((pte_v & ~HPTE_V_VALID) |\r\nHPTE_V_ABSENT);\r\ndo_tlbies(kvm, &rb, 1, global_invalidates(kvm, flags),\r\ntrue);\r\nr |= be64_to_cpu(hpte[1]) & (HPTE_R_R | HPTE_R_C);\r\nhpte[1] = cpu_to_be64(r);\r\n}\r\n}\r\nunlock_hpte(hpte, pte_v & ~HPTE_V_HVLOCK);\r\nasm volatile("ptesync" : : : "memory");\r\nif (is_mmio_hpte(v, pte_r))\r\natomic64_inc(&kvm->arch.mmio_update);\r\nreturn H_SUCCESS;\r\n}\r\nlong kvmppc_h_read(struct kvm_vcpu *vcpu, unsigned long flags,\r\nunsigned long pte_index)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\n__be64 *hpte;\r\nunsigned long v, r;\r\nint i, n = 1;\r\nstruct revmap_entry *rev = NULL;\r\nif (kvm_is_radix(kvm))\r\nreturn H_FUNCTION;\r\nif (pte_index >= kvmppc_hpt_npte(&kvm->arch.hpt))\r\nreturn H_PARAMETER;\r\nif (flags & H_READ_4) {\r\npte_index &= ~3;\r\nn = 4;\r\n}\r\nrev = real_vmalloc_addr(&kvm->arch.hpt.rev[pte_index]);\r\nfor (i = 0; i < n; ++i, ++pte_index) {\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (pte_index << 4));\r\nv = be64_to_cpu(hpte[0]) & ~HPTE_V_HVLOCK;\r\nr = be64_to_cpu(hpte[1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nv = hpte_new_to_old_v(v, r);\r\nr = hpte_new_to_old_r(r);\r\n}\r\nif (v & HPTE_V_ABSENT) {\r\nv &= ~HPTE_V_ABSENT;\r\nv |= HPTE_V_VALID;\r\n}\r\nif (v & HPTE_V_VALID) {\r\nr = rev[i].guest_rpte | (r & (HPTE_R_R | HPTE_R_C));\r\nr &= ~HPTE_GR_RESERVED;\r\n}\r\nvcpu->arch.gpr[4 + i * 2] = v;\r\nvcpu->arch.gpr[5 + i * 2] = r;\r\n}\r\nreturn H_SUCCESS;\r\n}\r\nlong kvmppc_h_clear_ref(struct kvm_vcpu *vcpu, unsigned long flags,\r\nunsigned long pte_index)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\n__be64 *hpte;\r\nunsigned long v, r, gr;\r\nstruct revmap_entry *rev;\r\nunsigned long *rmap;\r\nlong ret = H_NOT_FOUND;\r\nif (kvm_is_radix(kvm))\r\nreturn H_FUNCTION;\r\nif (pte_index >= kvmppc_hpt_npte(&kvm->arch.hpt))\r\nreturn H_PARAMETER;\r\nrev = real_vmalloc_addr(&kvm->arch.hpt.rev[pte_index]);\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (pte_index << 4));\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = be64_to_cpu(hpte[0]);\r\nr = be64_to_cpu(hpte[1]);\r\nif (!(v & (HPTE_V_VALID | HPTE_V_ABSENT)))\r\ngoto out;\r\ngr = rev->guest_rpte;\r\nif (rev->guest_rpte & HPTE_R_R) {\r\nrev->guest_rpte &= ~HPTE_R_R;\r\nnote_hpte_modification(kvm, rev);\r\n}\r\nif (v & HPTE_V_VALID) {\r\ngr |= r & (HPTE_R_R | HPTE_R_C);\r\nif (r & HPTE_R_R) {\r\nkvmppc_clear_ref_hpte(kvm, hpte, pte_index);\r\nrmap = revmap_for_hpte(kvm, v, gr);\r\nif (rmap) {\r\nlock_rmap(rmap);\r\n*rmap |= KVMPPC_RMAP_REFERENCED;\r\nunlock_rmap(rmap);\r\n}\r\n}\r\n}\r\nvcpu->arch.gpr[4] = gr;\r\nret = H_SUCCESS;\r\nout:\r\nunlock_hpte(hpte, v & ~HPTE_V_HVLOCK);\r\nreturn ret;\r\n}\r\nlong kvmppc_h_clear_mod(struct kvm_vcpu *vcpu, unsigned long flags,\r\nunsigned long pte_index)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\n__be64 *hpte;\r\nunsigned long v, r, gr;\r\nstruct revmap_entry *rev;\r\nunsigned long *rmap;\r\nlong ret = H_NOT_FOUND;\r\nif (kvm_is_radix(kvm))\r\nreturn H_FUNCTION;\r\nif (pte_index >= kvmppc_hpt_npte(&kvm->arch.hpt))\r\nreturn H_PARAMETER;\r\nrev = real_vmalloc_addr(&kvm->arch.hpt.rev[pte_index]);\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (pte_index << 4));\r\nwhile (!try_lock_hpte(hpte, HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = be64_to_cpu(hpte[0]);\r\nr = be64_to_cpu(hpte[1]);\r\nif (!(v & (HPTE_V_VALID | HPTE_V_ABSENT)))\r\ngoto out;\r\ngr = rev->guest_rpte;\r\nif (gr & HPTE_R_C) {\r\nrev->guest_rpte &= ~HPTE_R_C;\r\nnote_hpte_modification(kvm, rev);\r\n}\r\nif (v & HPTE_V_VALID) {\r\nhpte[0] |= cpu_to_be64(HPTE_V_ABSENT);\r\nkvmppc_invalidate_hpte(kvm, hpte, pte_index);\r\nr = be64_to_cpu(hpte[1]);\r\ngr |= r & (HPTE_R_R | HPTE_R_C);\r\nif (r & HPTE_R_C) {\r\nunsigned long psize = hpte_page_size(v, r);\r\nhpte[1] = cpu_to_be64(r & ~HPTE_R_C);\r\neieio();\r\nrmap = revmap_for_hpte(kvm, v, gr);\r\nif (rmap) {\r\nlock_rmap(rmap);\r\n*rmap |= KVMPPC_RMAP_CHANGED;\r\nkvmppc_update_rmap_change(rmap, psize);\r\nunlock_rmap(rmap);\r\n}\r\n}\r\n}\r\nvcpu->arch.gpr[4] = gr;\r\nret = H_SUCCESS;\r\nout:\r\nunlock_hpte(hpte, v & ~HPTE_V_HVLOCK);\r\nreturn ret;\r\n}\r\nvoid kvmppc_invalidate_hpte(struct kvm *kvm, __be64 *hptep,\r\nunsigned long pte_index)\r\n{\r\nunsigned long rb;\r\nu64 hp0, hp1;\r\nhptep[0] &= ~cpu_to_be64(HPTE_V_VALID);\r\nhp0 = be64_to_cpu(hptep[0]);\r\nhp1 = be64_to_cpu(hptep[1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nhp0 = hpte_new_to_old_v(hp0, hp1);\r\nhp1 = hpte_new_to_old_r(hp1);\r\n}\r\nrb = compute_tlbie_rb(hp0, hp1, pte_index);\r\ndo_tlbies(kvm, &rb, 1, 1, true);\r\n}\r\nvoid kvmppc_clear_ref_hpte(struct kvm *kvm, __be64 *hptep,\r\nunsigned long pte_index)\r\n{\r\nunsigned long rb;\r\nunsigned char rbyte;\r\nu64 hp0, hp1;\r\nhp0 = be64_to_cpu(hptep[0]);\r\nhp1 = be64_to_cpu(hptep[1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nhp0 = hpte_new_to_old_v(hp0, hp1);\r\nhp1 = hpte_new_to_old_r(hp1);\r\n}\r\nrb = compute_tlbie_rb(hp0, hp1, pte_index);\r\nrbyte = (be64_to_cpu(hptep[1]) & ~HPTE_R_R) >> 8;\r\n*((char *)hptep + 14) = rbyte;\r\ndo_tlbies(kvm, &rb, 1, 1, false);\r\n}\r\nstatic struct mmio_hpte_cache_entry *mmio_cache_search(struct kvm_vcpu *vcpu,\r\nunsigned long eaddr, unsigned long slb_v, long mmio_update)\r\n{\r\nstruct mmio_hpte_cache_entry *entry = NULL;\r\nunsigned int pshift;\r\nunsigned int i;\r\nfor (i = 0; i < MMIO_HPTE_CACHE_SIZE; i++) {\r\nentry = &vcpu->arch.mmio_cache.entry[i];\r\nif (entry->mmio_update == mmio_update) {\r\npshift = entry->slb_base_pshift;\r\nif ((entry->eaddr >> pshift) == (eaddr >> pshift) &&\r\nentry->slb_v == slb_v)\r\nreturn entry;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct mmio_hpte_cache_entry *\r\nnext_mmio_cache_entry(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned int index = vcpu->arch.mmio_cache.index;\r\nvcpu->arch.mmio_cache.index++;\r\nif (vcpu->arch.mmio_cache.index == MMIO_HPTE_CACHE_SIZE)\r\nvcpu->arch.mmio_cache.index = 0;\r\nreturn &vcpu->arch.mmio_cache.entry[index];\r\n}\r\nlong kvmppc_hv_find_lock_hpte(struct kvm *kvm, gva_t eaddr, unsigned long slb_v,\r\nunsigned long valid)\r\n{\r\nunsigned int i;\r\nunsigned int pshift;\r\nunsigned long somask;\r\nunsigned long vsid, hash;\r\nunsigned long avpn;\r\n__be64 *hpte;\r\nunsigned long mask, val;\r\nunsigned long v, r, orig_v;\r\nmask = SLB_VSID_B | HPTE_V_AVPN | HPTE_V_SECONDARY;\r\nval = 0;\r\npshift = 12;\r\nif (slb_v & SLB_VSID_L) {\r\nmask |= HPTE_V_LARGE;\r\nval |= HPTE_V_LARGE;\r\npshift = slb_base_page_shift[(slb_v & SLB_VSID_LP) >> 4];\r\n}\r\nif (slb_v & SLB_VSID_B_1T) {\r\nsomask = (1UL << 40) - 1;\r\nvsid = (slb_v & ~SLB_VSID_B) >> SLB_VSID_SHIFT_1T;\r\nvsid ^= vsid << 25;\r\n} else {\r\nsomask = (1UL << 28) - 1;\r\nvsid = (slb_v & ~SLB_VSID_B) >> SLB_VSID_SHIFT;\r\n}\r\nhash = (vsid ^ ((eaddr & somask) >> pshift)) & kvmppc_hpt_mask(&kvm->arch.hpt);\r\navpn = slb_v & ~(somask >> 16);\r\navpn |= (eaddr & somask) >> 16;\r\nif (pshift >= 24)\r\navpn &= ~((1UL << (pshift - 16)) - 1);\r\nelse\r\navpn &= ~0x7fUL;\r\nval |= avpn;\r\nfor (;;) {\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (hash << 7));\r\nfor (i = 0; i < 16; i += 2) {\r\nv = be64_to_cpu(hpte[i]) & ~HPTE_V_HVLOCK;\r\nif (cpu_has_feature(CPU_FTR_ARCH_300))\r\nv = hpte_new_to_old_v(v, be64_to_cpu(hpte[i+1]));\r\nif (!(v & valid) || (v & mask) != val)\r\ncontinue;\r\nwhile (!try_lock_hpte(&hpte[i], HPTE_V_HVLOCK))\r\ncpu_relax();\r\nv = orig_v = be64_to_cpu(hpte[i]) & ~HPTE_V_HVLOCK;\r\nr = be64_to_cpu(hpte[i+1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nv = hpte_new_to_old_v(v, r);\r\nr = hpte_new_to_old_r(r);\r\n}\r\nif ((v & valid) && (v & mask) == val &&\r\nhpte_base_page_size(v, r) == (1ul << pshift))\r\nreturn (hash << 3) + (i >> 1);\r\n__unlock_hpte(&hpte[i], orig_v);\r\n}\r\nif (val & HPTE_V_SECONDARY)\r\nbreak;\r\nval |= HPTE_V_SECONDARY;\r\nhash = hash ^ kvmppc_hpt_mask(&kvm->arch.hpt);\r\n}\r\nreturn -1;\r\n}\r\nlong kvmppc_hpte_hv_fault(struct kvm_vcpu *vcpu, unsigned long addr,\r\nunsigned long slb_v, unsigned int status, bool data)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nlong int index;\r\nunsigned long v, r, gr, orig_v;\r\n__be64 *hpte;\r\nunsigned long valid;\r\nstruct revmap_entry *rev;\r\nunsigned long pp, key;\r\nstruct mmio_hpte_cache_entry *cache_entry = NULL;\r\nlong mmio_update = 0;\r\nvalid = HPTE_V_VALID;\r\nif (status & DSISR_NOHPTE) {\r\nvalid |= HPTE_V_ABSENT;\r\nmmio_update = atomic64_read(&kvm->arch.mmio_update);\r\ncache_entry = mmio_cache_search(vcpu, addr, slb_v, mmio_update);\r\n}\r\nif (cache_entry) {\r\nindex = cache_entry->pte_index;\r\nv = cache_entry->hpte_v;\r\nr = cache_entry->hpte_r;\r\ngr = cache_entry->rpte;\r\n} else {\r\nindex = kvmppc_hv_find_lock_hpte(kvm, addr, slb_v, valid);\r\nif (index < 0) {\r\nif (status & DSISR_NOHPTE)\r\nreturn status;\r\nreturn 0;\r\n}\r\nhpte = (__be64 *)(kvm->arch.hpt.virt + (index << 4));\r\nv = orig_v = be64_to_cpu(hpte[0]) & ~HPTE_V_HVLOCK;\r\nr = be64_to_cpu(hpte[1]);\r\nif (cpu_has_feature(CPU_FTR_ARCH_300)) {\r\nv = hpte_new_to_old_v(v, r);\r\nr = hpte_new_to_old_r(r);\r\n}\r\nrev = real_vmalloc_addr(&kvm->arch.hpt.rev[index]);\r\ngr = rev->guest_rpte;\r\nunlock_hpte(hpte, orig_v);\r\n}\r\nif ((status & DSISR_NOHPTE) && (v & HPTE_V_VALID))\r\nreturn 0;\r\npp = gr & (HPTE_R_PP0 | HPTE_R_PP);\r\nkey = (vcpu->arch.shregs.msr & MSR_PR) ? SLB_VSID_KP : SLB_VSID_KS;\r\nstatus &= ~DSISR_NOHPTE;\r\nif (!data) {\r\nif (gr & (HPTE_R_N | HPTE_R_G))\r\nreturn status | SRR1_ISI_N_OR_G;\r\nif (!hpte_read_permission(pp, slb_v & key))\r\nreturn status | SRR1_ISI_PROT;\r\n} else if (status & DSISR_ISSTORE) {\r\nif (!hpte_write_permission(pp, slb_v & key))\r\nreturn status | DSISR_PROTFAULT;\r\n} else {\r\nif (!hpte_read_permission(pp, slb_v & key))\r\nreturn status | DSISR_PROTFAULT;\r\n}\r\nif (data && (vcpu->arch.shregs.msr & MSR_DR)) {\r\nunsigned int perm = hpte_get_skey_perm(gr, vcpu->arch.amr);\r\nif (status & DSISR_ISSTORE)\r\nperm >>= 1;\r\nif (perm & 1)\r\nreturn status | DSISR_KEYFAULT;\r\n}\r\nvcpu->arch.pgfault_addr = addr;\r\nvcpu->arch.pgfault_index = index;\r\nvcpu->arch.pgfault_hpte[0] = v;\r\nvcpu->arch.pgfault_hpte[1] = r;\r\nvcpu->arch.pgfault_cache = cache_entry;\r\nif ((r & (HPTE_R_KEY_HI | HPTE_R_KEY_LO)) ==\r\n(HPTE_R_KEY_HI | HPTE_R_KEY_LO)) {\r\nif (!cache_entry) {\r\nunsigned int pshift = 12;\r\nunsigned int pshift_index;\r\nif (slb_v & SLB_VSID_L) {\r\npshift_index = ((slb_v & SLB_VSID_LP) >> 4);\r\npshift = slb_base_page_shift[pshift_index];\r\n}\r\ncache_entry = next_mmio_cache_entry(vcpu);\r\ncache_entry->eaddr = addr;\r\ncache_entry->slb_base_pshift = pshift;\r\ncache_entry->pte_index = index;\r\ncache_entry->hpte_v = v;\r\ncache_entry->hpte_r = r;\r\ncache_entry->rpte = gr;\r\ncache_entry->slb_v = slb_v;\r\ncache_entry->mmio_update = mmio_update;\r\n}\r\nif (data && (vcpu->arch.shregs.msr & MSR_IR))\r\nreturn -2;\r\n}\r\nreturn -1;\r\n}
