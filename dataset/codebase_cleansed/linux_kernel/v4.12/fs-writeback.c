static inline struct inode *wb_inode(struct list_head *head)\r\n{\r\nreturn list_entry(head, struct inode, i_io_list);\r\n}\r\nstatic bool wb_io_lists_populated(struct bdi_writeback *wb)\r\n{\r\nif (wb_has_dirty_io(wb)) {\r\nreturn false;\r\n} else {\r\nset_bit(WB_has_dirty_io, &wb->state);\r\nWARN_ON_ONCE(!wb->avg_write_bandwidth);\r\natomic_long_add(wb->avg_write_bandwidth,\r\n&wb->bdi->tot_write_bandwidth);\r\nreturn true;\r\n}\r\n}\r\nstatic void wb_io_lists_depopulated(struct bdi_writeback *wb)\r\n{\r\nif (wb_has_dirty_io(wb) && list_empty(&wb->b_dirty) &&\r\nlist_empty(&wb->b_io) && list_empty(&wb->b_more_io)) {\r\nclear_bit(WB_has_dirty_io, &wb->state);\r\nWARN_ON_ONCE(atomic_long_sub_return(wb->avg_write_bandwidth,\r\n&wb->bdi->tot_write_bandwidth) < 0);\r\n}\r\n}\r\nstatic bool inode_io_list_move_locked(struct inode *inode,\r\nstruct bdi_writeback *wb,\r\nstruct list_head *head)\r\n{\r\nassert_spin_locked(&wb->list_lock);\r\nlist_move(&inode->i_io_list, head);\r\nif (head != &wb->b_dirty_time)\r\nreturn wb_io_lists_populated(wb);\r\nwb_io_lists_depopulated(wb);\r\nreturn false;\r\n}\r\nstatic void inode_io_list_del_locked(struct inode *inode,\r\nstruct bdi_writeback *wb)\r\n{\r\nassert_spin_locked(&wb->list_lock);\r\nlist_del_init(&inode->i_io_list);\r\nwb_io_lists_depopulated(wb);\r\n}\r\nstatic void wb_wakeup(struct bdi_writeback *wb)\r\n{\r\nspin_lock_bh(&wb->work_lock);\r\nif (test_bit(WB_registered, &wb->state))\r\nmod_delayed_work(bdi_wq, &wb->dwork, 0);\r\nspin_unlock_bh(&wb->work_lock);\r\n}\r\nstatic void finish_writeback_work(struct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\nstruct wb_completion *done = work->done;\r\nif (work->auto_free)\r\nkfree(work);\r\nif (done && atomic_dec_and_test(&done->cnt))\r\nwake_up_all(&wb->bdi->wb_waitq);\r\n}\r\nstatic void wb_queue_work(struct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\ntrace_writeback_queue(wb, work);\r\nif (work->done)\r\natomic_inc(&work->done->cnt);\r\nspin_lock_bh(&wb->work_lock);\r\nif (test_bit(WB_registered, &wb->state)) {\r\nlist_add_tail(&work->list, &wb->work_list);\r\nmod_delayed_work(bdi_wq, &wb->dwork, 0);\r\n} else\r\nfinish_writeback_work(wb, work);\r\nspin_unlock_bh(&wb->work_lock);\r\n}\r\nstatic void wb_wait_for_completion(struct backing_dev_info *bdi,\r\nstruct wb_completion *done)\r\n{\r\natomic_dec(&done->cnt);\r\nwait_event(bdi->wb_waitq, !atomic_read(&done->cnt));\r\n}\r\nvoid __inode_attach_wb(struct inode *inode, struct page *page)\r\n{\r\nstruct backing_dev_info *bdi = inode_to_bdi(inode);\r\nstruct bdi_writeback *wb = NULL;\r\nif (inode_cgwb_enabled(inode)) {\r\nstruct cgroup_subsys_state *memcg_css;\r\nif (page) {\r\nmemcg_css = mem_cgroup_css_from_page(page);\r\nwb = wb_get_create(bdi, memcg_css, GFP_ATOMIC);\r\n} else {\r\nmemcg_css = task_get_css(current, memory_cgrp_id);\r\nwb = wb_get_create(bdi, memcg_css, GFP_ATOMIC);\r\ncss_put(memcg_css);\r\n}\r\n}\r\nif (!wb)\r\nwb = &bdi->wb;\r\nif (unlikely(cmpxchg(&inode->i_wb, NULL, wb)))\r\nwb_put(wb);\r\n}\r\nstatic struct bdi_writeback *\r\nlocked_inode_to_wb_and_lock_list(struct inode *inode)\r\n__releases(&inode->i_lock\r\nstatic struct bdi_writeback *inode_to_wb_and_lock_list(struct inode *inode)\r\n__acquires(&wb->list_lock\r\nstatic void inode_switch_wbs_work_fn(struct work_struct *work)\r\n{\r\nstruct inode_switch_wbs_context *isw =\r\ncontainer_of(work, struct inode_switch_wbs_context, work);\r\nstruct inode *inode = isw->inode;\r\nstruct address_space *mapping = inode->i_mapping;\r\nstruct bdi_writeback *old_wb = inode->i_wb;\r\nstruct bdi_writeback *new_wb = isw->new_wb;\r\nstruct radix_tree_iter iter;\r\nbool switched = false;\r\nvoid **slot;\r\nif (old_wb < new_wb) {\r\nspin_lock(&old_wb->list_lock);\r\nspin_lock_nested(&new_wb->list_lock, SINGLE_DEPTH_NESTING);\r\n} else {\r\nspin_lock(&new_wb->list_lock);\r\nspin_lock_nested(&old_wb->list_lock, SINGLE_DEPTH_NESTING);\r\n}\r\nspin_lock(&inode->i_lock);\r\nspin_lock_irq(&mapping->tree_lock);\r\nif (unlikely(inode->i_state & I_FREEING))\r\ngoto skip_switch;\r\nradix_tree_for_each_tagged(slot, &mapping->page_tree, &iter, 0,\r\nPAGECACHE_TAG_DIRTY) {\r\nstruct page *page = radix_tree_deref_slot_protected(slot,\r\n&mapping->tree_lock);\r\nif (likely(page) && PageDirty(page)) {\r\n__dec_wb_stat(old_wb, WB_RECLAIMABLE);\r\n__inc_wb_stat(new_wb, WB_RECLAIMABLE);\r\n}\r\n}\r\nradix_tree_for_each_tagged(slot, &mapping->page_tree, &iter, 0,\r\nPAGECACHE_TAG_WRITEBACK) {\r\nstruct page *page = radix_tree_deref_slot_protected(slot,\r\n&mapping->tree_lock);\r\nif (likely(page)) {\r\nWARN_ON_ONCE(!PageWriteback(page));\r\n__dec_wb_stat(old_wb, WB_WRITEBACK);\r\n__inc_wb_stat(new_wb, WB_WRITEBACK);\r\n}\r\n}\r\nwb_get(new_wb);\r\nif (!list_empty(&inode->i_io_list)) {\r\nstruct inode *pos;\r\ninode_io_list_del_locked(inode, old_wb);\r\ninode->i_wb = new_wb;\r\nlist_for_each_entry(pos, &new_wb->b_dirty, i_io_list)\r\nif (time_after_eq(inode->dirtied_when,\r\npos->dirtied_when))\r\nbreak;\r\ninode_io_list_move_locked(inode, new_wb, pos->i_io_list.prev);\r\n} else {\r\ninode->i_wb = new_wb;\r\n}\r\ninode->i_wb_frn_winner = 0;\r\ninode->i_wb_frn_avg_time = 0;\r\ninode->i_wb_frn_history = 0;\r\nswitched = true;\r\nskip_switch:\r\nsmp_store_release(&inode->i_state, inode->i_state & ~I_WB_SWITCH);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nspin_unlock(&inode->i_lock);\r\nspin_unlock(&new_wb->list_lock);\r\nspin_unlock(&old_wb->list_lock);\r\nif (switched) {\r\nwb_wakeup(new_wb);\r\nwb_put(old_wb);\r\n}\r\nwb_put(new_wb);\r\niput(inode);\r\nkfree(isw);\r\natomic_dec(&isw_nr_in_flight);\r\n}\r\nstatic void inode_switch_wbs_rcu_fn(struct rcu_head *rcu_head)\r\n{\r\nstruct inode_switch_wbs_context *isw = container_of(rcu_head,\r\nstruct inode_switch_wbs_context, rcu_head);\r\nINIT_WORK(&isw->work, inode_switch_wbs_work_fn);\r\nqueue_work(isw_wq, &isw->work);\r\n}\r\nstatic void inode_switch_wbs(struct inode *inode, int new_wb_id)\r\n{\r\nstruct backing_dev_info *bdi = inode_to_bdi(inode);\r\nstruct cgroup_subsys_state *memcg_css;\r\nstruct inode_switch_wbs_context *isw;\r\nif (inode->i_state & I_WB_SWITCH)\r\nreturn;\r\nisw = kzalloc(sizeof(*isw), GFP_ATOMIC);\r\nif (!isw)\r\nreturn;\r\nrcu_read_lock();\r\nmemcg_css = css_from_id(new_wb_id, &memory_cgrp_subsys);\r\nif (memcg_css)\r\nisw->new_wb = wb_get_create(bdi, memcg_css, GFP_ATOMIC);\r\nrcu_read_unlock();\r\nif (!isw->new_wb)\r\ngoto out_free;\r\nspin_lock(&inode->i_lock);\r\nif (!(inode->i_sb->s_flags & MS_ACTIVE) ||\r\ninode->i_state & (I_WB_SWITCH | I_FREEING) ||\r\ninode_to_wb(inode) == isw->new_wb) {\r\nspin_unlock(&inode->i_lock);\r\ngoto out_free;\r\n}\r\ninode->i_state |= I_WB_SWITCH;\r\n__iget(inode);\r\nspin_unlock(&inode->i_lock);\r\nisw->inode = inode;\r\natomic_inc(&isw_nr_in_flight);\r\ncall_rcu(&isw->rcu_head, inode_switch_wbs_rcu_fn);\r\nreturn;\r\nout_free:\r\nif (isw->new_wb)\r\nwb_put(isw->new_wb);\r\nkfree(isw);\r\n}\r\nvoid wbc_attach_and_unlock_inode(struct writeback_control *wbc,\r\nstruct inode *inode)\r\n{\r\nif (!inode_cgwb_enabled(inode)) {\r\nspin_unlock(&inode->i_lock);\r\nreturn;\r\n}\r\nwbc->wb = inode_to_wb(inode);\r\nwbc->inode = inode;\r\nwbc->wb_id = wbc->wb->memcg_css->id;\r\nwbc->wb_lcand_id = inode->i_wb_frn_winner;\r\nwbc->wb_tcand_id = 0;\r\nwbc->wb_bytes = 0;\r\nwbc->wb_lcand_bytes = 0;\r\nwbc->wb_tcand_bytes = 0;\r\nwb_get(wbc->wb);\r\nspin_unlock(&inode->i_lock);\r\nif (unlikely(wb_dying(wbc->wb)))\r\ninode_switch_wbs(inode, wbc->wb_id);\r\n}\r\nvoid wbc_detach_inode(struct writeback_control *wbc)\r\n{\r\nstruct bdi_writeback *wb = wbc->wb;\r\nstruct inode *inode = wbc->inode;\r\nunsigned long avg_time, max_bytes, max_time;\r\nu16 history;\r\nint max_id;\r\nif (!wb)\r\nreturn;\r\nhistory = inode->i_wb_frn_history;\r\navg_time = inode->i_wb_frn_avg_time;\r\nif (wbc->wb_bytes >= wbc->wb_lcand_bytes &&\r\nwbc->wb_bytes >= wbc->wb_tcand_bytes) {\r\nmax_id = wbc->wb_id;\r\nmax_bytes = wbc->wb_bytes;\r\n} else if (wbc->wb_lcand_bytes >= wbc->wb_tcand_bytes) {\r\nmax_id = wbc->wb_lcand_id;\r\nmax_bytes = wbc->wb_lcand_bytes;\r\n} else {\r\nmax_id = wbc->wb_tcand_id;\r\nmax_bytes = wbc->wb_tcand_bytes;\r\n}\r\nmax_time = DIV_ROUND_UP((max_bytes >> PAGE_SHIFT) << WB_FRN_TIME_SHIFT,\r\nwb->avg_write_bandwidth);\r\nif (avg_time)\r\navg_time += (max_time >> WB_FRN_TIME_AVG_SHIFT) -\r\n(avg_time >> WB_FRN_TIME_AVG_SHIFT);\r\nelse\r\navg_time = max_time;\r\nif (max_time >= avg_time / WB_FRN_TIME_CUT_DIV) {\r\nint slots;\r\nslots = min(DIV_ROUND_UP(max_time, WB_FRN_HIST_UNIT),\r\n(unsigned long)WB_FRN_HIST_MAX_SLOTS);\r\nhistory <<= slots;\r\nif (wbc->wb_id != max_id)\r\nhistory |= (1U << slots) - 1;\r\nif (hweight32(history) > WB_FRN_HIST_THR_SLOTS)\r\ninode_switch_wbs(inode, max_id);\r\n}\r\ninode->i_wb_frn_winner = max_id;\r\ninode->i_wb_frn_avg_time = min(avg_time, (unsigned long)U16_MAX);\r\ninode->i_wb_frn_history = history;\r\nwb_put(wbc->wb);\r\nwbc->wb = NULL;\r\n}\r\nvoid wbc_account_io(struct writeback_control *wbc, struct page *page,\r\nsize_t bytes)\r\n{\r\nint id;\r\nif (!wbc->wb)\r\nreturn;\r\nid = mem_cgroup_css_from_page(page)->id;\r\nif (id == wbc->wb_id) {\r\nwbc->wb_bytes += bytes;\r\nreturn;\r\n}\r\nif (id == wbc->wb_lcand_id)\r\nwbc->wb_lcand_bytes += bytes;\r\nif (!wbc->wb_tcand_bytes)\r\nwbc->wb_tcand_id = id;\r\nif (id == wbc->wb_tcand_id)\r\nwbc->wb_tcand_bytes += bytes;\r\nelse\r\nwbc->wb_tcand_bytes -= min(bytes, wbc->wb_tcand_bytes);\r\n}\r\nint inode_congested(struct inode *inode, int cong_bits)\r\n{\r\nif (inode && inode_to_wb_is_valid(inode)) {\r\nstruct bdi_writeback *wb;\r\nbool locked, congested;\r\nwb = unlocked_inode_to_wb_begin(inode, &locked);\r\ncongested = wb_congested(wb, cong_bits);\r\nunlocked_inode_to_wb_end(inode, locked);\r\nreturn congested;\r\n}\r\nreturn wb_congested(&inode_to_bdi(inode)->wb, cong_bits);\r\n}\r\nstatic long wb_split_bdi_pages(struct bdi_writeback *wb, long nr_pages)\r\n{\r\nunsigned long this_bw = wb->avg_write_bandwidth;\r\nunsigned long tot_bw = atomic_long_read(&wb->bdi->tot_write_bandwidth);\r\nif (nr_pages == LONG_MAX)\r\nreturn LONG_MAX;\r\nif (!tot_bw || this_bw >= tot_bw)\r\nreturn nr_pages;\r\nelse\r\nreturn DIV_ROUND_UP_ULL((u64)nr_pages * this_bw, tot_bw);\r\n}\r\nstatic void bdi_split_work_to_wbs(struct backing_dev_info *bdi,\r\nstruct wb_writeback_work *base_work,\r\nbool skip_if_busy)\r\n{\r\nstruct bdi_writeback *last_wb = NULL;\r\nstruct bdi_writeback *wb = list_entry(&bdi->wb_list,\r\nstruct bdi_writeback, bdi_node);\r\nmight_sleep();\r\nrestart:\r\nrcu_read_lock();\r\nlist_for_each_entry_continue_rcu(wb, &bdi->wb_list, bdi_node) {\r\nDEFINE_WB_COMPLETION_ONSTACK(fallback_work_done);\r\nstruct wb_writeback_work fallback_work;\r\nstruct wb_writeback_work *work;\r\nlong nr_pages;\r\nif (last_wb) {\r\nwb_put(last_wb);\r\nlast_wb = NULL;\r\n}\r\nif (!wb_has_dirty_io(wb) &&\r\n(base_work->sync_mode == WB_SYNC_NONE ||\r\nlist_empty(&wb->b_dirty_time)))\r\ncontinue;\r\nif (skip_if_busy && writeback_in_progress(wb))\r\ncontinue;\r\nnr_pages = wb_split_bdi_pages(wb, base_work->nr_pages);\r\nwork = kmalloc(sizeof(*work), GFP_ATOMIC);\r\nif (work) {\r\n*work = *base_work;\r\nwork->nr_pages = nr_pages;\r\nwork->auto_free = 1;\r\nwb_queue_work(wb, work);\r\ncontinue;\r\n}\r\nwork = &fallback_work;\r\n*work = *base_work;\r\nwork->nr_pages = nr_pages;\r\nwork->auto_free = 0;\r\nwork->done = &fallback_work_done;\r\nwb_queue_work(wb, work);\r\nwb_get(wb);\r\nlast_wb = wb;\r\nrcu_read_unlock();\r\nwb_wait_for_completion(bdi, &fallback_work_done);\r\ngoto restart;\r\n}\r\nrcu_read_unlock();\r\nif (last_wb)\r\nwb_put(last_wb);\r\n}\r\nvoid cgroup_writeback_umount(void)\r\n{\r\nif (atomic_read(&isw_nr_in_flight)) {\r\nsynchronize_rcu();\r\nflush_workqueue(isw_wq);\r\n}\r\n}\r\nstatic int __init cgroup_writeback_init(void)\r\n{\r\nisw_wq = alloc_workqueue("inode_switch_wbs", 0, 0);\r\nif (!isw_wq)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic struct bdi_writeback *\r\nlocked_inode_to_wb_and_lock_list(struct inode *inode)\r\n__releases(&inode->i_lock\r\nstatic struct bdi_writeback *inode_to_wb_and_lock_list(struct inode *inode)\r\n__acquires(&wb->list_lock\r\nstatic long wb_split_bdi_pages(struct bdi_writeback *wb, long nr_pages)\r\n{\r\nreturn nr_pages;\r\n}\r\nstatic void bdi_split_work_to_wbs(struct backing_dev_info *bdi,\r\nstruct wb_writeback_work *base_work,\r\nbool skip_if_busy)\r\n{\r\nmight_sleep();\r\nif (!skip_if_busy || !writeback_in_progress(&bdi->wb)) {\r\nbase_work->auto_free = 0;\r\nwb_queue_work(&bdi->wb, base_work);\r\n}\r\n}\r\nvoid wb_start_writeback(struct bdi_writeback *wb, long nr_pages,\r\nbool range_cyclic, enum wb_reason reason)\r\n{\r\nstruct wb_writeback_work *work;\r\nif (!wb_has_dirty_io(wb))\r\nreturn;\r\nwork = kzalloc(sizeof(*work),\r\nGFP_NOWAIT | __GFP_NOMEMALLOC | __GFP_NOWARN);\r\nif (!work) {\r\ntrace_writeback_nowork(wb);\r\nwb_wakeup(wb);\r\nreturn;\r\n}\r\nwork->sync_mode = WB_SYNC_NONE;\r\nwork->nr_pages = nr_pages;\r\nwork->range_cyclic = range_cyclic;\r\nwork->reason = reason;\r\nwork->auto_free = 1;\r\nwb_queue_work(wb, work);\r\n}\r\nvoid wb_start_background_writeback(struct bdi_writeback *wb)\r\n{\r\ntrace_writeback_wake_background(wb);\r\nwb_wakeup(wb);\r\n}\r\nvoid inode_io_list_del(struct inode *inode)\r\n{\r\nstruct bdi_writeback *wb;\r\nwb = inode_to_wb_and_lock_list(inode);\r\ninode_io_list_del_locked(inode, wb);\r\nspin_unlock(&wb->list_lock);\r\n}\r\nvoid sb_mark_inode_writeback(struct inode *inode)\r\n{\r\nstruct super_block *sb = inode->i_sb;\r\nunsigned long flags;\r\nif (list_empty(&inode->i_wb_list)) {\r\nspin_lock_irqsave(&sb->s_inode_wblist_lock, flags);\r\nif (list_empty(&inode->i_wb_list)) {\r\nlist_add_tail(&inode->i_wb_list, &sb->s_inodes_wb);\r\ntrace_sb_mark_inode_writeback(inode);\r\n}\r\nspin_unlock_irqrestore(&sb->s_inode_wblist_lock, flags);\r\n}\r\n}\r\nvoid sb_clear_inode_writeback(struct inode *inode)\r\n{\r\nstruct super_block *sb = inode->i_sb;\r\nunsigned long flags;\r\nif (!list_empty(&inode->i_wb_list)) {\r\nspin_lock_irqsave(&sb->s_inode_wblist_lock, flags);\r\nif (!list_empty(&inode->i_wb_list)) {\r\nlist_del_init(&inode->i_wb_list);\r\ntrace_sb_clear_inode_writeback(inode);\r\n}\r\nspin_unlock_irqrestore(&sb->s_inode_wblist_lock, flags);\r\n}\r\n}\r\nstatic void redirty_tail(struct inode *inode, struct bdi_writeback *wb)\r\n{\r\nif (!list_empty(&wb->b_dirty)) {\r\nstruct inode *tail;\r\ntail = wb_inode(wb->b_dirty.next);\r\nif (time_before(inode->dirtied_when, tail->dirtied_when))\r\ninode->dirtied_when = jiffies;\r\n}\r\ninode_io_list_move_locked(inode, wb, &wb->b_dirty);\r\n}\r\nstatic void requeue_io(struct inode *inode, struct bdi_writeback *wb)\r\n{\r\ninode_io_list_move_locked(inode, wb, &wb->b_more_io);\r\n}\r\nstatic void inode_sync_complete(struct inode *inode)\r\n{\r\ninode->i_state &= ~I_SYNC;\r\ninode_add_lru(inode);\r\nsmp_mb();\r\nwake_up_bit(&inode->i_state, __I_SYNC);\r\n}\r\nstatic bool inode_dirtied_after(struct inode *inode, unsigned long t)\r\n{\r\nbool ret = time_after(inode->dirtied_when, t);\r\n#ifndef CONFIG_64BIT\r\nret = ret && time_before_eq(inode->dirtied_when, jiffies);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int move_expired_inodes(struct list_head *delaying_queue,\r\nstruct list_head *dispatch_queue,\r\nint flags,\r\nstruct wb_writeback_work *work)\r\n{\r\nunsigned long *older_than_this = NULL;\r\nunsigned long expire_time;\r\nLIST_HEAD(tmp);\r\nstruct list_head *pos, *node;\r\nstruct super_block *sb = NULL;\r\nstruct inode *inode;\r\nint do_sb_sort = 0;\r\nint moved = 0;\r\nif ((flags & EXPIRE_DIRTY_ATIME) == 0)\r\nolder_than_this = work->older_than_this;\r\nelse if (!work->for_sync) {\r\nexpire_time = jiffies - (dirtytime_expire_interval * HZ);\r\nolder_than_this = &expire_time;\r\n}\r\nwhile (!list_empty(delaying_queue)) {\r\ninode = wb_inode(delaying_queue->prev);\r\nif (older_than_this &&\r\ninode_dirtied_after(inode, *older_than_this))\r\nbreak;\r\nlist_move(&inode->i_io_list, &tmp);\r\nmoved++;\r\nif (flags & EXPIRE_DIRTY_ATIME)\r\nset_bit(__I_DIRTY_TIME_EXPIRED, &inode->i_state);\r\nif (sb_is_blkdev_sb(inode->i_sb))\r\ncontinue;\r\nif (sb && sb != inode->i_sb)\r\ndo_sb_sort = 1;\r\nsb = inode->i_sb;\r\n}\r\nif (!do_sb_sort) {\r\nlist_splice(&tmp, dispatch_queue);\r\ngoto out;\r\n}\r\nwhile (!list_empty(&tmp)) {\r\nsb = wb_inode(tmp.prev)->i_sb;\r\nlist_for_each_prev_safe(pos, node, &tmp) {\r\ninode = wb_inode(pos);\r\nif (inode->i_sb == sb)\r\nlist_move(&inode->i_io_list, dispatch_queue);\r\n}\r\n}\r\nout:\r\nreturn moved;\r\n}\r\nstatic void queue_io(struct bdi_writeback *wb, struct wb_writeback_work *work)\r\n{\r\nint moved;\r\nassert_spin_locked(&wb->list_lock);\r\nlist_splice_init(&wb->b_more_io, &wb->b_io);\r\nmoved = move_expired_inodes(&wb->b_dirty, &wb->b_io, 0, work);\r\nmoved += move_expired_inodes(&wb->b_dirty_time, &wb->b_io,\r\nEXPIRE_DIRTY_ATIME, work);\r\nif (moved)\r\nwb_io_lists_populated(wb);\r\ntrace_writeback_queue_io(wb, work, moved);\r\n}\r\nstatic int write_inode(struct inode *inode, struct writeback_control *wbc)\r\n{\r\nint ret;\r\nif (inode->i_sb->s_op->write_inode && !is_bad_inode(inode)) {\r\ntrace_writeback_write_inode_start(inode, wbc);\r\nret = inode->i_sb->s_op->write_inode(inode, wbc);\r\ntrace_writeback_write_inode(inode, wbc);\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __inode_wait_for_writeback(struct inode *inode)\r\n__releases(inode->i_lock)\r\n__acquires(inode->i_lock)\r\n{\r\nDEFINE_WAIT_BIT(wq, &inode->i_state, __I_SYNC);\r\nwait_queue_head_t *wqh;\r\nwqh = bit_waitqueue(&inode->i_state, __I_SYNC);\r\nwhile (inode->i_state & I_SYNC) {\r\nspin_unlock(&inode->i_lock);\r\n__wait_on_bit(wqh, &wq, bit_wait,\r\nTASK_UNINTERRUPTIBLE);\r\nspin_lock(&inode->i_lock);\r\n}\r\n}\r\nvoid inode_wait_for_writeback(struct inode *inode)\r\n{\r\nspin_lock(&inode->i_lock);\r\n__inode_wait_for_writeback(inode);\r\nspin_unlock(&inode->i_lock);\r\n}\r\nstatic void inode_sleep_on_writeback(struct inode *inode)\r\n__releases(inode->i_lock)\r\n{\r\nDEFINE_WAIT(wait);\r\nwait_queue_head_t *wqh = bit_waitqueue(&inode->i_state, __I_SYNC);\r\nint sleep;\r\nprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);\r\nsleep = inode->i_state & I_SYNC;\r\nspin_unlock(&inode->i_lock);\r\nif (sleep)\r\nschedule();\r\nfinish_wait(wqh, &wait);\r\n}\r\nstatic void requeue_inode(struct inode *inode, struct bdi_writeback *wb,\r\nstruct writeback_control *wbc)\r\n{\r\nif (inode->i_state & I_FREEING)\r\nreturn;\r\nif ((inode->i_state & I_DIRTY) &&\r\n(wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages))\r\ninode->dirtied_when = jiffies;\r\nif (wbc->pages_skipped) {\r\nredirty_tail(inode, wb);\r\nreturn;\r\n}\r\nif (mapping_tagged(inode->i_mapping, PAGECACHE_TAG_DIRTY)) {\r\nif (wbc->nr_to_write <= 0) {\r\nrequeue_io(inode, wb);\r\n} else {\r\nredirty_tail(inode, wb);\r\n}\r\n} else if (inode->i_state & I_DIRTY) {\r\nredirty_tail(inode, wb);\r\n} else if (inode->i_state & I_DIRTY_TIME) {\r\ninode->dirtied_when = jiffies;\r\ninode_io_list_move_locked(inode, wb, &wb->b_dirty_time);\r\n} else {\r\ninode_io_list_del_locked(inode, wb);\r\n}\r\n}\r\nstatic int\r\n__writeback_single_inode(struct inode *inode, struct writeback_control *wbc)\r\n{\r\nstruct address_space *mapping = inode->i_mapping;\r\nlong nr_to_write = wbc->nr_to_write;\r\nunsigned dirty;\r\nint ret;\r\nWARN_ON(!(inode->i_state & I_SYNC));\r\ntrace_writeback_single_inode_start(inode, wbc, nr_to_write);\r\nret = do_writepages(mapping, wbc);\r\nif (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync) {\r\nint err = filemap_fdatawait(mapping);\r\nif (ret == 0)\r\nret = err;\r\n}\r\nspin_lock(&inode->i_lock);\r\ndirty = inode->i_state & I_DIRTY;\r\nif (inode->i_state & I_DIRTY_TIME) {\r\nif ((dirty & (I_DIRTY_SYNC | I_DIRTY_DATASYNC)) ||\r\nwbc->sync_mode == WB_SYNC_ALL ||\r\nunlikely(inode->i_state & I_DIRTY_TIME_EXPIRED) ||\r\nunlikely(time_after(jiffies,\r\n(inode->dirtied_time_when +\r\ndirtytime_expire_interval * HZ)))) {\r\ndirty |= I_DIRTY_TIME | I_DIRTY_TIME_EXPIRED;\r\ntrace_writeback_lazytime(inode);\r\n}\r\n} else\r\ninode->i_state &= ~I_DIRTY_TIME_EXPIRED;\r\ninode->i_state &= ~dirty;\r\nsmp_mb();\r\nif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\r\ninode->i_state |= I_DIRTY_PAGES;\r\nspin_unlock(&inode->i_lock);\r\nif (dirty & I_DIRTY_TIME)\r\nmark_inode_dirty_sync(inode);\r\nif (dirty & ~I_DIRTY_PAGES) {\r\nint err = write_inode(inode, wbc);\r\nif (ret == 0)\r\nret = err;\r\n}\r\ntrace_writeback_single_inode(inode, wbc, nr_to_write);\r\nreturn ret;\r\n}\r\nstatic int writeback_single_inode(struct inode *inode,\r\nstruct writeback_control *wbc)\r\n{\r\nstruct bdi_writeback *wb;\r\nint ret = 0;\r\nspin_lock(&inode->i_lock);\r\nif (!atomic_read(&inode->i_count))\r\nWARN_ON(!(inode->i_state & (I_WILL_FREE|I_FREEING)));\r\nelse\r\nWARN_ON(inode->i_state & I_WILL_FREE);\r\nif (inode->i_state & I_SYNC) {\r\nif (wbc->sync_mode != WB_SYNC_ALL)\r\ngoto out;\r\n__inode_wait_for_writeback(inode);\r\n}\r\nWARN_ON(inode->i_state & I_SYNC);\r\nif (!(inode->i_state & I_DIRTY_ALL) &&\r\n(wbc->sync_mode != WB_SYNC_ALL ||\r\n!mapping_tagged(inode->i_mapping, PAGECACHE_TAG_WRITEBACK)))\r\ngoto out;\r\ninode->i_state |= I_SYNC;\r\nwbc_attach_and_unlock_inode(wbc, inode);\r\nret = __writeback_single_inode(inode, wbc);\r\nwbc_detach_inode(wbc);\r\nwb = inode_to_wb_and_lock_list(inode);\r\nspin_lock(&inode->i_lock);\r\nif (!(inode->i_state & I_DIRTY_ALL))\r\ninode_io_list_del_locked(inode, wb);\r\nspin_unlock(&wb->list_lock);\r\ninode_sync_complete(inode);\r\nout:\r\nspin_unlock(&inode->i_lock);\r\nreturn ret;\r\n}\r\nstatic long writeback_chunk_size(struct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\nlong pages;\r\nif (work->sync_mode == WB_SYNC_ALL || work->tagged_writepages)\r\npages = LONG_MAX;\r\nelse {\r\npages = min(wb->avg_write_bandwidth / 2,\r\nglobal_wb_domain.dirty_limit / DIRTY_SCOPE);\r\npages = min(pages, work->nr_pages);\r\npages = round_down(pages + MIN_WRITEBACK_PAGES,\r\nMIN_WRITEBACK_PAGES);\r\n}\r\nreturn pages;\r\n}\r\nstatic long writeback_sb_inodes(struct super_block *sb,\r\nstruct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\nstruct writeback_control wbc = {\r\n.sync_mode = work->sync_mode,\r\n.tagged_writepages = work->tagged_writepages,\r\n.for_kupdate = work->for_kupdate,\r\n.for_background = work->for_background,\r\n.for_sync = work->for_sync,\r\n.range_cyclic = work->range_cyclic,\r\n.range_start = 0,\r\n.range_end = LLONG_MAX,\r\n};\r\nunsigned long start_time = jiffies;\r\nlong write_chunk;\r\nlong wrote = 0;\r\nwhile (!list_empty(&wb->b_io)) {\r\nstruct inode *inode = wb_inode(wb->b_io.prev);\r\nstruct bdi_writeback *tmp_wb;\r\nif (inode->i_sb != sb) {\r\nif (work->sb) {\r\nredirty_tail(inode, wb);\r\ncontinue;\r\n}\r\nbreak;\r\n}\r\nspin_lock(&inode->i_lock);\r\nif (inode->i_state & (I_NEW | I_FREEING | I_WILL_FREE)) {\r\nspin_unlock(&inode->i_lock);\r\nredirty_tail(inode, wb);\r\ncontinue;\r\n}\r\nif ((inode->i_state & I_SYNC) && wbc.sync_mode != WB_SYNC_ALL) {\r\nspin_unlock(&inode->i_lock);\r\nrequeue_io(inode, wb);\r\ntrace_writeback_sb_inodes_requeue(inode);\r\ncontinue;\r\n}\r\nspin_unlock(&wb->list_lock);\r\nif (inode->i_state & I_SYNC) {\r\ninode_sleep_on_writeback(inode);\r\nspin_lock(&wb->list_lock);\r\ncontinue;\r\n}\r\ninode->i_state |= I_SYNC;\r\nwbc_attach_and_unlock_inode(&wbc, inode);\r\nwrite_chunk = writeback_chunk_size(wb, work);\r\nwbc.nr_to_write = write_chunk;\r\nwbc.pages_skipped = 0;\r\n__writeback_single_inode(inode, &wbc);\r\nwbc_detach_inode(&wbc);\r\nwork->nr_pages -= write_chunk - wbc.nr_to_write;\r\nwrote += write_chunk - wbc.nr_to_write;\r\nif (need_resched()) {\r\nblk_flush_plug(current);\r\ncond_resched();\r\n}\r\ntmp_wb = inode_to_wb_and_lock_list(inode);\r\nspin_lock(&inode->i_lock);\r\nif (!(inode->i_state & I_DIRTY_ALL))\r\nwrote++;\r\nrequeue_inode(inode, tmp_wb, &wbc);\r\ninode_sync_complete(inode);\r\nspin_unlock(&inode->i_lock);\r\nif (unlikely(tmp_wb != wb)) {\r\nspin_unlock(&tmp_wb->list_lock);\r\nspin_lock(&wb->list_lock);\r\n}\r\nif (wrote) {\r\nif (time_is_before_jiffies(start_time + HZ / 10UL))\r\nbreak;\r\nif (work->nr_pages <= 0)\r\nbreak;\r\n}\r\n}\r\nreturn wrote;\r\n}\r\nstatic long __writeback_inodes_wb(struct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\nunsigned long start_time = jiffies;\r\nlong wrote = 0;\r\nwhile (!list_empty(&wb->b_io)) {\r\nstruct inode *inode = wb_inode(wb->b_io.prev);\r\nstruct super_block *sb = inode->i_sb;\r\nif (!trylock_super(sb)) {\r\nredirty_tail(inode, wb);\r\ncontinue;\r\n}\r\nwrote += writeback_sb_inodes(sb, wb, work);\r\nup_read(&sb->s_umount);\r\nif (wrote) {\r\nif (time_is_before_jiffies(start_time + HZ / 10UL))\r\nbreak;\r\nif (work->nr_pages <= 0)\r\nbreak;\r\n}\r\n}\r\nreturn wrote;\r\n}\r\nstatic long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages,\r\nenum wb_reason reason)\r\n{\r\nstruct wb_writeback_work work = {\r\n.nr_pages = nr_pages,\r\n.sync_mode = WB_SYNC_NONE,\r\n.range_cyclic = 1,\r\n.reason = reason,\r\n};\r\nstruct blk_plug plug;\r\nblk_start_plug(&plug);\r\nspin_lock(&wb->list_lock);\r\nif (list_empty(&wb->b_io))\r\nqueue_io(wb, &work);\r\n__writeback_inodes_wb(wb, &work);\r\nspin_unlock(&wb->list_lock);\r\nblk_finish_plug(&plug);\r\nreturn nr_pages - work.nr_pages;\r\n}\r\nstatic long wb_writeback(struct bdi_writeback *wb,\r\nstruct wb_writeback_work *work)\r\n{\r\nunsigned long wb_start = jiffies;\r\nlong nr_pages = work->nr_pages;\r\nunsigned long oldest_jif;\r\nstruct inode *inode;\r\nlong progress;\r\nstruct blk_plug plug;\r\noldest_jif = jiffies;\r\nwork->older_than_this = &oldest_jif;\r\nblk_start_plug(&plug);\r\nspin_lock(&wb->list_lock);\r\nfor (;;) {\r\nif (work->nr_pages <= 0)\r\nbreak;\r\nif ((work->for_background || work->for_kupdate) &&\r\n!list_empty(&wb->work_list))\r\nbreak;\r\nif (work->for_background && !wb_over_bg_thresh(wb))\r\nbreak;\r\nif (work->for_kupdate) {\r\noldest_jif = jiffies -\r\nmsecs_to_jiffies(dirty_expire_interval * 10);\r\n} else if (work->for_background)\r\noldest_jif = jiffies;\r\ntrace_writeback_start(wb, work);\r\nif (list_empty(&wb->b_io))\r\nqueue_io(wb, work);\r\nif (work->sb)\r\nprogress = writeback_sb_inodes(work->sb, wb, work);\r\nelse\r\nprogress = __writeback_inodes_wb(wb, work);\r\ntrace_writeback_written(wb, work);\r\nwb_update_bandwidth(wb, wb_start);\r\nif (progress)\r\ncontinue;\r\nif (list_empty(&wb->b_more_io))\r\nbreak;\r\ntrace_writeback_wait(wb, work);\r\ninode = wb_inode(wb->b_more_io.prev);\r\nspin_lock(&inode->i_lock);\r\nspin_unlock(&wb->list_lock);\r\ninode_sleep_on_writeback(inode);\r\nspin_lock(&wb->list_lock);\r\n}\r\nspin_unlock(&wb->list_lock);\r\nblk_finish_plug(&plug);\r\nreturn nr_pages - work->nr_pages;\r\n}\r\nstatic struct wb_writeback_work *get_next_work_item(struct bdi_writeback *wb)\r\n{\r\nstruct wb_writeback_work *work = NULL;\r\nspin_lock_bh(&wb->work_lock);\r\nif (!list_empty(&wb->work_list)) {\r\nwork = list_entry(wb->work_list.next,\r\nstruct wb_writeback_work, list);\r\nlist_del_init(&work->list);\r\n}\r\nspin_unlock_bh(&wb->work_lock);\r\nreturn work;\r\n}\r\nstatic unsigned long get_nr_dirty_pages(void)\r\n{\r\nreturn global_node_page_state(NR_FILE_DIRTY) +\r\nglobal_node_page_state(NR_UNSTABLE_NFS) +\r\nget_nr_dirty_inodes();\r\n}\r\nstatic long wb_check_background_flush(struct bdi_writeback *wb)\r\n{\r\nif (wb_over_bg_thresh(wb)) {\r\nstruct wb_writeback_work work = {\r\n.nr_pages = LONG_MAX,\r\n.sync_mode = WB_SYNC_NONE,\r\n.for_background = 1,\r\n.range_cyclic = 1,\r\n.reason = WB_REASON_BACKGROUND,\r\n};\r\nreturn wb_writeback(wb, &work);\r\n}\r\nreturn 0;\r\n}\r\nstatic long wb_check_old_data_flush(struct bdi_writeback *wb)\r\n{\r\nunsigned long expired;\r\nlong nr_pages;\r\nif (!dirty_writeback_interval)\r\nreturn 0;\r\nexpired = wb->last_old_flush +\r\nmsecs_to_jiffies(dirty_writeback_interval * 10);\r\nif (time_before(jiffies, expired))\r\nreturn 0;\r\nwb->last_old_flush = jiffies;\r\nnr_pages = get_nr_dirty_pages();\r\nif (nr_pages) {\r\nstruct wb_writeback_work work = {\r\n.nr_pages = nr_pages,\r\n.sync_mode = WB_SYNC_NONE,\r\n.for_kupdate = 1,\r\n.range_cyclic = 1,\r\n.reason = WB_REASON_PERIODIC,\r\n};\r\nreturn wb_writeback(wb, &work);\r\n}\r\nreturn 0;\r\n}\r\nstatic long wb_do_writeback(struct bdi_writeback *wb)\r\n{\r\nstruct wb_writeback_work *work;\r\nlong wrote = 0;\r\nset_bit(WB_writeback_running, &wb->state);\r\nwhile ((work = get_next_work_item(wb)) != NULL) {\r\ntrace_writeback_exec(wb, work);\r\nwrote += wb_writeback(wb, work);\r\nfinish_writeback_work(wb, work);\r\n}\r\nwrote += wb_check_old_data_flush(wb);\r\nwrote += wb_check_background_flush(wb);\r\nclear_bit(WB_writeback_running, &wb->state);\r\nreturn wrote;\r\n}\r\nvoid wb_workfn(struct work_struct *work)\r\n{\r\nstruct bdi_writeback *wb = container_of(to_delayed_work(work),\r\nstruct bdi_writeback, dwork);\r\nlong pages_written;\r\nset_worker_desc("flush-%s", dev_name(wb->bdi->dev));\r\ncurrent->flags |= PF_SWAPWRITE;\r\nif (likely(!current_is_workqueue_rescuer() ||\r\n!test_bit(WB_registered, &wb->state))) {\r\ndo {\r\npages_written = wb_do_writeback(wb);\r\ntrace_writeback_pages_written(pages_written);\r\n} while (!list_empty(&wb->work_list));\r\n} else {\r\npages_written = writeback_inodes_wb(wb, 1024,\r\nWB_REASON_FORKER_THREAD);\r\ntrace_writeback_pages_written(pages_written);\r\n}\r\nif (!list_empty(&wb->work_list))\r\nmod_delayed_work(bdi_wq, &wb->dwork, 0);\r\nelse if (wb_has_dirty_io(wb) && dirty_writeback_interval)\r\nwb_wakeup_delayed(wb);\r\ncurrent->flags &= ~PF_SWAPWRITE;\r\n}\r\nvoid wakeup_flusher_threads(long nr_pages, enum wb_reason reason)\r\n{\r\nstruct backing_dev_info *bdi;\r\nif (blk_needs_flush_plug(current))\r\nblk_schedule_flush_plug(current);\r\nif (!nr_pages)\r\nnr_pages = get_nr_dirty_pages();\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list) {\r\nstruct bdi_writeback *wb;\r\nif (!bdi_has_dirty_io(bdi))\r\ncontinue;\r\nlist_for_each_entry_rcu(wb, &bdi->wb_list, bdi_node)\r\nwb_start_writeback(wb, wb_split_bdi_pages(wb, nr_pages),\r\nfalse, reason);\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic void wakeup_dirtytime_writeback(struct work_struct *w)\r\n{\r\nstruct backing_dev_info *bdi;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list) {\r\nstruct bdi_writeback *wb;\r\nlist_for_each_entry_rcu(wb, &bdi->wb_list, bdi_node)\r\nif (!list_empty(&wb->b_dirty_time))\r\nwb_wakeup(wb);\r\n}\r\nrcu_read_unlock();\r\nschedule_delayed_work(&dirtytime_work, dirtytime_expire_interval * HZ);\r\n}\r\nstatic int __init start_dirtytime_writeback(void)\r\n{\r\nschedule_delayed_work(&dirtytime_work, dirtytime_expire_interval * HZ);\r\nreturn 0;\r\n}\r\nint dirtytime_interval_handler(struct ctl_table *table, int write,\r\nvoid __user *buffer, size_t *lenp, loff_t *ppos)\r\n{\r\nint ret;\r\nret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\r\nif (ret == 0 && write)\r\nmod_delayed_work(system_wq, &dirtytime_work, 0);\r\nreturn ret;\r\n}\r\nstatic noinline void block_dump___mark_inode_dirty(struct inode *inode)\r\n{\r\nif (inode->i_ino || strcmp(inode->i_sb->s_id, "bdev")) {\r\nstruct dentry *dentry;\r\nconst char *name = "?";\r\ndentry = d_find_alias(inode);\r\nif (dentry) {\r\nspin_lock(&dentry->d_lock);\r\nname = (const char *) dentry->d_name.name;\r\n}\r\nprintk(KERN_DEBUG\r\n"%s(%d): dirtied inode %lu (%s) on %s\n",\r\ncurrent->comm, task_pid_nr(current), inode->i_ino,\r\nname, inode->i_sb->s_id);\r\nif (dentry) {\r\nspin_unlock(&dentry->d_lock);\r\ndput(dentry);\r\n}\r\n}\r\n}\r\nvoid __mark_inode_dirty(struct inode *inode, int flags)\r\n{\r\n#define I_DIRTY_INODE (I_DIRTY_SYNC | I_DIRTY_DATASYNC)\r\nstruct super_block *sb = inode->i_sb;\r\nint dirtytime;\r\ntrace_writeback_mark_inode_dirty(inode, flags);\r\nif (flags & (I_DIRTY_SYNC | I_DIRTY_DATASYNC | I_DIRTY_TIME)) {\r\ntrace_writeback_dirty_inode_start(inode, flags);\r\nif (sb->s_op->dirty_inode)\r\nsb->s_op->dirty_inode(inode, flags);\r\ntrace_writeback_dirty_inode(inode, flags);\r\n}\r\nif (flags & I_DIRTY_INODE)\r\nflags &= ~I_DIRTY_TIME;\r\ndirtytime = flags & I_DIRTY_TIME;\r\nsmp_mb();\r\nif (((inode->i_state & flags) == flags) ||\r\n(dirtytime && (inode->i_state & I_DIRTY_INODE)))\r\nreturn;\r\nif (unlikely(block_dump))\r\nblock_dump___mark_inode_dirty(inode);\r\nspin_lock(&inode->i_lock);\r\nif (dirtytime && (inode->i_state & I_DIRTY_INODE))\r\ngoto out_unlock_inode;\r\nif ((inode->i_state & flags) != flags) {\r\nconst int was_dirty = inode->i_state & I_DIRTY;\r\ninode_attach_wb(inode, NULL);\r\nif (flags & I_DIRTY_INODE)\r\ninode->i_state &= ~I_DIRTY_TIME;\r\ninode->i_state |= flags;\r\nif (inode->i_state & I_SYNC)\r\ngoto out_unlock_inode;\r\nif (!S_ISBLK(inode->i_mode)) {\r\nif (inode_unhashed(inode))\r\ngoto out_unlock_inode;\r\n}\r\nif (inode->i_state & I_FREEING)\r\ngoto out_unlock_inode;\r\nif (!was_dirty) {\r\nstruct bdi_writeback *wb;\r\nstruct list_head *dirty_list;\r\nbool wakeup_bdi = false;\r\nwb = locked_inode_to_wb_and_lock_list(inode);\r\nWARN(bdi_cap_writeback_dirty(wb->bdi) &&\r\n!test_bit(WB_registered, &wb->state),\r\n"bdi-%s not registered\n", wb->bdi->name);\r\ninode->dirtied_when = jiffies;\r\nif (dirtytime)\r\ninode->dirtied_time_when = jiffies;\r\nif (inode->i_state & (I_DIRTY_INODE | I_DIRTY_PAGES))\r\ndirty_list = &wb->b_dirty;\r\nelse\r\ndirty_list = &wb->b_dirty_time;\r\nwakeup_bdi = inode_io_list_move_locked(inode, wb,\r\ndirty_list);\r\nspin_unlock(&wb->list_lock);\r\ntrace_writeback_dirty_inode_enqueue(inode);\r\nif (bdi_cap_writeback_dirty(wb->bdi) && wakeup_bdi)\r\nwb_wakeup_delayed(wb);\r\nreturn;\r\n}\r\n}\r\nout_unlock_inode:\r\nspin_unlock(&inode->i_lock);\r\n#undef I_DIRTY_INODE\r\n}\r\nstatic void wait_sb_inodes(struct super_block *sb)\r\n{\r\nLIST_HEAD(sync_list);\r\nWARN_ON(!rwsem_is_locked(&sb->s_umount));\r\nmutex_lock(&sb->s_sync_lock);\r\nrcu_read_lock();\r\nspin_lock_irq(&sb->s_inode_wblist_lock);\r\nlist_splice_init(&sb->s_inodes_wb, &sync_list);\r\nwhile (!list_empty(&sync_list)) {\r\nstruct inode *inode = list_first_entry(&sync_list, struct inode,\r\ni_wb_list);\r\nstruct address_space *mapping = inode->i_mapping;\r\nlist_move_tail(&inode->i_wb_list, &sb->s_inodes_wb);\r\nif (!mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK))\r\ncontinue;\r\nspin_unlock_irq(&sb->s_inode_wblist_lock);\r\nspin_lock(&inode->i_lock);\r\nif (inode->i_state & (I_FREEING|I_WILL_FREE|I_NEW)) {\r\nspin_unlock(&inode->i_lock);\r\nspin_lock_irq(&sb->s_inode_wblist_lock);\r\ncontinue;\r\n}\r\n__iget(inode);\r\nspin_unlock(&inode->i_lock);\r\nrcu_read_unlock();\r\nfilemap_fdatawait_keep_errors(mapping);\r\ncond_resched();\r\niput(inode);\r\nrcu_read_lock();\r\nspin_lock_irq(&sb->s_inode_wblist_lock);\r\n}\r\nspin_unlock_irq(&sb->s_inode_wblist_lock);\r\nrcu_read_unlock();\r\nmutex_unlock(&sb->s_sync_lock);\r\n}\r\nstatic void __writeback_inodes_sb_nr(struct super_block *sb, unsigned long nr,\r\nenum wb_reason reason, bool skip_if_busy)\r\n{\r\nDEFINE_WB_COMPLETION_ONSTACK(done);\r\nstruct wb_writeback_work work = {\r\n.sb = sb,\r\n.sync_mode = WB_SYNC_NONE,\r\n.tagged_writepages = 1,\r\n.done = &done,\r\n.nr_pages = nr,\r\n.reason = reason,\r\n};\r\nstruct backing_dev_info *bdi = sb->s_bdi;\r\nif (!bdi_has_dirty_io(bdi) || bdi == &noop_backing_dev_info)\r\nreturn;\r\nWARN_ON(!rwsem_is_locked(&sb->s_umount));\r\nbdi_split_work_to_wbs(sb->s_bdi, &work, skip_if_busy);\r\nwb_wait_for_completion(bdi, &done);\r\n}\r\nvoid writeback_inodes_sb_nr(struct super_block *sb,\r\nunsigned long nr,\r\nenum wb_reason reason)\r\n{\r\n__writeback_inodes_sb_nr(sb, nr, reason, false);\r\n}\r\nvoid writeback_inodes_sb(struct super_block *sb, enum wb_reason reason)\r\n{\r\nreturn writeback_inodes_sb_nr(sb, get_nr_dirty_pages(), reason);\r\n}\r\nbool try_to_writeback_inodes_sb_nr(struct super_block *sb, unsigned long nr,\r\nenum wb_reason reason)\r\n{\r\nif (!down_read_trylock(&sb->s_umount))\r\nreturn false;\r\n__writeback_inodes_sb_nr(sb, nr, reason, true);\r\nup_read(&sb->s_umount);\r\nreturn true;\r\n}\r\nbool try_to_writeback_inodes_sb(struct super_block *sb, enum wb_reason reason)\r\n{\r\nreturn try_to_writeback_inodes_sb_nr(sb, get_nr_dirty_pages(), reason);\r\n}\r\nvoid sync_inodes_sb(struct super_block *sb)\r\n{\r\nDEFINE_WB_COMPLETION_ONSTACK(done);\r\nstruct wb_writeback_work work = {\r\n.sb = sb,\r\n.sync_mode = WB_SYNC_ALL,\r\n.nr_pages = LONG_MAX,\r\n.range_cyclic = 0,\r\n.done = &done,\r\n.reason = WB_REASON_SYNC,\r\n.for_sync = 1,\r\n};\r\nstruct backing_dev_info *bdi = sb->s_bdi;\r\nif (bdi == &noop_backing_dev_info)\r\nreturn;\r\nWARN_ON(!rwsem_is_locked(&sb->s_umount));\r\nbdi_split_work_to_wbs(bdi, &work, false);\r\nwb_wait_for_completion(bdi, &done);\r\nwait_sb_inodes(sb);\r\n}\r\nint write_inode_now(struct inode *inode, int sync)\r\n{\r\nstruct writeback_control wbc = {\r\n.nr_to_write = LONG_MAX,\r\n.sync_mode = sync ? WB_SYNC_ALL : WB_SYNC_NONE,\r\n.range_start = 0,\r\n.range_end = LLONG_MAX,\r\n};\r\nif (!mapping_cap_writeback_dirty(inode->i_mapping))\r\nwbc.nr_to_write = 0;\r\nmight_sleep();\r\nreturn writeback_single_inode(inode, &wbc);\r\n}\r\nint sync_inode(struct inode *inode, struct writeback_control *wbc)\r\n{\r\nreturn writeback_single_inode(inode, wbc);\r\n}\r\nint sync_inode_metadata(struct inode *inode, int wait)\r\n{\r\nstruct writeback_control wbc = {\r\n.sync_mode = wait ? WB_SYNC_ALL : WB_SYNC_NONE,\r\n.nr_to_write = 0,\r\n};\r\nreturn sync_inode(inode, &wbc);\r\n}
