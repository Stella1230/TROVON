static inline struct vmd_dev *vmd_from_bus(struct pci_bus *bus)\r\n{\r\nreturn container_of(bus->sysdata, struct vmd_dev, sysdata);\r\n}\r\nstatic inline unsigned int index_from_irqs(struct vmd_dev *vmd,\r\nstruct vmd_irq_list *irqs)\r\n{\r\nreturn irqs - vmd->irqs;\r\n}\r\nstatic void vmd_compose_msi_msg(struct irq_data *data, struct msi_msg *msg)\r\n{\r\nstruct vmd_irq *vmdirq = data->chip_data;\r\nstruct vmd_irq_list *irq = vmdirq->irq;\r\nstruct vmd_dev *vmd = irq_data_get_irq_handler_data(data);\r\nmsg->address_hi = MSI_ADDR_BASE_HI;\r\nmsg->address_lo = MSI_ADDR_BASE_LO |\r\nMSI_ADDR_DEST_ID(index_from_irqs(vmd, irq));\r\nmsg->data = 0;\r\n}\r\nstatic void vmd_irq_enable(struct irq_data *data)\r\n{\r\nstruct vmd_irq *vmdirq = data->chip_data;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&list_lock, flags);\r\nWARN_ON(vmdirq->enabled);\r\nlist_add_tail_rcu(&vmdirq->node, &vmdirq->irq->irq_list);\r\nvmdirq->enabled = true;\r\nraw_spin_unlock_irqrestore(&list_lock, flags);\r\ndata->chip->irq_unmask(data);\r\n}\r\nstatic void vmd_irq_disable(struct irq_data *data)\r\n{\r\nstruct vmd_irq *vmdirq = data->chip_data;\r\nunsigned long flags;\r\ndata->chip->irq_mask(data);\r\nraw_spin_lock_irqsave(&list_lock, flags);\r\nif (vmdirq->enabled) {\r\nlist_del_rcu(&vmdirq->node);\r\nvmdirq->enabled = false;\r\n}\r\nraw_spin_unlock_irqrestore(&list_lock, flags);\r\n}\r\nstatic int vmd_irq_set_affinity(struct irq_data *data,\r\nconst struct cpumask *dest, bool force)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic irq_hw_number_t vmd_get_hwirq(struct msi_domain_info *info,\r\nmsi_alloc_info_t *arg)\r\n{\r\nreturn 0;\r\n}\r\nstatic struct vmd_irq_list *vmd_next_irq(struct vmd_dev *vmd, struct msi_desc *desc)\r\n{\r\nint i, best = 1;\r\nunsigned long flags;\r\nif (!desc->msi_attrib.is_msix || vmd->msix_count == 1)\r\nreturn &vmd->irqs[0];\r\nraw_spin_lock_irqsave(&list_lock, flags);\r\nfor (i = 1; i < vmd->msix_count; i++)\r\nif (vmd->irqs[i].count < vmd->irqs[best].count)\r\nbest = i;\r\nvmd->irqs[best].count++;\r\nraw_spin_unlock_irqrestore(&list_lock, flags);\r\nreturn &vmd->irqs[best];\r\n}\r\nstatic int vmd_msi_init(struct irq_domain *domain, struct msi_domain_info *info,\r\nunsigned int virq, irq_hw_number_t hwirq,\r\nmsi_alloc_info_t *arg)\r\n{\r\nstruct msi_desc *desc = arg->desc;\r\nstruct vmd_dev *vmd = vmd_from_bus(msi_desc_to_pci_dev(desc)->bus);\r\nstruct vmd_irq *vmdirq = kzalloc(sizeof(*vmdirq), GFP_KERNEL);\r\nunsigned int index, vector;\r\nif (!vmdirq)\r\nreturn -ENOMEM;\r\nINIT_LIST_HEAD(&vmdirq->node);\r\nvmdirq->irq = vmd_next_irq(vmd, desc);\r\nvmdirq->virq = virq;\r\nindex = index_from_irqs(vmd, vmdirq->irq);\r\nvector = pci_irq_vector(vmd->dev, index);\r\nirq_domain_set_info(domain, virq, vector, info->chip, vmdirq,\r\nhandle_untracked_irq, vmd, NULL);\r\nreturn 0;\r\n}\r\nstatic void vmd_msi_free(struct irq_domain *domain,\r\nstruct msi_domain_info *info, unsigned int virq)\r\n{\r\nstruct vmd_irq *vmdirq = irq_get_chip_data(virq);\r\nunsigned long flags;\r\nsynchronize_srcu(&vmdirq->irq->srcu);\r\nraw_spin_lock_irqsave(&list_lock, flags);\r\nvmdirq->irq->count--;\r\nraw_spin_unlock_irqrestore(&list_lock, flags);\r\nkfree(vmdirq);\r\n}\r\nstatic int vmd_msi_prepare(struct irq_domain *domain, struct device *dev,\r\nint nvec, msi_alloc_info_t *arg)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct vmd_dev *vmd = vmd_from_bus(pdev->bus);\r\nif (nvec > vmd->msix_count)\r\nreturn vmd->msix_count;\r\nmemset(arg, 0, sizeof(*arg));\r\nreturn 0;\r\n}\r\nstatic void vmd_set_desc(msi_alloc_info_t *arg, struct msi_desc *desc)\r\n{\r\narg->desc = desc;\r\n}\r\nstatic struct device *to_vmd_dev(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\nstruct vmd_dev *vmd = vmd_from_bus(pdev->bus);\r\nreturn &vmd->dev->dev;\r\n}\r\nstatic const struct dma_map_ops *vmd_dma_ops(struct device *dev)\r\n{\r\nreturn get_dma_ops(to_vmd_dev(dev));\r\n}\r\nstatic void *vmd_alloc(struct device *dev, size_t size, dma_addr_t *addr,\r\ngfp_t flag, unsigned long attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->alloc(to_vmd_dev(dev), size, addr, flag,\r\nattrs);\r\n}\r\nstatic void vmd_free(struct device *dev, size_t size, void *vaddr,\r\ndma_addr_t addr, unsigned long attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->free(to_vmd_dev(dev), size, vaddr, addr,\r\nattrs);\r\n}\r\nstatic int vmd_mmap(struct device *dev, struct vm_area_struct *vma,\r\nvoid *cpu_addr, dma_addr_t addr, size_t size,\r\nunsigned long attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->mmap(to_vmd_dev(dev), vma, cpu_addr, addr,\r\nsize, attrs);\r\n}\r\nstatic int vmd_get_sgtable(struct device *dev, struct sg_table *sgt,\r\nvoid *cpu_addr, dma_addr_t addr, size_t size,\r\nunsigned long attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->get_sgtable(to_vmd_dev(dev), sgt, cpu_addr,\r\naddr, size, attrs);\r\n}\r\nstatic dma_addr_t vmd_map_page(struct device *dev, struct page *page,\r\nunsigned long offset, size_t size,\r\nenum dma_data_direction dir,\r\nunsigned long attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->map_page(to_vmd_dev(dev), page, offset, size,\r\ndir, attrs);\r\n}\r\nstatic void vmd_unmap_page(struct device *dev, dma_addr_t addr, size_t size,\r\nenum dma_data_direction dir, unsigned long attrs)\r\n{\r\nvmd_dma_ops(dev)->unmap_page(to_vmd_dev(dev), addr, size, dir, attrs);\r\n}\r\nstatic int vmd_map_sg(struct device *dev, struct scatterlist *sg, int nents,\r\nenum dma_data_direction dir, unsigned long attrs)\r\n{\r\nreturn vmd_dma_ops(dev)->map_sg(to_vmd_dev(dev), sg, nents, dir, attrs);\r\n}\r\nstatic void vmd_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\r\nenum dma_data_direction dir, unsigned long attrs)\r\n{\r\nvmd_dma_ops(dev)->unmap_sg(to_vmd_dev(dev), sg, nents, dir, attrs);\r\n}\r\nstatic void vmd_sync_single_for_cpu(struct device *dev, dma_addr_t addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nvmd_dma_ops(dev)->sync_single_for_cpu(to_vmd_dev(dev), addr, size, dir);\r\n}\r\nstatic void vmd_sync_single_for_device(struct device *dev, dma_addr_t addr,\r\nsize_t size, enum dma_data_direction dir)\r\n{\r\nvmd_dma_ops(dev)->sync_single_for_device(to_vmd_dev(dev), addr, size,\r\ndir);\r\n}\r\nstatic void vmd_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction dir)\r\n{\r\nvmd_dma_ops(dev)->sync_sg_for_cpu(to_vmd_dev(dev), sg, nents, dir);\r\n}\r\nstatic void vmd_sync_sg_for_device(struct device *dev, struct scatterlist *sg,\r\nint nents, enum dma_data_direction dir)\r\n{\r\nvmd_dma_ops(dev)->sync_sg_for_device(to_vmd_dev(dev), sg, nents, dir);\r\n}\r\nstatic int vmd_mapping_error(struct device *dev, dma_addr_t addr)\r\n{\r\nreturn vmd_dma_ops(dev)->mapping_error(to_vmd_dev(dev), addr);\r\n}\r\nstatic int vmd_dma_supported(struct device *dev, u64 mask)\r\n{\r\nreturn vmd_dma_ops(dev)->dma_supported(to_vmd_dev(dev), mask);\r\n}\r\nstatic u64 vmd_get_required_mask(struct device *dev)\r\n{\r\nreturn vmd_dma_ops(dev)->get_required_mask(to_vmd_dev(dev));\r\n}\r\nstatic void vmd_teardown_dma_ops(struct vmd_dev *vmd)\r\n{\r\nstruct dma_domain *domain = &vmd->dma_domain;\r\nif (get_dma_ops(&vmd->dev->dev))\r\ndel_dma_domain(domain);\r\n}\r\nstatic void vmd_setup_dma_ops(struct vmd_dev *vmd)\r\n{\r\nconst struct dma_map_ops *source = get_dma_ops(&vmd->dev->dev);\r\nstruct dma_map_ops *dest = &vmd->dma_ops;\r\nstruct dma_domain *domain = &vmd->dma_domain;\r\ndomain->domain_nr = vmd->sysdata.domain;\r\ndomain->dma_ops = dest;\r\nif (!source)\r\nreturn;\r\nASSIGN_VMD_DMA_OPS(source, dest, alloc);\r\nASSIGN_VMD_DMA_OPS(source, dest, free);\r\nASSIGN_VMD_DMA_OPS(source, dest, mmap);\r\nASSIGN_VMD_DMA_OPS(source, dest, get_sgtable);\r\nASSIGN_VMD_DMA_OPS(source, dest, map_page);\r\nASSIGN_VMD_DMA_OPS(source, dest, unmap_page);\r\nASSIGN_VMD_DMA_OPS(source, dest, map_sg);\r\nASSIGN_VMD_DMA_OPS(source, dest, unmap_sg);\r\nASSIGN_VMD_DMA_OPS(source, dest, sync_single_for_cpu);\r\nASSIGN_VMD_DMA_OPS(source, dest, sync_single_for_device);\r\nASSIGN_VMD_DMA_OPS(source, dest, sync_sg_for_cpu);\r\nASSIGN_VMD_DMA_OPS(source, dest, sync_sg_for_device);\r\nASSIGN_VMD_DMA_OPS(source, dest, mapping_error);\r\nASSIGN_VMD_DMA_OPS(source, dest, dma_supported);\r\n#ifdef ARCH_HAS_DMA_GET_REQUIRED_MASK\r\nASSIGN_VMD_DMA_OPS(source, dest, get_required_mask);\r\n#endif\r\nadd_dma_domain(domain);\r\n}\r\nstatic void vmd_teardown_dma_ops(struct vmd_dev *vmd) {}\r\nstatic void vmd_setup_dma_ops(struct vmd_dev *vmd) {}\r\nstatic char __iomem *vmd_cfg_addr(struct vmd_dev *vmd, struct pci_bus *bus,\r\nunsigned int devfn, int reg, int len)\r\n{\r\nchar __iomem *addr = vmd->cfgbar +\r\n(bus->number << 20) + (devfn << 12) + reg;\r\nif ((addr - vmd->cfgbar) + len >=\r\nresource_size(&vmd->dev->resource[VMD_CFGBAR]))\r\nreturn NULL;\r\nreturn addr;\r\n}\r\nstatic int vmd_pci_read(struct pci_bus *bus, unsigned int devfn, int reg,\r\nint len, u32 *value)\r\n{\r\nstruct vmd_dev *vmd = vmd_from_bus(bus);\r\nchar __iomem *addr = vmd_cfg_addr(vmd, bus, devfn, reg, len);\r\nunsigned long flags;\r\nint ret = 0;\r\nif (!addr)\r\nreturn -EFAULT;\r\nspin_lock_irqsave(&vmd->cfg_lock, flags);\r\nswitch (len) {\r\ncase 1:\r\n*value = readb(addr);\r\nbreak;\r\ncase 2:\r\n*value = readw(addr);\r\nbreak;\r\ncase 4:\r\n*value = readl(addr);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&vmd->cfg_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int vmd_pci_write(struct pci_bus *bus, unsigned int devfn, int reg,\r\nint len, u32 value)\r\n{\r\nstruct vmd_dev *vmd = vmd_from_bus(bus);\r\nchar __iomem *addr = vmd_cfg_addr(vmd, bus, devfn, reg, len);\r\nunsigned long flags;\r\nint ret = 0;\r\nif (!addr)\r\nreturn -EFAULT;\r\nspin_lock_irqsave(&vmd->cfg_lock, flags);\r\nswitch (len) {\r\ncase 1:\r\nwriteb(value, addr);\r\nreadb(addr);\r\nbreak;\r\ncase 2:\r\nwritew(value, addr);\r\nreadw(addr);\r\nbreak;\r\ncase 4:\r\nwritel(value, addr);\r\nreadl(addr);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&vmd->cfg_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void vmd_attach_resources(struct vmd_dev *vmd)\r\n{\r\nvmd->dev->resource[VMD_MEMBAR1].child = &vmd->resources[1];\r\nvmd->dev->resource[VMD_MEMBAR2].child = &vmd->resources[2];\r\n}\r\nstatic void vmd_detach_resources(struct vmd_dev *vmd)\r\n{\r\nvmd->dev->resource[VMD_MEMBAR1].child = NULL;\r\nvmd->dev->resource[VMD_MEMBAR2].child = NULL;\r\n}\r\nstatic int vmd_find_free_domain(void)\r\n{\r\nint domain = 0xffff;\r\nstruct pci_bus *bus = NULL;\r\nwhile ((bus = pci_find_next_bus(bus)) != NULL)\r\ndomain = max_t(int, domain, pci_domain_nr(bus));\r\nreturn domain + 1;\r\n}\r\nstatic int vmd_enable_domain(struct vmd_dev *vmd)\r\n{\r\nstruct pci_sysdata *sd = &vmd->sysdata;\r\nstruct resource *res;\r\nu32 upper_bits;\r\nunsigned long flags;\r\nLIST_HEAD(resources);\r\nres = &vmd->dev->resource[VMD_CFGBAR];\r\nvmd->resources[0] = (struct resource) {\r\n.name = "VMD CFGBAR",\r\n.start = 0,\r\n.end = (resource_size(res) >> 20) - 1,\r\n.flags = IORESOURCE_BUS | IORESOURCE_PCI_FIXED,\r\n};\r\nres = &vmd->dev->resource[VMD_MEMBAR1];\r\nupper_bits = upper_32_bits(res->end);\r\nflags = res->flags & ~IORESOURCE_SIZEALIGN;\r\nif (!upper_bits)\r\nflags &= ~IORESOURCE_MEM_64;\r\nvmd->resources[1] = (struct resource) {\r\n.name = "VMD MEMBAR1",\r\n.start = res->start,\r\n.end = res->end,\r\n.flags = flags,\r\n.parent = res,\r\n};\r\nres = &vmd->dev->resource[VMD_MEMBAR2];\r\nupper_bits = upper_32_bits(res->end);\r\nflags = res->flags & ~IORESOURCE_SIZEALIGN;\r\nif (!upper_bits)\r\nflags &= ~IORESOURCE_MEM_64;\r\nvmd->resources[2] = (struct resource) {\r\n.name = "VMD MEMBAR2",\r\n.start = res->start + 0x2000,\r\n.end = res->end,\r\n.flags = flags,\r\n.parent = res,\r\n};\r\nsd->vmd_domain = true;\r\nsd->domain = vmd_find_free_domain();\r\nif (sd->domain < 0)\r\nreturn sd->domain;\r\nsd->node = pcibus_to_node(vmd->dev->bus);\r\nvmd->irq_domain = pci_msi_create_irq_domain(NULL, &vmd_msi_domain_info,\r\nx86_vector_domain);\r\nif (!vmd->irq_domain)\r\nreturn -ENODEV;\r\npci_add_resource(&resources, &vmd->resources[0]);\r\npci_add_resource(&resources, &vmd->resources[1]);\r\npci_add_resource(&resources, &vmd->resources[2]);\r\nvmd->bus = pci_create_root_bus(&vmd->dev->dev, 0, &vmd_ops, sd,\r\n&resources);\r\nif (!vmd->bus) {\r\npci_free_resource_list(&resources);\r\nirq_domain_remove(vmd->irq_domain);\r\nreturn -ENODEV;\r\n}\r\nvmd_attach_resources(vmd);\r\nvmd_setup_dma_ops(vmd);\r\ndev_set_msi_domain(&vmd->bus->dev, vmd->irq_domain);\r\npci_rescan_bus(vmd->bus);\r\nWARN(sysfs_create_link(&vmd->dev->dev.kobj, &vmd->bus->dev.kobj,\r\n"domain"), "Can't create symlink to domain\n");\r\nreturn 0;\r\n}\r\nstatic irqreturn_t vmd_irq(int irq, void *data)\r\n{\r\nstruct vmd_irq_list *irqs = data;\r\nstruct vmd_irq *vmdirq;\r\nint idx;\r\nidx = srcu_read_lock(&irqs->srcu);\r\nlist_for_each_entry_rcu(vmdirq, &irqs->irq_list, node)\r\ngeneric_handle_irq(vmdirq->virq);\r\nsrcu_read_unlock(&irqs->srcu, idx);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int vmd_probe(struct pci_dev *dev, const struct pci_device_id *id)\r\n{\r\nstruct vmd_dev *vmd;\r\nint i, err;\r\nif (resource_size(&dev->resource[VMD_CFGBAR]) < (1 << 20))\r\nreturn -ENOMEM;\r\nvmd = devm_kzalloc(&dev->dev, sizeof(*vmd), GFP_KERNEL);\r\nif (!vmd)\r\nreturn -ENOMEM;\r\nvmd->dev = dev;\r\nerr = pcim_enable_device(dev);\r\nif (err < 0)\r\nreturn err;\r\nvmd->cfgbar = pcim_iomap(dev, VMD_CFGBAR, 0);\r\nif (!vmd->cfgbar)\r\nreturn -ENOMEM;\r\npci_set_master(dev);\r\nif (dma_set_mask_and_coherent(&dev->dev, DMA_BIT_MASK(64)) &&\r\ndma_set_mask_and_coherent(&dev->dev, DMA_BIT_MASK(32)))\r\nreturn -ENODEV;\r\nvmd->msix_count = pci_msix_vec_count(dev);\r\nif (vmd->msix_count < 0)\r\nreturn -ENODEV;\r\nvmd->msix_count = pci_alloc_irq_vectors(dev, 1, vmd->msix_count,\r\nPCI_IRQ_MSIX | PCI_IRQ_AFFINITY);\r\nif (vmd->msix_count < 0)\r\nreturn vmd->msix_count;\r\nvmd->irqs = devm_kcalloc(&dev->dev, vmd->msix_count, sizeof(*vmd->irqs),\r\nGFP_KERNEL);\r\nif (!vmd->irqs)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < vmd->msix_count; i++) {\r\nerr = init_srcu_struct(&vmd->irqs[i].srcu);\r\nif (err)\r\nreturn err;\r\nINIT_LIST_HEAD(&vmd->irqs[i].irq_list);\r\nerr = devm_request_irq(&dev->dev, pci_irq_vector(dev, i),\r\nvmd_irq, 0, "vmd", &vmd->irqs[i]);\r\nif (err)\r\nreturn err;\r\n}\r\nspin_lock_init(&vmd->cfg_lock);\r\npci_set_drvdata(dev, vmd);\r\nerr = vmd_enable_domain(vmd);\r\nif (err)\r\nreturn err;\r\ndev_info(&vmd->dev->dev, "Bound to PCI domain %04x\n",\r\nvmd->sysdata.domain);\r\nreturn 0;\r\n}\r\nstatic void vmd_cleanup_srcu(struct vmd_dev *vmd)\r\n{\r\nint i;\r\nfor (i = 0; i < vmd->msix_count; i++)\r\ncleanup_srcu_struct(&vmd->irqs[i].srcu);\r\n}\r\nstatic void vmd_remove(struct pci_dev *dev)\r\n{\r\nstruct vmd_dev *vmd = pci_get_drvdata(dev);\r\nvmd_detach_resources(vmd);\r\nvmd_cleanup_srcu(vmd);\r\nsysfs_remove_link(&vmd->dev->dev.kobj, "domain");\r\npci_stop_root_bus(vmd->bus);\r\npci_remove_root_bus(vmd->bus);\r\nvmd_teardown_dma_ops(vmd);\r\nirq_domain_remove(vmd->irq_domain);\r\n}\r\nstatic int vmd_suspend(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\npci_save_state(pdev);\r\nreturn 0;\r\n}\r\nstatic int vmd_resume(struct device *dev)\r\n{\r\nstruct pci_dev *pdev = to_pci_dev(dev);\r\npci_restore_state(pdev);\r\nreturn 0;\r\n}
