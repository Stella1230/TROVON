static struct ipmmu_vmsa_domain *to_vmsa_domain(struct iommu_domain *dom)\r\n{\r\nreturn container_of(dom, struct ipmmu_vmsa_domain, io_domain);\r\n}\r\nstatic u32 ipmmu_read(struct ipmmu_vmsa_device *mmu, unsigned int offset)\r\n{\r\nreturn ioread32(mmu->base + offset);\r\n}\r\nstatic void ipmmu_write(struct ipmmu_vmsa_device *mmu, unsigned int offset,\r\nu32 data)\r\n{\r\niowrite32(data, mmu->base + offset);\r\n}\r\nstatic u32 ipmmu_ctx_read(struct ipmmu_vmsa_domain *domain, unsigned int reg)\r\n{\r\nreturn ipmmu_read(domain->mmu, domain->context_id * IM_CTX_SIZE + reg);\r\n}\r\nstatic void ipmmu_ctx_write(struct ipmmu_vmsa_domain *domain, unsigned int reg,\r\nu32 data)\r\n{\r\nipmmu_write(domain->mmu, domain->context_id * IM_CTX_SIZE + reg, data);\r\n}\r\nstatic void ipmmu_tlb_sync(struct ipmmu_vmsa_domain *domain)\r\n{\r\nunsigned int count = 0;\r\nwhile (ipmmu_ctx_read(domain, IMCTR) & IMCTR_FLUSH) {\r\ncpu_relax();\r\nif (++count == TLB_LOOP_TIMEOUT) {\r\ndev_err_ratelimited(domain->mmu->dev,\r\n"TLB sync timed out -- MMU may be deadlocked\n");\r\nreturn;\r\n}\r\nudelay(1);\r\n}\r\n}\r\nstatic void ipmmu_tlb_invalidate(struct ipmmu_vmsa_domain *domain)\r\n{\r\nu32 reg;\r\nreg = ipmmu_ctx_read(domain, IMCTR);\r\nreg |= IMCTR_FLUSH;\r\nipmmu_ctx_write(domain, IMCTR, reg);\r\nipmmu_tlb_sync(domain);\r\n}\r\nstatic void ipmmu_utlb_enable(struct ipmmu_vmsa_domain *domain,\r\nunsigned int utlb)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = domain->mmu;\r\nipmmu_write(mmu, IMUASID(utlb), 0);\r\nipmmu_write(mmu, IMUCTR(utlb),\r\nIMUCTR_TTSEL_MMU(domain->context_id) | IMUCTR_FLUSH |\r\nIMUCTR_MMUEN);\r\n}\r\nstatic void ipmmu_utlb_disable(struct ipmmu_vmsa_domain *domain,\r\nunsigned int utlb)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = domain->mmu;\r\nipmmu_write(mmu, IMUCTR(utlb), 0);\r\n}\r\nstatic void ipmmu_tlb_flush_all(void *cookie)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = cookie;\r\nipmmu_tlb_invalidate(domain);\r\n}\r\nstatic void ipmmu_tlb_add_flush(unsigned long iova, size_t size,\r\nsize_t granule, bool leaf, void *cookie)\r\n{\r\n}\r\nstatic int ipmmu_domain_init_context(struct ipmmu_vmsa_domain *domain)\r\n{\r\nu64 ttbr;\r\ndomain->cfg.quirks = IO_PGTABLE_QUIRK_ARM_NS;\r\ndomain->cfg.pgsize_bitmap = SZ_1G | SZ_2M | SZ_4K,\r\ndomain->cfg.ias = 32;\r\ndomain->cfg.oas = 40;\r\ndomain->cfg.tlb = &ipmmu_gather_ops;\r\ndomain->io_domain.geometry.aperture_end = DMA_BIT_MASK(32);\r\ndomain->io_domain.geometry.force_aperture = true;\r\ndomain->cfg.iommu_dev = domain->mmu->dev;\r\ndomain->iop = alloc_io_pgtable_ops(ARM_32_LPAE_S1, &domain->cfg,\r\ndomain);\r\nif (!domain->iop)\r\nreturn -EINVAL;\r\ndomain->context_id = 0;\r\nttbr = domain->cfg.arm_lpae_s1_cfg.ttbr[0];\r\nipmmu_ctx_write(domain, IMTTLBR0, ttbr);\r\nipmmu_ctx_write(domain, IMTTUBR0, ttbr >> 32);\r\nipmmu_ctx_write(domain, IMTTBCR, IMTTBCR_EAE |\r\nIMTTBCR_SH0_INNER_SHAREABLE | IMTTBCR_ORGN0_WB_WA |\r\nIMTTBCR_IRGN0_WB_WA | IMTTBCR_SL0_LVL_1);\r\nipmmu_ctx_write(domain, IMMAIR0, domain->cfg.arm_lpae_s1_cfg.mair[0]);\r\nipmmu_ctx_write(domain, IMBUSCR,\r\nipmmu_ctx_read(domain, IMBUSCR) &\r\n~(IMBUSCR_DVM | IMBUSCR_BUSSEL_MASK));\r\nipmmu_ctx_write(domain, IMSTR, ipmmu_ctx_read(domain, IMSTR));\r\nipmmu_ctx_write(domain, IMCTR, IMCTR_INTEN | IMCTR_FLUSH | IMCTR_MMUEN);\r\nreturn 0;\r\n}\r\nstatic void ipmmu_domain_destroy_context(struct ipmmu_vmsa_domain *domain)\r\n{\r\nipmmu_ctx_write(domain, IMCTR, IMCTR_FLUSH);\r\nipmmu_tlb_sync(domain);\r\n}\r\nstatic irqreturn_t ipmmu_domain_irq(struct ipmmu_vmsa_domain *domain)\r\n{\r\nconst u32 err_mask = IMSTR_MHIT | IMSTR_ABORT | IMSTR_PF | IMSTR_TF;\r\nstruct ipmmu_vmsa_device *mmu = domain->mmu;\r\nu32 status;\r\nu32 iova;\r\nstatus = ipmmu_ctx_read(domain, IMSTR);\r\nif (!(status & err_mask))\r\nreturn IRQ_NONE;\r\niova = ipmmu_ctx_read(domain, IMEAR);\r\nipmmu_ctx_write(domain, IMSTR, 0);\r\nif (status & IMSTR_MHIT)\r\ndev_err_ratelimited(mmu->dev, "Multiple TLB hits @0x%08x\n",\r\niova);\r\nif (status & IMSTR_ABORT)\r\ndev_err_ratelimited(mmu->dev, "Page Table Walk Abort @0x%08x\n",\r\niova);\r\nif (!(status & (IMSTR_PF | IMSTR_TF)))\r\nreturn IRQ_NONE;\r\nif (!report_iommu_fault(&domain->io_domain, mmu->dev, iova, 0))\r\nreturn IRQ_HANDLED;\r\ndev_err_ratelimited(mmu->dev,\r\n"Unhandled fault: status 0x%08x iova 0x%08x\n",\r\nstatus, iova);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t ipmmu_irq(int irq, void *dev)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = dev;\r\nstruct iommu_domain *io_domain;\r\nstruct ipmmu_vmsa_domain *domain;\r\nif (!mmu->mapping)\r\nreturn IRQ_NONE;\r\nio_domain = mmu->mapping->domain;\r\ndomain = to_vmsa_domain(io_domain);\r\nreturn ipmmu_domain_irq(domain);\r\n}\r\nstatic struct iommu_domain *ipmmu_domain_alloc(unsigned type)\r\n{\r\nstruct ipmmu_vmsa_domain *domain;\r\nif (type != IOMMU_DOMAIN_UNMANAGED)\r\nreturn NULL;\r\ndomain = kzalloc(sizeof(*domain), GFP_KERNEL);\r\nif (!domain)\r\nreturn NULL;\r\nspin_lock_init(&domain->lock);\r\nreturn &domain->io_domain;\r\n}\r\nstatic void ipmmu_domain_free(struct iommu_domain *io_domain)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\r\nipmmu_domain_destroy_context(domain);\r\nfree_io_pgtable_ops(domain->iop);\r\nkfree(domain);\r\n}\r\nstatic int ipmmu_attach_device(struct iommu_domain *io_domain,\r\nstruct device *dev)\r\n{\r\nstruct ipmmu_vmsa_archdata *archdata = dev->archdata.iommu;\r\nstruct ipmmu_vmsa_device *mmu = archdata->mmu;\r\nstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\r\nunsigned long flags;\r\nunsigned int i;\r\nint ret = 0;\r\nif (!mmu) {\r\ndev_err(dev, "Cannot attach to IPMMU\n");\r\nreturn -ENXIO;\r\n}\r\nspin_lock_irqsave(&domain->lock, flags);\r\nif (!domain->mmu) {\r\ndomain->mmu = mmu;\r\nret = ipmmu_domain_init_context(domain);\r\n} else if (domain->mmu != mmu) {\r\ndev_err(dev, "Can't attach IPMMU %s to domain on IPMMU %s\n",\r\ndev_name(mmu->dev), dev_name(domain->mmu->dev));\r\nret = -EINVAL;\r\n}\r\nspin_unlock_irqrestore(&domain->lock, flags);\r\nif (ret < 0)\r\nreturn ret;\r\nfor (i = 0; i < archdata->num_utlbs; ++i)\r\nipmmu_utlb_enable(domain, archdata->utlbs[i]);\r\nreturn 0;\r\n}\r\nstatic void ipmmu_detach_device(struct iommu_domain *io_domain,\r\nstruct device *dev)\r\n{\r\nstruct ipmmu_vmsa_archdata *archdata = dev->archdata.iommu;\r\nstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\r\nunsigned int i;\r\nfor (i = 0; i < archdata->num_utlbs; ++i)\r\nipmmu_utlb_disable(domain, archdata->utlbs[i]);\r\n}\r\nstatic int ipmmu_map(struct iommu_domain *io_domain, unsigned long iova,\r\nphys_addr_t paddr, size_t size, int prot)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\r\nif (!domain)\r\nreturn -ENODEV;\r\nreturn domain->iop->map(domain->iop, iova, paddr, size, prot);\r\n}\r\nstatic size_t ipmmu_unmap(struct iommu_domain *io_domain, unsigned long iova,\r\nsize_t size)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\r\nreturn domain->iop->unmap(domain->iop, iova, size);\r\n}\r\nstatic phys_addr_t ipmmu_iova_to_phys(struct iommu_domain *io_domain,\r\ndma_addr_t iova)\r\n{\r\nstruct ipmmu_vmsa_domain *domain = to_vmsa_domain(io_domain);\r\nreturn domain->iop->iova_to_phys(domain->iop, iova);\r\n}\r\nstatic int ipmmu_find_utlbs(struct ipmmu_vmsa_device *mmu, struct device *dev,\r\nunsigned int *utlbs, unsigned int num_utlbs)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < num_utlbs; ++i) {\r\nstruct of_phandle_args args;\r\nint ret;\r\nret = of_parse_phandle_with_args(dev->of_node, "iommus",\r\n"#iommu-cells", i, &args);\r\nif (ret < 0)\r\nreturn ret;\r\nof_node_put(args.np);\r\nif (args.np != mmu->dev->of_node || args.args_count != 1)\r\nreturn -EINVAL;\r\nutlbs[i] = args.args[0];\r\n}\r\nreturn 0;\r\n}\r\nstatic int ipmmu_add_device(struct device *dev)\r\n{\r\nstruct ipmmu_vmsa_archdata *archdata;\r\nstruct ipmmu_vmsa_device *mmu;\r\nstruct iommu_group *group = NULL;\r\nunsigned int *utlbs;\r\nunsigned int i;\r\nint num_utlbs;\r\nint ret = -ENODEV;\r\nif (dev->archdata.iommu) {\r\ndev_warn(dev, "IOMMU driver already assigned to device %s\n",\r\ndev_name(dev));\r\nreturn -EINVAL;\r\n}\r\nnum_utlbs = of_count_phandle_with_args(dev->of_node, "iommus",\r\n"#iommu-cells");\r\nif (num_utlbs < 0)\r\nreturn -ENODEV;\r\nutlbs = kcalloc(num_utlbs, sizeof(*utlbs), GFP_KERNEL);\r\nif (!utlbs)\r\nreturn -ENOMEM;\r\nspin_lock(&ipmmu_devices_lock);\r\nlist_for_each_entry(mmu, &ipmmu_devices, list) {\r\nret = ipmmu_find_utlbs(mmu, dev, utlbs, num_utlbs);\r\nif (!ret) {\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&ipmmu_devices_lock);\r\nif (ret < 0)\r\ngoto error;\r\nfor (i = 0; i < num_utlbs; ++i) {\r\nif (utlbs[i] >= mmu->num_utlbs) {\r\nret = -EINVAL;\r\ngoto error;\r\n}\r\n}\r\ngroup = iommu_group_alloc();\r\nif (IS_ERR(group)) {\r\ndev_err(dev, "Failed to allocate IOMMU group\n");\r\nret = PTR_ERR(group);\r\ngoto error;\r\n}\r\nret = iommu_group_add_device(group, dev);\r\niommu_group_put(group);\r\nif (ret < 0) {\r\ndev_err(dev, "Failed to add device to IPMMU group\n");\r\ngroup = NULL;\r\ngoto error;\r\n}\r\narchdata = kzalloc(sizeof(*archdata), GFP_KERNEL);\r\nif (!archdata) {\r\nret = -ENOMEM;\r\ngoto error;\r\n}\r\narchdata->mmu = mmu;\r\narchdata->utlbs = utlbs;\r\narchdata->num_utlbs = num_utlbs;\r\ndev->archdata.iommu = archdata;\r\nif (!mmu->mapping) {\r\nstruct dma_iommu_mapping *mapping;\r\nmapping = arm_iommu_create_mapping(&platform_bus_type,\r\nSZ_1G, SZ_2G);\r\nif (IS_ERR(mapping)) {\r\ndev_err(mmu->dev, "failed to create ARM IOMMU mapping\n");\r\nret = PTR_ERR(mapping);\r\ngoto error;\r\n}\r\nmmu->mapping = mapping;\r\n}\r\nret = arm_iommu_attach_device(dev, mmu->mapping);\r\nif (ret < 0) {\r\ndev_err(dev, "Failed to attach device to VA mapping\n");\r\ngoto error;\r\n}\r\nreturn 0;\r\nerror:\r\narm_iommu_release_mapping(mmu->mapping);\r\nkfree(dev->archdata.iommu);\r\nkfree(utlbs);\r\ndev->archdata.iommu = NULL;\r\nif (!IS_ERR_OR_NULL(group))\r\niommu_group_remove_device(dev);\r\nreturn ret;\r\n}\r\nstatic void ipmmu_remove_device(struct device *dev)\r\n{\r\nstruct ipmmu_vmsa_archdata *archdata = dev->archdata.iommu;\r\narm_iommu_detach_device(dev);\r\niommu_group_remove_device(dev);\r\nkfree(archdata->utlbs);\r\nkfree(archdata);\r\ndev->archdata.iommu = NULL;\r\n}\r\nstatic void ipmmu_device_reset(struct ipmmu_vmsa_device *mmu)\r\n{\r\nunsigned int i;\r\nfor (i = 0; i < 4; ++i)\r\nipmmu_write(mmu, i * IM_CTX_SIZE + IMCTR, 0);\r\n}\r\nstatic int ipmmu_probe(struct platform_device *pdev)\r\n{\r\nstruct ipmmu_vmsa_device *mmu;\r\nstruct resource *res;\r\nint irq;\r\nint ret;\r\nif (!IS_ENABLED(CONFIG_OF) && !pdev->dev.platform_data) {\r\ndev_err(&pdev->dev, "missing platform data\n");\r\nreturn -EINVAL;\r\n}\r\nmmu = devm_kzalloc(&pdev->dev, sizeof(*mmu), GFP_KERNEL);\r\nif (!mmu) {\r\ndev_err(&pdev->dev, "cannot allocate device data\n");\r\nreturn -ENOMEM;\r\n}\r\nmmu->dev = &pdev->dev;\r\nmmu->num_utlbs = 32;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nmmu->base = devm_ioremap_resource(&pdev->dev, res);\r\nif (IS_ERR(mmu->base))\r\nreturn PTR_ERR(mmu->base);\r\nmmu->base += IM_NS_ALIAS_OFFSET;\r\nirq = platform_get_irq(pdev, 0);\r\nif (irq < 0) {\r\ndev_err(&pdev->dev, "no IRQ found\n");\r\nreturn irq;\r\n}\r\nret = devm_request_irq(&pdev->dev, irq, ipmmu_irq, 0,\r\ndev_name(&pdev->dev), mmu);\r\nif (ret < 0) {\r\ndev_err(&pdev->dev, "failed to request IRQ %d\n", irq);\r\nreturn ret;\r\n}\r\nipmmu_device_reset(mmu);\r\nspin_lock(&ipmmu_devices_lock);\r\nlist_add(&mmu->list, &ipmmu_devices);\r\nspin_unlock(&ipmmu_devices_lock);\r\nplatform_set_drvdata(pdev, mmu);\r\nreturn 0;\r\n}\r\nstatic int ipmmu_remove(struct platform_device *pdev)\r\n{\r\nstruct ipmmu_vmsa_device *mmu = platform_get_drvdata(pdev);\r\nspin_lock(&ipmmu_devices_lock);\r\nlist_del(&mmu->list);\r\nspin_unlock(&ipmmu_devices_lock);\r\narm_iommu_release_mapping(mmu->mapping);\r\nipmmu_device_reset(mmu);\r\nreturn 0;\r\n}\r\nstatic int __init ipmmu_init(void)\r\n{\r\nint ret;\r\nret = platform_driver_register(&ipmmu_driver);\r\nif (ret < 0)\r\nreturn ret;\r\nif (!iommu_present(&platform_bus_type))\r\nbus_set_iommu(&platform_bus_type, &ipmmu_ops);\r\nreturn 0;\r\n}\r\nstatic void __exit ipmmu_exit(void)\r\n{\r\nreturn platform_driver_unregister(&ipmmu_driver);\r\n}
