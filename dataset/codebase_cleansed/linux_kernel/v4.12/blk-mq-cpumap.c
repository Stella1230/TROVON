static int cpu_to_queue_index(unsigned int nr_cpus, unsigned int nr_queues,\r\nconst int cpu)\r\n{\r\nreturn cpu * nr_queues / nr_cpus;\r\n}\r\nstatic int get_first_sibling(unsigned int cpu)\r\n{\r\nunsigned int ret;\r\nret = cpumask_first(topology_sibling_cpumask(cpu));\r\nif (ret < nr_cpu_ids)\r\nreturn ret;\r\nreturn cpu;\r\n}\r\nint blk_mq_map_queues(struct blk_mq_tag_set *set)\r\n{\r\nunsigned int *map = set->mq_map;\r\nunsigned int nr_queues = set->nr_hw_queues;\r\nconst struct cpumask *online_mask = cpu_online_mask;\r\nunsigned int i, nr_cpus, nr_uniq_cpus, queue, first_sibling;\r\ncpumask_var_t cpus;\r\nif (!alloc_cpumask_var(&cpus, GFP_ATOMIC))\r\nreturn -ENOMEM;\r\ncpumask_clear(cpus);\r\nnr_cpus = nr_uniq_cpus = 0;\r\nfor_each_cpu(i, online_mask) {\r\nnr_cpus++;\r\nfirst_sibling = get_first_sibling(i);\r\nif (!cpumask_test_cpu(first_sibling, cpus))\r\nnr_uniq_cpus++;\r\ncpumask_set_cpu(i, cpus);\r\n}\r\nqueue = 0;\r\nfor_each_possible_cpu(i) {\r\nif (!cpumask_test_cpu(i, online_mask)) {\r\nmap[i] = 0;\r\ncontinue;\r\n}\r\nif (nr_queues >= nr_cpus || nr_cpus == nr_uniq_cpus) {\r\nmap[i] = cpu_to_queue_index(nr_cpus, nr_queues, queue);\r\nqueue++;\r\ncontinue;\r\n}\r\nfirst_sibling = get_first_sibling(i);\r\nif (first_sibling == i) {\r\nmap[i] = cpu_to_queue_index(nr_uniq_cpus, nr_queues,\r\nqueue);\r\nqueue++;\r\n} else\r\nmap[i] = map[first_sibling];\r\n}\r\nfree_cpumask_var(cpus);\r\nreturn 0;\r\n}\r\nint blk_mq_hw_queue_to_node(unsigned int *mq_map, unsigned int index)\r\n{\r\nint i;\r\nfor_each_possible_cpu(i) {\r\nif (index == mq_map[i])\r\nreturn local_memory_node(cpu_to_node(i));\r\n}\r\nreturn NUMA_NO_NODE;\r\n}
