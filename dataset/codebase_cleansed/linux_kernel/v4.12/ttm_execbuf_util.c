static void ttm_eu_backoff_reservation_reverse(struct list_head *list,\r\nstruct ttm_validate_buffer *entry)\r\n{\r\nlist_for_each_entry_continue_reverse(entry, list, head) {\r\nstruct ttm_buffer_object *bo = entry->bo;\r\n__ttm_bo_unreserve(bo);\r\n}\r\n}\r\nstatic void ttm_eu_del_from_lru_locked(struct list_head *list)\r\n{\r\nstruct ttm_validate_buffer *entry;\r\nlist_for_each_entry(entry, list, head) {\r\nstruct ttm_buffer_object *bo = entry->bo;\r\nttm_bo_del_from_lru(bo);\r\n}\r\n}\r\nvoid ttm_eu_backoff_reservation(struct ww_acquire_ctx *ticket,\r\nstruct list_head *list)\r\n{\r\nstruct ttm_validate_buffer *entry;\r\nstruct ttm_bo_global *glob;\r\nif (list_empty(list))\r\nreturn;\r\nentry = list_first_entry(list, struct ttm_validate_buffer, head);\r\nglob = entry->bo->glob;\r\nspin_lock(&glob->lru_lock);\r\nlist_for_each_entry(entry, list, head) {\r\nstruct ttm_buffer_object *bo = entry->bo;\r\nttm_bo_add_to_lru(bo);\r\n__ttm_bo_unreserve(bo);\r\n}\r\nspin_unlock(&glob->lru_lock);\r\nif (ticket)\r\nww_acquire_fini(ticket);\r\n}\r\nint ttm_eu_reserve_buffers(struct ww_acquire_ctx *ticket,\r\nstruct list_head *list, bool intr,\r\nstruct list_head *dups)\r\n{\r\nstruct ttm_bo_global *glob;\r\nstruct ttm_validate_buffer *entry;\r\nint ret;\r\nif (list_empty(list))\r\nreturn 0;\r\nentry = list_first_entry(list, struct ttm_validate_buffer, head);\r\nglob = entry->bo->glob;\r\nif (ticket)\r\nww_acquire_init(ticket, &reservation_ww_class);\r\nlist_for_each_entry(entry, list, head) {\r\nstruct ttm_buffer_object *bo = entry->bo;\r\nret = __ttm_bo_reserve(bo, intr, (ticket == NULL), ticket);\r\nif (!ret && unlikely(atomic_read(&bo->cpu_writers) > 0)) {\r\n__ttm_bo_unreserve(bo);\r\nret = -EBUSY;\r\n} else if (ret == -EALREADY && dups) {\r\nstruct ttm_validate_buffer *safe = entry;\r\nentry = list_prev_entry(entry, head);\r\nlist_del(&safe->head);\r\nlist_add(&safe->head, dups);\r\ncontinue;\r\n}\r\nif (!ret) {\r\nif (!entry->shared)\r\ncontinue;\r\nret = reservation_object_reserve_shared(bo->resv);\r\nif (!ret)\r\ncontinue;\r\n}\r\nttm_eu_backoff_reservation_reverse(list, entry);\r\nif (ret == -EDEADLK && intr) {\r\nret = ww_mutex_lock_slow_interruptible(&bo->resv->lock,\r\nticket);\r\n} else if (ret == -EDEADLK) {\r\nww_mutex_lock_slow(&bo->resv->lock, ticket);\r\nret = 0;\r\n}\r\nif (!ret && entry->shared)\r\nret = reservation_object_reserve_shared(bo->resv);\r\nif (unlikely(ret != 0)) {\r\nif (ret == -EINTR)\r\nret = -ERESTARTSYS;\r\nif (ticket) {\r\nww_acquire_done(ticket);\r\nww_acquire_fini(ticket);\r\n}\r\nreturn ret;\r\n}\r\nlist_del(&entry->head);\r\nlist_add(&entry->head, list);\r\n}\r\nif (ticket)\r\nww_acquire_done(ticket);\r\nspin_lock(&glob->lru_lock);\r\nttm_eu_del_from_lru_locked(list);\r\nspin_unlock(&glob->lru_lock);\r\nreturn 0;\r\n}\r\nvoid ttm_eu_fence_buffer_objects(struct ww_acquire_ctx *ticket,\r\nstruct list_head *list,\r\nstruct dma_fence *fence)\r\n{\r\nstruct ttm_validate_buffer *entry;\r\nstruct ttm_buffer_object *bo;\r\nstruct ttm_bo_global *glob;\r\nstruct ttm_bo_device *bdev;\r\nstruct ttm_bo_driver *driver;\r\nif (list_empty(list))\r\nreturn;\r\nbo = list_first_entry(list, struct ttm_validate_buffer, head)->bo;\r\nbdev = bo->bdev;\r\ndriver = bdev->driver;\r\nglob = bo->glob;\r\nspin_lock(&glob->lru_lock);\r\nlist_for_each_entry(entry, list, head) {\r\nbo = entry->bo;\r\nif (entry->shared)\r\nreservation_object_add_shared_fence(bo->resv, fence);\r\nelse\r\nreservation_object_add_excl_fence(bo->resv, fence);\r\nttm_bo_add_to_lru(bo);\r\n__ttm_bo_unreserve(bo);\r\n}\r\nspin_unlock(&glob->lru_lock);\r\nif (ticket)\r\nww_acquire_fini(ticket);\r\n}
