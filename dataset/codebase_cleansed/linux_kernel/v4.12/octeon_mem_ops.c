static inline void\r\nocteon_toggle_bar1_swapmode(struct octeon_device *oct, u32 idx)\r\n{\r\nu32 mask;\r\nmask = oct->fn_list.bar1_idx_read(oct, idx);\r\nmask = (mask & 0x2) ? (mask & ~2) : (mask | 2);\r\noct->fn_list.bar1_idx_write(oct, idx, mask);\r\n}\r\nstatic void\r\nocteon_pci_fastwrite(struct octeon_device *oct, u8 __iomem *mapped_addr,\r\nu8 *hostbuf, u32 len)\r\n{\r\nwhile ((len) && ((unsigned long)mapped_addr) & 7) {\r\nwriteb(*(hostbuf++), mapped_addr++);\r\nlen--;\r\n}\r\nocteon_toggle_bar1_swapmode(oct, MEMOPS_IDX);\r\nwhile (len >= 8) {\r\nwriteq(*((u64 *)hostbuf), mapped_addr);\r\nmapped_addr += 8;\r\nhostbuf += 8;\r\nlen -= 8;\r\n}\r\nocteon_toggle_bar1_swapmode(oct, MEMOPS_IDX);\r\nwhile (len--)\r\nwriteb(*(hostbuf++), mapped_addr++);\r\n}\r\nstatic void\r\nocteon_pci_fastread(struct octeon_device *oct, u8 __iomem *mapped_addr,\r\nu8 *hostbuf, u32 len)\r\n{\r\nwhile ((len) && ((unsigned long)mapped_addr) & 7) {\r\n*(hostbuf++) = readb(mapped_addr++);\r\nlen--;\r\n}\r\nocteon_toggle_bar1_swapmode(oct, MEMOPS_IDX);\r\nwhile (len >= 8) {\r\n*((u64 *)hostbuf) = readq(mapped_addr);\r\nmapped_addr += 8;\r\nhostbuf += 8;\r\nlen -= 8;\r\n}\r\nocteon_toggle_bar1_swapmode(oct, MEMOPS_IDX);\r\nwhile (len--)\r\n*(hostbuf++) = readb(mapped_addr++);\r\n}\r\nstatic void\r\n__octeon_pci_rw_core_mem(struct octeon_device *oct, u64 addr,\r\nu8 *hostbuf, u32 len, u32 op)\r\n{\r\nu32 copy_len = 0, index_reg_val = 0;\r\nunsigned long flags;\r\nu8 __iomem *mapped_addr;\r\nu64 static_mapping_base;\r\nstatic_mapping_base = oct->console_nb_info.dram_region_base;\r\nif (static_mapping_base &&\r\nstatic_mapping_base == (addr & ~(OCTEON_BAR1_ENTRY_SIZE - 1ULL))) {\r\nint bar1_index = oct->console_nb_info.bar1_index;\r\nmapped_addr = oct->mmio[1].hw_addr\r\n+ (bar1_index << ilog2(OCTEON_BAR1_ENTRY_SIZE))\r\n+ (addr & (OCTEON_BAR1_ENTRY_SIZE - 1ULL));\r\nif (op)\r\nocteon_pci_fastread(oct, mapped_addr, hostbuf, len);\r\nelse\r\nocteon_pci_fastwrite(oct, mapped_addr, hostbuf, len);\r\nreturn;\r\n}\r\nspin_lock_irqsave(&oct->mem_access_lock, flags);\r\nindex_reg_val = oct->fn_list.bar1_idx_read(oct, MEMOPS_IDX);\r\ndo {\r\noct->fn_list.bar1_idx_setup(oct, addr, MEMOPS_IDX, 1);\r\nmapped_addr = oct->mmio[1].hw_addr\r\n+ (MEMOPS_IDX << 22) + (addr & 0x3fffff);\r\nif (((addr + len - 1) & ~(0x3fffff)) != (addr & ~(0x3fffff))) {\r\ncopy_len = (u32)(((addr & ~(0x3fffff)) +\r\n(MEMOPS_IDX << 22)) - addr);\r\n} else {\r\ncopy_len = len;\r\n}\r\nif (op) {\r\nocteon_pci_fastread(oct, mapped_addr, hostbuf,\r\ncopy_len);\r\n} else {\r\nocteon_pci_fastwrite(oct, mapped_addr, hostbuf,\r\ncopy_len);\r\n}\r\nlen -= copy_len;\r\naddr += copy_len;\r\nhostbuf += copy_len;\r\n} while (len);\r\noct->fn_list.bar1_idx_write(oct, MEMOPS_IDX, index_reg_val);\r\nspin_unlock_irqrestore(&oct->mem_access_lock, flags);\r\n}\r\nvoid\r\nocteon_pci_read_core_mem(struct octeon_device *oct,\r\nu64 coreaddr,\r\nu8 *buf,\r\nu32 len)\r\n{\r\n__octeon_pci_rw_core_mem(oct, coreaddr, buf, len, 1);\r\n}\r\nvoid\r\nocteon_pci_write_core_mem(struct octeon_device *oct,\r\nu64 coreaddr,\r\nu8 *buf,\r\nu32 len)\r\n{\r\n__octeon_pci_rw_core_mem(oct, coreaddr, buf, len, 0);\r\n}\r\nu64 octeon_read_device_mem64(struct octeon_device *oct, u64 coreaddr)\r\n{\r\n__be64 ret;\r\n__octeon_pci_rw_core_mem(oct, coreaddr, (u8 *)&ret, 8, 1);\r\nreturn be64_to_cpu(ret);\r\n}\r\nu32 octeon_read_device_mem32(struct octeon_device *oct, u64 coreaddr)\r\n{\r\n__be32 ret;\r\n__octeon_pci_rw_core_mem(oct, coreaddr, (u8 *)&ret, 4, 1);\r\nreturn be32_to_cpu(ret);\r\n}\r\nvoid octeon_write_device_mem32(struct octeon_device *oct, u64 coreaddr,\r\nu32 val)\r\n{\r\n__be32 t = cpu_to_be32(val);\r\n__octeon_pci_rw_core_mem(oct, coreaddr, (u8 *)&t, 4, 0);\r\n}
