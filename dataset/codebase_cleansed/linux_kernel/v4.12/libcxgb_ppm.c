int cxgbi_ppm_find_page_index(struct cxgbi_ppm *ppm, unsigned long pgsz)\r\n{\r\nstruct cxgbi_tag_format *tformat = &ppm->tformat;\r\nint i;\r\nfor (i = 0; i < DDP_PGIDX_MAX; i++) {\r\nif (pgsz == 1UL << (DDP_PGSZ_BASE_SHIFT +\r\ntformat->pgsz_order[i])) {\r\npr_debug("%s: %s ppm, pgsz %lu -> idx %d.\n",\r\n__func__, ppm->ndev->name, pgsz, i);\r\nreturn i;\r\n}\r\n}\r\npr_info("ippm: ddp page size %lu not supported.\n", pgsz);\r\nreturn DDP_PGIDX_MAX;\r\n}\r\nstatic int ppm_find_unused_entries(unsigned long *bmap,\r\nunsigned int max_ppods,\r\nunsigned int start,\r\nunsigned int nr,\r\nunsigned int align_mask)\r\n{\r\nunsigned long i;\r\ni = bitmap_find_next_zero_area(bmap, max_ppods, start, nr, align_mask);\r\nif (unlikely(i >= max_ppods) && (start > nr))\r\ni = bitmap_find_next_zero_area(bmap, max_ppods, 0, start - 1,\r\nalign_mask);\r\nif (unlikely(i >= max_ppods))\r\nreturn -ENOSPC;\r\nbitmap_set(bmap, i, nr);\r\nreturn (int)i;\r\n}\r\nstatic void ppm_mark_entries(struct cxgbi_ppm *ppm, int i, int count,\r\nunsigned long caller_data)\r\n{\r\nstruct cxgbi_ppod_data *pdata = ppm->ppod_data + i;\r\npdata->caller_data = caller_data;\r\npdata->npods = count;\r\nif (pdata->color == ((1 << PPOD_IDX_SHIFT) - 1))\r\npdata->color = 0;\r\nelse\r\npdata->color++;\r\n}\r\nstatic int ppm_get_cpu_entries(struct cxgbi_ppm *ppm, unsigned int count,\r\nunsigned long caller_data)\r\n{\r\nstruct cxgbi_ppm_pool *pool;\r\nunsigned int cpu;\r\nint i;\r\ncpu = get_cpu();\r\npool = per_cpu_ptr(ppm->pool, cpu);\r\nspin_lock_bh(&pool->lock);\r\nput_cpu();\r\ni = ppm_find_unused_entries(pool->bmap, ppm->pool_index_max,\r\npool->next, count, 0);\r\nif (i < 0) {\r\npool->next = 0;\r\nspin_unlock_bh(&pool->lock);\r\nreturn -ENOSPC;\r\n}\r\npool->next = i + count;\r\nif (pool->next >= ppm->pool_index_max)\r\npool->next = 0;\r\nspin_unlock_bh(&pool->lock);\r\npr_debug("%s: cpu %u, idx %d + %d (%d), next %u.\n",\r\n__func__, cpu, i, count, i + cpu * ppm->pool_index_max,\r\npool->next);\r\ni += cpu * ppm->pool_index_max;\r\nppm_mark_entries(ppm, i, count, caller_data);\r\nreturn i;\r\n}\r\nstatic int ppm_get_entries(struct cxgbi_ppm *ppm, unsigned int count,\r\nunsigned long caller_data)\r\n{\r\nint i;\r\nspin_lock_bh(&ppm->map_lock);\r\ni = ppm_find_unused_entries(ppm->ppod_bmap, ppm->bmap_index_max,\r\nppm->next, count, 0);\r\nif (i < 0) {\r\nppm->next = 0;\r\nspin_unlock_bh(&ppm->map_lock);\r\npr_debug("ippm: NO suitable entries %u available.\n",\r\ncount);\r\nreturn -ENOSPC;\r\n}\r\nppm->next = i + count;\r\nif (ppm->next >= ppm->bmap_index_max)\r\nppm->next = 0;\r\nspin_unlock_bh(&ppm->map_lock);\r\npr_debug("%s: idx %d + %d (%d), next %u, caller_data 0x%lx.\n",\r\n__func__, i, count, i + ppm->pool_rsvd, ppm->next,\r\ncaller_data);\r\ni += ppm->pool_rsvd;\r\nppm_mark_entries(ppm, i, count, caller_data);\r\nreturn i;\r\n}\r\nstatic void ppm_unmark_entries(struct cxgbi_ppm *ppm, int i, int count)\r\n{\r\npr_debug("%s: idx %d + %d.\n", __func__, i, count);\r\nif (i < ppm->pool_rsvd) {\r\nunsigned int cpu;\r\nstruct cxgbi_ppm_pool *pool;\r\ncpu = i / ppm->pool_index_max;\r\ni %= ppm->pool_index_max;\r\npool = per_cpu_ptr(ppm->pool, cpu);\r\nspin_lock_bh(&pool->lock);\r\nbitmap_clear(pool->bmap, i, count);\r\nif (i < pool->next)\r\npool->next = i;\r\nspin_unlock_bh(&pool->lock);\r\npr_debug("%s: cpu %u, idx %d, next %u.\n",\r\n__func__, cpu, i, pool->next);\r\n} else {\r\nspin_lock_bh(&ppm->map_lock);\r\ni -= ppm->pool_rsvd;\r\nbitmap_clear(ppm->ppod_bmap, i, count);\r\nif (i < ppm->next)\r\nppm->next = i;\r\nspin_unlock_bh(&ppm->map_lock);\r\npr_debug("%s: idx %d, next %u.\n", __func__, i, ppm->next);\r\n}\r\n}\r\nvoid cxgbi_ppm_ppod_release(struct cxgbi_ppm *ppm, u32 idx)\r\n{\r\nstruct cxgbi_ppod_data *pdata;\r\nif (idx >= ppm->ppmax) {\r\npr_warn("ippm: idx too big %u > %u.\n", idx, ppm->ppmax);\r\nreturn;\r\n}\r\npdata = ppm->ppod_data + idx;\r\nif (!pdata->npods) {\r\npr_warn("ippm: idx %u, npods 0.\n", idx);\r\nreturn;\r\n}\r\npr_debug("release idx %u, npods %u.\n", idx, pdata->npods);\r\nppm_unmark_entries(ppm, idx, pdata->npods);\r\n}\r\nint cxgbi_ppm_ppods_reserve(struct cxgbi_ppm *ppm, unsigned short nr_pages,\r\nu32 per_tag_pg_idx, u32 *ppod_idx,\r\nu32 *ddp_tag, unsigned long caller_data)\r\n{\r\nstruct cxgbi_ppod_data *pdata;\r\nunsigned int npods;\r\nint idx = -1;\r\nunsigned int hwidx;\r\nu32 tag;\r\nnpods = (nr_pages + PPOD_PAGES_MAX - 1) >> PPOD_PAGES_SHIFT;\r\nif (!npods) {\r\npr_warn("%s: pages %u -> npods %u, full.\n",\r\n__func__, nr_pages, npods);\r\nreturn -EINVAL;\r\n}\r\nidx = ppm_get_cpu_entries(ppm, npods, caller_data);\r\nif (idx < 0)\r\nidx = ppm_get_entries(ppm, npods, caller_data);\r\nif (idx < 0) {\r\npr_debug("ippm: pages %u, nospc %u, nxt %u, 0x%lx.\n",\r\nnr_pages, npods, ppm->next, caller_data);\r\nreturn idx;\r\n}\r\npdata = ppm->ppod_data + idx;\r\nhwidx = ppm->base_idx + idx;\r\ntag = cxgbi_ppm_make_ddp_tag(hwidx, pdata->color);\r\nif (per_tag_pg_idx)\r\ntag |= (per_tag_pg_idx << 30) & 0xC0000000;\r\n*ppod_idx = idx;\r\n*ddp_tag = tag;\r\npr_debug("ippm: sg %u, tag 0x%x(%u,%u), data 0x%lx.\n",\r\nnr_pages, tag, idx, npods, caller_data);\r\nreturn npods;\r\n}\r\nvoid cxgbi_ppm_make_ppod_hdr(struct cxgbi_ppm *ppm, u32 tag,\r\nunsigned int tid, unsigned int offset,\r\nunsigned int length,\r\nstruct cxgbi_pagepod_hdr *hdr)\r\n{\r\ntag &= 0x3FFFFFFF;\r\nhdr->vld_tid = htonl(PPOD_VALID_FLAG | PPOD_TID(tid));\r\nhdr->rsvd = 0;\r\nhdr->pgsz_tag_clr = htonl(tag & ppm->tformat.idx_clr_mask);\r\nhdr->max_offset = htonl(length);\r\nhdr->page_offset = htonl(offset);\r\npr_debug("ippm: tag 0x%x, tid 0x%x, xfer %u, off %u.\n",\r\ntag, tid, length, offset);\r\n}\r\nstatic void ppm_free(struct cxgbi_ppm *ppm)\r\n{\r\nvfree(ppm);\r\n}\r\nstatic void ppm_destroy(struct kref *kref)\r\n{\r\nstruct cxgbi_ppm *ppm = container_of(kref,\r\nstruct cxgbi_ppm,\r\nrefcnt);\r\npr_info("ippm: kref 0, destroy %s ppm 0x%p.\n",\r\nppm->ndev->name, ppm);\r\n*ppm->ppm_pp = NULL;\r\nfree_percpu(ppm->pool);\r\nppm_free(ppm);\r\n}\r\nint cxgbi_ppm_release(struct cxgbi_ppm *ppm)\r\n{\r\nif (ppm) {\r\nint rv;\r\nrv = kref_put(&ppm->refcnt, ppm_destroy);\r\nreturn rv;\r\n}\r\nreturn 1;\r\n}\r\nstatic struct cxgbi_ppm_pool *ppm_alloc_cpu_pool(unsigned int *total,\r\nunsigned int *pcpu_ppmax)\r\n{\r\nstruct cxgbi_ppm_pool *pools;\r\nunsigned int ppmax = (*total) / num_possible_cpus();\r\nunsigned int max = (PCPU_MIN_UNIT_SIZE - sizeof(*pools)) << 3;\r\nunsigned int bmap;\r\nunsigned int alloc_sz;\r\nunsigned int count = 0;\r\nunsigned int cpu;\r\nif (ppmax > max)\r\nppmax = max;\r\nbmap = BITS_TO_LONGS(ppmax);\r\nppmax = (bmap * sizeof(unsigned long)) << 3;\r\nalloc_sz = sizeof(*pools) + sizeof(unsigned long) * bmap;\r\npools = __alloc_percpu(alloc_sz, __alignof__(struct cxgbi_ppm_pool));\r\nif (!pools)\r\nreturn NULL;\r\nfor_each_possible_cpu(cpu) {\r\nstruct cxgbi_ppm_pool *ppool = per_cpu_ptr(pools, cpu);\r\nmemset(ppool, 0, alloc_sz);\r\nspin_lock_init(&ppool->lock);\r\ncount += ppmax;\r\n}\r\n*total = count;\r\n*pcpu_ppmax = ppmax;\r\nreturn pools;\r\n}\r\nint cxgbi_ppm_init(void **ppm_pp, struct net_device *ndev,\r\nstruct pci_dev *pdev, void *lldev,\r\nstruct cxgbi_tag_format *tformat,\r\nunsigned int ppmax,\r\nunsigned int llimit,\r\nunsigned int start,\r\nunsigned int reserve_factor)\r\n{\r\nstruct cxgbi_ppm *ppm = (struct cxgbi_ppm *)(*ppm_pp);\r\nstruct cxgbi_ppm_pool *pool = NULL;\r\nunsigned int ppmax_pool = 0;\r\nunsigned int pool_index_max = 0;\r\nunsigned int alloc_sz;\r\nunsigned int ppod_bmap_size;\r\nif (ppm) {\r\npr_info("ippm: %s, ppm 0x%p,0x%p already initialized, %u/%u.\n",\r\nndev->name, ppm_pp, ppm, ppm->ppmax, ppmax);\r\nkref_get(&ppm->refcnt);\r\nreturn 1;\r\n}\r\nif (reserve_factor) {\r\nppmax_pool = ppmax / reserve_factor;\r\npool = ppm_alloc_cpu_pool(&ppmax_pool, &pool_index_max);\r\npr_debug("%s: ppmax %u, cpu total %u, per cpu %u.\n",\r\nndev->name, ppmax, ppmax_pool, pool_index_max);\r\n}\r\nppod_bmap_size = BITS_TO_LONGS(ppmax - ppmax_pool);\r\nalloc_sz = sizeof(struct cxgbi_ppm) +\r\nppmax * (sizeof(struct cxgbi_ppod_data)) +\r\nppod_bmap_size * sizeof(unsigned long);\r\nppm = vmalloc(alloc_sz);\r\nif (!ppm)\r\ngoto release_ppm_pool;\r\nmemset(ppm, 0, alloc_sz);\r\nppm->ppod_bmap = (unsigned long *)(&ppm->ppod_data[ppmax]);\r\nif ((ppod_bmap_size >> 3) > (ppmax - ppmax_pool)) {\r\nunsigned int start = ppmax - ppmax_pool;\r\nunsigned int end = ppod_bmap_size >> 3;\r\nbitmap_set(ppm->ppod_bmap, ppmax, end - start);\r\npr_info("%s: %u - %u < %u * 8, mask extra bits %u, %u.\n",\r\n__func__, ppmax, ppmax_pool, ppod_bmap_size, start,\r\nend);\r\n}\r\nspin_lock_init(&ppm->map_lock);\r\nkref_init(&ppm->refcnt);\r\nmemcpy(&ppm->tformat, tformat, sizeof(struct cxgbi_tag_format));\r\nppm->ppm_pp = ppm_pp;\r\nppm->ndev = ndev;\r\nppm->pdev = pdev;\r\nppm->lldev = lldev;\r\nppm->ppmax = ppmax;\r\nppm->next = 0;\r\nppm->llimit = llimit;\r\nppm->base_idx = start > llimit ?\r\n(start - llimit + 1) >> PPOD_SIZE_SHIFT : 0;\r\nppm->bmap_index_max = ppmax - ppmax_pool;\r\nppm->pool = pool;\r\nppm->pool_rsvd = ppmax_pool;\r\nppm->pool_index_max = pool_index_max;\r\nif (*ppm_pp) {\r\nppm_free(ppm);\r\nppm = (struct cxgbi_ppm *)(*ppm_pp);\r\npr_info("ippm: %s, ppm 0x%p,0x%p already initialized, %u/%u.\n",\r\nndev->name, ppm_pp, *ppm_pp, ppm->ppmax, ppmax);\r\nkref_get(&ppm->refcnt);\r\nreturn 1;\r\n}\r\n*ppm_pp = ppm;\r\nppm->tformat.pgsz_idx_dflt = cxgbi_ppm_find_page_index(ppm, PAGE_SIZE);\r\npr_info("ippm %s: ppm 0x%p, 0x%p, base %u/%u, pg %lu,%u, rsvd %u,%u.\n",\r\nndev->name, ppm_pp, ppm, ppm->base_idx, ppm->ppmax, PAGE_SIZE,\r\nppm->tformat.pgsz_idx_dflt, ppm->pool_rsvd,\r\nppm->pool_index_max);\r\nreturn 0;\r\nrelease_ppm_pool:\r\nfree_percpu(pool);\r\nreturn -ENOMEM;\r\n}\r\nunsigned int cxgbi_tagmask_set(unsigned int ppmax)\r\n{\r\nunsigned int bits = fls(ppmax);\r\nif (bits > PPOD_IDX_MAX_SIZE)\r\nbits = PPOD_IDX_MAX_SIZE;\r\npr_info("ippm: ppmax %u/0x%x -> bits %u, tagmask 0x%x.\n",\r\nppmax, ppmax, bits, 1 << (bits + PPOD_IDX_SHIFT));\r\nreturn 1 << (bits + PPOD_IDX_SHIFT);\r\n}
