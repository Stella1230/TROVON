static int rcu_perf_read_lock(void) __acquires(RCU)\r\n{\r\nrcu_read_lock();\r\nreturn 0;\r\n}\r\nstatic void rcu_perf_read_unlock(int idx) __releases(RCU)\r\n{\r\nrcu_read_unlock();\r\n}\r\nstatic unsigned long __maybe_unused rcu_no_completed(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic void rcu_sync_perf_init(void)\r\n{\r\n}\r\nstatic int rcu_bh_perf_read_lock(void) __acquires(RCU_BH)\r\n{\r\nrcu_read_lock_bh();\r\nreturn 0;\r\n}\r\nstatic void rcu_bh_perf_read_unlock(int idx) __releases(RCU_BH)\r\n{\r\nrcu_read_unlock_bh();\r\n}\r\nstatic int srcu_perf_read_lock(void) __acquires(srcu_ctlp)\r\n{\r\nreturn srcu_read_lock(srcu_ctlp);\r\n}\r\nstatic void srcu_perf_read_unlock(int idx) __releases(srcu_ctlp)\r\n{\r\nsrcu_read_unlock(srcu_ctlp, idx);\r\n}\r\nstatic unsigned long srcu_perf_completed(void)\r\n{\r\nreturn srcu_batches_completed(srcu_ctlp);\r\n}\r\nstatic void srcu_perf_synchronize(void)\r\n{\r\nsynchronize_srcu(srcu_ctlp);\r\n}\r\nstatic void srcu_perf_synchronize_expedited(void)\r\n{\r\nsynchronize_srcu_expedited(srcu_ctlp);\r\n}\r\nstatic int sched_perf_read_lock(void)\r\n{\r\npreempt_disable();\r\nreturn 0;\r\n}\r\nstatic void sched_perf_read_unlock(int idx)\r\n{\r\npreempt_enable();\r\n}\r\nstatic int tasks_perf_read_lock(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic void tasks_perf_read_unlock(int idx)\r\n{\r\n}\r\nstatic bool __maybe_unused torturing_tasks(void)\r\n{\r\nreturn cur_ops == &tasks_ops;\r\n}\r\nstatic bool __maybe_unused torturing_tasks(void)\r\n{\r\nreturn false;\r\n}\r\nstatic void rcu_perf_wait_shutdown(void)\r\n{\r\ncond_resched_rcu_qs();\r\nif (atomic_read(&n_rcu_perf_writer_finished) < nrealwriters)\r\nreturn;\r\nwhile (!torture_must_stop())\r\nschedule_timeout_uninterruptible(1);\r\n}\r\nstatic int\r\nrcu_perf_reader(void *arg)\r\n{\r\nunsigned long flags;\r\nint idx;\r\nlong me = (long)arg;\r\nVERBOSE_PERFOUT_STRING("rcu_perf_reader task started");\r\nset_cpus_allowed_ptr(current, cpumask_of(me % nr_cpu_ids));\r\nset_user_nice(current, MAX_NICE);\r\natomic_inc(&n_rcu_perf_reader_started);\r\ndo {\r\nlocal_irq_save(flags);\r\nidx = cur_ops->readlock();\r\ncur_ops->readunlock(idx);\r\nlocal_irq_restore(flags);\r\nrcu_perf_wait_shutdown();\r\n} while (!torture_must_stop());\r\ntorture_kthread_stopping("rcu_perf_reader");\r\nreturn 0;\r\n}\r\nstatic int\r\nrcu_perf_writer(void *arg)\r\n{\r\nint i = 0;\r\nint i_max;\r\nlong me = (long)arg;\r\nstruct sched_param sp;\r\nbool started = false, done = false, alldone = false;\r\nu64 t;\r\nu64 *wdp;\r\nu64 *wdpp = writer_durations[me];\r\nVERBOSE_PERFOUT_STRING("rcu_perf_writer task started");\r\nWARN_ON(!wdpp);\r\nset_cpus_allowed_ptr(current, cpumask_of(me % nr_cpu_ids));\r\nsp.sched_priority = 1;\r\nsched_setscheduler_nocheck(current, SCHED_FIFO, &sp);\r\nif (holdoff)\r\nschedule_timeout_uninterruptible(holdoff * HZ);\r\nt = ktime_get_mono_fast_ns();\r\nif (atomic_inc_return(&n_rcu_perf_writer_started) >= nrealwriters) {\r\nt_rcu_perf_writer_started = t;\r\nif (gp_exp) {\r\nb_rcu_perf_writer_started =\r\ncur_ops->exp_completed() / 2;\r\n} else {\r\nb_rcu_perf_writer_started =\r\ncur_ops->completed();\r\n}\r\n}\r\ndo {\r\nwdp = &wdpp[i];\r\n*wdp = ktime_get_mono_fast_ns();\r\nif (gp_exp) {\r\nrcu_perf_writer_state = RTWS_EXP_SYNC;\r\ncur_ops->exp_sync();\r\n} else {\r\nrcu_perf_writer_state = RTWS_SYNC;\r\ncur_ops->sync();\r\n}\r\nrcu_perf_writer_state = RTWS_IDLE;\r\nt = ktime_get_mono_fast_ns();\r\n*wdp = t - *wdp;\r\ni_max = i;\r\nif (!started &&\r\natomic_read(&n_rcu_perf_writer_started) >= nrealwriters)\r\nstarted = true;\r\nif (!done && i >= MIN_MEAS) {\r\ndone = true;\r\nsp.sched_priority = 0;\r\nsched_setscheduler_nocheck(current,\r\nSCHED_NORMAL, &sp);\r\npr_alert("%s%s rcu_perf_writer %ld has %d measurements\n",\r\nperf_type, PERF_FLAG, me, MIN_MEAS);\r\nif (atomic_inc_return(&n_rcu_perf_writer_finished) >=\r\nnrealwriters) {\r\nschedule_timeout_interruptible(10);\r\nrcu_ftrace_dump(DUMP_ALL);\r\nPERFOUT_STRING("Test complete");\r\nt_rcu_perf_writer_finished = t;\r\nif (gp_exp) {\r\nb_rcu_perf_writer_finished =\r\ncur_ops->exp_completed() / 2;\r\n} else {\r\nb_rcu_perf_writer_finished =\r\ncur_ops->completed();\r\n}\r\nif (shutdown) {\r\nsmp_mb();\r\nwake_up(&shutdown_wq);\r\n}\r\n}\r\n}\r\nif (done && !alldone &&\r\natomic_read(&n_rcu_perf_writer_finished) >= nrealwriters)\r\nalldone = true;\r\nif (started && !alldone && i < MAX_MEAS - 1)\r\ni++;\r\nrcu_perf_wait_shutdown();\r\n} while (!torture_must_stop());\r\nrcu_perf_writer_state = RTWS_STOPPING;\r\nwriter_n_durations[me] = i_max;\r\ntorture_kthread_stopping("rcu_perf_writer");\r\nreturn 0;\r\n}\r\nstatic inline void\r\nrcu_perf_print_module_parms(struct rcu_perf_ops *cur_ops, const char *tag)\r\n{\r\npr_alert("%s" PERF_FLAG\r\n"--- %s: nreaders=%d nwriters=%d verbose=%d shutdown=%d\n",\r\nperf_type, tag, nrealreaders, nrealwriters, verbose, shutdown);\r\n}\r\nstatic void\r\nrcu_perf_cleanup(void)\r\n{\r\nint i;\r\nint j;\r\nint ngps = 0;\r\nu64 *wdp;\r\nu64 *wdpp;\r\nif (torture_cleanup_begin())\r\nreturn;\r\nif (reader_tasks) {\r\nfor (i = 0; i < nrealreaders; i++)\r\ntorture_stop_kthread(rcu_perf_reader,\r\nreader_tasks[i]);\r\nkfree(reader_tasks);\r\n}\r\nif (writer_tasks) {\r\nfor (i = 0; i < nrealwriters; i++) {\r\ntorture_stop_kthread(rcu_perf_writer,\r\nwriter_tasks[i]);\r\nif (!writer_n_durations)\r\ncontinue;\r\nj = writer_n_durations[i];\r\npr_alert("%s%s writer %d gps: %d\n",\r\nperf_type, PERF_FLAG, i, j);\r\nngps += j;\r\n}\r\npr_alert("%s%s start: %llu end: %llu duration: %llu gps: %d batches: %ld\n",\r\nperf_type, PERF_FLAG,\r\nt_rcu_perf_writer_started, t_rcu_perf_writer_finished,\r\nt_rcu_perf_writer_finished -\r\nt_rcu_perf_writer_started,\r\nngps,\r\nb_rcu_perf_writer_finished -\r\nb_rcu_perf_writer_started);\r\nfor (i = 0; i < nrealwriters; i++) {\r\nif (!writer_durations)\r\nbreak;\r\nif (!writer_n_durations)\r\ncontinue;\r\nwdpp = writer_durations[i];\r\nif (!wdpp)\r\ncontinue;\r\nfor (j = 0; j <= writer_n_durations[i]; j++) {\r\nwdp = &wdpp[j];\r\npr_alert("%s%s %4d writer-duration: %5d %llu\n",\r\nperf_type, PERF_FLAG,\r\ni, j, *wdp);\r\nif (j % 100 == 0)\r\nschedule_timeout_uninterruptible(1);\r\n}\r\nkfree(writer_durations[i]);\r\n}\r\nkfree(writer_tasks);\r\nkfree(writer_durations);\r\nkfree(writer_n_durations);\r\n}\r\nif (cur_ops->cleanup != NULL)\r\ncur_ops->cleanup();\r\ntorture_cleanup_end();\r\n}\r\nstatic int compute_real(int n)\r\n{\r\nint nr;\r\nif (n >= 0) {\r\nnr = n;\r\n} else {\r\nnr = num_online_cpus() + 1 + n;\r\nif (nr <= 0)\r\nnr = 1;\r\n}\r\nreturn nr;\r\n}\r\nstatic int\r\nrcu_perf_shutdown(void *arg)\r\n{\r\ndo {\r\nwait_event(shutdown_wq,\r\natomic_read(&n_rcu_perf_writer_finished) >=\r\nnrealwriters);\r\n} while (atomic_read(&n_rcu_perf_writer_finished) < nrealwriters);\r\nsmp_mb();\r\nrcu_perf_cleanup();\r\nkernel_power_off();\r\nreturn -EINVAL;\r\n}\r\nstatic int __init\r\nrcu_perf_init(void)\r\n{\r\nlong i;\r\nint firsterr = 0;\r\nstatic struct rcu_perf_ops *perf_ops[] = {\r\n&rcu_ops, &rcu_bh_ops, &srcu_ops, &sched_ops,\r\nRCUPERF_TASKS_OPS\r\n};\r\nif (!torture_init_begin(perf_type, verbose, &perf_runnable))\r\nreturn -EBUSY;\r\nfor (i = 0; i < ARRAY_SIZE(perf_ops); i++) {\r\ncur_ops = perf_ops[i];\r\nif (strcmp(perf_type, cur_ops->name) == 0)\r\nbreak;\r\n}\r\nif (i == ARRAY_SIZE(perf_ops)) {\r\npr_alert("rcu-perf: invalid perf type: \"%s\"\n",\r\nperf_type);\r\npr_alert("rcu-perf types:");\r\nfor (i = 0; i < ARRAY_SIZE(perf_ops); i++)\r\npr_alert(" %s", perf_ops[i]->name);\r\npr_alert("\n");\r\nfirsterr = -EINVAL;\r\ngoto unwind;\r\n}\r\nif (cur_ops->init)\r\ncur_ops->init();\r\nnrealwriters = compute_real(nwriters);\r\nnrealreaders = compute_real(nreaders);\r\natomic_set(&n_rcu_perf_reader_started, 0);\r\natomic_set(&n_rcu_perf_writer_started, 0);\r\natomic_set(&n_rcu_perf_writer_finished, 0);\r\nrcu_perf_print_module_parms(cur_ops, "Start of test");\r\nif (shutdown) {\r\ninit_waitqueue_head(&shutdown_wq);\r\nfirsterr = torture_create_kthread(rcu_perf_shutdown, NULL,\r\nshutdown_task);\r\nif (firsterr)\r\ngoto unwind;\r\nschedule_timeout_uninterruptible(1);\r\n}\r\nreader_tasks = kcalloc(nrealreaders, sizeof(reader_tasks[0]),\r\nGFP_KERNEL);\r\nif (reader_tasks == NULL) {\r\nVERBOSE_PERFOUT_ERRSTRING("out of memory");\r\nfirsterr = -ENOMEM;\r\ngoto unwind;\r\n}\r\nfor (i = 0; i < nrealreaders; i++) {\r\nfirsterr = torture_create_kthread(rcu_perf_reader, (void *)i,\r\nreader_tasks[i]);\r\nif (firsterr)\r\ngoto unwind;\r\n}\r\nwhile (atomic_read(&n_rcu_perf_reader_started) < nrealreaders)\r\nschedule_timeout_uninterruptible(1);\r\nwriter_tasks = kcalloc(nrealwriters, sizeof(reader_tasks[0]),\r\nGFP_KERNEL);\r\nwriter_durations = kcalloc(nrealwriters, sizeof(*writer_durations),\r\nGFP_KERNEL);\r\nwriter_n_durations =\r\nkcalloc(nrealwriters, sizeof(*writer_n_durations),\r\nGFP_KERNEL);\r\nif (!writer_tasks || !writer_durations || !writer_n_durations) {\r\nVERBOSE_PERFOUT_ERRSTRING("out of memory");\r\nfirsterr = -ENOMEM;\r\ngoto unwind;\r\n}\r\nif (rcu_gp_is_expedited() && !rcu_gp_is_normal() && !gp_exp) {\r\nVERBOSE_PERFOUT_ERRSTRING("All grace periods expedited, no normal ones to measure!");\r\nfirsterr = -EINVAL;\r\ngoto unwind;\r\n}\r\nif (rcu_gp_is_normal() && gp_exp) {\r\nVERBOSE_PERFOUT_ERRSTRING("All grace periods normal, no expedited ones to measure!");\r\nfirsterr = -EINVAL;\r\ngoto unwind;\r\n}\r\nfor (i = 0; i < nrealwriters; i++) {\r\nwriter_durations[i] =\r\nkcalloc(MAX_MEAS, sizeof(*writer_durations[i]),\r\nGFP_KERNEL);\r\nif (!writer_durations[i]) {\r\nfirsterr = -ENOMEM;\r\ngoto unwind;\r\n}\r\nfirsterr = torture_create_kthread(rcu_perf_writer, (void *)i,\r\nwriter_tasks[i]);\r\nif (firsterr)\r\ngoto unwind;\r\n}\r\ntorture_init_end();\r\nreturn 0;\r\nunwind:\r\ntorture_init_end();\r\nrcu_perf_cleanup();\r\nreturn firsterr;\r\n}
