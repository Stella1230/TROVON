static inline int is_vxge_card_up(struct vxgedev *vdev)\r\n{\r\nreturn test_bit(__VXGE_STATE_CARD_UP, &vdev->state);\r\n}\r\nstatic inline void VXGE_COMPLETE_VPATH_TX(struct vxge_fifo *fifo)\r\n{\r\nstruct sk_buff **skb_ptr = NULL;\r\nstruct sk_buff **temp;\r\n#define NR_SKB_COMPLETED 128\r\nstruct sk_buff *completed[NR_SKB_COMPLETED];\r\nint more;\r\ndo {\r\nmore = 0;\r\nskb_ptr = completed;\r\nif (__netif_tx_trylock(fifo->txq)) {\r\nvxge_hw_vpath_poll_tx(fifo->handle, &skb_ptr,\r\nNR_SKB_COMPLETED, &more);\r\n__netif_tx_unlock(fifo->txq);\r\n}\r\nfor (temp = completed; temp != skb_ptr; temp++)\r\ndev_kfree_skb_irq(*temp);\r\n} while (more);\r\n}\r\nstatic inline void VXGE_COMPLETE_ALL_TX(struct vxgedev *vdev)\r\n{\r\nint i;\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nVXGE_COMPLETE_VPATH_TX(&vdev->vpaths[i].fifo);\r\n}\r\nstatic inline void VXGE_COMPLETE_ALL_RX(struct vxgedev *vdev)\r\n{\r\nint i;\r\nstruct vxge_ring *ring;\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nring = &vdev->vpaths[i].ring;\r\nvxge_hw_vpath_poll_rx(ring->handle);\r\n}\r\n}\r\nstatic void vxge_callback_link_up(struct __vxge_hw_device *hldev)\r\n{\r\nstruct net_device *dev = hldev->ndev;\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\nvdev->ndev->name, __func__, __LINE__);\r\nnetdev_notice(vdev->ndev, "Link Up\n");\r\nvdev->stats.link_up++;\r\nnetif_carrier_on(vdev->ndev);\r\nnetif_tx_wake_all_queues(vdev->ndev);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...", vdev->ndev->name, __func__, __LINE__);\r\n}\r\nstatic void vxge_callback_link_down(struct __vxge_hw_device *hldev)\r\n{\r\nstruct net_device *dev = hldev->ndev;\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d", vdev->ndev->name, __func__, __LINE__);\r\nnetdev_notice(vdev->ndev, "Link Down\n");\r\nvdev->stats.link_down++;\r\nnetif_carrier_off(vdev->ndev);\r\nnetif_tx_stop_all_queues(vdev->ndev);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...", vdev->ndev->name, __func__, __LINE__);\r\n}\r\nstatic struct sk_buff *\r\nvxge_rx_alloc(void *dtrh, struct vxge_ring *ring, const int skb_size)\r\n{\r\nstruct net_device *dev;\r\nstruct sk_buff *skb;\r\nstruct vxge_rx_priv *rx_priv;\r\ndev = ring->ndev;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\nring->ndev->name, __func__, __LINE__);\r\nrx_priv = vxge_hw_ring_rxd_private_get(dtrh);\r\nskb = netdev_alloc_skb(dev, skb_size +\r\nVXGE_HW_HEADER_ETHERNET_II_802_3_ALIGN);\r\nif (skb == NULL) {\r\nvxge_debug_mem(VXGE_ERR,\r\n"%s: out of memory to allocate SKB", dev->name);\r\nring->stats.skb_alloc_fail++;\r\nreturn NULL;\r\n}\r\nvxge_debug_mem(VXGE_TRACE,\r\n"%s: %s:%d Skb : 0x%p", ring->ndev->name,\r\n__func__, __LINE__, skb);\r\nskb_reserve(skb, VXGE_HW_HEADER_ETHERNET_II_802_3_ALIGN);\r\nrx_priv->skb = skb;\r\nrx_priv->skb_data = NULL;\r\nrx_priv->data_size = skb_size;\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...", ring->ndev->name, __func__, __LINE__);\r\nreturn skb;\r\n}\r\nstatic int vxge_rx_map(void *dtrh, struct vxge_ring *ring)\r\n{\r\nstruct vxge_rx_priv *rx_priv;\r\ndma_addr_t dma_addr;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\nring->ndev->name, __func__, __LINE__);\r\nrx_priv = vxge_hw_ring_rxd_private_get(dtrh);\r\nrx_priv->skb_data = rx_priv->skb->data;\r\ndma_addr = pci_map_single(ring->pdev, rx_priv->skb_data,\r\nrx_priv->data_size, PCI_DMA_FROMDEVICE);\r\nif (unlikely(pci_dma_mapping_error(ring->pdev, dma_addr))) {\r\nring->stats.pci_map_fail++;\r\nreturn -EIO;\r\n}\r\nvxge_debug_mem(VXGE_TRACE,\r\n"%s: %s:%d 1 buffer mode dma_addr = 0x%llx",\r\nring->ndev->name, __func__, __LINE__,\r\n(unsigned long long)dma_addr);\r\nvxge_hw_ring_rxd_1b_set(dtrh, dma_addr, rx_priv->data_size);\r\nrx_priv->data_dma = dma_addr;\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...", ring->ndev->name, __func__, __LINE__);\r\nreturn 0;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_rx_initial_replenish(void *dtrh, void *userdata)\r\n{\r\nstruct vxge_ring *ring = (struct vxge_ring *)userdata;\r\nstruct vxge_rx_priv *rx_priv;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\nring->ndev->name, __func__, __LINE__);\r\nif (vxge_rx_alloc(dtrh, ring,\r\nVXGE_LL_MAX_FRAME_SIZE(ring->ndev)) == NULL)\r\nreturn VXGE_HW_FAIL;\r\nif (vxge_rx_map(dtrh, ring)) {\r\nrx_priv = vxge_hw_ring_rxd_private_get(dtrh);\r\ndev_kfree_skb(rx_priv->skb);\r\nreturn VXGE_HW_FAIL;\r\n}\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...", ring->ndev->name, __func__, __LINE__);\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic inline void\r\nvxge_rx_complete(struct vxge_ring *ring, struct sk_buff *skb, u16 vlan,\r\nint pkt_length, struct vxge_hw_ring_rxd_info *ext_info)\r\n{\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\nring->ndev->name, __func__, __LINE__);\r\nskb_record_rx_queue(skb, ring->driver_id);\r\nskb->protocol = eth_type_trans(skb, ring->ndev);\r\nu64_stats_update_begin(&ring->stats.syncp);\r\nring->stats.rx_frms++;\r\nring->stats.rx_bytes += pkt_length;\r\nif (skb->pkt_type == PACKET_MULTICAST)\r\nring->stats.rx_mcast++;\r\nu64_stats_update_end(&ring->stats.syncp);\r\nvxge_debug_rx(VXGE_TRACE,\r\n"%s: %s:%d skb protocol = %d",\r\nring->ndev->name, __func__, __LINE__, skb->protocol);\r\nif (ext_info->vlan &&\r\nring->vlan_tag_strip == VXGE_HW_VPATH_RPA_STRIP_VLAN_TAG_ENABLE)\r\n__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ext_info->vlan);\r\nnapi_gro_receive(ring->napi_p, skb);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...", ring->ndev->name, __func__, __LINE__);\r\n}\r\nstatic inline void vxge_re_pre_post(void *dtr, struct vxge_ring *ring,\r\nstruct vxge_rx_priv *rx_priv)\r\n{\r\npci_dma_sync_single_for_device(ring->pdev,\r\nrx_priv->data_dma, rx_priv->data_size, PCI_DMA_FROMDEVICE);\r\nvxge_hw_ring_rxd_1b_set(dtr, rx_priv->data_dma, rx_priv->data_size);\r\nvxge_hw_ring_rxd_pre_post(ring->handle, dtr);\r\n}\r\nstatic inline void vxge_post(int *dtr_cnt, void **first_dtr,\r\nvoid *post_dtr, struct __vxge_hw_ring *ringh)\r\n{\r\nint dtr_count = *dtr_cnt;\r\nif ((*dtr_cnt % VXGE_HW_RXSYNC_FREQ_CNT) == 0) {\r\nif (*first_dtr)\r\nvxge_hw_ring_rxd_post_post_wmb(ringh, *first_dtr);\r\n*first_dtr = post_dtr;\r\n} else\r\nvxge_hw_ring_rxd_post_post(ringh, post_dtr);\r\ndtr_count++;\r\n*dtr_cnt = dtr_count;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_rx_1b_compl(struct __vxge_hw_ring *ringh, void *dtr,\r\nu8 t_code, void *userdata)\r\n{\r\nstruct vxge_ring *ring = (struct vxge_ring *)userdata;\r\nstruct net_device *dev = ring->ndev;\r\nunsigned int dma_sizes;\r\nvoid *first_dtr = NULL;\r\nint dtr_cnt = 0;\r\nint data_size;\r\ndma_addr_t data_dma;\r\nint pkt_length;\r\nstruct sk_buff *skb;\r\nstruct vxge_rx_priv *rx_priv;\r\nstruct vxge_hw_ring_rxd_info ext_info;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\nring->ndev->name, __func__, __LINE__);\r\nif (ring->budget <= 0)\r\ngoto out;\r\ndo {\r\nprefetch((char *)dtr + L1_CACHE_BYTES);\r\nrx_priv = vxge_hw_ring_rxd_private_get(dtr);\r\nskb = rx_priv->skb;\r\ndata_size = rx_priv->data_size;\r\ndata_dma = rx_priv->data_dma;\r\nprefetch(rx_priv->skb_data);\r\nvxge_debug_rx(VXGE_TRACE,\r\n"%s: %s:%d skb = 0x%p",\r\nring->ndev->name, __func__, __LINE__, skb);\r\nvxge_hw_ring_rxd_1b_get(ringh, dtr, &dma_sizes);\r\npkt_length = dma_sizes;\r\npkt_length -= ETH_FCS_LEN;\r\nvxge_debug_rx(VXGE_TRACE,\r\n"%s: %s:%d Packet Length = %d",\r\nring->ndev->name, __func__, __LINE__, pkt_length);\r\nvxge_hw_ring_rxd_1b_info_get(ringh, dtr, &ext_info);\r\nvxge_assert(skb);\r\nprefetch((char *)skb + L1_CACHE_BYTES);\r\nif (unlikely(t_code)) {\r\nif (vxge_hw_ring_handle_tcode(ringh, dtr, t_code) !=\r\nVXGE_HW_OK) {\r\nring->stats.rx_errors++;\r\nvxge_debug_rx(VXGE_TRACE,\r\n"%s: %s :%d Rx T_code is %d",\r\nring->ndev->name, __func__,\r\n__LINE__, t_code);\r\nvxge_re_pre_post(dtr, ring, rx_priv);\r\nvxge_post(&dtr_cnt, &first_dtr, dtr, ringh);\r\nring->stats.rx_dropped++;\r\ncontinue;\r\n}\r\n}\r\nif (pkt_length > VXGE_LL_RX_COPY_THRESHOLD) {\r\nif (vxge_rx_alloc(dtr, ring, data_size) != NULL) {\r\nif (!vxge_rx_map(dtr, ring)) {\r\nskb_put(skb, pkt_length);\r\npci_unmap_single(ring->pdev, data_dma,\r\ndata_size, PCI_DMA_FROMDEVICE);\r\nvxge_hw_ring_rxd_pre_post(ringh, dtr);\r\nvxge_post(&dtr_cnt, &first_dtr, dtr,\r\nringh);\r\n} else {\r\ndev_kfree_skb(rx_priv->skb);\r\nrx_priv->skb = skb;\r\nrx_priv->data_size = data_size;\r\nvxge_re_pre_post(dtr, ring, rx_priv);\r\nvxge_post(&dtr_cnt, &first_dtr, dtr,\r\nringh);\r\nring->stats.rx_dropped++;\r\nbreak;\r\n}\r\n} else {\r\nvxge_re_pre_post(dtr, ring, rx_priv);\r\nvxge_post(&dtr_cnt, &first_dtr, dtr, ringh);\r\nring->stats.rx_dropped++;\r\nbreak;\r\n}\r\n} else {\r\nstruct sk_buff *skb_up;\r\nskb_up = netdev_alloc_skb(dev, pkt_length +\r\nVXGE_HW_HEADER_ETHERNET_II_802_3_ALIGN);\r\nif (skb_up != NULL) {\r\nskb_reserve(skb_up,\r\nVXGE_HW_HEADER_ETHERNET_II_802_3_ALIGN);\r\npci_dma_sync_single_for_cpu(ring->pdev,\r\ndata_dma, data_size,\r\nPCI_DMA_FROMDEVICE);\r\nvxge_debug_mem(VXGE_TRACE,\r\n"%s: %s:%d skb_up = %p",\r\nring->ndev->name, __func__,\r\n__LINE__, skb);\r\nmemcpy(skb_up->data, skb->data, pkt_length);\r\nvxge_re_pre_post(dtr, ring, rx_priv);\r\nvxge_post(&dtr_cnt, &first_dtr, dtr,\r\nringh);\r\nskb = skb_up;\r\nskb_put(skb, pkt_length);\r\n} else {\r\nvxge_re_pre_post(dtr, ring, rx_priv);\r\nvxge_post(&dtr_cnt, &first_dtr, dtr, ringh);\r\nvxge_debug_rx(VXGE_ERR,\r\n"%s: vxge_rx_1b_compl: out of "\r\n"memory", dev->name);\r\nring->stats.skb_alloc_fail++;\r\nbreak;\r\n}\r\n}\r\nif ((ext_info.proto & VXGE_HW_FRAME_PROTO_TCP_OR_UDP) &&\r\n!(ext_info.proto & VXGE_HW_FRAME_PROTO_IP_FRAG) &&\r\n(dev->features & NETIF_F_RXCSUM) &&\r\next_info.l3_cksum == VXGE_HW_L3_CKSUM_OK &&\r\next_info.l4_cksum == VXGE_HW_L4_CKSUM_OK)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nelse\r\nskb_checksum_none_assert(skb);\r\nif (ring->rx_hwts) {\r\nstruct skb_shared_hwtstamps *skb_hwts;\r\nu32 ns = *(u32 *)(skb->head + pkt_length);\r\nskb_hwts = skb_hwtstamps(skb);\r\nskb_hwts->hwtstamp = ns_to_ktime(ns);\r\n}\r\nif (ext_info.rth_value)\r\nskb_set_hash(skb, ext_info.rth_value,\r\nPKT_HASH_TYPE_L3);\r\nvxge_rx_complete(ring, skb, ext_info.vlan,\r\npkt_length, &ext_info);\r\nring->budget--;\r\nring->pkts_processed++;\r\nif (!ring->budget)\r\nbreak;\r\n} while (vxge_hw_ring_rxd_next_completed(ringh, &dtr,\r\n&t_code) == VXGE_HW_OK);\r\nif (first_dtr)\r\nvxge_hw_ring_rxd_post_post_wmb(ringh, first_dtr);\r\nout:\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d Exiting...",\r\n__func__, __LINE__);\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_xmit_compl(struct __vxge_hw_fifo *fifo_hw, void *dtr,\r\nenum vxge_hw_fifo_tcode t_code, void *userdata,\r\nstruct sk_buff ***skb_ptr, int nr_skb, int *more)\r\n{\r\nstruct vxge_fifo *fifo = (struct vxge_fifo *)userdata;\r\nstruct sk_buff *skb, **done_skb = *skb_ptr;\r\nint pkt_cnt = 0;\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d Entered....", __func__, __LINE__);\r\ndo {\r\nint frg_cnt;\r\nskb_frag_t *frag;\r\nint i = 0, j;\r\nstruct vxge_tx_priv *txd_priv =\r\nvxge_hw_fifo_txdl_private_get(dtr);\r\nskb = txd_priv->skb;\r\nfrg_cnt = skb_shinfo(skb)->nr_frags;\r\nfrag = &skb_shinfo(skb)->frags[0];\r\nvxge_debug_tx(VXGE_TRACE,\r\n"%s: %s:%d fifo_hw = %p dtr = %p "\r\n"tcode = 0x%x", fifo->ndev->name, __func__,\r\n__LINE__, fifo_hw, dtr, t_code);\r\nvxge_assert(skb);\r\nvxge_debug_tx(VXGE_TRACE,\r\n"%s: %s:%d skb = %p itxd_priv = %p frg_cnt = %d",\r\nfifo->ndev->name, __func__, __LINE__,\r\nskb, txd_priv, frg_cnt);\r\nif (unlikely(t_code)) {\r\nfifo->stats.tx_errors++;\r\nvxge_debug_tx(VXGE_ERR,\r\n"%s: tx: dtr %p completed due to "\r\n"error t_code %01x", fifo->ndev->name,\r\ndtr, t_code);\r\nvxge_hw_fifo_handle_tcode(fifo_hw, dtr, t_code);\r\n}\r\npci_unmap_single(fifo->pdev, txd_priv->dma_buffers[i++],\r\nskb_headlen(skb), PCI_DMA_TODEVICE);\r\nfor (j = 0; j < frg_cnt; j++) {\r\npci_unmap_page(fifo->pdev,\r\ntxd_priv->dma_buffers[i++],\r\nskb_frag_size(frag), PCI_DMA_TODEVICE);\r\nfrag += 1;\r\n}\r\nvxge_hw_fifo_txdl_free(fifo_hw, dtr);\r\nu64_stats_update_begin(&fifo->stats.syncp);\r\nfifo->stats.tx_frms++;\r\nfifo->stats.tx_bytes += skb->len;\r\nu64_stats_update_end(&fifo->stats.syncp);\r\n*done_skb++ = skb;\r\nif (--nr_skb <= 0) {\r\n*more = 1;\r\nbreak;\r\n}\r\npkt_cnt++;\r\nif (pkt_cnt > fifo->indicate_max_pkts)\r\nbreak;\r\n} while (vxge_hw_fifo_txdl_next_completed(fifo_hw,\r\n&dtr, &t_code) == VXGE_HW_OK);\r\n*skb_ptr = done_skb;\r\nif (netif_tx_queue_stopped(fifo->txq))\r\nnetif_tx_wake_queue(fifo->txq);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...",\r\nfifo->ndev->name, __func__, __LINE__);\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic u32 vxge_get_vpath_no(struct vxgedev *vdev, struct sk_buff *skb)\r\n{\r\nu16 queue_len, counter = 0;\r\nif (skb->protocol == htons(ETH_P_IP)) {\r\nstruct iphdr *ip;\r\nstruct tcphdr *th;\r\nip = ip_hdr(skb);\r\nif (!ip_is_fragment(ip)) {\r\nth = (struct tcphdr *)(((unsigned char *)ip) +\r\nip->ihl*4);\r\nqueue_len = vdev->no_of_vpath;\r\ncounter = (ntohs(th->source) +\r\nntohs(th->dest)) &\r\nvdev->vpath_selector[queue_len - 1];\r\nif (counter >= queue_len)\r\ncounter = queue_len - 1;\r\n}\r\n}\r\nreturn counter;\r\n}\r\nstatic enum vxge_hw_status vxge_search_mac_addr_in_list(\r\nstruct vxge_vpath *vpath, u64 del_mac)\r\n{\r\nstruct list_head *entry, *next;\r\nlist_for_each_safe(entry, next, &vpath->mac_addr_list) {\r\nif (((struct vxge_mac_addrs *)entry)->macaddr == del_mac)\r\nreturn TRUE;\r\n}\r\nreturn FALSE;\r\n}\r\nstatic int vxge_mac_list_add(struct vxge_vpath *vpath, struct macInfo *mac)\r\n{\r\nstruct vxge_mac_addrs *new_mac_entry;\r\nu8 *mac_address = NULL;\r\nif (vpath->mac_addr_cnt >= VXGE_MAX_LEARN_MAC_ADDR_CNT)\r\nreturn TRUE;\r\nnew_mac_entry = kzalloc(sizeof(struct vxge_mac_addrs), GFP_ATOMIC);\r\nif (!new_mac_entry) {\r\nvxge_debug_mem(VXGE_ERR,\r\n"%s: memory allocation failed",\r\nVXGE_DRIVER_NAME);\r\nreturn FALSE;\r\n}\r\nlist_add(&new_mac_entry->item, &vpath->mac_addr_list);\r\nmac_address = (u8 *)&new_mac_entry->macaddr;\r\nmemcpy(mac_address, mac->macaddr, ETH_ALEN);\r\nnew_mac_entry->state = mac->state;\r\nvpath->mac_addr_cnt++;\r\nif (is_multicast_ether_addr(mac->macaddr))\r\nvpath->mcast_addr_cnt++;\r\nreturn TRUE;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_add_mac_addr(struct vxgedev *vdev, struct macInfo *mac)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_vpath *vpath;\r\nenum vxge_hw_vpath_mac_addr_add_mode duplicate_mode;\r\nif (is_multicast_ether_addr(mac->macaddr))\r\nduplicate_mode = VXGE_HW_VPATH_MAC_ADDR_ADD_DUPLICATE;\r\nelse\r\nduplicate_mode = VXGE_HW_VPATH_MAC_ADDR_REPLACE_DUPLICATE;\r\nvpath = &vdev->vpaths[mac->vpath_no];\r\nstatus = vxge_hw_vpath_mac_addr_add(vpath->handle, mac->macaddr,\r\nmac->macmask, duplicate_mode);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"DA config add entry failed for vpath:%d",\r\nvpath->device_id);\r\n} else\r\nif (FALSE == vxge_mac_list_add(vpath, mac))\r\nstatus = -EPERM;\r\nreturn status;\r\n}\r\nstatic int vxge_learn_mac(struct vxgedev *vdev, u8 *mac_header)\r\n{\r\nstruct macInfo mac_info;\r\nu8 *mac_address = NULL;\r\nu64 mac_addr = 0, vpath_vector = 0;\r\nint vpath_idx = 0;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_vpath *vpath = NULL;\r\nmac_address = (u8 *)&mac_addr;\r\nmemcpy(mac_address, mac_header, ETH_ALEN);\r\nfor (vpath_idx = 0; vpath_idx < vdev->no_of_vpath; vpath_idx++) {\r\nvpath = &vdev->vpaths[vpath_idx];\r\nif (vxge_search_mac_addr_in_list(vpath, mac_addr))\r\nreturn vpath_idx;\r\n}\r\nmemset(&mac_info, 0, sizeof(struct macInfo));\r\nmemcpy(mac_info.macaddr, mac_header, ETH_ALEN);\r\nfor (vpath_idx = 0; vpath_idx < vdev->no_of_vpath; vpath_idx++) {\r\nvpath = &vdev->vpaths[vpath_idx];\r\nif (vpath->mac_addr_cnt < vpath->max_mac_addr_cnt) {\r\nmac_info.vpath_no = vpath_idx;\r\nmac_info.state = VXGE_LL_MAC_ADDR_IN_DA_TABLE;\r\nstatus = vxge_add_mac_addr(vdev, &mac_info);\r\nif (status != VXGE_HW_OK)\r\nreturn -EPERM;\r\nreturn vpath_idx;\r\n}\r\n}\r\nmac_info.state = VXGE_LL_MAC_ADDR_IN_LIST;\r\nvpath_idx = 0;\r\nmac_info.vpath_no = vpath_idx;\r\nvpath = &vdev->vpaths[vpath_idx];\r\nif (vpath->mac_addr_cnt > vpath->max_mac_addr_cnt) {\r\nif (FALSE == vxge_mac_list_add(vpath, &mac_info))\r\nreturn -EPERM;\r\nreturn vpath_idx;\r\n}\r\nvpath_vector = vxge_mBIT(vpath->device_id);\r\nstatus = vxge_hw_mgmt_reg_write(vpath->vdev->devh,\r\nvxge_hw_mgmt_reg_type_mrpcim,\r\n0,\r\n(ulong)offsetof(\r\nstruct vxge_hw_mrpcim_reg,\r\nrts_mgr_cbasin_cfg),\r\nvpath_vector);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_tx(VXGE_ERR,\r\n"%s: Unable to set the vpath-%d in catch-basin mode",\r\nVXGE_DRIVER_NAME, vpath->device_id);\r\nreturn -EPERM;\r\n}\r\nif (FALSE == vxge_mac_list_add(vpath, &mac_info))\r\nreturn -EPERM;\r\nreturn vpath_idx;\r\n}\r\nstatic netdev_tx_t\r\nvxge_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct vxge_fifo *fifo = NULL;\r\nvoid *dtr_priv;\r\nvoid *dtr = NULL;\r\nstruct vxgedev *vdev = NULL;\r\nenum vxge_hw_status status;\r\nint frg_cnt, first_frg_len;\r\nskb_frag_t *frag;\r\nint i = 0, j = 0, avail;\r\nu64 dma_pointer;\r\nstruct vxge_tx_priv *txdl_priv = NULL;\r\nstruct __vxge_hw_fifo *fifo_hw;\r\nint offload_type;\r\nint vpath_no = 0;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\ndev->name, __func__, __LINE__);\r\nif (unlikely(skb->len <= 0)) {\r\nvxge_debug_tx(VXGE_ERR,\r\n"%s: Buffer has no data..", dev->name);\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nvdev = netdev_priv(dev);\r\nif (unlikely(!is_vxge_card_up(vdev))) {\r\nvxge_debug_tx(VXGE_ERR,\r\n"%s: vdev not initialized", dev->name);\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nif (vdev->config.addr_learn_en) {\r\nvpath_no = vxge_learn_mac(vdev, skb->data + ETH_ALEN);\r\nif (vpath_no == -EPERM) {\r\nvxge_debug_tx(VXGE_ERR,\r\n"%s: Failed to store the mac address",\r\ndev->name);\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\n}\r\nif (vdev->config.tx_steering_type == TX_MULTIQ_STEERING)\r\nvpath_no = skb_get_queue_mapping(skb);\r\nelse if (vdev->config.tx_steering_type == TX_PORT_STEERING)\r\nvpath_no = vxge_get_vpath_no(vdev, skb);\r\nvxge_debug_tx(VXGE_TRACE, "%s: vpath_no= %d", dev->name, vpath_no);\r\nif (vpath_no >= vdev->no_of_vpath)\r\nvpath_no = 0;\r\nfifo = &vdev->vpaths[vpath_no].fifo;\r\nfifo_hw = fifo->handle;\r\nif (netif_tx_queue_stopped(fifo->txq))\r\nreturn NETDEV_TX_BUSY;\r\navail = vxge_hw_fifo_free_txdl_count_get(fifo_hw);\r\nif (avail == 0) {\r\nvxge_debug_tx(VXGE_ERR,\r\n"%s: No free TXDs available", dev->name);\r\nfifo->stats.txd_not_free++;\r\ngoto _exit0;\r\n}\r\nif (avail == 1)\r\nnetif_tx_stop_queue(fifo->txq);\r\nstatus = vxge_hw_fifo_txdl_reserve(fifo_hw, &dtr, &dtr_priv);\r\nif (unlikely(status != VXGE_HW_OK)) {\r\nvxge_debug_tx(VXGE_ERR,\r\n"%s: Out of descriptors .", dev->name);\r\nfifo->stats.txd_out_of_desc++;\r\ngoto _exit0;\r\n}\r\nvxge_debug_tx(VXGE_TRACE,\r\n"%s: %s:%d fifo_hw = %p dtr = %p dtr_priv = %p",\r\ndev->name, __func__, __LINE__,\r\nfifo_hw, dtr, dtr_priv);\r\nif (skb_vlan_tag_present(skb)) {\r\nu16 vlan_tag = skb_vlan_tag_get(skb);\r\nvxge_hw_fifo_txdl_vlan_set(dtr, vlan_tag);\r\n}\r\nfirst_frg_len = skb_headlen(skb);\r\ndma_pointer = pci_map_single(fifo->pdev, skb->data, first_frg_len,\r\nPCI_DMA_TODEVICE);\r\nif (unlikely(pci_dma_mapping_error(fifo->pdev, dma_pointer))) {\r\nvxge_hw_fifo_txdl_free(fifo_hw, dtr);\r\nfifo->stats.pci_map_fail++;\r\ngoto _exit0;\r\n}\r\ntxdl_priv = vxge_hw_fifo_txdl_private_get(dtr);\r\ntxdl_priv->skb = skb;\r\ntxdl_priv->dma_buffers[j] = dma_pointer;\r\nfrg_cnt = skb_shinfo(skb)->nr_frags;\r\nvxge_debug_tx(VXGE_TRACE,\r\n"%s: %s:%d skb = %p txdl_priv = %p "\r\n"frag_cnt = %d dma_pointer = 0x%llx", dev->name,\r\n__func__, __LINE__, skb, txdl_priv,\r\nfrg_cnt, (unsigned long long)dma_pointer);\r\nvxge_hw_fifo_txdl_buffer_set(fifo_hw, dtr, j++, dma_pointer,\r\nfirst_frg_len);\r\nfrag = &skb_shinfo(skb)->frags[0];\r\nfor (i = 0; i < frg_cnt; i++) {\r\nif (!skb_frag_size(frag))\r\ncontinue;\r\ndma_pointer = (u64)skb_frag_dma_map(&fifo->pdev->dev, frag,\r\n0, skb_frag_size(frag),\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(&fifo->pdev->dev, dma_pointer)))\r\ngoto _exit2;\r\nvxge_debug_tx(VXGE_TRACE,\r\n"%s: %s:%d frag = %d dma_pointer = 0x%llx",\r\ndev->name, __func__, __LINE__, i,\r\n(unsigned long long)dma_pointer);\r\ntxdl_priv->dma_buffers[j] = dma_pointer;\r\nvxge_hw_fifo_txdl_buffer_set(fifo_hw, dtr, j++, dma_pointer,\r\nskb_frag_size(frag));\r\nfrag += 1;\r\n}\r\noffload_type = vxge_offload_type(skb);\r\nif (offload_type & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)) {\r\nint mss = vxge_tcp_mss(skb);\r\nif (mss) {\r\nvxge_debug_tx(VXGE_TRACE, "%s: %s:%d mss = %d",\r\ndev->name, __func__, __LINE__, mss);\r\nvxge_hw_fifo_txdl_mss_set(dtr, mss);\r\n} else {\r\nvxge_assert(skb->len <=\r\ndev->mtu + VXGE_HW_MAC_HEADER_MAX_SIZE);\r\nvxge_assert(0);\r\ngoto _exit1;\r\n}\r\n}\r\nif (skb->ip_summed == CHECKSUM_PARTIAL)\r\nvxge_hw_fifo_txdl_cksum_set_bits(dtr,\r\nVXGE_HW_FIFO_TXD_TX_CKO_IPV4_EN |\r\nVXGE_HW_FIFO_TXD_TX_CKO_TCP_EN |\r\nVXGE_HW_FIFO_TXD_TX_CKO_UDP_EN);\r\nvxge_hw_fifo_txdl_post(fifo_hw, dtr);\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d Exiting...",\r\ndev->name, __func__, __LINE__);\r\nreturn NETDEV_TX_OK;\r\n_exit2:\r\nvxge_debug_tx(VXGE_TRACE, "%s: pci_map_page failed", dev->name);\r\n_exit1:\r\nj = 0;\r\nfrag = &skb_shinfo(skb)->frags[0];\r\npci_unmap_single(fifo->pdev, txdl_priv->dma_buffers[j++],\r\nskb_headlen(skb), PCI_DMA_TODEVICE);\r\nfor (; j < i; j++) {\r\npci_unmap_page(fifo->pdev, txdl_priv->dma_buffers[j],\r\nskb_frag_size(frag), PCI_DMA_TODEVICE);\r\nfrag += 1;\r\n}\r\nvxge_hw_fifo_txdl_free(fifo_hw, dtr);\r\n_exit0:\r\nnetif_tx_stop_queue(fifo->txq);\r\ndev_kfree_skb_any(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void\r\nvxge_rx_term(void *dtrh, enum vxge_hw_rxd_state state, void *userdata)\r\n{\r\nstruct vxge_ring *ring = (struct vxge_ring *)userdata;\r\nstruct vxge_rx_priv *rx_priv =\r\nvxge_hw_ring_rxd_private_get(dtrh);\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\nring->ndev->name, __func__, __LINE__);\r\nif (state != VXGE_HW_RXD_STATE_POSTED)\r\nreturn;\r\npci_unmap_single(ring->pdev, rx_priv->data_dma,\r\nrx_priv->data_size, PCI_DMA_FROMDEVICE);\r\ndev_kfree_skb(rx_priv->skb);\r\nrx_priv->skb_data = NULL;\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...",\r\nring->ndev->name, __func__, __LINE__);\r\n}\r\nstatic void\r\nvxge_tx_term(void *dtrh, enum vxge_hw_txdl_state state, void *userdata)\r\n{\r\nstruct vxge_fifo *fifo = (struct vxge_fifo *)userdata;\r\nskb_frag_t *frag;\r\nint i = 0, j, frg_cnt;\r\nstruct vxge_tx_priv *txd_priv = vxge_hw_fifo_txdl_private_get(dtrh);\r\nstruct sk_buff *skb = txd_priv->skb;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s:%d", __func__, __LINE__);\r\nif (state != VXGE_HW_TXDL_STATE_POSTED)\r\nreturn;\r\nvxge_assert(skb);\r\nfrg_cnt = skb_shinfo(skb)->nr_frags;\r\nfrag = &skb_shinfo(skb)->frags[0];\r\npci_unmap_single(fifo->pdev, txd_priv->dma_buffers[i++],\r\nskb_headlen(skb), PCI_DMA_TODEVICE);\r\nfor (j = 0; j < frg_cnt; j++) {\r\npci_unmap_page(fifo->pdev, txd_priv->dma_buffers[i++],\r\nskb_frag_size(frag), PCI_DMA_TODEVICE);\r\nfrag += 1;\r\n}\r\ndev_kfree_skb(skb);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d Exiting...", __func__, __LINE__);\r\n}\r\nstatic int vxge_mac_list_del(struct vxge_vpath *vpath, struct macInfo *mac)\r\n{\r\nstruct list_head *entry, *next;\r\nu64 del_mac = 0;\r\nu8 *mac_address = (u8 *) (&del_mac);\r\nmemcpy(mac_address, mac->macaddr, ETH_ALEN);\r\nlist_for_each_safe(entry, next, &vpath->mac_addr_list) {\r\nif (((struct vxge_mac_addrs *)entry)->macaddr == del_mac) {\r\nlist_del(entry);\r\nkfree((struct vxge_mac_addrs *)entry);\r\nvpath->mac_addr_cnt--;\r\nif (is_multicast_ether_addr(mac->macaddr))\r\nvpath->mcast_addr_cnt--;\r\nreturn TRUE;\r\n}\r\n}\r\nreturn FALSE;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_del_mac_addr(struct vxgedev *vdev, struct macInfo *mac)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_vpath *vpath;\r\nvpath = &vdev->vpaths[mac->vpath_no];\r\nstatus = vxge_hw_vpath_mac_addr_delete(vpath->handle, mac->macaddr,\r\nmac->macmask);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"DA config delete entry failed for vpath:%d",\r\nvpath->device_id);\r\n} else\r\nvxge_mac_list_del(vpath, mac);\r\nreturn status;\r\n}\r\nstatic void vxge_set_multicast(struct net_device *dev)\r\n{\r\nstruct netdev_hw_addr *ha;\r\nstruct vxgedev *vdev;\r\nint i, mcast_cnt = 0;\r\nstruct __vxge_hw_device *hldev;\r\nstruct vxge_vpath *vpath;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct macInfo mac_info;\r\nint vpath_idx = 0;\r\nstruct vxge_mac_addrs *mac_entry;\r\nstruct list_head *list_head;\r\nstruct list_head *entry, *next;\r\nu8 *mac_address = NULL;\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d", __func__, __LINE__);\r\nvdev = netdev_priv(dev);\r\nhldev = vdev->devh;\r\nif (unlikely(!is_vxge_card_up(vdev)))\r\nreturn;\r\nif ((dev->flags & IFF_ALLMULTI) && (!vdev->all_multi_flg)) {\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nvxge_assert(vpath->is_open);\r\nstatus = vxge_hw_vpath_mcast_enable(vpath->handle);\r\nif (status != VXGE_HW_OK)\r\nvxge_debug_init(VXGE_ERR, "failed to enable "\r\n"multicast, status %d", status);\r\nvdev->all_multi_flg = 1;\r\n}\r\n} else if (!(dev->flags & IFF_ALLMULTI) && (vdev->all_multi_flg)) {\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nvxge_assert(vpath->is_open);\r\nstatus = vxge_hw_vpath_mcast_disable(vpath->handle);\r\nif (status != VXGE_HW_OK)\r\nvxge_debug_init(VXGE_ERR, "failed to disable "\r\n"multicast, status %d", status);\r\nvdev->all_multi_flg = 0;\r\n}\r\n}\r\nif (!vdev->config.addr_learn_en) {\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nvxge_assert(vpath->is_open);\r\nif (dev->flags & IFF_PROMISC)\r\nstatus = vxge_hw_vpath_promisc_enable(\r\nvpath->handle);\r\nelse\r\nstatus = vxge_hw_vpath_promisc_disable(\r\nvpath->handle);\r\nif (status != VXGE_HW_OK)\r\nvxge_debug_init(VXGE_ERR, "failed to %s promisc"\r\n", status %d", dev->flags&IFF_PROMISC ?\r\n"enable" : "disable", status);\r\n}\r\n}\r\nmemset(&mac_info, 0, sizeof(struct macInfo));\r\nif ((!vdev->all_multi_flg) && netdev_mc_count(dev)) {\r\nmcast_cnt = vdev->vpaths[0].mcast_addr_cnt;\r\nlist_head = &vdev->vpaths[0].mac_addr_list;\r\nif ((netdev_mc_count(dev) +\r\n(vdev->vpaths[0].mac_addr_cnt - mcast_cnt)) >\r\nvdev->vpaths[0].max_mac_addr_cnt)\r\ngoto _set_all_mcast;\r\nfor (i = 0; i < mcast_cnt; i++) {\r\nlist_for_each_safe(entry, next, list_head) {\r\nmac_entry = (struct vxge_mac_addrs *)entry;\r\nmac_address = (u8 *)&mac_entry->macaddr;\r\nmemcpy(mac_info.macaddr, mac_address, ETH_ALEN);\r\nif (is_multicast_ether_addr(mac_info.macaddr)) {\r\nfor (vpath_idx = 0; vpath_idx <\r\nvdev->no_of_vpath;\r\nvpath_idx++) {\r\nmac_info.vpath_no = vpath_idx;\r\nstatus = vxge_del_mac_addr(\r\nvdev,\r\n&mac_info);\r\n}\r\n}\r\n}\r\n}\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nmemcpy(mac_info.macaddr, ha->addr, ETH_ALEN);\r\nfor (vpath_idx = 0; vpath_idx < vdev->no_of_vpath;\r\nvpath_idx++) {\r\nmac_info.vpath_no = vpath_idx;\r\nmac_info.state = VXGE_LL_MAC_ADDR_IN_DA_TABLE;\r\nstatus = vxge_add_mac_addr(vdev, &mac_info);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s:%d Setting individual"\r\n"multicast address failed",\r\n__func__, __LINE__);\r\ngoto _set_all_mcast;\r\n}\r\n}\r\n}\r\nreturn;\r\n_set_all_mcast:\r\nmcast_cnt = vdev->vpaths[0].mcast_addr_cnt;\r\nfor (i = 0; i < mcast_cnt; i++) {\r\nlist_for_each_safe(entry, next, list_head) {\r\nmac_entry = (struct vxge_mac_addrs *)entry;\r\nmac_address = (u8 *)&mac_entry->macaddr;\r\nmemcpy(mac_info.macaddr, mac_address, ETH_ALEN);\r\nif (is_multicast_ether_addr(mac_info.macaddr))\r\nbreak;\r\n}\r\nfor (vpath_idx = 0; vpath_idx < vdev->no_of_vpath;\r\nvpath_idx++) {\r\nmac_info.vpath_no = vpath_idx;\r\nstatus = vxge_del_mac_addr(vdev, &mac_info);\r\n}\r\n}\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nvxge_assert(vpath->is_open);\r\nstatus = vxge_hw_vpath_mcast_enable(vpath->handle);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s:%d Enabling all multicasts failed",\r\n__func__, __LINE__);\r\n}\r\nvdev->all_multi_flg = 1;\r\n}\r\ndev->flags |= IFF_ALLMULTI;\r\n}\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d Exiting...", __func__, __LINE__);\r\n}\r\nstatic int vxge_set_mac_addr(struct net_device *dev, void *p)\r\n{\r\nstruct sockaddr *addr = p;\r\nstruct vxgedev *vdev;\r\nstruct __vxge_hw_device *hldev;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct macInfo mac_info_new, mac_info_old;\r\nint vpath_idx = 0;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s:%d", __func__, __LINE__);\r\nvdev = netdev_priv(dev);\r\nhldev = vdev->devh;\r\nif (!is_valid_ether_addr(addr->sa_data))\r\nreturn -EINVAL;\r\nmemset(&mac_info_new, 0, sizeof(struct macInfo));\r\nmemset(&mac_info_old, 0, sizeof(struct macInfo));\r\nvxge_debug_entryexit(VXGE_TRACE, "%s:%d Exiting...",\r\n__func__, __LINE__);\r\nmemcpy(mac_info_old.macaddr, dev->dev_addr, dev->addr_len);\r\nmemcpy(mac_info_new.macaddr, addr->sa_data, dev->addr_len);\r\nfor (vpath_idx = 0; vpath_idx < vdev->no_of_vpath; vpath_idx++) {\r\nstruct vxge_vpath *vpath = &vdev->vpaths[vpath_idx];\r\nif (!vpath->is_open) {\r\nvxge_mac_list_del(vpath, &mac_info_old);\r\nvxge_mac_list_add(vpath, &mac_info_new);\r\ncontinue;\r\n}\r\nmac_info_old.vpath_no = vpath_idx;\r\nstatus = vxge_del_mac_addr(vdev, &mac_info_old);\r\n}\r\nif (unlikely(!is_vxge_card_up(vdev))) {\r\nmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\r\nreturn VXGE_HW_OK;\r\n}\r\nfor (vpath_idx = 0; vpath_idx < vdev->no_of_vpath; vpath_idx++) {\r\nmac_info_new.vpath_no = vpath_idx;\r\nmac_info_new.state = VXGE_LL_MAC_ADDR_IN_DA_TABLE;\r\nstatus = vxge_add_mac_addr(vdev, &mac_info_new);\r\nif (status != VXGE_HW_OK)\r\nreturn -EINVAL;\r\n}\r\nmemcpy(dev->dev_addr, addr->sa_data, dev->addr_len);\r\nreturn status;\r\n}\r\nstatic void vxge_vpath_intr_enable(struct vxgedev *vdev, int vp_id)\r\n{\r\nstruct vxge_vpath *vpath = &vdev->vpaths[vp_id];\r\nint msix_id = 0;\r\nint tim_msix_id[4] = {0, 1, 0, 0};\r\nint alarm_msix_id = VXGE_ALARM_MSIX_ID;\r\nvxge_hw_vpath_intr_enable(vpath->handle);\r\nif (vdev->config.intr_type == INTA)\r\nvxge_hw_vpath_inta_unmask_tx_rx(vpath->handle);\r\nelse {\r\nvxge_hw_vpath_msix_set(vpath->handle, tim_msix_id,\r\nalarm_msix_id);\r\nmsix_id = vpath->device_id * VXGE_HW_VPATH_MSIX_ACTIVE;\r\nvxge_hw_vpath_msix_unmask(vpath->handle, msix_id);\r\nvxge_hw_vpath_msix_unmask(vpath->handle, msix_id + 1);\r\nmsix_id = (vpath->handle->vpath->hldev->first_vp_id *\r\nVXGE_HW_VPATH_MSIX_ACTIVE) + alarm_msix_id;\r\nvxge_hw_vpath_msix_unmask(vpath->handle, msix_id);\r\n}\r\n}\r\nstatic void vxge_vpath_intr_disable(struct vxgedev *vdev, int vp_id)\r\n{\r\nstruct vxge_vpath *vpath = &vdev->vpaths[vp_id];\r\nstruct __vxge_hw_device *hldev;\r\nint msix_id;\r\nhldev = pci_get_drvdata(vdev->pdev);\r\nvxge_hw_vpath_wait_receive_idle(hldev, vpath->device_id);\r\nvxge_hw_vpath_intr_disable(vpath->handle);\r\nif (vdev->config.intr_type == INTA)\r\nvxge_hw_vpath_inta_mask_tx_rx(vpath->handle);\r\nelse {\r\nmsix_id = vpath->device_id * VXGE_HW_VPATH_MSIX_ACTIVE;\r\nvxge_hw_vpath_msix_mask(vpath->handle, msix_id);\r\nvxge_hw_vpath_msix_mask(vpath->handle, msix_id + 1);\r\nmsix_id = (vpath->handle->vpath->hldev->first_vp_id *\r\nVXGE_HW_VPATH_MSIX_ACTIVE) + VXGE_ALARM_MSIX_ID;\r\nvxge_hw_vpath_msix_mask(vpath->handle, msix_id);\r\n}\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_search_mac_addr_in_da_table(struct vxge_vpath *vpath, struct macInfo *mac)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nunsigned char macmask[ETH_ALEN];\r\nunsigned char macaddr[ETH_ALEN];\r\nstatus = vxge_hw_vpath_mac_addr_get(vpath->handle,\r\nmacaddr, macmask);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"DA config list entry failed for vpath:%d",\r\nvpath->device_id);\r\nreturn status;\r\n}\r\nwhile (!ether_addr_equal(mac->macaddr, macaddr)) {\r\nstatus = vxge_hw_vpath_mac_addr_get_next(vpath->handle,\r\nmacaddr, macmask);\r\nif (status != VXGE_HW_OK)\r\nbreak;\r\n}\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status vxge_restore_vpath_mac_addr(struct vxge_vpath *vpath)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct macInfo mac_info;\r\nu8 *mac_address = NULL;\r\nstruct list_head *entry, *next;\r\nmemset(&mac_info, 0, sizeof(struct macInfo));\r\nif (vpath->is_open) {\r\nlist_for_each_safe(entry, next, &vpath->mac_addr_list) {\r\nmac_address =\r\n(u8 *)&\r\n((struct vxge_mac_addrs *)entry)->macaddr;\r\nmemcpy(mac_info.macaddr, mac_address, ETH_ALEN);\r\n((struct vxge_mac_addrs *)entry)->state =\r\nVXGE_LL_MAC_ADDR_IN_DA_TABLE;\r\nstatus = vxge_search_mac_addr_in_da_table(vpath,\r\n&mac_info);\r\nif (status != VXGE_HW_OK) {\r\nstatus = vxge_hw_vpath_mac_addr_add(\r\nvpath->handle, mac_info.macaddr,\r\nmac_info.macmask,\r\nVXGE_HW_VPATH_MAC_ADDR_ADD_DUPLICATE);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"DA add entry failed for vpath:%d",\r\nvpath->device_id);\r\n((struct vxge_mac_addrs *)entry)->state\r\n= VXGE_LL_MAC_ADDR_IN_LIST;\r\n}\r\n}\r\n}\r\n}\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status\r\nvxge_restore_vpath_vid_table(struct vxge_vpath *vpath)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxgedev *vdev = vpath->vdev;\r\nu16 vid;\r\nif (!vpath->is_open)\r\nreturn status;\r\nfor_each_set_bit(vid, vdev->active_vlans, VLAN_N_VID)\r\nstatus = vxge_hw_vpath_vid_add(vpath->handle, vid);\r\nreturn status;\r\n}\r\nstatic int vxge_reset_vpath(struct vxgedev *vdev, int vp_id)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_vpath *vpath = &vdev->vpaths[vp_id];\r\nint ret = 0;\r\nif (unlikely(!is_vxge_card_up(vdev)))\r\nreturn 0;\r\nif (test_bit(__VXGE_STATE_RESET_CARD, &vdev->state))\r\nreturn 0;\r\nif (vpath->handle) {\r\nif (vxge_hw_vpath_reset(vpath->handle) == VXGE_HW_OK) {\r\nif (is_vxge_card_up(vdev) &&\r\nvxge_hw_vpath_recover_from_reset(vpath->handle)\r\n!= VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"vxge_hw_vpath_recover_from_reset"\r\n"failed for vpath:%d", vp_id);\r\nreturn status;\r\n}\r\n} else {\r\nvxge_debug_init(VXGE_ERR,\r\n"vxge_hw_vpath_reset failed for"\r\n"vpath:%d", vp_id);\r\nreturn status;\r\n}\r\n} else\r\nreturn VXGE_HW_FAIL;\r\nvxge_restore_vpath_mac_addr(vpath);\r\nvxge_restore_vpath_vid_table(vpath);\r\nvxge_hw_vpath_bcast_enable(vpath->handle);\r\nif (vdev->all_multi_flg) {\r\nstatus = vxge_hw_vpath_mcast_enable(vpath->handle);\r\nif (status != VXGE_HW_OK)\r\nvxge_debug_init(VXGE_ERR,\r\n"%s:%d Enabling multicast failed",\r\n__func__, __LINE__);\r\n}\r\nvxge_vpath_intr_enable(vdev, vp_id);\r\nsmp_wmb();\r\nvxge_hw_vpath_enable(vpath->handle);\r\nsmp_wmb();\r\nvxge_hw_vpath_rx_doorbell_init(vpath->handle);\r\nvpath->ring.last_status = VXGE_HW_OK;\r\nclear_bit(vp_id, &vdev->vp_reset);\r\nif (netif_tx_queue_stopped(vpath->fifo.txq))\r\nnetif_tx_wake_queue(vpath->fifo.txq);\r\nreturn ret;\r\n}\r\nstatic void vxge_config_ci_for_tti_rti(struct vxgedev *vdev)\r\n{\r\nint i = 0;\r\nif (vdev->config.intr_type == MSI_X) {\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nstruct __vxge_hw_ring *hw_ring;\r\nhw_ring = vdev->vpaths[i].ring.handle;\r\nvxge_hw_vpath_dynamic_rti_ci_set(hw_ring);\r\n}\r\n}\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nstruct __vxge_hw_fifo *hw_fifo = vdev->vpaths[i].fifo.handle;\r\nvxge_hw_vpath_tti_ci_set(hw_fifo);\r\nif ((vdev->config.intr_type == INTA) && (i == 0))\r\nbreak;\r\n}\r\nreturn;\r\n}\r\nstatic int do_vxge_reset(struct vxgedev *vdev, int event)\r\n{\r\nenum vxge_hw_status status;\r\nint ret = 0, vp_id, i;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s:%d", __func__, __LINE__);\r\nif ((event == VXGE_LL_FULL_RESET) || (event == VXGE_LL_START_RESET)) {\r\nif (unlikely(!is_vxge_card_up(vdev)))\r\nreturn 0;\r\nif (test_and_set_bit(__VXGE_STATE_RESET_CARD, &vdev->state))\r\nreturn 0;\r\n}\r\nif (event == VXGE_LL_FULL_RESET) {\r\nnetif_carrier_off(vdev->ndev);\r\nfor (vp_id = 0; vp_id < vdev->no_of_vpath; vp_id++) {\r\nwhile (test_bit(vp_id, &vdev->vp_reset))\r\nmsleep(50);\r\n}\r\nnetif_carrier_on(vdev->ndev);\r\nif (unlikely(vdev->exec_mode)) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: execution mode is debug, returning..",\r\nvdev->ndev->name);\r\nclear_bit(__VXGE_STATE_CARD_UP, &vdev->state);\r\nnetif_tx_stop_all_queues(vdev->ndev);\r\nreturn 0;\r\n}\r\n}\r\nif (event == VXGE_LL_FULL_RESET) {\r\nvxge_hw_device_wait_receive_idle(vdev->devh);\r\nvxge_hw_device_intr_disable(vdev->devh);\r\nswitch (vdev->cric_err_event) {\r\ncase VXGE_HW_EVENT_UNKNOWN:\r\nnetif_tx_stop_all_queues(vdev->ndev);\r\nvxge_debug_init(VXGE_ERR,\r\n"fatal: %s: Disabling device due to"\r\n"unknown error",\r\nvdev->ndev->name);\r\nret = -EPERM;\r\ngoto out;\r\ncase VXGE_HW_EVENT_RESET_START:\r\nbreak;\r\ncase VXGE_HW_EVENT_RESET_COMPLETE:\r\ncase VXGE_HW_EVENT_LINK_DOWN:\r\ncase VXGE_HW_EVENT_LINK_UP:\r\ncase VXGE_HW_EVENT_ALARM_CLEARED:\r\ncase VXGE_HW_EVENT_ECCERR:\r\ncase VXGE_HW_EVENT_MRPCIM_ECCERR:\r\nret = -EPERM;\r\ngoto out;\r\ncase VXGE_HW_EVENT_FIFO_ERR:\r\ncase VXGE_HW_EVENT_VPATH_ERR:\r\nbreak;\r\ncase VXGE_HW_EVENT_CRITICAL_ERR:\r\nnetif_tx_stop_all_queues(vdev->ndev);\r\nvxge_debug_init(VXGE_ERR,\r\n"fatal: %s: Disabling device due to"\r\n"serious error",\r\nvdev->ndev->name);\r\nret = -EPERM;\r\ngoto out;\r\ncase VXGE_HW_EVENT_SERR:\r\nnetif_tx_stop_all_queues(vdev->ndev);\r\nvxge_debug_init(VXGE_ERR,\r\n"fatal: %s: Disabling device due to"\r\n"serious error",\r\nvdev->ndev->name);\r\nret = -EPERM;\r\ngoto out;\r\ncase VXGE_HW_EVENT_SRPCIM_SERR:\r\ncase VXGE_HW_EVENT_MRPCIM_SERR:\r\nret = -EPERM;\r\ngoto out;\r\ncase VXGE_HW_EVENT_SLOT_FREEZE:\r\nnetif_tx_stop_all_queues(vdev->ndev);\r\nvxge_debug_init(VXGE_ERR,\r\n"fatal: %s: Disabling device due to"\r\n"slot freeze",\r\nvdev->ndev->name);\r\nret = -EPERM;\r\ngoto out;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nif ((event == VXGE_LL_FULL_RESET) || (event == VXGE_LL_START_RESET))\r\nnetif_tx_stop_all_queues(vdev->ndev);\r\nif (event == VXGE_LL_FULL_RESET) {\r\nstatus = vxge_reset_all_vpaths(vdev);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"fatal: %s: can not reset vpaths",\r\nvdev->ndev->name);\r\nret = -EPERM;\r\ngoto out;\r\n}\r\n}\r\nif (event == VXGE_LL_COMPL_RESET) {\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nif (vdev->vpaths[i].handle) {\r\nif (vxge_hw_vpath_recover_from_reset(\r\nvdev->vpaths[i].handle)\r\n!= VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"vxge_hw_vpath_recover_"\r\n"from_reset failed for vpath: "\r\n"%d", i);\r\nret = -EPERM;\r\ngoto out;\r\n}\r\n} else {\r\nvxge_debug_init(VXGE_ERR,\r\n"vxge_hw_vpath_reset failed for "\r\n"vpath:%d", i);\r\nret = -EPERM;\r\ngoto out;\r\n}\r\n}\r\nif ((event == VXGE_LL_FULL_RESET) || (event == VXGE_LL_COMPL_RESET)) {\r\nfor (vp_id = 0; vp_id < vdev->no_of_vpath; vp_id++) {\r\nvxge_restore_vpath_mac_addr(&vdev->vpaths[vp_id]);\r\nvxge_restore_vpath_vid_table(&vdev->vpaths[vp_id]);\r\n}\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nvxge_vpath_intr_enable(vdev, i);\r\nvxge_hw_device_intr_enable(vdev->devh);\r\nsmp_wmb();\r\nset_bit(__VXGE_STATE_CARD_UP, &vdev->state);\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvxge_hw_vpath_enable(vdev->vpaths[i].handle);\r\nsmp_wmb();\r\nvxge_hw_vpath_rx_doorbell_init(vdev->vpaths[i].handle);\r\n}\r\nnetif_tx_wake_all_queues(vdev->ndev);\r\n}\r\nvxge_config_ci_for_tti_rti(vdev);\r\nout:\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d Exiting...", __func__, __LINE__);\r\nif ((event == VXGE_LL_FULL_RESET) || (event == VXGE_LL_COMPL_RESET))\r\nclear_bit(__VXGE_STATE_RESET_CARD, &vdev->state);\r\nreturn ret;\r\n}\r\nstatic void vxge_reset(struct work_struct *work)\r\n{\r\nstruct vxgedev *vdev = container_of(work, struct vxgedev, reset_task);\r\nif (!netif_running(vdev->ndev))\r\nreturn;\r\ndo_vxge_reset(vdev, VXGE_LL_FULL_RESET);\r\n}\r\nstatic int vxge_poll_msix(struct napi_struct *napi, int budget)\r\n{\r\nstruct vxge_ring *ring = container_of(napi, struct vxge_ring, napi);\r\nint pkts_processed;\r\nint budget_org = budget;\r\nring->budget = budget;\r\nring->pkts_processed = 0;\r\nvxge_hw_vpath_poll_rx(ring->handle);\r\npkts_processed = ring->pkts_processed;\r\nif (pkts_processed < budget_org) {\r\nnapi_complete_done(napi, pkts_processed);\r\nvxge_hw_channel_msix_unmask(\r\n(struct __vxge_hw_channel *)ring->handle,\r\nring->rx_vector_no);\r\nmmiowb();\r\n}\r\nreturn pkts_processed;\r\n}\r\nstatic int vxge_poll_inta(struct napi_struct *napi, int budget)\r\n{\r\nstruct vxgedev *vdev = container_of(napi, struct vxgedev, napi);\r\nint pkts_processed = 0;\r\nint i;\r\nint budget_org = budget;\r\nstruct vxge_ring *ring;\r\nstruct __vxge_hw_device *hldev = pci_get_drvdata(vdev->pdev);\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nring = &vdev->vpaths[i].ring;\r\nring->budget = budget;\r\nring->pkts_processed = 0;\r\nvxge_hw_vpath_poll_rx(ring->handle);\r\npkts_processed += ring->pkts_processed;\r\nbudget -= ring->pkts_processed;\r\nif (budget <= 0)\r\nbreak;\r\n}\r\nVXGE_COMPLETE_ALL_TX(vdev);\r\nif (pkts_processed < budget_org) {\r\nnapi_complete_done(napi, pkts_processed);\r\nvxge_hw_device_unmask_all(hldev);\r\nvxge_hw_device_flush_io(hldev);\r\n}\r\nreturn pkts_processed;\r\n}\r\nstatic void vxge_netpoll(struct net_device *dev)\r\n{\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nstruct pci_dev *pdev = vdev->pdev;\r\nstruct __vxge_hw_device *hldev = pci_get_drvdata(pdev);\r\nconst int irq = pdev->irq;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s:%d", __func__, __LINE__);\r\nif (pci_channel_offline(pdev))\r\nreturn;\r\ndisable_irq(irq);\r\nvxge_hw_device_clear_tx_rx(hldev);\r\nvxge_hw_device_clear_tx_rx(hldev);\r\nVXGE_COMPLETE_ALL_RX(vdev);\r\nVXGE_COMPLETE_ALL_TX(vdev);\r\nenable_irq(irq);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d Exiting...", __func__, __LINE__);\r\n}\r\nstatic enum vxge_hw_status vxge_rth_configure(struct vxgedev *vdev)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_hw_rth_hash_types hash_types;\r\nu8 itable[256] = {0};\r\nu8 mtable[256] = {0};\r\nint index;\r\nfor (index = 0; index < (1 << vdev->config.rth_bkt_sz); index++) {\r\nitable[index] = index;\r\nmtable[index] = index % vdev->no_of_vpath;\r\n}\r\nstatus = vxge_hw_vpath_rts_rth_itable_set(vdev->vp_handles,\r\nvdev->no_of_vpath,\r\nmtable, itable,\r\nvdev->config.rth_bkt_sz);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"RTH indirection table configuration failed "\r\n"for vpath:%d", vdev->vpaths[0].device_id);\r\nreturn status;\r\n}\r\nhash_types.hash_type_tcpipv4_en = vdev->config.rth_hash_type_tcpipv4;\r\nhash_types.hash_type_ipv4_en = vdev->config.rth_hash_type_ipv4;\r\nhash_types.hash_type_tcpipv6_en = vdev->config.rth_hash_type_tcpipv6;\r\nhash_types.hash_type_ipv6_en = vdev->config.rth_hash_type_ipv6;\r\nhash_types.hash_type_tcpipv6ex_en =\r\nvdev->config.rth_hash_type_tcpipv6ex;\r\nhash_types.hash_type_ipv6ex_en = vdev->config.rth_hash_type_ipv6ex;\r\nfor (index = 0; index < vdev->no_of_vpath; index++) {\r\nstatus = vxge_hw_vpath_rts_rth_set(\r\nvdev->vpaths[index].handle,\r\nvdev->config.rth_algorithm,\r\n&hash_types,\r\nvdev->config.rth_bkt_sz);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"RTH configuration failed for vpath:%d",\r\nvdev->vpaths[index].device_id);\r\nreturn status;\r\n}\r\n}\r\nreturn status;\r\n}\r\nstatic enum vxge_hw_status vxge_reset_all_vpaths(struct vxgedev *vdev)\r\n{\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_vpath *vpath;\r\nint i;\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nif (vpath->handle) {\r\nif (vxge_hw_vpath_reset(vpath->handle) == VXGE_HW_OK) {\r\nif (is_vxge_card_up(vdev) &&\r\nvxge_hw_vpath_recover_from_reset(\r\nvpath->handle) != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"vxge_hw_vpath_recover_"\r\n"from_reset failed for vpath: "\r\n"%d", i);\r\nreturn status;\r\n}\r\n} else {\r\nvxge_debug_init(VXGE_ERR,\r\n"vxge_hw_vpath_reset failed for "\r\n"vpath:%d", i);\r\nreturn status;\r\n}\r\n}\r\n}\r\nreturn status;\r\n}\r\nstatic void vxge_close_vpaths(struct vxgedev *vdev, int index)\r\n{\r\nstruct vxge_vpath *vpath;\r\nint i;\r\nfor (i = index; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nif (vpath->handle && vpath->is_open) {\r\nvxge_hw_vpath_close(vpath->handle);\r\nvdev->stats.vpaths_open--;\r\n}\r\nvpath->is_open = 0;\r\nvpath->handle = NULL;\r\n}\r\n}\r\nstatic int vxge_open_vpaths(struct vxgedev *vdev)\r\n{\r\nstruct vxge_hw_vpath_attr attr;\r\nenum vxge_hw_status status;\r\nstruct vxge_vpath *vpath;\r\nu32 vp_id = 0;\r\nint i;\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nvxge_assert(vpath->is_configured);\r\nif (!vdev->titan1) {\r\nstruct vxge_hw_vp_config *vcfg;\r\nvcfg = &vdev->devh->config.vp_config[vpath->device_id];\r\nvcfg->rti.urange_a = RTI_T1A_RX_URANGE_A;\r\nvcfg->rti.urange_b = RTI_T1A_RX_URANGE_B;\r\nvcfg->rti.urange_c = RTI_T1A_RX_URANGE_C;\r\nvcfg->tti.uec_a = TTI_T1A_TX_UFC_A;\r\nvcfg->tti.uec_b = TTI_T1A_TX_UFC_B;\r\nvcfg->tti.uec_c = TTI_T1A_TX_UFC_C(vdev->mtu);\r\nvcfg->tti.uec_d = TTI_T1A_TX_UFC_D(vdev->mtu);\r\nvcfg->tti.ltimer_val = VXGE_T1A_TTI_LTIMER_VAL;\r\nvcfg->tti.rtimer_val = VXGE_T1A_TTI_RTIMER_VAL;\r\n}\r\nattr.vp_id = vpath->device_id;\r\nattr.fifo_attr.callback = vxge_xmit_compl;\r\nattr.fifo_attr.txdl_term = vxge_tx_term;\r\nattr.fifo_attr.per_txdl_space = sizeof(struct vxge_tx_priv);\r\nattr.fifo_attr.userdata = &vpath->fifo;\r\nattr.ring_attr.callback = vxge_rx_1b_compl;\r\nattr.ring_attr.rxd_init = vxge_rx_initial_replenish;\r\nattr.ring_attr.rxd_term = vxge_rx_term;\r\nattr.ring_attr.per_rxd_space = sizeof(struct vxge_rx_priv);\r\nattr.ring_attr.userdata = &vpath->ring;\r\nvpath->ring.ndev = vdev->ndev;\r\nvpath->ring.pdev = vdev->pdev;\r\nstatus = vxge_hw_vpath_open(vdev->devh, &attr, &vpath->handle);\r\nif (status == VXGE_HW_OK) {\r\nvpath->fifo.handle =\r\n(struct __vxge_hw_fifo *)attr.fifo_attr.userdata;\r\nvpath->ring.handle =\r\n(struct __vxge_hw_ring *)attr.ring_attr.userdata;\r\nvpath->fifo.tx_steering_type =\r\nvdev->config.tx_steering_type;\r\nvpath->fifo.ndev = vdev->ndev;\r\nvpath->fifo.pdev = vdev->pdev;\r\nu64_stats_init(&vpath->fifo.stats.syncp);\r\nu64_stats_init(&vpath->ring.stats.syncp);\r\nif (vdev->config.tx_steering_type)\r\nvpath->fifo.txq =\r\nnetdev_get_tx_queue(vdev->ndev, i);\r\nelse\r\nvpath->fifo.txq =\r\nnetdev_get_tx_queue(vdev->ndev, 0);\r\nvpath->fifo.indicate_max_pkts =\r\nvdev->config.fifo_indicate_max_pkts;\r\nvpath->fifo.tx_vector_no = 0;\r\nvpath->ring.rx_vector_no = 0;\r\nvpath->ring.rx_hwts = vdev->rx_hwts;\r\nvpath->is_open = 1;\r\nvdev->vp_handles[i] = vpath->handle;\r\nvpath->ring.vlan_tag_strip = vdev->vlan_tag_strip;\r\nvdev->stats.vpaths_open++;\r\n} else {\r\nvdev->stats.vpath_open_fail++;\r\nvxge_debug_init(VXGE_ERR, "%s: vpath: %d failed to "\r\n"open with status: %d",\r\nvdev->ndev->name, vpath->device_id,\r\nstatus);\r\nvxge_close_vpaths(vdev, 0);\r\nreturn -EPERM;\r\n}\r\nvp_id = vpath->handle->vpath->vp_id;\r\nvdev->vpaths_deployed |= vxge_mBIT(vp_id);\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic void adaptive_coalesce_tx_interrupts(struct vxge_fifo *fifo)\r\n{\r\nfifo->interrupt_count++;\r\nif (time_before(fifo->jiffies + HZ / 100, jiffies)) {\r\nstruct __vxge_hw_fifo *hw_fifo = fifo->handle;\r\nfifo->jiffies = jiffies;\r\nif (fifo->interrupt_count > VXGE_T1A_MAX_TX_INTERRUPT_COUNT &&\r\nhw_fifo->rtimer != VXGE_TTI_RTIMER_ADAPT_VAL) {\r\nhw_fifo->rtimer = VXGE_TTI_RTIMER_ADAPT_VAL;\r\nvxge_hw_vpath_dynamic_tti_rtimer_set(hw_fifo);\r\n} else if (hw_fifo->rtimer != 0) {\r\nhw_fifo->rtimer = 0;\r\nvxge_hw_vpath_dynamic_tti_rtimer_set(hw_fifo);\r\n}\r\nfifo->interrupt_count = 0;\r\n}\r\n}\r\nstatic void adaptive_coalesce_rx_interrupts(struct vxge_ring *ring)\r\n{\r\nring->interrupt_count++;\r\nif (time_before(ring->jiffies + HZ / 100, jiffies)) {\r\nstruct __vxge_hw_ring *hw_ring = ring->handle;\r\nring->jiffies = jiffies;\r\nif (ring->interrupt_count > VXGE_T1A_MAX_INTERRUPT_COUNT &&\r\nhw_ring->rtimer != VXGE_RTI_RTIMER_ADAPT_VAL) {\r\nhw_ring->rtimer = VXGE_RTI_RTIMER_ADAPT_VAL;\r\nvxge_hw_vpath_dynamic_rti_rtimer_set(hw_ring);\r\n} else if (hw_ring->rtimer != 0) {\r\nhw_ring->rtimer = 0;\r\nvxge_hw_vpath_dynamic_rti_rtimer_set(hw_ring);\r\n}\r\nring->interrupt_count = 0;\r\n}\r\n}\r\nstatic irqreturn_t vxge_isr_napi(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev;\r\nstruct __vxge_hw_device *hldev;\r\nu64 reason;\r\nenum vxge_hw_status status;\r\nstruct vxgedev *vdev = (struct vxgedev *)dev_id;\r\nvxge_debug_intr(VXGE_TRACE, "%s:%d", __func__, __LINE__);\r\ndev = vdev->ndev;\r\nhldev = pci_get_drvdata(vdev->pdev);\r\nif (pci_channel_offline(vdev->pdev))\r\nreturn IRQ_NONE;\r\nif (unlikely(!is_vxge_card_up(vdev)))\r\nreturn IRQ_HANDLED;\r\nstatus = vxge_hw_device_begin_irq(hldev, vdev->exec_mode, &reason);\r\nif (status == VXGE_HW_OK) {\r\nvxge_hw_device_mask_all(hldev);\r\nif (reason &\r\nVXGE_HW_TITAN_GENERAL_INT_STATUS_VPATH_TRAFFIC_INT(\r\nvdev->vpaths_deployed >>\r\n(64 - VXGE_HW_MAX_VIRTUAL_PATHS))) {\r\nvxge_hw_device_clear_tx_rx(hldev);\r\nnapi_schedule(&vdev->napi);\r\nvxge_debug_intr(VXGE_TRACE,\r\n"%s:%d Exiting...", __func__, __LINE__);\r\nreturn IRQ_HANDLED;\r\n} else\r\nvxge_hw_device_unmask_all(hldev);\r\n} else if (unlikely((status == VXGE_HW_ERR_VPATH) ||\r\n(status == VXGE_HW_ERR_CRITICAL) ||\r\n(status == VXGE_HW_ERR_FIFO))) {\r\nvxge_hw_device_mask_all(hldev);\r\nvxge_hw_device_flush_io(hldev);\r\nreturn IRQ_HANDLED;\r\n} else if (unlikely(status == VXGE_HW_ERR_SLOT_FREEZE))\r\nreturn IRQ_HANDLED;\r\nvxge_debug_intr(VXGE_TRACE, "%s:%d Exiting...", __func__, __LINE__);\r\nreturn IRQ_NONE;\r\n}\r\nstatic irqreturn_t vxge_tx_msix_handle(int irq, void *dev_id)\r\n{\r\nstruct vxge_fifo *fifo = (struct vxge_fifo *)dev_id;\r\nadaptive_coalesce_tx_interrupts(fifo);\r\nvxge_hw_channel_msix_mask((struct __vxge_hw_channel *)fifo->handle,\r\nfifo->tx_vector_no);\r\nvxge_hw_channel_msix_clear((struct __vxge_hw_channel *)fifo->handle,\r\nfifo->tx_vector_no);\r\nVXGE_COMPLETE_VPATH_TX(fifo);\r\nvxge_hw_channel_msix_unmask((struct __vxge_hw_channel *)fifo->handle,\r\nfifo->tx_vector_no);\r\nmmiowb();\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t vxge_rx_msix_napi_handle(int irq, void *dev_id)\r\n{\r\nstruct vxge_ring *ring = (struct vxge_ring *)dev_id;\r\nadaptive_coalesce_rx_interrupts(ring);\r\nvxge_hw_channel_msix_mask((struct __vxge_hw_channel *)ring->handle,\r\nring->rx_vector_no);\r\nvxge_hw_channel_msix_clear((struct __vxge_hw_channel *)ring->handle,\r\nring->rx_vector_no);\r\nnapi_schedule(&ring->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t\r\nvxge_alarm_msix_handle(int irq, void *dev_id)\r\n{\r\nint i;\r\nenum vxge_hw_status status;\r\nstruct vxge_vpath *vpath = (struct vxge_vpath *)dev_id;\r\nstruct vxgedev *vdev = vpath->vdev;\r\nint msix_id = (vpath->handle->vpath->vp_id *\r\nVXGE_HW_VPATH_MSIX_ACTIVE) + VXGE_ALARM_MSIX_ID;\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvxge_hw_vpath_msix_mask(vdev->vpaths[i].handle, msix_id);\r\nvxge_hw_vpath_msix_clear(vdev->vpaths[i].handle, msix_id);\r\nmmiowb();\r\nstatus = vxge_hw_vpath_alarm_process(vdev->vpaths[i].handle,\r\nvdev->exec_mode);\r\nif (status == VXGE_HW_OK) {\r\nvxge_hw_vpath_msix_unmask(vdev->vpaths[i].handle,\r\nmsix_id);\r\nmmiowb();\r\ncontinue;\r\n}\r\nvxge_debug_intr(VXGE_ERR,\r\n"%s: vxge_hw_vpath_alarm_process failed %x ",\r\nVXGE_DRIVER_NAME, status);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int vxge_alloc_msix(struct vxgedev *vdev)\r\n{\r\nint j, i, ret = 0;\r\nint msix_intr_vect = 0, temp;\r\nvdev->intr_cnt = 0;\r\nstart:\r\nvdev->intr_cnt = vdev->no_of_vpath * 2;\r\nvdev->intr_cnt++;\r\nvdev->entries = kcalloc(vdev->intr_cnt, sizeof(struct msix_entry),\r\nGFP_KERNEL);\r\nif (!vdev->entries) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: memory allocation failed",\r\nVXGE_DRIVER_NAME);\r\nret = -ENOMEM;\r\ngoto alloc_entries_failed;\r\n}\r\nvdev->vxge_entries = kcalloc(vdev->intr_cnt,\r\nsizeof(struct vxge_msix_entry),\r\nGFP_KERNEL);\r\nif (!vdev->vxge_entries) {\r\nvxge_debug_init(VXGE_ERR, "%s: memory allocation failed",\r\nVXGE_DRIVER_NAME);\r\nret = -ENOMEM;\r\ngoto alloc_vxge_entries_failed;\r\n}\r\nfor (i = 0, j = 0; i < vdev->no_of_vpath; i++) {\r\nmsix_intr_vect = i * VXGE_HW_VPATH_MSIX_ACTIVE;\r\nvdev->entries[j].entry = msix_intr_vect;\r\nvdev->vxge_entries[j].entry = msix_intr_vect;\r\nvdev->vxge_entries[j].in_use = 0;\r\nj++;\r\nvdev->entries[j].entry = msix_intr_vect + 1;\r\nvdev->vxge_entries[j].entry = msix_intr_vect + 1;\r\nvdev->vxge_entries[j].in_use = 0;\r\nj++;\r\n}\r\nvdev->entries[j].entry = VXGE_ALARM_MSIX_ID;\r\nvdev->vxge_entries[j].entry = VXGE_ALARM_MSIX_ID;\r\nvdev->vxge_entries[j].in_use = 0;\r\nret = pci_enable_msix_range(vdev->pdev,\r\nvdev->entries, 3, vdev->intr_cnt);\r\nif (ret < 0) {\r\nret = -ENODEV;\r\ngoto enable_msix_failed;\r\n} else if (ret < vdev->intr_cnt) {\r\npci_disable_msix(vdev->pdev);\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: MSI-X enable failed for %d vectors, ret: %d",\r\nVXGE_DRIVER_NAME, vdev->intr_cnt, ret);\r\nif (max_config_vpath != VXGE_USE_DEFAULT) {\r\nret = -ENODEV;\r\ngoto enable_msix_failed;\r\n}\r\nkfree(vdev->entries);\r\nkfree(vdev->vxge_entries);\r\nvdev->entries = NULL;\r\nvdev->vxge_entries = NULL;\r\ntemp = (ret - 1)/2;\r\nvxge_close_vpaths(vdev, temp);\r\nvdev->no_of_vpath = temp;\r\ngoto start;\r\n}\r\nreturn 0;\r\nenable_msix_failed:\r\nkfree(vdev->vxge_entries);\r\nalloc_vxge_entries_failed:\r\nkfree(vdev->entries);\r\nalloc_entries_failed:\r\nreturn ret;\r\n}\r\nstatic int vxge_enable_msix(struct vxgedev *vdev)\r\n{\r\nint i, ret = 0;\r\nint tim_msix_id[4] = {0, 1, 0, 0};\r\nvdev->intr_cnt = 0;\r\nret = vxge_alloc_msix(vdev);\r\nif (!ret) {\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nstruct vxge_vpath *vpath = &vdev->vpaths[i];\r\nvpath->ring.rx_vector_no = (vpath->device_id *\r\nVXGE_HW_VPATH_MSIX_ACTIVE) + 1;\r\nvpath->fifo.tx_vector_no = (vpath->device_id *\r\nVXGE_HW_VPATH_MSIX_ACTIVE);\r\nvxge_hw_vpath_msix_set(vpath->handle, tim_msix_id,\r\nVXGE_ALARM_MSIX_ID);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic void vxge_rem_msix_isr(struct vxgedev *vdev)\r\n{\r\nint intr_cnt;\r\nfor (intr_cnt = 0; intr_cnt < (vdev->no_of_vpath * 2 + 1);\r\nintr_cnt++) {\r\nif (vdev->vxge_entries[intr_cnt].in_use) {\r\nsynchronize_irq(vdev->entries[intr_cnt].vector);\r\nfree_irq(vdev->entries[intr_cnt].vector,\r\nvdev->vxge_entries[intr_cnt].arg);\r\nvdev->vxge_entries[intr_cnt].in_use = 0;\r\n}\r\n}\r\nkfree(vdev->entries);\r\nkfree(vdev->vxge_entries);\r\nvdev->entries = NULL;\r\nvdev->vxge_entries = NULL;\r\nif (vdev->config.intr_type == MSI_X)\r\npci_disable_msix(vdev->pdev);\r\n}\r\nstatic void vxge_rem_isr(struct vxgedev *vdev)\r\n{\r\nif (IS_ENABLED(CONFIG_PCI_MSI) &&\r\nvdev->config.intr_type == MSI_X) {\r\nvxge_rem_msix_isr(vdev);\r\n} else if (vdev->config.intr_type == INTA) {\r\nsynchronize_irq(vdev->pdev->irq);\r\nfree_irq(vdev->pdev->irq, vdev);\r\n}\r\n}\r\nstatic int vxge_add_isr(struct vxgedev *vdev)\r\n{\r\nint ret = 0;\r\nint vp_idx = 0, intr_idx = 0, intr_cnt = 0, msix_idx = 0, irq_req = 0;\r\nint pci_fun = PCI_FUNC(vdev->pdev->devfn);\r\nif (IS_ENABLED(CONFIG_PCI_MSI) && vdev->config.intr_type == MSI_X)\r\nret = vxge_enable_msix(vdev);\r\nif (ret) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Enabling MSI-X Failed", VXGE_DRIVER_NAME);\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Defaulting to INTA", VXGE_DRIVER_NAME);\r\nvdev->config.intr_type = INTA;\r\n}\r\nif (IS_ENABLED(CONFIG_PCI_MSI) && vdev->config.intr_type == MSI_X) {\r\nfor (intr_idx = 0;\r\nintr_idx < (vdev->no_of_vpath *\r\nVXGE_HW_VPATH_MSIX_ACTIVE); intr_idx++) {\r\nmsix_idx = intr_idx % VXGE_HW_VPATH_MSIX_ACTIVE;\r\nirq_req = 0;\r\nswitch (msix_idx) {\r\ncase 0:\r\nsnprintf(vdev->desc[intr_cnt], VXGE_INTR_STRLEN,\r\n"%s:vxge:MSI-X %d - Tx - fn:%d vpath:%d",\r\nvdev->ndev->name,\r\nvdev->entries[intr_cnt].entry,\r\npci_fun, vp_idx);\r\nret = request_irq(\r\nvdev->entries[intr_cnt].vector,\r\nvxge_tx_msix_handle, 0,\r\nvdev->desc[intr_cnt],\r\n&vdev->vpaths[vp_idx].fifo);\r\nvdev->vxge_entries[intr_cnt].arg =\r\n&vdev->vpaths[vp_idx].fifo;\r\nirq_req = 1;\r\nbreak;\r\ncase 1:\r\nsnprintf(vdev->desc[intr_cnt], VXGE_INTR_STRLEN,\r\n"%s:vxge:MSI-X %d - Rx - fn:%d vpath:%d",\r\nvdev->ndev->name,\r\nvdev->entries[intr_cnt].entry,\r\npci_fun, vp_idx);\r\nret = request_irq(\r\nvdev->entries[intr_cnt].vector,\r\nvxge_rx_msix_napi_handle,\r\n0,\r\nvdev->desc[intr_cnt],\r\n&vdev->vpaths[vp_idx].ring);\r\nvdev->vxge_entries[intr_cnt].arg =\r\n&vdev->vpaths[vp_idx].ring;\r\nirq_req = 1;\r\nbreak;\r\n}\r\nif (ret) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: MSIX - %d Registration failed",\r\nvdev->ndev->name, intr_cnt);\r\nvxge_rem_msix_isr(vdev);\r\nvdev->config.intr_type = INTA;\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Defaulting to INTA"\r\n, vdev->ndev->name);\r\ngoto INTA_MODE;\r\n}\r\nif (irq_req) {\r\nvdev->vxge_entries[intr_cnt].in_use = 1;\r\nmsix_idx += vdev->vpaths[vp_idx].device_id *\r\nVXGE_HW_VPATH_MSIX_ACTIVE;\r\nvxge_hw_vpath_msix_unmask(\r\nvdev->vpaths[vp_idx].handle,\r\nmsix_idx);\r\nintr_cnt++;\r\n}\r\nif (((intr_idx + 1) % VXGE_HW_VPATH_MSIX_ACTIVE == 0) &&\r\n(vp_idx < (vdev->no_of_vpath - 1)))\r\nvp_idx++;\r\n}\r\nintr_cnt = vdev->no_of_vpath * 2;\r\nsnprintf(vdev->desc[intr_cnt], VXGE_INTR_STRLEN,\r\n"%s:vxge:MSI-X %d - Alarm - fn:%d",\r\nvdev->ndev->name,\r\nvdev->entries[intr_cnt].entry,\r\npci_fun);\r\nret = request_irq(vdev->entries[intr_cnt].vector,\r\nvxge_alarm_msix_handle, 0,\r\nvdev->desc[intr_cnt],\r\n&vdev->vpaths[0]);\r\nif (ret) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: MSIX - %d Registration failed",\r\nvdev->ndev->name, intr_cnt);\r\nvxge_rem_msix_isr(vdev);\r\nvdev->config.intr_type = INTA;\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Defaulting to INTA",\r\nvdev->ndev->name);\r\ngoto INTA_MODE;\r\n}\r\nmsix_idx = (vdev->vpaths[0].handle->vpath->vp_id *\r\nVXGE_HW_VPATH_MSIX_ACTIVE) + VXGE_ALARM_MSIX_ID;\r\nvxge_hw_vpath_msix_unmask(vdev->vpaths[vp_idx].handle,\r\nmsix_idx);\r\nvdev->vxge_entries[intr_cnt].in_use = 1;\r\nvdev->vxge_entries[intr_cnt].arg = &vdev->vpaths[0];\r\n}\r\nINTA_MODE:\r\nif (vdev->config.intr_type == INTA) {\r\nsnprintf(vdev->desc[0], VXGE_INTR_STRLEN,\r\n"%s:vxge:INTA", vdev->ndev->name);\r\nvxge_hw_device_set_intr_type(vdev->devh,\r\nVXGE_HW_INTR_MODE_IRQLINE);\r\nvxge_hw_vpath_tti_ci_set(vdev->vpaths[0].fifo.handle);\r\nret = request_irq((int) vdev->pdev->irq,\r\nvxge_isr_napi,\r\nIRQF_SHARED, vdev->desc[0], vdev);\r\nif (ret) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s %s-%d: ISR registration failed",\r\nVXGE_DRIVER_NAME, "IRQ", vdev->pdev->irq);\r\nreturn -ENODEV;\r\n}\r\nvxge_debug_init(VXGE_TRACE,\r\n"new %s-%d line allocated",\r\n"IRQ", vdev->pdev->irq);\r\n}\r\nreturn VXGE_HW_OK;\r\n}\r\nstatic void vxge_poll_vp_reset(unsigned long data)\r\n{\r\nstruct vxgedev *vdev = (struct vxgedev *)data;\r\nint i, j = 0;\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nif (test_bit(i, &vdev->vp_reset)) {\r\nvxge_reset_vpath(vdev, i);\r\nj++;\r\n}\r\n}\r\nif (j && (vdev->config.intr_type != MSI_X)) {\r\nvxge_hw_device_unmask_all(vdev->devh);\r\nvxge_hw_device_flush_io(vdev->devh);\r\n}\r\nmod_timer(&vdev->vp_reset_timer, jiffies + HZ / 2);\r\n}\r\nstatic void vxge_poll_vp_lockup(unsigned long data)\r\n{\r\nstruct vxgedev *vdev = (struct vxgedev *)data;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxge_vpath *vpath;\r\nstruct vxge_ring *ring;\r\nint i;\r\nunsigned long rx_frms;\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nring = &vdev->vpaths[i].ring;\r\nrx_frms = ACCESS_ONCE(ring->stats.rx_frms);\r\nif (ring->stats.prev_rx_frms == rx_frms) {\r\nstatus = vxge_hw_vpath_check_leak(ring->handle);\r\nif ((VXGE_HW_FAIL == status) &&\r\n(VXGE_HW_FAIL == ring->last_status)) {\r\nif (!test_and_set_bit(i, &vdev->vp_reset)) {\r\nvpath = &vdev->vpaths[i];\r\nvxge_vpath_intr_disable(vdev, i);\r\nnetif_tx_stop_queue(vpath->fifo.txq);\r\ncontinue;\r\n}\r\n}\r\n}\r\nring->stats.prev_rx_frms = rx_frms;\r\nring->last_status = status;\r\n}\r\nmod_timer(&vdev->vp_lockup_timer, jiffies + HZ / 1000);\r\n}\r\nstatic netdev_features_t vxge_fix_features(struct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nnetdev_features_t changed = dev->features ^ features;\r\nif ((changed & NETIF_F_RXHASH) && netif_running(dev))\r\nfeatures ^= NETIF_F_RXHASH;\r\nreturn features;\r\n}\r\nstatic int vxge_set_features(struct net_device *dev, netdev_features_t features)\r\n{\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nnetdev_features_t changed = dev->features ^ features;\r\nif (!(changed & NETIF_F_RXHASH))\r\nreturn 0;\r\nvdev->devh->config.rth_en = !!(features & NETIF_F_RXHASH);\r\nif (vxge_reset_all_vpaths(vdev) != VXGE_HW_OK) {\r\ndev->features = features ^ NETIF_F_RXHASH;\r\nvdev->devh->config.rth_en = !!(dev->features & NETIF_F_RXHASH);\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int vxge_open(struct net_device *dev)\r\n{\r\nenum vxge_hw_status status;\r\nstruct vxgedev *vdev;\r\nstruct __vxge_hw_device *hldev;\r\nstruct vxge_vpath *vpath;\r\nint ret = 0;\r\nint i;\r\nu64 val64, function_mode;\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d", dev->name, __func__, __LINE__);\r\nvdev = netdev_priv(dev);\r\nhldev = pci_get_drvdata(vdev->pdev);\r\nfunction_mode = vdev->config.device_hw_info.function_mode;\r\nnetif_carrier_off(dev);\r\nstatus = vxge_open_vpaths(vdev);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: fatal: Vpath open failed", vdev->ndev->name);\r\nret = -EPERM;\r\ngoto out0;\r\n}\r\nvdev->mtu = dev->mtu;\r\nstatus = vxge_add_isr(vdev);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: fatal: ISR add failed", dev->name);\r\nret = -EPERM;\r\ngoto out1;\r\n}\r\nif (vdev->config.intr_type != MSI_X) {\r\nnetif_napi_add(dev, &vdev->napi, vxge_poll_inta,\r\nvdev->config.napi_weight);\r\nnapi_enable(&vdev->napi);\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nvpath->ring.napi_p = &vdev->napi;\r\n}\r\n} else {\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nnetif_napi_add(dev, &vpath->ring.napi,\r\nvxge_poll_msix, vdev->config.napi_weight);\r\nnapi_enable(&vpath->ring.napi);\r\nvpath->ring.napi_p = &vpath->ring.napi;\r\n}\r\n}\r\nif (vdev->config.rth_steering) {\r\nstatus = vxge_rth_configure(vdev);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: fatal: RTH configuration failed",\r\ndev->name);\r\nret = -EPERM;\r\ngoto out2;\r\n}\r\n}\r\nprintk(KERN_INFO "%s: Receive Hashing Offload %s\n", dev->name,\r\nhldev->config.rth_en ? "enabled" : "disabled");\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nstatus = vxge_hw_vpath_mtu_set(vpath->handle, vdev->mtu);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: fatal: can not set new MTU", dev->name);\r\nret = -EPERM;\r\ngoto out2;\r\n}\r\n}\r\nVXGE_DEVICE_DEBUG_LEVEL_SET(VXGE_TRACE, VXGE_COMPONENT_LL, vdev);\r\nvxge_debug_init(vdev->level_trace,\r\n"%s: MTU is %d", vdev->ndev->name, vdev->mtu);\r\nVXGE_DEVICE_DEBUG_LEVEL_SET(VXGE_ERR, VXGE_COMPONENT_LL, vdev);\r\nif (vdev->all_multi_flg) {\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nvxge_restore_vpath_mac_addr(vpath);\r\nvxge_restore_vpath_vid_table(vpath);\r\nstatus = vxge_hw_vpath_mcast_enable(vpath->handle);\r\nif (status != VXGE_HW_OK)\r\nvxge_debug_init(VXGE_ERR,\r\n"%s:%d Enabling multicast failed",\r\n__func__, __LINE__);\r\n}\r\n}\r\nval64 = 0;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++)\r\nval64 |= VXGE_HW_RXMAC_AUTHORIZE_ALL_ADDR_VP(i);\r\nvxge_hw_mgmt_reg_write(vdev->devh,\r\nvxge_hw_mgmt_reg_type_mrpcim,\r\n0,\r\n(ulong)offsetof(struct vxge_hw_mrpcim_reg,\r\nrxmac_authorize_all_addr),\r\nval64);\r\nvxge_hw_mgmt_reg_write(vdev->devh,\r\nvxge_hw_mgmt_reg_type_mrpcim,\r\n0,\r\n(ulong)offsetof(struct vxge_hw_mrpcim_reg,\r\nrxmac_authorize_all_vid),\r\nval64);\r\nvxge_set_multicast(dev);\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nstatus = vxge_hw_vpath_bcast_enable(vpath->handle);\r\nif (status != VXGE_HW_OK)\r\nvxge_debug_init(VXGE_ERR,\r\n"%s : Can not enable bcast for vpath "\r\n"id %d", dev->name, i);\r\nif (vdev->config.addr_learn_en) {\r\nstatus = vxge_hw_vpath_mcast_enable(vpath->handle);\r\nif (status != VXGE_HW_OK)\r\nvxge_debug_init(VXGE_ERR,\r\n"%s : Can not enable mcast for vpath "\r\n"id %d", dev->name, i);\r\n}\r\n}\r\nvxge_hw_device_setpause_data(vdev->devh, 0,\r\nvdev->config.tx_pause_enable,\r\nvdev->config.rx_pause_enable);\r\nif (vdev->vp_reset_timer.function == NULL)\r\nvxge_os_timer(&vdev->vp_reset_timer, vxge_poll_vp_reset, vdev,\r\nHZ / 2);\r\nif (vdev->titan1 && vdev->vp_lockup_timer.function == NULL)\r\nvxge_os_timer(&vdev->vp_lockup_timer, vxge_poll_vp_lockup, vdev,\r\nHZ / 2);\r\nset_bit(__VXGE_STATE_CARD_UP, &vdev->state);\r\nsmp_wmb();\r\nif (vxge_hw_device_link_state_get(vdev->devh) == VXGE_HW_LINK_UP) {\r\nnetif_carrier_on(vdev->ndev);\r\nnetdev_notice(vdev->ndev, "Link Up\n");\r\nvdev->stats.link_up++;\r\n}\r\nvxge_hw_device_intr_enable(vdev->devh);\r\nsmp_wmb();\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nvpath = &vdev->vpaths[i];\r\nvxge_hw_vpath_enable(vpath->handle);\r\nsmp_wmb();\r\nvxge_hw_vpath_rx_doorbell_init(vpath->handle);\r\n}\r\nnetif_tx_start_all_queues(vdev->ndev);\r\nvxge_config_ci_for_tti_rti(vdev);\r\ngoto out0;\r\nout2:\r\nvxge_rem_isr(vdev);\r\nif (vdev->config.intr_type != MSI_X)\r\nnapi_disable(&vdev->napi);\r\nelse {\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nnapi_disable(&vdev->vpaths[i].ring.napi);\r\n}\r\nout1:\r\nvxge_close_vpaths(vdev, 0);\r\nout0:\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...",\r\ndev->name, __func__, __LINE__);\r\nreturn ret;\r\n}\r\nstatic void vxge_free_mac_add_list(struct vxge_vpath *vpath)\r\n{\r\nstruct list_head *entry, *next;\r\nif (list_empty(&vpath->mac_addr_list))\r\nreturn;\r\nlist_for_each_safe(entry, next, &vpath->mac_addr_list) {\r\nlist_del(entry);\r\nkfree((struct vxge_mac_addrs *)entry);\r\n}\r\n}\r\nstatic void vxge_napi_del_all(struct vxgedev *vdev)\r\n{\r\nint i;\r\nif (vdev->config.intr_type != MSI_X)\r\nnetif_napi_del(&vdev->napi);\r\nelse {\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nnetif_napi_del(&vdev->vpaths[i].ring.napi);\r\n}\r\n}\r\nstatic int do_vxge_close(struct net_device *dev, int do_io)\r\n{\r\nenum vxge_hw_status status;\r\nstruct vxgedev *vdev;\r\nstruct __vxge_hw_device *hldev;\r\nint i;\r\nu64 val64, vpath_vector;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d",\r\ndev->name, __func__, __LINE__);\r\nvdev = netdev_priv(dev);\r\nhldev = pci_get_drvdata(vdev->pdev);\r\nif (unlikely(!is_vxge_card_up(vdev)))\r\nreturn 0;\r\nwhile (test_and_set_bit(__VXGE_STATE_RESET_CARD, &vdev->state))\r\nmsleep(50);\r\nif (do_io) {\r\nvpath_vector = vxge_mBIT(vdev->vpaths[0].device_id);\r\nstatus = vxge_hw_mgmt_reg_read(vdev->devh,\r\nvxge_hw_mgmt_reg_type_mrpcim,\r\n0,\r\n(ulong)offsetof(\r\nstruct vxge_hw_mrpcim_reg,\r\nrts_mgr_cbasin_cfg),\r\n&val64);\r\nif (status == VXGE_HW_OK) {\r\nval64 &= ~vpath_vector;\r\nstatus = vxge_hw_mgmt_reg_write(vdev->devh,\r\nvxge_hw_mgmt_reg_type_mrpcim,\r\n0,\r\n(ulong)offsetof(\r\nstruct vxge_hw_mrpcim_reg,\r\nrts_mgr_cbasin_cfg),\r\nval64);\r\n}\r\nvxge_hw_mgmt_reg_write(vdev->devh,\r\nvxge_hw_mgmt_reg_type_mrpcim,\r\n0,\r\n(ulong)offsetof(struct vxge_hw_mrpcim_reg,\r\nrxmac_authorize_all_addr),\r\n0);\r\nvxge_hw_mgmt_reg_write(vdev->devh,\r\nvxge_hw_mgmt_reg_type_mrpcim,\r\n0,\r\n(ulong)offsetof(struct vxge_hw_mrpcim_reg,\r\nrxmac_authorize_all_vid),\r\n0);\r\nsmp_wmb();\r\n}\r\nif (vdev->titan1)\r\ndel_timer_sync(&vdev->vp_lockup_timer);\r\ndel_timer_sync(&vdev->vp_reset_timer);\r\nif (do_io)\r\nvxge_hw_device_wait_receive_idle(hldev);\r\nclear_bit(__VXGE_STATE_CARD_UP, &vdev->state);\r\nif (vdev->config.intr_type != MSI_X)\r\nnapi_disable(&vdev->napi);\r\nelse {\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nnapi_disable(&vdev->vpaths[i].ring.napi);\r\n}\r\nnetif_carrier_off(vdev->ndev);\r\nnetdev_notice(vdev->ndev, "Link Down\n");\r\nnetif_tx_stop_all_queues(vdev->ndev);\r\nif (do_io)\r\nvxge_hw_device_intr_disable(vdev->devh);\r\nvxge_rem_isr(vdev);\r\nvxge_napi_del_all(vdev);\r\nif (do_io)\r\nvxge_reset_all_vpaths(vdev);\r\nvxge_close_vpaths(vdev, 0);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s: %s:%d Exiting...", dev->name, __func__, __LINE__);\r\nclear_bit(__VXGE_STATE_RESET_CARD, &vdev->state);\r\nreturn 0;\r\n}\r\nstatic int vxge_close(struct net_device *dev)\r\n{\r\ndo_vxge_close(dev, 1);\r\nreturn 0;\r\n}\r\nstatic int vxge_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nvxge_debug_entryexit(vdev->level_trace,\r\n"%s:%d", __func__, __LINE__);\r\nif (unlikely(!is_vxge_card_up(vdev))) {\r\ndev->mtu = new_mtu;\r\nvxge_debug_init(vdev->level_err,\r\n"%s", "device is down on MTU change");\r\nreturn 0;\r\n}\r\nvxge_debug_init(vdev->level_trace,\r\n"trying to apply new MTU %d", new_mtu);\r\nif (vxge_close(dev))\r\nreturn -EIO;\r\ndev->mtu = new_mtu;\r\nvdev->mtu = new_mtu;\r\nif (vxge_open(dev))\r\nreturn -EIO;\r\nvxge_debug_init(vdev->level_trace,\r\n"%s: MTU changed to %d", vdev->ndev->name, new_mtu);\r\nvxge_debug_entryexit(vdev->level_trace,\r\n"%s:%d Exiting...", __func__, __LINE__);\r\nreturn 0;\r\n}\r\nstatic void\r\nvxge_get_stats64(struct net_device *dev, struct rtnl_link_stats64 *net_stats)\r\n{\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nint k;\r\nfor (k = 0; k < vdev->no_of_vpath; k++) {\r\nstruct vxge_ring_stats *rxstats = &vdev->vpaths[k].ring.stats;\r\nstruct vxge_fifo_stats *txstats = &vdev->vpaths[k].fifo.stats;\r\nunsigned int start;\r\nu64 packets, bytes, multicast;\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&rxstats->syncp);\r\npackets = rxstats->rx_frms;\r\nmulticast = rxstats->rx_mcast;\r\nbytes = rxstats->rx_bytes;\r\n} while (u64_stats_fetch_retry_irq(&rxstats->syncp, start));\r\nnet_stats->rx_packets += packets;\r\nnet_stats->rx_bytes += bytes;\r\nnet_stats->multicast += multicast;\r\nnet_stats->rx_errors += rxstats->rx_errors;\r\nnet_stats->rx_dropped += rxstats->rx_dropped;\r\ndo {\r\nstart = u64_stats_fetch_begin_irq(&txstats->syncp);\r\npackets = txstats->tx_frms;\r\nbytes = txstats->tx_bytes;\r\n} while (u64_stats_fetch_retry_irq(&txstats->syncp, start));\r\nnet_stats->tx_packets += packets;\r\nnet_stats->tx_bytes += bytes;\r\nnet_stats->tx_errors += txstats->tx_errors;\r\n}\r\n}\r\nstatic enum vxge_hw_status vxge_timestamp_config(struct __vxge_hw_device *devh)\r\n{\r\nenum vxge_hw_status status;\r\nu64 val64;\r\nval64 = VXGE_HW_XMAC_TIMESTAMP_EN |\r\nVXGE_HW_XMAC_TIMESTAMP_USE_LINK_ID(0) |\r\nVXGE_HW_XMAC_TIMESTAMP_INTERVAL(0);\r\nstatus = vxge_hw_mgmt_reg_write(devh,\r\nvxge_hw_mgmt_reg_type_mrpcim,\r\n0,\r\noffsetof(struct vxge_hw_mrpcim_reg,\r\nxmac_timestamp),\r\nval64);\r\nvxge_hw_device_flush_io(devh);\r\ndevh->config.hwts_en = VXGE_HW_HWTS_ENABLE;\r\nreturn status;\r\n}\r\nstatic int vxge_hwtstamp_set(struct vxgedev *vdev, void __user *data)\r\n{\r\nstruct hwtstamp_config config;\r\nint i;\r\nif (copy_from_user(&config, data, sizeof(config)))\r\nreturn -EFAULT;\r\nif (config.flags)\r\nreturn -EINVAL;\r\nswitch (config.tx_type) {\r\ncase HWTSTAMP_TX_OFF:\r\nbreak;\r\ncase HWTSTAMP_TX_ON:\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nswitch (config.rx_filter) {\r\ncase HWTSTAMP_FILTER_NONE:\r\nvdev->rx_hwts = 0;\r\nconfig.rx_filter = HWTSTAMP_FILTER_NONE;\r\nbreak;\r\ncase HWTSTAMP_FILTER_ALL:\r\ncase HWTSTAMP_FILTER_SOME:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:\r\ncase HWTSTAMP_FILTER_PTP_V2_EVENT:\r\ncase HWTSTAMP_FILTER_PTP_V2_SYNC:\r\ncase HWTSTAMP_FILTER_PTP_V2_DELAY_REQ:\r\nif (vdev->devh->config.hwts_en != VXGE_HW_HWTS_ENABLE)\r\nreturn -EFAULT;\r\nvdev->rx_hwts = 1;\r\nconfig.rx_filter = HWTSTAMP_FILTER_ALL;\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nvdev->vpaths[i].ring.rx_hwts = vdev->rx_hwts;\r\nif (copy_to_user(data, &config, sizeof(config)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int vxge_hwtstamp_get(struct vxgedev *vdev, void __user *data)\r\n{\r\nstruct hwtstamp_config config;\r\nconfig.flags = 0;\r\nconfig.tx_type = HWTSTAMP_TX_OFF;\r\nconfig.rx_filter = (vdev->rx_hwts ?\r\nHWTSTAMP_FILTER_ALL : HWTSTAMP_FILTER_NONE);\r\nif (copy_to_user(data, &config, sizeof(config)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int vxge_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nswitch (cmd) {\r\ncase SIOCSHWTSTAMP:\r\nreturn vxge_hwtstamp_set(vdev, rq->ifr_data);\r\ncase SIOCGHWTSTAMP:\r\nreturn vxge_hwtstamp_get(vdev, rq->ifr_data);\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void vxge_tx_watchdog(struct net_device *dev)\r\n{\r\nstruct vxgedev *vdev;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s:%d", __func__, __LINE__);\r\nvdev = netdev_priv(dev);\r\nvdev->cric_err_event = VXGE_HW_EVENT_RESET_START;\r\nschedule_work(&vdev->reset_task);\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d Exiting...", __func__, __LINE__);\r\n}\r\nstatic int\r\nvxge_vlan_rx_add_vid(struct net_device *dev, __be16 proto, u16 vid)\r\n{\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nstruct vxge_vpath *vpath;\r\nint vp_id;\r\nfor (vp_id = 0; vp_id < vdev->no_of_vpath; vp_id++) {\r\nvpath = &vdev->vpaths[vp_id];\r\nif (!vpath->is_open)\r\ncontinue;\r\nvxge_hw_vpath_vid_add(vpath->handle, vid);\r\n}\r\nset_bit(vid, vdev->active_vlans);\r\nreturn 0;\r\n}\r\nstatic int\r\nvxge_vlan_rx_kill_vid(struct net_device *dev, __be16 proto, u16 vid)\r\n{\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nstruct vxge_vpath *vpath;\r\nint vp_id;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s:%d", __func__, __LINE__);\r\nfor (vp_id = 0; vp_id < vdev->no_of_vpath; vp_id++) {\r\nvpath = &vdev->vpaths[vp_id];\r\nif (!vpath->is_open)\r\ncontinue;\r\nvxge_hw_vpath_vid_delete(vpath->handle, vid);\r\n}\r\nvxge_debug_entryexit(VXGE_TRACE,\r\n"%s:%d Exiting...", __func__, __LINE__);\r\nclear_bit(vid, vdev->active_vlans);\r\nreturn 0;\r\n}\r\nstatic int vxge_device_register(struct __vxge_hw_device *hldev,\r\nstruct vxge_config *config, int high_dma,\r\nint no_of_vpath, struct vxgedev **vdev_out)\r\n{\r\nstruct net_device *ndev;\r\nenum vxge_hw_status status = VXGE_HW_OK;\r\nstruct vxgedev *vdev;\r\nint ret = 0, no_of_queue = 1;\r\nu64 stat;\r\n*vdev_out = NULL;\r\nif (config->tx_steering_type)\r\nno_of_queue = no_of_vpath;\r\nndev = alloc_etherdev_mq(sizeof(struct vxgedev),\r\nno_of_queue);\r\nif (ndev == NULL) {\r\nvxge_debug_init(\r\nvxge_hw_device_trace_level_get(hldev),\r\n"%s : device allocation failed", __func__);\r\nret = -ENODEV;\r\ngoto _out0;\r\n}\r\nvxge_debug_entryexit(\r\nvxge_hw_device_trace_level_get(hldev),\r\n"%s: %s:%d Entering...",\r\nndev->name, __func__, __LINE__);\r\nvdev = netdev_priv(ndev);\r\nmemset(vdev, 0, sizeof(struct vxgedev));\r\nvdev->ndev = ndev;\r\nvdev->devh = hldev;\r\nvdev->pdev = hldev->pdev;\r\nmemcpy(&vdev->config, config, sizeof(struct vxge_config));\r\nvdev->rx_hwts = 0;\r\nvdev->titan1 = (vdev->pdev->revision == VXGE_HW_TITAN1_PCI_REVISION);\r\nSET_NETDEV_DEV(ndev, &vdev->pdev->dev);\r\nndev->hw_features = NETIF_F_RXCSUM | NETIF_F_SG |\r\nNETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\r\nNETIF_F_TSO | NETIF_F_TSO6 |\r\nNETIF_F_HW_VLAN_CTAG_TX;\r\nif (vdev->config.rth_steering != NO_STEERING)\r\nndev->hw_features |= NETIF_F_RXHASH;\r\nndev->features |= ndev->hw_features |\r\nNETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_CTAG_FILTER;\r\nndev->netdev_ops = &vxge_netdev_ops;\r\nndev->watchdog_timeo = VXGE_LL_WATCH_DOG_TIMEOUT;\r\nINIT_WORK(&vdev->reset_task, vxge_reset);\r\nvxge_initialize_ethtool_ops(ndev);\r\nvdev->vpaths = kzalloc((sizeof(struct vxge_vpath)) *\r\nno_of_vpath, GFP_KERNEL);\r\nif (!vdev->vpaths) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: vpath memory allocation failed",\r\nvdev->ndev->name);\r\nret = -ENOMEM;\r\ngoto _out1;\r\n}\r\nvxge_debug_init(vxge_hw_device_trace_level_get(hldev),\r\n"%s : checksumming enabled", __func__);\r\nif (high_dma) {\r\nndev->features |= NETIF_F_HIGHDMA;\r\nvxge_debug_init(vxge_hw_device_trace_level_get(hldev),\r\n"%s : using High DMA", __func__);\r\n}\r\nndev->min_mtu = VXGE_HW_MIN_MTU;\r\nndev->max_mtu = VXGE_HW_MAX_MTU;\r\nret = register_netdev(ndev);\r\nif (ret) {\r\nvxge_debug_init(vxge_hw_device_trace_level_get(hldev),\r\n"%s: %s : device registration failed!",\r\nndev->name, __func__);\r\ngoto _out2;\r\n}\r\nndev->addr_len = ETH_ALEN;\r\nnetif_carrier_off(ndev);\r\nvxge_debug_init(vxge_hw_device_trace_level_get(hldev),\r\n"%s: Ethernet device registered",\r\nndev->name);\r\nhldev->ndev = ndev;\r\n*vdev_out = vdev;\r\nstatus = vxge_hw_mrpcim_stats_access(\r\nhldev,\r\nVXGE_HW_STATS_OP_CLEAR_ALL_STATS,\r\n0,\r\n0,\r\n&stat);\r\nif (status == VXGE_HW_ERR_PRIVILAGED_OPEARATION)\r\nvxge_debug_init(\r\nvxge_hw_device_trace_level_get(hldev),\r\n"%s: device stats clear returns"\r\n"VXGE_HW_ERR_PRIVILAGED_OPEARATION", ndev->name);\r\nvxge_debug_entryexit(vxge_hw_device_trace_level_get(hldev),\r\n"%s: %s:%d Exiting...",\r\nndev->name, __func__, __LINE__);\r\nreturn ret;\r\n_out2:\r\nkfree(vdev->vpaths);\r\n_out1:\r\nfree_netdev(ndev);\r\n_out0:\r\nreturn ret;\r\n}\r\nstatic void vxge_device_unregister(struct __vxge_hw_device *hldev)\r\n{\r\nstruct vxgedev *vdev;\r\nstruct net_device *dev;\r\nchar buf[IFNAMSIZ];\r\ndev = hldev->ndev;\r\nvdev = netdev_priv(dev);\r\nvxge_debug_entryexit(vdev->level_trace, "%s: %s:%d", vdev->ndev->name,\r\n__func__, __LINE__);\r\nstrlcpy(buf, dev->name, IFNAMSIZ);\r\nflush_work(&vdev->reset_task);\r\nunregister_netdev(dev);\r\nkfree(vdev->vpaths);\r\nfree_netdev(dev);\r\nvxge_debug_init(vdev->level_trace, "%s: ethernet device unregistered",\r\nbuf);\r\nvxge_debug_entryexit(vdev->level_trace, "%s: %s:%d Exiting...", buf,\r\n__func__, __LINE__);\r\n}\r\nstatic void\r\nvxge_callback_crit_err(struct __vxge_hw_device *hldev,\r\nenum vxge_hw_event type, u64 vp_id)\r\n{\r\nstruct net_device *dev = hldev->ndev;\r\nstruct vxgedev *vdev = netdev_priv(dev);\r\nstruct vxge_vpath *vpath = NULL;\r\nint vpath_idx;\r\nvxge_debug_entryexit(vdev->level_trace,\r\n"%s: %s:%d", vdev->ndev->name, __func__, __LINE__);\r\nvdev->cric_err_event = type;\r\nfor (vpath_idx = 0; vpath_idx < vdev->no_of_vpath; vpath_idx++) {\r\nvpath = &vdev->vpaths[vpath_idx];\r\nif (vpath->device_id == vp_id)\r\nbreak;\r\n}\r\nif (!test_bit(__VXGE_STATE_RESET_CARD, &vdev->state)) {\r\nif (type == VXGE_HW_EVENT_SLOT_FREEZE) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Slot is frozen", vdev->ndev->name);\r\n} else if (type == VXGE_HW_EVENT_SERR) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Encountered Serious Error",\r\nvdev->ndev->name);\r\n} else if (type == VXGE_HW_EVENT_CRITICAL_ERR)\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Encountered Critical Error",\r\nvdev->ndev->name);\r\n}\r\nif ((type == VXGE_HW_EVENT_SERR) ||\r\n(type == VXGE_HW_EVENT_SLOT_FREEZE)) {\r\nif (unlikely(vdev->exec_mode))\r\nclear_bit(__VXGE_STATE_CARD_UP, &vdev->state);\r\n} else if (type == VXGE_HW_EVENT_CRITICAL_ERR) {\r\nvxge_hw_device_mask_all(hldev);\r\nif (unlikely(vdev->exec_mode))\r\nclear_bit(__VXGE_STATE_CARD_UP, &vdev->state);\r\n} else if ((type == VXGE_HW_EVENT_FIFO_ERR) ||\r\n(type == VXGE_HW_EVENT_VPATH_ERR)) {\r\nif (unlikely(vdev->exec_mode))\r\nclear_bit(__VXGE_STATE_CARD_UP, &vdev->state);\r\nelse {\r\nif (!test_and_set_bit(vpath_idx, &vdev->vp_reset)) {\r\nvxge_vpath_intr_disable(vdev, vpath_idx);\r\nnetif_tx_stop_queue(vpath->fifo.txq);\r\n}\r\n}\r\n}\r\nvxge_debug_entryexit(vdev->level_trace,\r\n"%s: %s:%d Exiting...",\r\nvdev->ndev->name, __func__, __LINE__);\r\n}\r\nstatic void verify_bandwidth(void)\r\n{\r\nint i, band_width, total = 0, equal_priority = 0;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (bw_percentage[i] == 0) {\r\nequal_priority = 1;\r\nbreak;\r\n}\r\n}\r\nif (!equal_priority) {\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (bw_percentage[i] == 0xFF)\r\nbreak;\r\ntotal += bw_percentage[i];\r\nif (total > VXGE_HW_VPATH_BANDWIDTH_MAX) {\r\nequal_priority = 1;\r\nbreak;\r\n}\r\n}\r\n}\r\nif (!equal_priority) {\r\nif (total < VXGE_HW_VPATH_BANDWIDTH_MAX) {\r\nif (i < VXGE_HW_MAX_VIRTUAL_PATHS) {\r\nband_width =\r\n(VXGE_HW_VPATH_BANDWIDTH_MAX - total) /\r\n(VXGE_HW_MAX_VIRTUAL_PATHS - i);\r\nif (band_width < 2)\r\nequal_priority = 1;\r\nelse {\r\nfor (; i < VXGE_HW_MAX_VIRTUAL_PATHS;\r\ni++)\r\nbw_percentage[i] =\r\nband_width;\r\n}\r\n}\r\n} else if (i < VXGE_HW_MAX_VIRTUAL_PATHS)\r\nequal_priority = 1;\r\n}\r\nif (equal_priority) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Assigning equal bandwidth to all the vpaths",\r\nVXGE_DRIVER_NAME);\r\nbw_percentage[0] = VXGE_HW_VPATH_BANDWIDTH_MAX /\r\nVXGE_HW_MAX_VIRTUAL_PATHS;\r\nfor (i = 1; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++)\r\nbw_percentage[i] = bw_percentage[0];\r\n}\r\n}\r\nstatic int vxge_config_vpaths(struct vxge_hw_device_config *device_config,\r\nu64 vpath_mask, struct vxge_config *config_param)\r\n{\r\nint i, no_of_vpaths = 0, default_no_vpath = 0, temp;\r\nu32 txdl_size, txdl_per_memblock;\r\ntemp = driver_config->vpath_per_dev;\r\nif ((driver_config->vpath_per_dev == VXGE_USE_DEFAULT) &&\r\n(max_config_dev == VXGE_MAX_CONFIG_DEV)) {\r\nif (driver_config->g_no_cpus == -1)\r\nreturn 0;\r\nif (!driver_config->g_no_cpus)\r\ndriver_config->g_no_cpus =\r\nnetif_get_num_default_rss_queues();\r\ndriver_config->vpath_per_dev = driver_config->g_no_cpus >> 1;\r\nif (!driver_config->vpath_per_dev)\r\ndriver_config->vpath_per_dev = 1;\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++)\r\nif (!vxge_bVALn(vpath_mask, i, 1))\r\ncontinue;\r\nelse\r\ndefault_no_vpath++;\r\nif (default_no_vpath < driver_config->vpath_per_dev)\r\ndriver_config->vpath_per_dev = default_no_vpath;\r\ndriver_config->g_no_cpus = driver_config->g_no_cpus -\r\n(driver_config->vpath_per_dev * 2);\r\nif (driver_config->g_no_cpus <= 0)\r\ndriver_config->g_no_cpus = -1;\r\n}\r\nif (driver_config->vpath_per_dev == 1) {\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s: Disable tx and rx steering, "\r\n"as single vpath is configured", VXGE_DRIVER_NAME);\r\nconfig_param->rth_steering = NO_STEERING;\r\nconfig_param->tx_steering_type = NO_STEERING;\r\ndevice_config->rth_en = 0;\r\n}\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++)\r\ndevice_config->vp_config[i].min_bandwidth = bw_percentage[i];\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\ndevice_config->vp_config[i].vp_id = i;\r\ndevice_config->vp_config[i].mtu = VXGE_HW_DEFAULT_MTU;\r\nif (no_of_vpaths < driver_config->vpath_per_dev) {\r\nif (!vxge_bVALn(vpath_mask, i, 1)) {\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s: vpath: %d is not available",\r\nVXGE_DRIVER_NAME, i);\r\ncontinue;\r\n} else {\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s: vpath: %d available",\r\nVXGE_DRIVER_NAME, i);\r\nno_of_vpaths++;\r\n}\r\n} else {\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s: vpath: %d is not configured, "\r\n"max_config_vpath exceeded",\r\nVXGE_DRIVER_NAME, i);\r\nbreak;\r\n}\r\ndevice_config->vp_config[i].fifo.enable =\r\nVXGE_HW_FIFO_ENABLE;\r\ndevice_config->vp_config[i].fifo.max_frags =\r\nMAX_SKB_FRAGS + 1;\r\ndevice_config->vp_config[i].fifo.memblock_size =\r\nVXGE_HW_MIN_FIFO_MEMBLOCK_SIZE;\r\ntxdl_size = device_config->vp_config[i].fifo.max_frags *\r\nsizeof(struct vxge_hw_fifo_txd);\r\ntxdl_per_memblock = VXGE_HW_MIN_FIFO_MEMBLOCK_SIZE / txdl_size;\r\ndevice_config->vp_config[i].fifo.fifo_blocks =\r\n((VXGE_DEF_FIFO_LENGTH - 1) / txdl_per_memblock) + 1;\r\ndevice_config->vp_config[i].fifo.intr =\r\nVXGE_HW_FIFO_QUEUE_INTR_DISABLE;\r\ndevice_config->vp_config[i].tti.intr_enable =\r\nVXGE_HW_TIM_INTR_ENABLE;\r\ndevice_config->vp_config[i].tti.btimer_val =\r\n(VXGE_TTI_BTIMER_VAL * 1000) / 272;\r\ndevice_config->vp_config[i].tti.timer_ac_en =\r\nVXGE_HW_TIM_TIMER_AC_ENABLE;\r\ndevice_config->vp_config[i].tti.timer_ci_en =\r\nVXGE_HW_TIM_TIMER_CI_DISABLE;\r\ndevice_config->vp_config[i].tti.timer_ri_en =\r\nVXGE_HW_TIM_TIMER_RI_DISABLE;\r\ndevice_config->vp_config[i].tti.util_sel =\r\nVXGE_HW_TIM_UTIL_SEL_LEGACY_TX_NET_UTIL;\r\ndevice_config->vp_config[i].tti.ltimer_val =\r\n(VXGE_TTI_LTIMER_VAL * 1000) / 272;\r\ndevice_config->vp_config[i].tti.rtimer_val =\r\n(VXGE_TTI_RTIMER_VAL * 1000) / 272;\r\ndevice_config->vp_config[i].tti.urange_a = TTI_TX_URANGE_A;\r\ndevice_config->vp_config[i].tti.urange_b = TTI_TX_URANGE_B;\r\ndevice_config->vp_config[i].tti.urange_c = TTI_TX_URANGE_C;\r\ndevice_config->vp_config[i].tti.uec_a = TTI_TX_UFC_A;\r\ndevice_config->vp_config[i].tti.uec_b = TTI_TX_UFC_B;\r\ndevice_config->vp_config[i].tti.uec_c = TTI_TX_UFC_C;\r\ndevice_config->vp_config[i].tti.uec_d = TTI_TX_UFC_D;\r\ndevice_config->vp_config[i].ring.enable =\r\nVXGE_HW_RING_ENABLE;\r\ndevice_config->vp_config[i].ring.ring_blocks =\r\nVXGE_HW_DEF_RING_BLOCKS;\r\ndevice_config->vp_config[i].ring.buffer_mode =\r\nVXGE_HW_RING_RXD_BUFFER_MODE_1;\r\ndevice_config->vp_config[i].ring.rxds_limit =\r\nVXGE_HW_DEF_RING_RXDS_LIMIT;\r\ndevice_config->vp_config[i].ring.scatter_mode =\r\nVXGE_HW_RING_SCATTER_MODE_A;\r\ndevice_config->vp_config[i].rti.intr_enable =\r\nVXGE_HW_TIM_INTR_ENABLE;\r\ndevice_config->vp_config[i].rti.btimer_val =\r\n(VXGE_RTI_BTIMER_VAL * 1000)/272;\r\ndevice_config->vp_config[i].rti.timer_ac_en =\r\nVXGE_HW_TIM_TIMER_AC_ENABLE;\r\ndevice_config->vp_config[i].rti.timer_ci_en =\r\nVXGE_HW_TIM_TIMER_CI_DISABLE;\r\ndevice_config->vp_config[i].rti.timer_ri_en =\r\nVXGE_HW_TIM_TIMER_RI_DISABLE;\r\ndevice_config->vp_config[i].rti.util_sel =\r\nVXGE_HW_TIM_UTIL_SEL_LEGACY_RX_NET_UTIL;\r\ndevice_config->vp_config[i].rti.urange_a =\r\nRTI_RX_URANGE_A;\r\ndevice_config->vp_config[i].rti.urange_b =\r\nRTI_RX_URANGE_B;\r\ndevice_config->vp_config[i].rti.urange_c =\r\nRTI_RX_URANGE_C;\r\ndevice_config->vp_config[i].rti.uec_a = RTI_RX_UFC_A;\r\ndevice_config->vp_config[i].rti.uec_b = RTI_RX_UFC_B;\r\ndevice_config->vp_config[i].rti.uec_c = RTI_RX_UFC_C;\r\ndevice_config->vp_config[i].rti.uec_d = RTI_RX_UFC_D;\r\ndevice_config->vp_config[i].rti.rtimer_val =\r\n(VXGE_RTI_RTIMER_VAL * 1000) / 272;\r\ndevice_config->vp_config[i].rti.ltimer_val =\r\n(VXGE_RTI_LTIMER_VAL * 1000) / 272;\r\ndevice_config->vp_config[i].rpa_strip_vlan_tag =\r\nvlan_tag_strip;\r\n}\r\ndriver_config->vpath_per_dev = temp;\r\nreturn no_of_vpaths;\r\n}\r\nstatic void vxge_device_config_init(struct vxge_hw_device_config *device_config,\r\nint *intr_type)\r\n{\r\ndevice_config->dma_blockpool_initial =\r\nVXGE_HW_INITIAL_DMA_BLOCK_POOL_SIZE;\r\ndevice_config->dma_blockpool_max =\r\nVXGE_HW_MAX_DMA_BLOCK_POOL_SIZE;\r\nif (max_mac_vpath > VXGE_MAX_MAC_ADDR_COUNT)\r\nmax_mac_vpath = VXGE_MAX_MAC_ADDR_COUNT;\r\nif (!IS_ENABLED(CONFIG_PCI_MSI)) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: This Kernel does not support "\r\n"MSI-X. Defaulting to INTA", VXGE_DRIVER_NAME);\r\n*intr_type = INTA;\r\n}\r\nswitch (*intr_type) {\r\ncase INTA:\r\ndevice_config->intr_mode = VXGE_HW_INTR_MODE_IRQLINE;\r\nbreak;\r\ncase MSI_X:\r\ndevice_config->intr_mode = VXGE_HW_INTR_MODE_MSIX_ONE_SHOT;\r\nbreak;\r\n}\r\ndevice_config->device_poll_millis = VXGE_TIMER_DELAY;\r\ndevice_config->rts_mac_en = addr_learn_en;\r\ndevice_config->rth_it_type = VXGE_HW_RTH_IT_TYPE_MULTI_IT;\r\nvxge_debug_ll_config(VXGE_TRACE, "%s : Device Config Params ",\r\n__func__);\r\nvxge_debug_ll_config(VXGE_TRACE, "intr_mode : %d",\r\ndevice_config->intr_mode);\r\nvxge_debug_ll_config(VXGE_TRACE, "device_poll_millis : %d",\r\ndevice_config->device_poll_millis);\r\nvxge_debug_ll_config(VXGE_TRACE, "rth_en : %d",\r\ndevice_config->rth_en);\r\nvxge_debug_ll_config(VXGE_TRACE, "rth_it_type : %d",\r\ndevice_config->rth_it_type);\r\n}\r\nstatic void vxge_print_parm(struct vxgedev *vdev, u64 vpath_mask)\r\n{\r\nint i;\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: %d Vpath(s) opened",\r\nvdev->ndev->name, vdev->no_of_vpath);\r\nswitch (vdev->config.intr_type) {\r\ncase INTA:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Interrupt type INTA", vdev->ndev->name);\r\nbreak;\r\ncase MSI_X:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Interrupt type MSI-X", vdev->ndev->name);\r\nbreak;\r\n}\r\nif (vdev->config.rth_steering) {\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: RTH steering enabled for TCP_IPV4",\r\nvdev->ndev->name);\r\n} else {\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: RTH steering disabled", vdev->ndev->name);\r\n}\r\nswitch (vdev->config.tx_steering_type) {\r\ncase NO_STEERING:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Tx steering disabled", vdev->ndev->name);\r\nbreak;\r\ncase TX_PRIORITY_STEERING:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Unsupported tx steering option",\r\nvdev->ndev->name);\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Tx steering disabled", vdev->ndev->name);\r\nvdev->config.tx_steering_type = 0;\r\nbreak;\r\ncase TX_VLAN_STEERING:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Unsupported tx steering option",\r\nvdev->ndev->name);\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Tx steering disabled", vdev->ndev->name);\r\nvdev->config.tx_steering_type = 0;\r\nbreak;\r\ncase TX_MULTIQ_STEERING:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Tx multiqueue steering enabled",\r\nvdev->ndev->name);\r\nbreak;\r\ncase TX_PORT_STEERING:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Tx port steering enabled",\r\nvdev->ndev->name);\r\nbreak;\r\ndefault:\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Unsupported tx steering type",\r\nvdev->ndev->name);\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Tx steering disabled", vdev->ndev->name);\r\nvdev->config.tx_steering_type = 0;\r\n}\r\nif (vdev->config.addr_learn_en)\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: MAC Address learning enabled", vdev->ndev->name);\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!vxge_bVALn(vpath_mask, i, 1))\r\ncontinue;\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s: MTU size - %d", vdev->ndev->name,\r\n((vdev->devh))->\r\nconfig.vp_config[i].mtu);\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: VLAN tag stripping %s", vdev->ndev->name,\r\n((vdev->devh))->\r\nconfig.vp_config[i].rpa_strip_vlan_tag\r\n? "Enabled" : "Disabled");\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s: Max frags : %d", vdev->ndev->name,\r\n((vdev->devh))->\r\nconfig.vp_config[i].fifo.max_frags);\r\nbreak;\r\n}\r\n}\r\nstatic int vxge_pm_suspend(struct pci_dev *pdev, pm_message_t state)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic int vxge_pm_resume(struct pci_dev *pdev)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic pci_ers_result_t vxge_io_error_detected(struct pci_dev *pdev,\r\npci_channel_state_t state)\r\n{\r\nstruct __vxge_hw_device *hldev = pci_get_drvdata(pdev);\r\nstruct net_device *netdev = hldev->ndev;\r\nnetif_device_detach(netdev);\r\nif (state == pci_channel_io_perm_failure)\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\nif (netif_running(netdev)) {\r\ndo_vxge_close(netdev, 0);\r\n}\r\npci_disable_device(pdev);\r\nreturn PCI_ERS_RESULT_NEED_RESET;\r\n}\r\nstatic pci_ers_result_t vxge_io_slot_reset(struct pci_dev *pdev)\r\n{\r\nstruct __vxge_hw_device *hldev = pci_get_drvdata(pdev);\r\nstruct net_device *netdev = hldev->ndev;\r\nstruct vxgedev *vdev = netdev_priv(netdev);\r\nif (pci_enable_device(pdev)) {\r\nnetdev_err(netdev, "Cannot re-enable device after reset\n");\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\n}\r\npci_set_master(pdev);\r\ndo_vxge_reset(vdev, VXGE_LL_FULL_RESET);\r\nreturn PCI_ERS_RESULT_RECOVERED;\r\n}\r\nstatic void vxge_io_resume(struct pci_dev *pdev)\r\n{\r\nstruct __vxge_hw_device *hldev = pci_get_drvdata(pdev);\r\nstruct net_device *netdev = hldev->ndev;\r\nif (netif_running(netdev)) {\r\nif (vxge_open(netdev)) {\r\nnetdev_err(netdev,\r\n"Can't bring device back up after reset\n");\r\nreturn;\r\n}\r\n}\r\nnetif_device_attach(netdev);\r\n}\r\nstatic inline u32 vxge_get_num_vfs(u64 function_mode)\r\n{\r\nu32 num_functions = 0;\r\nswitch (function_mode) {\r\ncase VXGE_HW_FUNCTION_MODE_MULTI_FUNCTION:\r\ncase VXGE_HW_FUNCTION_MODE_SRIOV_8:\r\nnum_functions = 8;\r\nbreak;\r\ncase VXGE_HW_FUNCTION_MODE_SINGLE_FUNCTION:\r\nnum_functions = 1;\r\nbreak;\r\ncase VXGE_HW_FUNCTION_MODE_SRIOV:\r\ncase VXGE_HW_FUNCTION_MODE_MRIOV:\r\ncase VXGE_HW_FUNCTION_MODE_MULTI_FUNCTION_17:\r\nnum_functions = 17;\r\nbreak;\r\ncase VXGE_HW_FUNCTION_MODE_SRIOV_4:\r\nnum_functions = 4;\r\nbreak;\r\ncase VXGE_HW_FUNCTION_MODE_MULTI_FUNCTION_2:\r\nnum_functions = 2;\r\nbreak;\r\ncase VXGE_HW_FUNCTION_MODE_MRIOV_8:\r\nnum_functions = 8;\r\nbreak;\r\n}\r\nreturn num_functions;\r\n}\r\nint vxge_fw_upgrade(struct vxgedev *vdev, char *fw_name, int override)\r\n{\r\nstruct __vxge_hw_device *hldev = vdev->devh;\r\nu32 maj, min, bld, cmaj, cmin, cbld;\r\nenum vxge_hw_status status;\r\nconst struct firmware *fw;\r\nint ret;\r\nret = request_firmware(&fw, fw_name, &vdev->pdev->dev);\r\nif (ret) {\r\nvxge_debug_init(VXGE_ERR, "%s: Firmware file '%s' not found",\r\nVXGE_DRIVER_NAME, fw_name);\r\ngoto out;\r\n}\r\nstatus = vxge_update_fw_image(hldev, fw->data, fw->size);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: FW image download to adapter failed '%s'.",\r\nVXGE_DRIVER_NAME, fw_name);\r\nret = -EIO;\r\ngoto out;\r\n}\r\nstatus = vxge_hw_upgrade_read_version(hldev, &maj, &min, &bld);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Upgrade read version failed '%s'.",\r\nVXGE_DRIVER_NAME, fw_name);\r\nret = -EIO;\r\ngoto out;\r\n}\r\ncmaj = vdev->config.device_hw_info.fw_version.major;\r\ncmin = vdev->config.device_hw_info.fw_version.minor;\r\ncbld = vdev->config.device_hw_info.fw_version.build;\r\nif (VXGE_FW_VER(maj, min, bld) == VXGE_FW_VER(cmaj, cmin, cbld) &&\r\n!override) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nprintk(KERN_NOTICE "Upgrade to firmware version %d.%d.%d commencing\n",\r\nmaj, min, bld);\r\nstatus = vxge_hw_flash_fw(hldev);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR, "%s: Upgrade commit failed '%s'.",\r\nVXGE_DRIVER_NAME, fw_name);\r\nret = -EIO;\r\ngoto out;\r\n}\r\nprintk(KERN_NOTICE "Upgrade of firmware successful! Adapter must be "\r\n"hard reset before using, thus requiring a system reboot or a "\r\n"hotplug event.\n");\r\nout:\r\nrelease_firmware(fw);\r\nreturn ret;\r\n}\r\nstatic int vxge_probe_fw_update(struct vxgedev *vdev)\r\n{\r\nu32 maj, min, bld;\r\nint ret, gpxe = 0;\r\nchar *fw_name;\r\nmaj = vdev->config.device_hw_info.fw_version.major;\r\nmin = vdev->config.device_hw_info.fw_version.minor;\r\nbld = vdev->config.device_hw_info.fw_version.build;\r\nif (VXGE_FW_VER(maj, min, bld) == VXGE_CERT_FW_VER)\r\nreturn 0;\r\nif (VXGE_FW_VER(maj, min, 0) > VXGE_CERT_FW_VER) {\r\nvxge_debug_init(VXGE_ERR, "%s: Firmware newer than last known "\r\n"version, unable to load driver\n",\r\nVXGE_DRIVER_NAME);\r\nreturn -EINVAL;\r\n}\r\nif (VXGE_FW_VER(maj, min, bld) <= VXGE_FW_DEAD_VER) {\r\nvxge_debug_init(VXGE_ERR, "%s: Firmware %d.%d.%d cannot be "\r\n"upgraded\n", VXGE_DRIVER_NAME, maj, min, bld);\r\nreturn -EINVAL;\r\n}\r\nif (VXGE_FW_VER(maj, min, bld) >= VXGE_EPROM_FW_VER) {\r\nint i;\r\nfor (i = 0; i < VXGE_HW_MAX_ROM_IMAGES; i++)\r\nif (vdev->devh->eprom_versions[i]) {\r\ngpxe = 1;\r\nbreak;\r\n}\r\n}\r\nif (gpxe)\r\nfw_name = "vxge/X3fw-pxe.ncf";\r\nelse\r\nfw_name = "vxge/X3fw.ncf";\r\nret = vxge_fw_upgrade(vdev, fw_name, 0);\r\nif (ret != -EINVAL && ret != -ENOENT)\r\nreturn -EIO;\r\nelse\r\nret = 0;\r\nif (VXGE_FW_VER(VXGE_CERT_FW_VER_MAJOR, VXGE_CERT_FW_VER_MINOR, 0) >\r\nVXGE_FW_VER(maj, min, 0)) {\r\nvxge_debug_init(VXGE_ERR, "%s: Firmware %d.%d.%d is too old to"\r\n" be used with this driver.",\r\nVXGE_DRIVER_NAME, maj, min, bld);\r\nreturn -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nstatic int is_sriov_initialized(struct pci_dev *pdev)\r\n{\r\nint pos;\r\nu16 ctrl;\r\npos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_SRIOV);\r\nif (pos) {\r\npci_read_config_word(pdev, pos + PCI_SRIOV_CTRL, &ctrl);\r\nif (ctrl & PCI_SRIOV_CTRL_VFE)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nvxge_probe(struct pci_dev *pdev, const struct pci_device_id *pre)\r\n{\r\nstruct __vxge_hw_device *hldev;\r\nenum vxge_hw_status status;\r\nint ret;\r\nint high_dma = 0;\r\nu64 vpath_mask = 0;\r\nstruct vxgedev *vdev;\r\nstruct vxge_config *ll_config = NULL;\r\nstruct vxge_hw_device_config *device_config = NULL;\r\nstruct vxge_hw_device_attr attr;\r\nint i, j, no_of_vpath = 0, max_vpath_supported = 0;\r\nu8 *macaddr;\r\nstruct vxge_mac_addrs *entry;\r\nstatic int bus = -1, device = -1;\r\nu32 host_type;\r\nu8 new_device = 0;\r\nenum vxge_hw_status is_privileged;\r\nu32 function_mode;\r\nu32 num_vfs = 0;\r\nvxge_debug_entryexit(VXGE_TRACE, "%s:%d", __func__, __LINE__);\r\nattr.pdev = pdev;\r\nif (((bus != pdev->bus->number) || (device != PCI_SLOT(pdev->devfn))) &&\r\n!pdev->is_virtfn)\r\nnew_device = 1;\r\nbus = pdev->bus->number;\r\ndevice = PCI_SLOT(pdev->devfn);\r\nif (new_device) {\r\nif (driver_config->config_dev_cnt &&\r\n(driver_config->config_dev_cnt !=\r\ndriver_config->total_dev_cnt))\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Configured %d of %d devices",\r\nVXGE_DRIVER_NAME,\r\ndriver_config->config_dev_cnt,\r\ndriver_config->total_dev_cnt);\r\ndriver_config->config_dev_cnt = 0;\r\ndriver_config->total_dev_cnt = 0;\r\n}\r\ndriver_config->g_no_cpus = 0;\r\ndriver_config->vpath_per_dev = max_config_vpath;\r\ndriver_config->total_dev_cnt++;\r\nif (++driver_config->config_dev_cnt > max_config_dev) {\r\nret = 0;\r\ngoto _exit0;\r\n}\r\ndevice_config = kzalloc(sizeof(struct vxge_hw_device_config),\r\nGFP_KERNEL);\r\nif (!device_config) {\r\nret = -ENOMEM;\r\nvxge_debug_init(VXGE_ERR,\r\n"device_config : malloc failed %s %d",\r\n__FILE__, __LINE__);\r\ngoto _exit0;\r\n}\r\nll_config = kzalloc(sizeof(struct vxge_config), GFP_KERNEL);\r\nif (!ll_config) {\r\nret = -ENOMEM;\r\nvxge_debug_init(VXGE_ERR,\r\n"device_config : malloc failed %s %d",\r\n__FILE__, __LINE__);\r\ngoto _exit0;\r\n}\r\nll_config->tx_steering_type = TX_MULTIQ_STEERING;\r\nll_config->intr_type = MSI_X;\r\nll_config->napi_weight = NEW_NAPI_WEIGHT;\r\nll_config->rth_steering = RTH_STEERING;\r\nvxge_hw_device_config_default_get(device_config);\r\nvxge_device_config_init(device_config, &ll_config->intr_type);\r\nret = pci_enable_device(pdev);\r\nif (ret) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s : can not enable PCI device", __func__);\r\ngoto _exit0;\r\n}\r\nif (!pci_set_dma_mask(pdev, DMA_BIT_MASK(64))) {\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s : using 64bit DMA", __func__);\r\nhigh_dma = 1;\r\nif (pci_set_consistent_dma_mask(pdev,\r\nDMA_BIT_MASK(64))) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s : unable to obtain 64bit DMA for "\r\n"consistent allocations", __func__);\r\nret = -ENOMEM;\r\ngoto _exit1;\r\n}\r\n} else if (!pci_set_dma_mask(pdev, DMA_BIT_MASK(32))) {\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s : using 32bit DMA", __func__);\r\n} else {\r\nret = -ENOMEM;\r\ngoto _exit1;\r\n}\r\nret = pci_request_region(pdev, 0, VXGE_DRIVER_NAME);\r\nif (ret) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s : request regions failed", __func__);\r\ngoto _exit1;\r\n}\r\npci_set_master(pdev);\r\nattr.bar0 = pci_ioremap_bar(pdev, 0);\r\nif (!attr.bar0) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s : cannot remap io memory bar0", __func__);\r\nret = -ENODEV;\r\ngoto _exit2;\r\n}\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"pci ioremap bar0: %p:0x%llx",\r\nattr.bar0,\r\n(unsigned long long)pci_resource_start(pdev, 0));\r\nstatus = vxge_hw_device_hw_info_get(attr.bar0,\r\n&ll_config->device_hw_info);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Reading of hardware info failed."\r\n"Please try upgrading the firmware.", VXGE_DRIVER_NAME);\r\nret = -EINVAL;\r\ngoto _exit3;\r\n}\r\nvpath_mask = ll_config->device_hw_info.vpath_mask;\r\nif (vpath_mask == 0) {\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s: No vpaths available in device", VXGE_DRIVER_NAME);\r\nret = -EINVAL;\r\ngoto _exit3;\r\n}\r\nvxge_debug_ll_config(VXGE_TRACE,\r\n"%s:%d Vpath mask = %llx", __func__, __LINE__,\r\n(unsigned long long)vpath_mask);\r\nfunction_mode = ll_config->device_hw_info.function_mode;\r\nhost_type = ll_config->device_hw_info.host_type;\r\nis_privileged = __vxge_hw_device_is_privilaged(host_type,\r\nll_config->device_hw_info.func_id);\r\nfor (i = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!((vpath_mask) & vxge_mBIT(i)))\r\ncontinue;\r\nmax_vpath_supported++;\r\n}\r\nif (new_device)\r\nnum_vfs = vxge_get_num_vfs(function_mode) - 1;\r\nif (is_sriov(function_mode) && !is_sriov_initialized(pdev) &&\r\n(ll_config->intr_type != INTA)) {\r\nret = pci_enable_sriov(pdev, num_vfs);\r\nif (ret)\r\nvxge_debug_ll_config(VXGE_ERR,\r\n"Failed in enabling SRIOV mode: %d\n", ret);\r\n}\r\nno_of_vpath = vxge_config_vpaths(device_config, vpath_mask, ll_config);\r\nif (!no_of_vpath) {\r\nvxge_debug_ll_config(VXGE_ERR,\r\n"%s: No more vpaths to configure", VXGE_DRIVER_NAME);\r\nret = 0;\r\ngoto _exit3;\r\n}\r\nattr.uld_callbacks = &vxge_callbacks;\r\nstatus = vxge_hw_device_initialize(&hldev, &attr, device_config);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR,\r\n"Failed to initialize device (%d)", status);\r\nret = -EINVAL;\r\ngoto _exit3;\r\n}\r\nif (VXGE_FW_VER(ll_config->device_hw_info.fw_version.major,\r\nll_config->device_hw_info.fw_version.minor,\r\nll_config->device_hw_info.fw_version.build) >=\r\nVXGE_EPROM_FW_VER) {\r\nstruct eprom_image img[VXGE_HW_MAX_ROM_IMAGES];\r\nstatus = vxge_hw_vpath_eprom_img_ver_get(hldev, img);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR, "%s: Reading of EPROM failed",\r\nVXGE_DRIVER_NAME);\r\n}\r\nfor (i = 0; i < VXGE_HW_MAX_ROM_IMAGES; i++) {\r\nhldev->eprom_versions[i] = img[i].version;\r\nif (!img[i].is_valid)\r\nbreak;\r\nvxge_debug_init(VXGE_TRACE, "%s: EPROM %d, version "\r\n"%d.%d.%d.%d", VXGE_DRIVER_NAME, i,\r\nVXGE_EPROM_IMG_MAJOR(img[i].version),\r\nVXGE_EPROM_IMG_MINOR(img[i].version),\r\nVXGE_EPROM_IMG_FIX(img[i].version),\r\nVXGE_EPROM_IMG_BUILD(img[i].version));\r\n}\r\n}\r\nstatus = vxge_hw_vpath_strip_fcs_check(hldev, vpath_mask);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR, "%s: FCS stripping is enabled in MAC"\r\n" failing driver load", VXGE_DRIVER_NAME);\r\nret = -EINVAL;\r\ngoto _exit4;\r\n}\r\nif (is_privileged == VXGE_HW_OK) {\r\nstatus = vxge_timestamp_config(hldev);\r\nif (status != VXGE_HW_OK) {\r\nvxge_debug_init(VXGE_ERR, "%s: HWTS enable failed",\r\nVXGE_DRIVER_NAME);\r\nret = -EFAULT;\r\ngoto _exit4;\r\n}\r\n}\r\nvxge_hw_device_debug_set(hldev, VXGE_ERR, VXGE_COMPONENT_LL);\r\npci_set_drvdata(pdev, hldev);\r\nll_config->fifo_indicate_max_pkts = VXGE_FIFO_INDICATE_MAX_PKTS;\r\nll_config->addr_learn_en = addr_learn_en;\r\nll_config->rth_algorithm = RTH_ALG_JENKINS;\r\nll_config->rth_hash_type_tcpipv4 = 1;\r\nll_config->rth_hash_type_ipv4 = 0;\r\nll_config->rth_hash_type_tcpipv6 = 0;\r\nll_config->rth_hash_type_ipv6 = 0;\r\nll_config->rth_hash_type_tcpipv6ex = 0;\r\nll_config->rth_hash_type_ipv6ex = 0;\r\nll_config->rth_bkt_sz = RTH_BUCKET_SIZE;\r\nll_config->tx_pause_enable = VXGE_PAUSE_CTRL_ENABLE;\r\nll_config->rx_pause_enable = VXGE_PAUSE_CTRL_ENABLE;\r\nret = vxge_device_register(hldev, ll_config, high_dma, no_of_vpath,\r\n&vdev);\r\nif (ret) {\r\nret = -EINVAL;\r\ngoto _exit4;\r\n}\r\nret = vxge_probe_fw_update(vdev);\r\nif (ret)\r\ngoto _exit5;\r\nvxge_hw_device_debug_set(hldev, VXGE_TRACE, VXGE_COMPONENT_LL);\r\nVXGE_COPY_DEBUG_INFO_TO_LL(vdev, vxge_hw_device_error_level_get(hldev),\r\nvxge_hw_device_trace_level_get(hldev));\r\nvdev->mtu = VXGE_HW_DEFAULT_MTU;\r\nvdev->bar0 = attr.bar0;\r\nvdev->max_vpath_supported = max_vpath_supported;\r\nvdev->no_of_vpath = no_of_vpath;\r\nfor (i = 0, j = 0; i < VXGE_HW_MAX_VIRTUAL_PATHS; i++) {\r\nif (!vxge_bVALn(vpath_mask, i, 1))\r\ncontinue;\r\nif (j >= vdev->no_of_vpath)\r\nbreak;\r\nvdev->vpaths[j].is_configured = 1;\r\nvdev->vpaths[j].device_id = i;\r\nvdev->vpaths[j].ring.driver_id = j;\r\nvdev->vpaths[j].vdev = vdev;\r\nvdev->vpaths[j].max_mac_addr_cnt = max_mac_vpath;\r\nmemcpy((u8 *)vdev->vpaths[j].macaddr,\r\nll_config->device_hw_info.mac_addrs[i],\r\nETH_ALEN);\r\nINIT_LIST_HEAD(&vdev->vpaths[j].mac_addr_list);\r\nvdev->vpaths[j].mac_addr_cnt = 0;\r\nvdev->vpaths[j].mcast_addr_cnt = 0;\r\nj++;\r\n}\r\nvdev->exec_mode = VXGE_EXEC_MODE_DISABLE;\r\nvdev->max_config_port = max_config_port;\r\nvdev->vlan_tag_strip = vlan_tag_strip;\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nvdev->vpath_selector[i] = vpath_selector[i];\r\nmacaddr = (u8 *)vdev->vpaths[0].macaddr;\r\nll_config->device_hw_info.serial_number[VXGE_HW_INFO_LEN - 1] = '\0';\r\nll_config->device_hw_info.product_desc[VXGE_HW_INFO_LEN - 1] = '\0';\r\nll_config->device_hw_info.part_number[VXGE_HW_INFO_LEN - 1] = '\0';\r\nvxge_debug_init(VXGE_TRACE, "%s: SERIAL NUMBER: %s",\r\nvdev->ndev->name, ll_config->device_hw_info.serial_number);\r\nvxge_debug_init(VXGE_TRACE, "%s: PART NUMBER: %s",\r\nvdev->ndev->name, ll_config->device_hw_info.part_number);\r\nvxge_debug_init(VXGE_TRACE, "%s: Neterion %s Server Adapter",\r\nvdev->ndev->name, ll_config->device_hw_info.product_desc);\r\nvxge_debug_init(VXGE_TRACE, "%s: MAC ADDR: %pM",\r\nvdev->ndev->name, macaddr);\r\nvxge_debug_init(VXGE_TRACE, "%s: Link Width x%d",\r\nvdev->ndev->name, vxge_hw_device_link_width_get(hldev));\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Firmware version : %s Date : %s", vdev->ndev->name,\r\nll_config->device_hw_info.fw_version.version,\r\nll_config->device_hw_info.fw_date.date);\r\nif (new_device) {\r\nswitch (ll_config->device_hw_info.function_mode) {\r\ncase VXGE_HW_FUNCTION_MODE_SINGLE_FUNCTION:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Single Function Mode Enabled", vdev->ndev->name);\r\nbreak;\r\ncase VXGE_HW_FUNCTION_MODE_MULTI_FUNCTION:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Multi Function Mode Enabled", vdev->ndev->name);\r\nbreak;\r\ncase VXGE_HW_FUNCTION_MODE_SRIOV:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Single Root IOV Mode Enabled", vdev->ndev->name);\r\nbreak;\r\ncase VXGE_HW_FUNCTION_MODE_MRIOV:\r\nvxge_debug_init(VXGE_TRACE,\r\n"%s: Multi Root IOV Mode Enabled", vdev->ndev->name);\r\nbreak;\r\n}\r\n}\r\nvxge_print_parm(vdev, vpath_mask);\r\nstrcpy(vdev->fw_version, ll_config->device_hw_info.fw_version.version);\r\nmemcpy(vdev->ndev->dev_addr, (u8 *)vdev->vpaths[0].macaddr, ETH_ALEN);\r\nfor (i = 0; i < vdev->no_of_vpath; i++) {\r\nentry = kzalloc(sizeof(struct vxge_mac_addrs), GFP_KERNEL);\r\nif (NULL == entry) {\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: mac_addr_list : memory allocation failed",\r\nvdev->ndev->name);\r\nret = -EPERM;\r\ngoto _exit6;\r\n}\r\nmacaddr = (u8 *)&entry->macaddr;\r\nmemcpy(macaddr, vdev->ndev->dev_addr, ETH_ALEN);\r\nlist_add(&entry->item, &vdev->vpaths[i].mac_addr_list);\r\nvdev->vpaths[i].mac_addr_cnt = 1;\r\n}\r\nkfree(device_config);\r\nif (ll_config->device_hw_info.function_mode ==\r\nVXGE_HW_FUNCTION_MODE_MULTI_FUNCTION)\r\nif (vdev->config.intr_type == INTA)\r\nvxge_hw_device_unmask_all(hldev);\r\nvxge_debug_entryexit(VXGE_TRACE, "%s: %s:%d Exiting...",\r\nvdev->ndev->name, __func__, __LINE__);\r\nvxge_hw_device_debug_set(hldev, VXGE_ERR, VXGE_COMPONENT_LL);\r\nVXGE_COPY_DEBUG_INFO_TO_LL(vdev, vxge_hw_device_error_level_get(hldev),\r\nvxge_hw_device_trace_level_get(hldev));\r\nkfree(ll_config);\r\nreturn 0;\r\n_exit6:\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nvxge_free_mac_add_list(&vdev->vpaths[i]);\r\n_exit5:\r\nvxge_device_unregister(hldev);\r\n_exit4:\r\nvxge_hw_device_terminate(hldev);\r\npci_disable_sriov(pdev);\r\n_exit3:\r\niounmap(attr.bar0);\r\n_exit2:\r\npci_release_region(pdev, 0);\r\n_exit1:\r\npci_disable_device(pdev);\r\n_exit0:\r\nkfree(ll_config);\r\nkfree(device_config);\r\ndriver_config->config_dev_cnt--;\r\ndriver_config->total_dev_cnt--;\r\nreturn ret;\r\n}\r\nstatic void vxge_remove(struct pci_dev *pdev)\r\n{\r\nstruct __vxge_hw_device *hldev;\r\nstruct vxgedev *vdev;\r\nint i;\r\nhldev = pci_get_drvdata(pdev);\r\nif (hldev == NULL)\r\nreturn;\r\nvdev = netdev_priv(hldev->ndev);\r\nvxge_debug_entryexit(vdev->level_trace, "%s:%d", __func__, __LINE__);\r\nvxge_debug_init(vdev->level_trace, "%s : removing PCI device...",\r\n__func__);\r\nfor (i = 0; i < vdev->no_of_vpath; i++)\r\nvxge_free_mac_add_list(&vdev->vpaths[i]);\r\nvxge_device_unregister(hldev);\r\nvxge_hw_device_terminate(hldev);\r\niounmap(vdev->bar0);\r\npci_release_region(pdev, 0);\r\npci_disable_device(pdev);\r\ndriver_config->config_dev_cnt--;\r\ndriver_config->total_dev_cnt--;\r\nvxge_debug_init(vdev->level_trace, "%s:%d Device unregistered",\r\n__func__, __LINE__);\r\nvxge_debug_entryexit(vdev->level_trace, "%s:%d Exiting...", __func__,\r\n__LINE__);\r\n}\r\nstatic int __init\r\nvxge_starter(void)\r\n{\r\nint ret = 0;\r\npr_info("Copyright(c) 2002-2010 Exar Corp.\n");\r\npr_info("Driver version: %s\n", DRV_VERSION);\r\nverify_bandwidth();\r\ndriver_config = kzalloc(sizeof(struct vxge_drv_config), GFP_KERNEL);\r\nif (!driver_config)\r\nreturn -ENOMEM;\r\nret = pci_register_driver(&vxge_driver);\r\nif (ret) {\r\nkfree(driver_config);\r\ngoto err;\r\n}\r\nif (driver_config->config_dev_cnt &&\r\n(driver_config->config_dev_cnt != driver_config->total_dev_cnt))\r\nvxge_debug_init(VXGE_ERR,\r\n"%s: Configured %d of %d devices",\r\nVXGE_DRIVER_NAME, driver_config->config_dev_cnt,\r\ndriver_config->total_dev_cnt);\r\nerr:\r\nreturn ret;\r\n}\r\nstatic void __exit\r\nvxge_closer(void)\r\n{\r\npci_unregister_driver(&vxge_driver);\r\nkfree(driver_config);\r\n}
