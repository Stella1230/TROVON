int intel_svm_alloc_pasid_tables(struct intel_iommu *iommu)\r\n{\r\nstruct page *pages;\r\nint order;\r\niommu->pasid_max = 2 << ecap_pss(iommu->ecap);\r\nif (iommu->pasid_max > 0x20000)\r\niommu->pasid_max = 0x20000;\r\norder = get_order(sizeof(struct pasid_entry) * iommu->pasid_max);\r\npages = alloc_pages(GFP_KERNEL | __GFP_ZERO, order);\r\nif (!pages) {\r\npr_warn("IOMMU: %s: Failed to allocate PASID table\n",\r\niommu->name);\r\nreturn -ENOMEM;\r\n}\r\niommu->pasid_table = page_address(pages);\r\npr_info("%s: Allocated order %d PASID table.\n", iommu->name, order);\r\nif (ecap_dis(iommu->ecap)) {\r\nBUILD_BUG_ON(sizeof(struct pasid_entry) != sizeof(struct pasid_state_entry));\r\npages = alloc_pages(GFP_KERNEL | __GFP_ZERO, order);\r\nif (pages)\r\niommu->pasid_state_table = page_address(pages);\r\nelse\r\npr_warn("IOMMU: %s: Failed to allocate PASID state table\n",\r\niommu->name);\r\n}\r\nidr_init(&iommu->pasid_idr);\r\nreturn 0;\r\n}\r\nint intel_svm_free_pasid_tables(struct intel_iommu *iommu)\r\n{\r\nint order = get_order(sizeof(struct pasid_entry) * iommu->pasid_max);\r\nif (iommu->pasid_table) {\r\nfree_pages((unsigned long)iommu->pasid_table, order);\r\niommu->pasid_table = NULL;\r\n}\r\nif (iommu->pasid_state_table) {\r\nfree_pages((unsigned long)iommu->pasid_state_table, order);\r\niommu->pasid_state_table = NULL;\r\n}\r\nidr_destroy(&iommu->pasid_idr);\r\nreturn 0;\r\n}\r\nint intel_svm_enable_prq(struct intel_iommu *iommu)\r\n{\r\nstruct page *pages;\r\nint irq, ret;\r\npages = alloc_pages(GFP_KERNEL | __GFP_ZERO, PRQ_ORDER);\r\nif (!pages) {\r\npr_warn("IOMMU: %s: Failed to allocate page request queue\n",\r\niommu->name);\r\nreturn -ENOMEM;\r\n}\r\niommu->prq = page_address(pages);\r\nirq = dmar_alloc_hwirq(DMAR_UNITS_SUPPORTED + iommu->seq_id, iommu->node, iommu);\r\nif (irq <= 0) {\r\npr_err("IOMMU: %s: Failed to create IRQ vector for page request queue\n",\r\niommu->name);\r\nret = -EINVAL;\r\nerr:\r\nfree_pages((unsigned long)iommu->prq, PRQ_ORDER);\r\niommu->prq = NULL;\r\nreturn ret;\r\n}\r\niommu->pr_irq = irq;\r\nsnprintf(iommu->prq_name, sizeof(iommu->prq_name), "dmar%d-prq", iommu->seq_id);\r\nret = request_threaded_irq(irq, NULL, prq_event_thread, IRQF_ONESHOT,\r\niommu->prq_name, iommu);\r\nif (ret) {\r\npr_err("IOMMU: %s: Failed to request IRQ for page request queue\n",\r\niommu->name);\r\ndmar_free_hwirq(irq);\r\ngoto err;\r\n}\r\ndmar_writeq(iommu->reg + DMAR_PQH_REG, 0ULL);\r\ndmar_writeq(iommu->reg + DMAR_PQT_REG, 0ULL);\r\ndmar_writeq(iommu->reg + DMAR_PQA_REG, virt_to_phys(iommu->prq) | PRQ_ORDER);\r\nreturn 0;\r\n}\r\nint intel_svm_finish_prq(struct intel_iommu *iommu)\r\n{\r\ndmar_writeq(iommu->reg + DMAR_PQH_REG, 0ULL);\r\ndmar_writeq(iommu->reg + DMAR_PQT_REG, 0ULL);\r\ndmar_writeq(iommu->reg + DMAR_PQA_REG, 0ULL);\r\nfree_irq(iommu->pr_irq, iommu);\r\ndmar_free_hwirq(iommu->pr_irq);\r\niommu->pr_irq = 0;\r\nfree_pages((unsigned long)iommu->prq, PRQ_ORDER);\r\niommu->prq = NULL;\r\nreturn 0;\r\n}\r\nstatic void intel_flush_svm_range_dev (struct intel_svm *svm, struct intel_svm_dev *sdev,\r\nunsigned long address, unsigned long pages, int ih, int gl)\r\n{\r\nstruct qi_desc desc;\r\nif (pages == -1) {\r\nif (gl)\r\ndesc.low = QI_EIOTLB_PASID(svm->pasid) | QI_EIOTLB_DID(sdev->did) |\r\nQI_EIOTLB_GRAN(QI_GRAN_ALL_ALL) | QI_EIOTLB_TYPE;\r\nelse\r\ndesc.low = QI_EIOTLB_PASID(svm->pasid) | QI_EIOTLB_DID(sdev->did) |\r\nQI_EIOTLB_GRAN(QI_GRAN_NONG_PASID) | QI_EIOTLB_TYPE;\r\ndesc.high = 0;\r\n} else {\r\nint mask = ilog2(__roundup_pow_of_two(pages));\r\ndesc.low = QI_EIOTLB_PASID(svm->pasid) | QI_EIOTLB_DID(sdev->did) |\r\nQI_EIOTLB_GRAN(QI_GRAN_PSI_PASID) | QI_EIOTLB_TYPE;\r\ndesc.high = QI_EIOTLB_ADDR(address) | QI_EIOTLB_GL(gl) |\r\nQI_EIOTLB_IH(ih) | QI_EIOTLB_AM(mask);\r\n}\r\nqi_submit_sync(&desc, svm->iommu);\r\nif (sdev->dev_iotlb) {\r\ndesc.low = QI_DEV_EIOTLB_PASID(svm->pasid) | QI_DEV_EIOTLB_SID(sdev->sid) |\r\nQI_DEV_EIOTLB_QDEP(sdev->qdep) | QI_DEIOTLB_TYPE;\r\nif (pages == -1) {\r\ndesc.high = QI_DEV_EIOTLB_ADDR(-1ULL >> 1) | QI_DEV_EIOTLB_SIZE;\r\n} else if (pages > 1) {\r\nunsigned long last = address + ((unsigned long)(pages - 1) << VTD_PAGE_SHIFT);\r\nunsigned long mask = __rounddown_pow_of_two(address ^ last);;\r\ndesc.high = QI_DEV_EIOTLB_ADDR((address & ~mask) | (mask - 1)) | QI_DEV_EIOTLB_SIZE;\r\n} else {\r\ndesc.high = QI_DEV_EIOTLB_ADDR(address);\r\n}\r\nqi_submit_sync(&desc, svm->iommu);\r\n}\r\n}\r\nstatic void intel_flush_svm_range(struct intel_svm *svm, unsigned long address,\r\nunsigned long pages, int ih, int gl)\r\n{\r\nstruct intel_svm_dev *sdev;\r\nif (svm->iommu->pasid_state_table &&\r\n!cmpxchg64(&svm->iommu->pasid_state_table[svm->pasid].val, 0, 1ULL << 63))\r\nreturn;\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(sdev, &svm->devs, list)\r\nintel_flush_svm_range_dev(svm, sdev, address, pages, ih, gl);\r\nrcu_read_unlock();\r\n}\r\nstatic void intel_change_pte(struct mmu_notifier *mn, struct mm_struct *mm,\r\nunsigned long address, pte_t pte)\r\n{\r\nstruct intel_svm *svm = container_of(mn, struct intel_svm, notifier);\r\nintel_flush_svm_range(svm, address, 1, 1, 0);\r\n}\r\nstatic void intel_invalidate_page(struct mmu_notifier *mn, struct mm_struct *mm,\r\nunsigned long address)\r\n{\r\nstruct intel_svm *svm = container_of(mn, struct intel_svm, notifier);\r\nintel_flush_svm_range(svm, address, 1, 1, 0);\r\n}\r\nstatic void intel_invalidate_range(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct intel_svm *svm = container_of(mn, struct intel_svm, notifier);\r\nintel_flush_svm_range(svm, start,\r\n(end - start + PAGE_SIZE - 1) >> VTD_PAGE_SHIFT, 0, 0);\r\n}\r\nstatic void intel_flush_pasid_dev(struct intel_svm *svm, struct intel_svm_dev *sdev, int pasid)\r\n{\r\nstruct qi_desc desc;\r\ndesc.high = 0;\r\ndesc.low = QI_PC_TYPE | QI_PC_DID(sdev->did) | QI_PC_PASID_SEL | QI_PC_PASID(pasid);\r\nqi_submit_sync(&desc, svm->iommu);\r\n}\r\nstatic void intel_mm_release(struct mmu_notifier *mn, struct mm_struct *mm)\r\n{\r\nstruct intel_svm *svm = container_of(mn, struct intel_svm, notifier);\r\nstruct intel_svm_dev *sdev;\r\nsvm->iommu->pasid_table[svm->pasid].val = 0;\r\nwmb();\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(sdev, &svm->devs, list) {\r\nintel_flush_pasid_dev(svm, sdev, svm->pasid);\r\nintel_flush_svm_range_dev(svm, sdev, 0, -1, 0, !svm->mm);\r\n}\r\nrcu_read_unlock();\r\n}\r\nint intel_svm_bind_mm(struct device *dev, int *pasid, int flags, struct svm_dev_ops *ops)\r\n{\r\nstruct intel_iommu *iommu = intel_svm_device_to_iommu(dev);\r\nstruct intel_svm_dev *sdev;\r\nstruct intel_svm *svm = NULL;\r\nstruct mm_struct *mm = NULL;\r\nint pasid_max;\r\nint ret;\r\nif (WARN_ON(!iommu))\r\nreturn -EINVAL;\r\nif (dev_is_pci(dev)) {\r\npasid_max = pci_max_pasids(to_pci_dev(dev));\r\nif (pasid_max < 0)\r\nreturn -EINVAL;\r\n} else\r\npasid_max = 1 << 20;\r\nif ((flags & SVM_FLAG_SUPERVISOR_MODE)) {\r\nif (!ecap_srs(iommu->ecap))\r\nreturn -EINVAL;\r\n} else if (pasid) {\r\nmm = get_task_mm(current);\r\nBUG_ON(!mm);\r\n}\r\nmutex_lock(&pasid_mutex);\r\nif (pasid && !(flags & SVM_FLAG_PRIVATE_PASID)) {\r\nint i;\r\nidr_for_each_entry(&iommu->pasid_idr, svm, i) {\r\nif (svm->mm != mm ||\r\n(svm->flags & SVM_FLAG_PRIVATE_PASID))\r\ncontinue;\r\nif (svm->pasid >= pasid_max) {\r\ndev_warn(dev,\r\n"Limited PASID width. Cannot use existing PASID %d\n",\r\nsvm->pasid);\r\nret = -ENOSPC;\r\ngoto out;\r\n}\r\nlist_for_each_entry(sdev, &svm->devs, list) {\r\nif (dev == sdev->dev) {\r\nif (sdev->ops != ops) {\r\nret = -EBUSY;\r\ngoto out;\r\n}\r\nsdev->users++;\r\ngoto success;\r\n}\r\n}\r\nbreak;\r\n}\r\n}\r\nsdev = kzalloc(sizeof(*sdev), GFP_KERNEL);\r\nif (!sdev) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nsdev->dev = dev;\r\nret = intel_iommu_enable_pasid(iommu, sdev);\r\nif (ret || !pasid) {\r\nkfree(sdev);\r\ngoto out;\r\n}\r\nsdev->users = 1;\r\nsdev->ops = ops;\r\ninit_rcu_head(&sdev->rcu);\r\nif (!svm) {\r\nsvm = kzalloc(sizeof(*svm), GFP_KERNEL);\r\nif (!svm) {\r\nret = -ENOMEM;\r\nkfree(sdev);\r\ngoto out;\r\n}\r\nsvm->iommu = iommu;\r\nif (pasid_max > iommu->pasid_max)\r\npasid_max = iommu->pasid_max;\r\nret = idr_alloc(&iommu->pasid_idr, svm,\r\n!!cap_caching_mode(iommu->cap),\r\npasid_max - 1, GFP_KERNEL);\r\nif (ret < 0) {\r\nkfree(svm);\r\ngoto out;\r\n}\r\nsvm->pasid = ret;\r\nsvm->notifier.ops = &intel_mmuops;\r\nsvm->mm = mm;\r\nsvm->flags = flags;\r\nINIT_LIST_HEAD_RCU(&svm->devs);\r\nret = -ENOMEM;\r\nif (mm) {\r\nret = mmu_notifier_register(&svm->notifier, mm);\r\nif (ret) {\r\nidr_remove(&svm->iommu->pasid_idr, svm->pasid);\r\nkfree(svm);\r\nkfree(sdev);\r\ngoto out;\r\n}\r\niommu->pasid_table[svm->pasid].val = (u64)__pa(mm->pgd) | 1;\r\n} else\r\niommu->pasid_table[svm->pasid].val = (u64)__pa(init_mm.pgd) | 1 | (1ULL << 11);\r\nwmb();\r\nif (cap_caching_mode(iommu->cap))\r\nintel_flush_pasid_dev(svm, sdev, 0);\r\n}\r\nlist_add_rcu(&sdev->list, &svm->devs);\r\nsuccess:\r\n*pasid = svm->pasid;\r\nret = 0;\r\nout:\r\nmutex_unlock(&pasid_mutex);\r\nif (mm)\r\nmmput(mm);\r\nreturn ret;\r\n}\r\nint intel_svm_unbind_mm(struct device *dev, int pasid)\r\n{\r\nstruct intel_svm_dev *sdev;\r\nstruct intel_iommu *iommu;\r\nstruct intel_svm *svm;\r\nint ret = -EINVAL;\r\nmutex_lock(&pasid_mutex);\r\niommu = intel_svm_device_to_iommu(dev);\r\nif (!iommu || !iommu->pasid_table)\r\ngoto out;\r\nsvm = idr_find(&iommu->pasid_idr, pasid);\r\nif (!svm)\r\ngoto out;\r\nlist_for_each_entry(sdev, &svm->devs, list) {\r\nif (dev == sdev->dev) {\r\nret = 0;\r\nsdev->users--;\r\nif (!sdev->users) {\r\nlist_del_rcu(&sdev->list);\r\nintel_flush_pasid_dev(svm, sdev, svm->pasid);\r\nintel_flush_svm_range_dev(svm, sdev, 0, -1, 0, !svm->mm);\r\nkfree_rcu(sdev, rcu);\r\nif (list_empty(&svm->devs)) {\r\nidr_remove(&svm->iommu->pasid_idr, svm->pasid);\r\nif (svm->mm)\r\nmmu_notifier_unregister(&svm->notifier, svm->mm);\r\nmemset(svm, 0x6b, sizeof(*svm));\r\nkfree(svm);\r\n}\r\n}\r\nbreak;\r\n}\r\n}\r\nout:\r\nmutex_unlock(&pasid_mutex);\r\nreturn ret;\r\n}\r\nstatic bool access_error(struct vm_area_struct *vma, struct page_req_dsc *req)\r\n{\r\nunsigned long requested = 0;\r\nif (req->exe_req)\r\nrequested |= VM_EXEC;\r\nif (req->rd_req)\r\nrequested |= VM_READ;\r\nif (req->wr_req)\r\nrequested |= VM_WRITE;\r\nreturn (requested & ~vma->vm_flags) != 0;\r\n}\r\nstatic irqreturn_t prq_event_thread(int irq, void *d)\r\n{\r\nstruct intel_iommu *iommu = d;\r\nstruct intel_svm *svm = NULL;\r\nint head, tail, handled = 0;\r\nwritel(DMA_PRS_PPR, iommu->reg + DMAR_PRS_REG);\r\ntail = dmar_readq(iommu->reg + DMAR_PQT_REG) & PRQ_RING_MASK;\r\nhead = dmar_readq(iommu->reg + DMAR_PQH_REG) & PRQ_RING_MASK;\r\nwhile (head != tail) {\r\nstruct intel_svm_dev *sdev;\r\nstruct vm_area_struct *vma;\r\nstruct page_req_dsc *req;\r\nstruct qi_desc resp;\r\nint ret, result;\r\nu64 address;\r\nhandled = 1;\r\nreq = &iommu->prq[head / sizeof(*req)];\r\nresult = QI_RESP_FAILURE;\r\naddress = (u64)req->addr << VTD_PAGE_SHIFT;\r\nif (!req->pasid_present) {\r\npr_err("%s: Page request without PASID: %08llx %08llx\n",\r\niommu->name, ((unsigned long long *)req)[0],\r\n((unsigned long long *)req)[1]);\r\ngoto bad_req;\r\n}\r\nif (!svm || svm->pasid != req->pasid) {\r\nrcu_read_lock();\r\nsvm = idr_find(&iommu->pasid_idr, req->pasid);\r\nrcu_read_unlock();\r\nif (!svm) {\r\npr_err("%s: Page request for invalid PASID %d: %08llx %08llx\n",\r\niommu->name, req->pasid, ((unsigned long long *)req)[0],\r\n((unsigned long long *)req)[1]);\r\ngoto no_pasid;\r\n}\r\n}\r\nresult = QI_RESP_INVALID;\r\nif (!svm->mm)\r\ngoto bad_req;\r\nif (!mmget_not_zero(svm->mm))\r\ngoto bad_req;\r\ndown_read(&svm->mm->mmap_sem);\r\nvma = find_extend_vma(svm->mm, address);\r\nif (!vma || address < vma->vm_start)\r\ngoto invalid;\r\nif (access_error(vma, req))\r\ngoto invalid;\r\nret = handle_mm_fault(vma, address,\r\nreq->wr_req ? FAULT_FLAG_WRITE : 0);\r\nif (ret & VM_FAULT_ERROR)\r\ngoto invalid;\r\nresult = QI_RESP_SUCCESS;\r\ninvalid:\r\nup_read(&svm->mm->mmap_sem);\r\nmmput(svm->mm);\r\nbad_req:\r\nrcu_read_lock();\r\nlist_for_each_entry_rcu(sdev, &svm->devs, list) {\r\nif (sdev->sid == PCI_DEVID(req->bus, req->devfn))\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\nif (WARN_ON(&sdev->list == &svm->devs))\r\nsdev = NULL;\r\nif (sdev && sdev->ops && sdev->ops->fault_cb) {\r\nint rwxp = (req->rd_req << 3) | (req->wr_req << 2) |\r\n(req->exe_req << 1) | (req->priv_req);\r\nsdev->ops->fault_cb(sdev->dev, req->pasid, req->addr, req->private, rwxp, result);\r\n}\r\nsdev = NULL;\r\nsvm = NULL;\r\nno_pasid:\r\nif (req->lpig) {\r\nresp.low = QI_PGRP_PASID(req->pasid) |\r\nQI_PGRP_DID((req->bus << 8) | req->devfn) |\r\nQI_PGRP_PASID_P(req->pasid_present) |\r\nQI_PGRP_RESP_TYPE;\r\nresp.high = QI_PGRP_IDX(req->prg_index) |\r\nQI_PGRP_PRIV(req->private) | QI_PGRP_RESP_CODE(result);\r\nqi_submit_sync(&resp, iommu);\r\n} else if (req->srr) {\r\nresp.low = QI_PSTRM_IDX(req->prg_index) |\r\nQI_PSTRM_PRIV(req->private) | QI_PSTRM_BUS(req->bus) |\r\nQI_PSTRM_PASID(req->pasid) | QI_PSTRM_RESP_TYPE;\r\nresp.high = QI_PSTRM_ADDR(address) | QI_PSTRM_DEVFN(req->devfn) |\r\nQI_PSTRM_RESP_CODE(result);\r\nqi_submit_sync(&resp, iommu);\r\n}\r\nhead = (head + sizeof(*req)) & PRQ_RING_MASK;\r\n}\r\ndmar_writeq(iommu->reg + DMAR_PQH_REG, tail);\r\nreturn IRQ_RETVAL(handled);\r\n}
