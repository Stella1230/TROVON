int pvrdma_req_notify_cq(struct ib_cq *ibcq,\r\nenum ib_cq_notify_flags notify_flags)\r\n{\r\nstruct pvrdma_dev *dev = to_vdev(ibcq->device);\r\nstruct pvrdma_cq *cq = to_vcq(ibcq);\r\nu32 val = cq->cq_handle;\r\nval |= (notify_flags & IB_CQ_SOLICITED_MASK) == IB_CQ_SOLICITED ?\r\nPVRDMA_UAR_CQ_ARM_SOL : PVRDMA_UAR_CQ_ARM;\r\npvrdma_write_uar_cq(dev, val);\r\nreturn 0;\r\n}\r\nstruct ib_cq *pvrdma_create_cq(struct ib_device *ibdev,\r\nconst struct ib_cq_init_attr *attr,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata)\r\n{\r\nint entries = attr->cqe;\r\nstruct pvrdma_dev *dev = to_vdev(ibdev);\r\nstruct pvrdma_cq *cq;\r\nint ret;\r\nint npages;\r\nunsigned long flags;\r\nunion pvrdma_cmd_req req;\r\nunion pvrdma_cmd_resp rsp;\r\nstruct pvrdma_cmd_create_cq *cmd = &req.create_cq;\r\nstruct pvrdma_cmd_create_cq_resp *resp = &rsp.create_cq_resp;\r\nstruct pvrdma_create_cq ucmd;\r\nBUILD_BUG_ON(sizeof(struct pvrdma_cqe) != 64);\r\nentries = roundup_pow_of_two(entries);\r\nif (entries < 1 || entries > dev->dsr->caps.max_cqe)\r\nreturn ERR_PTR(-EINVAL);\r\nif (!atomic_add_unless(&dev->num_cqs, 1, dev->dsr->caps.max_cq))\r\nreturn ERR_PTR(-ENOMEM);\r\ncq = kzalloc(sizeof(*cq), GFP_KERNEL);\r\nif (!cq) {\r\natomic_dec(&dev->num_cqs);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\ncq->ibcq.cqe = entries;\r\nif (context) {\r\nif (ib_copy_from_udata(&ucmd, udata, sizeof(ucmd))) {\r\nret = -EFAULT;\r\ngoto err_cq;\r\n}\r\ncq->umem = ib_umem_get(context, ucmd.buf_addr, ucmd.buf_size,\r\nIB_ACCESS_LOCAL_WRITE, 1);\r\nif (IS_ERR(cq->umem)) {\r\nret = PTR_ERR(cq->umem);\r\ngoto err_cq;\r\n}\r\nnpages = ib_umem_page_count(cq->umem);\r\n} else {\r\ncq->is_kernel = true;\r\nnpages = 1 + (entries * sizeof(struct pvrdma_cqe) +\r\nPAGE_SIZE - 1) / PAGE_SIZE;\r\ncq->offset = PAGE_SIZE;\r\n}\r\nif (npages < 0 || npages > PVRDMA_PAGE_DIR_MAX_PAGES) {\r\ndev_warn(&dev->pdev->dev,\r\n"overflow pages in completion queue\n");\r\nret = -EINVAL;\r\ngoto err_umem;\r\n}\r\nret = pvrdma_page_dir_init(dev, &cq->pdir, npages, cq->is_kernel);\r\nif (ret) {\r\ndev_warn(&dev->pdev->dev,\r\n"could not allocate page directory\n");\r\ngoto err_umem;\r\n}\r\nif (cq->is_kernel)\r\ncq->ring_state = cq->pdir.pages[0];\r\nelse\r\npvrdma_page_dir_insert_umem(&cq->pdir, cq->umem, 0);\r\natomic_set(&cq->refcnt, 1);\r\ninit_waitqueue_head(&cq->wait);\r\nspin_lock_init(&cq->cq_lock);\r\nmemset(cmd, 0, sizeof(*cmd));\r\ncmd->hdr.cmd = PVRDMA_CMD_CREATE_CQ;\r\ncmd->nchunks = npages;\r\ncmd->ctx_handle = (context) ?\r\n(u64)to_vucontext(context)->ctx_handle : 0;\r\ncmd->cqe = entries;\r\ncmd->pdir_dma = cq->pdir.dir_dma;\r\nret = pvrdma_cmd_post(dev, &req, &rsp, PVRDMA_CMD_CREATE_CQ_RESP);\r\nif (ret < 0) {\r\ndev_warn(&dev->pdev->dev,\r\n"could not create completion queue, error: %d\n", ret);\r\ngoto err_page_dir;\r\n}\r\ncq->ibcq.cqe = resp->cqe;\r\ncq->cq_handle = resp->cq_handle;\r\nspin_lock_irqsave(&dev->cq_tbl_lock, flags);\r\ndev->cq_tbl[cq->cq_handle % dev->dsr->caps.max_cq] = cq;\r\nspin_unlock_irqrestore(&dev->cq_tbl_lock, flags);\r\nif (context) {\r\ncq->uar = &(to_vucontext(context)->uar);\r\nif (ib_copy_to_udata(udata, &cq->cq_handle, sizeof(__u32))) {\r\ndev_warn(&dev->pdev->dev,\r\n"failed to copy back udata\n");\r\npvrdma_destroy_cq(&cq->ibcq);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\n}\r\nreturn &cq->ibcq;\r\nerr_page_dir:\r\npvrdma_page_dir_cleanup(dev, &cq->pdir);\r\nerr_umem:\r\nif (context)\r\nib_umem_release(cq->umem);\r\nerr_cq:\r\natomic_dec(&dev->num_cqs);\r\nkfree(cq);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void pvrdma_free_cq(struct pvrdma_dev *dev, struct pvrdma_cq *cq)\r\n{\r\natomic_dec(&cq->refcnt);\r\nwait_event(cq->wait, !atomic_read(&cq->refcnt));\r\nif (!cq->is_kernel)\r\nib_umem_release(cq->umem);\r\npvrdma_page_dir_cleanup(dev, &cq->pdir);\r\nkfree(cq);\r\n}\r\nint pvrdma_destroy_cq(struct ib_cq *cq)\r\n{\r\nstruct pvrdma_cq *vcq = to_vcq(cq);\r\nunion pvrdma_cmd_req req;\r\nstruct pvrdma_cmd_destroy_cq *cmd = &req.destroy_cq;\r\nstruct pvrdma_dev *dev = to_vdev(cq->device);\r\nunsigned long flags;\r\nint ret;\r\nmemset(cmd, 0, sizeof(*cmd));\r\ncmd->hdr.cmd = PVRDMA_CMD_DESTROY_CQ;\r\ncmd->cq_handle = vcq->cq_handle;\r\nret = pvrdma_cmd_post(dev, &req, NULL, 0);\r\nif (ret < 0)\r\ndev_warn(&dev->pdev->dev,\r\n"could not destroy completion queue, error: %d\n",\r\nret);\r\nspin_lock_irqsave(&dev->cq_tbl_lock, flags);\r\ndev->cq_tbl[vcq->cq_handle] = NULL;\r\nspin_unlock_irqrestore(&dev->cq_tbl_lock, flags);\r\npvrdma_free_cq(dev, vcq);\r\natomic_dec(&dev->num_cqs);\r\nreturn ret;\r\n}\r\nint pvrdma_modify_cq(struct ib_cq *cq, u16 cq_count, u16 cq_period)\r\n{\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic inline struct pvrdma_cqe *get_cqe(struct pvrdma_cq *cq, int i)\r\n{\r\nreturn (struct pvrdma_cqe *)pvrdma_page_dir_get_ptr(\r\n&cq->pdir,\r\ncq->offset +\r\nsizeof(struct pvrdma_cqe) * i);\r\n}\r\nvoid _pvrdma_flush_cqe(struct pvrdma_qp *qp, struct pvrdma_cq *cq)\r\n{\r\nint head;\r\nint has_data;\r\nif (!cq->is_kernel)\r\nreturn;\r\nhas_data = pvrdma_idx_ring_has_data(&cq->ring_state->rx,\r\ncq->ibcq.cqe, &head);\r\nif (unlikely(has_data > 0)) {\r\nint items;\r\nint curr;\r\nint tail = pvrdma_idx(&cq->ring_state->rx.prod_tail,\r\ncq->ibcq.cqe);\r\nstruct pvrdma_cqe *cqe;\r\nstruct pvrdma_cqe *curr_cqe;\r\nitems = (tail > head) ? (tail - head) :\r\n(cq->ibcq.cqe - head + tail);\r\ncurr = --tail;\r\nwhile (items-- > 0) {\r\nif (curr < 0)\r\ncurr = cq->ibcq.cqe - 1;\r\nif (tail < 0)\r\ntail = cq->ibcq.cqe - 1;\r\ncurr_cqe = get_cqe(cq, curr);\r\nif ((curr_cqe->qp & 0xFFFF) != qp->qp_handle) {\r\nif (curr != tail) {\r\ncqe = get_cqe(cq, tail);\r\n*cqe = *curr_cqe;\r\n}\r\ntail--;\r\n} else {\r\npvrdma_idx_ring_inc(\r\n&cq->ring_state->rx.cons_head,\r\ncq->ibcq.cqe);\r\n}\r\ncurr--;\r\n}\r\n}\r\n}\r\nstatic int pvrdma_poll_one(struct pvrdma_cq *cq, struct pvrdma_qp **cur_qp,\r\nstruct ib_wc *wc)\r\n{\r\nstruct pvrdma_dev *dev = to_vdev(cq->ibcq.device);\r\nint has_data;\r\nunsigned int head;\r\nbool tried = false;\r\nstruct pvrdma_cqe *cqe;\r\nretry:\r\nhas_data = pvrdma_idx_ring_has_data(&cq->ring_state->rx,\r\ncq->ibcq.cqe, &head);\r\nif (has_data == 0) {\r\nif (tried)\r\nreturn -EAGAIN;\r\npvrdma_write_uar_cq(dev, cq->cq_handle | PVRDMA_UAR_CQ_POLL);\r\ntried = true;\r\ngoto retry;\r\n} else if (has_data == PVRDMA_INVALID_IDX) {\r\ndev_err(&dev->pdev->dev, "CQ ring state invalid\n");\r\nreturn -EAGAIN;\r\n}\r\ncqe = get_cqe(cq, head);\r\nrmb();\r\nif (dev->qp_tbl[cqe->qp & 0xffff])\r\n*cur_qp = (struct pvrdma_qp *)dev->qp_tbl[cqe->qp & 0xffff];\r\nelse\r\nreturn -EAGAIN;\r\nwc->opcode = pvrdma_wc_opcode_to_ib(cqe->opcode);\r\nwc->status = pvrdma_wc_status_to_ib(cqe->status);\r\nwc->wr_id = cqe->wr_id;\r\nwc->qp = &(*cur_qp)->ibqp;\r\nwc->byte_len = cqe->byte_len;\r\nwc->ex.imm_data = cqe->imm_data;\r\nwc->src_qp = cqe->src_qp;\r\nwc->wc_flags = pvrdma_wc_flags_to_ib(cqe->wc_flags);\r\nwc->pkey_index = cqe->pkey_index;\r\nwc->slid = cqe->slid;\r\nwc->sl = cqe->sl;\r\nwc->dlid_path_bits = cqe->dlid_path_bits;\r\nwc->port_num = cqe->port_num;\r\nwc->vendor_err = cqe->vendor_err;\r\npvrdma_idx_ring_inc(&cq->ring_state->rx.cons_head, cq->ibcq.cqe);\r\nreturn 0;\r\n}\r\nint pvrdma_poll_cq(struct ib_cq *ibcq, int num_entries, struct ib_wc *wc)\r\n{\r\nstruct pvrdma_cq *cq = to_vcq(ibcq);\r\nstruct pvrdma_qp *cur_qp = NULL;\r\nunsigned long flags;\r\nint npolled;\r\nif (num_entries < 1 || wc == NULL)\r\nreturn 0;\r\nspin_lock_irqsave(&cq->cq_lock, flags);\r\nfor (npolled = 0; npolled < num_entries; ++npolled) {\r\nif (pvrdma_poll_one(cq, &cur_qp, wc + npolled))\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&cq->cq_lock, flags);\r\nreturn npolled;\r\n}\r\nint pvrdma_resize_cq(struct ib_cq *ibcq, int entries, struct ib_udata *udata)\r\n{\r\nreturn -EOPNOTSUPP;\r\n}
