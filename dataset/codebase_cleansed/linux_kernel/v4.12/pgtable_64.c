void __iomem * __ioremap_at(phys_addr_t pa, void *ea, unsigned long size,\r\nunsigned long flags)\r\n{\r\nunsigned long i;\r\nif ((flags & _PAGE_PRESENT) == 0)\r\nflags |= pgprot_val(PAGE_KERNEL);\r\nif (flags & H_PAGE_4K_PFN)\r\nreturn NULL;\r\nWARN_ON(pa & ~PAGE_MASK);\r\nWARN_ON(((unsigned long)ea) & ~PAGE_MASK);\r\nWARN_ON(size & ~PAGE_MASK);\r\nfor (i = 0; i < size; i += PAGE_SIZE)\r\nif (map_kernel_page((unsigned long)ea+i, pa+i, flags))\r\nreturn NULL;\r\nreturn (void __iomem *)ea;\r\n}\r\nvoid __iounmap_at(void *ea, unsigned long size)\r\n{\r\nWARN_ON(((unsigned long)ea) & ~PAGE_MASK);\r\nWARN_ON(size & ~PAGE_MASK);\r\nunmap_kernel_range((unsigned long)ea, size);\r\n}\r\nvoid __iomem * __ioremap_caller(phys_addr_t addr, unsigned long size,\r\nunsigned long flags, void *caller)\r\n{\r\nphys_addr_t paligned;\r\nvoid __iomem *ret;\r\npaligned = addr & PAGE_MASK;\r\nsize = PAGE_ALIGN(addr + size) - paligned;\r\nif ((size == 0) || (paligned == 0))\r\nreturn NULL;\r\nif (slab_is_available()) {\r\nstruct vm_struct *area;\r\narea = __get_vm_area_caller(size, VM_IOREMAP,\r\nioremap_bot, IOREMAP_END,\r\ncaller);\r\nif (area == NULL)\r\nreturn NULL;\r\narea->phys_addr = paligned;\r\nret = __ioremap_at(paligned, area->addr, size, flags);\r\nif (!ret)\r\nvunmap(area->addr);\r\n} else {\r\nret = __ioremap_at(paligned, (void *)ioremap_bot, size, flags);\r\nif (ret)\r\nioremap_bot += size;\r\n}\r\nif (ret)\r\nret += addr & ~PAGE_MASK;\r\nreturn ret;\r\n}\r\nvoid __iomem * __ioremap(phys_addr_t addr, unsigned long size,\r\nunsigned long flags)\r\n{\r\nreturn __ioremap_caller(addr, size, flags, __builtin_return_address(0));\r\n}\r\nvoid __iomem * ioremap(phys_addr_t addr, unsigned long size)\r\n{\r\nunsigned long flags = pgprot_val(pgprot_noncached(__pgprot(0)));\r\nvoid *caller = __builtin_return_address(0);\r\nif (ppc_md.ioremap)\r\nreturn ppc_md.ioremap(addr, size, flags, caller);\r\nreturn __ioremap_caller(addr, size, flags, caller);\r\n}\r\nvoid __iomem * ioremap_wc(phys_addr_t addr, unsigned long size)\r\n{\r\nunsigned long flags = pgprot_val(pgprot_noncached_wc(__pgprot(0)));\r\nvoid *caller = __builtin_return_address(0);\r\nif (ppc_md.ioremap)\r\nreturn ppc_md.ioremap(addr, size, flags, caller);\r\nreturn __ioremap_caller(addr, size, flags, caller);\r\n}\r\nvoid __iomem * ioremap_prot(phys_addr_t addr, unsigned long size,\r\nunsigned long flags)\r\n{\r\nvoid *caller = __builtin_return_address(0);\r\nif (flags & _PAGE_WRITE)\r\nflags |= _PAGE_DIRTY;\r\nflags &= ~_PAGE_EXEC;\r\n#if defined(CONFIG_PPC_BOOK3S_64)\r\nflags |= _PAGE_PRIVILEGED;\r\n#else\r\nflags &= ~_PAGE_USER;\r\n#endif\r\n#ifdef _PAGE_BAP_SR\r\nflags |= _PAGE_BAP_SR;\r\n#endif\r\nif (ppc_md.ioremap)\r\nreturn ppc_md.ioremap(addr, size, flags, caller);\r\nreturn __ioremap_caller(addr, size, flags, caller);\r\n}\r\nvoid __iounmap(volatile void __iomem *token)\r\n{\r\nvoid *addr;\r\nif (!slab_is_available())\r\nreturn;\r\naddr = (void *) ((unsigned long __force)\r\nPCI_FIX_ADDR(token) & PAGE_MASK);\r\nif ((unsigned long)addr < ioremap_bot) {\r\nprintk(KERN_WARNING "Attempt to iounmap early bolted mapping"\r\n" at 0x%p\n", addr);\r\nreturn;\r\n}\r\nvunmap(addr);\r\n}\r\nvoid iounmap(volatile void __iomem *token)\r\n{\r\nif (ppc_md.iounmap)\r\nppc_md.iounmap(token);\r\nelse\r\n__iounmap(token);\r\n}\r\nstruct page *pgd_page(pgd_t pgd)\r\n{\r\nif (pgd_huge(pgd))\r\nreturn pte_page(pgd_pte(pgd));\r\nreturn virt_to_page(pgd_page_vaddr(pgd));\r\n}\r\nstruct page *pud_page(pud_t pud)\r\n{\r\nif (pud_huge(pud))\r\nreturn pte_page(pud_pte(pud));\r\nreturn virt_to_page(pud_page_vaddr(pud));\r\n}\r\nstruct page *pmd_page(pmd_t pmd)\r\n{\r\nif (pmd_trans_huge(pmd) || pmd_huge(pmd))\r\nreturn pte_page(pmd_pte(pmd));\r\nreturn virt_to_page(pmd_page_vaddr(pmd));\r\n}\r\nstatic pte_t *get_from_cache(struct mm_struct *mm)\r\n{\r\nvoid *pte_frag, *ret;\r\nspin_lock(&mm->page_table_lock);\r\nret = mm->context.pte_frag;\r\nif (ret) {\r\npte_frag = ret + PTE_FRAG_SIZE;\r\nif (((unsigned long)pte_frag & ~PAGE_MASK) == 0)\r\npte_frag = NULL;\r\nmm->context.pte_frag = pte_frag;\r\n}\r\nspin_unlock(&mm->page_table_lock);\r\nreturn (pte_t *)ret;\r\n}\r\nstatic pte_t *__alloc_for_cache(struct mm_struct *mm, int kernel)\r\n{\r\nvoid *ret = NULL;\r\nstruct page *page = alloc_page(GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);\r\nif (!page)\r\nreturn NULL;\r\nif (!kernel && !pgtable_page_ctor(page)) {\r\n__free_page(page);\r\nreturn NULL;\r\n}\r\nret = page_address(page);\r\nspin_lock(&mm->page_table_lock);\r\nif (likely(!mm->context.pte_frag)) {\r\nset_page_count(page, PTE_FRAG_NR);\r\nmm->context.pte_frag = ret + PTE_FRAG_SIZE;\r\n}\r\nspin_unlock(&mm->page_table_lock);\r\nreturn (pte_t *)ret;\r\n}\r\npte_t *pte_fragment_alloc(struct mm_struct *mm, unsigned long vmaddr, int kernel)\r\n{\r\npte_t *pte;\r\npte = get_from_cache(mm);\r\nif (pte)\r\nreturn pte;\r\nreturn __alloc_for_cache(mm, kernel);\r\n}\r\nvoid pte_fragment_free(unsigned long *table, int kernel)\r\n{\r\nstruct page *page = virt_to_page(table);\r\nif (put_page_testzero(page)) {\r\nif (!kernel)\r\npgtable_page_dtor(page);\r\nfree_hot_cold_page(page, 0);\r\n}\r\n}\r\nvoid pgtable_free_tlb(struct mmu_gather *tlb, void *table, int shift)\r\n{\r\nunsigned long pgf = (unsigned long)table;\r\nBUG_ON(shift > MAX_PGTABLE_INDEX_SIZE);\r\npgf |= shift;\r\ntlb_remove_table(tlb, (void *)pgf);\r\n}\r\nvoid __tlb_remove_table(void *_table)\r\n{\r\nvoid *table = (void *)((unsigned long)_table & ~MAX_PGTABLE_INDEX_SIZE);\r\nunsigned shift = (unsigned long)_table & MAX_PGTABLE_INDEX_SIZE;\r\nif (!shift)\r\npte_fragment_free(table, 0);\r\nelse {\r\nBUG_ON(shift > MAX_PGTABLE_INDEX_SIZE);\r\nkmem_cache_free(PGT_CACHE(shift), table);\r\n}\r\n}\r\nvoid pgtable_free_tlb(struct mmu_gather *tlb, void *table, int shift)\r\n{\r\nif (!shift) {\r\npte_fragment_free(table, 0);\r\n} else {\r\nBUG_ON(shift > MAX_PGTABLE_INDEX_SIZE);\r\nkmem_cache_free(PGT_CACHE(shift), table);\r\n}\r\n}\r\nvoid __init mmu_partition_table_init(void)\r\n{\r\nunsigned long patb_size = 1UL << PATB_SIZE_SHIFT;\r\nunsigned long ptcr;\r\nBUILD_BUG_ON_MSG((PATB_SIZE_SHIFT > 36), "Partition table size too large.");\r\npartition_tb = __va(memblock_alloc_base(patb_size, patb_size,\r\nMEMBLOCK_ALLOC_ANYWHERE));\r\nmemset((void *)partition_tb, 0, patb_size);\r\nptcr = __pa(partition_tb) | (PATB_SIZE_SHIFT - 12);\r\nmtspr(SPRN_PTCR, ptcr);\r\npowernv_set_nmmu_ptcr(ptcr);\r\n}\r\nvoid mmu_partition_table_set_entry(unsigned int lpid, unsigned long dw0,\r\nunsigned long dw1)\r\n{\r\nunsigned long old = be64_to_cpu(partition_tb[lpid].patb0);\r\npartition_tb[lpid].patb0 = cpu_to_be64(dw0);\r\npartition_tb[lpid].patb1 = cpu_to_be64(dw1);\r\nasm volatile("ptesync" : : : "memory");\r\nif (old & PATB_HR)\r\nasm volatile(PPC_TLBIE_5(%0,%1,2,0,1) : :\r\n"r" (TLBIEL_INVAL_SET_LPID), "r" (lpid));\r\nelse\r\nasm volatile(PPC_TLBIE_5(%0,%1,2,0,0) : :\r\n"r" (TLBIEL_INVAL_SET_LPID), "r" (lpid));\r\nasm volatile("eieio; tlbsync; ptesync" : : : "memory");\r\n}
