static int psci_ops_check(void)\r\n{\r\nint migrate_type = -1;\r\nint cpu;\r\nif (!(psci_ops.cpu_off && psci_ops.cpu_on && psci_ops.cpu_suspend)) {\r\npr_warn("Missing PSCI operations, aborting tests\n");\r\nreturn -EOPNOTSUPP;\r\n}\r\nif (psci_ops.migrate_info_type)\r\nmigrate_type = psci_ops.migrate_info_type();\r\nif (migrate_type == PSCI_0_2_TOS_UP_MIGRATE ||\r\nmigrate_type == PSCI_0_2_TOS_UP_NO_MIGRATE) {\r\nfor_each_online_cpu(cpu)\r\nif (psci_tos_resident_on(cpu)) {\r\ntos_resident_cpu = cpu;\r\nbreak;\r\n}\r\nif (tos_resident_cpu == -1)\r\npr_warn("UP Trusted OS resides on no online CPU\n");\r\n}\r\nreturn 0;\r\n}\r\nstatic int find_clusters(const struct cpumask *cpus,\r\nconst struct cpumask **clusters)\r\n{\r\nunsigned int nb = 0;\r\ncpumask_var_t tmp;\r\nif (!alloc_cpumask_var(&tmp, GFP_KERNEL))\r\nreturn -ENOMEM;\r\ncpumask_copy(tmp, cpus);\r\nwhile (!cpumask_empty(tmp)) {\r\nconst struct cpumask *cluster =\r\ntopology_core_cpumask(cpumask_any(tmp));\r\nclusters[nb++] = cluster;\r\ncpumask_andnot(tmp, tmp, cluster);\r\n}\r\nfree_cpumask_var(tmp);\r\nreturn nb;\r\n}\r\nstatic unsigned int down_and_up_cpus(const struct cpumask *cpus,\r\nstruct cpumask *offlined_cpus)\r\n{\r\nint cpu;\r\nint err = 0;\r\ncpumask_clear(offlined_cpus);\r\nfor_each_cpu(cpu, cpus) {\r\nint ret = cpu_down(cpu);\r\nif (cpumask_weight(offlined_cpus) + 1 == nb_available_cpus) {\r\nif (ret != -EBUSY) {\r\npr_err("Unexpected return code %d while trying "\r\n"to power down last online CPU %d\n",\r\nret, cpu);\r\n++err;\r\n}\r\n} else if (cpu == tos_resident_cpu) {\r\nif (ret != -EPERM) {\r\npr_err("Unexpected return code %d while trying "\r\n"to power down TOS resident CPU %d\n",\r\nret, cpu);\r\n++err;\r\n}\r\n} else if (ret != 0) {\r\npr_err("Error occurred (%d) while trying "\r\n"to power down CPU %d\n", ret, cpu);\r\n++err;\r\n}\r\nif (ret == 0)\r\ncpumask_set_cpu(cpu, offlined_cpus);\r\n}\r\nfor_each_cpu(cpu, offlined_cpus) {\r\nint ret = cpu_up(cpu);\r\nif (ret != 0) {\r\npr_err("Error occurred (%d) while trying "\r\n"to power up CPU %d\n", ret, cpu);\r\n++err;\r\n} else {\r\ncpumask_clear_cpu(cpu, offlined_cpus);\r\n}\r\n}\r\nWARN_ON(!cpumask_empty(offlined_cpus) ||\r\nnum_online_cpus() != nb_available_cpus);\r\nreturn err;\r\n}\r\nstatic int hotplug_tests(void)\r\n{\r\nint err;\r\ncpumask_var_t offlined_cpus;\r\nint i, nb_cluster;\r\nconst struct cpumask **clusters;\r\nchar *page_buf;\r\nerr = -ENOMEM;\r\nif (!alloc_cpumask_var(&offlined_cpus, GFP_KERNEL))\r\nreturn err;\r\nclusters = kmalloc_array(nb_available_cpus, sizeof(*clusters),\r\nGFP_KERNEL);\r\nif (!clusters)\r\ngoto out_free_cpus;\r\npage_buf = (char *)__get_free_page(GFP_KERNEL);\r\nif (!page_buf)\r\ngoto out_free_clusters;\r\nerr = 0;\r\nnb_cluster = find_clusters(cpu_online_mask, clusters);\r\npr_info("Trying to turn off and on again all CPUs\n");\r\nerr += down_and_up_cpus(cpu_online_mask, offlined_cpus);\r\nfor (i = 0; i < nb_cluster; ++i) {\r\nint cluster_id =\r\ntopology_physical_package_id(cpumask_any(clusters[i]));\r\nssize_t len = cpumap_print_to_pagebuf(true, page_buf,\r\nclusters[i]);\r\npage_buf[len - 1] = '\0';\r\npr_info("Trying to turn off and on again cluster %d "\r\n"(CPUs %s)\n", cluster_id, page_buf);\r\nerr += down_and_up_cpus(clusters[i], offlined_cpus);\r\n}\r\nfree_page((unsigned long)page_buf);\r\nout_free_clusters:\r\nkfree(clusters);\r\nout_free_cpus:\r\nfree_cpumask_var(offlined_cpus);\r\nreturn err;\r\n}\r\nstatic void dummy_callback(unsigned long ignored) {}\r\nstatic int suspend_cpu(int index, bool broadcast)\r\n{\r\nint ret;\r\narch_cpu_idle_enter();\r\nif (broadcast) {\r\nret = tick_broadcast_enter();\r\nif (ret) {\r\ncpu_do_idle();\r\nret = 0;\r\ngoto out_arch_exit;\r\n}\r\n}\r\nret = CPU_PM_CPU_IDLE_ENTER(arm_cpuidle_suspend, index);\r\nif (broadcast)\r\ntick_broadcast_exit();\r\nout_arch_exit:\r\narch_cpu_idle_exit();\r\nreturn ret;\r\n}\r\nstatic int suspend_test_thread(void *arg)\r\n{\r\nint cpu = (long)arg;\r\nint i, nb_suspend = 0, nb_shallow_sleep = 0, nb_err = 0;\r\nstruct sched_param sched_priority = { .sched_priority = MAX_RT_PRIO-1 };\r\nstruct cpuidle_device *dev;\r\nstruct cpuidle_driver *drv;\r\nstruct timer_list wakeup_timer;\r\nwait_for_completion(&suspend_threads_started);\r\nif (sched_setscheduler_nocheck(current, SCHED_FIFO, &sched_priority))\r\npr_warn("Failed to set suspend thread scheduler on CPU %d\n",\r\ncpu);\r\ndev = this_cpu_read(cpuidle_devices);\r\ndrv = cpuidle_get_cpu_driver(dev);\r\npr_info("CPU %d entering suspend cycles, states 1 through %d\n",\r\ncpu, drv->state_count - 1);\r\nsetup_timer_on_stack(&wakeup_timer, dummy_callback, 0);\r\nfor (i = 0; i < NUM_SUSPEND_CYCLE; ++i) {\r\nint index;\r\nfor (index = 1; index < drv->state_count; ++index) {\r\nstruct cpuidle_state *state = &drv->states[index];\r\nbool broadcast = state->flags & CPUIDLE_FLAG_TIMER_STOP;\r\nint ret;\r\nmod_timer(&wakeup_timer, jiffies +\r\nusecs_to_jiffies(state->target_residency));\r\nlocal_irq_disable();\r\nret = suspend_cpu(index, broadcast);\r\nlocal_irq_enable();\r\nif (ret == index) {\r\n++nb_suspend;\r\n} else if (ret >= 0) {\r\n++nb_shallow_sleep;\r\n} else {\r\npr_err("Failed to suspend CPU %d: error %d "\r\n"(requested state %d, cycle %d)\n",\r\ncpu, ret, index, i);\r\n++nb_err;\r\n}\r\n}\r\n}\r\ndel_timer(&wakeup_timer);\r\nif (atomic_dec_return_relaxed(&nb_active_threads) == 0)\r\ncomplete(&suspend_threads_done);\r\nsched_priority.sched_priority = 0;\r\nif (sched_setscheduler_nocheck(current, SCHED_NORMAL, &sched_priority))\r\npr_warn("Failed to set suspend thread scheduler on CPU %d\n",\r\ncpu);\r\nfor (;;) {\r\nset_current_state(TASK_INTERRUPTIBLE);\r\nif (kthread_should_stop()) {\r\n__set_current_state(TASK_RUNNING);\r\nbreak;\r\n}\r\nschedule();\r\n}\r\npr_info("CPU %d suspend test results: success %d, shallow states %d, errors %d\n",\r\ncpu, nb_suspend, nb_shallow_sleep, nb_err);\r\nreturn nb_err;\r\n}\r\nstatic int suspend_tests(void)\r\n{\r\nint i, cpu, err = 0;\r\nstruct task_struct **threads;\r\nint nb_threads = 0;\r\nthreads = kmalloc_array(nb_available_cpus, sizeof(*threads),\r\nGFP_KERNEL);\r\nif (!threads)\r\nreturn -ENOMEM;\r\ncpuidle_pause_and_lock();\r\nfor_each_online_cpu(cpu) {\r\nstruct task_struct *thread;\r\nstruct cpuidle_device *dev = per_cpu(cpuidle_devices, cpu);\r\nstruct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);\r\nif (!dev || !drv) {\r\npr_warn("cpuidle not available on CPU %d, ignoring\n",\r\ncpu);\r\ncontinue;\r\n}\r\nthread = kthread_create_on_cpu(suspend_test_thread,\r\n(void *)(long)cpu, cpu,\r\n"psci_suspend_test");\r\nif (IS_ERR(thread))\r\npr_err("Failed to create kthread on CPU %d\n", cpu);\r\nelse\r\nthreads[nb_threads++] = thread;\r\n}\r\nif (nb_threads < 1) {\r\nerr = -ENODEV;\r\ngoto out;\r\n}\r\natomic_set(&nb_active_threads, nb_threads);\r\nfor (i = 0; i < nb_threads; ++i)\r\nwake_up_process(threads[i]);\r\ncomplete_all(&suspend_threads_started);\r\nwait_for_completion(&suspend_threads_done);\r\nfor (i = 0; i < nb_threads; ++i)\r\nerr += kthread_stop(threads[i]);\r\nout:\r\ncpuidle_resume_and_unlock();\r\nkfree(threads);\r\nreturn err;\r\n}\r\nstatic int __init psci_checker(void)\r\n{\r\nint ret;\r\nnb_available_cpus = num_online_cpus();\r\nret = psci_ops_check();\r\nif (ret)\r\nreturn ret;\r\npr_info("PSCI checker started using %u CPUs\n", nb_available_cpus);\r\npr_info("Starting hotplug tests\n");\r\nret = hotplug_tests();\r\nif (ret == 0)\r\npr_info("Hotplug tests passed OK\n");\r\nelse if (ret > 0)\r\npr_err("%d error(s) encountered in hotplug tests\n", ret);\r\nelse {\r\npr_err("Out of memory\n");\r\nreturn ret;\r\n}\r\npr_info("Starting suspend tests (%d cycles per state)\n",\r\nNUM_SUSPEND_CYCLE);\r\nret = suspend_tests();\r\nif (ret == 0)\r\npr_info("Suspend tests passed OK\n");\r\nelse if (ret > 0)\r\npr_err("%d error(s) encountered in suspend tests\n", ret);\r\nelse {\r\nswitch (ret) {\r\ncase -ENOMEM:\r\npr_err("Out of memory\n");\r\nbreak;\r\ncase -ENODEV:\r\npr_warn("Could not start suspend tests on any CPU\n");\r\nbreak;\r\n}\r\n}\r\npr_info("PSCI checker completed\n");\r\nreturn ret < 0 ? ret : 0;\r\n}
