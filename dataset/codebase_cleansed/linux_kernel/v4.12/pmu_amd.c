static unsigned amd_find_arch_event(struct kvm_pmu *pmu,\r\nu8 event_select,\r\nu8 unit_mask)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(amd_event_mapping); i++)\r\nif (amd_event_mapping[i].eventsel == event_select\r\n&& amd_event_mapping[i].unit_mask == unit_mask)\r\nbreak;\r\nif (i == ARRAY_SIZE(amd_event_mapping))\r\nreturn PERF_COUNT_HW_MAX;\r\nreturn amd_event_mapping[i].event_type;\r\n}\r\nstatic unsigned amd_find_fixed_event(int idx)\r\n{\r\nreturn PERF_COUNT_HW_MAX;\r\n}\r\nstatic bool amd_pmc_is_enabled(struct kvm_pmc *pmc)\r\n{\r\nreturn true;\r\n}\r\nstatic struct kvm_pmc *amd_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)\r\n{\r\nreturn get_gp_pmc(pmu, MSR_K7_EVNTSEL0 + pmc_idx, MSR_K7_EVNTSEL0);\r\n}\r\nstatic int amd_is_valid_msr_idx(struct kvm_vcpu *vcpu, unsigned idx)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nidx &= ~(3u << 30);\r\nreturn (idx >= pmu->nr_arch_gp_counters);\r\n}\r\nstatic struct kvm_pmc *amd_msr_idx_to_pmc(struct kvm_vcpu *vcpu, unsigned idx)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nstruct kvm_pmc *counters;\r\nidx &= ~(3u << 30);\r\nif (idx >= pmu->nr_arch_gp_counters)\r\nreturn NULL;\r\ncounters = pmu->gp_counters;\r\nreturn &counters[idx];\r\n}\r\nstatic bool amd_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nint ret = false;\r\nret = get_gp_pmc(pmu, msr, MSR_K7_PERFCTR0) ||\r\nget_gp_pmc(pmu, msr, MSR_K7_EVNTSEL0);\r\nreturn ret;\r\n}\r\nstatic int amd_pmu_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *data)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nstruct kvm_pmc *pmc;\r\npmc = get_gp_pmc(pmu, msr, MSR_K7_PERFCTR0);\r\nif (pmc) {\r\n*data = pmc_read_counter(pmc);\r\nreturn 0;\r\n}\r\npmc = get_gp_pmc(pmu, msr, MSR_K7_EVNTSEL0);\r\nif (pmc) {\r\n*data = pmc->eventsel;\r\nreturn 0;\r\n}\r\nreturn 1;\r\n}\r\nstatic int amd_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nstruct kvm_pmc *pmc;\r\nu32 msr = msr_info->index;\r\nu64 data = msr_info->data;\r\npmc = get_gp_pmc(pmu, msr, MSR_K7_PERFCTR0);\r\nif (pmc) {\r\npmc->counter += data - pmc_read_counter(pmc);\r\nreturn 0;\r\n}\r\npmc = get_gp_pmc(pmu, msr, MSR_K7_EVNTSEL0);\r\nif (pmc) {\r\nif (data == pmc->eventsel)\r\nreturn 0;\r\nif (!(data & pmu->reserved_bits)) {\r\nreprogram_gp_counter(pmc, data);\r\nreturn 0;\r\n}\r\n}\r\nreturn 1;\r\n}\r\nstatic void amd_pmu_refresh(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\npmu->nr_arch_gp_counters = AMD64_NUM_COUNTERS;\r\npmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << 48) - 1;\r\npmu->reserved_bits = 0xffffffff00200000ull;\r\npmu->counter_bitmask[KVM_PMC_FIXED] = 0;\r\npmu->nr_arch_fixed_counters = 0;\r\npmu->version = 0;\r\npmu->global_status = 0;\r\n}\r\nstatic void amd_pmu_init(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nint i;\r\nfor (i = 0; i < AMD64_NUM_COUNTERS ; i++) {\r\npmu->gp_counters[i].type = KVM_PMC_GP;\r\npmu->gp_counters[i].vcpu = vcpu;\r\npmu->gp_counters[i].idx = i;\r\n}\r\n}\r\nstatic void amd_pmu_reset(struct kvm_vcpu *vcpu)\r\n{\r\nstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\r\nint i;\r\nfor (i = 0; i < AMD64_NUM_COUNTERS; i++) {\r\nstruct kvm_pmc *pmc = &pmu->gp_counters[i];\r\npmc_stop_counter(pmc);\r\npmc->counter = pmc->eventsel = 0;\r\n}\r\n}
