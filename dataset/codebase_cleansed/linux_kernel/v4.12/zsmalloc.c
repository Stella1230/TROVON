static void SetPageHugeObject(struct page *page)\r\n{\r\nSetPageOwnerPriv1(page);\r\n}\r\nstatic void ClearPageHugeObject(struct page *page)\r\n{\r\nClearPageOwnerPriv1(page);\r\n}\r\nstatic int PageHugeObject(struct page *page)\r\n{\r\nreturn PageOwnerPriv1(page);\r\n}\r\nstatic int zsmalloc_mount(void) { return 0; }\r\nstatic void zsmalloc_unmount(void) {}\r\nstatic int zs_register_migration(struct zs_pool *pool) { return 0; }\r\nstatic void zs_unregister_migration(struct zs_pool *pool) {}\r\nstatic void migrate_lock_init(struct zspage *zspage) {}\r\nstatic void migrate_read_lock(struct zspage *zspage) {}\r\nstatic void migrate_read_unlock(struct zspage *zspage) {}\r\nstatic void kick_deferred_free(struct zs_pool *pool) {}\r\nstatic void init_deferred_free(struct zs_pool *pool) {}\r\nstatic void SetZsPageMovable(struct zs_pool *pool, struct zspage *zspage) {}\r\nstatic int create_cache(struct zs_pool *pool)\r\n{\r\npool->handle_cachep = kmem_cache_create("zs_handle", ZS_HANDLE_SIZE,\r\n0, 0, NULL);\r\nif (!pool->handle_cachep)\r\nreturn 1;\r\npool->zspage_cachep = kmem_cache_create("zspage", sizeof(struct zspage),\r\n0, 0, NULL);\r\nif (!pool->zspage_cachep) {\r\nkmem_cache_destroy(pool->handle_cachep);\r\npool->handle_cachep = NULL;\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void destroy_cache(struct zs_pool *pool)\r\n{\r\nkmem_cache_destroy(pool->handle_cachep);\r\nkmem_cache_destroy(pool->zspage_cachep);\r\n}\r\nstatic unsigned long cache_alloc_handle(struct zs_pool *pool, gfp_t gfp)\r\n{\r\nreturn (unsigned long)kmem_cache_alloc(pool->handle_cachep,\r\ngfp & ~(__GFP_HIGHMEM|__GFP_MOVABLE));\r\n}\r\nstatic void cache_free_handle(struct zs_pool *pool, unsigned long handle)\r\n{\r\nkmem_cache_free(pool->handle_cachep, (void *)handle);\r\n}\r\nstatic struct zspage *cache_alloc_zspage(struct zs_pool *pool, gfp_t flags)\r\n{\r\nreturn kmem_cache_alloc(pool->zspage_cachep,\r\nflags & ~(__GFP_HIGHMEM|__GFP_MOVABLE));\r\n}\r\nstatic void cache_free_zspage(struct zs_pool *pool, struct zspage *zspage)\r\n{\r\nkmem_cache_free(pool->zspage_cachep, zspage);\r\n}\r\nstatic void record_obj(unsigned long handle, unsigned long obj)\r\n{\r\nWRITE_ONCE(*(unsigned long *)handle, obj);\r\n}\r\nstatic void *zs_zpool_create(const char *name, gfp_t gfp,\r\nconst struct zpool_ops *zpool_ops,\r\nstruct zpool *zpool)\r\n{\r\nreturn zs_create_pool(name);\r\n}\r\nstatic void zs_zpool_destroy(void *pool)\r\n{\r\nzs_destroy_pool(pool);\r\n}\r\nstatic int zs_zpool_malloc(void *pool, size_t size, gfp_t gfp,\r\nunsigned long *handle)\r\n{\r\n*handle = zs_malloc(pool, size, gfp);\r\nreturn *handle ? 0 : -1;\r\n}\r\nstatic void zs_zpool_free(void *pool, unsigned long handle)\r\n{\r\nzs_free(pool, handle);\r\n}\r\nstatic int zs_zpool_shrink(void *pool, unsigned int pages,\r\nunsigned int *reclaimed)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic void *zs_zpool_map(void *pool, unsigned long handle,\r\nenum zpool_mapmode mm)\r\n{\r\nenum zs_mapmode zs_mm;\r\nswitch (mm) {\r\ncase ZPOOL_MM_RO:\r\nzs_mm = ZS_MM_RO;\r\nbreak;\r\ncase ZPOOL_MM_WO:\r\nzs_mm = ZS_MM_WO;\r\nbreak;\r\ncase ZPOOL_MM_RW:\r\ndefault:\r\nzs_mm = ZS_MM_RW;\r\nbreak;\r\n}\r\nreturn zs_map_object(pool, handle, zs_mm);\r\n}\r\nstatic void zs_zpool_unmap(void *pool, unsigned long handle)\r\n{\r\nzs_unmap_object(pool, handle);\r\n}\r\nstatic u64 zs_zpool_total_size(void *pool)\r\n{\r\nreturn zs_get_total_pages(pool) << PAGE_SHIFT;\r\n}\r\nstatic bool is_zspage_isolated(struct zspage *zspage)\r\n{\r\nreturn zspage->isolated;\r\n}\r\nstatic int is_first_page(struct page *page)\r\n{\r\nreturn PagePrivate(page);\r\n}\r\nstatic inline int get_zspage_inuse(struct zspage *zspage)\r\n{\r\nreturn zspage->inuse;\r\n}\r\nstatic inline void set_zspage_inuse(struct zspage *zspage, int val)\r\n{\r\nzspage->inuse = val;\r\n}\r\nstatic inline void mod_zspage_inuse(struct zspage *zspage, int val)\r\n{\r\nzspage->inuse += val;\r\n}\r\nstatic inline struct page *get_first_page(struct zspage *zspage)\r\n{\r\nstruct page *first_page = zspage->first_page;\r\nVM_BUG_ON_PAGE(!is_first_page(first_page), first_page);\r\nreturn first_page;\r\n}\r\nstatic inline int get_first_obj_offset(struct page *page)\r\n{\r\nreturn page->units;\r\n}\r\nstatic inline void set_first_obj_offset(struct page *page, int offset)\r\n{\r\npage->units = offset;\r\n}\r\nstatic inline unsigned int get_freeobj(struct zspage *zspage)\r\n{\r\nreturn zspage->freeobj;\r\n}\r\nstatic inline void set_freeobj(struct zspage *zspage, unsigned int obj)\r\n{\r\nzspage->freeobj = obj;\r\n}\r\nstatic void get_zspage_mapping(struct zspage *zspage,\r\nunsigned int *class_idx,\r\nenum fullness_group *fullness)\r\n{\r\nBUG_ON(zspage->magic != ZSPAGE_MAGIC);\r\n*fullness = zspage->fullness;\r\n*class_idx = zspage->class;\r\n}\r\nstatic void set_zspage_mapping(struct zspage *zspage,\r\nunsigned int class_idx,\r\nenum fullness_group fullness)\r\n{\r\nzspage->class = class_idx;\r\nzspage->fullness = fullness;\r\n}\r\nstatic int get_size_class_index(int size)\r\n{\r\nint idx = 0;\r\nif (likely(size > ZS_MIN_ALLOC_SIZE))\r\nidx = DIV_ROUND_UP(size - ZS_MIN_ALLOC_SIZE,\r\nZS_SIZE_CLASS_DELTA);\r\nreturn min(zs_size_classes - 1, idx);\r\n}\r\nstatic inline void zs_stat_inc(struct size_class *class,\r\nenum zs_stat_type type, unsigned long cnt)\r\n{\r\nclass->stats.objs[type] += cnt;\r\n}\r\nstatic inline void zs_stat_dec(struct size_class *class,\r\nenum zs_stat_type type, unsigned long cnt)\r\n{\r\nclass->stats.objs[type] -= cnt;\r\n}\r\nstatic inline unsigned long zs_stat_get(struct size_class *class,\r\nenum zs_stat_type type)\r\n{\r\nreturn class->stats.objs[type];\r\n}\r\nstatic void __init zs_stat_init(void)\r\n{\r\nif (!debugfs_initialized()) {\r\npr_warn("debugfs not available, stat dir not created\n");\r\nreturn;\r\n}\r\nzs_stat_root = debugfs_create_dir("zsmalloc", NULL);\r\nif (!zs_stat_root)\r\npr_warn("debugfs 'zsmalloc' stat dir creation failed\n");\r\n}\r\nstatic void __exit zs_stat_exit(void)\r\n{\r\ndebugfs_remove_recursive(zs_stat_root);\r\n}\r\nstatic int zs_stats_size_show(struct seq_file *s, void *v)\r\n{\r\nint i;\r\nstruct zs_pool *pool = s->private;\r\nstruct size_class *class;\r\nint objs_per_zspage;\r\nunsigned long class_almost_full, class_almost_empty;\r\nunsigned long obj_allocated, obj_used, pages_used, freeable;\r\nunsigned long total_class_almost_full = 0, total_class_almost_empty = 0;\r\nunsigned long total_objs = 0, total_used_objs = 0, total_pages = 0;\r\nunsigned long total_freeable = 0;\r\nseq_printf(s, " %5s %5s %11s %12s %13s %10s %10s %16s %8s\n",\r\n"class", "size", "almost_full", "almost_empty",\r\n"obj_allocated", "obj_used", "pages_used",\r\n"pages_per_zspage", "freeable");\r\nfor (i = 0; i < zs_size_classes; i++) {\r\nclass = pool->size_class[i];\r\nif (class->index != i)\r\ncontinue;\r\nspin_lock(&class->lock);\r\nclass_almost_full = zs_stat_get(class, CLASS_ALMOST_FULL);\r\nclass_almost_empty = zs_stat_get(class, CLASS_ALMOST_EMPTY);\r\nobj_allocated = zs_stat_get(class, OBJ_ALLOCATED);\r\nobj_used = zs_stat_get(class, OBJ_USED);\r\nfreeable = zs_can_compact(class);\r\nspin_unlock(&class->lock);\r\nobjs_per_zspage = class->objs_per_zspage;\r\npages_used = obj_allocated / objs_per_zspage *\r\nclass->pages_per_zspage;\r\nseq_printf(s, " %5u %5u %11lu %12lu %13lu"\r\n" %10lu %10lu %16d %8lu\n",\r\ni, class->size, class_almost_full, class_almost_empty,\r\nobj_allocated, obj_used, pages_used,\r\nclass->pages_per_zspage, freeable);\r\ntotal_class_almost_full += class_almost_full;\r\ntotal_class_almost_empty += class_almost_empty;\r\ntotal_objs += obj_allocated;\r\ntotal_used_objs += obj_used;\r\ntotal_pages += pages_used;\r\ntotal_freeable += freeable;\r\n}\r\nseq_puts(s, "\n");\r\nseq_printf(s, " %5s %5s %11lu %12lu %13lu %10lu %10lu %16s %8lu\n",\r\n"Total", "", total_class_almost_full,\r\ntotal_class_almost_empty, total_objs,\r\ntotal_used_objs, total_pages, "", total_freeable);\r\nreturn 0;\r\n}\r\nstatic int zs_stats_size_open(struct inode *inode, struct file *file)\r\n{\r\nreturn single_open(file, zs_stats_size_show, inode->i_private);\r\n}\r\nstatic void zs_pool_stat_create(struct zs_pool *pool, const char *name)\r\n{\r\nstruct dentry *entry;\r\nif (!zs_stat_root) {\r\npr_warn("no root stat dir, not creating <%s> stat dir\n", name);\r\nreturn;\r\n}\r\nentry = debugfs_create_dir(name, zs_stat_root);\r\nif (!entry) {\r\npr_warn("debugfs dir <%s> creation failed\n", name);\r\nreturn;\r\n}\r\npool->stat_dentry = entry;\r\nentry = debugfs_create_file("classes", S_IFREG | S_IRUGO,\r\npool->stat_dentry, pool, &zs_stat_size_ops);\r\nif (!entry) {\r\npr_warn("%s: debugfs file entry <%s> creation failed\n",\r\nname, "classes");\r\ndebugfs_remove_recursive(pool->stat_dentry);\r\npool->stat_dentry = NULL;\r\n}\r\n}\r\nstatic void zs_pool_stat_destroy(struct zs_pool *pool)\r\n{\r\ndebugfs_remove_recursive(pool->stat_dentry);\r\n}\r\nstatic void __init zs_stat_init(void)\r\n{\r\n}\r\nstatic void __exit zs_stat_exit(void)\r\n{\r\n}\r\nstatic inline void zs_pool_stat_create(struct zs_pool *pool, const char *name)\r\n{\r\n}\r\nstatic inline void zs_pool_stat_destroy(struct zs_pool *pool)\r\n{\r\n}\r\nstatic enum fullness_group get_fullness_group(struct size_class *class,\r\nstruct zspage *zspage)\r\n{\r\nint inuse, objs_per_zspage;\r\nenum fullness_group fg;\r\ninuse = get_zspage_inuse(zspage);\r\nobjs_per_zspage = class->objs_per_zspage;\r\nif (inuse == 0)\r\nfg = ZS_EMPTY;\r\nelse if (inuse == objs_per_zspage)\r\nfg = ZS_FULL;\r\nelse if (inuse <= 3 * objs_per_zspage / fullness_threshold_frac)\r\nfg = ZS_ALMOST_EMPTY;\r\nelse\r\nfg = ZS_ALMOST_FULL;\r\nreturn fg;\r\n}\r\nstatic void insert_zspage(struct size_class *class,\r\nstruct zspage *zspage,\r\nenum fullness_group fullness)\r\n{\r\nstruct zspage *head;\r\nzs_stat_inc(class, fullness, 1);\r\nhead = list_first_entry_or_null(&class->fullness_list[fullness],\r\nstruct zspage, list);\r\nif (head) {\r\nif (get_zspage_inuse(zspage) < get_zspage_inuse(head)) {\r\nlist_add(&zspage->list, &head->list);\r\nreturn;\r\n}\r\n}\r\nlist_add(&zspage->list, &class->fullness_list[fullness]);\r\n}\r\nstatic void remove_zspage(struct size_class *class,\r\nstruct zspage *zspage,\r\nenum fullness_group fullness)\r\n{\r\nVM_BUG_ON(list_empty(&class->fullness_list[fullness]));\r\nVM_BUG_ON(is_zspage_isolated(zspage));\r\nlist_del_init(&zspage->list);\r\nzs_stat_dec(class, fullness, 1);\r\n}\r\nstatic enum fullness_group fix_fullness_group(struct size_class *class,\r\nstruct zspage *zspage)\r\n{\r\nint class_idx;\r\nenum fullness_group currfg, newfg;\r\nget_zspage_mapping(zspage, &class_idx, &currfg);\r\nnewfg = get_fullness_group(class, zspage);\r\nif (newfg == currfg)\r\ngoto out;\r\nif (!is_zspage_isolated(zspage)) {\r\nremove_zspage(class, zspage, currfg);\r\ninsert_zspage(class, zspage, newfg);\r\n}\r\nset_zspage_mapping(zspage, class_idx, newfg);\r\nout:\r\nreturn newfg;\r\n}\r\nstatic int get_pages_per_zspage(int class_size)\r\n{\r\nint i, max_usedpc = 0;\r\nint max_usedpc_order = 1;\r\nfor (i = 1; i <= ZS_MAX_PAGES_PER_ZSPAGE; i++) {\r\nint zspage_size;\r\nint waste, usedpc;\r\nzspage_size = i * PAGE_SIZE;\r\nwaste = zspage_size % class_size;\r\nusedpc = (zspage_size - waste) * 100 / zspage_size;\r\nif (usedpc > max_usedpc) {\r\nmax_usedpc = usedpc;\r\nmax_usedpc_order = i;\r\n}\r\n}\r\nreturn max_usedpc_order;\r\n}\r\nstatic struct zspage *get_zspage(struct page *page)\r\n{\r\nstruct zspage *zspage = (struct zspage *)page->private;\r\nBUG_ON(zspage->magic != ZSPAGE_MAGIC);\r\nreturn zspage;\r\n}\r\nstatic struct page *get_next_page(struct page *page)\r\n{\r\nif (unlikely(PageHugeObject(page)))\r\nreturn NULL;\r\nreturn page->freelist;\r\n}\r\nstatic void obj_to_location(unsigned long obj, struct page **page,\r\nunsigned int *obj_idx)\r\n{\r\nobj >>= OBJ_TAG_BITS;\r\n*page = pfn_to_page(obj >> OBJ_INDEX_BITS);\r\n*obj_idx = (obj & OBJ_INDEX_MASK);\r\n}\r\nstatic unsigned long location_to_obj(struct page *page, unsigned int obj_idx)\r\n{\r\nunsigned long obj;\r\nobj = page_to_pfn(page) << OBJ_INDEX_BITS;\r\nobj |= obj_idx & OBJ_INDEX_MASK;\r\nobj <<= OBJ_TAG_BITS;\r\nreturn obj;\r\n}\r\nstatic unsigned long handle_to_obj(unsigned long handle)\r\n{\r\nreturn *(unsigned long *)handle;\r\n}\r\nstatic unsigned long obj_to_head(struct page *page, void *obj)\r\n{\r\nif (unlikely(PageHugeObject(page))) {\r\nVM_BUG_ON_PAGE(!is_first_page(page), page);\r\nreturn page->index;\r\n} else\r\nreturn *(unsigned long *)obj;\r\n}\r\nstatic inline int testpin_tag(unsigned long handle)\r\n{\r\nreturn bit_spin_is_locked(HANDLE_PIN_BIT, (unsigned long *)handle);\r\n}\r\nstatic inline int trypin_tag(unsigned long handle)\r\n{\r\nreturn bit_spin_trylock(HANDLE_PIN_BIT, (unsigned long *)handle);\r\n}\r\nstatic void pin_tag(unsigned long handle)\r\n{\r\nbit_spin_lock(HANDLE_PIN_BIT, (unsigned long *)handle);\r\n}\r\nstatic void unpin_tag(unsigned long handle)\r\n{\r\nbit_spin_unlock(HANDLE_PIN_BIT, (unsigned long *)handle);\r\n}\r\nstatic void reset_page(struct page *page)\r\n{\r\n__ClearPageMovable(page);\r\nClearPagePrivate(page);\r\nset_page_private(page, 0);\r\npage_mapcount_reset(page);\r\nClearPageHugeObject(page);\r\npage->freelist = NULL;\r\n}\r\nvoid lock_zspage(struct zspage *zspage)\r\n{\r\nstruct page *page = get_first_page(zspage);\r\ndo {\r\nlock_page(page);\r\n} while ((page = get_next_page(page)) != NULL);\r\n}\r\nint trylock_zspage(struct zspage *zspage)\r\n{\r\nstruct page *cursor, *fail;\r\nfor (cursor = get_first_page(zspage); cursor != NULL; cursor =\r\nget_next_page(cursor)) {\r\nif (!trylock_page(cursor)) {\r\nfail = cursor;\r\ngoto unlock;\r\n}\r\n}\r\nreturn 1;\r\nunlock:\r\nfor (cursor = get_first_page(zspage); cursor != fail; cursor =\r\nget_next_page(cursor))\r\nunlock_page(cursor);\r\nreturn 0;\r\n}\r\nstatic void __free_zspage(struct zs_pool *pool, struct size_class *class,\r\nstruct zspage *zspage)\r\n{\r\nstruct page *page, *next;\r\nenum fullness_group fg;\r\nunsigned int class_idx;\r\nget_zspage_mapping(zspage, &class_idx, &fg);\r\nassert_spin_locked(&class->lock);\r\nVM_BUG_ON(get_zspage_inuse(zspage));\r\nVM_BUG_ON(fg != ZS_EMPTY);\r\nnext = page = get_first_page(zspage);\r\ndo {\r\nVM_BUG_ON_PAGE(!PageLocked(page), page);\r\nnext = get_next_page(page);\r\nreset_page(page);\r\nunlock_page(page);\r\ndec_zone_page_state(page, NR_ZSPAGES);\r\nput_page(page);\r\npage = next;\r\n} while (page != NULL);\r\ncache_free_zspage(pool, zspage);\r\nzs_stat_dec(class, OBJ_ALLOCATED, class->objs_per_zspage);\r\natomic_long_sub(class->pages_per_zspage,\r\n&pool->pages_allocated);\r\n}\r\nstatic void free_zspage(struct zs_pool *pool, struct size_class *class,\r\nstruct zspage *zspage)\r\n{\r\nVM_BUG_ON(get_zspage_inuse(zspage));\r\nVM_BUG_ON(list_empty(&zspage->list));\r\nif (!trylock_zspage(zspage)) {\r\nkick_deferred_free(pool);\r\nreturn;\r\n}\r\nremove_zspage(class, zspage, ZS_EMPTY);\r\n__free_zspage(pool, class, zspage);\r\n}\r\nstatic void init_zspage(struct size_class *class, struct zspage *zspage)\r\n{\r\nunsigned int freeobj = 1;\r\nunsigned long off = 0;\r\nstruct page *page = get_first_page(zspage);\r\nwhile (page) {\r\nstruct page *next_page;\r\nstruct link_free *link;\r\nvoid *vaddr;\r\nset_first_obj_offset(page, off);\r\nvaddr = kmap_atomic(page);\r\nlink = (struct link_free *)vaddr + off / sizeof(*link);\r\nwhile ((off += class->size) < PAGE_SIZE) {\r\nlink->next = freeobj++ << OBJ_TAG_BITS;\r\nlink += class->size / sizeof(*link);\r\n}\r\nnext_page = get_next_page(page);\r\nif (next_page) {\r\nlink->next = freeobj++ << OBJ_TAG_BITS;\r\n} else {\r\nlink->next = -1 << OBJ_TAG_BITS;\r\n}\r\nkunmap_atomic(vaddr);\r\npage = next_page;\r\noff %= PAGE_SIZE;\r\n}\r\nset_freeobj(zspage, 0);\r\n}\r\nstatic void create_page_chain(struct size_class *class, struct zspage *zspage,\r\nstruct page *pages[])\r\n{\r\nint i;\r\nstruct page *page;\r\nstruct page *prev_page = NULL;\r\nint nr_pages = class->pages_per_zspage;\r\nfor (i = 0; i < nr_pages; i++) {\r\npage = pages[i];\r\nset_page_private(page, (unsigned long)zspage);\r\npage->freelist = NULL;\r\nif (i == 0) {\r\nzspage->first_page = page;\r\nSetPagePrivate(page);\r\nif (unlikely(class->objs_per_zspage == 1 &&\r\nclass->pages_per_zspage == 1))\r\nSetPageHugeObject(page);\r\n} else {\r\nprev_page->freelist = page;\r\n}\r\nprev_page = page;\r\n}\r\n}\r\nstatic struct zspage *alloc_zspage(struct zs_pool *pool,\r\nstruct size_class *class,\r\ngfp_t gfp)\r\n{\r\nint i;\r\nstruct page *pages[ZS_MAX_PAGES_PER_ZSPAGE];\r\nstruct zspage *zspage = cache_alloc_zspage(pool, gfp);\r\nif (!zspage)\r\nreturn NULL;\r\nmemset(zspage, 0, sizeof(struct zspage));\r\nzspage->magic = ZSPAGE_MAGIC;\r\nmigrate_lock_init(zspage);\r\nfor (i = 0; i < class->pages_per_zspage; i++) {\r\nstruct page *page;\r\npage = alloc_page(gfp);\r\nif (!page) {\r\nwhile (--i >= 0) {\r\ndec_zone_page_state(pages[i], NR_ZSPAGES);\r\n__free_page(pages[i]);\r\n}\r\ncache_free_zspage(pool, zspage);\r\nreturn NULL;\r\n}\r\ninc_zone_page_state(page, NR_ZSPAGES);\r\npages[i] = page;\r\n}\r\ncreate_page_chain(class, zspage, pages);\r\ninit_zspage(class, zspage);\r\nreturn zspage;\r\n}\r\nstatic struct zspage *find_get_zspage(struct size_class *class)\r\n{\r\nint i;\r\nstruct zspage *zspage;\r\nfor (i = ZS_ALMOST_FULL; i >= ZS_EMPTY; i--) {\r\nzspage = list_first_entry_or_null(&class->fullness_list[i],\r\nstruct zspage, list);\r\nif (zspage)\r\nbreak;\r\n}\r\nreturn zspage;\r\n}\r\nstatic inline int __zs_cpu_up(struct mapping_area *area)\r\n{\r\nif (area->vm)\r\nreturn 0;\r\narea->vm = alloc_vm_area(PAGE_SIZE * 2, NULL);\r\nif (!area->vm)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic inline void __zs_cpu_down(struct mapping_area *area)\r\n{\r\nif (area->vm)\r\nfree_vm_area(area->vm);\r\narea->vm = NULL;\r\n}\r\nstatic inline void *__zs_map_object(struct mapping_area *area,\r\nstruct page *pages[2], int off, int size)\r\n{\r\nBUG_ON(map_vm_area(area->vm, PAGE_KERNEL, pages));\r\narea->vm_addr = area->vm->addr;\r\nreturn area->vm_addr + off;\r\n}\r\nstatic inline void __zs_unmap_object(struct mapping_area *area,\r\nstruct page *pages[2], int off, int size)\r\n{\r\nunsigned long addr = (unsigned long)area->vm_addr;\r\nunmap_kernel_range(addr, PAGE_SIZE * 2);\r\n}\r\nstatic inline int __zs_cpu_up(struct mapping_area *area)\r\n{\r\nif (area->vm_buf)\r\nreturn 0;\r\narea->vm_buf = kmalloc(ZS_MAX_ALLOC_SIZE, GFP_KERNEL);\r\nif (!area->vm_buf)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic inline void __zs_cpu_down(struct mapping_area *area)\r\n{\r\nkfree(area->vm_buf);\r\narea->vm_buf = NULL;\r\n}\r\nstatic void *__zs_map_object(struct mapping_area *area,\r\nstruct page *pages[2], int off, int size)\r\n{\r\nint sizes[2];\r\nvoid *addr;\r\nchar *buf = area->vm_buf;\r\npagefault_disable();\r\nif (area->vm_mm == ZS_MM_WO)\r\ngoto out;\r\nsizes[0] = PAGE_SIZE - off;\r\nsizes[1] = size - sizes[0];\r\naddr = kmap_atomic(pages[0]);\r\nmemcpy(buf, addr + off, sizes[0]);\r\nkunmap_atomic(addr);\r\naddr = kmap_atomic(pages[1]);\r\nmemcpy(buf + sizes[0], addr, sizes[1]);\r\nkunmap_atomic(addr);\r\nout:\r\nreturn area->vm_buf;\r\n}\r\nstatic void __zs_unmap_object(struct mapping_area *area,\r\nstruct page *pages[2], int off, int size)\r\n{\r\nint sizes[2];\r\nvoid *addr;\r\nchar *buf;\r\nif (area->vm_mm == ZS_MM_RO)\r\ngoto out;\r\nbuf = area->vm_buf;\r\nbuf = buf + ZS_HANDLE_SIZE;\r\nsize -= ZS_HANDLE_SIZE;\r\noff += ZS_HANDLE_SIZE;\r\nsizes[0] = PAGE_SIZE - off;\r\nsizes[1] = size - sizes[0];\r\naddr = kmap_atomic(pages[0]);\r\nmemcpy(addr + off, buf, sizes[0]);\r\nkunmap_atomic(addr);\r\naddr = kmap_atomic(pages[1]);\r\nmemcpy(addr, buf + sizes[0], sizes[1]);\r\nkunmap_atomic(addr);\r\nout:\r\npagefault_enable();\r\n}\r\nstatic int zs_cpu_prepare(unsigned int cpu)\r\n{\r\nstruct mapping_area *area;\r\narea = &per_cpu(zs_map_area, cpu);\r\nreturn __zs_cpu_up(area);\r\n}\r\nstatic int zs_cpu_dead(unsigned int cpu)\r\n{\r\nstruct mapping_area *area;\r\narea = &per_cpu(zs_map_area, cpu);\r\n__zs_cpu_down(area);\r\nreturn 0;\r\n}\r\nstatic void __init init_zs_size_classes(void)\r\n{\r\nint nr;\r\nnr = (ZS_MAX_ALLOC_SIZE - ZS_MIN_ALLOC_SIZE) / ZS_SIZE_CLASS_DELTA + 1;\r\nif ((ZS_MAX_ALLOC_SIZE - ZS_MIN_ALLOC_SIZE) % ZS_SIZE_CLASS_DELTA)\r\nnr += 1;\r\nzs_size_classes = nr;\r\n}\r\nstatic bool can_merge(struct size_class *prev, int pages_per_zspage,\r\nint objs_per_zspage)\r\n{\r\nif (prev->pages_per_zspage == pages_per_zspage &&\r\nprev->objs_per_zspage == objs_per_zspage)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic bool zspage_full(struct size_class *class, struct zspage *zspage)\r\n{\r\nreturn get_zspage_inuse(zspage) == class->objs_per_zspage;\r\n}\r\nunsigned long zs_get_total_pages(struct zs_pool *pool)\r\n{\r\nreturn atomic_long_read(&pool->pages_allocated);\r\n}\r\nvoid *zs_map_object(struct zs_pool *pool, unsigned long handle,\r\nenum zs_mapmode mm)\r\n{\r\nstruct zspage *zspage;\r\nstruct page *page;\r\nunsigned long obj, off;\r\nunsigned int obj_idx;\r\nunsigned int class_idx;\r\nenum fullness_group fg;\r\nstruct size_class *class;\r\nstruct mapping_area *area;\r\nstruct page *pages[2];\r\nvoid *ret;\r\nWARN_ON_ONCE(in_interrupt());\r\npin_tag(handle);\r\nobj = handle_to_obj(handle);\r\nobj_to_location(obj, &page, &obj_idx);\r\nzspage = get_zspage(page);\r\nmigrate_read_lock(zspage);\r\nget_zspage_mapping(zspage, &class_idx, &fg);\r\nclass = pool->size_class[class_idx];\r\noff = (class->size * obj_idx) & ~PAGE_MASK;\r\narea = &get_cpu_var(zs_map_area);\r\narea->vm_mm = mm;\r\nif (off + class->size <= PAGE_SIZE) {\r\narea->vm_addr = kmap_atomic(page);\r\nret = area->vm_addr + off;\r\ngoto out;\r\n}\r\npages[0] = page;\r\npages[1] = get_next_page(page);\r\nBUG_ON(!pages[1]);\r\nret = __zs_map_object(area, pages, off, class->size);\r\nout:\r\nif (likely(!PageHugeObject(page)))\r\nret += ZS_HANDLE_SIZE;\r\nreturn ret;\r\n}\r\nvoid zs_unmap_object(struct zs_pool *pool, unsigned long handle)\r\n{\r\nstruct zspage *zspage;\r\nstruct page *page;\r\nunsigned long obj, off;\r\nunsigned int obj_idx;\r\nunsigned int class_idx;\r\nenum fullness_group fg;\r\nstruct size_class *class;\r\nstruct mapping_area *area;\r\nobj = handle_to_obj(handle);\r\nobj_to_location(obj, &page, &obj_idx);\r\nzspage = get_zspage(page);\r\nget_zspage_mapping(zspage, &class_idx, &fg);\r\nclass = pool->size_class[class_idx];\r\noff = (class->size * obj_idx) & ~PAGE_MASK;\r\narea = this_cpu_ptr(&zs_map_area);\r\nif (off + class->size <= PAGE_SIZE)\r\nkunmap_atomic(area->vm_addr);\r\nelse {\r\nstruct page *pages[2];\r\npages[0] = page;\r\npages[1] = get_next_page(page);\r\nBUG_ON(!pages[1]);\r\n__zs_unmap_object(area, pages, off, class->size);\r\n}\r\nput_cpu_var(zs_map_area);\r\nmigrate_read_unlock(zspage);\r\nunpin_tag(handle);\r\n}\r\nstatic unsigned long obj_malloc(struct size_class *class,\r\nstruct zspage *zspage, unsigned long handle)\r\n{\r\nint i, nr_page, offset;\r\nunsigned long obj;\r\nstruct link_free *link;\r\nstruct page *m_page;\r\nunsigned long m_offset;\r\nvoid *vaddr;\r\nhandle |= OBJ_ALLOCATED_TAG;\r\nobj = get_freeobj(zspage);\r\noffset = obj * class->size;\r\nnr_page = offset >> PAGE_SHIFT;\r\nm_offset = offset & ~PAGE_MASK;\r\nm_page = get_first_page(zspage);\r\nfor (i = 0; i < nr_page; i++)\r\nm_page = get_next_page(m_page);\r\nvaddr = kmap_atomic(m_page);\r\nlink = (struct link_free *)vaddr + m_offset / sizeof(*link);\r\nset_freeobj(zspage, link->next >> OBJ_TAG_BITS);\r\nif (likely(!PageHugeObject(m_page)))\r\nlink->handle = handle;\r\nelse\r\nzspage->first_page->index = handle;\r\nkunmap_atomic(vaddr);\r\nmod_zspage_inuse(zspage, 1);\r\nzs_stat_inc(class, OBJ_USED, 1);\r\nobj = location_to_obj(m_page, obj);\r\nreturn obj;\r\n}\r\nunsigned long zs_malloc(struct zs_pool *pool, size_t size, gfp_t gfp)\r\n{\r\nunsigned long handle, obj;\r\nstruct size_class *class;\r\nenum fullness_group newfg;\r\nstruct zspage *zspage;\r\nif (unlikely(!size || size > ZS_MAX_ALLOC_SIZE))\r\nreturn 0;\r\nhandle = cache_alloc_handle(pool, gfp);\r\nif (!handle)\r\nreturn 0;\r\nsize += ZS_HANDLE_SIZE;\r\nclass = pool->size_class[get_size_class_index(size)];\r\nspin_lock(&class->lock);\r\nzspage = find_get_zspage(class);\r\nif (likely(zspage)) {\r\nobj = obj_malloc(class, zspage, handle);\r\nfix_fullness_group(class, zspage);\r\nrecord_obj(handle, obj);\r\nspin_unlock(&class->lock);\r\nreturn handle;\r\n}\r\nspin_unlock(&class->lock);\r\nzspage = alloc_zspage(pool, class, gfp);\r\nif (!zspage) {\r\ncache_free_handle(pool, handle);\r\nreturn 0;\r\n}\r\nspin_lock(&class->lock);\r\nobj = obj_malloc(class, zspage, handle);\r\nnewfg = get_fullness_group(class, zspage);\r\ninsert_zspage(class, zspage, newfg);\r\nset_zspage_mapping(zspage, class->index, newfg);\r\nrecord_obj(handle, obj);\r\natomic_long_add(class->pages_per_zspage,\r\n&pool->pages_allocated);\r\nzs_stat_inc(class, OBJ_ALLOCATED, class->objs_per_zspage);\r\nSetZsPageMovable(pool, zspage);\r\nspin_unlock(&class->lock);\r\nreturn handle;\r\n}\r\nstatic void obj_free(struct size_class *class, unsigned long obj)\r\n{\r\nstruct link_free *link;\r\nstruct zspage *zspage;\r\nstruct page *f_page;\r\nunsigned long f_offset;\r\nunsigned int f_objidx;\r\nvoid *vaddr;\r\nobj &= ~OBJ_ALLOCATED_TAG;\r\nobj_to_location(obj, &f_page, &f_objidx);\r\nf_offset = (class->size * f_objidx) & ~PAGE_MASK;\r\nzspage = get_zspage(f_page);\r\nvaddr = kmap_atomic(f_page);\r\nlink = (struct link_free *)(vaddr + f_offset);\r\nlink->next = get_freeobj(zspage) << OBJ_TAG_BITS;\r\nkunmap_atomic(vaddr);\r\nset_freeobj(zspage, f_objidx);\r\nmod_zspage_inuse(zspage, -1);\r\nzs_stat_dec(class, OBJ_USED, 1);\r\n}\r\nvoid zs_free(struct zs_pool *pool, unsigned long handle)\r\n{\r\nstruct zspage *zspage;\r\nstruct page *f_page;\r\nunsigned long obj;\r\nunsigned int f_objidx;\r\nint class_idx;\r\nstruct size_class *class;\r\nenum fullness_group fullness;\r\nbool isolated;\r\nif (unlikely(!handle))\r\nreturn;\r\npin_tag(handle);\r\nobj = handle_to_obj(handle);\r\nobj_to_location(obj, &f_page, &f_objidx);\r\nzspage = get_zspage(f_page);\r\nmigrate_read_lock(zspage);\r\nget_zspage_mapping(zspage, &class_idx, &fullness);\r\nclass = pool->size_class[class_idx];\r\nspin_lock(&class->lock);\r\nobj_free(class, obj);\r\nfullness = fix_fullness_group(class, zspage);\r\nif (fullness != ZS_EMPTY) {\r\nmigrate_read_unlock(zspage);\r\ngoto out;\r\n}\r\nisolated = is_zspage_isolated(zspage);\r\nmigrate_read_unlock(zspage);\r\nif (likely(!isolated))\r\nfree_zspage(pool, class, zspage);\r\nout:\r\nspin_unlock(&class->lock);\r\nunpin_tag(handle);\r\ncache_free_handle(pool, handle);\r\n}\r\nstatic void zs_object_copy(struct size_class *class, unsigned long dst,\r\nunsigned long src)\r\n{\r\nstruct page *s_page, *d_page;\r\nunsigned int s_objidx, d_objidx;\r\nunsigned long s_off, d_off;\r\nvoid *s_addr, *d_addr;\r\nint s_size, d_size, size;\r\nint written = 0;\r\ns_size = d_size = class->size;\r\nobj_to_location(src, &s_page, &s_objidx);\r\nobj_to_location(dst, &d_page, &d_objidx);\r\ns_off = (class->size * s_objidx) & ~PAGE_MASK;\r\nd_off = (class->size * d_objidx) & ~PAGE_MASK;\r\nif (s_off + class->size > PAGE_SIZE)\r\ns_size = PAGE_SIZE - s_off;\r\nif (d_off + class->size > PAGE_SIZE)\r\nd_size = PAGE_SIZE - d_off;\r\ns_addr = kmap_atomic(s_page);\r\nd_addr = kmap_atomic(d_page);\r\nwhile (1) {\r\nsize = min(s_size, d_size);\r\nmemcpy(d_addr + d_off, s_addr + s_off, size);\r\nwritten += size;\r\nif (written == class->size)\r\nbreak;\r\ns_off += size;\r\ns_size -= size;\r\nd_off += size;\r\nd_size -= size;\r\nif (s_off >= PAGE_SIZE) {\r\nkunmap_atomic(d_addr);\r\nkunmap_atomic(s_addr);\r\ns_page = get_next_page(s_page);\r\ns_addr = kmap_atomic(s_page);\r\nd_addr = kmap_atomic(d_page);\r\ns_size = class->size - written;\r\ns_off = 0;\r\n}\r\nif (d_off >= PAGE_SIZE) {\r\nkunmap_atomic(d_addr);\r\nd_page = get_next_page(d_page);\r\nd_addr = kmap_atomic(d_page);\r\nd_size = class->size - written;\r\nd_off = 0;\r\n}\r\n}\r\nkunmap_atomic(d_addr);\r\nkunmap_atomic(s_addr);\r\n}\r\nstatic unsigned long find_alloced_obj(struct size_class *class,\r\nstruct page *page, int *obj_idx)\r\n{\r\nunsigned long head;\r\nint offset = 0;\r\nint index = *obj_idx;\r\nunsigned long handle = 0;\r\nvoid *addr = kmap_atomic(page);\r\noffset = get_first_obj_offset(page);\r\noffset += class->size * index;\r\nwhile (offset < PAGE_SIZE) {\r\nhead = obj_to_head(page, addr + offset);\r\nif (head & OBJ_ALLOCATED_TAG) {\r\nhandle = head & ~OBJ_ALLOCATED_TAG;\r\nif (trypin_tag(handle))\r\nbreak;\r\nhandle = 0;\r\n}\r\noffset += class->size;\r\nindex++;\r\n}\r\nkunmap_atomic(addr);\r\n*obj_idx = index;\r\nreturn handle;\r\n}\r\nstatic int migrate_zspage(struct zs_pool *pool, struct size_class *class,\r\nstruct zs_compact_control *cc)\r\n{\r\nunsigned long used_obj, free_obj;\r\nunsigned long handle;\r\nstruct page *s_page = cc->s_page;\r\nstruct page *d_page = cc->d_page;\r\nint obj_idx = cc->obj_idx;\r\nint ret = 0;\r\nwhile (1) {\r\nhandle = find_alloced_obj(class, s_page, &obj_idx);\r\nif (!handle) {\r\ns_page = get_next_page(s_page);\r\nif (!s_page)\r\nbreak;\r\nobj_idx = 0;\r\ncontinue;\r\n}\r\nif (zspage_full(class, get_zspage(d_page))) {\r\nunpin_tag(handle);\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nused_obj = handle_to_obj(handle);\r\nfree_obj = obj_malloc(class, get_zspage(d_page), handle);\r\nzs_object_copy(class, free_obj, used_obj);\r\nobj_idx++;\r\nfree_obj |= BIT(HANDLE_PIN_BIT);\r\nrecord_obj(handle, free_obj);\r\nunpin_tag(handle);\r\nobj_free(class, used_obj);\r\n}\r\ncc->s_page = s_page;\r\ncc->obj_idx = obj_idx;\r\nreturn ret;\r\n}\r\nstatic struct zspage *isolate_zspage(struct size_class *class, bool source)\r\n{\r\nint i;\r\nstruct zspage *zspage;\r\nenum fullness_group fg[2] = {ZS_ALMOST_EMPTY, ZS_ALMOST_FULL};\r\nif (!source) {\r\nfg[0] = ZS_ALMOST_FULL;\r\nfg[1] = ZS_ALMOST_EMPTY;\r\n}\r\nfor (i = 0; i < 2; i++) {\r\nzspage = list_first_entry_or_null(&class->fullness_list[fg[i]],\r\nstruct zspage, list);\r\nif (zspage) {\r\nVM_BUG_ON(is_zspage_isolated(zspage));\r\nremove_zspage(class, zspage, fg[i]);\r\nreturn zspage;\r\n}\r\n}\r\nreturn zspage;\r\n}\r\nstatic enum fullness_group putback_zspage(struct size_class *class,\r\nstruct zspage *zspage)\r\n{\r\nenum fullness_group fullness;\r\nVM_BUG_ON(is_zspage_isolated(zspage));\r\nfullness = get_fullness_group(class, zspage);\r\ninsert_zspage(class, zspage, fullness);\r\nset_zspage_mapping(zspage, class->index, fullness);\r\nreturn fullness;\r\n}\r\nstatic struct dentry *zs_mount(struct file_system_type *fs_type,\r\nint flags, const char *dev_name, void *data)\r\n{\r\nstatic const struct dentry_operations ops = {\r\n.d_dname = simple_dname,\r\n};\r\nreturn mount_pseudo(fs_type, "zsmalloc:", NULL, &ops, ZSMALLOC_MAGIC);\r\n}\r\nstatic int zsmalloc_mount(void)\r\n{\r\nint ret = 0;\r\nzsmalloc_mnt = kern_mount(&zsmalloc_fs);\r\nif (IS_ERR(zsmalloc_mnt))\r\nret = PTR_ERR(zsmalloc_mnt);\r\nreturn ret;\r\n}\r\nstatic void zsmalloc_unmount(void)\r\n{\r\nkern_unmount(zsmalloc_mnt);\r\n}\r\nstatic void migrate_lock_init(struct zspage *zspage)\r\n{\r\nrwlock_init(&zspage->lock);\r\n}\r\nstatic void migrate_read_lock(struct zspage *zspage)\r\n{\r\nread_lock(&zspage->lock);\r\n}\r\nstatic void migrate_read_unlock(struct zspage *zspage)\r\n{\r\nread_unlock(&zspage->lock);\r\n}\r\nstatic void migrate_write_lock(struct zspage *zspage)\r\n{\r\nwrite_lock(&zspage->lock);\r\n}\r\nstatic void migrate_write_unlock(struct zspage *zspage)\r\n{\r\nwrite_unlock(&zspage->lock);\r\n}\r\nstatic void inc_zspage_isolation(struct zspage *zspage)\r\n{\r\nzspage->isolated++;\r\n}\r\nstatic void dec_zspage_isolation(struct zspage *zspage)\r\n{\r\nzspage->isolated--;\r\n}\r\nstatic void replace_sub_page(struct size_class *class, struct zspage *zspage,\r\nstruct page *newpage, struct page *oldpage)\r\n{\r\nstruct page *page;\r\nstruct page *pages[ZS_MAX_PAGES_PER_ZSPAGE] = {NULL, };\r\nint idx = 0;\r\npage = get_first_page(zspage);\r\ndo {\r\nif (page == oldpage)\r\npages[idx] = newpage;\r\nelse\r\npages[idx] = page;\r\nidx++;\r\n} while ((page = get_next_page(page)) != NULL);\r\ncreate_page_chain(class, zspage, pages);\r\nset_first_obj_offset(newpage, get_first_obj_offset(oldpage));\r\nif (unlikely(PageHugeObject(oldpage)))\r\nnewpage->index = oldpage->index;\r\n__SetPageMovable(newpage, page_mapping(oldpage));\r\n}\r\nbool zs_page_isolate(struct page *page, isolate_mode_t mode)\r\n{\r\nstruct zs_pool *pool;\r\nstruct size_class *class;\r\nint class_idx;\r\nenum fullness_group fullness;\r\nstruct zspage *zspage;\r\nstruct address_space *mapping;\r\nVM_BUG_ON_PAGE(!PageMovable(page), page);\r\nVM_BUG_ON_PAGE(PageIsolated(page), page);\r\nzspage = get_zspage(page);\r\nget_zspage_mapping(zspage, &class_idx, &fullness);\r\nmapping = page_mapping(page);\r\npool = mapping->private_data;\r\nclass = pool->size_class[class_idx];\r\nspin_lock(&class->lock);\r\nif (get_zspage_inuse(zspage) == 0) {\r\nspin_unlock(&class->lock);\r\nreturn false;\r\n}\r\nif (list_empty(&zspage->list) && !is_zspage_isolated(zspage)) {\r\nspin_unlock(&class->lock);\r\nreturn false;\r\n}\r\nif (!list_empty(&zspage->list) && !is_zspage_isolated(zspage)) {\r\nget_zspage_mapping(zspage, &class_idx, &fullness);\r\nremove_zspage(class, zspage, fullness);\r\n}\r\ninc_zspage_isolation(zspage);\r\nspin_unlock(&class->lock);\r\nreturn true;\r\n}\r\nint zs_page_migrate(struct address_space *mapping, struct page *newpage,\r\nstruct page *page, enum migrate_mode mode)\r\n{\r\nstruct zs_pool *pool;\r\nstruct size_class *class;\r\nint class_idx;\r\nenum fullness_group fullness;\r\nstruct zspage *zspage;\r\nstruct page *dummy;\r\nvoid *s_addr, *d_addr, *addr;\r\nint offset, pos;\r\nunsigned long handle, head;\r\nunsigned long old_obj, new_obj;\r\nunsigned int obj_idx;\r\nint ret = -EAGAIN;\r\nVM_BUG_ON_PAGE(!PageMovable(page), page);\r\nVM_BUG_ON_PAGE(!PageIsolated(page), page);\r\nzspage = get_zspage(page);\r\nmigrate_write_lock(zspage);\r\nget_zspage_mapping(zspage, &class_idx, &fullness);\r\npool = mapping->private_data;\r\nclass = pool->size_class[class_idx];\r\noffset = get_first_obj_offset(page);\r\nspin_lock(&class->lock);\r\nif (!get_zspage_inuse(zspage)) {\r\nret = -EBUSY;\r\ngoto unlock_class;\r\n}\r\npos = offset;\r\ns_addr = kmap_atomic(page);\r\nwhile (pos < PAGE_SIZE) {\r\nhead = obj_to_head(page, s_addr + pos);\r\nif (head & OBJ_ALLOCATED_TAG) {\r\nhandle = head & ~OBJ_ALLOCATED_TAG;\r\nif (!trypin_tag(handle))\r\ngoto unpin_objects;\r\n}\r\npos += class->size;\r\n}\r\nd_addr = kmap_atomic(newpage);\r\nmemcpy(d_addr, s_addr, PAGE_SIZE);\r\nkunmap_atomic(d_addr);\r\nfor (addr = s_addr + offset; addr < s_addr + pos;\r\naddr += class->size) {\r\nhead = obj_to_head(page, addr);\r\nif (head & OBJ_ALLOCATED_TAG) {\r\nhandle = head & ~OBJ_ALLOCATED_TAG;\r\nif (!testpin_tag(handle))\r\nBUG();\r\nold_obj = handle_to_obj(handle);\r\nobj_to_location(old_obj, &dummy, &obj_idx);\r\nnew_obj = (unsigned long)location_to_obj(newpage,\r\nobj_idx);\r\nnew_obj |= BIT(HANDLE_PIN_BIT);\r\nrecord_obj(handle, new_obj);\r\n}\r\n}\r\nreplace_sub_page(class, zspage, newpage, page);\r\nget_page(newpage);\r\ndec_zspage_isolation(zspage);\r\nif (!is_zspage_isolated(zspage))\r\nputback_zspage(class, zspage);\r\nreset_page(page);\r\nput_page(page);\r\npage = newpage;\r\nret = MIGRATEPAGE_SUCCESS;\r\nunpin_objects:\r\nfor (addr = s_addr + offset; addr < s_addr + pos;\r\naddr += class->size) {\r\nhead = obj_to_head(page, addr);\r\nif (head & OBJ_ALLOCATED_TAG) {\r\nhandle = head & ~OBJ_ALLOCATED_TAG;\r\nif (!testpin_tag(handle))\r\nBUG();\r\nunpin_tag(handle);\r\n}\r\n}\r\nkunmap_atomic(s_addr);\r\nunlock_class:\r\nspin_unlock(&class->lock);\r\nmigrate_write_unlock(zspage);\r\nreturn ret;\r\n}\r\nvoid zs_page_putback(struct page *page)\r\n{\r\nstruct zs_pool *pool;\r\nstruct size_class *class;\r\nint class_idx;\r\nenum fullness_group fg;\r\nstruct address_space *mapping;\r\nstruct zspage *zspage;\r\nVM_BUG_ON_PAGE(!PageMovable(page), page);\r\nVM_BUG_ON_PAGE(!PageIsolated(page), page);\r\nzspage = get_zspage(page);\r\nget_zspage_mapping(zspage, &class_idx, &fg);\r\nmapping = page_mapping(page);\r\npool = mapping->private_data;\r\nclass = pool->size_class[class_idx];\r\nspin_lock(&class->lock);\r\ndec_zspage_isolation(zspage);\r\nif (!is_zspage_isolated(zspage)) {\r\nfg = putback_zspage(class, zspage);\r\nif (fg == ZS_EMPTY)\r\nschedule_work(&pool->free_work);\r\n}\r\nspin_unlock(&class->lock);\r\n}\r\nstatic int zs_register_migration(struct zs_pool *pool)\r\n{\r\npool->inode = alloc_anon_inode(zsmalloc_mnt->mnt_sb);\r\nif (IS_ERR(pool->inode)) {\r\npool->inode = NULL;\r\nreturn 1;\r\n}\r\npool->inode->i_mapping->private_data = pool;\r\npool->inode->i_mapping->a_ops = &zsmalloc_aops;\r\nreturn 0;\r\n}\r\nstatic void zs_unregister_migration(struct zs_pool *pool)\r\n{\r\nflush_work(&pool->free_work);\r\niput(pool->inode);\r\n}\r\nstatic void async_free_zspage(struct work_struct *work)\r\n{\r\nint i;\r\nstruct size_class *class;\r\nunsigned int class_idx;\r\nenum fullness_group fullness;\r\nstruct zspage *zspage, *tmp;\r\nLIST_HEAD(free_pages);\r\nstruct zs_pool *pool = container_of(work, struct zs_pool,\r\nfree_work);\r\nfor (i = 0; i < zs_size_classes; i++) {\r\nclass = pool->size_class[i];\r\nif (class->index != i)\r\ncontinue;\r\nspin_lock(&class->lock);\r\nlist_splice_init(&class->fullness_list[ZS_EMPTY], &free_pages);\r\nspin_unlock(&class->lock);\r\n}\r\nlist_for_each_entry_safe(zspage, tmp, &free_pages, list) {\r\nlist_del(&zspage->list);\r\nlock_zspage(zspage);\r\nget_zspage_mapping(zspage, &class_idx, &fullness);\r\nVM_BUG_ON(fullness != ZS_EMPTY);\r\nclass = pool->size_class[class_idx];\r\nspin_lock(&class->lock);\r\n__free_zspage(pool, pool->size_class[class_idx], zspage);\r\nspin_unlock(&class->lock);\r\n}\r\n}\r\nstatic void kick_deferred_free(struct zs_pool *pool)\r\n{\r\nschedule_work(&pool->free_work);\r\n}\r\nstatic void init_deferred_free(struct zs_pool *pool)\r\n{\r\nINIT_WORK(&pool->free_work, async_free_zspage);\r\n}\r\nstatic void SetZsPageMovable(struct zs_pool *pool, struct zspage *zspage)\r\n{\r\nstruct page *page = get_first_page(zspage);\r\ndo {\r\nWARN_ON(!trylock_page(page));\r\n__SetPageMovable(page, pool->inode->i_mapping);\r\nunlock_page(page);\r\n} while ((page = get_next_page(page)) != NULL);\r\n}\r\nstatic unsigned long zs_can_compact(struct size_class *class)\r\n{\r\nunsigned long obj_wasted;\r\nunsigned long obj_allocated = zs_stat_get(class, OBJ_ALLOCATED);\r\nunsigned long obj_used = zs_stat_get(class, OBJ_USED);\r\nif (obj_allocated <= obj_used)\r\nreturn 0;\r\nobj_wasted = obj_allocated - obj_used;\r\nobj_wasted /= class->objs_per_zspage;\r\nreturn obj_wasted * class->pages_per_zspage;\r\n}\r\nstatic void __zs_compact(struct zs_pool *pool, struct size_class *class)\r\n{\r\nstruct zs_compact_control cc;\r\nstruct zspage *src_zspage;\r\nstruct zspage *dst_zspage = NULL;\r\nspin_lock(&class->lock);\r\nwhile ((src_zspage = isolate_zspage(class, true))) {\r\nif (!zs_can_compact(class))\r\nbreak;\r\ncc.obj_idx = 0;\r\ncc.s_page = get_first_page(src_zspage);\r\nwhile ((dst_zspage = isolate_zspage(class, false))) {\r\ncc.d_page = get_first_page(dst_zspage);\r\nif (!migrate_zspage(pool, class, &cc))\r\nbreak;\r\nputback_zspage(class, dst_zspage);\r\n}\r\nif (dst_zspage == NULL)\r\nbreak;\r\nputback_zspage(class, dst_zspage);\r\nif (putback_zspage(class, src_zspage) == ZS_EMPTY) {\r\nfree_zspage(pool, class, src_zspage);\r\npool->stats.pages_compacted += class->pages_per_zspage;\r\n}\r\nspin_unlock(&class->lock);\r\ncond_resched();\r\nspin_lock(&class->lock);\r\n}\r\nif (src_zspage)\r\nputback_zspage(class, src_zspage);\r\nspin_unlock(&class->lock);\r\n}\r\nunsigned long zs_compact(struct zs_pool *pool)\r\n{\r\nint i;\r\nstruct size_class *class;\r\nfor (i = zs_size_classes - 1; i >= 0; i--) {\r\nclass = pool->size_class[i];\r\nif (!class)\r\ncontinue;\r\nif (class->index != i)\r\ncontinue;\r\n__zs_compact(pool, class);\r\n}\r\nreturn pool->stats.pages_compacted;\r\n}\r\nvoid zs_pool_stats(struct zs_pool *pool, struct zs_pool_stats *stats)\r\n{\r\nmemcpy(stats, &pool->stats, sizeof(struct zs_pool_stats));\r\n}\r\nstatic unsigned long zs_shrinker_scan(struct shrinker *shrinker,\r\nstruct shrink_control *sc)\r\n{\r\nunsigned long pages_freed;\r\nstruct zs_pool *pool = container_of(shrinker, struct zs_pool,\r\nshrinker);\r\npages_freed = pool->stats.pages_compacted;\r\npages_freed = zs_compact(pool) - pages_freed;\r\nreturn pages_freed ? pages_freed : SHRINK_STOP;\r\n}\r\nstatic unsigned long zs_shrinker_count(struct shrinker *shrinker,\r\nstruct shrink_control *sc)\r\n{\r\nint i;\r\nstruct size_class *class;\r\nunsigned long pages_to_free = 0;\r\nstruct zs_pool *pool = container_of(shrinker, struct zs_pool,\r\nshrinker);\r\nfor (i = zs_size_classes - 1; i >= 0; i--) {\r\nclass = pool->size_class[i];\r\nif (!class)\r\ncontinue;\r\nif (class->index != i)\r\ncontinue;\r\npages_to_free += zs_can_compact(class);\r\n}\r\nreturn pages_to_free;\r\n}\r\nstatic void zs_unregister_shrinker(struct zs_pool *pool)\r\n{\r\nif (pool->shrinker_enabled) {\r\nunregister_shrinker(&pool->shrinker);\r\npool->shrinker_enabled = false;\r\n}\r\n}\r\nstatic int zs_register_shrinker(struct zs_pool *pool)\r\n{\r\npool->shrinker.scan_objects = zs_shrinker_scan;\r\npool->shrinker.count_objects = zs_shrinker_count;\r\npool->shrinker.batch = 0;\r\npool->shrinker.seeks = DEFAULT_SEEKS;\r\nreturn register_shrinker(&pool->shrinker);\r\n}\r\nstruct zs_pool *zs_create_pool(const char *name)\r\n{\r\nint i;\r\nstruct zs_pool *pool;\r\nstruct size_class *prev_class = NULL;\r\npool = kzalloc(sizeof(*pool), GFP_KERNEL);\r\nif (!pool)\r\nreturn NULL;\r\ninit_deferred_free(pool);\r\npool->size_class = kcalloc(zs_size_classes, sizeof(struct size_class *),\r\nGFP_KERNEL);\r\nif (!pool->size_class) {\r\nkfree(pool);\r\nreturn NULL;\r\n}\r\npool->name = kstrdup(name, GFP_KERNEL);\r\nif (!pool->name)\r\ngoto err;\r\nif (create_cache(pool))\r\ngoto err;\r\nfor (i = zs_size_classes - 1; i >= 0; i--) {\r\nint size;\r\nint pages_per_zspage;\r\nint objs_per_zspage;\r\nstruct size_class *class;\r\nint fullness = 0;\r\nsize = ZS_MIN_ALLOC_SIZE + i * ZS_SIZE_CLASS_DELTA;\r\nif (size > ZS_MAX_ALLOC_SIZE)\r\nsize = ZS_MAX_ALLOC_SIZE;\r\npages_per_zspage = get_pages_per_zspage(size);\r\nobjs_per_zspage = pages_per_zspage * PAGE_SIZE / size;\r\nif (prev_class) {\r\nif (can_merge(prev_class, pages_per_zspage, objs_per_zspage)) {\r\npool->size_class[i] = prev_class;\r\ncontinue;\r\n}\r\n}\r\nclass = kzalloc(sizeof(struct size_class), GFP_KERNEL);\r\nif (!class)\r\ngoto err;\r\nclass->size = size;\r\nclass->index = i;\r\nclass->pages_per_zspage = pages_per_zspage;\r\nclass->objs_per_zspage = objs_per_zspage;\r\nspin_lock_init(&class->lock);\r\npool->size_class[i] = class;\r\nfor (fullness = ZS_EMPTY; fullness < NR_ZS_FULLNESS;\r\nfullness++)\r\nINIT_LIST_HEAD(&class->fullness_list[fullness]);\r\nprev_class = class;\r\n}\r\nzs_pool_stat_create(pool, name);\r\nif (zs_register_migration(pool))\r\ngoto err;\r\nif (zs_register_shrinker(pool) == 0)\r\npool->shrinker_enabled = true;\r\nreturn pool;\r\nerr:\r\nzs_destroy_pool(pool);\r\nreturn NULL;\r\n}\r\nvoid zs_destroy_pool(struct zs_pool *pool)\r\n{\r\nint i;\r\nzs_unregister_shrinker(pool);\r\nzs_unregister_migration(pool);\r\nzs_pool_stat_destroy(pool);\r\nfor (i = 0; i < zs_size_classes; i++) {\r\nint fg;\r\nstruct size_class *class = pool->size_class[i];\r\nif (!class)\r\ncontinue;\r\nif (class->index != i)\r\ncontinue;\r\nfor (fg = ZS_EMPTY; fg < NR_ZS_FULLNESS; fg++) {\r\nif (!list_empty(&class->fullness_list[fg])) {\r\npr_info("Freeing non-empty class with size %db, fullness group %d\n",\r\nclass->size, fg);\r\n}\r\n}\r\nkfree(class);\r\n}\r\ndestroy_cache(pool);\r\nkfree(pool->size_class);\r\nkfree(pool->name);\r\nkfree(pool);\r\n}\r\nstatic int __init zs_init(void)\r\n{\r\nint ret;\r\nret = zsmalloc_mount();\r\nif (ret)\r\ngoto out;\r\nret = cpuhp_setup_state(CPUHP_MM_ZS_PREPARE, "mm/zsmalloc:prepare",\r\nzs_cpu_prepare, zs_cpu_dead);\r\nif (ret)\r\ngoto hp_setup_fail;\r\ninit_zs_size_classes();\r\n#ifdef CONFIG_ZPOOL\r\nzpool_register_driver(&zs_zpool_driver);\r\n#endif\r\nzs_stat_init();\r\nreturn 0;\r\nhp_setup_fail:\r\nzsmalloc_unmount();\r\nout:\r\nreturn ret;\r\n}\r\nstatic void __exit zs_exit(void)\r\n{\r\n#ifdef CONFIG_ZPOOL\r\nzpool_unregister_driver(&zs_zpool_driver);\r\n#endif\r\nzsmalloc_unmount();\r\ncpuhp_remove_state(CPUHP_MM_ZS_PREPARE);\r\nzs_stat_exit();\r\n}
