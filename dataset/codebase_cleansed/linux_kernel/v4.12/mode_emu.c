static void pin_core_to_node(int core_id, int node_id)\r\n{\r\nif (emu_cores->to_node_id[core_id] == NODE_ID_FREE) {\r\nemu_cores->per_node[node_id]++;\r\nemu_cores->to_node_id[core_id] = node_id;\r\nemu_cores->total++;\r\n} else {\r\nWARN_ON(emu_cores->to_node_id[core_id] != node_id);\r\n}\r\n}\r\nstatic int cores_pinned(struct toptree *node)\r\n{\r\nreturn emu_cores->per_node[node->id];\r\n}\r\nstatic int core_pinned_to_node_id(struct toptree *core)\r\n{\r\nreturn emu_cores->to_node_id[core->id];\r\n}\r\nstatic int cores_free(struct toptree *tree)\r\n{\r\nstruct toptree *core;\r\nint count = 0;\r\ntoptree_for_each(core, tree, CORE) {\r\nif (core_pinned_to_node_id(core) == NODE_ID_FREE)\r\ncount++;\r\n}\r\nreturn count;\r\n}\r\nstatic struct toptree *core_node(struct toptree *core)\r\n{\r\nreturn core->parent->parent->parent->parent;\r\n}\r\nstatic struct toptree *core_drawer(struct toptree *core)\r\n{\r\nreturn core->parent->parent->parent;\r\n}\r\nstatic struct toptree *core_book(struct toptree *core)\r\n{\r\nreturn core->parent->parent;\r\n}\r\nstatic struct toptree *core_mc(struct toptree *core)\r\n{\r\nreturn core->parent;\r\n}\r\nstatic int dist_core_to_core(struct toptree *core1, struct toptree *core2)\r\n{\r\nif (core_drawer(core1)->id != core_drawer(core2)->id)\r\nreturn DIST_DRAWER;\r\nif (core_book(core1)->id != core_book(core2)->id)\r\nreturn DIST_BOOK;\r\nif (core_mc(core1)->id != core_mc(core2)->id)\r\nreturn DIST_MC;\r\nreturn DIST_CORE;\r\n}\r\nstatic int dist_node_to_core(struct toptree *node, struct toptree *core)\r\n{\r\nstruct toptree *core_node;\r\nint dist_min = DIST_MAX;\r\ntoptree_for_each(core_node, node, CORE)\r\ndist_min = min(dist_min, dist_core_to_core(core_node, core));\r\nreturn dist_min == DIST_MAX ? DIST_EMPTY : dist_min;\r\n}\r\nstatic void toptree_unify_tree(struct toptree *tree)\r\n{\r\nint nid;\r\ntoptree_unify(tree);\r\nfor (nid = 0; nid < emu_nodes; nid++)\r\ntoptree_get_child(tree, nid);\r\n}\r\nstatic struct toptree *node_for_core(struct toptree *numa, struct toptree *core,\r\nint extra)\r\n{\r\nstruct toptree *node, *node_best = NULL;\r\nint dist_cur, dist_best, cores_target;\r\ncores_target = emu_cores->per_node_target + extra;\r\ndist_best = DIST_MAX;\r\nnode_best = NULL;\r\ntoptree_for_each(node, numa, NODE) {\r\nif (core_pinned_to_node_id(core) == node->id) {\r\nnode_best = node;\r\nbreak;\r\n}\r\nif (cores_pinned(node) >= cores_target)\r\ncontinue;\r\ndist_cur = dist_node_to_core(node, core);\r\nif (dist_cur < dist_best) {\r\ndist_best = dist_cur;\r\nnode_best = node;\r\n}\r\n}\r\nreturn node_best;\r\n}\r\nstatic void toptree_to_numa_single(struct toptree *numa, struct toptree *phys,\r\nint extra)\r\n{\r\nstruct toptree *node, *core, *tmp;\r\ntoptree_for_each_safe(core, tmp, phys, CORE) {\r\nnode = node_for_core(numa, core, extra);\r\nif (!node)\r\nreturn;\r\ntoptree_move(core, node);\r\npin_core_to_node(core->id, node->id);\r\n}\r\n}\r\nstatic void move_level_to_numa_node(struct toptree *node, struct toptree *phys,\r\nenum toptree_level level, bool perfect)\r\n{\r\nint cores_free, cores_target = emu_cores->per_node_target;\r\nstruct toptree *cur, *tmp;\r\ntoptree_for_each_safe(cur, tmp, phys, level) {\r\ncores_free = cores_target - toptree_count(node, CORE);\r\nif (perfect) {\r\nif (cores_free == toptree_count(cur, CORE))\r\ntoptree_move(cur, node);\r\n} else {\r\nif (cores_free >= toptree_count(cur, CORE))\r\ntoptree_move(cur, node);\r\n}\r\n}\r\n}\r\nstatic void move_level_to_numa(struct toptree *numa, struct toptree *phys,\r\nenum toptree_level level, bool perfect)\r\n{\r\nstruct toptree *node;\r\ntoptree_for_each(node, numa, NODE)\r\nmove_level_to_numa_node(node, phys, level, perfect);\r\n}\r\nstatic void toptree_to_numa_first(struct toptree *numa, struct toptree *phys)\r\n{\r\nstruct toptree *core;\r\nmove_level_to_numa(numa, phys, DRAWER, true);\r\nmove_level_to_numa(numa, phys, DRAWER, false);\r\nmove_level_to_numa(numa, phys, BOOK, true);\r\nmove_level_to_numa(numa, phys, BOOK, false);\r\nmove_level_to_numa(numa, phys, MC, true);\r\nmove_level_to_numa(numa, phys, MC, false);\r\ntoptree_for_each(core, numa, CORE)\r\npin_core_to_node(core->id, core_node(core)->id);\r\n}\r\nstatic struct toptree *toptree_new(int id, int nodes)\r\n{\r\nstruct toptree *tree;\r\nint nid;\r\ntree = toptree_alloc(TOPOLOGY, id);\r\nif (!tree)\r\ngoto fail;\r\nfor (nid = 0; nid < nodes; nid++) {\r\nif (!toptree_get_child(tree, nid))\r\ngoto fail;\r\n}\r\nreturn tree;\r\nfail:\r\npanic("NUMA emulation could not allocate topology");\r\n}\r\nstatic void __ref create_core_to_node_map(void)\r\n{\r\nint i;\r\nemu_cores = memblock_virt_alloc(sizeof(*emu_cores), 8);\r\nfor (i = 0; i < ARRAY_SIZE(emu_cores->to_node_id); i++)\r\nemu_cores->to_node_id[i] = NODE_ID_FREE;\r\n}\r\nstatic struct toptree *toptree_to_numa(struct toptree *phys)\r\n{\r\nstatic int first = 1;\r\nstruct toptree *numa;\r\nint cores_total;\r\ncores_total = emu_cores->total + cores_free(phys);\r\nemu_cores->per_node_target = cores_total / emu_nodes;\r\nnuma = toptree_new(TOPTREE_ID_NUMA, emu_nodes);\r\nif (first) {\r\ntoptree_to_numa_first(numa, phys);\r\nfirst = 0;\r\n}\r\ntoptree_to_numa_single(numa, phys, 0);\r\ntoptree_to_numa_single(numa, phys, 1);\r\ntoptree_unify_tree(numa);\r\nWARN_ON(cpumask_weight(&phys->mask));\r\nreturn numa;\r\n}\r\nstatic struct toptree *toptree_from_topology(void)\r\n{\r\nstruct toptree *phys, *node, *drawer, *book, *mc, *core;\r\nstruct cpu_topology_s390 *top;\r\nint cpu;\r\nphys = toptree_new(TOPTREE_ID_PHYS, 1);\r\nfor_each_cpu(cpu, &cpus_with_topology) {\r\ntop = &cpu_topology[cpu];\r\nnode = toptree_get_child(phys, 0);\r\ndrawer = toptree_get_child(node, top->drawer_id);\r\nbook = toptree_get_child(drawer, top->book_id);\r\nmc = toptree_get_child(book, top->socket_id);\r\ncore = toptree_get_child(mc, smp_get_base_cpu(cpu));\r\nif (!drawer || !book || !mc || !core)\r\npanic("NUMA emulation could not allocate memory");\r\ncpumask_set_cpu(cpu, &core->mask);\r\ntoptree_update_mask(mc);\r\n}\r\nreturn phys;\r\n}\r\nstatic void topology_add_core(struct toptree *core)\r\n{\r\nstruct cpu_topology_s390 *top;\r\nint cpu;\r\nfor_each_cpu(cpu, &core->mask) {\r\ntop = &cpu_topology[cpu];\r\ncpumask_copy(&top->thread_mask, &core->mask);\r\ncpumask_copy(&top->core_mask, &core_mc(core)->mask);\r\ncpumask_copy(&top->book_mask, &core_book(core)->mask);\r\ncpumask_copy(&top->drawer_mask, &core_drawer(core)->mask);\r\ncpumask_set_cpu(cpu, &node_to_cpumask_map[core_node(core)->id]);\r\ntop->node_id = core_node(core)->id;\r\n}\r\n}\r\nstatic void toptree_to_topology(struct toptree *numa)\r\n{\r\nstruct toptree *core;\r\nint i;\r\nfor (i = 0; i < MAX_NUMNODES; i++)\r\ncpumask_clear(&node_to_cpumask_map[i]);\r\ntoptree_for_each(core, numa, CORE)\r\ntopology_add_core(core);\r\n}\r\nstatic void print_node_to_core_map(void)\r\n{\r\nint nid, cid;\r\nif (!numa_debug_enabled)\r\nreturn;\r\nprintk(KERN_DEBUG "NUMA node to core mapping\n");\r\nfor (nid = 0; nid < emu_nodes; nid++) {\r\nprintk(KERN_DEBUG " node %3d: ", nid);\r\nfor (cid = 0; cid < ARRAY_SIZE(emu_cores->to_node_id); cid++) {\r\nif (emu_cores->to_node_id[cid] == nid)\r\nprintk(KERN_CONT "%d ", cid);\r\n}\r\nprintk(KERN_CONT "\n");\r\n}\r\n}\r\nstatic void pin_all_possible_cpus(void)\r\n{\r\nint core_id, node_id, cpu;\r\nstatic int initialized;\r\nif (initialized)\r\nreturn;\r\nprint_node_to_core_map();\r\nnode_id = 0;\r\nfor_each_possible_cpu(cpu) {\r\ncore_id = smp_get_base_cpu(cpu);\r\nif (emu_cores->to_node_id[core_id] != NODE_ID_FREE)\r\ncontinue;\r\npin_core_to_node(core_id, node_id);\r\ncpu_topology[cpu].node_id = node_id;\r\nnode_id = (node_id + 1) % emu_nodes;\r\n}\r\nprint_node_to_core_map();\r\ninitialized = 1;\r\n}\r\nstatic void emu_update_cpu_topology(void)\r\n{\r\nstruct toptree *phys, *numa;\r\nif (emu_cores == NULL)\r\ncreate_core_to_node_map();\r\nphys = toptree_from_topology();\r\nnuma = toptree_to_numa(phys);\r\ntoptree_free(phys);\r\ntoptree_to_topology(numa);\r\ntoptree_free(numa);\r\npin_all_possible_cpus();\r\n}\r\nstatic unsigned long emu_setup_size_adjust(unsigned long size)\r\n{\r\nunsigned long size_new;\r\nsize = size ? : CONFIG_EMU_SIZE;\r\nsize_new = roundup(size, memory_block_size_bytes());\r\nif (size_new == size)\r\nreturn size;\r\npr_warn("Increasing memory stripe size from %ld MB to %ld MB\n",\r\nsize >> 20, size_new >> 20);\r\nreturn size_new;\r\n}\r\nstatic int emu_setup_nodes_adjust(int nodes)\r\n{\r\nint nodes_max;\r\nnodes_max = memblock.memory.total_size / emu_size;\r\nnodes_max = max(nodes_max, 1);\r\nif (nodes_max >= nodes)\r\nreturn nodes;\r\npr_warn("Not enough memory for %d nodes, reducing node count\n", nodes);\r\nreturn nodes_max;\r\n}\r\nstatic void emu_setup(void)\r\n{\r\nint nid;\r\nemu_size = emu_setup_size_adjust(emu_size);\r\nemu_nodes = emu_setup_nodes_adjust(emu_nodes);\r\nfor (nid = 0; nid < emu_nodes; nid++)\r\nnode_set(nid, node_possible_map);\r\npr_info("Creating %d nodes with memory stripe size %ld MB\n",\r\nemu_nodes, emu_size >> 20);\r\n}\r\nstatic int emu_pfn_to_nid(unsigned long pfn)\r\n{\r\nreturn (pfn / (emu_size >> PAGE_SHIFT)) % emu_nodes;\r\n}\r\nstatic unsigned long emu_align(void)\r\n{\r\nreturn emu_size;\r\n}\r\nstatic int emu_distance(int node1, int node2)\r\n{\r\nreturn (node1 != node2) * EMU_NODE_DIST;\r\n}\r\nstatic int __init early_parse_emu_nodes(char *p)\r\n{\r\nint count;\r\nif (kstrtoint(p, 0, &count) != 0 || count <= 0)\r\nreturn 0;\r\nif (count <= 0)\r\nreturn 0;\r\nemu_nodes = min(count, MAX_NUMNODES);\r\nreturn 0;\r\n}\r\nstatic int __init early_parse_emu_size(char *p)\r\n{\r\nemu_size = memparse(p, NULL);\r\nreturn 0;\r\n}
