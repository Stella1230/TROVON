static void stop_nic_rx(void __iomem *ioaddr, long crvalue)\r\n{\r\nint delay = 0x1000;\r\niowrite32(crvalue & ~(CR_W_RXEN), ioaddr + TCRRCR);\r\nwhile (--delay) {\r\nif ( (ioread32(ioaddr + TCRRCR) & CR_R_RXSTOP) == CR_R_RXSTOP)\r\nbreak;\r\n}\r\n}\r\nstatic void stop_nic_rxtx(void __iomem *ioaddr, long crvalue)\r\n{\r\nint delay = 0x1000;\r\niowrite32(crvalue & ~(CR_W_RXEN+CR_W_TXEN), ioaddr + TCRRCR);\r\nwhile (--delay) {\r\nif ( (ioread32(ioaddr + TCRRCR) & (CR_R_RXSTOP+CR_R_TXSTOP))\r\n== (CR_R_RXSTOP+CR_R_TXSTOP) )\r\nbreak;\r\n}\r\n}\r\nstatic int fealnx_init_one(struct pci_dev *pdev,\r\nconst struct pci_device_id *ent)\r\n{\r\nstruct netdev_private *np;\r\nint i, option, err, irq;\r\nstatic int card_idx = -1;\r\nchar boardname[12];\r\nvoid __iomem *ioaddr;\r\nunsigned long len;\r\nunsigned int chip_id = ent->driver_data;\r\nstruct net_device *dev;\r\nvoid *ring_space;\r\ndma_addr_t ring_dma;\r\n#ifdef USE_IO_OPS\r\nint bar = 0;\r\n#else\r\nint bar = 1;\r\n#endif\r\n#ifndef MODULE\r\nstatic int printed_version;\r\nif (!printed_version++)\r\nprintk(version);\r\n#endif\r\ncard_idx++;\r\nsprintf(boardname, "fealnx%d", card_idx);\r\noption = card_idx < MAX_UNITS ? options[card_idx] : 0;\r\ni = pci_enable_device(pdev);\r\nif (i) return i;\r\npci_set_master(pdev);\r\nlen = pci_resource_len(pdev, bar);\r\nif (len < MIN_REGION_SIZE) {\r\ndev_err(&pdev->dev,\r\n"region size %ld too small, aborting\n", len);\r\nreturn -ENODEV;\r\n}\r\ni = pci_request_regions(pdev, boardname);\r\nif (i)\r\nreturn i;\r\nirq = pdev->irq;\r\nioaddr = pci_iomap(pdev, bar, len);\r\nif (!ioaddr) {\r\nerr = -ENOMEM;\r\ngoto err_out_res;\r\n}\r\ndev = alloc_etherdev(sizeof(struct netdev_private));\r\nif (!dev) {\r\nerr = -ENOMEM;\r\ngoto err_out_unmap;\r\n}\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nfor (i = 0; i < 6; ++i)\r\ndev->dev_addr[i] = ioread8(ioaddr + PAR0 + i);\r\niowrite32(0x00000001, ioaddr + BCR);\r\nnp = netdev_priv(dev);\r\nnp->mem = ioaddr;\r\nspin_lock_init(&np->lock);\r\nnp->pci_dev = pdev;\r\nnp->flags = skel_netdrv_tbl[chip_id].flags;\r\npci_set_drvdata(pdev, dev);\r\nnp->mii.dev = dev;\r\nnp->mii.mdio_read = mdio_read;\r\nnp->mii.mdio_write = mdio_write;\r\nnp->mii.phy_id_mask = 0x1f;\r\nnp->mii.reg_num_mask = 0x1f;\r\nring_space = pci_alloc_consistent(pdev, RX_TOTAL_SIZE, &ring_dma);\r\nif (!ring_space) {\r\nerr = -ENOMEM;\r\ngoto err_out_free_dev;\r\n}\r\nnp->rx_ring = ring_space;\r\nnp->rx_ring_dma = ring_dma;\r\nring_space = pci_alloc_consistent(pdev, TX_TOTAL_SIZE, &ring_dma);\r\nif (!ring_space) {\r\nerr = -ENOMEM;\r\ngoto err_out_free_rx;\r\n}\r\nnp->tx_ring = ring_space;\r\nnp->tx_ring_dma = ring_dma;\r\nif (np->flags == HAS_MII_XCVR) {\r\nint phy, phy_idx = 0;\r\nfor (phy = 1; phy < 32 && phy_idx < ARRAY_SIZE(np->phys);\r\nphy++) {\r\nint mii_status = mdio_read(dev, phy, 1);\r\nif (mii_status != 0xffff && mii_status != 0x0000) {\r\nnp->phys[phy_idx++] = phy;\r\ndev_info(&pdev->dev,\r\n"MII PHY found at address %d, status "\r\n"0x%4.4x.\n", phy, mii_status);\r\n{\r\nunsigned int data;\r\ndata = mdio_read(dev, np->phys[0], 2);\r\nif (data == SeeqPHYID0)\r\nnp->PHYType = SeeqPHY;\r\nelse if (data == AhdocPHYID0)\r\nnp->PHYType = AhdocPHY;\r\nelse if (data == MarvellPHYID0)\r\nnp->PHYType = MarvellPHY;\r\nelse if (data == MysonPHYID0)\r\nnp->PHYType = Myson981;\r\nelse if (data == LevelOnePHYID0)\r\nnp->PHYType = LevelOnePHY;\r\nelse\r\nnp->PHYType = OtherPHY;\r\n}\r\n}\r\n}\r\nnp->mii_cnt = phy_idx;\r\nif (phy_idx == 0)\r\ndev_warn(&pdev->dev,\r\n"MII PHY not found -- this device may "\r\n"not operate correctly.\n");\r\n} else {\r\nnp->phys[0] = 32;\r\nif (ioread32(ioaddr + PHYIDENTIFIER) == MysonPHYID)\r\nnp->PHYType = MysonPHY;\r\nelse\r\nnp->PHYType = OtherPHY;\r\n}\r\nnp->mii.phy_id = np->phys[0];\r\nif (dev->mem_start)\r\noption = dev->mem_start;\r\nif (option > 0) {\r\nif (option & 0x200)\r\nnp->mii.full_duplex = 1;\r\nnp->default_port = option & 15;\r\n}\r\nif (card_idx < MAX_UNITS && full_duplex[card_idx] > 0)\r\nnp->mii.full_duplex = full_duplex[card_idx];\r\nif (np->mii.full_duplex) {\r\ndev_info(&pdev->dev, "Media type forced to Full Duplex.\n");\r\nif ((np->PHYType == MarvellPHY) || (np->PHYType == LevelOnePHY)) {\r\nunsigned int data;\r\ndata = mdio_read(dev, np->phys[0], 9);\r\ndata = (data & 0xfcff) | 0x0200;\r\nmdio_write(dev, np->phys[0], 9, data);\r\n}\r\nif (np->flags == HAS_MII_XCVR)\r\nmdio_write(dev, np->phys[0], MII_ADVERTISE, ADVERTISE_FULL);\r\nelse\r\niowrite32(ADVERTISE_FULL, ioaddr + ANARANLPAR);\r\nnp->mii.force_media = 1;\r\n}\r\ndev->netdev_ops = &netdev_ops;\r\ndev->ethtool_ops = &netdev_ethtool_ops;\r\ndev->watchdog_timeo = TX_TIMEOUT;\r\nerr = register_netdev(dev);\r\nif (err)\r\ngoto err_out_free_tx;\r\nprintk(KERN_INFO "%s: %s at %p, %pM, IRQ %d.\n",\r\ndev->name, skel_netdrv_tbl[chip_id].chip_name, ioaddr,\r\ndev->dev_addr, irq);\r\nreturn 0;\r\nerr_out_free_tx:\r\npci_free_consistent(pdev, TX_TOTAL_SIZE, np->tx_ring, np->tx_ring_dma);\r\nerr_out_free_rx:\r\npci_free_consistent(pdev, RX_TOTAL_SIZE, np->rx_ring, np->rx_ring_dma);\r\nerr_out_free_dev:\r\nfree_netdev(dev);\r\nerr_out_unmap:\r\npci_iounmap(pdev, ioaddr);\r\nerr_out_res:\r\npci_release_regions(pdev);\r\nreturn err;\r\n}\r\nstatic void fealnx_remove_one(struct pci_dev *pdev)\r\n{\r\nstruct net_device *dev = pci_get_drvdata(pdev);\r\nif (dev) {\r\nstruct netdev_private *np = netdev_priv(dev);\r\npci_free_consistent(pdev, TX_TOTAL_SIZE, np->tx_ring,\r\nnp->tx_ring_dma);\r\npci_free_consistent(pdev, RX_TOTAL_SIZE, np->rx_ring,\r\nnp->rx_ring_dma);\r\nunregister_netdev(dev);\r\npci_iounmap(pdev, np->mem);\r\nfree_netdev(dev);\r\npci_release_regions(pdev);\r\n} else\r\nprintk(KERN_ERR "fealnx: remove for unknown device\n");\r\n}\r\nstatic ulong m80x_send_cmd_to_phy(void __iomem *miiport, int opcode, int phyad, int regad)\r\n{\r\nulong miir;\r\nint i;\r\nunsigned int mask, data;\r\nmiir = (ulong) ioread32(miiport);\r\nmiir &= 0xfffffff0;\r\nmiir |= MASK_MIIR_MII_WRITE + MASK_MIIR_MII_MDO;\r\nfor (i = 0; i < 32; i++) {\r\nmiir &= ~MASK_MIIR_MII_MDC;\r\niowrite32(miir, miiport);\r\nmiir |= MASK_MIIR_MII_MDC;\r\niowrite32(miir, miiport);\r\n}\r\ndata = opcode | (phyad << 7) | (regad << 2);\r\nmask = 0x8000;\r\nwhile (mask) {\r\nmiir &= ~(MASK_MIIR_MII_MDC + MASK_MIIR_MII_MDO);\r\nif (mask & data)\r\nmiir |= MASK_MIIR_MII_MDO;\r\niowrite32(miir, miiport);\r\nmiir |= MASK_MIIR_MII_MDC;\r\niowrite32(miir, miiport);\r\nudelay(30);\r\nmask >>= 1;\r\nif (mask == 0x2 && opcode == OP_READ)\r\nmiir &= ~MASK_MIIR_MII_WRITE;\r\n}\r\nreturn miir;\r\n}\r\nstatic int mdio_read(struct net_device *dev, int phyad, int regad)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *miiport = np->mem + MANAGEMENT;\r\nulong miir;\r\nunsigned int mask, data;\r\nmiir = m80x_send_cmd_to_phy(miiport, OP_READ, phyad, regad);\r\nmask = 0x8000;\r\ndata = 0;\r\nwhile (mask) {\r\nmiir &= ~MASK_MIIR_MII_MDC;\r\niowrite32(miir, miiport);\r\nmiir = ioread32(miiport);\r\nif (miir & MASK_MIIR_MII_MDI)\r\ndata |= mask;\r\nmiir |= MASK_MIIR_MII_MDC;\r\niowrite32(miir, miiport);\r\nudelay(30);\r\nmask >>= 1;\r\n}\r\nmiir &= ~MASK_MIIR_MII_MDC;\r\niowrite32(miir, miiport);\r\nreturn data & 0xffff;\r\n}\r\nstatic void mdio_write(struct net_device *dev, int phyad, int regad, int data)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *miiport = np->mem + MANAGEMENT;\r\nulong miir;\r\nunsigned int mask;\r\nmiir = m80x_send_cmd_to_phy(miiport, OP_WRITE, phyad, regad);\r\nmask = 0x8000;\r\nwhile (mask) {\r\nmiir &= ~(MASK_MIIR_MII_MDC + MASK_MIIR_MII_MDO);\r\nif (mask & data)\r\nmiir |= MASK_MIIR_MII_MDO;\r\niowrite32(miir, miiport);\r\nmiir |= MASK_MIIR_MII_MDC;\r\niowrite32(miir, miiport);\r\nmask >>= 1;\r\n}\r\nmiir &= ~MASK_MIIR_MII_MDC;\r\niowrite32(miir, miiport);\r\n}\r\nstatic int netdev_open(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nconst int irq = np->pci_dev->irq;\r\nint rc, i;\r\niowrite32(0x00000001, ioaddr + BCR);\r\nrc = request_irq(irq, intr_handler, IRQF_SHARED, dev->name, dev);\r\nif (rc)\r\nreturn -EAGAIN;\r\nfor (i = 0; i < 3; i++)\r\niowrite16(((unsigned short*)dev->dev_addr)[i],\r\nioaddr + PAR0 + i*2);\r\ninit_ring(dev);\r\niowrite32(np->rx_ring_dma, ioaddr + RXLBA);\r\niowrite32(np->tx_ring_dma, ioaddr + TXLBA);\r\nnp->bcrvalue = 0x10;\r\n#ifdef __BIG_ENDIAN\r\nnp->bcrvalue |= 0x04;\r\n#endif\r\n#if defined(__i386__) && !defined(MODULE)\r\nif (boot_cpu_data.x86 <= 4)\r\nnp->crvalue = 0xa00;\r\nelse\r\n#endif\r\nnp->crvalue = 0xe00;\r\nnp->imrvalue = TUNF | CNTOVF | RBU | TI | RI;\r\nif (np->pci_dev->device == 0x891) {\r\nnp->bcrvalue |= 0x200;\r\nnp->crvalue |= CR_W_ENH;\r\nnp->imrvalue |= ETI;\r\n}\r\niowrite32(np->bcrvalue, ioaddr + BCR);\r\nif (dev->if_port == 0)\r\ndev->if_port = np->default_port;\r\niowrite32(0, ioaddr + RXPDR);\r\nnp->crvalue |= 0x00e40001;\r\nnp->mii.full_duplex = np->mii.force_media;\r\ngetlinkstatus(dev);\r\nif (np->linkok)\r\ngetlinktype(dev);\r\n__set_rx_mode(dev);\r\nnetif_start_queue(dev);\r\niowrite32(FBE | TUNF | CNTOVF | RBU | TI | RI, ioaddr + ISR);\r\niowrite32(np->imrvalue, ioaddr + IMR);\r\nif (debug)\r\nprintk(KERN_DEBUG "%s: Done netdev_open().\n", dev->name);\r\ninit_timer(&np->timer);\r\nnp->timer.expires = RUN_AT(3 * HZ);\r\nnp->timer.data = (unsigned long) dev;\r\nnp->timer.function = netdev_timer;\r\nadd_timer(&np->timer);\r\ninit_timer(&np->reset_timer);\r\nnp->reset_timer.data = (unsigned long) dev;\r\nnp->reset_timer.function = reset_timer;\r\nnp->reset_timer_armed = 0;\r\nreturn rc;\r\n}\r\nstatic void getlinkstatus(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nunsigned int i, DelayTime = 0x1000;\r\nnp->linkok = 0;\r\nif (np->PHYType == MysonPHY) {\r\nfor (i = 0; i < DelayTime; ++i) {\r\nif (ioread32(np->mem + BMCRSR) & LinkIsUp2) {\r\nnp->linkok = 1;\r\nreturn;\r\n}\r\nudelay(100);\r\n}\r\n} else {\r\nfor (i = 0; i < DelayTime; ++i) {\r\nif (mdio_read(dev, np->phys[0], MII_BMSR) & BMSR_LSTATUS) {\r\nnp->linkok = 1;\r\nreturn;\r\n}\r\nudelay(100);\r\n}\r\n}\r\n}\r\nstatic void getlinktype(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nif (np->PHYType == MysonPHY) {\r\nif (ioread32(np->mem + TCRRCR) & CR_R_FD)\r\nnp->duplexmode = 2;\r\nelse\r\nnp->duplexmode = 1;\r\nif (ioread32(np->mem + TCRRCR) & CR_R_PS10)\r\nnp->line_speed = 1;\r\nelse\r\nnp->line_speed = 2;\r\n} else {\r\nif (np->PHYType == SeeqPHY) {\r\nunsigned int data;\r\ndata = mdio_read(dev, np->phys[0], MIIRegister18);\r\nif (data & SPD_DET_100)\r\nnp->line_speed = 2;\r\nelse\r\nnp->line_speed = 1;\r\nif (data & DPLX_DET_FULL)\r\nnp->duplexmode = 2;\r\nelse\r\nnp->duplexmode = 1;\r\n} else if (np->PHYType == AhdocPHY) {\r\nunsigned int data;\r\ndata = mdio_read(dev, np->phys[0], DiagnosticReg);\r\nif (data & Speed_100)\r\nnp->line_speed = 2;\r\nelse\r\nnp->line_speed = 1;\r\nif (data & DPLX_FULL)\r\nnp->duplexmode = 2;\r\nelse\r\nnp->duplexmode = 1;\r\n}\r\nelse if (np->PHYType == MarvellPHY) {\r\nunsigned int data;\r\ndata = mdio_read(dev, np->phys[0], SpecificReg);\r\nif (data & Full_Duplex)\r\nnp->duplexmode = 2;\r\nelse\r\nnp->duplexmode = 1;\r\ndata &= SpeedMask;\r\nif (data == Speed_1000M)\r\nnp->line_speed = 3;\r\nelse if (data == Speed_100M)\r\nnp->line_speed = 2;\r\nelse\r\nnp->line_speed = 1;\r\n}\r\nelse if (np->PHYType == Myson981) {\r\nunsigned int data;\r\ndata = mdio_read(dev, np->phys[0], StatusRegister);\r\nif (data & SPEED100)\r\nnp->line_speed = 2;\r\nelse\r\nnp->line_speed = 1;\r\nif (data & FULLMODE)\r\nnp->duplexmode = 2;\r\nelse\r\nnp->duplexmode = 1;\r\n}\r\nelse if (np->PHYType == LevelOnePHY) {\r\nunsigned int data;\r\ndata = mdio_read(dev, np->phys[0], SpecificReg);\r\nif (data & LXT1000_Full)\r\nnp->duplexmode = 2;\r\nelse\r\nnp->duplexmode = 1;\r\ndata &= SpeedMask;\r\nif (data == LXT1000_1000M)\r\nnp->line_speed = 3;\r\nelse if (data == LXT1000_100M)\r\nnp->line_speed = 2;\r\nelse\r\nnp->line_speed = 1;\r\n}\r\nnp->crvalue &= (~CR_W_PS10) & (~CR_W_FD) & (~CR_W_PS1000);\r\nif (np->line_speed == 1)\r\nnp->crvalue |= CR_W_PS10;\r\nelse if (np->line_speed == 3)\r\nnp->crvalue |= CR_W_PS1000;\r\nif (np->duplexmode == 2)\r\nnp->crvalue |= CR_W_FD;\r\n}\r\n}\r\nstatic void allocate_rx_buffers(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nwhile (np->really_rx_count != RX_RING_SIZE) {\r\nstruct sk_buff *skb;\r\nskb = netdev_alloc_skb(dev, np->rx_buf_sz);\r\nif (skb == NULL)\r\nbreak;\r\nwhile (np->lack_rxbuf->skbuff)\r\nnp->lack_rxbuf = np->lack_rxbuf->next_desc_logical;\r\nnp->lack_rxbuf->skbuff = skb;\r\nnp->lack_rxbuf->buffer = pci_map_single(np->pci_dev, skb->data,\r\nnp->rx_buf_sz, PCI_DMA_FROMDEVICE);\r\nnp->lack_rxbuf->status = RXOWN;\r\n++np->really_rx_count;\r\n}\r\n}\r\nstatic void netdev_timer(unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nint old_crvalue = np->crvalue;\r\nunsigned int old_linkok = np->linkok;\r\nunsigned long flags;\r\nif (debug)\r\nprintk(KERN_DEBUG "%s: Media selection timer tick, status %8.8x "\r\n"config %8.8x.\n", dev->name, ioread32(ioaddr + ISR),\r\nioread32(ioaddr + TCRRCR));\r\nspin_lock_irqsave(&np->lock, flags);\r\nif (np->flags == HAS_MII_XCVR) {\r\ngetlinkstatus(dev);\r\nif ((old_linkok == 0) && (np->linkok == 1)) {\r\ngetlinktype(dev);\r\nif (np->crvalue != old_crvalue) {\r\nstop_nic_rxtx(ioaddr, np->crvalue);\r\niowrite32(np->crvalue, ioaddr + TCRRCR);\r\n}\r\n}\r\n}\r\nallocate_rx_buffers(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnp->timer.expires = RUN_AT(10 * HZ);\r\nadd_timer(&np->timer);\r\n}\r\nstatic void reset_and_disable_rxtx(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nint delay=51;\r\nstop_nic_rxtx(ioaddr, 0);\r\niowrite32(0, ioaddr + IMR);\r\niowrite32(0x00000001, ioaddr + BCR);\r\nwhile (--delay) {\r\nioread32(ioaddr + BCR);\r\nrmb();\r\n}\r\n}\r\nstatic void enable_rxtx(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nreset_rx_descriptors(dev);\r\niowrite32(np->tx_ring_dma + ((char*)np->cur_tx - (char*)np->tx_ring),\r\nioaddr + TXLBA);\r\niowrite32(np->rx_ring_dma + ((char*)np->cur_rx - (char*)np->rx_ring),\r\nioaddr + RXLBA);\r\niowrite32(np->bcrvalue, ioaddr + BCR);\r\niowrite32(0, ioaddr + RXPDR);\r\n__set_rx_mode(dev);\r\niowrite32(FBE | TUNF | CNTOVF | RBU | TI | RI, ioaddr + ISR);\r\niowrite32(np->imrvalue, ioaddr + IMR);\r\niowrite32(0, ioaddr + TXPDR);\r\n}\r\nstatic void reset_timer(unsigned long data)\r\n{\r\nstruct net_device *dev = (struct net_device *) data;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nunsigned long flags;\r\nprintk(KERN_WARNING "%s: resetting tx and rx machinery\n", dev->name);\r\nspin_lock_irqsave(&np->lock, flags);\r\nnp->crvalue = np->crvalue_sv;\r\nnp->imrvalue = np->imrvalue_sv;\r\nreset_and_disable_rxtx(dev);\r\nenable_rxtx(dev);\r\nnetif_start_queue(dev);\r\nnp->reset_timer_armed = 0;\r\nspin_unlock_irqrestore(&np->lock, flags);\r\n}\r\nstatic void fealnx_tx_timeout(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nunsigned long flags;\r\nint i;\r\nprintk(KERN_WARNING\r\n"%s: Transmit timed out, status %8.8x, resetting...\n",\r\ndev->name, ioread32(ioaddr + ISR));\r\n{\r\nprintk(KERN_DEBUG " Rx ring %p: ", np->rx_ring);\r\nfor (i = 0; i < RX_RING_SIZE; i++)\r\nprintk(KERN_CONT " %8.8x",\r\n(unsigned int) np->rx_ring[i].status);\r\nprintk(KERN_CONT "\n");\r\nprintk(KERN_DEBUG " Tx ring %p: ", np->tx_ring);\r\nfor (i = 0; i < TX_RING_SIZE; i++)\r\nprintk(KERN_CONT " %4.4x", np->tx_ring[i].status);\r\nprintk(KERN_CONT "\n");\r\n}\r\nspin_lock_irqsave(&np->lock, flags);\r\nreset_and_disable_rxtx(dev);\r\nreset_tx_descriptors(dev);\r\nenable_rxtx(dev);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nnetif_trans_update(dev);\r\ndev->stats.tx_errors++;\r\nnetif_wake_queue(dev);\r\n}\r\nstatic void init_ring(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint i;\r\nnp->rx_buf_sz = (dev->mtu <= 1500 ? PKT_BUF_SZ : dev->mtu + 32);\r\nnp->cur_rx = &np->rx_ring[0];\r\nnp->lack_rxbuf = np->rx_ring;\r\nnp->really_rx_count = 0;\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nnp->rx_ring[i].status = 0;\r\nnp->rx_ring[i].control = np->rx_buf_sz << RBSShift;\r\nnp->rx_ring[i].next_desc = np->rx_ring_dma +\r\n(i + 1)*sizeof(struct fealnx_desc);\r\nnp->rx_ring[i].next_desc_logical = &np->rx_ring[i + 1];\r\nnp->rx_ring[i].skbuff = NULL;\r\n}\r\nnp->rx_ring[i - 1].next_desc = np->rx_ring_dma;\r\nnp->rx_ring[i - 1].next_desc_logical = np->rx_ring;\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nstruct sk_buff *skb = netdev_alloc_skb(dev, np->rx_buf_sz);\r\nif (skb == NULL) {\r\nnp->lack_rxbuf = &np->rx_ring[i];\r\nbreak;\r\n}\r\n++np->really_rx_count;\r\nnp->rx_ring[i].skbuff = skb;\r\nnp->rx_ring[i].buffer = pci_map_single(np->pci_dev, skb->data,\r\nnp->rx_buf_sz, PCI_DMA_FROMDEVICE);\r\nnp->rx_ring[i].status = RXOWN;\r\nnp->rx_ring[i].control |= RXIC;\r\n}\r\nnp->cur_tx = &np->tx_ring[0];\r\nnp->cur_tx_copy = &np->tx_ring[0];\r\nnp->really_tx_count = 0;\r\nnp->free_tx_count = TX_RING_SIZE;\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nnp->tx_ring[i].status = 0;\r\nnp->tx_ring[i].next_desc = np->tx_ring_dma +\r\n(i + 1)*sizeof(struct fealnx_desc);\r\nnp->tx_ring[i].next_desc_logical = &np->tx_ring[i + 1];\r\nnp->tx_ring[i].skbuff = NULL;\r\n}\r\nnp->tx_ring[i - 1].next_desc = np->tx_ring_dma;\r\nnp->tx_ring[i - 1].next_desc_logical = &np->tx_ring[0];\r\n}\r\nstatic netdev_tx_t start_tx(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nunsigned long flags;\r\nspin_lock_irqsave(&np->lock, flags);\r\nnp->cur_tx_copy->skbuff = skb;\r\n#define one_buffer\r\n#define BPT 1022\r\n#if defined(one_buffer)\r\nnp->cur_tx_copy->buffer = pci_map_single(np->pci_dev, skb->data,\r\nskb->len, PCI_DMA_TODEVICE);\r\nnp->cur_tx_copy->control = TXIC | TXLD | TXFD | CRCEnable | PADEnable;\r\nnp->cur_tx_copy->control |= (skb->len << PKTSShift);\r\nnp->cur_tx_copy->control |= (skb->len << TBSShift);\r\nif (np->pci_dev->device == 0x891)\r\nnp->cur_tx_copy->control |= ETIControl | RetryTxLC;\r\nnp->cur_tx_copy->status = TXOWN;\r\nnp->cur_tx_copy = np->cur_tx_copy->next_desc_logical;\r\n--np->free_tx_count;\r\n#elif defined(two_buffer)\r\nif (skb->len > BPT) {\r\nstruct fealnx_desc *next;\r\nnp->cur_tx_copy->buffer = pci_map_single(np->pci_dev, skb->data,\r\nBPT, PCI_DMA_TODEVICE);\r\nnp->cur_tx_copy->control = TXIC | TXFD | CRCEnable | PADEnable;\r\nnp->cur_tx_copy->control |= (skb->len << PKTSShift);\r\nnp->cur_tx_copy->control |= (BPT << TBSShift);\r\nnext = np->cur_tx_copy->next_desc_logical;\r\nnext->skbuff = skb;\r\nnext->control = TXIC | TXLD | CRCEnable | PADEnable;\r\nnext->control |= (skb->len << PKTSShift);\r\nnext->control |= ((skb->len - BPT) << TBSShift);\r\nif (np->pci_dev->device == 0x891)\r\nnp->cur_tx_copy->control |= ETIControl | RetryTxLC;\r\nnext->buffer = pci_map_single(ep->pci_dev, skb->data + BPT,\r\nskb->len - BPT, PCI_DMA_TODEVICE);\r\nnext->status = TXOWN;\r\nnp->cur_tx_copy->status = TXOWN;\r\nnp->cur_tx_copy = next->next_desc_logical;\r\nnp->free_tx_count -= 2;\r\n} else {\r\nnp->cur_tx_copy->buffer = pci_map_single(np->pci_dev, skb->data,\r\nskb->len, PCI_DMA_TODEVICE);\r\nnp->cur_tx_copy->control = TXIC | TXLD | TXFD | CRCEnable | PADEnable;\r\nnp->cur_tx_copy->control |= (skb->len << PKTSShift);\r\nnp->cur_tx_copy->control |= (skb->len << TBSShift);\r\nif (np->pci_dev->device == 0x891)\r\nnp->cur_tx_copy->control |= ETIControl | RetryTxLC;\r\nnp->cur_tx_copy->status = TXOWN;\r\nnp->cur_tx_copy = np->cur_tx_copy->next_desc_logical;\r\n--np->free_tx_count;\r\n}\r\n#endif\r\nif (np->free_tx_count < 2)\r\nnetif_stop_queue(dev);\r\n++np->really_tx_count;\r\niowrite32(0, np->mem + TXPDR);\r\nspin_unlock_irqrestore(&np->lock, flags);\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void reset_tx_descriptors(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nstruct fealnx_desc *cur;\r\nint i;\r\nnp->cur_tx = &np->tx_ring[0];\r\nnp->cur_tx_copy = &np->tx_ring[0];\r\nnp->really_tx_count = 0;\r\nnp->free_tx_count = TX_RING_SIZE;\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\ncur = &np->tx_ring[i];\r\nif (cur->skbuff) {\r\npci_unmap_single(np->pci_dev, cur->buffer,\r\ncur->skbuff->len, PCI_DMA_TODEVICE);\r\ndev_kfree_skb_any(cur->skbuff);\r\ncur->skbuff = NULL;\r\n}\r\ncur->status = 0;\r\ncur->control = 0;\r\ncur->next_desc = np->tx_ring_dma +\r\n(i + 1)*sizeof(struct fealnx_desc);\r\ncur->next_desc_logical = &np->tx_ring[i + 1];\r\n}\r\nnp->tx_ring[TX_RING_SIZE - 1].next_desc = np->tx_ring_dma;\r\nnp->tx_ring[TX_RING_SIZE - 1].next_desc_logical = &np->tx_ring[0];\r\n}\r\nstatic void reset_rx_descriptors(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nstruct fealnx_desc *cur = np->cur_rx;\r\nint i;\r\nallocate_rx_buffers(dev);\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nif (cur->skbuff)\r\ncur->status = RXOWN;\r\ncur = cur->next_desc_logical;\r\n}\r\niowrite32(np->rx_ring_dma + ((char*)np->cur_rx - (char*)np->rx_ring),\r\nnp->mem + RXLBA);\r\n}\r\nstatic irqreturn_t intr_handler(int irq, void *dev_instance)\r\n{\r\nstruct net_device *dev = (struct net_device *) dev_instance;\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nlong boguscnt = max_interrupt_work;\r\nunsigned int num_tx = 0;\r\nint handled = 0;\r\nspin_lock(&np->lock);\r\niowrite32(0, ioaddr + IMR);\r\ndo {\r\nu32 intr_status = ioread32(ioaddr + ISR);\r\niowrite32(intr_status, ioaddr + ISR);\r\nif (debug)\r\nprintk(KERN_DEBUG "%s: Interrupt, status %4.4x.\n", dev->name,\r\nintr_status);\r\nif (!(intr_status & np->imrvalue))\r\nbreak;\r\nhandled = 1;\r\nif (intr_status & TUNF)\r\niowrite32(0, ioaddr + TXPDR);\r\nif (intr_status & CNTOVF) {\r\ndev->stats.rx_missed_errors +=\r\nioread32(ioaddr + TALLY) & 0x7fff;\r\ndev->stats.rx_crc_errors +=\r\n(ioread32(ioaddr + TALLY) & 0x7fff0000) >> 16;\r\n}\r\nif (intr_status & (RI | RBU)) {\r\nif (intr_status & RI)\r\nnetdev_rx(dev);\r\nelse {\r\nstop_nic_rx(ioaddr, np->crvalue);\r\nreset_rx_descriptors(dev);\r\niowrite32(np->crvalue, ioaddr + TCRRCR);\r\n}\r\n}\r\nwhile (np->really_tx_count) {\r\nlong tx_status = np->cur_tx->status;\r\nlong tx_control = np->cur_tx->control;\r\nif (!(tx_control & TXLD)) {\r\nstruct fealnx_desc *next;\r\nnext = np->cur_tx->next_desc_logical;\r\ntx_status = next->status;\r\ntx_control = next->control;\r\n}\r\nif (tx_status & TXOWN)\r\nbreak;\r\nif (!(np->crvalue & CR_W_ENH)) {\r\nif (tx_status & (CSL | LC | EC | UDF | HF)) {\r\ndev->stats.tx_errors++;\r\nif (tx_status & EC)\r\ndev->stats.tx_aborted_errors++;\r\nif (tx_status & CSL)\r\ndev->stats.tx_carrier_errors++;\r\nif (tx_status & LC)\r\ndev->stats.tx_window_errors++;\r\nif (tx_status & UDF)\r\ndev->stats.tx_fifo_errors++;\r\nif ((tx_status & HF) && np->mii.full_duplex == 0)\r\ndev->stats.tx_heartbeat_errors++;\r\n} else {\r\ndev->stats.tx_bytes +=\r\n((tx_control & PKTSMask) >> PKTSShift);\r\ndev->stats.collisions +=\r\n((tx_status & NCRMask) >> NCRShift);\r\ndev->stats.tx_packets++;\r\n}\r\n} else {\r\ndev->stats.tx_bytes +=\r\n((tx_control & PKTSMask) >> PKTSShift);\r\ndev->stats.tx_packets++;\r\n}\r\npci_unmap_single(np->pci_dev, np->cur_tx->buffer,\r\nnp->cur_tx->skbuff->len, PCI_DMA_TODEVICE);\r\ndev_kfree_skb_irq(np->cur_tx->skbuff);\r\nnp->cur_tx->skbuff = NULL;\r\n--np->really_tx_count;\r\nif (np->cur_tx->control & TXLD) {\r\nnp->cur_tx = np->cur_tx->next_desc_logical;\r\n++np->free_tx_count;\r\n} else {\r\nnp->cur_tx = np->cur_tx->next_desc_logical;\r\nnp->cur_tx = np->cur_tx->next_desc_logical;\r\nnp->free_tx_count += 2;\r\n}\r\nnum_tx++;\r\n}\r\nif (num_tx && np->free_tx_count >= 2)\r\nnetif_wake_queue(dev);\r\nif (np->crvalue & CR_W_ENH) {\r\nlong data;\r\ndata = ioread32(ioaddr + TSR);\r\ndev->stats.tx_errors += (data & 0xff000000) >> 24;\r\ndev->stats.tx_aborted_errors +=\r\n(data & 0xff000000) >> 24;\r\ndev->stats.tx_window_errors +=\r\n(data & 0x00ff0000) >> 16;\r\ndev->stats.collisions += (data & 0x0000ffff);\r\n}\r\nif (--boguscnt < 0) {\r\nprintk(KERN_WARNING "%s: Too much work at interrupt, "\r\n"status=0x%4.4x.\n", dev->name, intr_status);\r\nif (!np->reset_timer_armed) {\r\nnp->reset_timer_armed = 1;\r\nnp->reset_timer.expires = RUN_AT(HZ/2);\r\nadd_timer(&np->reset_timer);\r\nstop_nic_rxtx(ioaddr, 0);\r\nnetif_stop_queue(dev);\r\nnp->crvalue_sv = np->crvalue;\r\nnp->imrvalue_sv = np->imrvalue;\r\nnp->crvalue &= ~(CR_W_TXEN | CR_W_RXEN);\r\nnp->imrvalue = 0;\r\n}\r\nbreak;\r\n}\r\n} while (1);\r\ndev->stats.rx_missed_errors += ioread32(ioaddr + TALLY) & 0x7fff;\r\ndev->stats.rx_crc_errors +=\r\n(ioread32(ioaddr + TALLY) & 0x7fff0000) >> 16;\r\nif (debug)\r\nprintk(KERN_DEBUG "%s: exiting interrupt, status=%#4.4x.\n",\r\ndev->name, ioread32(ioaddr + ISR));\r\niowrite32(np->imrvalue, ioaddr + IMR);\r\nspin_unlock(&np->lock);\r\nreturn IRQ_RETVAL(handled);\r\n}\r\nstatic int netdev_rx(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nwhile (!(np->cur_rx->status & RXOWN) && np->cur_rx->skbuff) {\r\ns32 rx_status = np->cur_rx->status;\r\nif (np->really_rx_count == 0)\r\nbreak;\r\nif (debug)\r\nprintk(KERN_DEBUG " netdev_rx() status was %8.8x.\n", rx_status);\r\nif ((!((rx_status & RXFSD) && (rx_status & RXLSD))) ||\r\n(rx_status & ErrorSummary)) {\r\nif (rx_status & ErrorSummary) {\r\nif (debug)\r\nprintk(KERN_DEBUG\r\n"%s: Receive error, Rx status %8.8x.\n",\r\ndev->name, rx_status);\r\ndev->stats.rx_errors++;\r\nif (rx_status & (LONG | RUNT))\r\ndev->stats.rx_length_errors++;\r\nif (rx_status & RXER)\r\ndev->stats.rx_frame_errors++;\r\nif (rx_status & CRC)\r\ndev->stats.rx_crc_errors++;\r\n} else {\r\nint need_to_reset = 0;\r\nint desno = 0;\r\nif (rx_status & RXFSD) {\r\nstruct fealnx_desc *cur;\r\ncur = np->cur_rx;\r\nwhile (desno <= np->really_rx_count) {\r\n++desno;\r\nif ((!(cur->status & RXOWN)) &&\r\n(cur->status & RXLSD))\r\nbreak;\r\ncur = cur->next_desc_logical;\r\n}\r\nif (desno > np->really_rx_count)\r\nneed_to_reset = 1;\r\n} else\r\nneed_to_reset = 1;\r\nif (need_to_reset == 0) {\r\nint i;\r\ndev->stats.rx_length_errors++;\r\nfor (i = 0; i < desno; ++i) {\r\nif (!np->cur_rx->skbuff) {\r\nprintk(KERN_DEBUG\r\n"%s: I'm scared\n", dev->name);\r\nbreak;\r\n}\r\nnp->cur_rx->status = RXOWN;\r\nnp->cur_rx = np->cur_rx->next_desc_logical;\r\n}\r\ncontinue;\r\n} else {\r\nstop_nic_rx(ioaddr, np->crvalue);\r\nreset_rx_descriptors(dev);\r\niowrite32(np->crvalue, ioaddr + TCRRCR);\r\n}\r\nbreak;\r\n}\r\n} else {\r\nstruct sk_buff *skb;\r\nshort pkt_len = ((rx_status & FLNGMASK) >> FLNGShift) - 4;\r\n#ifndef final_version\r\nif (debug)\r\nprintk(KERN_DEBUG " netdev_rx() normal Rx pkt length %d"\r\n" status %x.\n", pkt_len, rx_status);\r\n#endif\r\nif (pkt_len < rx_copybreak &&\r\n(skb = netdev_alloc_skb(dev, pkt_len + 2)) != NULL) {\r\nskb_reserve(skb, 2);\r\npci_dma_sync_single_for_cpu(np->pci_dev,\r\nnp->cur_rx->buffer,\r\nnp->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE);\r\n#if ! defined(__alpha__)\r\nskb_copy_to_linear_data(skb,\r\nnp->cur_rx->skbuff->data, pkt_len);\r\nskb_put(skb, pkt_len);\r\n#else\r\nmemcpy(skb_put(skb, pkt_len),\r\nnp->cur_rx->skbuff->data, pkt_len);\r\n#endif\r\npci_dma_sync_single_for_device(np->pci_dev,\r\nnp->cur_rx->buffer,\r\nnp->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE);\r\n} else {\r\npci_unmap_single(np->pci_dev,\r\nnp->cur_rx->buffer,\r\nnp->rx_buf_sz,\r\nPCI_DMA_FROMDEVICE);\r\nskb_put(skb = np->cur_rx->skbuff, pkt_len);\r\nnp->cur_rx->skbuff = NULL;\r\n--np->really_rx_count;\r\n}\r\nskb->protocol = eth_type_trans(skb, dev);\r\nnetif_rx(skb);\r\ndev->stats.rx_packets++;\r\ndev->stats.rx_bytes += pkt_len;\r\n}\r\nnp->cur_rx = np->cur_rx->next_desc_logical;\r\n}\r\nallocate_rx_buffers(dev);\r\nreturn 0;\r\n}\r\nstatic struct net_device_stats *get_stats(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nif (netif_running(dev)) {\r\ndev->stats.rx_missed_errors +=\r\nioread32(ioaddr + TALLY) & 0x7fff;\r\ndev->stats.rx_crc_errors +=\r\n(ioread32(ioaddr + TALLY) & 0x7fff0000) >> 16;\r\n}\r\nreturn &dev->stats;\r\n}\r\nstatic void set_rx_mode(struct net_device *dev)\r\n{\r\nspinlock_t *lp = &((struct netdev_private *)netdev_priv(dev))->lock;\r\nunsigned long flags;\r\nspin_lock_irqsave(lp, flags);\r\n__set_rx_mode(dev);\r\nspin_unlock_irqrestore(lp, flags);\r\n}\r\nstatic void __set_rx_mode(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nu32 mc_filter[2];\r\nu32 rx_mode;\r\nif (dev->flags & IFF_PROMISC) {\r\nmemset(mc_filter, 0xff, sizeof(mc_filter));\r\nrx_mode = CR_W_PROM | CR_W_AB | CR_W_AM;\r\n} else if ((netdev_mc_count(dev) > multicast_filter_limit) ||\r\n(dev->flags & IFF_ALLMULTI)) {\r\nmemset(mc_filter, 0xff, sizeof(mc_filter));\r\nrx_mode = CR_W_AB | CR_W_AM;\r\n} else {\r\nstruct netdev_hw_addr *ha;\r\nmemset(mc_filter, 0, sizeof(mc_filter));\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nunsigned int bit;\r\nbit = (ether_crc(ETH_ALEN, ha->addr) >> 26) ^ 0x3F;\r\nmc_filter[bit >> 5] |= (1 << bit);\r\n}\r\nrx_mode = CR_W_AB | CR_W_AM;\r\n}\r\nstop_nic_rxtx(ioaddr, np->crvalue);\r\niowrite32(mc_filter[0], ioaddr + MAR0);\r\niowrite32(mc_filter[1], ioaddr + MAR1);\r\nnp->crvalue &= ~CR_W_RXMODEMASK;\r\nnp->crvalue |= rx_mode;\r\niowrite32(np->crvalue, ioaddr + TCRRCR);\r\n}\r\nstatic void netdev_get_drvinfo(struct net_device *dev, struct ethtool_drvinfo *info)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\r\nstrlcpy(info->version, DRV_VERSION, sizeof(info->version));\r\nstrlcpy(info->bus_info, pci_name(np->pci_dev), sizeof(info->bus_info));\r\n}\r\nstatic int netdev_get_link_ksettings(struct net_device *dev,\r\nstruct ethtool_link_ksettings *cmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint rc;\r\nspin_lock_irq(&np->lock);\r\nrc = mii_ethtool_get_link_ksettings(&np->mii, cmd);\r\nspin_unlock_irq(&np->lock);\r\nreturn rc;\r\n}\r\nstatic int netdev_set_link_ksettings(struct net_device *dev,\r\nconst struct ethtool_link_ksettings *cmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint rc;\r\nspin_lock_irq(&np->lock);\r\nrc = mii_ethtool_set_link_ksettings(&np->mii, cmd);\r\nspin_unlock_irq(&np->lock);\r\nreturn rc;\r\n}\r\nstatic int netdev_nway_reset(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nreturn mii_nway_restart(&np->mii);\r\n}\r\nstatic u32 netdev_get_link(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nreturn mii_link_ok(&np->mii);\r\n}\r\nstatic u32 netdev_get_msglevel(struct net_device *dev)\r\n{\r\nreturn debug;\r\n}\r\nstatic void netdev_set_msglevel(struct net_device *dev, u32 value)\r\n{\r\ndebug = value;\r\n}\r\nstatic int mii_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nint rc;\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nspin_lock_irq(&np->lock);\r\nrc = generic_mii_ioctl(&np->mii, if_mii(rq), cmd, NULL);\r\nspin_unlock_irq(&np->lock);\r\nreturn rc;\r\n}\r\nstatic int netdev_close(struct net_device *dev)\r\n{\r\nstruct netdev_private *np = netdev_priv(dev);\r\nvoid __iomem *ioaddr = np->mem;\r\nint i;\r\nnetif_stop_queue(dev);\r\niowrite32(0x0000, ioaddr + IMR);\r\nstop_nic_rxtx(ioaddr, 0);\r\ndel_timer_sync(&np->timer);\r\ndel_timer_sync(&np->reset_timer);\r\nfree_irq(np->pci_dev->irq, dev);\r\nfor (i = 0; i < RX_RING_SIZE; i++) {\r\nstruct sk_buff *skb = np->rx_ring[i].skbuff;\r\nnp->rx_ring[i].status = 0;\r\nif (skb) {\r\npci_unmap_single(np->pci_dev, np->rx_ring[i].buffer,\r\nnp->rx_buf_sz, PCI_DMA_FROMDEVICE);\r\ndev_kfree_skb(skb);\r\nnp->rx_ring[i].skbuff = NULL;\r\n}\r\n}\r\nfor (i = 0; i < TX_RING_SIZE; i++) {\r\nstruct sk_buff *skb = np->tx_ring[i].skbuff;\r\nif (skb) {\r\npci_unmap_single(np->pci_dev, np->tx_ring[i].buffer,\r\nskb->len, PCI_DMA_TODEVICE);\r\ndev_kfree_skb(skb);\r\nnp->tx_ring[i].skbuff = NULL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init fealnx_init(void)\r\n{\r\n#ifdef MODULE\r\nprintk(version);\r\n#endif\r\nreturn pci_register_driver(&fealnx_driver);\r\n}\r\nstatic void __exit fealnx_exit(void)\r\n{\r\npci_unregister_driver(&fealnx_driver);\r\n}
