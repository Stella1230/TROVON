static void __intel_pmu_lbr_enable(bool pmi)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nu64 debugctl, lbr_select = 0, orig_debugctl;\r\nif (pmi && x86_pmu.version >= 4)\r\nreturn;\r\nif (cpuc->lbr_sel)\r\nlbr_select = cpuc->lbr_sel->config & x86_pmu.lbr_sel_mask;\r\nif (!pmi && cpuc->lbr_sel)\r\nwrmsrl(MSR_LBR_SELECT, lbr_select);\r\nrdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\r\norig_debugctl = debugctl;\r\ndebugctl |= DEBUGCTLMSR_LBR;\r\nif (!(lbr_select & LBR_CALL_STACK))\r\ndebugctl |= DEBUGCTLMSR_FREEZE_LBRS_ON_PMI;\r\nif (orig_debugctl != debugctl)\r\nwrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\r\n}\r\nstatic void __intel_pmu_lbr_disable(void)\r\n{\r\nu64 debugctl;\r\nrdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\r\ndebugctl &= ~(DEBUGCTLMSR_LBR | DEBUGCTLMSR_FREEZE_LBRS_ON_PMI);\r\nwrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\r\n}\r\nstatic void intel_pmu_lbr_reset_32(void)\r\n{\r\nint i;\r\nfor (i = 0; i < x86_pmu.lbr_nr; i++)\r\nwrmsrl(x86_pmu.lbr_from + i, 0);\r\n}\r\nstatic void intel_pmu_lbr_reset_64(void)\r\n{\r\nint i;\r\nfor (i = 0; i < x86_pmu.lbr_nr; i++) {\r\nwrmsrl(x86_pmu.lbr_from + i, 0);\r\nwrmsrl(x86_pmu.lbr_to + i, 0);\r\nif (x86_pmu.intel_cap.lbr_format == LBR_FORMAT_INFO)\r\nwrmsrl(MSR_LBR_INFO_0 + i, 0);\r\n}\r\n}\r\nvoid intel_pmu_lbr_reset(void)\r\n{\r\nif (!x86_pmu.lbr_nr)\r\nreturn;\r\nif (x86_pmu.intel_cap.lbr_format == LBR_FORMAT_32)\r\nintel_pmu_lbr_reset_32();\r\nelse\r\nintel_pmu_lbr_reset_64();\r\n}\r\nstatic inline u64 intel_pmu_lbr_tos(void)\r\n{\r\nu64 tos;\r\nrdmsrl(x86_pmu.lbr_tos, tos);\r\nreturn tos;\r\n}\r\nstatic inline bool lbr_from_signext_quirk_needed(void)\r\n{\r\nint lbr_format = x86_pmu.intel_cap.lbr_format;\r\nbool tsx_support = boot_cpu_has(X86_FEATURE_HLE) ||\r\nboot_cpu_has(X86_FEATURE_RTM);\r\nreturn !tsx_support && (lbr_desc[lbr_format] & LBR_TSX);\r\n}\r\ninline u64 lbr_from_signext_quirk_wr(u64 val)\r\n{\r\nif (static_branch_unlikely(&lbr_from_quirk_key)) {\r\nval |= (LBR_FROM_SIGNEXT_2MSB & val) << 2;\r\n}\r\nreturn val;\r\n}\r\nu64 lbr_from_signext_quirk_rd(u64 val)\r\n{\r\nif (static_branch_unlikely(&lbr_from_quirk_key)) {\r\nval &= ~(LBR_FROM_FLAG_IN_TX | LBR_FROM_FLAG_ABORT);\r\n}\r\nreturn val;\r\n}\r\nstatic inline void wrlbr_from(unsigned int idx, u64 val)\r\n{\r\nval = lbr_from_signext_quirk_wr(val);\r\nwrmsrl(x86_pmu.lbr_from + idx, val);\r\n}\r\nstatic inline void wrlbr_to(unsigned int idx, u64 val)\r\n{\r\nwrmsrl(x86_pmu.lbr_to + idx, val);\r\n}\r\nstatic inline u64 rdlbr_from(unsigned int idx)\r\n{\r\nu64 val;\r\nrdmsrl(x86_pmu.lbr_from + idx, val);\r\nreturn lbr_from_signext_quirk_rd(val);\r\n}\r\nstatic inline u64 rdlbr_to(unsigned int idx)\r\n{\r\nu64 val;\r\nrdmsrl(x86_pmu.lbr_to + idx, val);\r\nreturn val;\r\n}\r\nstatic void __intel_pmu_lbr_restore(struct x86_perf_task_context *task_ctx)\r\n{\r\nint i;\r\nunsigned lbr_idx, mask;\r\nu64 tos;\r\nif (task_ctx->lbr_callstack_users == 0 ||\r\ntask_ctx->lbr_stack_state == LBR_NONE) {\r\nintel_pmu_lbr_reset();\r\nreturn;\r\n}\r\nmask = x86_pmu.lbr_nr - 1;\r\ntos = task_ctx->tos;\r\nfor (i = 0; i < tos; i++) {\r\nlbr_idx = (tos - i) & mask;\r\nwrlbr_from(lbr_idx, task_ctx->lbr_from[i]);\r\nwrlbr_to (lbr_idx, task_ctx->lbr_to[i]);\r\nif (x86_pmu.intel_cap.lbr_format == LBR_FORMAT_INFO)\r\nwrmsrl(MSR_LBR_INFO_0 + lbr_idx, task_ctx->lbr_info[i]);\r\n}\r\nwrmsrl(x86_pmu.lbr_tos, tos);\r\ntask_ctx->lbr_stack_state = LBR_NONE;\r\n}\r\nstatic void __intel_pmu_lbr_save(struct x86_perf_task_context *task_ctx)\r\n{\r\nunsigned lbr_idx, mask;\r\nu64 tos;\r\nint i;\r\nif (task_ctx->lbr_callstack_users == 0) {\r\ntask_ctx->lbr_stack_state = LBR_NONE;\r\nreturn;\r\n}\r\nmask = x86_pmu.lbr_nr - 1;\r\ntos = intel_pmu_lbr_tos();\r\nfor (i = 0; i < tos; i++) {\r\nlbr_idx = (tos - i) & mask;\r\ntask_ctx->lbr_from[i] = rdlbr_from(lbr_idx);\r\ntask_ctx->lbr_to[i] = rdlbr_to(lbr_idx);\r\nif (x86_pmu.intel_cap.lbr_format == LBR_FORMAT_INFO)\r\nrdmsrl(MSR_LBR_INFO_0 + lbr_idx, task_ctx->lbr_info[i]);\r\n}\r\ntask_ctx->tos = tos;\r\ntask_ctx->lbr_stack_state = LBR_VALID;\r\n}\r\nvoid intel_pmu_lbr_sched_task(struct perf_event_context *ctx, bool sched_in)\r\n{\r\nstruct x86_perf_task_context *task_ctx;\r\ntask_ctx = ctx ? ctx->task_ctx_data : NULL;\r\nif (task_ctx) {\r\nif (sched_in)\r\n__intel_pmu_lbr_restore(task_ctx);\r\nelse\r\n__intel_pmu_lbr_save(task_ctx);\r\nreturn;\r\n}\r\nif (sched_in)\r\nintel_pmu_lbr_reset();\r\n}\r\nstatic inline bool branch_user_callstack(unsigned br_sel)\r\n{\r\nreturn (br_sel & X86_BR_USER) && (br_sel & X86_BR_CALL_STACK);\r\n}\r\nvoid intel_pmu_lbr_add(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nstruct x86_perf_task_context *task_ctx;\r\nif (!x86_pmu.lbr_nr)\r\nreturn;\r\ncpuc->br_sel = event->hw.branch_reg.reg;\r\nif (branch_user_callstack(cpuc->br_sel) && event->ctx->task_ctx_data) {\r\ntask_ctx = event->ctx->task_ctx_data;\r\ntask_ctx->lbr_callstack_users++;\r\n}\r\nperf_sched_cb_inc(event->ctx->pmu);\r\nif (!cpuc->lbr_users++ && !event->total_time_running)\r\nintel_pmu_lbr_reset();\r\n}\r\nvoid intel_pmu_lbr_del(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nstruct x86_perf_task_context *task_ctx;\r\nif (!x86_pmu.lbr_nr)\r\nreturn;\r\nif (branch_user_callstack(cpuc->br_sel) &&\r\nevent->ctx->task_ctx_data) {\r\ntask_ctx = event->ctx->task_ctx_data;\r\ntask_ctx->lbr_callstack_users--;\r\n}\r\ncpuc->lbr_users--;\r\nWARN_ON_ONCE(cpuc->lbr_users < 0);\r\nperf_sched_cb_dec(event->ctx->pmu);\r\n}\r\nvoid intel_pmu_lbr_enable_all(bool pmi)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nif (cpuc->lbr_users)\r\n__intel_pmu_lbr_enable(pmi);\r\n}\r\nvoid intel_pmu_lbr_disable_all(void)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nif (cpuc->lbr_users)\r\n__intel_pmu_lbr_disable();\r\n}\r\nstatic void intel_pmu_lbr_read_32(struct cpu_hw_events *cpuc)\r\n{\r\nunsigned long mask = x86_pmu.lbr_nr - 1;\r\nu64 tos = intel_pmu_lbr_tos();\r\nint i;\r\nfor (i = 0; i < x86_pmu.lbr_nr; i++) {\r\nunsigned long lbr_idx = (tos - i) & mask;\r\nunion {\r\nstruct {\r\nu32 from;\r\nu32 to;\r\n};\r\nu64 lbr;\r\n} msr_lastbranch;\r\nrdmsrl(x86_pmu.lbr_from + lbr_idx, msr_lastbranch.lbr);\r\ncpuc->lbr_entries[i].from = msr_lastbranch.from;\r\ncpuc->lbr_entries[i].to = msr_lastbranch.to;\r\ncpuc->lbr_entries[i].mispred = 0;\r\ncpuc->lbr_entries[i].predicted = 0;\r\ncpuc->lbr_entries[i].in_tx = 0;\r\ncpuc->lbr_entries[i].abort = 0;\r\ncpuc->lbr_entries[i].cycles = 0;\r\ncpuc->lbr_entries[i].reserved = 0;\r\n}\r\ncpuc->lbr_stack.nr = i;\r\n}\r\nstatic void intel_pmu_lbr_read_64(struct cpu_hw_events *cpuc)\r\n{\r\nbool need_info = false;\r\nunsigned long mask = x86_pmu.lbr_nr - 1;\r\nint lbr_format = x86_pmu.intel_cap.lbr_format;\r\nu64 tos = intel_pmu_lbr_tos();\r\nint i;\r\nint out = 0;\r\nint num = x86_pmu.lbr_nr;\r\nif (cpuc->lbr_sel) {\r\nneed_info = !(cpuc->lbr_sel->config & LBR_NO_INFO);\r\nif (cpuc->lbr_sel->config & LBR_CALL_STACK)\r\nnum = tos;\r\n}\r\nfor (i = 0; i < num; i++) {\r\nunsigned long lbr_idx = (tos - i) & mask;\r\nu64 from, to, mis = 0, pred = 0, in_tx = 0, abort = 0;\r\nint skip = 0;\r\nu16 cycles = 0;\r\nint lbr_flags = lbr_desc[lbr_format];\r\nfrom = rdlbr_from(lbr_idx);\r\nto = rdlbr_to(lbr_idx);\r\nif (lbr_format == LBR_FORMAT_INFO && need_info) {\r\nu64 info;\r\nrdmsrl(MSR_LBR_INFO_0 + lbr_idx, info);\r\nmis = !!(info & LBR_INFO_MISPRED);\r\npred = !mis;\r\nin_tx = !!(info & LBR_INFO_IN_TX);\r\nabort = !!(info & LBR_INFO_ABORT);\r\ncycles = (info & LBR_INFO_CYCLES);\r\n}\r\nif (lbr_format == LBR_FORMAT_TIME) {\r\nmis = !!(from & LBR_FROM_FLAG_MISPRED);\r\npred = !mis;\r\nskip = 1;\r\ncycles = ((to >> 48) & LBR_INFO_CYCLES);\r\nto = (u64)((((s64)to) << 16) >> 16);\r\n}\r\nif (lbr_flags & LBR_EIP_FLAGS) {\r\nmis = !!(from & LBR_FROM_FLAG_MISPRED);\r\npred = !mis;\r\nskip = 1;\r\n}\r\nif (lbr_flags & LBR_TSX) {\r\nin_tx = !!(from & LBR_FROM_FLAG_IN_TX);\r\nabort = !!(from & LBR_FROM_FLAG_ABORT);\r\nskip = 3;\r\n}\r\nfrom = (u64)((((s64)from) << skip) >> skip);\r\nif (abort && x86_pmu.lbr_double_abort && out > 0)\r\nout--;\r\ncpuc->lbr_entries[out].from = from;\r\ncpuc->lbr_entries[out].to = to;\r\ncpuc->lbr_entries[out].mispred = mis;\r\ncpuc->lbr_entries[out].predicted = pred;\r\ncpuc->lbr_entries[out].in_tx = in_tx;\r\ncpuc->lbr_entries[out].abort = abort;\r\ncpuc->lbr_entries[out].cycles = cycles;\r\ncpuc->lbr_entries[out].reserved = 0;\r\nout++;\r\n}\r\ncpuc->lbr_stack.nr = out;\r\n}\r\nvoid intel_pmu_lbr_read(void)\r\n{\r\nstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\r\nif (!cpuc->lbr_users)\r\nreturn;\r\nif (x86_pmu.intel_cap.lbr_format == LBR_FORMAT_32)\r\nintel_pmu_lbr_read_32(cpuc);\r\nelse\r\nintel_pmu_lbr_read_64(cpuc);\r\nintel_pmu_lbr_filter(cpuc);\r\n}\r\nstatic int intel_pmu_setup_sw_lbr_filter(struct perf_event *event)\r\n{\r\nu64 br_type = event->attr.branch_sample_type;\r\nint mask = 0;\r\nif (br_type & PERF_SAMPLE_BRANCH_USER)\r\nmask |= X86_BR_USER;\r\nif (br_type & PERF_SAMPLE_BRANCH_KERNEL)\r\nmask |= X86_BR_KERNEL;\r\nif (br_type & PERF_SAMPLE_BRANCH_ANY)\r\nmask |= X86_BR_ANY;\r\nif (br_type & PERF_SAMPLE_BRANCH_ANY_CALL)\r\nmask |= X86_BR_ANY_CALL;\r\nif (br_type & PERF_SAMPLE_BRANCH_ANY_RETURN)\r\nmask |= X86_BR_RET | X86_BR_IRET | X86_BR_SYSRET;\r\nif (br_type & PERF_SAMPLE_BRANCH_IND_CALL)\r\nmask |= X86_BR_IND_CALL;\r\nif (br_type & PERF_SAMPLE_BRANCH_ABORT_TX)\r\nmask |= X86_BR_ABORT;\r\nif (br_type & PERF_SAMPLE_BRANCH_IN_TX)\r\nmask |= X86_BR_IN_TX;\r\nif (br_type & PERF_SAMPLE_BRANCH_NO_TX)\r\nmask |= X86_BR_NO_TX;\r\nif (br_type & PERF_SAMPLE_BRANCH_COND)\r\nmask |= X86_BR_JCC;\r\nif (br_type & PERF_SAMPLE_BRANCH_CALL_STACK) {\r\nif (!x86_pmu_has_lbr_callstack())\r\nreturn -EOPNOTSUPP;\r\nif (mask & ~(X86_BR_USER | X86_BR_KERNEL))\r\nreturn -EINVAL;\r\nmask |= X86_BR_CALL | X86_BR_IND_CALL | X86_BR_RET |\r\nX86_BR_CALL_STACK;\r\n}\r\nif (br_type & PERF_SAMPLE_BRANCH_IND_JUMP)\r\nmask |= X86_BR_IND_JMP;\r\nif (br_type & PERF_SAMPLE_BRANCH_CALL)\r\nmask |= X86_BR_CALL | X86_BR_ZERO_CALL;\r\nevent->hw.branch_reg.reg = mask;\r\nreturn 0;\r\n}\r\nstatic int intel_pmu_setup_hw_lbr_filter(struct perf_event *event)\r\n{\r\nstruct hw_perf_event_extra *reg;\r\nu64 br_type = event->attr.branch_sample_type;\r\nu64 mask = 0, v;\r\nint i;\r\nfor (i = 0; i < PERF_SAMPLE_BRANCH_MAX_SHIFT; i++) {\r\nif (!(br_type & (1ULL << i)))\r\ncontinue;\r\nv = x86_pmu.lbr_sel_map[i];\r\nif (v == LBR_NOT_SUPP)\r\nreturn -EOPNOTSUPP;\r\nif (v != LBR_IGN)\r\nmask |= v;\r\n}\r\nreg = &event->hw.branch_reg;\r\nreg->idx = EXTRA_REG_LBR;\r\nreg->config = mask ^ (x86_pmu.lbr_sel_mask & ~LBR_CALL_STACK);\r\nif ((br_type & PERF_SAMPLE_BRANCH_NO_CYCLES) &&\r\n(br_type & PERF_SAMPLE_BRANCH_NO_FLAGS) &&\r\n(x86_pmu.intel_cap.lbr_format == LBR_FORMAT_INFO))\r\nreg->config |= LBR_NO_INFO;\r\nreturn 0;\r\n}\r\nint intel_pmu_setup_lbr_filter(struct perf_event *event)\r\n{\r\nint ret = 0;\r\nif (!x86_pmu.lbr_nr)\r\nreturn -EOPNOTSUPP;\r\nret = intel_pmu_setup_sw_lbr_filter(event);\r\nif (ret)\r\nreturn ret;\r\nif (x86_pmu.lbr_sel_map)\r\nret = intel_pmu_setup_hw_lbr_filter(event);\r\nreturn ret;\r\n}\r\nstatic int branch_type(unsigned long from, unsigned long to, int abort)\r\n{\r\nstruct insn insn;\r\nvoid *addr;\r\nint bytes_read, bytes_left;\r\nint ret = X86_BR_NONE;\r\nint ext, to_plm, from_plm;\r\nu8 buf[MAX_INSN_SIZE];\r\nint is64 = 0;\r\nto_plm = kernel_ip(to) ? X86_BR_KERNEL : X86_BR_USER;\r\nfrom_plm = kernel_ip(from) ? X86_BR_KERNEL : X86_BR_USER;\r\nif (from == 0 || to == 0)\r\nreturn X86_BR_NONE;\r\nif (abort)\r\nreturn X86_BR_ABORT | to_plm;\r\nif (from_plm == X86_BR_USER) {\r\nif (!current->mm)\r\nreturn X86_BR_NONE;\r\nbytes_left = copy_from_user_nmi(buf, (void __user *)from,\r\nMAX_INSN_SIZE);\r\nbytes_read = MAX_INSN_SIZE - bytes_left;\r\nif (!bytes_read)\r\nreturn X86_BR_NONE;\r\naddr = buf;\r\n} else {\r\nif (kernel_text_address(from)) {\r\naddr = (void *)from;\r\nbytes_read = MAX_INSN_SIZE;\r\n} else {\r\nreturn X86_BR_NONE;\r\n}\r\n}\r\n#ifdef CONFIG_X86_64\r\nis64 = kernel_ip((unsigned long)addr) || !test_thread_flag(TIF_IA32);\r\n#endif\r\ninsn_init(&insn, addr, bytes_read, is64);\r\ninsn_get_opcode(&insn);\r\nif (!insn.opcode.got)\r\nreturn X86_BR_ABORT;\r\nswitch (insn.opcode.bytes[0]) {\r\ncase 0xf:\r\nswitch (insn.opcode.bytes[1]) {\r\ncase 0x05:\r\ncase 0x34:\r\nret = X86_BR_SYSCALL;\r\nbreak;\r\ncase 0x07:\r\ncase 0x35:\r\nret = X86_BR_SYSRET;\r\nbreak;\r\ncase 0x80 ... 0x8f:\r\nret = X86_BR_JCC;\r\nbreak;\r\ndefault:\r\nret = X86_BR_NONE;\r\n}\r\nbreak;\r\ncase 0x70 ... 0x7f:\r\nret = X86_BR_JCC;\r\nbreak;\r\ncase 0xc2:\r\ncase 0xc3:\r\ncase 0xca:\r\ncase 0xcb:\r\nret = X86_BR_RET;\r\nbreak;\r\ncase 0xcf:\r\nret = X86_BR_IRET;\r\nbreak;\r\ncase 0xcc ... 0xce:\r\nret = X86_BR_INT;\r\nbreak;\r\ncase 0xe8:\r\ninsn_get_immediate(&insn);\r\nif (insn.immediate1.value == 0) {\r\nret = X86_BR_ZERO_CALL;\r\nbreak;\r\n}\r\ncase 0x9a:\r\nret = X86_BR_CALL;\r\nbreak;\r\ncase 0xe0 ... 0xe3:\r\nret = X86_BR_JCC;\r\nbreak;\r\ncase 0xe9 ... 0xeb:\r\nret = X86_BR_JMP;\r\nbreak;\r\ncase 0xff:\r\ninsn_get_modrm(&insn);\r\next = (insn.modrm.bytes[0] >> 3) & 0x7;\r\nswitch (ext) {\r\ncase 2:\r\ncase 3:\r\nret = X86_BR_IND_CALL;\r\nbreak;\r\ncase 4:\r\ncase 5:\r\nret = X86_BR_IND_JMP;\r\nbreak;\r\n}\r\nbreak;\r\ndefault:\r\nret = X86_BR_NONE;\r\n}\r\nif (from_plm == X86_BR_USER && to_plm == X86_BR_KERNEL\r\n&& ret != X86_BR_SYSCALL && ret != X86_BR_INT)\r\nret = X86_BR_IRQ;\r\nif (ret != X86_BR_NONE)\r\nret |= to_plm;\r\nreturn ret;\r\n}\r\nstatic void\r\nintel_pmu_lbr_filter(struct cpu_hw_events *cpuc)\r\n{\r\nu64 from, to;\r\nint br_sel = cpuc->br_sel;\r\nint i, j, type;\r\nbool compress = false;\r\nif ((br_sel & X86_BR_ALL) == X86_BR_ALL)\r\nreturn;\r\nfor (i = 0; i < cpuc->lbr_stack.nr; i++) {\r\nfrom = cpuc->lbr_entries[i].from;\r\nto = cpuc->lbr_entries[i].to;\r\ntype = branch_type(from, to, cpuc->lbr_entries[i].abort);\r\nif (type != X86_BR_NONE && (br_sel & X86_BR_ANYTX)) {\r\nif (cpuc->lbr_entries[i].in_tx)\r\ntype |= X86_BR_IN_TX;\r\nelse\r\ntype |= X86_BR_NO_TX;\r\n}\r\nif (type == X86_BR_NONE || (br_sel & type) != type) {\r\ncpuc->lbr_entries[i].from = 0;\r\ncompress = true;\r\n}\r\n}\r\nif (!compress)\r\nreturn;\r\nfor (i = 0; i < cpuc->lbr_stack.nr; ) {\r\nif (!cpuc->lbr_entries[i].from) {\r\nj = i;\r\nwhile (++j < cpuc->lbr_stack.nr)\r\ncpuc->lbr_entries[j-1] = cpuc->lbr_entries[j];\r\ncpuc->lbr_stack.nr--;\r\nif (!cpuc->lbr_entries[i].from)\r\ncontinue;\r\n}\r\ni++;\r\n}\r\n}\r\nvoid __init intel_pmu_lbr_init_core(void)\r\n{\r\nx86_pmu.lbr_nr = 4;\r\nx86_pmu.lbr_tos = MSR_LBR_TOS;\r\nx86_pmu.lbr_from = MSR_LBR_CORE_FROM;\r\nx86_pmu.lbr_to = MSR_LBR_CORE_TO;\r\n}\r\nvoid __init intel_pmu_lbr_init_nhm(void)\r\n{\r\nx86_pmu.lbr_nr = 16;\r\nx86_pmu.lbr_tos = MSR_LBR_TOS;\r\nx86_pmu.lbr_from = MSR_LBR_NHM_FROM;\r\nx86_pmu.lbr_to = MSR_LBR_NHM_TO;\r\nx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\r\nx86_pmu.lbr_sel_map = nhm_lbr_sel_map;\r\n}\r\nvoid __init intel_pmu_lbr_init_snb(void)\r\n{\r\nx86_pmu.lbr_nr = 16;\r\nx86_pmu.lbr_tos = MSR_LBR_TOS;\r\nx86_pmu.lbr_from = MSR_LBR_NHM_FROM;\r\nx86_pmu.lbr_to = MSR_LBR_NHM_TO;\r\nx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\r\nx86_pmu.lbr_sel_map = snb_lbr_sel_map;\r\n}\r\nvoid intel_pmu_lbr_init_hsw(void)\r\n{\r\nx86_pmu.lbr_nr = 16;\r\nx86_pmu.lbr_tos = MSR_LBR_TOS;\r\nx86_pmu.lbr_from = MSR_LBR_NHM_FROM;\r\nx86_pmu.lbr_to = MSR_LBR_NHM_TO;\r\nx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\r\nx86_pmu.lbr_sel_map = hsw_lbr_sel_map;\r\nif (lbr_from_signext_quirk_needed())\r\nstatic_branch_enable(&lbr_from_quirk_key);\r\n}\r\n__init void intel_pmu_lbr_init_skl(void)\r\n{\r\nx86_pmu.lbr_nr = 32;\r\nx86_pmu.lbr_tos = MSR_LBR_TOS;\r\nx86_pmu.lbr_from = MSR_LBR_NHM_FROM;\r\nx86_pmu.lbr_to = MSR_LBR_NHM_TO;\r\nx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\r\nx86_pmu.lbr_sel_map = hsw_lbr_sel_map;\r\n}\r\nvoid __init intel_pmu_lbr_init_atom(void)\r\n{\r\nif (boot_cpu_data.x86_model == 28\r\n&& boot_cpu_data.x86_mask < 10) {\r\npr_cont("LBR disabled due to erratum");\r\nreturn;\r\n}\r\nx86_pmu.lbr_nr = 8;\r\nx86_pmu.lbr_tos = MSR_LBR_TOS;\r\nx86_pmu.lbr_from = MSR_LBR_CORE_FROM;\r\nx86_pmu.lbr_to = MSR_LBR_CORE_TO;\r\n}\r\nvoid __init intel_pmu_lbr_init_slm(void)\r\n{\r\nx86_pmu.lbr_nr = 8;\r\nx86_pmu.lbr_tos = MSR_LBR_TOS;\r\nx86_pmu.lbr_from = MSR_LBR_CORE_FROM;\r\nx86_pmu.lbr_to = MSR_LBR_CORE_TO;\r\nx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\r\nx86_pmu.lbr_sel_map = nhm_lbr_sel_map;\r\npr_cont("8-deep LBR, ");\r\n}\r\nvoid intel_pmu_lbr_init_knl(void)\r\n{\r\nx86_pmu.lbr_nr = 8;\r\nx86_pmu.lbr_tos = MSR_LBR_TOS;\r\nx86_pmu.lbr_from = MSR_LBR_NHM_FROM;\r\nx86_pmu.lbr_to = MSR_LBR_NHM_TO;\r\nx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\r\nx86_pmu.lbr_sel_map = snb_lbr_sel_map;\r\n}
