static inline u8 *ef4_rx_buf_va(struct ef4_rx_buffer *buf)\r\n{\r\nreturn page_address(buf->page) + buf->page_offset;\r\n}\r\nstatic inline u32 ef4_rx_buf_hash(struct ef4_nic *efx, const u8 *eh)\r\n{\r\n#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)\r\nreturn __le32_to_cpup((const __le32 *)(eh + efx->rx_packet_hash_offset));\r\n#else\r\nconst u8 *data = eh + efx->rx_packet_hash_offset;\r\nreturn (u32)data[0] |\r\n(u32)data[1] << 8 |\r\n(u32)data[2] << 16 |\r\n(u32)data[3] << 24;\r\n#endif\r\n}\r\nstatic inline struct ef4_rx_buffer *\r\nef4_rx_buf_next(struct ef4_rx_queue *rx_queue, struct ef4_rx_buffer *rx_buf)\r\n{\r\nif (unlikely(rx_buf == ef4_rx_buffer(rx_queue, rx_queue->ptr_mask)))\r\nreturn ef4_rx_buffer(rx_queue, 0);\r\nelse\r\nreturn rx_buf + 1;\r\n}\r\nstatic inline void ef4_sync_rx_buffer(struct ef4_nic *efx,\r\nstruct ef4_rx_buffer *rx_buf,\r\nunsigned int len)\r\n{\r\ndma_sync_single_for_cpu(&efx->pci_dev->dev, rx_buf->dma_addr, len,\r\nDMA_FROM_DEVICE);\r\n}\r\nvoid ef4_rx_config_page_split(struct ef4_nic *efx)\r\n{\r\nefx->rx_page_buf_step = ALIGN(efx->rx_dma_len + efx->rx_ip_align,\r\nEF4_RX_BUF_ALIGNMENT);\r\nefx->rx_bufs_per_page = efx->rx_buffer_order ? 1 :\r\n((PAGE_SIZE - sizeof(struct ef4_rx_page_state)) /\r\nefx->rx_page_buf_step);\r\nefx->rx_buffer_truesize = (PAGE_SIZE << efx->rx_buffer_order) /\r\nefx->rx_bufs_per_page;\r\nefx->rx_pages_per_batch = DIV_ROUND_UP(EF4_RX_PREFERRED_BATCH,\r\nefx->rx_bufs_per_page);\r\n}\r\nstatic struct page *ef4_reuse_page(struct ef4_rx_queue *rx_queue)\r\n{\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nstruct page *page;\r\nstruct ef4_rx_page_state *state;\r\nunsigned index;\r\nindex = rx_queue->page_remove & rx_queue->page_ptr_mask;\r\npage = rx_queue->page_ring[index];\r\nif (page == NULL)\r\nreturn NULL;\r\nrx_queue->page_ring[index] = NULL;\r\nif (rx_queue->page_remove != rx_queue->page_add)\r\n++rx_queue->page_remove;\r\nif (page_count(page) == 1) {\r\n++rx_queue->page_recycle_count;\r\nreturn page;\r\n} else {\r\nstate = page_address(page);\r\ndma_unmap_page(&efx->pci_dev->dev, state->dma_addr,\r\nPAGE_SIZE << efx->rx_buffer_order,\r\nDMA_FROM_DEVICE);\r\nput_page(page);\r\n++rx_queue->page_recycle_failed;\r\n}\r\nreturn NULL;\r\n}\r\nstatic int ef4_init_rx_buffers(struct ef4_rx_queue *rx_queue, bool atomic)\r\n{\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nstruct ef4_rx_buffer *rx_buf;\r\nstruct page *page;\r\nunsigned int page_offset;\r\nstruct ef4_rx_page_state *state;\r\ndma_addr_t dma_addr;\r\nunsigned index, count;\r\ncount = 0;\r\ndo {\r\npage = ef4_reuse_page(rx_queue);\r\nif (page == NULL) {\r\npage = alloc_pages(__GFP_COLD | __GFP_COMP |\r\n(atomic ? GFP_ATOMIC : GFP_KERNEL),\r\nefx->rx_buffer_order);\r\nif (unlikely(page == NULL))\r\nreturn -ENOMEM;\r\ndma_addr =\r\ndma_map_page(&efx->pci_dev->dev, page, 0,\r\nPAGE_SIZE << efx->rx_buffer_order,\r\nDMA_FROM_DEVICE);\r\nif (unlikely(dma_mapping_error(&efx->pci_dev->dev,\r\ndma_addr))) {\r\n__free_pages(page, efx->rx_buffer_order);\r\nreturn -EIO;\r\n}\r\nstate = page_address(page);\r\nstate->dma_addr = dma_addr;\r\n} else {\r\nstate = page_address(page);\r\ndma_addr = state->dma_addr;\r\n}\r\ndma_addr += sizeof(struct ef4_rx_page_state);\r\npage_offset = sizeof(struct ef4_rx_page_state);\r\ndo {\r\nindex = rx_queue->added_count & rx_queue->ptr_mask;\r\nrx_buf = ef4_rx_buffer(rx_queue, index);\r\nrx_buf->dma_addr = dma_addr + efx->rx_ip_align;\r\nrx_buf->page = page;\r\nrx_buf->page_offset = page_offset + efx->rx_ip_align;\r\nrx_buf->len = efx->rx_dma_len;\r\nrx_buf->flags = 0;\r\n++rx_queue->added_count;\r\nget_page(page);\r\ndma_addr += efx->rx_page_buf_step;\r\npage_offset += efx->rx_page_buf_step;\r\n} while (page_offset + efx->rx_page_buf_step <= PAGE_SIZE);\r\nrx_buf->flags = EF4_RX_BUF_LAST_IN_PAGE;\r\n} while (++count < efx->rx_pages_per_batch);\r\nreturn 0;\r\n}\r\nstatic void ef4_unmap_rx_buffer(struct ef4_nic *efx,\r\nstruct ef4_rx_buffer *rx_buf)\r\n{\r\nstruct page *page = rx_buf->page;\r\nif (page) {\r\nstruct ef4_rx_page_state *state = page_address(page);\r\ndma_unmap_page(&efx->pci_dev->dev,\r\nstate->dma_addr,\r\nPAGE_SIZE << efx->rx_buffer_order,\r\nDMA_FROM_DEVICE);\r\n}\r\n}\r\nstatic void ef4_free_rx_buffers(struct ef4_rx_queue *rx_queue,\r\nstruct ef4_rx_buffer *rx_buf,\r\nunsigned int num_bufs)\r\n{\r\ndo {\r\nif (rx_buf->page) {\r\nput_page(rx_buf->page);\r\nrx_buf->page = NULL;\r\n}\r\nrx_buf = ef4_rx_buf_next(rx_queue, rx_buf);\r\n} while (--num_bufs);\r\n}\r\nstatic void ef4_recycle_rx_page(struct ef4_channel *channel,\r\nstruct ef4_rx_buffer *rx_buf)\r\n{\r\nstruct page *page = rx_buf->page;\r\nstruct ef4_rx_queue *rx_queue = ef4_channel_get_rx_queue(channel);\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nunsigned index;\r\nif (!(rx_buf->flags & EF4_RX_BUF_LAST_IN_PAGE))\r\nreturn;\r\nindex = rx_queue->page_add & rx_queue->page_ptr_mask;\r\nif (rx_queue->page_ring[index] == NULL) {\r\nunsigned read_index = rx_queue->page_remove &\r\nrx_queue->page_ptr_mask;\r\nif (read_index == index)\r\n++rx_queue->page_remove;\r\nrx_queue->page_ring[index] = page;\r\n++rx_queue->page_add;\r\nreturn;\r\n}\r\n++rx_queue->page_recycle_full;\r\nef4_unmap_rx_buffer(efx, rx_buf);\r\nput_page(rx_buf->page);\r\n}\r\nstatic void ef4_fini_rx_buffer(struct ef4_rx_queue *rx_queue,\r\nstruct ef4_rx_buffer *rx_buf)\r\n{\r\nif (rx_buf->page)\r\nput_page(rx_buf->page);\r\nif (rx_buf->flags & EF4_RX_BUF_LAST_IN_PAGE) {\r\nef4_unmap_rx_buffer(rx_queue->efx, rx_buf);\r\nef4_free_rx_buffers(rx_queue, rx_buf, 1);\r\n}\r\nrx_buf->page = NULL;\r\n}\r\nstatic void ef4_recycle_rx_pages(struct ef4_channel *channel,\r\nstruct ef4_rx_buffer *rx_buf,\r\nunsigned int n_frags)\r\n{\r\nstruct ef4_rx_queue *rx_queue = ef4_channel_get_rx_queue(channel);\r\ndo {\r\nef4_recycle_rx_page(channel, rx_buf);\r\nrx_buf = ef4_rx_buf_next(rx_queue, rx_buf);\r\n} while (--n_frags);\r\n}\r\nstatic void ef4_discard_rx_packet(struct ef4_channel *channel,\r\nstruct ef4_rx_buffer *rx_buf,\r\nunsigned int n_frags)\r\n{\r\nstruct ef4_rx_queue *rx_queue = ef4_channel_get_rx_queue(channel);\r\nef4_recycle_rx_pages(channel, rx_buf, n_frags);\r\nef4_free_rx_buffers(rx_queue, rx_buf, n_frags);\r\n}\r\nvoid ef4_fast_push_rx_descriptors(struct ef4_rx_queue *rx_queue, bool atomic)\r\n{\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nunsigned int fill_level, batch_size;\r\nint space, rc = 0;\r\nif (!rx_queue->refill_enabled)\r\nreturn;\r\nfill_level = (rx_queue->added_count - rx_queue->removed_count);\r\nEF4_BUG_ON_PARANOID(fill_level > rx_queue->efx->rxq_entries);\r\nif (fill_level >= rx_queue->fast_fill_trigger)\r\ngoto out;\r\nif (unlikely(fill_level < rx_queue->min_fill)) {\r\nif (fill_level)\r\nrx_queue->min_fill = fill_level;\r\n}\r\nbatch_size = efx->rx_pages_per_batch * efx->rx_bufs_per_page;\r\nspace = rx_queue->max_fill - fill_level;\r\nEF4_BUG_ON_PARANOID(space < batch_size);\r\nnetif_vdbg(rx_queue->efx, rx_status, rx_queue->efx->net_dev,\r\n"RX queue %d fast-filling descriptor ring from"\r\n" level %d to level %d\n",\r\nef4_rx_queue_index(rx_queue), fill_level,\r\nrx_queue->max_fill);\r\ndo {\r\nrc = ef4_init_rx_buffers(rx_queue, atomic);\r\nif (unlikely(rc)) {\r\nif (rx_queue->added_count == rx_queue->removed_count)\r\nef4_schedule_slow_fill(rx_queue);\r\ngoto out;\r\n}\r\n} while ((space -= batch_size) >= batch_size);\r\nnetif_vdbg(rx_queue->efx, rx_status, rx_queue->efx->net_dev,\r\n"RX queue %d fast-filled descriptor ring "\r\n"to level %d\n", ef4_rx_queue_index(rx_queue),\r\nrx_queue->added_count - rx_queue->removed_count);\r\nout:\r\nif (rx_queue->notified_count != rx_queue->added_count)\r\nef4_nic_notify_rx_desc(rx_queue);\r\n}\r\nvoid ef4_rx_slow_fill(unsigned long context)\r\n{\r\nstruct ef4_rx_queue *rx_queue = (struct ef4_rx_queue *)context;\r\nef4_nic_generate_fill_event(rx_queue);\r\n++rx_queue->slow_fill_count;\r\n}\r\nstatic void ef4_rx_packet__check_len(struct ef4_rx_queue *rx_queue,\r\nstruct ef4_rx_buffer *rx_buf,\r\nint len)\r\n{\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nunsigned max_len = rx_buf->len - efx->type->rx_buffer_padding;\r\nif (likely(len <= max_len))\r\nreturn;\r\nrx_buf->flags |= EF4_RX_PKT_DISCARD;\r\nif ((len > rx_buf->len) && EF4_WORKAROUND_8071(efx)) {\r\nif (net_ratelimit())\r\nnetif_err(efx, rx_err, efx->net_dev,\r\n" RX queue %d seriously overlength "\r\n"RX event (0x%x > 0x%x+0x%x). Leaking\n",\r\nef4_rx_queue_index(rx_queue), len, max_len,\r\nefx->type->rx_buffer_padding);\r\nef4_schedule_reset(efx, RESET_TYPE_RX_RECOVERY);\r\n} else {\r\nif (net_ratelimit())\r\nnetif_err(efx, rx_err, efx->net_dev,\r\n" RX queue %d overlength RX event "\r\n"(0x%x > 0x%x)\n",\r\nef4_rx_queue_index(rx_queue), len, max_len);\r\n}\r\nef4_rx_queue_channel(rx_queue)->n_rx_overlength++;\r\n}\r\nstatic void\r\nef4_rx_packet_gro(struct ef4_channel *channel, struct ef4_rx_buffer *rx_buf,\r\nunsigned int n_frags, u8 *eh)\r\n{\r\nstruct napi_struct *napi = &channel->napi_str;\r\ngro_result_t gro_result;\r\nstruct ef4_nic *efx = channel->efx;\r\nstruct sk_buff *skb;\r\nskb = napi_get_frags(napi);\r\nif (unlikely(!skb)) {\r\nstruct ef4_rx_queue *rx_queue;\r\nrx_queue = ef4_channel_get_rx_queue(channel);\r\nef4_free_rx_buffers(rx_queue, rx_buf, n_frags);\r\nreturn;\r\n}\r\nif (efx->net_dev->features & NETIF_F_RXHASH)\r\nskb_set_hash(skb, ef4_rx_buf_hash(efx, eh),\r\nPKT_HASH_TYPE_L3);\r\nskb->ip_summed = ((rx_buf->flags & EF4_RX_PKT_CSUMMED) ?\r\nCHECKSUM_UNNECESSARY : CHECKSUM_NONE);\r\nfor (;;) {\r\nskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\r\nrx_buf->page, rx_buf->page_offset,\r\nrx_buf->len);\r\nrx_buf->page = NULL;\r\nskb->len += rx_buf->len;\r\nif (skb_shinfo(skb)->nr_frags == n_frags)\r\nbreak;\r\nrx_buf = ef4_rx_buf_next(&channel->rx_queue, rx_buf);\r\n}\r\nskb->data_len = skb->len;\r\nskb->truesize += n_frags * efx->rx_buffer_truesize;\r\nskb_record_rx_queue(skb, channel->rx_queue.core_index);\r\ngro_result = napi_gro_frags(napi);\r\nif (gro_result != GRO_DROP)\r\nchannel->irq_mod_score += 2;\r\n}\r\nstatic struct sk_buff *ef4_rx_mk_skb(struct ef4_channel *channel,\r\nstruct ef4_rx_buffer *rx_buf,\r\nunsigned int n_frags,\r\nu8 *eh, int hdr_len)\r\n{\r\nstruct ef4_nic *efx = channel->efx;\r\nstruct sk_buff *skb;\r\nskb = netdev_alloc_skb(efx->net_dev,\r\nefx->rx_ip_align + efx->rx_prefix_size +\r\nhdr_len);\r\nif (unlikely(skb == NULL)) {\r\natomic_inc(&efx->n_rx_noskb_drops);\r\nreturn NULL;\r\n}\r\nEF4_BUG_ON_PARANOID(rx_buf->len < hdr_len);\r\nmemcpy(skb->data + efx->rx_ip_align, eh - efx->rx_prefix_size,\r\nefx->rx_prefix_size + hdr_len);\r\nskb_reserve(skb, efx->rx_ip_align + efx->rx_prefix_size);\r\n__skb_put(skb, hdr_len);\r\nif (rx_buf->len > hdr_len) {\r\nrx_buf->page_offset += hdr_len;\r\nrx_buf->len -= hdr_len;\r\nfor (;;) {\r\nskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\r\nrx_buf->page, rx_buf->page_offset,\r\nrx_buf->len);\r\nrx_buf->page = NULL;\r\nskb->len += rx_buf->len;\r\nskb->data_len += rx_buf->len;\r\nif (skb_shinfo(skb)->nr_frags == n_frags)\r\nbreak;\r\nrx_buf = ef4_rx_buf_next(&channel->rx_queue, rx_buf);\r\n}\r\n} else {\r\n__free_pages(rx_buf->page, efx->rx_buffer_order);\r\nrx_buf->page = NULL;\r\nn_frags = 0;\r\n}\r\nskb->truesize += n_frags * efx->rx_buffer_truesize;\r\nskb->protocol = eth_type_trans(skb, efx->net_dev);\r\nskb_mark_napi_id(skb, &channel->napi_str);\r\nreturn skb;\r\n}\r\nvoid ef4_rx_packet(struct ef4_rx_queue *rx_queue, unsigned int index,\r\nunsigned int n_frags, unsigned int len, u16 flags)\r\n{\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nstruct ef4_channel *channel = ef4_rx_queue_channel(rx_queue);\r\nstruct ef4_rx_buffer *rx_buf;\r\nrx_queue->rx_packets++;\r\nrx_buf = ef4_rx_buffer(rx_queue, index);\r\nrx_buf->flags |= flags;\r\nif (n_frags == 1) {\r\nif (!(flags & EF4_RX_PKT_PREFIX_LEN))\r\nef4_rx_packet__check_len(rx_queue, rx_buf, len);\r\n} else if (unlikely(n_frags > EF4_RX_MAX_FRAGS) ||\r\nunlikely(len <= (n_frags - 1) * efx->rx_dma_len) ||\r\nunlikely(len > n_frags * efx->rx_dma_len) ||\r\nunlikely(!efx->rx_scatter)) {\r\nWARN_ON(!(len == 0 && rx_buf->flags & EF4_RX_PKT_DISCARD));\r\nrx_buf->flags |= EF4_RX_PKT_DISCARD;\r\n}\r\nnetif_vdbg(efx, rx_status, efx->net_dev,\r\n"RX queue %d received ids %x-%x len %d %s%s\n",\r\nef4_rx_queue_index(rx_queue), index,\r\n(index + n_frags - 1) & rx_queue->ptr_mask, len,\r\n(rx_buf->flags & EF4_RX_PKT_CSUMMED) ? " [SUMMED]" : "",\r\n(rx_buf->flags & EF4_RX_PKT_DISCARD) ? " [DISCARD]" : "");\r\nif (unlikely(rx_buf->flags & EF4_RX_PKT_DISCARD)) {\r\nef4_rx_flush_packet(channel);\r\nef4_discard_rx_packet(channel, rx_buf, n_frags);\r\nreturn;\r\n}\r\nif (n_frags == 1 && !(flags & EF4_RX_PKT_PREFIX_LEN))\r\nrx_buf->len = len;\r\nef4_sync_rx_buffer(efx, rx_buf, rx_buf->len);\r\nprefetch(ef4_rx_buf_va(rx_buf));\r\nrx_buf->page_offset += efx->rx_prefix_size;\r\nrx_buf->len -= efx->rx_prefix_size;\r\nif (n_frags > 1) {\r\nunsigned int tail_frags = n_frags - 1;\r\nfor (;;) {\r\nrx_buf = ef4_rx_buf_next(rx_queue, rx_buf);\r\nif (--tail_frags == 0)\r\nbreak;\r\nef4_sync_rx_buffer(efx, rx_buf, efx->rx_dma_len);\r\n}\r\nrx_buf->len = len - (n_frags - 1) * efx->rx_dma_len;\r\nef4_sync_rx_buffer(efx, rx_buf, rx_buf->len);\r\n}\r\nrx_buf = ef4_rx_buffer(rx_queue, index);\r\nef4_recycle_rx_pages(channel, rx_buf, n_frags);\r\nef4_rx_flush_packet(channel);\r\nchannel->rx_pkt_n_frags = n_frags;\r\nchannel->rx_pkt_index = index;\r\n}\r\nstatic void ef4_rx_deliver(struct ef4_channel *channel, u8 *eh,\r\nstruct ef4_rx_buffer *rx_buf,\r\nunsigned int n_frags)\r\n{\r\nstruct sk_buff *skb;\r\nu16 hdr_len = min_t(u16, rx_buf->len, EF4_SKB_HEADERS);\r\nskb = ef4_rx_mk_skb(channel, rx_buf, n_frags, eh, hdr_len);\r\nif (unlikely(skb == NULL)) {\r\nstruct ef4_rx_queue *rx_queue;\r\nrx_queue = ef4_channel_get_rx_queue(channel);\r\nef4_free_rx_buffers(rx_queue, rx_buf, n_frags);\r\nreturn;\r\n}\r\nskb_record_rx_queue(skb, channel->rx_queue.core_index);\r\nskb_checksum_none_assert(skb);\r\nif (likely(rx_buf->flags & EF4_RX_PKT_CSUMMED))\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nif (channel->type->receive_skb)\r\nif (channel->type->receive_skb(channel, skb))\r\nreturn;\r\nnetif_receive_skb(skb);\r\n}\r\nvoid __ef4_rx_packet(struct ef4_channel *channel)\r\n{\r\nstruct ef4_nic *efx = channel->efx;\r\nstruct ef4_rx_buffer *rx_buf =\r\nef4_rx_buffer(&channel->rx_queue, channel->rx_pkt_index);\r\nu8 *eh = ef4_rx_buf_va(rx_buf);\r\nif (rx_buf->flags & EF4_RX_PKT_PREFIX_LEN)\r\nrx_buf->len = le16_to_cpup((__le16 *)\r\n(eh + efx->rx_packet_len_offset));\r\nif (unlikely(efx->loopback_selftest)) {\r\nstruct ef4_rx_queue *rx_queue;\r\nef4_loopback_rx_packet(efx, eh, rx_buf->len);\r\nrx_queue = ef4_channel_get_rx_queue(channel);\r\nef4_free_rx_buffers(rx_queue, rx_buf,\r\nchannel->rx_pkt_n_frags);\r\ngoto out;\r\n}\r\nif (unlikely(!(efx->net_dev->features & NETIF_F_RXCSUM)))\r\nrx_buf->flags &= ~EF4_RX_PKT_CSUMMED;\r\nif ((rx_buf->flags & EF4_RX_PKT_TCP) && !channel->type->receive_skb)\r\nef4_rx_packet_gro(channel, rx_buf, channel->rx_pkt_n_frags, eh);\r\nelse\r\nef4_rx_deliver(channel, eh, rx_buf, channel->rx_pkt_n_frags);\r\nout:\r\nchannel->rx_pkt_n_frags = 0;\r\n}\r\nint ef4_probe_rx_queue(struct ef4_rx_queue *rx_queue)\r\n{\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nunsigned int entries;\r\nint rc;\r\nentries = max(roundup_pow_of_two(efx->rxq_entries), EF4_MIN_DMAQ_SIZE);\r\nEF4_BUG_ON_PARANOID(entries > EF4_MAX_DMAQ_SIZE);\r\nrx_queue->ptr_mask = entries - 1;\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"creating RX queue %d size %#x mask %#x\n",\r\nef4_rx_queue_index(rx_queue), efx->rxq_entries,\r\nrx_queue->ptr_mask);\r\nrx_queue->buffer = kcalloc(entries, sizeof(*rx_queue->buffer),\r\nGFP_KERNEL);\r\nif (!rx_queue->buffer)\r\nreturn -ENOMEM;\r\nrc = ef4_nic_probe_rx(rx_queue);\r\nif (rc) {\r\nkfree(rx_queue->buffer);\r\nrx_queue->buffer = NULL;\r\n}\r\nreturn rc;\r\n}\r\nstatic void ef4_init_rx_recycle_ring(struct ef4_nic *efx,\r\nstruct ef4_rx_queue *rx_queue)\r\n{\r\nunsigned int bufs_in_recycle_ring, page_ring_size;\r\n#ifdef CONFIG_PPC64\r\nbufs_in_recycle_ring = EF4_RECYCLE_RING_SIZE_IOMMU;\r\n#else\r\nif (iommu_present(&pci_bus_type))\r\nbufs_in_recycle_ring = EF4_RECYCLE_RING_SIZE_IOMMU;\r\nelse\r\nbufs_in_recycle_ring = EF4_RECYCLE_RING_SIZE_NOIOMMU;\r\n#endif\r\npage_ring_size = roundup_pow_of_two(bufs_in_recycle_ring /\r\nefx->rx_bufs_per_page);\r\nrx_queue->page_ring = kcalloc(page_ring_size,\r\nsizeof(*rx_queue->page_ring), GFP_KERNEL);\r\nrx_queue->page_ptr_mask = page_ring_size - 1;\r\n}\r\nvoid ef4_init_rx_queue(struct ef4_rx_queue *rx_queue)\r\n{\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nunsigned int max_fill, trigger, max_trigger;\r\nnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\r\n"initialising RX queue %d\n", ef4_rx_queue_index(rx_queue));\r\nrx_queue->added_count = 0;\r\nrx_queue->notified_count = 0;\r\nrx_queue->removed_count = 0;\r\nrx_queue->min_fill = -1U;\r\nef4_init_rx_recycle_ring(efx, rx_queue);\r\nrx_queue->page_remove = 0;\r\nrx_queue->page_add = rx_queue->page_ptr_mask + 1;\r\nrx_queue->page_recycle_count = 0;\r\nrx_queue->page_recycle_failed = 0;\r\nrx_queue->page_recycle_full = 0;\r\nmax_fill = efx->rxq_entries - EF4_RXD_HEAD_ROOM;\r\nmax_trigger =\r\nmax_fill - efx->rx_pages_per_batch * efx->rx_bufs_per_page;\r\nif (rx_refill_threshold != 0) {\r\ntrigger = max_fill * min(rx_refill_threshold, 100U) / 100U;\r\nif (trigger > max_trigger)\r\ntrigger = max_trigger;\r\n} else {\r\ntrigger = max_trigger;\r\n}\r\nrx_queue->max_fill = max_fill;\r\nrx_queue->fast_fill_trigger = trigger;\r\nrx_queue->refill_enabled = true;\r\nef4_nic_init_rx(rx_queue);\r\n}\r\nvoid ef4_fini_rx_queue(struct ef4_rx_queue *rx_queue)\r\n{\r\nint i;\r\nstruct ef4_nic *efx = rx_queue->efx;\r\nstruct ef4_rx_buffer *rx_buf;\r\nnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\r\n"shutting down RX queue %d\n", ef4_rx_queue_index(rx_queue));\r\ndel_timer_sync(&rx_queue->slow_fill);\r\nif (rx_queue->buffer) {\r\nfor (i = rx_queue->removed_count; i < rx_queue->added_count;\r\ni++) {\r\nunsigned index = i & rx_queue->ptr_mask;\r\nrx_buf = ef4_rx_buffer(rx_queue, index);\r\nef4_fini_rx_buffer(rx_queue, rx_buf);\r\n}\r\n}\r\nfor (i = 0; i <= rx_queue->page_ptr_mask; i++) {\r\nstruct page *page = rx_queue->page_ring[i];\r\nstruct ef4_rx_page_state *state;\r\nif (page == NULL)\r\ncontinue;\r\nstate = page_address(page);\r\ndma_unmap_page(&efx->pci_dev->dev, state->dma_addr,\r\nPAGE_SIZE << efx->rx_buffer_order,\r\nDMA_FROM_DEVICE);\r\nput_page(page);\r\n}\r\nkfree(rx_queue->page_ring);\r\nrx_queue->page_ring = NULL;\r\n}\r\nvoid ef4_remove_rx_queue(struct ef4_rx_queue *rx_queue)\r\n{\r\nnetif_dbg(rx_queue->efx, drv, rx_queue->efx->net_dev,\r\n"destroying RX queue %d\n", ef4_rx_queue_index(rx_queue));\r\nef4_nic_remove_rx(rx_queue);\r\nkfree(rx_queue->buffer);\r\nrx_queue->buffer = NULL;\r\n}\r\nint ef4_filter_rfs(struct net_device *net_dev, const struct sk_buff *skb,\r\nu16 rxq_index, u32 flow_id)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nstruct ef4_channel *channel;\r\nstruct ef4_filter_spec spec;\r\nstruct flow_keys fk;\r\nint rc;\r\nif (flow_id == RPS_FLOW_ID_INVALID)\r\nreturn -EINVAL;\r\nif (!skb_flow_dissect_flow_keys(skb, &fk, 0))\r\nreturn -EPROTONOSUPPORT;\r\nif (fk.basic.n_proto != htons(ETH_P_IP) && fk.basic.n_proto != htons(ETH_P_IPV6))\r\nreturn -EPROTONOSUPPORT;\r\nif (fk.control.flags & FLOW_DIS_IS_FRAGMENT)\r\nreturn -EPROTONOSUPPORT;\r\nef4_filter_init_rx(&spec, EF4_FILTER_PRI_HINT,\r\nefx->rx_scatter ? EF4_FILTER_FLAG_RX_SCATTER : 0,\r\nrxq_index);\r\nspec.match_flags =\r\nEF4_FILTER_MATCH_ETHER_TYPE | EF4_FILTER_MATCH_IP_PROTO |\r\nEF4_FILTER_MATCH_LOC_HOST | EF4_FILTER_MATCH_LOC_PORT |\r\nEF4_FILTER_MATCH_REM_HOST | EF4_FILTER_MATCH_REM_PORT;\r\nspec.ether_type = fk.basic.n_proto;\r\nspec.ip_proto = fk.basic.ip_proto;\r\nif (fk.basic.n_proto == htons(ETH_P_IP)) {\r\nspec.rem_host[0] = fk.addrs.v4addrs.src;\r\nspec.loc_host[0] = fk.addrs.v4addrs.dst;\r\n} else {\r\nmemcpy(spec.rem_host, &fk.addrs.v6addrs.src, sizeof(struct in6_addr));\r\nmemcpy(spec.loc_host, &fk.addrs.v6addrs.dst, sizeof(struct in6_addr));\r\n}\r\nspec.rem_port = fk.ports.src;\r\nspec.loc_port = fk.ports.dst;\r\nrc = efx->type->filter_rfs_insert(efx, &spec);\r\nif (rc < 0)\r\nreturn rc;\r\nchannel = ef4_get_channel(efx, rxq_index);\r\nchannel->rps_flow_id[rc] = flow_id;\r\n++channel->rfs_filters_added;\r\nif (spec.ether_type == htons(ETH_P_IP))\r\nnetif_info(efx, rx_status, efx->net_dev,\r\n"steering %s %pI4:%u:%pI4:%u to queue %u [flow %u filter %d]\n",\r\n(spec.ip_proto == IPPROTO_TCP) ? "TCP" : "UDP",\r\nspec.rem_host, ntohs(spec.rem_port), spec.loc_host,\r\nntohs(spec.loc_port), rxq_index, flow_id, rc);\r\nelse\r\nnetif_info(efx, rx_status, efx->net_dev,\r\n"steering %s [%pI6]:%u:[%pI6]:%u to queue %u [flow %u filter %d]\n",\r\n(spec.ip_proto == IPPROTO_TCP) ? "TCP" : "UDP",\r\nspec.rem_host, ntohs(spec.rem_port), spec.loc_host,\r\nntohs(spec.loc_port), rxq_index, flow_id, rc);\r\nreturn rc;\r\n}\r\nbool __ef4_filter_rfs_expire(struct ef4_nic *efx, unsigned int quota)\r\n{\r\nbool (*expire_one)(struct ef4_nic *efx, u32 flow_id, unsigned int index);\r\nunsigned int channel_idx, index, size;\r\nu32 flow_id;\r\nif (!spin_trylock_bh(&efx->filter_lock))\r\nreturn false;\r\nexpire_one = efx->type->filter_rfs_expire_one;\r\nchannel_idx = efx->rps_expire_channel;\r\nindex = efx->rps_expire_index;\r\nsize = efx->type->max_rx_ip_filters;\r\nwhile (quota--) {\r\nstruct ef4_channel *channel = ef4_get_channel(efx, channel_idx);\r\nflow_id = channel->rps_flow_id[index];\r\nif (flow_id != RPS_FLOW_ID_INVALID &&\r\nexpire_one(efx, flow_id, index)) {\r\nnetif_info(efx, rx_status, efx->net_dev,\r\n"expired filter %d [queue %u flow %u]\n",\r\nindex, channel_idx, flow_id);\r\nchannel->rps_flow_id[index] = RPS_FLOW_ID_INVALID;\r\n}\r\nif (++index == size) {\r\nif (++channel_idx == efx->n_channels)\r\nchannel_idx = 0;\r\nindex = 0;\r\n}\r\n}\r\nefx->rps_expire_channel = channel_idx;\r\nefx->rps_expire_index = index;\r\nspin_unlock_bh(&efx->filter_lock);\r\nreturn true;\r\n}\r\nbool ef4_filter_is_mc_recipient(const struct ef4_filter_spec *spec)\r\n{\r\nif (!(spec->flags & EF4_FILTER_FLAG_RX) ||\r\nspec->dmaq_id == EF4_FILTER_RX_DMAQ_ID_DROP)\r\nreturn false;\r\nif (spec->match_flags &\r\n(EF4_FILTER_MATCH_LOC_MAC | EF4_FILTER_MATCH_LOC_MAC_IG) &&\r\nis_multicast_ether_addr(spec->loc_mac))\r\nreturn true;\r\nif ((spec->match_flags &\r\n(EF4_FILTER_MATCH_ETHER_TYPE | EF4_FILTER_MATCH_LOC_HOST)) ==\r\n(EF4_FILTER_MATCH_ETHER_TYPE | EF4_FILTER_MATCH_LOC_HOST)) {\r\nif (spec->ether_type == htons(ETH_P_IP) &&\r\nipv4_is_multicast(spec->loc_host[0]))\r\nreturn true;\r\nif (spec->ether_type == htons(ETH_P_IPV6) &&\r\n((const u8 *)spec->loc_host)[0] == 0xff)\r\nreturn true;\r\n}\r\nreturn false;\r\n}
