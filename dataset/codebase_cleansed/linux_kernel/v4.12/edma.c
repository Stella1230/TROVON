static inline unsigned int edma_read(struct edma_cc *ecc, int offset)\r\n{\r\nreturn (unsigned int)__raw_readl(ecc->base + offset);\r\n}\r\nstatic inline void edma_write(struct edma_cc *ecc, int offset, int val)\r\n{\r\n__raw_writel(val, ecc->base + offset);\r\n}\r\nstatic inline void edma_modify(struct edma_cc *ecc, int offset, unsigned and,\r\nunsigned or)\r\n{\r\nunsigned val = edma_read(ecc, offset);\r\nval &= and;\r\nval |= or;\r\nedma_write(ecc, offset, val);\r\n}\r\nstatic inline void edma_and(struct edma_cc *ecc, int offset, unsigned and)\r\n{\r\nunsigned val = edma_read(ecc, offset);\r\nval &= and;\r\nedma_write(ecc, offset, val);\r\n}\r\nstatic inline void edma_or(struct edma_cc *ecc, int offset, unsigned or)\r\n{\r\nunsigned val = edma_read(ecc, offset);\r\nval |= or;\r\nedma_write(ecc, offset, val);\r\n}\r\nstatic inline unsigned int edma_read_array(struct edma_cc *ecc, int offset,\r\nint i)\r\n{\r\nreturn edma_read(ecc, offset + (i << 2));\r\n}\r\nstatic inline void edma_write_array(struct edma_cc *ecc, int offset, int i,\r\nunsigned val)\r\n{\r\nedma_write(ecc, offset + (i << 2), val);\r\n}\r\nstatic inline void edma_modify_array(struct edma_cc *ecc, int offset, int i,\r\nunsigned and, unsigned or)\r\n{\r\nedma_modify(ecc, offset + (i << 2), and, or);\r\n}\r\nstatic inline void edma_or_array(struct edma_cc *ecc, int offset, int i,\r\nunsigned or)\r\n{\r\nedma_or(ecc, offset + (i << 2), or);\r\n}\r\nstatic inline void edma_or_array2(struct edma_cc *ecc, int offset, int i, int j,\r\nunsigned or)\r\n{\r\nedma_or(ecc, offset + ((i * 2 + j) << 2), or);\r\n}\r\nstatic inline void edma_write_array2(struct edma_cc *ecc, int offset, int i,\r\nint j, unsigned val)\r\n{\r\nedma_write(ecc, offset + ((i * 2 + j) << 2), val);\r\n}\r\nstatic inline unsigned int edma_shadow0_read(struct edma_cc *ecc, int offset)\r\n{\r\nreturn edma_read(ecc, EDMA_SHADOW0 + offset);\r\n}\r\nstatic inline unsigned int edma_shadow0_read_array(struct edma_cc *ecc,\r\nint offset, int i)\r\n{\r\nreturn edma_read(ecc, EDMA_SHADOW0 + offset + (i << 2));\r\n}\r\nstatic inline void edma_shadow0_write(struct edma_cc *ecc, int offset,\r\nunsigned val)\r\n{\r\nedma_write(ecc, EDMA_SHADOW0 + offset, val);\r\n}\r\nstatic inline void edma_shadow0_write_array(struct edma_cc *ecc, int offset,\r\nint i, unsigned val)\r\n{\r\nedma_write(ecc, EDMA_SHADOW0 + offset + (i << 2), val);\r\n}\r\nstatic inline unsigned int edma_param_read(struct edma_cc *ecc, int offset,\r\nint param_no)\r\n{\r\nreturn edma_read(ecc, EDMA_PARM + offset + (param_no << 5));\r\n}\r\nstatic inline void edma_param_write(struct edma_cc *ecc, int offset,\r\nint param_no, unsigned val)\r\n{\r\nedma_write(ecc, EDMA_PARM + offset + (param_no << 5), val);\r\n}\r\nstatic inline void edma_param_modify(struct edma_cc *ecc, int offset,\r\nint param_no, unsigned and, unsigned or)\r\n{\r\nedma_modify(ecc, EDMA_PARM + offset + (param_no << 5), and, or);\r\n}\r\nstatic inline void edma_param_and(struct edma_cc *ecc, int offset, int param_no,\r\nunsigned and)\r\n{\r\nedma_and(ecc, EDMA_PARM + offset + (param_no << 5), and);\r\n}\r\nstatic inline void edma_param_or(struct edma_cc *ecc, int offset, int param_no,\r\nunsigned or)\r\n{\r\nedma_or(ecc, EDMA_PARM + offset + (param_no << 5), or);\r\n}\r\nstatic inline void edma_set_bits(int offset, int len, unsigned long *p)\r\n{\r\nfor (; len > 0; len--)\r\nset_bit(offset + (len - 1), p);\r\n}\r\nstatic void edma_assign_priority_to_queue(struct edma_cc *ecc, int queue_no,\r\nint priority)\r\n{\r\nint bit = queue_no * 4;\r\nedma_modify(ecc, EDMA_QUEPRI, ~(0x7 << bit), ((priority & 0x7) << bit));\r\n}\r\nstatic void edma_set_chmap(struct edma_chan *echan, int slot)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nif (ecc->chmap_exist) {\r\nslot = EDMA_CHAN_SLOT(slot);\r\nedma_write_array(ecc, EDMA_DCHMAP, channel, (slot << 5));\r\n}\r\n}\r\nstatic void edma_setup_interrupt(struct edma_chan *echan, bool enable)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nif (enable) {\r\nedma_shadow0_write_array(ecc, SH_ICR, channel >> 5,\r\nBIT(channel & 0x1f));\r\nedma_shadow0_write_array(ecc, SH_IESR, channel >> 5,\r\nBIT(channel & 0x1f));\r\n} else {\r\nedma_shadow0_write_array(ecc, SH_IECR, channel >> 5,\r\nBIT(channel & 0x1f));\r\n}\r\n}\r\nstatic void edma_write_slot(struct edma_cc *ecc, unsigned slot,\r\nconst struct edmacc_param *param)\r\n{\r\nslot = EDMA_CHAN_SLOT(slot);\r\nif (slot >= ecc->num_slots)\r\nreturn;\r\nmemcpy_toio(ecc->base + PARM_OFFSET(slot), param, PARM_SIZE);\r\n}\r\nstatic int edma_read_slot(struct edma_cc *ecc, unsigned slot,\r\nstruct edmacc_param *param)\r\n{\r\nslot = EDMA_CHAN_SLOT(slot);\r\nif (slot >= ecc->num_slots)\r\nreturn -EINVAL;\r\nmemcpy_fromio(param, ecc->base + PARM_OFFSET(slot), PARM_SIZE);\r\nreturn 0;\r\n}\r\nstatic int edma_alloc_slot(struct edma_cc *ecc, int slot)\r\n{\r\nif (slot >= 0) {\r\nslot = EDMA_CHAN_SLOT(slot);\r\nif (ecc->chmap_exist && slot < ecc->num_channels)\r\nslot = EDMA_SLOT_ANY;\r\n}\r\nif (slot < 0) {\r\nif (ecc->chmap_exist)\r\nslot = 0;\r\nelse\r\nslot = ecc->num_channels;\r\nfor (;;) {\r\nslot = find_next_zero_bit(ecc->slot_inuse,\r\necc->num_slots,\r\nslot);\r\nif (slot == ecc->num_slots)\r\nreturn -ENOMEM;\r\nif (!test_and_set_bit(slot, ecc->slot_inuse))\r\nbreak;\r\n}\r\n} else if (slot >= ecc->num_slots) {\r\nreturn -EINVAL;\r\n} else if (test_and_set_bit(slot, ecc->slot_inuse)) {\r\nreturn -EBUSY;\r\n}\r\nedma_write_slot(ecc, slot, &dummy_paramset);\r\nreturn EDMA_CTLR_CHAN(ecc->id, slot);\r\n}\r\nstatic void edma_free_slot(struct edma_cc *ecc, unsigned slot)\r\n{\r\nslot = EDMA_CHAN_SLOT(slot);\r\nif (slot >= ecc->num_slots)\r\nreturn;\r\nedma_write_slot(ecc, slot, &dummy_paramset);\r\nclear_bit(slot, ecc->slot_inuse);\r\n}\r\nstatic void edma_link(struct edma_cc *ecc, unsigned from, unsigned to)\r\n{\r\nif (unlikely(EDMA_CTLR(from) != EDMA_CTLR(to)))\r\ndev_warn(ecc->dev, "Ignoring eDMA instance for linking\n");\r\nfrom = EDMA_CHAN_SLOT(from);\r\nto = EDMA_CHAN_SLOT(to);\r\nif (from >= ecc->num_slots || to >= ecc->num_slots)\r\nreturn;\r\nedma_param_modify(ecc, PARM_LINK_BCNTRLD, from, 0xffff0000,\r\nPARM_OFFSET(to));\r\n}\r\nstatic dma_addr_t edma_get_position(struct edma_cc *ecc, unsigned slot,\r\nbool dst)\r\n{\r\nu32 offs;\r\nslot = EDMA_CHAN_SLOT(slot);\r\noffs = PARM_OFFSET(slot);\r\noffs += dst ? PARM_DST : PARM_SRC;\r\nreturn edma_read(ecc, offs);\r\n}\r\nstatic void edma_start(struct edma_chan *echan)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nint j = (channel >> 5);\r\nunsigned int mask = BIT(channel & 0x1f);\r\nif (!echan->hw_triggered) {\r\ndev_dbg(ecc->dev, "ESR%d %08x\n", j,\r\nedma_shadow0_read_array(ecc, SH_ESR, j));\r\nedma_shadow0_write_array(ecc, SH_ESR, j, mask);\r\n} else {\r\ndev_dbg(ecc->dev, "ER%d %08x\n", j,\r\nedma_shadow0_read_array(ecc, SH_ER, j));\r\nedma_write_array(ecc, EDMA_ECR, j, mask);\r\nedma_write_array(ecc, EDMA_EMCR, j, mask);\r\nedma_shadow0_write_array(ecc, SH_SECR, j, mask);\r\nedma_shadow0_write_array(ecc, SH_EESR, j, mask);\r\ndev_dbg(ecc->dev, "EER%d %08x\n", j,\r\nedma_shadow0_read_array(ecc, SH_EER, j));\r\n}\r\n}\r\nstatic void edma_stop(struct edma_chan *echan)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nint j = (channel >> 5);\r\nunsigned int mask = BIT(channel & 0x1f);\r\nedma_shadow0_write_array(ecc, SH_EECR, j, mask);\r\nedma_shadow0_write_array(ecc, SH_ECR, j, mask);\r\nedma_shadow0_write_array(ecc, SH_SECR, j, mask);\r\nedma_write_array(ecc, EDMA_EMCR, j, mask);\r\nedma_shadow0_write_array(ecc, SH_ICR, j, mask);\r\ndev_dbg(ecc->dev, "EER%d %08x\n", j,\r\nedma_shadow0_read_array(ecc, SH_EER, j));\r\n}\r\nstatic void edma_pause(struct edma_chan *echan)\r\n{\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nunsigned int mask = BIT(channel & 0x1f);\r\nedma_shadow0_write_array(echan->ecc, SH_EECR, channel >> 5, mask);\r\n}\r\nstatic void edma_resume(struct edma_chan *echan)\r\n{\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nunsigned int mask = BIT(channel & 0x1f);\r\nedma_shadow0_write_array(echan->ecc, SH_EESR, channel >> 5, mask);\r\n}\r\nstatic void edma_trigger_channel(struct edma_chan *echan)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nunsigned int mask = BIT(channel & 0x1f);\r\nedma_shadow0_write_array(ecc, SH_ESR, (channel >> 5), mask);\r\ndev_dbg(ecc->dev, "ESR%d %08x\n", (channel >> 5),\r\nedma_shadow0_read_array(ecc, SH_ESR, (channel >> 5)));\r\n}\r\nstatic void edma_clean_channel(struct edma_chan *echan)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nint j = (channel >> 5);\r\nunsigned int mask = BIT(channel & 0x1f);\r\ndev_dbg(ecc->dev, "EMR%d %08x\n", j, edma_read_array(ecc, EDMA_EMR, j));\r\nedma_shadow0_write_array(ecc, SH_ECR, j, mask);\r\nedma_write_array(ecc, EDMA_EMCR, j, mask);\r\nedma_shadow0_write_array(ecc, SH_SECR, j, mask);\r\nedma_write(ecc, EDMA_CCERRCLR, BIT(16) | BIT(1) | BIT(0));\r\n}\r\nstatic void edma_assign_channel_eventq(struct edma_chan *echan,\r\nenum dma_event_q eventq_no)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nint bit = (channel & 0x7) * 4;\r\nif (eventq_no == EVENTQ_DEFAULT)\r\neventq_no = ecc->default_queue;\r\nif (eventq_no >= ecc->num_tc)\r\nreturn;\r\neventq_no &= 7;\r\nedma_modify_array(ecc, EDMA_DMAQNUM, (channel >> 3), ~(0x7 << bit),\r\neventq_no << bit);\r\n}\r\nstatic int edma_alloc_channel(struct edma_chan *echan,\r\nenum dma_event_q eventq_no)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nint channel = EDMA_CHAN_SLOT(echan->ch_num);\r\nedma_or_array2(ecc, EDMA_DRAE, 0, channel >> 5, BIT(channel & 0x1f));\r\nedma_stop(echan);\r\nedma_setup_interrupt(echan, true);\r\nedma_assign_channel_eventq(echan, eventq_no);\r\nreturn 0;\r\n}\r\nstatic void edma_free_channel(struct edma_chan *echan)\r\n{\r\nedma_stop(echan);\r\nedma_setup_interrupt(echan, false);\r\n}\r\nstatic inline struct edma_cc *to_edma_cc(struct dma_device *d)\r\n{\r\nreturn container_of(d, struct edma_cc, dma_slave);\r\n}\r\nstatic inline struct edma_chan *to_edma_chan(struct dma_chan *c)\r\n{\r\nreturn container_of(c, struct edma_chan, vchan.chan);\r\n}\r\nstatic inline struct edma_desc *to_edma_desc(struct dma_async_tx_descriptor *tx)\r\n{\r\nreturn container_of(tx, struct edma_desc, vdesc.tx);\r\n}\r\nstatic void edma_desc_free(struct virt_dma_desc *vdesc)\r\n{\r\nkfree(container_of(vdesc, struct edma_desc, vdesc));\r\n}\r\nstatic void edma_execute(struct edma_chan *echan)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nstruct virt_dma_desc *vdesc;\r\nstruct edma_desc *edesc;\r\nstruct device *dev = echan->vchan.chan.device->dev;\r\nint i, j, left, nslots;\r\nif (!echan->edesc) {\r\nvdesc = vchan_next_desc(&echan->vchan);\r\nif (!vdesc)\r\nreturn;\r\nlist_del(&vdesc->node);\r\nechan->edesc = to_edma_desc(&vdesc->tx);\r\n}\r\nedesc = echan->edesc;\r\nleft = edesc->pset_nr - edesc->processed;\r\nnslots = min(MAX_NR_SG, left);\r\nedesc->sg_len = 0;\r\nfor (i = 0; i < nslots; i++) {\r\nj = i + edesc->processed;\r\nedma_write_slot(ecc, echan->slot[i], &edesc->pset[j].param);\r\nedesc->sg_len += edesc->pset[j].len;\r\ndev_vdbg(dev,\r\n"\n pset[%d]:\n"\r\n" chnum\t%d\n"\r\n" slot\t%d\n"\r\n" opt\t%08x\n"\r\n" src\t%08x\n"\r\n" dst\t%08x\n"\r\n" abcnt\t%08x\n"\r\n" ccnt\t%08x\n"\r\n" bidx\t%08x\n"\r\n" cidx\t%08x\n"\r\n" lkrld\t%08x\n",\r\nj, echan->ch_num, echan->slot[i],\r\nedesc->pset[j].param.opt,\r\nedesc->pset[j].param.src,\r\nedesc->pset[j].param.dst,\r\nedesc->pset[j].param.a_b_cnt,\r\nedesc->pset[j].param.ccnt,\r\nedesc->pset[j].param.src_dst_bidx,\r\nedesc->pset[j].param.src_dst_cidx,\r\nedesc->pset[j].param.link_bcntrld);\r\nif (i != (nslots - 1))\r\nedma_link(ecc, echan->slot[i], echan->slot[i + 1]);\r\n}\r\nedesc->processed += nslots;\r\nif (edesc->processed == edesc->pset_nr) {\r\nif (edesc->cyclic)\r\nedma_link(ecc, echan->slot[nslots - 1], echan->slot[1]);\r\nelse\r\nedma_link(ecc, echan->slot[nslots - 1],\r\nechan->ecc->dummy_slot);\r\n}\r\nif (echan->missed) {\r\ndev_dbg(dev, "missed event on channel %d\n", echan->ch_num);\r\nedma_clean_channel(echan);\r\nedma_stop(echan);\r\nedma_start(echan);\r\nedma_trigger_channel(echan);\r\nechan->missed = 0;\r\n} else if (edesc->processed <= MAX_NR_SG) {\r\ndev_dbg(dev, "first transfer starting on channel %d\n",\r\nechan->ch_num);\r\nedma_start(echan);\r\n} else {\r\ndev_dbg(dev, "chan: %d: completed %d elements, resuming\n",\r\nechan->ch_num, edesc->processed);\r\nedma_resume(echan);\r\n}\r\n}\r\nstatic int edma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\nspin_lock_irqsave(&echan->vchan.lock, flags);\r\nif (echan->edesc) {\r\nedma_stop(echan);\r\nif (!echan->tc && echan->edesc->cyclic)\r\nedma_assign_channel_eventq(echan, EVENTQ_DEFAULT);\r\nedma_desc_free(&echan->edesc->vdesc);\r\nechan->edesc = NULL;\r\n}\r\nvchan_get_all_descriptors(&echan->vchan, &head);\r\nspin_unlock_irqrestore(&echan->vchan.lock, flags);\r\nvchan_dma_desc_free_list(&echan->vchan, &head);\r\nreturn 0;\r\n}\r\nstatic void edma_synchronize(struct dma_chan *chan)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nvchan_synchronize(&echan->vchan);\r\n}\r\nstatic int edma_slave_config(struct dma_chan *chan,\r\nstruct dma_slave_config *cfg)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nif (cfg->src_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES ||\r\ncfg->dst_addr_width == DMA_SLAVE_BUSWIDTH_8_BYTES)\r\nreturn -EINVAL;\r\nmemcpy(&echan->cfg, cfg, sizeof(echan->cfg));\r\nreturn 0;\r\n}\r\nstatic int edma_dma_pause(struct dma_chan *chan)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nif (!echan->edesc)\r\nreturn -EINVAL;\r\nedma_pause(echan);\r\nreturn 0;\r\n}\r\nstatic int edma_dma_resume(struct dma_chan *chan)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nedma_resume(echan);\r\nreturn 0;\r\n}\r\nstatic int edma_config_pset(struct dma_chan *chan, struct edma_pset *epset,\r\ndma_addr_t src_addr, dma_addr_t dst_addr, u32 burst,\r\nunsigned int acnt, unsigned int dma_length,\r\nenum dma_transfer_direction direction)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nstruct device *dev = chan->device->dev;\r\nstruct edmacc_param *param = &epset->param;\r\nint bcnt, ccnt, cidx;\r\nint src_bidx, dst_bidx, src_cidx, dst_cidx;\r\nint absync;\r\nif (!burst)\r\nburst = 1;\r\nif (burst == 1) {\r\nabsync = false;\r\nccnt = dma_length / acnt / (SZ_64K - 1);\r\nbcnt = dma_length / acnt - ccnt * (SZ_64K - 1);\r\nif (bcnt)\r\nccnt++;\r\nelse\r\nbcnt = SZ_64K - 1;\r\ncidx = acnt;\r\n} else {\r\nabsync = true;\r\nbcnt = burst;\r\nccnt = dma_length / (acnt * bcnt);\r\nif (ccnt > (SZ_64K - 1)) {\r\ndev_err(dev, "Exceeded max SG segment size\n");\r\nreturn -EINVAL;\r\n}\r\ncidx = acnt * bcnt;\r\n}\r\nepset->len = dma_length;\r\nif (direction == DMA_MEM_TO_DEV) {\r\nsrc_bidx = acnt;\r\nsrc_cidx = cidx;\r\ndst_bidx = 0;\r\ndst_cidx = 0;\r\nepset->addr = src_addr;\r\n} else if (direction == DMA_DEV_TO_MEM) {\r\nsrc_bidx = 0;\r\nsrc_cidx = 0;\r\ndst_bidx = acnt;\r\ndst_cidx = cidx;\r\nepset->addr = dst_addr;\r\n} else if (direction == DMA_MEM_TO_MEM) {\r\nsrc_bidx = acnt;\r\nsrc_cidx = cidx;\r\ndst_bidx = acnt;\r\ndst_cidx = cidx;\r\n} else {\r\ndev_err(dev, "%s: direction not implemented yet\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nparam->opt = EDMA_TCC(EDMA_CHAN_SLOT(echan->ch_num));\r\nif (absync)\r\nparam->opt |= SYNCDIM;\r\nparam->src = src_addr;\r\nparam->dst = dst_addr;\r\nparam->src_dst_bidx = (dst_bidx << 16) | src_bidx;\r\nparam->src_dst_cidx = (dst_cidx << 16) | src_cidx;\r\nparam->a_b_cnt = bcnt << 16 | acnt;\r\nparam->ccnt = ccnt;\r\nparam->link_bcntrld = 0xffffffff;\r\nreturn absync;\r\n}\r\nstatic struct dma_async_tx_descriptor *edma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl,\r\nunsigned int sg_len, enum dma_transfer_direction direction,\r\nunsigned long tx_flags, void *context)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nstruct device *dev = chan->device->dev;\r\nstruct edma_desc *edesc;\r\ndma_addr_t src_addr = 0, dst_addr = 0;\r\nenum dma_slave_buswidth dev_width;\r\nu32 burst;\r\nstruct scatterlist *sg;\r\nint i, nslots, ret;\r\nif (unlikely(!echan || !sgl || !sg_len))\r\nreturn NULL;\r\nif (direction == DMA_DEV_TO_MEM) {\r\nsrc_addr = echan->cfg.src_addr;\r\ndev_width = echan->cfg.src_addr_width;\r\nburst = echan->cfg.src_maxburst;\r\n} else if (direction == DMA_MEM_TO_DEV) {\r\ndst_addr = echan->cfg.dst_addr;\r\ndev_width = echan->cfg.dst_addr_width;\r\nburst = echan->cfg.dst_maxburst;\r\n} else {\r\ndev_err(dev, "%s: bad direction: %d\n", __func__, direction);\r\nreturn NULL;\r\n}\r\nif (dev_width == DMA_SLAVE_BUSWIDTH_UNDEFINED) {\r\ndev_err(dev, "%s: Undefined slave buswidth\n", __func__);\r\nreturn NULL;\r\n}\r\nedesc = kzalloc(sizeof(*edesc) + sg_len * sizeof(edesc->pset[0]),\r\nGFP_ATOMIC);\r\nif (!edesc)\r\nreturn NULL;\r\nedesc->pset_nr = sg_len;\r\nedesc->residue = 0;\r\nedesc->direction = direction;\r\nedesc->echan = echan;\r\nnslots = min_t(unsigned, MAX_NR_SG, sg_len);\r\nfor (i = 0; i < nslots; i++) {\r\nif (echan->slot[i] < 0) {\r\nechan->slot[i] =\r\nedma_alloc_slot(echan->ecc, EDMA_SLOT_ANY);\r\nif (echan->slot[i] < 0) {\r\nkfree(edesc);\r\ndev_err(dev, "%s: Failed to allocate slot\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\n}\r\n}\r\nfor_each_sg(sgl, sg, sg_len, i) {\r\nif (direction == DMA_DEV_TO_MEM)\r\ndst_addr = sg_dma_address(sg);\r\nelse\r\nsrc_addr = sg_dma_address(sg);\r\nret = edma_config_pset(chan, &edesc->pset[i], src_addr,\r\ndst_addr, burst, dev_width,\r\nsg_dma_len(sg), direction);\r\nif (ret < 0) {\r\nkfree(edesc);\r\nreturn NULL;\r\n}\r\nedesc->absync = ret;\r\nedesc->residue += sg_dma_len(sg);\r\nif (i == sg_len - 1)\r\nedesc->pset[i].param.opt |= TCINTEN;\r\nelse if (!((i+1) % MAX_NR_SG))\r\nedesc->pset[i].param.opt |= (TCINTEN | TCCMODE);\r\n}\r\nedesc->residue_stat = edesc->residue;\r\nreturn vchan_tx_prep(&echan->vchan, &edesc->vdesc, tx_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *edma_prep_dma_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dest, dma_addr_t src,\r\nsize_t len, unsigned long tx_flags)\r\n{\r\nint ret, nslots;\r\nstruct edma_desc *edesc;\r\nstruct device *dev = chan->device->dev;\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nunsigned int width, pset_len;\r\nif (unlikely(!echan || !len))\r\nreturn NULL;\r\nif (len < SZ_64K) {\r\nwidth = len;\r\npset_len = len;\r\nnslots = 1;\r\n} else {\r\nwidth = SZ_32K - 1;\r\npset_len = rounddown(len, width);\r\nif (unlikely(pset_len == len))\r\nnslots = 1;\r\nelse\r\nnslots = 2;\r\n}\r\nedesc = kzalloc(sizeof(*edesc) + nslots * sizeof(edesc->pset[0]),\r\nGFP_ATOMIC);\r\nif (!edesc)\r\nreturn NULL;\r\nedesc->pset_nr = nslots;\r\nedesc->residue = edesc->residue_stat = len;\r\nedesc->direction = DMA_MEM_TO_MEM;\r\nedesc->echan = echan;\r\nret = edma_config_pset(chan, &edesc->pset[0], src, dest, 1,\r\nwidth, pset_len, DMA_MEM_TO_MEM);\r\nif (ret < 0) {\r\nkfree(edesc);\r\nreturn NULL;\r\n}\r\nedesc->absync = ret;\r\nedesc->pset[0].param.opt |= ITCCHEN;\r\nif (nslots == 1) {\r\nedesc->pset[0].param.opt |= TCINTEN;\r\n} else {\r\nedesc->pset[0].param.opt |= TCCHEN;\r\nif (echan->slot[1] < 0) {\r\nechan->slot[1] = edma_alloc_slot(echan->ecc,\r\nEDMA_SLOT_ANY);\r\nif (echan->slot[1] < 0) {\r\nkfree(edesc);\r\ndev_err(dev, "%s: Failed to allocate slot\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\n}\r\ndest += pset_len;\r\nsrc += pset_len;\r\npset_len = width = len % (SZ_32K - 1);\r\nret = edma_config_pset(chan, &edesc->pset[1], src, dest, 1,\r\nwidth, pset_len, DMA_MEM_TO_MEM);\r\nif (ret < 0) {\r\nkfree(edesc);\r\nreturn NULL;\r\n}\r\nedesc->pset[1].param.opt |= ITCCHEN;\r\nedesc->pset[1].param.opt |= TCINTEN;\r\n}\r\nreturn vchan_tx_prep(&echan->vchan, &edesc->vdesc, tx_flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *edma_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t buf_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction direction,\r\nunsigned long tx_flags)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nstruct device *dev = chan->device->dev;\r\nstruct edma_desc *edesc;\r\ndma_addr_t src_addr, dst_addr;\r\nenum dma_slave_buswidth dev_width;\r\nbool use_intermediate = false;\r\nu32 burst;\r\nint i, ret, nslots;\r\nif (unlikely(!echan || !buf_len || !period_len))\r\nreturn NULL;\r\nif (direction == DMA_DEV_TO_MEM) {\r\nsrc_addr = echan->cfg.src_addr;\r\ndst_addr = buf_addr;\r\ndev_width = echan->cfg.src_addr_width;\r\nburst = echan->cfg.src_maxburst;\r\n} else if (direction == DMA_MEM_TO_DEV) {\r\nsrc_addr = buf_addr;\r\ndst_addr = echan->cfg.dst_addr;\r\ndev_width = echan->cfg.dst_addr_width;\r\nburst = echan->cfg.dst_maxburst;\r\n} else {\r\ndev_err(dev, "%s: bad direction: %d\n", __func__, direction);\r\nreturn NULL;\r\n}\r\nif (dev_width == DMA_SLAVE_BUSWIDTH_UNDEFINED) {\r\ndev_err(dev, "%s: Undefined slave buswidth\n", __func__);\r\nreturn NULL;\r\n}\r\nif (unlikely(buf_len % period_len)) {\r\ndev_err(dev, "Period should be multiple of Buffer length\n");\r\nreturn NULL;\r\n}\r\nnslots = (buf_len / period_len) + 1;\r\nif (nslots > MAX_NR_SG) {\r\nif (burst == period_len) {\r\nperiod_len = buf_len;\r\nnslots = 2;\r\nuse_intermediate = true;\r\n} else {\r\nreturn NULL;\r\n}\r\n}\r\nedesc = kzalloc(sizeof(*edesc) + nslots * sizeof(edesc->pset[0]),\r\nGFP_ATOMIC);\r\nif (!edesc)\r\nreturn NULL;\r\nedesc->cyclic = 1;\r\nedesc->pset_nr = nslots;\r\nedesc->residue = edesc->residue_stat = buf_len;\r\nedesc->direction = direction;\r\nedesc->echan = echan;\r\ndev_dbg(dev, "%s: channel=%d nslots=%d period_len=%zu buf_len=%zu\n",\r\n__func__, echan->ch_num, nslots, period_len, buf_len);\r\nfor (i = 0; i < nslots; i++) {\r\nif (echan->slot[i] < 0) {\r\nechan->slot[i] =\r\nedma_alloc_slot(echan->ecc, EDMA_SLOT_ANY);\r\nif (echan->slot[i] < 0) {\r\nkfree(edesc);\r\ndev_err(dev, "%s: Failed to allocate slot\n",\r\n__func__);\r\nreturn NULL;\r\n}\r\n}\r\nif (i == nslots - 1) {\r\nmemcpy(&edesc->pset[i], &edesc->pset[0],\r\nsizeof(edesc->pset[0]));\r\nbreak;\r\n}\r\nret = edma_config_pset(chan, &edesc->pset[i], src_addr,\r\ndst_addr, burst, dev_width, period_len,\r\ndirection);\r\nif (ret < 0) {\r\nkfree(edesc);\r\nreturn NULL;\r\n}\r\nif (direction == DMA_DEV_TO_MEM)\r\ndst_addr += period_len;\r\nelse\r\nsrc_addr += period_len;\r\ndev_vdbg(dev, "%s: Configure period %d of buf:\n", __func__, i);\r\ndev_vdbg(dev,\r\n"\n pset[%d]:\n"\r\n" chnum\t%d\n"\r\n" slot\t%d\n"\r\n" opt\t%08x\n"\r\n" src\t%08x\n"\r\n" dst\t%08x\n"\r\n" abcnt\t%08x\n"\r\n" ccnt\t%08x\n"\r\n" bidx\t%08x\n"\r\n" cidx\t%08x\n"\r\n" lkrld\t%08x\n",\r\ni, echan->ch_num, echan->slot[i],\r\nedesc->pset[i].param.opt,\r\nedesc->pset[i].param.src,\r\nedesc->pset[i].param.dst,\r\nedesc->pset[i].param.a_b_cnt,\r\nedesc->pset[i].param.ccnt,\r\nedesc->pset[i].param.src_dst_bidx,\r\nedesc->pset[i].param.src_dst_cidx,\r\nedesc->pset[i].param.link_bcntrld);\r\nedesc->absync = ret;\r\nif (tx_flags & DMA_PREP_INTERRUPT) {\r\nedesc->pset[i].param.opt |= TCINTEN;\r\nif (use_intermediate)\r\nedesc->pset[i].param.opt |= ITCINTEN;\r\n}\r\n}\r\nif (!echan->tc)\r\nedma_assign_channel_eventq(echan, EVENTQ_0);\r\nreturn vchan_tx_prep(&echan->vchan, &edesc->vdesc, tx_flags);\r\n}\r\nstatic void edma_completion_handler(struct edma_chan *echan)\r\n{\r\nstruct device *dev = echan->vchan.chan.device->dev;\r\nstruct edma_desc *edesc;\r\nspin_lock(&echan->vchan.lock);\r\nedesc = echan->edesc;\r\nif (edesc) {\r\nif (edesc->cyclic) {\r\nvchan_cyclic_callback(&edesc->vdesc);\r\nspin_unlock(&echan->vchan.lock);\r\nreturn;\r\n} else if (edesc->processed == edesc->pset_nr) {\r\nedesc->residue = 0;\r\nedma_stop(echan);\r\nvchan_cookie_complete(&edesc->vdesc);\r\nechan->edesc = NULL;\r\ndev_dbg(dev, "Transfer completed on channel %d\n",\r\nechan->ch_num);\r\n} else {\r\ndev_dbg(dev, "Sub transfer completed on channel %d\n",\r\nechan->ch_num);\r\nedma_pause(echan);\r\nedesc->residue -= edesc->sg_len;\r\nedesc->residue_stat = edesc->residue;\r\nedesc->processed_stat = edesc->processed;\r\n}\r\nedma_execute(echan);\r\n}\r\nspin_unlock(&echan->vchan.lock);\r\n}\r\nstatic irqreturn_t dma_irq_handler(int irq, void *data)\r\n{\r\nstruct edma_cc *ecc = data;\r\nint ctlr;\r\nu32 sh_ier;\r\nu32 sh_ipr;\r\nu32 bank;\r\nctlr = ecc->id;\r\nif (ctlr < 0)\r\nreturn IRQ_NONE;\r\ndev_vdbg(ecc->dev, "dma_irq_handler\n");\r\nsh_ipr = edma_shadow0_read_array(ecc, SH_IPR, 0);\r\nif (!sh_ipr) {\r\nsh_ipr = edma_shadow0_read_array(ecc, SH_IPR, 1);\r\nif (!sh_ipr)\r\nreturn IRQ_NONE;\r\nsh_ier = edma_shadow0_read_array(ecc, SH_IER, 1);\r\nbank = 1;\r\n} else {\r\nsh_ier = edma_shadow0_read_array(ecc, SH_IER, 0);\r\nbank = 0;\r\n}\r\ndo {\r\nu32 slot;\r\nu32 channel;\r\nslot = __ffs(sh_ipr);\r\nsh_ipr &= ~(BIT(slot));\r\nif (sh_ier & BIT(slot)) {\r\nchannel = (bank << 5) | slot;\r\nedma_shadow0_write_array(ecc, SH_ICR, bank, BIT(slot));\r\nedma_completion_handler(&ecc->slave_chans[channel]);\r\n}\r\n} while (sh_ipr);\r\nedma_shadow0_write(ecc, SH_IEVAL, 1);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void edma_error_handler(struct edma_chan *echan)\r\n{\r\nstruct edma_cc *ecc = echan->ecc;\r\nstruct device *dev = echan->vchan.chan.device->dev;\r\nstruct edmacc_param p;\r\nint err;\r\nif (!echan->edesc)\r\nreturn;\r\nspin_lock(&echan->vchan.lock);\r\nerr = edma_read_slot(ecc, echan->slot[0], &p);\r\nif (err || (p.a_b_cnt == 0 && p.ccnt == 0)) {\r\ndev_dbg(dev, "Error on null slot, setting miss\n");\r\nechan->missed = 1;\r\n} else {\r\ndev_dbg(dev, "Missed event, TRIGGERING\n");\r\nedma_clean_channel(echan);\r\nedma_stop(echan);\r\nedma_start(echan);\r\nedma_trigger_channel(echan);\r\n}\r\nspin_unlock(&echan->vchan.lock);\r\n}\r\nstatic inline bool edma_error_pending(struct edma_cc *ecc)\r\n{\r\nif (edma_read_array(ecc, EDMA_EMR, 0) ||\r\nedma_read_array(ecc, EDMA_EMR, 1) ||\r\nedma_read(ecc, EDMA_QEMR) || edma_read(ecc, EDMA_CCERR))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic irqreturn_t dma_ccerr_handler(int irq, void *data)\r\n{\r\nstruct edma_cc *ecc = data;\r\nint i, j;\r\nint ctlr;\r\nunsigned int cnt = 0;\r\nunsigned int val;\r\nctlr = ecc->id;\r\nif (ctlr < 0)\r\nreturn IRQ_NONE;\r\ndev_vdbg(ecc->dev, "dma_ccerr_handler\n");\r\nif (!edma_error_pending(ecc)) {\r\ndev_err(ecc->dev, "%s: Error interrupt without error event!\n",\r\n__func__);\r\nedma_write(ecc, EDMA_EEVAL, 1);\r\nreturn IRQ_NONE;\r\n}\r\nwhile (1) {\r\nfor (j = 0; j < 2; j++) {\r\nunsigned long emr;\r\nval = edma_read_array(ecc, EDMA_EMR, j);\r\nif (!val)\r\ncontinue;\r\ndev_dbg(ecc->dev, "EMR%d 0x%08x\n", j, val);\r\nemr = val;\r\nfor (i = find_next_bit(&emr, 32, 0); i < 32;\r\ni = find_next_bit(&emr, 32, i + 1)) {\r\nint k = (j << 5) + i;\r\nedma_write_array(ecc, EDMA_EMCR, j, BIT(i));\r\nedma_shadow0_write_array(ecc, SH_SECR, j,\r\nBIT(i));\r\nedma_error_handler(&ecc->slave_chans[k]);\r\n}\r\n}\r\nval = edma_read(ecc, EDMA_QEMR);\r\nif (val) {\r\ndev_dbg(ecc->dev, "QEMR 0x%02x\n", val);\r\nedma_write(ecc, EDMA_QEMCR, val);\r\nedma_shadow0_write(ecc, SH_QSECR, val);\r\n}\r\nval = edma_read(ecc, EDMA_CCERR);\r\nif (val) {\r\ndev_warn(ecc->dev, "CCERR 0x%08x\n", val);\r\nedma_write(ecc, EDMA_CCERRCLR, val);\r\n}\r\nif (!edma_error_pending(ecc))\r\nbreak;\r\ncnt++;\r\nif (cnt > 10)\r\nbreak;\r\n}\r\nedma_write(ecc, EDMA_EEVAL, 1);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int edma_alloc_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nstruct edma_cc *ecc = echan->ecc;\r\nstruct device *dev = ecc->dev;\r\nenum dma_event_q eventq_no = EVENTQ_DEFAULT;\r\nint ret;\r\nif (echan->tc) {\r\neventq_no = echan->tc->id;\r\n} else if (ecc->tc_list) {\r\nechan->tc = &ecc->tc_list[ecc->info->default_queue];\r\neventq_no = echan->tc->id;\r\n}\r\nret = edma_alloc_channel(echan, eventq_no);\r\nif (ret)\r\nreturn ret;\r\nechan->slot[0] = edma_alloc_slot(ecc, echan->ch_num);\r\nif (echan->slot[0] < 0) {\r\ndev_err(dev, "Entry slot allocation failed for channel %u\n",\r\nEDMA_CHAN_SLOT(echan->ch_num));\r\nret = echan->slot[0];\r\ngoto err_slot;\r\n}\r\nedma_set_chmap(echan, echan->slot[0]);\r\nechan->alloced = true;\r\ndev_dbg(dev, "Got eDMA channel %d for virt channel %d (%s trigger)\n",\r\nEDMA_CHAN_SLOT(echan->ch_num), chan->chan_id,\r\nechan->hw_triggered ? "HW" : "SW");\r\nreturn 0;\r\nerr_slot:\r\nedma_free_channel(echan);\r\nreturn ret;\r\n}\r\nstatic void edma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nstruct device *dev = echan->ecc->dev;\r\nint i;\r\nedma_stop(echan);\r\nvchan_free_chan_resources(&echan->vchan);\r\nfor (i = 0; i < EDMA_MAX_SLOTS; i++) {\r\nif (echan->slot[i] >= 0) {\r\nedma_free_slot(echan->ecc, echan->slot[i]);\r\nechan->slot[i] = -1;\r\n}\r\n}\r\nedma_set_chmap(echan, echan->ecc->dummy_slot);\r\nif (echan->alloced) {\r\nedma_free_channel(echan);\r\nechan->alloced = false;\r\n}\r\nechan->tc = NULL;\r\nechan->hw_triggered = false;\r\ndev_dbg(dev, "Free eDMA channel %d for virt channel %d\n",\r\nEDMA_CHAN_SLOT(echan->ch_num), chan->chan_id);\r\n}\r\nstatic void edma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nunsigned long flags;\r\nspin_lock_irqsave(&echan->vchan.lock, flags);\r\nif (vchan_issue_pending(&echan->vchan) && !echan->edesc)\r\nedma_execute(echan);\r\nspin_unlock_irqrestore(&echan->vchan.lock, flags);\r\n}\r\nstatic u32 edma_residue(struct edma_desc *edesc)\r\n{\r\nbool dst = edesc->direction == DMA_DEV_TO_MEM;\r\nint loop_count = EDMA_MAX_TR_WAIT_LOOPS;\r\nstruct edma_chan *echan = edesc->echan;\r\nstruct edma_pset *pset = edesc->pset;\r\ndma_addr_t done, pos;\r\nint i;\r\npos = edma_get_position(echan->ecc, echan->slot[0], dst);\r\nwhile (edma_read(echan->ecc, EDMA_CCSTAT) & EDMA_CCSTAT_ACTV) {\r\nif (edma_get_position(echan->ecc,\r\nechan->slot[0], dst) != pos) {\r\nbreak;\r\n}\r\nif (!--loop_count) {\r\ndev_dbg_ratelimited(echan->vchan.chan.device->dev,\r\n"%s: timeout waiting for PaRAM update\n",\r\n__func__);\r\nbreak;\r\n}\r\ncpu_relax();\r\n}\r\nif (edesc->cyclic) {\r\ndone = pos - pset->addr;\r\nedesc->residue_stat = edesc->residue - done;\r\nreturn edesc->residue_stat;\r\n}\r\npset += edesc->processed_stat;\r\nfor (i = edesc->processed_stat; i < edesc->processed; i++, pset++) {\r\nif (pos >= pset->addr && pos < pset->addr + pset->len)\r\nreturn edesc->residue_stat - (pos - pset->addr);\r\nedesc->processed_stat++;\r\nedesc->residue_stat -= pset->len;\r\n}\r\nreturn edesc->residue_stat;\r\n}\r\nstatic enum dma_status edma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *txstate)\r\n{\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nstruct virt_dma_desc *vdesc;\r\nenum dma_status ret;\r\nunsigned long flags;\r\nret = dma_cookie_status(chan, cookie, txstate);\r\nif (ret == DMA_COMPLETE || !txstate)\r\nreturn ret;\r\nspin_lock_irqsave(&echan->vchan.lock, flags);\r\nif (echan->edesc && echan->edesc->vdesc.tx.cookie == cookie)\r\ntxstate->residue = edma_residue(echan->edesc);\r\nelse if ((vdesc = vchan_find_desc(&echan->vchan, cookie)))\r\ntxstate->residue = to_edma_desc(&vdesc->tx)->residue;\r\nspin_unlock_irqrestore(&echan->vchan.lock, flags);\r\nreturn ret;\r\n}\r\nstatic bool edma_is_memcpy_channel(int ch_num, s32 *memcpy_channels)\r\n{\r\nif (!memcpy_channels)\r\nreturn false;\r\nwhile (*memcpy_channels != -1) {\r\nif (*memcpy_channels == ch_num)\r\nreturn true;\r\nmemcpy_channels++;\r\n}\r\nreturn false;\r\n}\r\nstatic void edma_dma_init(struct edma_cc *ecc, bool legacy_mode)\r\n{\r\nstruct dma_device *s_ddev = &ecc->dma_slave;\r\nstruct dma_device *m_ddev = NULL;\r\ns32 *memcpy_channels = ecc->info->memcpy_channels;\r\nint i, j;\r\ndma_cap_zero(s_ddev->cap_mask);\r\ndma_cap_set(DMA_SLAVE, s_ddev->cap_mask);\r\ndma_cap_set(DMA_CYCLIC, s_ddev->cap_mask);\r\nif (ecc->legacy_mode && !memcpy_channels) {\r\ndev_warn(ecc->dev,\r\n"Legacy memcpy is enabled, things might not work\n");\r\ndma_cap_set(DMA_MEMCPY, s_ddev->cap_mask);\r\ns_ddev->device_prep_dma_memcpy = edma_prep_dma_memcpy;\r\ns_ddev->directions = BIT(DMA_MEM_TO_MEM);\r\n}\r\ns_ddev->device_prep_slave_sg = edma_prep_slave_sg;\r\ns_ddev->device_prep_dma_cyclic = edma_prep_dma_cyclic;\r\ns_ddev->device_alloc_chan_resources = edma_alloc_chan_resources;\r\ns_ddev->device_free_chan_resources = edma_free_chan_resources;\r\ns_ddev->device_issue_pending = edma_issue_pending;\r\ns_ddev->device_tx_status = edma_tx_status;\r\ns_ddev->device_config = edma_slave_config;\r\ns_ddev->device_pause = edma_dma_pause;\r\ns_ddev->device_resume = edma_dma_resume;\r\ns_ddev->device_terminate_all = edma_terminate_all;\r\ns_ddev->device_synchronize = edma_synchronize;\r\ns_ddev->src_addr_widths = EDMA_DMA_BUSWIDTHS;\r\ns_ddev->dst_addr_widths = EDMA_DMA_BUSWIDTHS;\r\ns_ddev->directions |= (BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV));\r\ns_ddev->residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\ns_ddev->dev = ecc->dev;\r\nINIT_LIST_HEAD(&s_ddev->channels);\r\nif (memcpy_channels) {\r\nm_ddev = devm_kzalloc(ecc->dev, sizeof(*m_ddev), GFP_KERNEL);\r\necc->dma_memcpy = m_ddev;\r\ndma_cap_zero(m_ddev->cap_mask);\r\ndma_cap_set(DMA_MEMCPY, m_ddev->cap_mask);\r\nm_ddev->device_prep_dma_memcpy = edma_prep_dma_memcpy;\r\nm_ddev->device_alloc_chan_resources = edma_alloc_chan_resources;\r\nm_ddev->device_free_chan_resources = edma_free_chan_resources;\r\nm_ddev->device_issue_pending = edma_issue_pending;\r\nm_ddev->device_tx_status = edma_tx_status;\r\nm_ddev->device_config = edma_slave_config;\r\nm_ddev->device_pause = edma_dma_pause;\r\nm_ddev->device_resume = edma_dma_resume;\r\nm_ddev->device_terminate_all = edma_terminate_all;\r\nm_ddev->device_synchronize = edma_synchronize;\r\nm_ddev->src_addr_widths = EDMA_DMA_BUSWIDTHS;\r\nm_ddev->dst_addr_widths = EDMA_DMA_BUSWIDTHS;\r\nm_ddev->directions = BIT(DMA_MEM_TO_MEM);\r\nm_ddev->residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;\r\nm_ddev->dev = ecc->dev;\r\nINIT_LIST_HEAD(&m_ddev->channels);\r\n} else if (!ecc->legacy_mode) {\r\ndev_info(ecc->dev, "memcpy is disabled\n");\r\n}\r\nfor (i = 0; i < ecc->num_channels; i++) {\r\nstruct edma_chan *echan = &ecc->slave_chans[i];\r\nechan->ch_num = EDMA_CTLR_CHAN(ecc->id, i);\r\nechan->ecc = ecc;\r\nechan->vchan.desc_free = edma_desc_free;\r\nif (m_ddev && edma_is_memcpy_channel(i, memcpy_channels))\r\nvchan_init(&echan->vchan, m_ddev);\r\nelse\r\nvchan_init(&echan->vchan, s_ddev);\r\nINIT_LIST_HEAD(&echan->node);\r\nfor (j = 0; j < EDMA_MAX_SLOTS; j++)\r\nechan->slot[j] = -1;\r\n}\r\n}\r\nstatic int edma_setup_from_hw(struct device *dev, struct edma_soc_info *pdata,\r\nstruct edma_cc *ecc)\r\n{\r\nint i;\r\nu32 value, cccfg;\r\ns8 (*queue_priority_map)[2];\r\ncccfg = edma_read(ecc, EDMA_CCCFG);\r\nvalue = GET_NUM_REGN(cccfg);\r\necc->num_region = BIT(value);\r\nvalue = GET_NUM_DMACH(cccfg);\r\necc->num_channels = BIT(value + 1);\r\nvalue = GET_NUM_QDMACH(cccfg);\r\necc->num_qchannels = value * 2;\r\nvalue = GET_NUM_PAENTRY(cccfg);\r\necc->num_slots = BIT(value + 4);\r\nvalue = GET_NUM_EVQUE(cccfg);\r\necc->num_tc = value + 1;\r\necc->chmap_exist = (cccfg & CHMAP_EXIST) ? true : false;\r\ndev_dbg(dev, "eDMA3 CC HW configuration (cccfg: 0x%08x):\n", cccfg);\r\ndev_dbg(dev, "num_region: %u\n", ecc->num_region);\r\ndev_dbg(dev, "num_channels: %u\n", ecc->num_channels);\r\ndev_dbg(dev, "num_qchannels: %u\n", ecc->num_qchannels);\r\ndev_dbg(dev, "num_slots: %u\n", ecc->num_slots);\r\ndev_dbg(dev, "num_tc: %u\n", ecc->num_tc);\r\ndev_dbg(dev, "chmap_exist: %s\n", ecc->chmap_exist ? "yes" : "no");\r\nif (pdata->queue_priority_mapping)\r\nreturn 0;\r\nqueue_priority_map = devm_kcalloc(dev, ecc->num_tc + 1, sizeof(s8),\r\nGFP_KERNEL);\r\nif (!queue_priority_map)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < ecc->num_tc; i++) {\r\nqueue_priority_map[i][0] = i;\r\nqueue_priority_map[i][1] = i;\r\n}\r\nqueue_priority_map[i][0] = -1;\r\nqueue_priority_map[i][1] = -1;\r\npdata->queue_priority_mapping = queue_priority_map;\r\npdata->default_queue = i - 1;\r\nreturn 0;\r\n}\r\nstatic int edma_xbar_event_map(struct device *dev, struct edma_soc_info *pdata,\r\nsize_t sz)\r\n{\r\nconst char pname[] = "ti,edma-xbar-event-map";\r\nstruct resource res;\r\nvoid __iomem *xbar;\r\ns16 (*xbar_chans)[2];\r\nsize_t nelm = sz / sizeof(s16);\r\nu32 shift, offset, mux;\r\nint ret, i;\r\nxbar_chans = devm_kcalloc(dev, nelm + 2, sizeof(s16), GFP_KERNEL);\r\nif (!xbar_chans)\r\nreturn -ENOMEM;\r\nret = of_address_to_resource(dev->of_node, 1, &res);\r\nif (ret)\r\nreturn -ENOMEM;\r\nxbar = devm_ioremap(dev, res.start, resource_size(&res));\r\nif (!xbar)\r\nreturn -ENOMEM;\r\nret = of_property_read_u16_array(dev->of_node, pname, (u16 *)xbar_chans,\r\nnelm);\r\nif (ret)\r\nreturn -EIO;\r\nnelm >>= 1;\r\nxbar_chans[nelm][0] = -1;\r\nxbar_chans[nelm][1] = -1;\r\nfor (i = 0; i < nelm; i++) {\r\nshift = (xbar_chans[i][1] & 0x03) << 3;\r\noffset = xbar_chans[i][1] & 0xfffffffc;\r\nmux = readl(xbar + offset);\r\nmux &= ~(0xff << shift);\r\nmux |= xbar_chans[i][0] << shift;\r\nwritel(mux, (xbar + offset));\r\n}\r\npdata->xbar_chans = (const s16 (*)[2]) xbar_chans;\r\nreturn 0;\r\n}\r\nstatic struct edma_soc_info *edma_setup_info_from_dt(struct device *dev,\r\nbool legacy_mode)\r\n{\r\nstruct edma_soc_info *info;\r\nstruct property *prop;\r\nint sz, ret;\r\ninfo = devm_kzalloc(dev, sizeof(struct edma_soc_info), GFP_KERNEL);\r\nif (!info)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (legacy_mode) {\r\nprop = of_find_property(dev->of_node, "ti,edma-xbar-event-map",\r\n&sz);\r\nif (prop) {\r\nret = edma_xbar_event_map(dev, info, sz);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\n}\r\nreturn info;\r\n}\r\nprop = of_find_property(dev->of_node, "ti,edma-memcpy-channels", &sz);\r\nif (prop) {\r\nconst char pname[] = "ti,edma-memcpy-channels";\r\nsize_t nelm = sz / sizeof(s32);\r\ns32 *memcpy_ch;\r\nmemcpy_ch = devm_kcalloc(dev, nelm + 1, sizeof(s32),\r\nGFP_KERNEL);\r\nif (!memcpy_ch)\r\nreturn ERR_PTR(-ENOMEM);\r\nret = of_property_read_u32_array(dev->of_node, pname,\r\n(u32 *)memcpy_ch, nelm);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\nmemcpy_ch[nelm] = -1;\r\ninfo->memcpy_channels = memcpy_ch;\r\n}\r\nprop = of_find_property(dev->of_node, "ti,edma-reserved-slot-ranges",\r\n&sz);\r\nif (prop) {\r\nconst char pname[] = "ti,edma-reserved-slot-ranges";\r\nu32 (*tmp)[2];\r\ns16 (*rsv_slots)[2];\r\nsize_t nelm = sz / sizeof(*tmp);\r\nstruct edma_rsv_info *rsv_info;\r\nint i;\r\nif (!nelm)\r\nreturn info;\r\ntmp = kcalloc(nelm, sizeof(*tmp), GFP_KERNEL);\r\nif (!tmp)\r\nreturn ERR_PTR(-ENOMEM);\r\nrsv_info = devm_kzalloc(dev, sizeof(*rsv_info), GFP_KERNEL);\r\nif (!rsv_info) {\r\nkfree(tmp);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nrsv_slots = devm_kcalloc(dev, nelm + 1, sizeof(*rsv_slots),\r\nGFP_KERNEL);\r\nif (!rsv_slots) {\r\nkfree(tmp);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nret = of_property_read_u32_array(dev->of_node, pname,\r\n(u32 *)tmp, nelm * 2);\r\nif (ret) {\r\nkfree(tmp);\r\nreturn ERR_PTR(ret);\r\n}\r\nfor (i = 0; i < nelm; i++) {\r\nrsv_slots[i][0] = tmp[i][0];\r\nrsv_slots[i][1] = tmp[i][1];\r\n}\r\nrsv_slots[nelm][0] = -1;\r\nrsv_slots[nelm][1] = -1;\r\ninfo->rsv = rsv_info;\r\ninfo->rsv->rsv_slots = (const s16 (*)[2])rsv_slots;\r\nkfree(tmp);\r\n}\r\nreturn info;\r\n}\r\nstatic struct dma_chan *of_edma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct edma_cc *ecc = ofdma->of_dma_data;\r\nstruct dma_chan *chan = NULL;\r\nstruct edma_chan *echan;\r\nint i;\r\nif (!ecc || dma_spec->args_count < 1)\r\nreturn NULL;\r\nfor (i = 0; i < ecc->num_channels; i++) {\r\nechan = &ecc->slave_chans[i];\r\nif (echan->ch_num == dma_spec->args[0]) {\r\nchan = &echan->vchan.chan;\r\nbreak;\r\n}\r\n}\r\nif (!chan)\r\nreturn NULL;\r\nif (echan->ecc->legacy_mode && dma_spec->args_count == 1)\r\ngoto out;\r\nif (!echan->ecc->legacy_mode && dma_spec->args_count == 2 &&\r\ndma_spec->args[1] < echan->ecc->num_tc) {\r\nechan->tc = &echan->ecc->tc_list[dma_spec->args[1]];\r\ngoto out;\r\n}\r\nreturn NULL;\r\nout:\r\nechan->hw_triggered = true;\r\nreturn dma_get_slave_channel(chan);\r\n}\r\nstatic struct edma_soc_info *edma_setup_info_from_dt(struct device *dev,\r\nbool legacy_mode)\r\n{\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nstatic struct dma_chan *of_edma_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nreturn NULL;\r\n}\r\nstatic int edma_probe(struct platform_device *pdev)\r\n{\r\nstruct edma_soc_info *info = pdev->dev.platform_data;\r\ns8 (*queue_priority_mapping)[2];\r\nint i, off, ln;\r\nconst s16 (*rsv_slots)[2];\r\nconst s16 (*xbar_chans)[2];\r\nint irq;\r\nchar *irq_name;\r\nstruct resource *mem;\r\nstruct device_node *node = pdev->dev.of_node;\r\nstruct device *dev = &pdev->dev;\r\nstruct edma_cc *ecc;\r\nbool legacy_mode = true;\r\nint ret;\r\nif (node) {\r\nconst struct of_device_id *match;\r\nmatch = of_match_node(edma_of_ids, node);\r\nif (match && (*(u32 *)match->data) == EDMA_BINDING_TPCC)\r\nlegacy_mode = false;\r\ninfo = edma_setup_info_from_dt(dev, legacy_mode);\r\nif (IS_ERR(info)) {\r\ndev_err(dev, "failed to get DT data\n");\r\nreturn PTR_ERR(info);\r\n}\r\n}\r\nif (!info)\r\nreturn -ENODEV;\r\npm_runtime_enable(dev);\r\nret = pm_runtime_get_sync(dev);\r\nif (ret < 0) {\r\ndev_err(dev, "pm_runtime_get_sync() failed\n");\r\nreturn ret;\r\n}\r\nret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));\r\nif (ret)\r\nreturn ret;\r\necc = devm_kzalloc(dev, sizeof(*ecc), GFP_KERNEL);\r\nif (!ecc)\r\nreturn -ENOMEM;\r\necc->dev = dev;\r\necc->id = pdev->id;\r\necc->legacy_mode = legacy_mode;\r\nif (ecc->id < 0)\r\necc->id = 0;\r\nmem = platform_get_resource_byname(pdev, IORESOURCE_MEM, "edma3_cc");\r\nif (!mem) {\r\ndev_dbg(dev, "mem resource not found, using index 0\n");\r\nmem = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (!mem) {\r\ndev_err(dev, "no mem resource?\n");\r\nreturn -ENODEV;\r\n}\r\n}\r\necc->base = devm_ioremap_resource(dev, mem);\r\nif (IS_ERR(ecc->base))\r\nreturn PTR_ERR(ecc->base);\r\nplatform_set_drvdata(pdev, ecc);\r\nret = edma_setup_from_hw(dev, info, ecc);\r\nif (ret)\r\nreturn ret;\r\necc->slave_chans = devm_kcalloc(dev, ecc->num_channels,\r\nsizeof(*ecc->slave_chans), GFP_KERNEL);\r\nif (!ecc->slave_chans)\r\nreturn -ENOMEM;\r\necc->slot_inuse = devm_kcalloc(dev, BITS_TO_LONGS(ecc->num_slots),\r\nsizeof(unsigned long), GFP_KERNEL);\r\nif (!ecc->slot_inuse)\r\nreturn -ENOMEM;\r\necc->default_queue = info->default_queue;\r\nfor (i = 0; i < ecc->num_slots; i++)\r\nedma_write_slot(ecc, i, &dummy_paramset);\r\nif (info->rsv) {\r\nrsv_slots = info->rsv->rsv_slots;\r\nif (rsv_slots) {\r\nfor (i = 0; rsv_slots[i][0] != -1; i++) {\r\noff = rsv_slots[i][0];\r\nln = rsv_slots[i][1];\r\nedma_set_bits(off, ln, ecc->slot_inuse);\r\n}\r\n}\r\n}\r\nxbar_chans = info->xbar_chans;\r\nif (xbar_chans) {\r\nfor (i = 0; xbar_chans[i][1] != -1; i++) {\r\noff = xbar_chans[i][1];\r\n}\r\n}\r\nirq = platform_get_irq_byname(pdev, "edma3_ccint");\r\nif (irq < 0 && node)\r\nirq = irq_of_parse_and_map(node, 0);\r\nif (irq >= 0) {\r\nirq_name = devm_kasprintf(dev, GFP_KERNEL, "%s_ccint",\r\ndev_name(dev));\r\nret = devm_request_irq(dev, irq, dma_irq_handler, 0, irq_name,\r\necc);\r\nif (ret) {\r\ndev_err(dev, "CCINT (%d) failed --> %d\n", irq, ret);\r\nreturn ret;\r\n}\r\necc->ccint = irq;\r\n}\r\nirq = platform_get_irq_byname(pdev, "edma3_ccerrint");\r\nif (irq < 0 && node)\r\nirq = irq_of_parse_and_map(node, 2);\r\nif (irq >= 0) {\r\nirq_name = devm_kasprintf(dev, GFP_KERNEL, "%s_ccerrint",\r\ndev_name(dev));\r\nret = devm_request_irq(dev, irq, dma_ccerr_handler, 0, irq_name,\r\necc);\r\nif (ret) {\r\ndev_err(dev, "CCERRINT (%d) failed --> %d\n", irq, ret);\r\nreturn ret;\r\n}\r\necc->ccerrint = irq;\r\n}\r\necc->dummy_slot = edma_alloc_slot(ecc, EDMA_SLOT_ANY);\r\nif (ecc->dummy_slot < 0) {\r\ndev_err(dev, "Can't allocate PaRAM dummy slot\n");\r\nreturn ecc->dummy_slot;\r\n}\r\nqueue_priority_mapping = info->queue_priority_mapping;\r\nif (!ecc->legacy_mode) {\r\nint lowest_priority = 0;\r\nstruct of_phandle_args tc_args;\r\necc->tc_list = devm_kcalloc(dev, ecc->num_tc,\r\nsizeof(*ecc->tc_list), GFP_KERNEL);\r\nif (!ecc->tc_list)\r\nreturn -ENOMEM;\r\nfor (i = 0;; i++) {\r\nret = of_parse_phandle_with_fixed_args(node, "ti,tptcs",\r\n1, i, &tc_args);\r\nif (ret || i == ecc->num_tc)\r\nbreak;\r\necc->tc_list[i].node = tc_args.np;\r\necc->tc_list[i].id = i;\r\nqueue_priority_mapping[i][1] = tc_args.args[0];\r\nif (queue_priority_mapping[i][1] > lowest_priority) {\r\nlowest_priority = queue_priority_mapping[i][1];\r\ninfo->default_queue = i;\r\n}\r\n}\r\n}\r\nfor (i = 0; queue_priority_mapping[i][0] != -1; i++)\r\nedma_assign_priority_to_queue(ecc, queue_priority_mapping[i][0],\r\nqueue_priority_mapping[i][1]);\r\nfor (i = 0; i < ecc->num_region; i++) {\r\nedma_write_array2(ecc, EDMA_DRAE, i, 0, 0x0);\r\nedma_write_array2(ecc, EDMA_DRAE, i, 1, 0x0);\r\nedma_write_array(ecc, EDMA_QRAE, i, 0x0);\r\n}\r\necc->info = info;\r\nedma_dma_init(ecc, legacy_mode);\r\nfor (i = 0; i < ecc->num_channels; i++) {\r\nedma_assign_channel_eventq(&ecc->slave_chans[i],\r\ninfo->default_queue);\r\nedma_set_chmap(&ecc->slave_chans[i], ecc->dummy_slot);\r\n}\r\necc->dma_slave.filter.map = info->slave_map;\r\necc->dma_slave.filter.mapcnt = info->slavecnt;\r\necc->dma_slave.filter.fn = edma_filter_fn;\r\nret = dma_async_device_register(&ecc->dma_slave);\r\nif (ret) {\r\ndev_err(dev, "slave ddev registration failed (%d)\n", ret);\r\ngoto err_reg1;\r\n}\r\nif (ecc->dma_memcpy) {\r\nret = dma_async_device_register(ecc->dma_memcpy);\r\nif (ret) {\r\ndev_err(dev, "memcpy ddev registration failed (%d)\n",\r\nret);\r\ndma_async_device_unregister(&ecc->dma_slave);\r\ngoto err_reg1;\r\n}\r\n}\r\nif (node)\r\nof_dma_controller_register(node, of_edma_xlate, ecc);\r\ndev_info(dev, "TI EDMA DMA engine driver\n");\r\nreturn 0;\r\nerr_reg1:\r\nedma_free_slot(ecc, ecc->dummy_slot);\r\nreturn ret;\r\n}\r\nstatic void edma_cleanupp_vchan(struct dma_device *dmadev)\r\n{\r\nstruct edma_chan *echan, *_echan;\r\nlist_for_each_entry_safe(echan, _echan,\r\n&dmadev->channels, vchan.chan.device_node) {\r\nlist_del(&echan->vchan.chan.device_node);\r\ntasklet_kill(&echan->vchan.task);\r\n}\r\n}\r\nstatic int edma_remove(struct platform_device *pdev)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct edma_cc *ecc = dev_get_drvdata(dev);\r\ndevm_free_irq(dev, ecc->ccint, ecc);\r\ndevm_free_irq(dev, ecc->ccerrint, ecc);\r\nedma_cleanupp_vchan(&ecc->dma_slave);\r\nif (dev->of_node)\r\nof_dma_controller_free(dev->of_node);\r\ndma_async_device_unregister(&ecc->dma_slave);\r\nif (ecc->dma_memcpy)\r\ndma_async_device_unregister(ecc->dma_memcpy);\r\nedma_free_slot(ecc, ecc->dummy_slot);\r\nreturn 0;\r\n}\r\nstatic int edma_pm_suspend(struct device *dev)\r\n{\r\nstruct edma_cc *ecc = dev_get_drvdata(dev);\r\nstruct edma_chan *echan = ecc->slave_chans;\r\nint i;\r\nfor (i = 0; i < ecc->num_channels; i++) {\r\nif (echan[i].alloced)\r\nedma_setup_interrupt(&echan[i], false);\r\n}\r\nreturn 0;\r\n}\r\nstatic int edma_pm_resume(struct device *dev)\r\n{\r\nstruct edma_cc *ecc = dev_get_drvdata(dev);\r\nstruct edma_chan *echan = ecc->slave_chans;\r\nint i;\r\ns8 (*queue_priority_mapping)[2];\r\nedma_write_slot(ecc, ecc->dummy_slot, &dummy_paramset);\r\nqueue_priority_mapping = ecc->info->queue_priority_mapping;\r\nfor (i = 0; queue_priority_mapping[i][0] != -1; i++)\r\nedma_assign_priority_to_queue(ecc, queue_priority_mapping[i][0],\r\nqueue_priority_mapping[i][1]);\r\nfor (i = 0; i < ecc->num_channels; i++) {\r\nif (echan[i].alloced) {\r\nedma_or_array2(ecc, EDMA_DRAE, 0, i >> 5,\r\nBIT(i & 0x1f));\r\nedma_setup_interrupt(&echan[i], true);\r\nedma_set_chmap(&echan[i], echan[i].slot[0]);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int edma_tptc_probe(struct platform_device *pdev)\r\n{\r\npm_runtime_enable(&pdev->dev);\r\nreturn pm_runtime_get_sync(&pdev->dev);\r\n}\r\nbool edma_filter_fn(struct dma_chan *chan, void *param)\r\n{\r\nbool match = false;\r\nif (chan->device->dev->driver == &edma_driver.driver) {\r\nstruct edma_chan *echan = to_edma_chan(chan);\r\nunsigned ch_req = *(unsigned *)param;\r\nif (ch_req == echan->ch_num) {\r\nechan->hw_triggered = true;\r\nmatch = true;\r\n}\r\n}\r\nreturn match;\r\n}\r\nstatic int edma_init(void)\r\n{\r\nint ret;\r\nret = platform_driver_register(&edma_tptc_driver);\r\nif (ret)\r\nreturn ret;\r\nreturn platform_driver_register(&edma_driver);\r\n}\r\nstatic void __exit edma_exit(void)\r\n{\r\nplatform_driver_unregister(&edma_driver);\r\nplatform_driver_unregister(&edma_tptc_driver);\r\n}
