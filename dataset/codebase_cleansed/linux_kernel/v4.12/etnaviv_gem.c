static void etnaviv_gem_scatter_map(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nstruct drm_device *dev = etnaviv_obj->base.dev;\r\nstruct sg_table *sgt = etnaviv_obj->sgt;\r\nif (etnaviv_obj->flags & ETNA_BO_CACHE_MASK)\r\ndma_map_sg(dev->dev, sgt->sgl, sgt->nents, DMA_BIDIRECTIONAL);\r\n}\r\nstatic void etnaviv_gem_scatterlist_unmap(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nstruct drm_device *dev = etnaviv_obj->base.dev;\r\nstruct sg_table *sgt = etnaviv_obj->sgt;\r\nif (etnaviv_obj->flags & ETNA_BO_CACHE_MASK)\r\ndma_unmap_sg(dev->dev, sgt->sgl, sgt->nents, DMA_BIDIRECTIONAL);\r\n}\r\nstatic int etnaviv_gem_shmem_get_pages(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nstruct drm_device *dev = etnaviv_obj->base.dev;\r\nstruct page **p = drm_gem_get_pages(&etnaviv_obj->base);\r\nif (IS_ERR(p)) {\r\ndev_err(dev->dev, "could not get pages: %ld\n", PTR_ERR(p));\r\nreturn PTR_ERR(p);\r\n}\r\netnaviv_obj->pages = p;\r\nreturn 0;\r\n}\r\nstatic void put_pages(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nif (etnaviv_obj->sgt) {\r\netnaviv_gem_scatterlist_unmap(etnaviv_obj);\r\nsg_free_table(etnaviv_obj->sgt);\r\nkfree(etnaviv_obj->sgt);\r\netnaviv_obj->sgt = NULL;\r\n}\r\nif (etnaviv_obj->pages) {\r\ndrm_gem_put_pages(&etnaviv_obj->base, etnaviv_obj->pages,\r\ntrue, false);\r\netnaviv_obj->pages = NULL;\r\n}\r\n}\r\nstruct page **etnaviv_gem_get_pages(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nint ret;\r\nlockdep_assert_held(&etnaviv_obj->lock);\r\nif (!etnaviv_obj->pages) {\r\nret = etnaviv_obj->ops->get_pages(etnaviv_obj);\r\nif (ret < 0)\r\nreturn ERR_PTR(ret);\r\n}\r\nif (!etnaviv_obj->sgt) {\r\nstruct drm_device *dev = etnaviv_obj->base.dev;\r\nint npages = etnaviv_obj->base.size >> PAGE_SHIFT;\r\nstruct sg_table *sgt;\r\nsgt = drm_prime_pages_to_sg(etnaviv_obj->pages, npages);\r\nif (IS_ERR(sgt)) {\r\ndev_err(dev->dev, "failed to allocate sgt: %ld\n",\r\nPTR_ERR(sgt));\r\nreturn ERR_CAST(sgt);\r\n}\r\netnaviv_obj->sgt = sgt;\r\netnaviv_gem_scatter_map(etnaviv_obj);\r\n}\r\nreturn etnaviv_obj->pages;\r\n}\r\nvoid etnaviv_gem_put_pages(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nlockdep_assert_held(&etnaviv_obj->lock);\r\n}\r\nstatic int etnaviv_gem_mmap_obj(struct etnaviv_gem_object *etnaviv_obj,\r\nstruct vm_area_struct *vma)\r\n{\r\npgprot_t vm_page_prot;\r\nvma->vm_flags &= ~VM_PFNMAP;\r\nvma->vm_flags |= VM_MIXEDMAP;\r\nvm_page_prot = vm_get_page_prot(vma->vm_flags);\r\nif (etnaviv_obj->flags & ETNA_BO_WC) {\r\nvma->vm_page_prot = pgprot_writecombine(vm_page_prot);\r\n} else if (etnaviv_obj->flags & ETNA_BO_UNCACHED) {\r\nvma->vm_page_prot = pgprot_noncached(vm_page_prot);\r\n} else {\r\nfput(vma->vm_file);\r\nget_file(etnaviv_obj->base.filp);\r\nvma->vm_pgoff = 0;\r\nvma->vm_file = etnaviv_obj->base.filp;\r\nvma->vm_page_prot = vm_page_prot;\r\n}\r\nreturn 0;\r\n}\r\nint etnaviv_gem_mmap(struct file *filp, struct vm_area_struct *vma)\r\n{\r\nstruct etnaviv_gem_object *obj;\r\nint ret;\r\nret = drm_gem_mmap(filp, vma);\r\nif (ret) {\r\nDBG("mmap failed: %d", ret);\r\nreturn ret;\r\n}\r\nobj = to_etnaviv_bo(vma->vm_private_data);\r\nreturn obj->ops->mmap(obj, vma);\r\n}\r\nint etnaviv_gem_fault(struct vm_fault *vmf)\r\n{\r\nstruct vm_area_struct *vma = vmf->vma;\r\nstruct drm_gem_object *obj = vma->vm_private_data;\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nstruct page **pages, *page;\r\npgoff_t pgoff;\r\nint ret;\r\nret = mutex_lock_interruptible(&etnaviv_obj->lock);\r\nif (ret)\r\ngoto out;\r\npages = etnaviv_gem_get_pages(etnaviv_obj);\r\nmutex_unlock(&etnaviv_obj->lock);\r\nif (IS_ERR(pages)) {\r\nret = PTR_ERR(pages);\r\ngoto out;\r\n}\r\npgoff = (vmf->address - vma->vm_start) >> PAGE_SHIFT;\r\npage = pages[pgoff];\r\nVERB("Inserting %p pfn %lx, pa %lx", (void *)vmf->address,\r\npage_to_pfn(page), page_to_pfn(page) << PAGE_SHIFT);\r\nret = vm_insert_page(vma, vmf->address, page);\r\nout:\r\nswitch (ret) {\r\ncase -EAGAIN:\r\ncase 0:\r\ncase -ERESTARTSYS:\r\ncase -EINTR:\r\ncase -EBUSY:\r\nreturn VM_FAULT_NOPAGE;\r\ncase -ENOMEM:\r\nreturn VM_FAULT_OOM;\r\ndefault:\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\n}\r\nint etnaviv_gem_mmap_offset(struct drm_gem_object *obj, u64 *offset)\r\n{\r\nint ret;\r\nret = drm_gem_create_mmap_offset(obj);\r\nif (ret)\r\ndev_err(obj->dev->dev, "could not allocate mmap offset\n");\r\nelse\r\n*offset = drm_vma_node_offset_addr(&obj->vma_node);\r\nreturn ret;\r\n}\r\nstatic struct etnaviv_vram_mapping *\r\netnaviv_gem_get_vram_mapping(struct etnaviv_gem_object *obj,\r\nstruct etnaviv_iommu *mmu)\r\n{\r\nstruct etnaviv_vram_mapping *mapping;\r\nlist_for_each_entry(mapping, &obj->vram_list, obj_node) {\r\nif (mapping->mmu == mmu)\r\nreturn mapping;\r\n}\r\nreturn NULL;\r\n}\r\nvoid etnaviv_gem_mapping_reference(struct etnaviv_vram_mapping *mapping)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = mapping->object;\r\ndrm_gem_object_reference(&etnaviv_obj->base);\r\nmutex_lock(&etnaviv_obj->lock);\r\nWARN_ON(mapping->use == 0);\r\nmapping->use += 1;\r\nmutex_unlock(&etnaviv_obj->lock);\r\n}\r\nvoid etnaviv_gem_mapping_unreference(struct etnaviv_vram_mapping *mapping)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = mapping->object;\r\nmutex_lock(&etnaviv_obj->lock);\r\nWARN_ON(mapping->use == 0);\r\nmapping->use -= 1;\r\nmutex_unlock(&etnaviv_obj->lock);\r\ndrm_gem_object_unreference_unlocked(&etnaviv_obj->base);\r\n}\r\nstruct etnaviv_vram_mapping *etnaviv_gem_mapping_get(\r\nstruct drm_gem_object *obj, struct etnaviv_gpu *gpu)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nstruct etnaviv_vram_mapping *mapping;\r\nstruct page **pages;\r\nint ret = 0;\r\nmutex_lock(&etnaviv_obj->lock);\r\nmapping = etnaviv_gem_get_vram_mapping(etnaviv_obj, gpu->mmu);\r\nif (mapping) {\r\nif (mapping->use == 0) {\r\nmutex_lock(&gpu->mmu->lock);\r\nif (mapping->mmu == gpu->mmu)\r\nmapping->use += 1;\r\nelse\r\nmapping = NULL;\r\nmutex_unlock(&gpu->mmu->lock);\r\nif (mapping)\r\ngoto out;\r\n} else {\r\nmapping->use += 1;\r\ngoto out;\r\n}\r\n}\r\npages = etnaviv_gem_get_pages(etnaviv_obj);\r\nif (IS_ERR(pages)) {\r\nret = PTR_ERR(pages);\r\ngoto out;\r\n}\r\nmapping = etnaviv_gem_get_vram_mapping(etnaviv_obj, NULL);\r\nif (!mapping) {\r\nmapping = kzalloc(sizeof(*mapping), GFP_KERNEL);\r\nif (!mapping) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nINIT_LIST_HEAD(&mapping->scan_node);\r\nmapping->object = etnaviv_obj;\r\n} else {\r\nlist_del(&mapping->obj_node);\r\n}\r\nmapping->mmu = gpu->mmu;\r\nmapping->use = 1;\r\nret = etnaviv_iommu_map_gem(gpu->mmu, etnaviv_obj, gpu->memory_base,\r\nmapping);\r\nif (ret < 0)\r\nkfree(mapping);\r\nelse\r\nlist_add_tail(&mapping->obj_node, &etnaviv_obj->vram_list);\r\nout:\r\nmutex_unlock(&etnaviv_obj->lock);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\ndrm_gem_object_reference(obj);\r\nreturn mapping;\r\n}\r\nvoid *etnaviv_gem_vmap(struct drm_gem_object *obj)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nif (etnaviv_obj->vaddr)\r\nreturn etnaviv_obj->vaddr;\r\nmutex_lock(&etnaviv_obj->lock);\r\nif (!etnaviv_obj->vaddr)\r\netnaviv_obj->vaddr = etnaviv_obj->ops->vmap(etnaviv_obj);\r\nmutex_unlock(&etnaviv_obj->lock);\r\nreturn etnaviv_obj->vaddr;\r\n}\r\nstatic void *etnaviv_gem_vmap_impl(struct etnaviv_gem_object *obj)\r\n{\r\nstruct page **pages;\r\nlockdep_assert_held(&obj->lock);\r\npages = etnaviv_gem_get_pages(obj);\r\nif (IS_ERR(pages))\r\nreturn NULL;\r\nreturn vmap(pages, obj->base.size >> PAGE_SHIFT,\r\nVM_MAP, pgprot_writecombine(PAGE_KERNEL));\r\n}\r\nstatic inline enum dma_data_direction etnaviv_op_to_dma_dir(u32 op)\r\n{\r\nif (op & ETNA_PREP_READ)\r\nreturn DMA_FROM_DEVICE;\r\nelse if (op & ETNA_PREP_WRITE)\r\nreturn DMA_TO_DEVICE;\r\nelse\r\nreturn DMA_BIDIRECTIONAL;\r\n}\r\nint etnaviv_gem_cpu_prep(struct drm_gem_object *obj, u32 op,\r\nstruct timespec *timeout)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nstruct drm_device *dev = obj->dev;\r\nbool write = !!(op & ETNA_PREP_WRITE);\r\nunsigned long remain =\r\nop & ETNA_PREP_NOSYNC ? 0 : etnaviv_timeout_to_jiffies(timeout);\r\nlong lret;\r\nlret = reservation_object_wait_timeout_rcu(etnaviv_obj->resv,\r\nwrite, true, remain);\r\nif (lret < 0)\r\nreturn lret;\r\nelse if (lret == 0)\r\nreturn remain == 0 ? -EBUSY : -ETIMEDOUT;\r\nif (etnaviv_obj->flags & ETNA_BO_CACHED) {\r\nif (!etnaviv_obj->sgt) {\r\nvoid *ret;\r\nmutex_lock(&etnaviv_obj->lock);\r\nret = etnaviv_gem_get_pages(etnaviv_obj);\r\nmutex_unlock(&etnaviv_obj->lock);\r\nif (IS_ERR(ret))\r\nreturn PTR_ERR(ret);\r\n}\r\ndma_sync_sg_for_cpu(dev->dev, etnaviv_obj->sgt->sgl,\r\netnaviv_obj->sgt->nents,\r\netnaviv_op_to_dma_dir(op));\r\netnaviv_obj->last_cpu_prep_op = op;\r\n}\r\nreturn 0;\r\n}\r\nint etnaviv_gem_cpu_fini(struct drm_gem_object *obj)\r\n{\r\nstruct drm_device *dev = obj->dev;\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nif (etnaviv_obj->flags & ETNA_BO_CACHED) {\r\nWARN_ON(etnaviv_obj->last_cpu_prep_op == 0);\r\ndma_sync_sg_for_device(dev->dev, etnaviv_obj->sgt->sgl,\r\netnaviv_obj->sgt->nents,\r\netnaviv_op_to_dma_dir(etnaviv_obj->last_cpu_prep_op));\r\netnaviv_obj->last_cpu_prep_op = 0;\r\n}\r\nreturn 0;\r\n}\r\nint etnaviv_gem_wait_bo(struct etnaviv_gpu *gpu, struct drm_gem_object *obj,\r\nstruct timespec *timeout)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nreturn etnaviv_gpu_wait_obj_inactive(gpu, etnaviv_obj, timeout);\r\n}\r\nstatic void etnaviv_gem_describe_fence(struct dma_fence *fence,\r\nconst char *type, struct seq_file *m)\r\n{\r\nif (!test_bit(DMA_FENCE_FLAG_SIGNALED_BIT, &fence->flags))\r\nseq_printf(m, "\t%9s: %s %s seq %u\n",\r\ntype,\r\nfence->ops->get_driver_name(fence),\r\nfence->ops->get_timeline_name(fence),\r\nfence->seqno);\r\n}\r\nstatic void etnaviv_gem_describe(struct drm_gem_object *obj, struct seq_file *m)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nstruct reservation_object *robj = etnaviv_obj->resv;\r\nstruct reservation_object_list *fobj;\r\nstruct dma_fence *fence;\r\nunsigned long off = drm_vma_node_start(&obj->vma_node);\r\nseq_printf(m, "%08x: %c %2d (%2d) %08lx %p %zd\n",\r\netnaviv_obj->flags, is_active(etnaviv_obj) ? 'A' : 'I',\r\nobj->name, kref_read(&obj->refcount),\r\noff, etnaviv_obj->vaddr, obj->size);\r\nrcu_read_lock();\r\nfobj = rcu_dereference(robj->fence);\r\nif (fobj) {\r\nunsigned int i, shared_count = fobj->shared_count;\r\nfor (i = 0; i < shared_count; i++) {\r\nfence = rcu_dereference(fobj->shared[i]);\r\netnaviv_gem_describe_fence(fence, "Shared", m);\r\n}\r\n}\r\nfence = rcu_dereference(robj->fence_excl);\r\nif (fence)\r\netnaviv_gem_describe_fence(fence, "Exclusive", m);\r\nrcu_read_unlock();\r\n}\r\nvoid etnaviv_gem_describe_objects(struct etnaviv_drm_private *priv,\r\nstruct seq_file *m)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj;\r\nint count = 0;\r\nsize_t size = 0;\r\nmutex_lock(&priv->gem_lock);\r\nlist_for_each_entry(etnaviv_obj, &priv->gem_list, gem_node) {\r\nstruct drm_gem_object *obj = &etnaviv_obj->base;\r\nseq_puts(m, " ");\r\netnaviv_gem_describe(obj, m);\r\ncount++;\r\nsize += obj->size;\r\n}\r\nmutex_unlock(&priv->gem_lock);\r\nseq_printf(m, "Total %d objects, %zu bytes\n", count, size);\r\n}\r\nstatic void etnaviv_gem_shmem_release(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nvunmap(etnaviv_obj->vaddr);\r\nput_pages(etnaviv_obj);\r\n}\r\nvoid etnaviv_gem_free_object(struct drm_gem_object *obj)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nstruct etnaviv_vram_mapping *mapping, *tmp;\r\nWARN_ON(is_active(etnaviv_obj));\r\nlist_del(&etnaviv_obj->gem_node);\r\nlist_for_each_entry_safe(mapping, tmp, &etnaviv_obj->vram_list,\r\nobj_node) {\r\nstruct etnaviv_iommu *mmu = mapping->mmu;\r\nWARN_ON(mapping->use);\r\nif (mmu)\r\netnaviv_iommu_unmap_gem(mmu, mapping);\r\nlist_del(&mapping->obj_node);\r\nkfree(mapping);\r\n}\r\ndrm_gem_free_mmap_offset(obj);\r\netnaviv_obj->ops->release(etnaviv_obj);\r\nif (etnaviv_obj->resv == &etnaviv_obj->_resv)\r\nreservation_object_fini(&etnaviv_obj->_resv);\r\ndrm_gem_object_release(obj);\r\nkfree(etnaviv_obj);\r\n}\r\nint etnaviv_gem_obj_add(struct drm_device *dev, struct drm_gem_object *obj)\r\n{\r\nstruct etnaviv_drm_private *priv = dev->dev_private;\r\nstruct etnaviv_gem_object *etnaviv_obj = to_etnaviv_bo(obj);\r\nmutex_lock(&priv->gem_lock);\r\nlist_add_tail(&etnaviv_obj->gem_node, &priv->gem_list);\r\nmutex_unlock(&priv->gem_lock);\r\nreturn 0;\r\n}\r\nstatic int etnaviv_gem_new_impl(struct drm_device *dev, u32 size, u32 flags,\r\nstruct reservation_object *robj, const struct etnaviv_gem_ops *ops,\r\nstruct drm_gem_object **obj)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj;\r\nunsigned sz = sizeof(*etnaviv_obj);\r\nbool valid = true;\r\nswitch (flags & ETNA_BO_CACHE_MASK) {\r\ncase ETNA_BO_UNCACHED:\r\ncase ETNA_BO_CACHED:\r\ncase ETNA_BO_WC:\r\nbreak;\r\ndefault:\r\nvalid = false;\r\n}\r\nif (!valid) {\r\ndev_err(dev->dev, "invalid cache flag: %x\n",\r\n(flags & ETNA_BO_CACHE_MASK));\r\nreturn -EINVAL;\r\n}\r\netnaviv_obj = kzalloc(sz, GFP_KERNEL);\r\nif (!etnaviv_obj)\r\nreturn -ENOMEM;\r\netnaviv_obj->flags = flags;\r\netnaviv_obj->ops = ops;\r\nif (robj) {\r\netnaviv_obj->resv = robj;\r\n} else {\r\netnaviv_obj->resv = &etnaviv_obj->_resv;\r\nreservation_object_init(&etnaviv_obj->_resv);\r\n}\r\nmutex_init(&etnaviv_obj->lock);\r\nINIT_LIST_HEAD(&etnaviv_obj->vram_list);\r\n*obj = &etnaviv_obj->base;\r\nreturn 0;\r\n}\r\nstatic struct drm_gem_object *__etnaviv_gem_new(struct drm_device *dev,\r\nu32 size, u32 flags)\r\n{\r\nstruct drm_gem_object *obj = NULL;\r\nint ret;\r\nsize = PAGE_ALIGN(size);\r\nret = etnaviv_gem_new_impl(dev, size, flags, NULL,\r\n&etnaviv_gem_shmem_ops, &obj);\r\nif (ret)\r\ngoto fail;\r\nret = drm_gem_object_init(dev, obj, size);\r\nif (ret == 0) {\r\nstruct address_space *mapping;\r\nmapping = obj->filp->f_mapping;\r\nmapping_set_gfp_mask(mapping, GFP_HIGHUSER);\r\n}\r\nif (ret)\r\ngoto fail;\r\nreturn obj;\r\nfail:\r\ndrm_gem_object_unreference_unlocked(obj);\r\nreturn ERR_PTR(ret);\r\n}\r\nint etnaviv_gem_new_handle(struct drm_device *dev, struct drm_file *file,\r\nu32 size, u32 flags, u32 *handle)\r\n{\r\nstruct drm_gem_object *obj;\r\nint ret;\r\nobj = __etnaviv_gem_new(dev, size, flags);\r\nif (IS_ERR(obj))\r\nreturn PTR_ERR(obj);\r\nret = etnaviv_gem_obj_add(dev, obj);\r\nif (ret < 0) {\r\ndrm_gem_object_unreference_unlocked(obj);\r\nreturn ret;\r\n}\r\nret = drm_gem_handle_create(file, obj, handle);\r\ndrm_gem_object_unreference_unlocked(obj);\r\nreturn ret;\r\n}\r\nstruct drm_gem_object *etnaviv_gem_new(struct drm_device *dev,\r\nu32 size, u32 flags)\r\n{\r\nstruct drm_gem_object *obj;\r\nint ret;\r\nobj = __etnaviv_gem_new(dev, size, flags);\r\nif (IS_ERR(obj))\r\nreturn obj;\r\nret = etnaviv_gem_obj_add(dev, obj);\r\nif (ret < 0) {\r\ndrm_gem_object_unreference_unlocked(obj);\r\nreturn ERR_PTR(ret);\r\n}\r\nreturn obj;\r\n}\r\nint etnaviv_gem_new_private(struct drm_device *dev, size_t size, u32 flags,\r\nstruct reservation_object *robj, const struct etnaviv_gem_ops *ops,\r\nstruct etnaviv_gem_object **res)\r\n{\r\nstruct drm_gem_object *obj;\r\nint ret;\r\nret = etnaviv_gem_new_impl(dev, size, flags, robj, ops, &obj);\r\nif (ret)\r\nreturn ret;\r\ndrm_gem_private_object_init(dev, obj, size);\r\n*res = to_etnaviv_bo(obj);\r\nreturn 0;\r\n}\r\nstatic struct page **etnaviv_gem_userptr_do_get_pages(\r\nstruct etnaviv_gem_object *etnaviv_obj, struct mm_struct *mm, struct task_struct *task)\r\n{\r\nint ret = 0, pinned, npages = etnaviv_obj->base.size >> PAGE_SHIFT;\r\nstruct page **pvec;\r\nuintptr_t ptr;\r\nunsigned int flags = 0;\r\npvec = drm_malloc_ab(npages, sizeof(struct page *));\r\nif (!pvec)\r\nreturn ERR_PTR(-ENOMEM);\r\nif (!etnaviv_obj->userptr.ro)\r\nflags |= FOLL_WRITE;\r\npinned = 0;\r\nptr = etnaviv_obj->userptr.ptr;\r\ndown_read(&mm->mmap_sem);\r\nwhile (pinned < npages) {\r\nret = get_user_pages_remote(task, mm, ptr, npages - pinned,\r\nflags, pvec + pinned, NULL, NULL);\r\nif (ret < 0)\r\nbreak;\r\nptr += ret * PAGE_SIZE;\r\npinned += ret;\r\n}\r\nup_read(&mm->mmap_sem);\r\nif (ret < 0) {\r\nrelease_pages(pvec, pinned, 0);\r\ndrm_free_large(pvec);\r\nreturn ERR_PTR(ret);\r\n}\r\nreturn pvec;\r\n}\r\nstatic void __etnaviv_gem_userptr_get_pages(struct work_struct *_work)\r\n{\r\nstruct get_pages_work *work = container_of(_work, typeof(*work), work);\r\nstruct etnaviv_gem_object *etnaviv_obj = work->etnaviv_obj;\r\nstruct page **pvec;\r\npvec = etnaviv_gem_userptr_do_get_pages(etnaviv_obj, work->mm, work->task);\r\nmutex_lock(&etnaviv_obj->lock);\r\nif (IS_ERR(pvec)) {\r\netnaviv_obj->userptr.work = ERR_CAST(pvec);\r\n} else {\r\netnaviv_obj->userptr.work = NULL;\r\netnaviv_obj->pages = pvec;\r\n}\r\nmutex_unlock(&etnaviv_obj->lock);\r\ndrm_gem_object_unreference_unlocked(&etnaviv_obj->base);\r\nmmput(work->mm);\r\nput_task_struct(work->task);\r\nkfree(work);\r\n}\r\nstatic int etnaviv_gem_userptr_get_pages(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nstruct page **pvec = NULL;\r\nstruct get_pages_work *work;\r\nstruct mm_struct *mm;\r\nint ret, pinned, npages = etnaviv_obj->base.size >> PAGE_SHIFT;\r\nif (etnaviv_obj->userptr.work) {\r\nif (IS_ERR(etnaviv_obj->userptr.work)) {\r\nret = PTR_ERR(etnaviv_obj->userptr.work);\r\netnaviv_obj->userptr.work = NULL;\r\n} else {\r\nret = -EAGAIN;\r\n}\r\nreturn ret;\r\n}\r\nmm = get_task_mm(etnaviv_obj->userptr.task);\r\npinned = 0;\r\nif (mm == current->mm) {\r\npvec = drm_malloc_ab(npages, sizeof(struct page *));\r\nif (!pvec) {\r\nmmput(mm);\r\nreturn -ENOMEM;\r\n}\r\npinned = __get_user_pages_fast(etnaviv_obj->userptr.ptr, npages,\r\n!etnaviv_obj->userptr.ro, pvec);\r\nif (pinned < 0) {\r\ndrm_free_large(pvec);\r\nmmput(mm);\r\nreturn pinned;\r\n}\r\nif (pinned == npages) {\r\netnaviv_obj->pages = pvec;\r\nmmput(mm);\r\nreturn 0;\r\n}\r\n}\r\nrelease_pages(pvec, pinned, 0);\r\ndrm_free_large(pvec);\r\nwork = kmalloc(sizeof(*work), GFP_KERNEL);\r\nif (!work) {\r\nmmput(mm);\r\nreturn -ENOMEM;\r\n}\r\nget_task_struct(current);\r\ndrm_gem_object_reference(&etnaviv_obj->base);\r\nwork->mm = mm;\r\nwork->task = current;\r\nwork->etnaviv_obj = etnaviv_obj;\r\netnaviv_obj->userptr.work = &work->work;\r\nINIT_WORK(&work->work, __etnaviv_gem_userptr_get_pages);\r\netnaviv_queue_work(etnaviv_obj->base.dev, &work->work);\r\nreturn -EAGAIN;\r\n}\r\nstatic void etnaviv_gem_userptr_release(struct etnaviv_gem_object *etnaviv_obj)\r\n{\r\nif (etnaviv_obj->sgt) {\r\netnaviv_gem_scatterlist_unmap(etnaviv_obj);\r\nsg_free_table(etnaviv_obj->sgt);\r\nkfree(etnaviv_obj->sgt);\r\n}\r\nif (etnaviv_obj->pages) {\r\nint npages = etnaviv_obj->base.size >> PAGE_SHIFT;\r\nrelease_pages(etnaviv_obj->pages, npages, 0);\r\ndrm_free_large(etnaviv_obj->pages);\r\n}\r\nput_task_struct(etnaviv_obj->userptr.task);\r\n}\r\nstatic int etnaviv_gem_userptr_mmap_obj(struct etnaviv_gem_object *etnaviv_obj,\r\nstruct vm_area_struct *vma)\r\n{\r\nreturn -EINVAL;\r\n}\r\nint etnaviv_gem_new_userptr(struct drm_device *dev, struct drm_file *file,\r\nuintptr_t ptr, u32 size, u32 flags, u32 *handle)\r\n{\r\nstruct etnaviv_gem_object *etnaviv_obj;\r\nint ret;\r\nret = etnaviv_gem_new_private(dev, size, ETNA_BO_CACHED, NULL,\r\n&etnaviv_gem_userptr_ops, &etnaviv_obj);\r\nif (ret)\r\nreturn ret;\r\netnaviv_obj->userptr.ptr = ptr;\r\netnaviv_obj->userptr.task = current;\r\netnaviv_obj->userptr.ro = !(flags & ETNA_USERPTR_WRITE);\r\nget_task_struct(current);\r\nret = etnaviv_gem_obj_add(dev, &etnaviv_obj->base);\r\nif (ret)\r\ngoto unreference;\r\nret = drm_gem_handle_create(file, &etnaviv_obj->base, handle);\r\nunreference:\r\ndrm_gem_object_unreference_unlocked(&etnaviv_obj->base);\r\nreturn ret;\r\n}
