static int ef4_check_disabled(struct ef4_nic *efx)\r\n{\r\nif (efx->state == STATE_DISABLED || efx->state == STATE_RECOVERY) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"device is disabled due to earlier errors\n");\r\nreturn -EIO;\r\n}\r\nreturn 0;\r\n}\r\nstatic int ef4_process_channel(struct ef4_channel *channel, int budget)\r\n{\r\nstruct ef4_tx_queue *tx_queue;\r\nint spent;\r\nif (unlikely(!channel->enabled))\r\nreturn 0;\r\nef4_for_each_channel_tx_queue(tx_queue, channel) {\r\ntx_queue->pkts_compl = 0;\r\ntx_queue->bytes_compl = 0;\r\n}\r\nspent = ef4_nic_process_eventq(channel, budget);\r\nif (spent && ef4_channel_has_rx_queue(channel)) {\r\nstruct ef4_rx_queue *rx_queue =\r\nef4_channel_get_rx_queue(channel);\r\nef4_rx_flush_packet(channel);\r\nef4_fast_push_rx_descriptors(rx_queue, true);\r\n}\r\nef4_for_each_channel_tx_queue(tx_queue, channel) {\r\nif (tx_queue->bytes_compl) {\r\nnetdev_tx_completed_queue(tx_queue->core_txq,\r\ntx_queue->pkts_compl, tx_queue->bytes_compl);\r\n}\r\n}\r\nreturn spent;\r\n}\r\nstatic void ef4_update_irq_mod(struct ef4_nic *efx, struct ef4_channel *channel)\r\n{\r\nint step = efx->irq_mod_step_us;\r\nif (channel->irq_mod_score < irq_adapt_low_thresh) {\r\nif (channel->irq_moderation_us > step) {\r\nchannel->irq_moderation_us -= step;\r\nefx->type->push_irq_moderation(channel);\r\n}\r\n} else if (channel->irq_mod_score > irq_adapt_high_thresh) {\r\nif (channel->irq_moderation_us <\r\nefx->irq_rx_moderation_us) {\r\nchannel->irq_moderation_us += step;\r\nefx->type->push_irq_moderation(channel);\r\n}\r\n}\r\nchannel->irq_count = 0;\r\nchannel->irq_mod_score = 0;\r\n}\r\nstatic int ef4_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct ef4_channel *channel =\r\ncontainer_of(napi, struct ef4_channel, napi_str);\r\nstruct ef4_nic *efx = channel->efx;\r\nint spent;\r\nnetif_vdbg(efx, intr, efx->net_dev,\r\n"channel %d NAPI poll executing on CPU %d\n",\r\nchannel->channel, raw_smp_processor_id());\r\nspent = ef4_process_channel(channel, budget);\r\nif (spent < budget) {\r\nif (ef4_channel_has_rx_queue(channel) &&\r\nefx->irq_rx_adaptive &&\r\nunlikely(++channel->irq_count == 1000)) {\r\nef4_update_irq_mod(efx, channel);\r\n}\r\nef4_filter_rfs_expire(channel);\r\nnapi_complete_done(napi, spent);\r\nef4_nic_eventq_read_ack(channel);\r\n}\r\nreturn spent;\r\n}\r\nstatic int ef4_probe_eventq(struct ef4_channel *channel)\r\n{\r\nstruct ef4_nic *efx = channel->efx;\r\nunsigned long entries;\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"chan %d create event queue\n", channel->channel);\r\nentries = roundup_pow_of_two(efx->rxq_entries + efx->txq_entries + 128);\r\nEF4_BUG_ON_PARANOID(entries > EF4_MAX_EVQ_SIZE);\r\nchannel->eventq_mask = max(entries, EF4_MIN_EVQ_SIZE) - 1;\r\nreturn ef4_nic_probe_eventq(channel);\r\n}\r\nstatic int ef4_init_eventq(struct ef4_channel *channel)\r\n{\r\nstruct ef4_nic *efx = channel->efx;\r\nint rc;\r\nEF4_WARN_ON_PARANOID(channel->eventq_init);\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"chan %d init event queue\n", channel->channel);\r\nrc = ef4_nic_init_eventq(channel);\r\nif (rc == 0) {\r\nefx->type->push_irq_moderation(channel);\r\nchannel->eventq_read_ptr = 0;\r\nchannel->eventq_init = true;\r\n}\r\nreturn rc;\r\n}\r\nvoid ef4_start_eventq(struct ef4_channel *channel)\r\n{\r\nnetif_dbg(channel->efx, ifup, channel->efx->net_dev,\r\n"chan %d start event queue\n", channel->channel);\r\nchannel->enabled = true;\r\nsmp_wmb();\r\nnapi_enable(&channel->napi_str);\r\nef4_nic_eventq_read_ack(channel);\r\n}\r\nvoid ef4_stop_eventq(struct ef4_channel *channel)\r\n{\r\nif (!channel->enabled)\r\nreturn;\r\nnapi_disable(&channel->napi_str);\r\nchannel->enabled = false;\r\n}\r\nstatic void ef4_fini_eventq(struct ef4_channel *channel)\r\n{\r\nif (!channel->eventq_init)\r\nreturn;\r\nnetif_dbg(channel->efx, drv, channel->efx->net_dev,\r\n"chan %d fini event queue\n", channel->channel);\r\nef4_nic_fini_eventq(channel);\r\nchannel->eventq_init = false;\r\n}\r\nstatic void ef4_remove_eventq(struct ef4_channel *channel)\r\n{\r\nnetif_dbg(channel->efx, drv, channel->efx->net_dev,\r\n"chan %d remove event queue\n", channel->channel);\r\nef4_nic_remove_eventq(channel);\r\n}\r\nstatic struct ef4_channel *\r\nef4_alloc_channel(struct ef4_nic *efx, int i, struct ef4_channel *old_channel)\r\n{\r\nstruct ef4_channel *channel;\r\nstruct ef4_rx_queue *rx_queue;\r\nstruct ef4_tx_queue *tx_queue;\r\nint j;\r\nchannel = kzalloc(sizeof(*channel), GFP_KERNEL);\r\nif (!channel)\r\nreturn NULL;\r\nchannel->efx = efx;\r\nchannel->channel = i;\r\nchannel->type = &ef4_default_channel_type;\r\nfor (j = 0; j < EF4_TXQ_TYPES; j++) {\r\ntx_queue = &channel->tx_queue[j];\r\ntx_queue->efx = efx;\r\ntx_queue->queue = i * EF4_TXQ_TYPES + j;\r\ntx_queue->channel = channel;\r\n}\r\nrx_queue = &channel->rx_queue;\r\nrx_queue->efx = efx;\r\nsetup_timer(&rx_queue->slow_fill, ef4_rx_slow_fill,\r\n(unsigned long)rx_queue);\r\nreturn channel;\r\n}\r\nstatic struct ef4_channel *\r\nef4_copy_channel(const struct ef4_channel *old_channel)\r\n{\r\nstruct ef4_channel *channel;\r\nstruct ef4_rx_queue *rx_queue;\r\nstruct ef4_tx_queue *tx_queue;\r\nint j;\r\nchannel = kmalloc(sizeof(*channel), GFP_KERNEL);\r\nif (!channel)\r\nreturn NULL;\r\n*channel = *old_channel;\r\nchannel->napi_dev = NULL;\r\nINIT_HLIST_NODE(&channel->napi_str.napi_hash_node);\r\nchannel->napi_str.napi_id = 0;\r\nchannel->napi_str.state = 0;\r\nmemset(&channel->eventq, 0, sizeof(channel->eventq));\r\nfor (j = 0; j < EF4_TXQ_TYPES; j++) {\r\ntx_queue = &channel->tx_queue[j];\r\nif (tx_queue->channel)\r\ntx_queue->channel = channel;\r\ntx_queue->buffer = NULL;\r\nmemset(&tx_queue->txd, 0, sizeof(tx_queue->txd));\r\n}\r\nrx_queue = &channel->rx_queue;\r\nrx_queue->buffer = NULL;\r\nmemset(&rx_queue->rxd, 0, sizeof(rx_queue->rxd));\r\nsetup_timer(&rx_queue->slow_fill, ef4_rx_slow_fill,\r\n(unsigned long)rx_queue);\r\nreturn channel;\r\n}\r\nstatic int ef4_probe_channel(struct ef4_channel *channel)\r\n{\r\nstruct ef4_tx_queue *tx_queue;\r\nstruct ef4_rx_queue *rx_queue;\r\nint rc;\r\nnetif_dbg(channel->efx, probe, channel->efx->net_dev,\r\n"creating channel %d\n", channel->channel);\r\nrc = channel->type->pre_probe(channel);\r\nif (rc)\r\ngoto fail;\r\nrc = ef4_probe_eventq(channel);\r\nif (rc)\r\ngoto fail;\r\nef4_for_each_channel_tx_queue(tx_queue, channel) {\r\nrc = ef4_probe_tx_queue(tx_queue);\r\nif (rc)\r\ngoto fail;\r\n}\r\nef4_for_each_channel_rx_queue(rx_queue, channel) {\r\nrc = ef4_probe_rx_queue(rx_queue);\r\nif (rc)\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nef4_remove_channel(channel);\r\nreturn rc;\r\n}\r\nstatic void\r\nef4_get_channel_name(struct ef4_channel *channel, char *buf, size_t len)\r\n{\r\nstruct ef4_nic *efx = channel->efx;\r\nconst char *type;\r\nint number;\r\nnumber = channel->channel;\r\nif (efx->tx_channel_offset == 0) {\r\ntype = "";\r\n} else if (channel->channel < efx->tx_channel_offset) {\r\ntype = "-rx";\r\n} else {\r\ntype = "-tx";\r\nnumber -= efx->tx_channel_offset;\r\n}\r\nsnprintf(buf, len, "%s%s-%d", efx->name, type, number);\r\n}\r\nstatic void ef4_set_channel_names(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nef4_for_each_channel(channel, efx)\r\nchannel->type->get_name(channel,\r\nefx->msi_context[channel->channel].name,\r\nsizeof(efx->msi_context[0].name));\r\n}\r\nstatic int ef4_probe_channels(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nint rc;\r\nefx->next_buffer_table = 0;\r\nef4_for_each_channel_rev(channel, efx) {\r\nrc = ef4_probe_channel(channel);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to create channel %d\n",\r\nchannel->channel);\r\ngoto fail;\r\n}\r\n}\r\nef4_set_channel_names(efx);\r\nreturn 0;\r\nfail:\r\nef4_remove_channels(efx);\r\nreturn rc;\r\n}\r\nstatic void ef4_start_datapath(struct ef4_nic *efx)\r\n{\r\nnetdev_features_t old_features = efx->net_dev->features;\r\nbool old_rx_scatter = efx->rx_scatter;\r\nstruct ef4_tx_queue *tx_queue;\r\nstruct ef4_rx_queue *rx_queue;\r\nstruct ef4_channel *channel;\r\nsize_t rx_buf_len;\r\nefx->rx_dma_len = (efx->rx_prefix_size +\r\nEF4_MAX_FRAME_LEN(efx->net_dev->mtu) +\r\nefx->type->rx_buffer_padding);\r\nrx_buf_len = (sizeof(struct ef4_rx_page_state) +\r\nefx->rx_ip_align + efx->rx_dma_len);\r\nif (rx_buf_len <= PAGE_SIZE) {\r\nefx->rx_scatter = efx->type->always_rx_scatter;\r\nefx->rx_buffer_order = 0;\r\n} else if (efx->type->can_rx_scatter) {\r\nBUILD_BUG_ON(EF4_RX_USR_BUF_SIZE % L1_CACHE_BYTES);\r\nBUILD_BUG_ON(sizeof(struct ef4_rx_page_state) +\r\n2 * ALIGN(NET_IP_ALIGN + EF4_RX_USR_BUF_SIZE,\r\nEF4_RX_BUF_ALIGNMENT) >\r\nPAGE_SIZE);\r\nefx->rx_scatter = true;\r\nefx->rx_dma_len = EF4_RX_USR_BUF_SIZE;\r\nefx->rx_buffer_order = 0;\r\n} else {\r\nefx->rx_scatter = false;\r\nefx->rx_buffer_order = get_order(rx_buf_len);\r\n}\r\nef4_rx_config_page_split(efx);\r\nif (efx->rx_buffer_order)\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"RX buf len=%u; page order=%u batch=%u\n",\r\nefx->rx_dma_len, efx->rx_buffer_order,\r\nefx->rx_pages_per_batch);\r\nelse\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"RX buf len=%u step=%u bpp=%u; page batch=%u\n",\r\nefx->rx_dma_len, efx->rx_page_buf_step,\r\nefx->rx_bufs_per_page, efx->rx_pages_per_batch);\r\nefx->net_dev->hw_features |= efx->net_dev->features;\r\nefx->net_dev->hw_features &= ~efx->fixed_features;\r\nefx->net_dev->features |= efx->fixed_features;\r\nif (efx->net_dev->features != old_features)\r\nnetdev_features_change(efx->net_dev);\r\nif (efx->rx_scatter != old_rx_scatter)\r\nefx->type->filter_update_rx_scatter(efx);\r\nefx->txq_stop_thresh = efx->txq_entries - ef4_tx_max_skb_descs(efx);\r\nefx->txq_wake_thresh = efx->txq_stop_thresh / 2;\r\nef4_for_each_channel(channel, efx) {\r\nef4_for_each_channel_tx_queue(tx_queue, channel) {\r\nef4_init_tx_queue(tx_queue);\r\natomic_inc(&efx->active_queues);\r\n}\r\nef4_for_each_channel_rx_queue(rx_queue, channel) {\r\nef4_init_rx_queue(rx_queue);\r\natomic_inc(&efx->active_queues);\r\nef4_stop_eventq(channel);\r\nef4_fast_push_rx_descriptors(rx_queue, false);\r\nef4_start_eventq(channel);\r\n}\r\nWARN_ON(channel->rx_pkt_n_frags);\r\n}\r\nif (netif_device_present(efx->net_dev))\r\nnetif_tx_wake_all_queues(efx->net_dev);\r\n}\r\nstatic void ef4_stop_datapath(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nstruct ef4_tx_queue *tx_queue;\r\nstruct ef4_rx_queue *rx_queue;\r\nint rc;\r\nEF4_ASSERT_RESET_SERIALISED(efx);\r\nBUG_ON(efx->port_enabled);\r\nef4_for_each_channel(channel, efx) {\r\nef4_for_each_channel_rx_queue(rx_queue, channel)\r\nrx_queue->refill_enabled = false;\r\n}\r\nef4_for_each_channel(channel, efx) {\r\nif (ef4_channel_has_rx_queue(channel)) {\r\nef4_stop_eventq(channel);\r\nef4_start_eventq(channel);\r\n}\r\n}\r\nrc = efx->type->fini_dmaq(efx);\r\nif (rc && EF4_WORKAROUND_7803(efx)) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"Resetting to recover from flush failure\n");\r\nef4_schedule_reset(efx, RESET_TYPE_ALL);\r\n} else if (rc) {\r\nnetif_err(efx, drv, efx->net_dev, "failed to flush queues\n");\r\n} else {\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"successfully flushed all queues\n");\r\n}\r\nef4_for_each_channel(channel, efx) {\r\nef4_for_each_channel_rx_queue(rx_queue, channel)\r\nef4_fini_rx_queue(rx_queue);\r\nef4_for_each_possible_channel_tx_queue(tx_queue, channel)\r\nef4_fini_tx_queue(tx_queue);\r\n}\r\n}\r\nstatic void ef4_remove_channel(struct ef4_channel *channel)\r\n{\r\nstruct ef4_tx_queue *tx_queue;\r\nstruct ef4_rx_queue *rx_queue;\r\nnetif_dbg(channel->efx, drv, channel->efx->net_dev,\r\n"destroy chan %d\n", channel->channel);\r\nef4_for_each_channel_rx_queue(rx_queue, channel)\r\nef4_remove_rx_queue(rx_queue);\r\nef4_for_each_possible_channel_tx_queue(tx_queue, channel)\r\nef4_remove_tx_queue(tx_queue);\r\nef4_remove_eventq(channel);\r\nchannel->type->post_remove(channel);\r\n}\r\nstatic void ef4_remove_channels(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nef4_for_each_channel(channel, efx)\r\nef4_remove_channel(channel);\r\n}\r\nint\r\nef4_realloc_channels(struct ef4_nic *efx, u32 rxq_entries, u32 txq_entries)\r\n{\r\nstruct ef4_channel *other_channel[EF4_MAX_CHANNELS], *channel;\r\nu32 old_rxq_entries, old_txq_entries;\r\nunsigned i, next_buffer_table = 0;\r\nint rc, rc2;\r\nrc = ef4_check_disabled(efx);\r\nif (rc)\r\nreturn rc;\r\nef4_for_each_channel(channel, efx) {\r\nstruct ef4_rx_queue *rx_queue;\r\nstruct ef4_tx_queue *tx_queue;\r\nif (channel->type->copy)\r\ncontinue;\r\nnext_buffer_table = max(next_buffer_table,\r\nchannel->eventq.index +\r\nchannel->eventq.entries);\r\nef4_for_each_channel_rx_queue(rx_queue, channel)\r\nnext_buffer_table = max(next_buffer_table,\r\nrx_queue->rxd.index +\r\nrx_queue->rxd.entries);\r\nef4_for_each_channel_tx_queue(tx_queue, channel)\r\nnext_buffer_table = max(next_buffer_table,\r\ntx_queue->txd.index +\r\ntx_queue->txd.entries);\r\n}\r\nef4_device_detach_sync(efx);\r\nef4_stop_all(efx);\r\nef4_soft_disable_interrupts(efx);\r\nmemset(other_channel, 0, sizeof(other_channel));\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = efx->channel[i];\r\nif (channel->type->copy)\r\nchannel = channel->type->copy(channel);\r\nif (!channel) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nother_channel[i] = channel;\r\n}\r\nold_rxq_entries = efx->rxq_entries;\r\nold_txq_entries = efx->txq_entries;\r\nefx->rxq_entries = rxq_entries;\r\nefx->txq_entries = txq_entries;\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = efx->channel[i];\r\nefx->channel[i] = other_channel[i];\r\nother_channel[i] = channel;\r\n}\r\nefx->next_buffer_table = next_buffer_table;\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = efx->channel[i];\r\nif (!channel->type->copy)\r\ncontinue;\r\nrc = ef4_probe_channel(channel);\r\nif (rc)\r\ngoto rollback;\r\nef4_init_napi_channel(efx->channel[i]);\r\n}\r\nout:\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = other_channel[i];\r\nif (channel && channel->type->copy) {\r\nef4_fini_napi_channel(channel);\r\nef4_remove_channel(channel);\r\nkfree(channel);\r\n}\r\n}\r\nrc2 = ef4_soft_enable_interrupts(efx);\r\nif (rc2) {\r\nrc = rc ? rc : rc2;\r\nnetif_err(efx, drv, efx->net_dev,\r\n"unable to restart interrupts on channel reallocation\n");\r\nef4_schedule_reset(efx, RESET_TYPE_DISABLE);\r\n} else {\r\nef4_start_all(efx);\r\nnetif_device_attach(efx->net_dev);\r\n}\r\nreturn rc;\r\nrollback:\r\nefx->rxq_entries = old_rxq_entries;\r\nefx->txq_entries = old_txq_entries;\r\nfor (i = 0; i < efx->n_channels; i++) {\r\nchannel = efx->channel[i];\r\nefx->channel[i] = other_channel[i];\r\nother_channel[i] = channel;\r\n}\r\ngoto out;\r\n}\r\nvoid ef4_schedule_slow_fill(struct ef4_rx_queue *rx_queue)\r\n{\r\nmod_timer(&rx_queue->slow_fill, jiffies + msecs_to_jiffies(100));\r\n}\r\nint ef4_channel_dummy_op_int(struct ef4_channel *channel)\r\n{\r\nreturn 0;\r\n}\r\nvoid ef4_channel_dummy_op_void(struct ef4_channel *channel)\r\n{\r\n}\r\nvoid ef4_link_status_changed(struct ef4_nic *efx)\r\n{\r\nstruct ef4_link_state *link_state = &efx->link_state;\r\nif (!netif_running(efx->net_dev))\r\nreturn;\r\nif (link_state->up != netif_carrier_ok(efx->net_dev)) {\r\nefx->n_link_state_changes++;\r\nif (link_state->up)\r\nnetif_carrier_on(efx->net_dev);\r\nelse\r\nnetif_carrier_off(efx->net_dev);\r\n}\r\nif (link_state->up)\r\nnetif_info(efx, link, efx->net_dev,\r\n"link up at %uMbps %s-duplex (MTU %d)\n",\r\nlink_state->speed, link_state->fd ? "full" : "half",\r\nefx->net_dev->mtu);\r\nelse\r\nnetif_info(efx, link, efx->net_dev, "link down\n");\r\n}\r\nvoid ef4_link_set_advertising(struct ef4_nic *efx, u32 advertising)\r\n{\r\nefx->link_advertising = advertising;\r\nif (advertising) {\r\nif (advertising & ADVERTISED_Pause)\r\nefx->wanted_fc |= (EF4_FC_TX | EF4_FC_RX);\r\nelse\r\nefx->wanted_fc &= ~(EF4_FC_TX | EF4_FC_RX);\r\nif (advertising & ADVERTISED_Asym_Pause)\r\nefx->wanted_fc ^= EF4_FC_TX;\r\n}\r\n}\r\nvoid ef4_link_set_wanted_fc(struct ef4_nic *efx, u8 wanted_fc)\r\n{\r\nefx->wanted_fc = wanted_fc;\r\nif (efx->link_advertising) {\r\nif (wanted_fc & EF4_FC_RX)\r\nefx->link_advertising |= (ADVERTISED_Pause |\r\nADVERTISED_Asym_Pause);\r\nelse\r\nefx->link_advertising &= ~(ADVERTISED_Pause |\r\nADVERTISED_Asym_Pause);\r\nif (wanted_fc & EF4_FC_TX)\r\nefx->link_advertising ^= ADVERTISED_Asym_Pause;\r\n}\r\n}\r\nvoid ef4_mac_reconfigure(struct ef4_nic *efx)\r\n{\r\ndown_read(&efx->filter_sem);\r\nefx->type->reconfigure_mac(efx);\r\nup_read(&efx->filter_sem);\r\n}\r\nint __ef4_reconfigure_port(struct ef4_nic *efx)\r\n{\r\nenum ef4_phy_mode phy_mode;\r\nint rc;\r\nWARN_ON(!mutex_is_locked(&efx->mac_lock));\r\nphy_mode = efx->phy_mode;\r\nif (LOOPBACK_INTERNAL(efx))\r\nefx->phy_mode |= PHY_MODE_TX_DISABLED;\r\nelse\r\nefx->phy_mode &= ~PHY_MODE_TX_DISABLED;\r\nrc = efx->type->reconfigure_port(efx);\r\nif (rc)\r\nefx->phy_mode = phy_mode;\r\nreturn rc;\r\n}\r\nint ef4_reconfigure_port(struct ef4_nic *efx)\r\n{\r\nint rc;\r\nEF4_ASSERT_RESET_SERIALISED(efx);\r\nmutex_lock(&efx->mac_lock);\r\nrc = __ef4_reconfigure_port(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nreturn rc;\r\n}\r\nstatic void ef4_mac_work(struct work_struct *data)\r\n{\r\nstruct ef4_nic *efx = container_of(data, struct ef4_nic, mac_work);\r\nmutex_lock(&efx->mac_lock);\r\nif (efx->port_enabled)\r\nef4_mac_reconfigure(efx);\r\nmutex_unlock(&efx->mac_lock);\r\n}\r\nstatic int ef4_probe_port(struct ef4_nic *efx)\r\n{\r\nint rc;\r\nnetif_dbg(efx, probe, efx->net_dev, "create port\n");\r\nif (phy_flash_cfg)\r\nefx->phy_mode = PHY_MODE_SPECIAL;\r\nrc = efx->type->probe_port(efx);\r\nif (rc)\r\nreturn rc;\r\nether_addr_copy(efx->net_dev->dev_addr, efx->net_dev->perm_addr);\r\nreturn 0;\r\n}\r\nstatic int ef4_init_port(struct ef4_nic *efx)\r\n{\r\nint rc;\r\nnetif_dbg(efx, drv, efx->net_dev, "init port\n");\r\nmutex_lock(&efx->mac_lock);\r\nrc = efx->phy_op->init(efx);\r\nif (rc)\r\ngoto fail1;\r\nefx->port_initialized = true;\r\nef4_mac_reconfigure(efx);\r\nrc = efx->phy_op->reconfigure(efx);\r\nif (rc && rc != -EPERM)\r\ngoto fail2;\r\nmutex_unlock(&efx->mac_lock);\r\nreturn 0;\r\nfail2:\r\nefx->phy_op->fini(efx);\r\nfail1:\r\nmutex_unlock(&efx->mac_lock);\r\nreturn rc;\r\n}\r\nstatic void ef4_start_port(struct ef4_nic *efx)\r\n{\r\nnetif_dbg(efx, ifup, efx->net_dev, "start port\n");\r\nBUG_ON(efx->port_enabled);\r\nmutex_lock(&efx->mac_lock);\r\nefx->port_enabled = true;\r\nef4_mac_reconfigure(efx);\r\nmutex_unlock(&efx->mac_lock);\r\n}\r\nstatic void ef4_stop_port(struct ef4_nic *efx)\r\n{\r\nnetif_dbg(efx, ifdown, efx->net_dev, "stop port\n");\r\nEF4_ASSERT_RESET_SERIALISED(efx);\r\nmutex_lock(&efx->mac_lock);\r\nefx->port_enabled = false;\r\nmutex_unlock(&efx->mac_lock);\r\nnetif_addr_lock_bh(efx->net_dev);\r\nnetif_addr_unlock_bh(efx->net_dev);\r\ncancel_delayed_work_sync(&efx->monitor_work);\r\nef4_selftest_async_cancel(efx);\r\ncancel_work_sync(&efx->mac_work);\r\n}\r\nstatic void ef4_fini_port(struct ef4_nic *efx)\r\n{\r\nnetif_dbg(efx, drv, efx->net_dev, "shut down port\n");\r\nif (!efx->port_initialized)\r\nreturn;\r\nefx->phy_op->fini(efx);\r\nefx->port_initialized = false;\r\nefx->link_state.up = false;\r\nef4_link_status_changed(efx);\r\n}\r\nstatic void ef4_remove_port(struct ef4_nic *efx)\r\n{\r\nnetif_dbg(efx, drv, efx->net_dev, "destroying port\n");\r\nefx->type->remove_port(efx);\r\n}\r\nstatic bool ef4_same_controller(struct ef4_nic *left, struct ef4_nic *right)\r\n{\r\nreturn left->type == right->type &&\r\nleft->vpd_sn && right->vpd_sn &&\r\n!strcmp(left->vpd_sn, right->vpd_sn);\r\n}\r\nstatic void ef4_associate(struct ef4_nic *efx)\r\n{\r\nstruct ef4_nic *other, *next;\r\nif (efx->primary == efx) {\r\nnetif_dbg(efx, probe, efx->net_dev, "adding to primary list\n");\r\nlist_add_tail(&efx->node, &ef4_primary_list);\r\nlist_for_each_entry_safe(other, next, &ef4_unassociated_list,\r\nnode) {\r\nif (ef4_same_controller(efx, other)) {\r\nlist_del(&other->node);\r\nnetif_dbg(other, probe, other->net_dev,\r\n"moving to secondary list of %s %s\n",\r\npci_name(efx->pci_dev),\r\nefx->net_dev->name);\r\nlist_add_tail(&other->node,\r\n&efx->secondary_list);\r\nother->primary = efx;\r\n}\r\n}\r\n} else {\r\nlist_for_each_entry(other, &ef4_primary_list, node) {\r\nif (ef4_same_controller(efx, other)) {\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"adding to secondary list of %s %s\n",\r\npci_name(other->pci_dev),\r\nother->net_dev->name);\r\nlist_add_tail(&efx->node,\r\n&other->secondary_list);\r\nefx->primary = other;\r\nreturn;\r\n}\r\n}\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"adding to unassociated list\n");\r\nlist_add_tail(&efx->node, &ef4_unassociated_list);\r\n}\r\n}\r\nstatic void ef4_dissociate(struct ef4_nic *efx)\r\n{\r\nstruct ef4_nic *other, *next;\r\nlist_del(&efx->node);\r\nefx->primary = NULL;\r\nlist_for_each_entry_safe(other, next, &efx->secondary_list, node) {\r\nlist_del(&other->node);\r\nnetif_dbg(other, probe, other->net_dev,\r\n"moving to unassociated list\n");\r\nlist_add_tail(&other->node, &ef4_unassociated_list);\r\nother->primary = NULL;\r\n}\r\n}\r\nstatic int ef4_init_io(struct ef4_nic *efx)\r\n{\r\nstruct pci_dev *pci_dev = efx->pci_dev;\r\ndma_addr_t dma_mask = efx->type->max_dma_mask;\r\nunsigned int mem_map_size = efx->type->mem_map_size(efx);\r\nint rc, bar;\r\nnetif_dbg(efx, probe, efx->net_dev, "initialising I/O\n");\r\nbar = efx->type->mem_bar;\r\nrc = pci_enable_device(pci_dev);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to enable PCI device\n");\r\ngoto fail1;\r\n}\r\npci_set_master(pci_dev);\r\nwhile (dma_mask > 0x7fffffffUL) {\r\nrc = dma_set_mask_and_coherent(&pci_dev->dev, dma_mask);\r\nif (rc == 0)\r\nbreak;\r\ndma_mask >>= 1;\r\n}\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"could not find a suitable DMA mask\n");\r\ngoto fail2;\r\n}\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"using DMA mask %llx\n", (unsigned long long) dma_mask);\r\nefx->membase_phys = pci_resource_start(efx->pci_dev, bar);\r\nrc = pci_request_region(pci_dev, bar, "sfc");\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"request for memory BAR failed\n");\r\nrc = -EIO;\r\ngoto fail3;\r\n}\r\nefx->membase = ioremap_nocache(efx->membase_phys, mem_map_size);\r\nif (!efx->membase) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"could not map memory BAR at %llx+%x\n",\r\n(unsigned long long)efx->membase_phys, mem_map_size);\r\nrc = -ENOMEM;\r\ngoto fail4;\r\n}\r\nnetif_dbg(efx, probe, efx->net_dev,\r\n"memory BAR at %llx+%x (virtual %p)\n",\r\n(unsigned long long)efx->membase_phys, mem_map_size,\r\nefx->membase);\r\nreturn 0;\r\nfail4:\r\npci_release_region(efx->pci_dev, bar);\r\nfail3:\r\nefx->membase_phys = 0;\r\nfail2:\r\npci_disable_device(efx->pci_dev);\r\nfail1:\r\nreturn rc;\r\n}\r\nstatic void ef4_fini_io(struct ef4_nic *efx)\r\n{\r\nint bar;\r\nnetif_dbg(efx, drv, efx->net_dev, "shutting down I/O\n");\r\nif (efx->membase) {\r\niounmap(efx->membase);\r\nefx->membase = NULL;\r\n}\r\nif (efx->membase_phys) {\r\nbar = efx->type->mem_bar;\r\npci_release_region(efx->pci_dev, bar);\r\nefx->membase_phys = 0;\r\n}\r\nif (!pci_vfs_assigned(efx->pci_dev))\r\npci_disable_device(efx->pci_dev);\r\n}\r\nvoid ef4_set_default_rx_indir_table(struct ef4_nic *efx)\r\n{\r\nsize_t i;\r\nfor (i = 0; i < ARRAY_SIZE(efx->rx_indir_table); i++)\r\nefx->rx_indir_table[i] =\r\nethtool_rxfh_indir_default(i, efx->rss_spread);\r\n}\r\nstatic unsigned int ef4_wanted_parallelism(struct ef4_nic *efx)\r\n{\r\ncpumask_var_t thread_mask;\r\nunsigned int count;\r\nint cpu;\r\nif (rss_cpus) {\r\ncount = rss_cpus;\r\n} else {\r\nif (unlikely(!zalloc_cpumask_var(&thread_mask, GFP_KERNEL))) {\r\nnetif_warn(efx, probe, efx->net_dev,\r\n"RSS disabled due to allocation failure\n");\r\nreturn 1;\r\n}\r\ncount = 0;\r\nfor_each_online_cpu(cpu) {\r\nif (!cpumask_test_cpu(cpu, thread_mask)) {\r\n++count;\r\ncpumask_or(thread_mask, thread_mask,\r\ntopology_sibling_cpumask(cpu));\r\n}\r\n}\r\nfree_cpumask_var(thread_mask);\r\n}\r\nif (count > EF4_MAX_RX_QUEUES) {\r\nnetif_cond_dbg(efx, probe, efx->net_dev, !rss_cpus, warn,\r\n"Reducing number of rx queues from %u to %u.\n",\r\ncount, EF4_MAX_RX_QUEUES);\r\ncount = EF4_MAX_RX_QUEUES;\r\n}\r\nreturn count;\r\n}\r\nstatic int ef4_probe_interrupts(struct ef4_nic *efx)\r\n{\r\nunsigned int extra_channels = 0;\r\nunsigned int i, j;\r\nint rc;\r\nfor (i = 0; i < EF4_MAX_EXTRA_CHANNELS; i++)\r\nif (efx->extra_channel_type[i])\r\n++extra_channels;\r\nif (efx->interrupt_mode == EF4_INT_MODE_MSIX) {\r\nstruct msix_entry xentries[EF4_MAX_CHANNELS];\r\nunsigned int n_channels;\r\nn_channels = ef4_wanted_parallelism(efx);\r\nif (ef4_separate_tx_channels)\r\nn_channels *= 2;\r\nn_channels += extra_channels;\r\nn_channels = min(n_channels, efx->max_channels);\r\nfor (i = 0; i < n_channels; i++)\r\nxentries[i].entry = i;\r\nrc = pci_enable_msix_range(efx->pci_dev,\r\nxentries, 1, n_channels);\r\nif (rc < 0) {\r\nefx->interrupt_mode = EF4_INT_MODE_MSI;\r\nnetif_err(efx, drv, efx->net_dev,\r\n"could not enable MSI-X\n");\r\n} else if (rc < n_channels) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"WARNING: Insufficient MSI-X vectors"\r\n" available (%d < %u).\n", rc, n_channels);\r\nnetif_err(efx, drv, efx->net_dev,\r\n"WARNING: Performance may be reduced.\n");\r\nn_channels = rc;\r\n}\r\nif (rc > 0) {\r\nefx->n_channels = n_channels;\r\nif (n_channels > extra_channels)\r\nn_channels -= extra_channels;\r\nif (ef4_separate_tx_channels) {\r\nefx->n_tx_channels = min(max(n_channels / 2,\r\n1U),\r\nefx->max_tx_channels);\r\nefx->n_rx_channels = max(n_channels -\r\nefx->n_tx_channels,\r\n1U);\r\n} else {\r\nefx->n_tx_channels = min(n_channels,\r\nefx->max_tx_channels);\r\nefx->n_rx_channels = n_channels;\r\n}\r\nfor (i = 0; i < efx->n_channels; i++)\r\nef4_get_channel(efx, i)->irq =\r\nxentries[i].vector;\r\n}\r\n}\r\nif (efx->interrupt_mode == EF4_INT_MODE_MSI) {\r\nefx->n_channels = 1;\r\nefx->n_rx_channels = 1;\r\nefx->n_tx_channels = 1;\r\nrc = pci_enable_msi(efx->pci_dev);\r\nif (rc == 0) {\r\nef4_get_channel(efx, 0)->irq = efx->pci_dev->irq;\r\n} else {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"could not enable MSI\n");\r\nefx->interrupt_mode = EF4_INT_MODE_LEGACY;\r\n}\r\n}\r\nif (efx->interrupt_mode == EF4_INT_MODE_LEGACY) {\r\nefx->n_channels = 1 + (ef4_separate_tx_channels ? 1 : 0);\r\nefx->n_rx_channels = 1;\r\nefx->n_tx_channels = 1;\r\nefx->legacy_irq = efx->pci_dev->irq;\r\n}\r\nj = efx->n_channels;\r\nfor (i = 0; i < EF4_MAX_EXTRA_CHANNELS; i++) {\r\nif (!efx->extra_channel_type[i])\r\ncontinue;\r\nif (efx->interrupt_mode != EF4_INT_MODE_MSIX ||\r\nefx->n_channels <= extra_channels) {\r\nefx->extra_channel_type[i]->handle_no_channel(efx);\r\n} else {\r\n--j;\r\nef4_get_channel(efx, j)->type =\r\nefx->extra_channel_type[i];\r\n}\r\n}\r\nefx->rss_spread = efx->n_rx_channels;\r\nreturn 0;\r\n}\r\nstatic int ef4_soft_enable_interrupts(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel, *end_channel;\r\nint rc;\r\nBUG_ON(efx->state == STATE_DISABLED);\r\nefx->irq_soft_enabled = true;\r\nsmp_wmb();\r\nef4_for_each_channel(channel, efx) {\r\nif (!channel->type->keep_eventq) {\r\nrc = ef4_init_eventq(channel);\r\nif (rc)\r\ngoto fail;\r\n}\r\nef4_start_eventq(channel);\r\n}\r\nreturn 0;\r\nfail:\r\nend_channel = channel;\r\nef4_for_each_channel(channel, efx) {\r\nif (channel == end_channel)\r\nbreak;\r\nef4_stop_eventq(channel);\r\nif (!channel->type->keep_eventq)\r\nef4_fini_eventq(channel);\r\n}\r\nreturn rc;\r\n}\r\nstatic void ef4_soft_disable_interrupts(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nif (efx->state == STATE_DISABLED)\r\nreturn;\r\nefx->irq_soft_enabled = false;\r\nsmp_wmb();\r\nif (efx->legacy_irq)\r\nsynchronize_irq(efx->legacy_irq);\r\nef4_for_each_channel(channel, efx) {\r\nif (channel->irq)\r\nsynchronize_irq(channel->irq);\r\nef4_stop_eventq(channel);\r\nif (!channel->type->keep_eventq)\r\nef4_fini_eventq(channel);\r\n}\r\n}\r\nstatic int ef4_enable_interrupts(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel, *end_channel;\r\nint rc;\r\nBUG_ON(efx->state == STATE_DISABLED);\r\nif (efx->eeh_disabled_legacy_irq) {\r\nenable_irq(efx->legacy_irq);\r\nefx->eeh_disabled_legacy_irq = false;\r\n}\r\nefx->type->irq_enable_master(efx);\r\nef4_for_each_channel(channel, efx) {\r\nif (channel->type->keep_eventq) {\r\nrc = ef4_init_eventq(channel);\r\nif (rc)\r\ngoto fail;\r\n}\r\n}\r\nrc = ef4_soft_enable_interrupts(efx);\r\nif (rc)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\nend_channel = channel;\r\nef4_for_each_channel(channel, efx) {\r\nif (channel == end_channel)\r\nbreak;\r\nif (channel->type->keep_eventq)\r\nef4_fini_eventq(channel);\r\n}\r\nefx->type->irq_disable_non_ev(efx);\r\nreturn rc;\r\n}\r\nstatic void ef4_disable_interrupts(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nef4_soft_disable_interrupts(efx);\r\nef4_for_each_channel(channel, efx) {\r\nif (channel->type->keep_eventq)\r\nef4_fini_eventq(channel);\r\n}\r\nefx->type->irq_disable_non_ev(efx);\r\n}\r\nstatic void ef4_remove_interrupts(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nef4_for_each_channel(channel, efx)\r\nchannel->irq = 0;\r\npci_disable_msi(efx->pci_dev);\r\npci_disable_msix(efx->pci_dev);\r\nefx->legacy_irq = 0;\r\n}\r\nstatic void ef4_set_channels(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nstruct ef4_tx_queue *tx_queue;\r\nefx->tx_channel_offset =\r\nef4_separate_tx_channels ?\r\nefx->n_channels - efx->n_tx_channels : 0;\r\nef4_for_each_channel(channel, efx) {\r\nif (channel->channel < efx->n_rx_channels)\r\nchannel->rx_queue.core_index = channel->channel;\r\nelse\r\nchannel->rx_queue.core_index = -1;\r\nef4_for_each_channel_tx_queue(tx_queue, channel)\r\ntx_queue->queue -= (efx->tx_channel_offset *\r\nEF4_TXQ_TYPES);\r\n}\r\n}\r\nstatic int ef4_probe_nic(struct ef4_nic *efx)\r\n{\r\nint rc;\r\nnetif_dbg(efx, probe, efx->net_dev, "creating NIC\n");\r\nrc = efx->type->probe(efx);\r\nif (rc)\r\nreturn rc;\r\ndo {\r\nif (!efx->max_channels || !efx->max_tx_channels) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"Insufficient resources to allocate"\r\n" any channels\n");\r\nrc = -ENOSPC;\r\ngoto fail1;\r\n}\r\nrc = ef4_probe_interrupts(efx);\r\nif (rc)\r\ngoto fail1;\r\nef4_set_channels(efx);\r\nrc = efx->type->dimension_resources(efx);\r\nif (rc != 0 && rc != -EAGAIN)\r\ngoto fail2;\r\nif (rc == -EAGAIN)\r\nef4_remove_interrupts(efx);\r\n} while (rc == -EAGAIN);\r\nif (efx->n_channels > 1)\r\nnetdev_rss_key_fill(&efx->rx_hash_key,\r\nsizeof(efx->rx_hash_key));\r\nef4_set_default_rx_indir_table(efx);\r\nnetif_set_real_num_tx_queues(efx->net_dev, efx->n_tx_channels);\r\nnetif_set_real_num_rx_queues(efx->net_dev, efx->n_rx_channels);\r\nefx->irq_mod_step_us = DIV_ROUND_UP(efx->timer_quantum_ns, 1000);\r\nef4_init_irq_moderation(efx, tx_irq_mod_usec, rx_irq_mod_usec, true,\r\ntrue);\r\nreturn 0;\r\nfail2:\r\nef4_remove_interrupts(efx);\r\nfail1:\r\nefx->type->remove(efx);\r\nreturn rc;\r\n}\r\nstatic void ef4_remove_nic(struct ef4_nic *efx)\r\n{\r\nnetif_dbg(efx, drv, efx->net_dev, "destroying NIC\n");\r\nef4_remove_interrupts(efx);\r\nefx->type->remove(efx);\r\n}\r\nstatic int ef4_probe_filters(struct ef4_nic *efx)\r\n{\r\nint rc;\r\nspin_lock_init(&efx->filter_lock);\r\ninit_rwsem(&efx->filter_sem);\r\nmutex_lock(&efx->mac_lock);\r\ndown_write(&efx->filter_sem);\r\nrc = efx->type->filter_table_probe(efx);\r\nif (rc)\r\ngoto out_unlock;\r\n#ifdef CONFIG_RFS_ACCEL\r\nif (efx->type->offload_features & NETIF_F_NTUPLE) {\r\nstruct ef4_channel *channel;\r\nint i, success = 1;\r\nef4_for_each_channel(channel, efx) {\r\nchannel->rps_flow_id =\r\nkcalloc(efx->type->max_rx_ip_filters,\r\nsizeof(*channel->rps_flow_id),\r\nGFP_KERNEL);\r\nif (!channel->rps_flow_id)\r\nsuccess = 0;\r\nelse\r\nfor (i = 0;\r\ni < efx->type->max_rx_ip_filters;\r\n++i)\r\nchannel->rps_flow_id[i] =\r\nRPS_FLOW_ID_INVALID;\r\n}\r\nif (!success) {\r\nef4_for_each_channel(channel, efx)\r\nkfree(channel->rps_flow_id);\r\nefx->type->filter_table_remove(efx);\r\nrc = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\nefx->rps_expire_index = efx->rps_expire_channel = 0;\r\n}\r\n#endif\r\nout_unlock:\r\nup_write(&efx->filter_sem);\r\nmutex_unlock(&efx->mac_lock);\r\nreturn rc;\r\n}\r\nstatic void ef4_remove_filters(struct ef4_nic *efx)\r\n{\r\n#ifdef CONFIG_RFS_ACCEL\r\nstruct ef4_channel *channel;\r\nef4_for_each_channel(channel, efx)\r\nkfree(channel->rps_flow_id);\r\n#endif\r\ndown_write(&efx->filter_sem);\r\nefx->type->filter_table_remove(efx);\r\nup_write(&efx->filter_sem);\r\n}\r\nstatic void ef4_restore_filters(struct ef4_nic *efx)\r\n{\r\ndown_read(&efx->filter_sem);\r\nefx->type->filter_table_restore(efx);\r\nup_read(&efx->filter_sem);\r\n}\r\nstatic int ef4_probe_all(struct ef4_nic *efx)\r\n{\r\nint rc;\r\nrc = ef4_probe_nic(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev, "failed to create NIC\n");\r\ngoto fail1;\r\n}\r\nrc = ef4_probe_port(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev, "failed to create port\n");\r\ngoto fail2;\r\n}\r\nBUILD_BUG_ON(EF4_DEFAULT_DMAQ_SIZE < EF4_RXQ_MIN_ENT);\r\nif (WARN_ON(EF4_DEFAULT_DMAQ_SIZE < EF4_TXQ_MIN_ENT(efx))) {\r\nrc = -EINVAL;\r\ngoto fail3;\r\n}\r\nefx->rxq_entries = efx->txq_entries = EF4_DEFAULT_DMAQ_SIZE;\r\nrc = ef4_probe_filters(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to create filter tables\n");\r\ngoto fail4;\r\n}\r\nrc = ef4_probe_channels(efx);\r\nif (rc)\r\ngoto fail5;\r\nreturn 0;\r\nfail5:\r\nef4_remove_filters(efx);\r\nfail4:\r\nfail3:\r\nef4_remove_port(efx);\r\nfail2:\r\nef4_remove_nic(efx);\r\nfail1:\r\nreturn rc;\r\n}\r\nstatic void ef4_start_all(struct ef4_nic *efx)\r\n{\r\nEF4_ASSERT_RESET_SERIALISED(efx);\r\nBUG_ON(efx->state == STATE_DISABLED);\r\nif (efx->port_enabled || !netif_running(efx->net_dev) ||\r\nefx->reset_pending)\r\nreturn;\r\nef4_start_port(efx);\r\nef4_start_datapath(efx);\r\nif (efx->type->monitor != NULL)\r\nqueue_delayed_work(efx->workqueue, &efx->monitor_work,\r\nef4_monitor_interval);\r\nefx->type->start_stats(efx);\r\nefx->type->pull_stats(efx);\r\nspin_lock_bh(&efx->stats_lock);\r\nefx->type->update_stats(efx, NULL, NULL);\r\nspin_unlock_bh(&efx->stats_lock);\r\n}\r\nstatic void ef4_stop_all(struct ef4_nic *efx)\r\n{\r\nEF4_ASSERT_RESET_SERIALISED(efx);\r\nif (!efx->port_enabled)\r\nreturn;\r\nefx->type->pull_stats(efx);\r\nspin_lock_bh(&efx->stats_lock);\r\nefx->type->update_stats(efx, NULL, NULL);\r\nspin_unlock_bh(&efx->stats_lock);\r\nefx->type->stop_stats(efx);\r\nef4_stop_port(efx);\r\nWARN_ON(netif_running(efx->net_dev) &&\r\nnetif_device_present(efx->net_dev));\r\nnetif_tx_disable(efx->net_dev);\r\nef4_stop_datapath(efx);\r\n}\r\nstatic void ef4_remove_all(struct ef4_nic *efx)\r\n{\r\nef4_remove_channels(efx);\r\nef4_remove_filters(efx);\r\nef4_remove_port(efx);\r\nef4_remove_nic(efx);\r\n}\r\nunsigned int ef4_usecs_to_ticks(struct ef4_nic *efx, unsigned int usecs)\r\n{\r\nif (usecs == 0)\r\nreturn 0;\r\nif (usecs * 1000 < efx->timer_quantum_ns)\r\nreturn 1;\r\nreturn usecs * 1000 / efx->timer_quantum_ns;\r\n}\r\nunsigned int ef4_ticks_to_usecs(struct ef4_nic *efx, unsigned int ticks)\r\n{\r\nreturn DIV_ROUND_UP(ticks * efx->timer_quantum_ns, 1000);\r\n}\r\nint ef4_init_irq_moderation(struct ef4_nic *efx, unsigned int tx_usecs,\r\nunsigned int rx_usecs, bool rx_adaptive,\r\nbool rx_may_override_tx)\r\n{\r\nstruct ef4_channel *channel;\r\nunsigned int timer_max_us;\r\nEF4_ASSERT_RESET_SERIALISED(efx);\r\ntimer_max_us = efx->timer_max_ns / 1000;\r\nif (tx_usecs > timer_max_us || rx_usecs > timer_max_us)\r\nreturn -EINVAL;\r\nif (tx_usecs != rx_usecs && efx->tx_channel_offset == 0 &&\r\n!rx_may_override_tx) {\r\nnetif_err(efx, drv, efx->net_dev, "Channels are shared. "\r\n"RX and TX IRQ moderation must be equal\n");\r\nreturn -EINVAL;\r\n}\r\nefx->irq_rx_adaptive = rx_adaptive;\r\nefx->irq_rx_moderation_us = rx_usecs;\r\nef4_for_each_channel(channel, efx) {\r\nif (ef4_channel_has_rx_queue(channel))\r\nchannel->irq_moderation_us = rx_usecs;\r\nelse if (ef4_channel_has_tx_queues(channel))\r\nchannel->irq_moderation_us = tx_usecs;\r\n}\r\nreturn 0;\r\n}\r\nvoid ef4_get_irq_moderation(struct ef4_nic *efx, unsigned int *tx_usecs,\r\nunsigned int *rx_usecs, bool *rx_adaptive)\r\n{\r\n*rx_adaptive = efx->irq_rx_adaptive;\r\n*rx_usecs = efx->irq_rx_moderation_us;\r\nif (efx->tx_channel_offset == 0) {\r\n*tx_usecs = *rx_usecs;\r\n} else {\r\nstruct ef4_channel *tx_channel;\r\ntx_channel = efx->channel[efx->tx_channel_offset];\r\n*tx_usecs = tx_channel->irq_moderation_us;\r\n}\r\n}\r\nstatic void ef4_monitor(struct work_struct *data)\r\n{\r\nstruct ef4_nic *efx = container_of(data, struct ef4_nic,\r\nmonitor_work.work);\r\nnetif_vdbg(efx, timer, efx->net_dev,\r\n"hardware monitor executing on CPU %d\n",\r\nraw_smp_processor_id());\r\nBUG_ON(efx->type->monitor == NULL);\r\nif (mutex_trylock(&efx->mac_lock)) {\r\nif (efx->port_enabled)\r\nefx->type->monitor(efx);\r\nmutex_unlock(&efx->mac_lock);\r\n}\r\nqueue_delayed_work(efx->workqueue, &efx->monitor_work,\r\nef4_monitor_interval);\r\n}\r\nstatic int ef4_ioctl(struct net_device *net_dev, struct ifreq *ifr, int cmd)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nstruct mii_ioctl_data *data = if_mii(ifr);\r\nif ((cmd == SIOCGMIIREG || cmd == SIOCSMIIREG) &&\r\n(data->phy_id & 0xfc00) == 0x0400)\r\ndata->phy_id ^= MDIO_PHY_ID_C45 | 0x0400;\r\nreturn mdio_mii_ioctl(&efx->mdio, data, cmd);\r\n}\r\nstatic void ef4_init_napi_channel(struct ef4_channel *channel)\r\n{\r\nstruct ef4_nic *efx = channel->efx;\r\nchannel->napi_dev = efx->net_dev;\r\nnetif_napi_add(channel->napi_dev, &channel->napi_str,\r\nef4_poll, napi_weight);\r\n}\r\nstatic void ef4_init_napi(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nef4_for_each_channel(channel, efx)\r\nef4_init_napi_channel(channel);\r\n}\r\nstatic void ef4_fini_napi_channel(struct ef4_channel *channel)\r\n{\r\nif (channel->napi_dev)\r\nnetif_napi_del(&channel->napi_str);\r\nchannel->napi_dev = NULL;\r\n}\r\nstatic void ef4_fini_napi(struct ef4_nic *efx)\r\n{\r\nstruct ef4_channel *channel;\r\nef4_for_each_channel(channel, efx)\r\nef4_fini_napi_channel(channel);\r\n}\r\nstatic void ef4_netpoll(struct net_device *net_dev)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nstruct ef4_channel *channel;\r\nef4_for_each_channel(channel, efx)\r\nef4_schedule_channel(channel);\r\n}\r\nint ef4_net_open(struct net_device *net_dev)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nint rc;\r\nnetif_dbg(efx, ifup, efx->net_dev, "opening device on CPU %d\n",\r\nraw_smp_processor_id());\r\nrc = ef4_check_disabled(efx);\r\nif (rc)\r\nreturn rc;\r\nif (efx->phy_mode & PHY_MODE_SPECIAL)\r\nreturn -EBUSY;\r\nef4_link_status_changed(efx);\r\nef4_start_all(efx);\r\nef4_selftest_async_start(efx);\r\nreturn 0;\r\n}\r\nint ef4_net_stop(struct net_device *net_dev)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nnetif_dbg(efx, ifdown, efx->net_dev, "closing on CPU %d\n",\r\nraw_smp_processor_id());\r\nef4_stop_all(efx);\r\nreturn 0;\r\n}\r\nstatic void ef4_net_stats(struct net_device *net_dev,\r\nstruct rtnl_link_stats64 *stats)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nspin_lock_bh(&efx->stats_lock);\r\nefx->type->update_stats(efx, NULL, stats);\r\nspin_unlock_bh(&efx->stats_lock);\r\n}\r\nstatic void ef4_watchdog(struct net_device *net_dev)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nnetif_err(efx, tx_err, efx->net_dev,\r\n"TX stuck with port_enabled=%d: resetting channels\n",\r\nefx->port_enabled);\r\nef4_schedule_reset(efx, RESET_TYPE_TX_WATCHDOG);\r\n}\r\nstatic int ef4_change_mtu(struct net_device *net_dev, int new_mtu)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nint rc;\r\nrc = ef4_check_disabled(efx);\r\nif (rc)\r\nreturn rc;\r\nnetif_dbg(efx, drv, efx->net_dev, "changing MTU to %d\n", new_mtu);\r\nef4_device_detach_sync(efx);\r\nef4_stop_all(efx);\r\nmutex_lock(&efx->mac_lock);\r\nnet_dev->mtu = new_mtu;\r\nef4_mac_reconfigure(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nef4_start_all(efx);\r\nnetif_device_attach(efx->net_dev);\r\nreturn 0;\r\n}\r\nstatic int ef4_set_mac_address(struct net_device *net_dev, void *data)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nstruct sockaddr *addr = data;\r\nu8 *new_addr = addr->sa_data;\r\nu8 old_addr[6];\r\nint rc;\r\nif (!is_valid_ether_addr(new_addr)) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"invalid ethernet MAC address requested: %pM\n",\r\nnew_addr);\r\nreturn -EADDRNOTAVAIL;\r\n}\r\nether_addr_copy(old_addr, net_dev->dev_addr);\r\nether_addr_copy(net_dev->dev_addr, new_addr);\r\nif (efx->type->set_mac_address) {\r\nrc = efx->type->set_mac_address(efx);\r\nif (rc) {\r\nether_addr_copy(net_dev->dev_addr, old_addr);\r\nreturn rc;\r\n}\r\n}\r\nmutex_lock(&efx->mac_lock);\r\nef4_mac_reconfigure(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nreturn 0;\r\n}\r\nstatic void ef4_set_rx_mode(struct net_device *net_dev)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nif (efx->port_enabled)\r\nqueue_work(efx->workqueue, &efx->mac_work);\r\n}\r\nstatic int ef4_set_features(struct net_device *net_dev, netdev_features_t data)\r\n{\r\nstruct ef4_nic *efx = netdev_priv(net_dev);\r\nint rc;\r\nif (net_dev->features & ~data & NETIF_F_NTUPLE) {\r\nrc = efx->type->filter_clear_rx(efx, EF4_FILTER_PRI_MANUAL);\r\nif (rc)\r\nreturn rc;\r\n}\r\nif ((net_dev->features ^ data) & NETIF_F_HW_VLAN_CTAG_FILTER) {\r\nef4_set_rx_mode(net_dev);\r\n}\r\nreturn 0;\r\n}\r\nstatic void ef4_update_name(struct ef4_nic *efx)\r\n{\r\nstrcpy(efx->name, efx->net_dev->name);\r\nef4_mtd_rename(efx);\r\nef4_set_channel_names(efx);\r\n}\r\nstatic int ef4_netdev_event(struct notifier_block *this,\r\nunsigned long event, void *ptr)\r\n{\r\nstruct net_device *net_dev = netdev_notifier_info_to_dev(ptr);\r\nif ((net_dev->netdev_ops == &ef4_netdev_ops) &&\r\nevent == NETDEV_CHANGENAME)\r\nef4_update_name(netdev_priv(net_dev));\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic ssize_t\r\nshow_phy_type(struct device *dev, struct device_attribute *attr, char *buf)\r\n{\r\nstruct ef4_nic *efx = pci_get_drvdata(to_pci_dev(dev));\r\nreturn sprintf(buf, "%d\n", efx->phy_type);\r\n}\r\nstatic int ef4_register_netdev(struct ef4_nic *efx)\r\n{\r\nstruct net_device *net_dev = efx->net_dev;\r\nstruct ef4_channel *channel;\r\nint rc;\r\nnet_dev->watchdog_timeo = 5 * HZ;\r\nnet_dev->irq = efx->pci_dev->irq;\r\nnet_dev->netdev_ops = &ef4_netdev_ops;\r\nnet_dev->ethtool_ops = &ef4_ethtool_ops;\r\nnet_dev->gso_max_segs = EF4_TSO_MAX_SEGS;\r\nnet_dev->min_mtu = EF4_MIN_MTU;\r\nnet_dev->max_mtu = EF4_MAX_MTU;\r\nrtnl_lock();\r\nefx->state = STATE_READY;\r\nsmp_mb();\r\nif (efx->reset_pending) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"aborting probe due to scheduled reset\n");\r\nrc = -EIO;\r\ngoto fail_locked;\r\n}\r\nrc = dev_alloc_name(net_dev, net_dev->name);\r\nif (rc < 0)\r\ngoto fail_locked;\r\nef4_update_name(efx);\r\nnetif_carrier_off(net_dev);\r\nrc = register_netdevice(net_dev);\r\nif (rc)\r\ngoto fail_locked;\r\nef4_for_each_channel(channel, efx) {\r\nstruct ef4_tx_queue *tx_queue;\r\nef4_for_each_channel_tx_queue(tx_queue, channel)\r\nef4_init_tx_queue_core_txq(tx_queue);\r\n}\r\nef4_associate(efx);\r\nrtnl_unlock();\r\nrc = device_create_file(&efx->pci_dev->dev, &dev_attr_phy_type);\r\nif (rc) {\r\nnetif_err(efx, drv, efx->net_dev,\r\n"failed to init net dev attributes\n");\r\ngoto fail_registered;\r\n}\r\nreturn 0;\r\nfail_registered:\r\nrtnl_lock();\r\nef4_dissociate(efx);\r\nunregister_netdevice(net_dev);\r\nfail_locked:\r\nefx->state = STATE_UNINIT;\r\nrtnl_unlock();\r\nnetif_err(efx, drv, efx->net_dev, "could not register net dev\n");\r\nreturn rc;\r\n}\r\nstatic void ef4_unregister_netdev(struct ef4_nic *efx)\r\n{\r\nif (!efx->net_dev)\r\nreturn;\r\nBUG_ON(netdev_priv(efx->net_dev) != efx);\r\nif (ef4_dev_registered(efx)) {\r\nstrlcpy(efx->name, pci_name(efx->pci_dev), sizeof(efx->name));\r\ndevice_remove_file(&efx->pci_dev->dev, &dev_attr_phy_type);\r\nunregister_netdev(efx->net_dev);\r\n}\r\n}\r\nvoid ef4_reset_down(struct ef4_nic *efx, enum reset_type method)\r\n{\r\nEF4_ASSERT_RESET_SERIALISED(efx);\r\nef4_stop_all(efx);\r\nef4_disable_interrupts(efx);\r\nmutex_lock(&efx->mac_lock);\r\nif (efx->port_initialized && method != RESET_TYPE_INVISIBLE &&\r\nmethod != RESET_TYPE_DATAPATH)\r\nefx->phy_op->fini(efx);\r\nefx->type->fini(efx);\r\n}\r\nint ef4_reset_up(struct ef4_nic *efx, enum reset_type method, bool ok)\r\n{\r\nint rc;\r\nEF4_ASSERT_RESET_SERIALISED(efx);\r\nrc = efx->type->init(efx);\r\nif (rc) {\r\nnetif_err(efx, drv, efx->net_dev, "failed to initialise NIC\n");\r\ngoto fail;\r\n}\r\nif (!ok)\r\ngoto fail;\r\nif (efx->port_initialized && method != RESET_TYPE_INVISIBLE &&\r\nmethod != RESET_TYPE_DATAPATH) {\r\nrc = efx->phy_op->init(efx);\r\nif (rc)\r\ngoto fail;\r\nrc = efx->phy_op->reconfigure(efx);\r\nif (rc && rc != -EPERM)\r\nnetif_err(efx, drv, efx->net_dev,\r\n"could not restore PHY settings\n");\r\n}\r\nrc = ef4_enable_interrupts(efx);\r\nif (rc)\r\ngoto fail;\r\ndown_read(&efx->filter_sem);\r\nef4_restore_filters(efx);\r\nup_read(&efx->filter_sem);\r\nmutex_unlock(&efx->mac_lock);\r\nef4_start_all(efx);\r\nreturn 0;\r\nfail:\r\nefx->port_initialized = false;\r\nmutex_unlock(&efx->mac_lock);\r\nreturn rc;\r\n}\r\nint ef4_reset(struct ef4_nic *efx, enum reset_type method)\r\n{\r\nint rc, rc2;\r\nbool disabled;\r\nnetif_info(efx, drv, efx->net_dev, "resetting (%s)\n",\r\nRESET_TYPE(method));\r\nef4_device_detach_sync(efx);\r\nef4_reset_down(efx, method);\r\nrc = efx->type->reset(efx, method);\r\nif (rc) {\r\nnetif_err(efx, drv, efx->net_dev, "failed to reset hardware\n");\r\ngoto out;\r\n}\r\nif (method < RESET_TYPE_MAX_METHOD)\r\nefx->reset_pending &= -(1 << (method + 1));\r\nelse\r\n__clear_bit(method, &efx->reset_pending);\r\npci_set_master(efx->pci_dev);\r\nout:\r\ndisabled = rc ||\r\nmethod == RESET_TYPE_DISABLE ||\r\nmethod == RESET_TYPE_RECOVER_OR_DISABLE;\r\nrc2 = ef4_reset_up(efx, method, !disabled);\r\nif (rc2) {\r\ndisabled = true;\r\nif (!rc)\r\nrc = rc2;\r\n}\r\nif (disabled) {\r\ndev_close(efx->net_dev);\r\nnetif_err(efx, drv, efx->net_dev, "has been disabled\n");\r\nefx->state = STATE_DISABLED;\r\n} else {\r\nnetif_dbg(efx, drv, efx->net_dev, "reset complete\n");\r\nnetif_device_attach(efx->net_dev);\r\n}\r\nreturn rc;\r\n}\r\nint ef4_try_recovery(struct ef4_nic *efx)\r\n{\r\n#ifdef CONFIG_EEH\r\nstruct eeh_dev *eehdev = pci_dev_to_eeh_dev(efx->pci_dev);\r\nif (eeh_dev_check_failure(eehdev)) {\r\nreturn 1;\r\n}\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void ef4_reset_work(struct work_struct *data)\r\n{\r\nstruct ef4_nic *efx = container_of(data, struct ef4_nic, reset_work);\r\nunsigned long pending;\r\nenum reset_type method;\r\npending = ACCESS_ONCE(efx->reset_pending);\r\nmethod = fls(pending) - 1;\r\nif ((method == RESET_TYPE_RECOVER_OR_DISABLE ||\r\nmethod == RESET_TYPE_RECOVER_OR_ALL) &&\r\nef4_try_recovery(efx))\r\nreturn;\r\nif (!pending)\r\nreturn;\r\nrtnl_lock();\r\nif (efx->state == STATE_READY)\r\n(void)ef4_reset(efx, method);\r\nrtnl_unlock();\r\n}\r\nvoid ef4_schedule_reset(struct ef4_nic *efx, enum reset_type type)\r\n{\r\nenum reset_type method;\r\nif (efx->state == STATE_RECOVERY) {\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"recovering: skip scheduling %s reset\n",\r\nRESET_TYPE(type));\r\nreturn;\r\n}\r\nswitch (type) {\r\ncase RESET_TYPE_INVISIBLE:\r\ncase RESET_TYPE_ALL:\r\ncase RESET_TYPE_RECOVER_OR_ALL:\r\ncase RESET_TYPE_WORLD:\r\ncase RESET_TYPE_DISABLE:\r\ncase RESET_TYPE_RECOVER_OR_DISABLE:\r\ncase RESET_TYPE_DATAPATH:\r\nmethod = type;\r\nnetif_dbg(efx, drv, efx->net_dev, "scheduling %s reset\n",\r\nRESET_TYPE(method));\r\nbreak;\r\ndefault:\r\nmethod = efx->type->map_reset_reason(type);\r\nnetif_dbg(efx, drv, efx->net_dev,\r\n"scheduling %s reset for %s\n",\r\nRESET_TYPE(method), RESET_TYPE(type));\r\nbreak;\r\n}\r\nset_bit(method, &efx->reset_pending);\r\nsmp_mb();\r\nif (ACCESS_ONCE(efx->state) != STATE_READY)\r\nreturn;\r\nqueue_work(reset_workqueue, &efx->reset_work);\r\n}\r\nint ef4_port_dummy_op_int(struct ef4_nic *efx)\r\n{\r\nreturn 0;\r\n}\r\nvoid ef4_port_dummy_op_void(struct ef4_nic *efx) {}\r\nstatic bool ef4_port_dummy_op_poll(struct ef4_nic *efx)\r\n{\r\nreturn false;\r\n}\r\nstatic int ef4_init_struct(struct ef4_nic *efx,\r\nstruct pci_dev *pci_dev, struct net_device *net_dev)\r\n{\r\nint i;\r\nINIT_LIST_HEAD(&efx->node);\r\nINIT_LIST_HEAD(&efx->secondary_list);\r\nspin_lock_init(&efx->biu_lock);\r\n#ifdef CONFIG_SFC_FALCON_MTD\r\nINIT_LIST_HEAD(&efx->mtd_list);\r\n#endif\r\nINIT_WORK(&efx->reset_work, ef4_reset_work);\r\nINIT_DELAYED_WORK(&efx->monitor_work, ef4_monitor);\r\nINIT_DELAYED_WORK(&efx->selftest_work, ef4_selftest_async_work);\r\nefx->pci_dev = pci_dev;\r\nefx->msg_enable = debug;\r\nefx->state = STATE_UNINIT;\r\nstrlcpy(efx->name, pci_name(pci_dev), sizeof(efx->name));\r\nefx->net_dev = net_dev;\r\nefx->rx_prefix_size = efx->type->rx_prefix_size;\r\nefx->rx_ip_align =\r\nNET_IP_ALIGN ? (efx->rx_prefix_size + NET_IP_ALIGN) % 4 : 0;\r\nefx->rx_packet_hash_offset =\r\nefx->type->rx_hash_offset - efx->type->rx_prefix_size;\r\nefx->rx_packet_ts_offset =\r\nefx->type->rx_ts_offset - efx->type->rx_prefix_size;\r\nspin_lock_init(&efx->stats_lock);\r\nmutex_init(&efx->mac_lock);\r\nefx->phy_op = &ef4_dummy_phy_operations;\r\nefx->mdio.dev = net_dev;\r\nINIT_WORK(&efx->mac_work, ef4_mac_work);\r\ninit_waitqueue_head(&efx->flush_wq);\r\nfor (i = 0; i < EF4_MAX_CHANNELS; i++) {\r\nefx->channel[i] = ef4_alloc_channel(efx, i, NULL);\r\nif (!efx->channel[i])\r\ngoto fail;\r\nefx->msi_context[i].efx = efx;\r\nefx->msi_context[i].index = i;\r\n}\r\nefx->interrupt_mode = max(efx->type->max_interrupt_mode,\r\ninterrupt_mode);\r\nsnprintf(efx->workqueue_name, sizeof(efx->workqueue_name), "sfc%s",\r\npci_name(pci_dev));\r\nefx->workqueue = create_singlethread_workqueue(efx->workqueue_name);\r\nif (!efx->workqueue)\r\ngoto fail;\r\nreturn 0;\r\nfail:\r\nef4_fini_struct(efx);\r\nreturn -ENOMEM;\r\n}\r\nstatic void ef4_fini_struct(struct ef4_nic *efx)\r\n{\r\nint i;\r\nfor (i = 0; i < EF4_MAX_CHANNELS; i++)\r\nkfree(efx->channel[i]);\r\nkfree(efx->vpd_sn);\r\nif (efx->workqueue) {\r\ndestroy_workqueue(efx->workqueue);\r\nefx->workqueue = NULL;\r\n}\r\n}\r\nvoid ef4_update_sw_stats(struct ef4_nic *efx, u64 *stats)\r\n{\r\nu64 n_rx_nodesc_trunc = 0;\r\nstruct ef4_channel *channel;\r\nef4_for_each_channel(channel, efx)\r\nn_rx_nodesc_trunc += channel->n_rx_nodesc_trunc;\r\nstats[GENERIC_STAT_rx_nodesc_trunc] = n_rx_nodesc_trunc;\r\nstats[GENERIC_STAT_rx_noskb_drops] = atomic_read(&efx->n_rx_noskb_drops);\r\n}\r\nstatic void ef4_pci_remove_main(struct ef4_nic *efx)\r\n{\r\nBUG_ON(efx->state == STATE_READY);\r\ncancel_work_sync(&efx->reset_work);\r\nef4_disable_interrupts(efx);\r\nef4_nic_fini_interrupt(efx);\r\nef4_fini_port(efx);\r\nefx->type->fini(efx);\r\nef4_fini_napi(efx);\r\nef4_remove_all(efx);\r\n}\r\nstatic void ef4_pci_remove(struct pci_dev *pci_dev)\r\n{\r\nstruct ef4_nic *efx;\r\nefx = pci_get_drvdata(pci_dev);\r\nif (!efx)\r\nreturn;\r\nrtnl_lock();\r\nef4_dissociate(efx);\r\ndev_close(efx->net_dev);\r\nef4_disable_interrupts(efx);\r\nefx->state = STATE_UNINIT;\r\nrtnl_unlock();\r\nef4_unregister_netdev(efx);\r\nef4_mtd_remove(efx);\r\nef4_pci_remove_main(efx);\r\nef4_fini_io(efx);\r\nnetif_dbg(efx, drv, efx->net_dev, "shutdown successful\n");\r\nef4_fini_struct(efx);\r\nfree_netdev(efx->net_dev);\r\npci_disable_pcie_error_reporting(pci_dev);\r\n}\r\nstatic void ef4_probe_vpd_strings(struct ef4_nic *efx)\r\n{\r\nstruct pci_dev *dev = efx->pci_dev;\r\nchar vpd_data[SFC_VPD_LEN];\r\nssize_t vpd_size;\r\nint ro_start, ro_size, i, j;\r\nvpd_size = pci_read_vpd(dev, 0, sizeof(vpd_data), vpd_data);\r\nif (vpd_size <= 0) {\r\nnetif_err(efx, drv, efx->net_dev, "Unable to read VPD\n");\r\nreturn;\r\n}\r\nro_start = pci_vpd_find_tag(vpd_data, 0, vpd_size, PCI_VPD_LRDT_RO_DATA);\r\nif (ro_start < 0) {\r\nnetif_err(efx, drv, efx->net_dev, "VPD Read-only not found\n");\r\nreturn;\r\n}\r\nro_size = pci_vpd_lrdt_size(&vpd_data[ro_start]);\r\nj = ro_size;\r\ni = ro_start + PCI_VPD_LRDT_TAG_SIZE;\r\nif (i + j > vpd_size)\r\nj = vpd_size - i;\r\ni = pci_vpd_find_info_keyword(vpd_data, i, j, "PN");\r\nif (i < 0) {\r\nnetif_err(efx, drv, efx->net_dev, "Part number not found\n");\r\nreturn;\r\n}\r\nj = pci_vpd_info_field_size(&vpd_data[i]);\r\ni += PCI_VPD_INFO_FLD_HDR_SIZE;\r\nif (i + j > vpd_size) {\r\nnetif_err(efx, drv, efx->net_dev, "Incomplete part number\n");\r\nreturn;\r\n}\r\nnetif_info(efx, drv, efx->net_dev,\r\n"Part Number : %.*s\n", j, &vpd_data[i]);\r\ni = ro_start + PCI_VPD_LRDT_TAG_SIZE;\r\nj = ro_size;\r\ni = pci_vpd_find_info_keyword(vpd_data, i, j, "SN");\r\nif (i < 0) {\r\nnetif_err(efx, drv, efx->net_dev, "Serial number not found\n");\r\nreturn;\r\n}\r\nj = pci_vpd_info_field_size(&vpd_data[i]);\r\ni += PCI_VPD_INFO_FLD_HDR_SIZE;\r\nif (i + j > vpd_size) {\r\nnetif_err(efx, drv, efx->net_dev, "Incomplete serial number\n");\r\nreturn;\r\n}\r\nefx->vpd_sn = kmalloc(j + 1, GFP_KERNEL);\r\nif (!efx->vpd_sn)\r\nreturn;\r\nsnprintf(efx->vpd_sn, j + 1, "%s", &vpd_data[i]);\r\n}\r\nstatic int ef4_pci_probe_main(struct ef4_nic *efx)\r\n{\r\nint rc;\r\nrc = ef4_probe_all(efx);\r\nif (rc)\r\ngoto fail1;\r\nef4_init_napi(efx);\r\nrc = efx->type->init(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to initialise NIC\n");\r\ngoto fail3;\r\n}\r\nrc = ef4_init_port(efx);\r\nif (rc) {\r\nnetif_err(efx, probe, efx->net_dev,\r\n"failed to initialise port\n");\r\ngoto fail4;\r\n}\r\nrc = ef4_nic_init_interrupt(efx);\r\nif (rc)\r\ngoto fail5;\r\nrc = ef4_enable_interrupts(efx);\r\nif (rc)\r\ngoto fail6;\r\nreturn 0;\r\nfail6:\r\nef4_nic_fini_interrupt(efx);\r\nfail5:\r\nef4_fini_port(efx);\r\nfail4:\r\nefx->type->fini(efx);\r\nfail3:\r\nef4_fini_napi(efx);\r\nef4_remove_all(efx);\r\nfail1:\r\nreturn rc;\r\n}\r\nstatic int ef4_pci_probe(struct pci_dev *pci_dev,\r\nconst struct pci_device_id *entry)\r\n{\r\nstruct net_device *net_dev;\r\nstruct ef4_nic *efx;\r\nint rc;\r\nnet_dev = alloc_etherdev_mqs(sizeof(*efx), EF4_MAX_CORE_TX_QUEUES,\r\nEF4_MAX_RX_QUEUES);\r\nif (!net_dev)\r\nreturn -ENOMEM;\r\nefx = netdev_priv(net_dev);\r\nefx->type = (const struct ef4_nic_type *) entry->driver_data;\r\nefx->fixed_features |= NETIF_F_HIGHDMA;\r\npci_set_drvdata(pci_dev, efx);\r\nSET_NETDEV_DEV(net_dev, &pci_dev->dev);\r\nrc = ef4_init_struct(efx, pci_dev, net_dev);\r\nif (rc)\r\ngoto fail1;\r\nnetif_info(efx, probe, efx->net_dev,\r\n"Solarflare NIC detected\n");\r\nef4_probe_vpd_strings(efx);\r\nrc = ef4_init_io(efx);\r\nif (rc)\r\ngoto fail2;\r\nrc = ef4_pci_probe_main(efx);\r\nif (rc)\r\ngoto fail3;\r\nnet_dev->features |= (efx->type->offload_features | NETIF_F_SG |\r\nNETIF_F_RXCSUM);\r\nnet_dev->vlan_features |= (NETIF_F_HW_CSUM | NETIF_F_SG |\r\nNETIF_F_HIGHDMA | NETIF_F_RXCSUM);\r\nnet_dev->hw_features = net_dev->features & ~efx->fixed_features;\r\nnet_dev->features &= ~NETIF_F_HW_VLAN_CTAG_FILTER;\r\nnet_dev->features |= efx->fixed_features;\r\nrc = ef4_register_netdev(efx);\r\nif (rc)\r\ngoto fail4;\r\nnetif_dbg(efx, probe, efx->net_dev, "initialisation successful\n");\r\nrtnl_lock();\r\nrc = ef4_mtd_probe(efx);\r\nrtnl_unlock();\r\nif (rc && rc != -EPERM)\r\nnetif_warn(efx, probe, efx->net_dev,\r\n"failed to create MTDs (%d)\n", rc);\r\nrc = pci_enable_pcie_error_reporting(pci_dev);\r\nif (rc && rc != -EINVAL)\r\nnetif_notice(efx, probe, efx->net_dev,\r\n"PCIE error reporting unavailable (%d).\n",\r\nrc);\r\nreturn 0;\r\nfail4:\r\nef4_pci_remove_main(efx);\r\nfail3:\r\nef4_fini_io(efx);\r\nfail2:\r\nef4_fini_struct(efx);\r\nfail1:\r\nWARN_ON(rc > 0);\r\nnetif_dbg(efx, drv, efx->net_dev, "initialisation failed. rc=%d\n", rc);\r\nfree_netdev(net_dev);\r\nreturn rc;\r\n}\r\nstatic int ef4_pm_freeze(struct device *dev)\r\n{\r\nstruct ef4_nic *efx = pci_get_drvdata(to_pci_dev(dev));\r\nrtnl_lock();\r\nif (efx->state != STATE_DISABLED) {\r\nefx->state = STATE_UNINIT;\r\nef4_device_detach_sync(efx);\r\nef4_stop_all(efx);\r\nef4_disable_interrupts(efx);\r\n}\r\nrtnl_unlock();\r\nreturn 0;\r\n}\r\nstatic int ef4_pm_thaw(struct device *dev)\r\n{\r\nint rc;\r\nstruct ef4_nic *efx = pci_get_drvdata(to_pci_dev(dev));\r\nrtnl_lock();\r\nif (efx->state != STATE_DISABLED) {\r\nrc = ef4_enable_interrupts(efx);\r\nif (rc)\r\ngoto fail;\r\nmutex_lock(&efx->mac_lock);\r\nefx->phy_op->reconfigure(efx);\r\nmutex_unlock(&efx->mac_lock);\r\nef4_start_all(efx);\r\nnetif_device_attach(efx->net_dev);\r\nefx->state = STATE_READY;\r\nefx->type->resume_wol(efx);\r\n}\r\nrtnl_unlock();\r\nqueue_work(reset_workqueue, &efx->reset_work);\r\nreturn 0;\r\nfail:\r\nrtnl_unlock();\r\nreturn rc;\r\n}\r\nstatic int ef4_pm_poweroff(struct device *dev)\r\n{\r\nstruct pci_dev *pci_dev = to_pci_dev(dev);\r\nstruct ef4_nic *efx = pci_get_drvdata(pci_dev);\r\nefx->type->fini(efx);\r\nefx->reset_pending = 0;\r\npci_save_state(pci_dev);\r\nreturn pci_set_power_state(pci_dev, PCI_D3hot);\r\n}\r\nstatic int ef4_pm_resume(struct device *dev)\r\n{\r\nstruct pci_dev *pci_dev = to_pci_dev(dev);\r\nstruct ef4_nic *efx = pci_get_drvdata(pci_dev);\r\nint rc;\r\nrc = pci_set_power_state(pci_dev, PCI_D0);\r\nif (rc)\r\nreturn rc;\r\npci_restore_state(pci_dev);\r\nrc = pci_enable_device(pci_dev);\r\nif (rc)\r\nreturn rc;\r\npci_set_master(efx->pci_dev);\r\nrc = efx->type->reset(efx, RESET_TYPE_ALL);\r\nif (rc)\r\nreturn rc;\r\nrc = efx->type->init(efx);\r\nif (rc)\r\nreturn rc;\r\nrc = ef4_pm_thaw(dev);\r\nreturn rc;\r\n}\r\nstatic int ef4_pm_suspend(struct device *dev)\r\n{\r\nint rc;\r\nef4_pm_freeze(dev);\r\nrc = ef4_pm_poweroff(dev);\r\nif (rc)\r\nef4_pm_resume(dev);\r\nreturn rc;\r\n}\r\nstatic pci_ers_result_t ef4_io_error_detected(struct pci_dev *pdev,\r\nenum pci_channel_state state)\r\n{\r\npci_ers_result_t status = PCI_ERS_RESULT_RECOVERED;\r\nstruct ef4_nic *efx = pci_get_drvdata(pdev);\r\nif (state == pci_channel_io_perm_failure)\r\nreturn PCI_ERS_RESULT_DISCONNECT;\r\nrtnl_lock();\r\nif (efx->state != STATE_DISABLED) {\r\nefx->state = STATE_RECOVERY;\r\nefx->reset_pending = 0;\r\nef4_device_detach_sync(efx);\r\nef4_stop_all(efx);\r\nef4_disable_interrupts(efx);\r\nstatus = PCI_ERS_RESULT_NEED_RESET;\r\n} else {\r\nstatus = PCI_ERS_RESULT_RECOVERED;\r\n}\r\nrtnl_unlock();\r\npci_disable_device(pdev);\r\nreturn status;\r\n}\r\nstatic pci_ers_result_t ef4_io_slot_reset(struct pci_dev *pdev)\r\n{\r\nstruct ef4_nic *efx = pci_get_drvdata(pdev);\r\npci_ers_result_t status = PCI_ERS_RESULT_RECOVERED;\r\nint rc;\r\nif (pci_enable_device(pdev)) {\r\nnetif_err(efx, hw, efx->net_dev,\r\n"Cannot re-enable PCI device after reset.\n");\r\nstatus = PCI_ERS_RESULT_DISCONNECT;\r\n}\r\nrc = pci_cleanup_aer_uncorrect_error_status(pdev);\r\nif (rc) {\r\nnetif_err(efx, hw, efx->net_dev,\r\n"pci_cleanup_aer_uncorrect_error_status failed (%d)\n", rc);\r\n}\r\nreturn status;\r\n}\r\nstatic void ef4_io_resume(struct pci_dev *pdev)\r\n{\r\nstruct ef4_nic *efx = pci_get_drvdata(pdev);\r\nint rc;\r\nrtnl_lock();\r\nif (efx->state == STATE_DISABLED)\r\ngoto out;\r\nrc = ef4_reset(efx, RESET_TYPE_ALL);\r\nif (rc) {\r\nnetif_err(efx, hw, efx->net_dev,\r\n"ef4_reset failed after PCI error (%d)\n", rc);\r\n} else {\r\nefx->state = STATE_READY;\r\nnetif_dbg(efx, hw, efx->net_dev,\r\n"Done resetting and resuming IO after PCI error.\n");\r\n}\r\nout:\r\nrtnl_unlock();\r\n}\r\nstatic int __init ef4_init_module(void)\r\n{\r\nint rc;\r\nprintk(KERN_INFO "Solarflare Falcon driver v" EF4_DRIVER_VERSION "\n");\r\nrc = register_netdevice_notifier(&ef4_netdev_notifier);\r\nif (rc)\r\ngoto err_notifier;\r\nreset_workqueue = create_singlethread_workqueue("sfc_reset");\r\nif (!reset_workqueue) {\r\nrc = -ENOMEM;\r\ngoto err_reset;\r\n}\r\nrc = pci_register_driver(&ef4_pci_driver);\r\nif (rc < 0)\r\ngoto err_pci;\r\nreturn 0;\r\nerr_pci:\r\ndestroy_workqueue(reset_workqueue);\r\nerr_reset:\r\nunregister_netdevice_notifier(&ef4_netdev_notifier);\r\nerr_notifier:\r\nreturn rc;\r\n}\r\nstatic void __exit ef4_exit_module(void)\r\n{\r\nprintk(KERN_INFO "Solarflare Falcon driver unloading\n");\r\npci_unregister_driver(&ef4_pci_driver);\r\ndestroy_workqueue(reset_workqueue);\r\nunregister_netdevice_notifier(&ef4_netdev_notifier);\r\n}
