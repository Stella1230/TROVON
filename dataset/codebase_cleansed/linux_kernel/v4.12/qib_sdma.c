static void sdma_get(struct qib_sdma_state *ss)\r\n{\r\nkref_get(&ss->kref);\r\n}\r\nstatic void sdma_complete(struct kref *kref)\r\n{\r\nstruct qib_sdma_state *ss =\r\ncontainer_of(kref, struct qib_sdma_state, kref);\r\ncomplete(&ss->comp);\r\n}\r\nstatic void sdma_put(struct qib_sdma_state *ss)\r\n{\r\nkref_put(&ss->kref, sdma_complete);\r\n}\r\nstatic void sdma_finalput(struct qib_sdma_state *ss)\r\n{\r\nsdma_put(ss);\r\nwait_for_completion(&ss->comp);\r\n}\r\nstatic void clear_sdma_activelist(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_sdma_txreq *txp, *txp_next;\r\nlist_for_each_entry_safe(txp, txp_next, &ppd->sdma_activelist, list) {\r\nlist_del_init(&txp->list);\r\nif (txp->flags & QIB_SDMA_TXREQ_F_FREEDESC) {\r\nunsigned idx;\r\nidx = txp->start_idx;\r\nwhile (idx != txp->next_descq_idx) {\r\nunmap_desc(ppd, idx);\r\nif (++idx == ppd->sdma_descq_cnt)\r\nidx = 0;\r\n}\r\n}\r\nif (txp->callback)\r\n(*txp->callback)(txp, QIB_SDMA_TXREQ_S_ABORTED);\r\n}\r\n}\r\nstatic void sdma_sw_clean_up_task(unsigned long opaque)\r\n{\r\nstruct qib_pportdata *ppd = (struct qib_pportdata *) opaque;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\nqib_sdma_make_progress(ppd);\r\nclear_sdma_activelist(ppd);\r\nppd->sdma_descq_removed = ppd->sdma_descq_added;\r\nppd->sdma_descq_tail = 0;\r\nppd->sdma_descq_head = 0;\r\nppd->sdma_head_dma[0] = 0;\r\nppd->sdma_generation = 0;\r\n__qib_sdma_process_event(ppd, qib_sdma_event_e40_sw_cleaned);\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\n}\r\nstatic void sdma_hw_start_up(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_sdma_state *ss = &ppd->sdma_state;\r\nunsigned bufno;\r\nfor (bufno = ss->first_sendbuf; bufno < ss->last_sendbuf; ++bufno)\r\nppd->dd->f_sendctrl(ppd, QIB_SENDCTRL_DISARM_BUF(bufno));\r\nppd->dd->f_sdma_hw_start_up(ppd);\r\n}\r\nstatic void sdma_sw_tear_down(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_sdma_state *ss = &ppd->sdma_state;\r\nsdma_put(ss);\r\n}\r\nstatic void sdma_start_sw_clean_up(struct qib_pportdata *ppd)\r\n{\r\ntasklet_hi_schedule(&ppd->sdma_sw_clean_up_task);\r\n}\r\nstatic void sdma_set_state(struct qib_pportdata *ppd,\r\nenum qib_sdma_states next_state)\r\n{\r\nstruct qib_sdma_state *ss = &ppd->sdma_state;\r\nstruct sdma_set_state_action *action = ss->set_state_action;\r\nunsigned op = 0;\r\nss->previous_state = ss->current_state;\r\nss->previous_op = ss->current_op;\r\nss->current_state = next_state;\r\nif (action[next_state].op_enable)\r\nop |= QIB_SDMA_SENDCTRL_OP_ENABLE;\r\nif (action[next_state].op_intenable)\r\nop |= QIB_SDMA_SENDCTRL_OP_INTENABLE;\r\nif (action[next_state].op_halt)\r\nop |= QIB_SDMA_SENDCTRL_OP_HALT;\r\nif (action[next_state].op_drain)\r\nop |= QIB_SDMA_SENDCTRL_OP_DRAIN;\r\nif (action[next_state].go_s99_running_tofalse)\r\nss->go_s99_running = 0;\r\nif (action[next_state].go_s99_running_totrue)\r\nss->go_s99_running = 1;\r\nss->current_op = op;\r\nppd->dd->f_sdma_sendctrl(ppd, ss->current_op);\r\n}\r\nstatic void unmap_desc(struct qib_pportdata *ppd, unsigned head)\r\n{\r\n__le64 *descqp = &ppd->sdma_descq[head].qw[0];\r\nu64 desc[2];\r\ndma_addr_t addr;\r\nsize_t len;\r\ndesc[0] = le64_to_cpu(descqp[0]);\r\ndesc[1] = le64_to_cpu(descqp[1]);\r\naddr = (desc[1] << 32) | (desc[0] >> 32);\r\nlen = (desc[0] >> 14) & (0x7ffULL << 2);\r\ndma_unmap_single(&ppd->dd->pcidev->dev, addr, len, DMA_TO_DEVICE);\r\n}\r\nstatic int alloc_sdma(struct qib_pportdata *ppd)\r\n{\r\nppd->sdma_descq_cnt = sdma_descq_cnt;\r\nif (!ppd->sdma_descq_cnt)\r\nppd->sdma_descq_cnt = 256;\r\nppd->sdma_descq = dma_alloc_coherent(&ppd->dd->pcidev->dev,\r\nppd->sdma_descq_cnt * sizeof(u64[2]), &ppd->sdma_descq_phys,\r\nGFP_KERNEL);\r\nif (!ppd->sdma_descq) {\r\nqib_dev_err(ppd->dd,\r\n"failed to allocate SendDMA descriptor FIFO memory\n");\r\ngoto bail;\r\n}\r\nppd->sdma_head_dma = dma_alloc_coherent(&ppd->dd->pcidev->dev,\r\nPAGE_SIZE, &ppd->sdma_head_phys, GFP_KERNEL);\r\nif (!ppd->sdma_head_dma) {\r\nqib_dev_err(ppd->dd,\r\n"failed to allocate SendDMA head memory\n");\r\ngoto cleanup_descq;\r\n}\r\nppd->sdma_head_dma[0] = 0;\r\nreturn 0;\r\ncleanup_descq:\r\ndma_free_coherent(&ppd->dd->pcidev->dev,\r\nppd->sdma_descq_cnt * sizeof(u64[2]), (void *)ppd->sdma_descq,\r\nppd->sdma_descq_phys);\r\nppd->sdma_descq = NULL;\r\nppd->sdma_descq_phys = 0;\r\nbail:\r\nppd->sdma_descq_cnt = 0;\r\nreturn -ENOMEM;\r\n}\r\nstatic void free_sdma(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_devdata *dd = ppd->dd;\r\nif (ppd->sdma_head_dma) {\r\ndma_free_coherent(&dd->pcidev->dev, PAGE_SIZE,\r\n(void *)ppd->sdma_head_dma,\r\nppd->sdma_head_phys);\r\nppd->sdma_head_dma = NULL;\r\nppd->sdma_head_phys = 0;\r\n}\r\nif (ppd->sdma_descq) {\r\ndma_free_coherent(&dd->pcidev->dev,\r\nppd->sdma_descq_cnt * sizeof(u64[2]),\r\nppd->sdma_descq, ppd->sdma_descq_phys);\r\nppd->sdma_descq = NULL;\r\nppd->sdma_descq_phys = 0;\r\n}\r\n}\r\nstatic inline void make_sdma_desc(struct qib_pportdata *ppd,\r\nu64 *sdmadesc, u64 addr, u64 dwlen,\r\nu64 dwoffset)\r\n{\r\nWARN_ON(addr & 3);\r\nsdmadesc[1] = addr >> 32;\r\nsdmadesc[0] = (addr & 0xfffffffcULL) << 32;\r\nsdmadesc[0] |= (ppd->sdma_generation & 3ULL) <<\r\nSDMA_DESC_GEN_LSB;\r\nsdmadesc[0] |= (dwlen & 0x7ffULL) << SDMA_DESC_COUNT_LSB;\r\nsdmadesc[0] |= dwoffset & 0x7ffULL;\r\n}\r\nint qib_sdma_make_progress(struct qib_pportdata *ppd)\r\n{\r\nstruct list_head *lp = NULL;\r\nstruct qib_sdma_txreq *txp = NULL;\r\nstruct qib_devdata *dd = ppd->dd;\r\nint progress = 0;\r\nu16 hwhead;\r\nu16 idx = 0;\r\nhwhead = dd->f_sdma_gethead(ppd);\r\nif (!list_empty(&ppd->sdma_activelist)) {\r\nlp = ppd->sdma_activelist.next;\r\ntxp = list_entry(lp, struct qib_sdma_txreq, list);\r\nidx = txp->start_idx;\r\n}\r\nwhile (ppd->sdma_descq_head != hwhead) {\r\nif (txp && (txp->flags & QIB_SDMA_TXREQ_F_FREEDESC) &&\r\n(idx == ppd->sdma_descq_head)) {\r\nunmap_desc(ppd, ppd->sdma_descq_head);\r\nif (++idx == ppd->sdma_descq_cnt)\r\nidx = 0;\r\n}\r\nppd->sdma_descq_removed++;\r\nif (++ppd->sdma_descq_head == ppd->sdma_descq_cnt)\r\nppd->sdma_descq_head = 0;\r\nif (txp && txp->next_descq_idx == ppd->sdma_descq_head) {\r\nlist_del_init(&txp->list);\r\nif (txp->callback)\r\n(*txp->callback)(txp, QIB_SDMA_TXREQ_S_OK);\r\nif (list_empty(&ppd->sdma_activelist))\r\ntxp = NULL;\r\nelse {\r\nlp = ppd->sdma_activelist.next;\r\ntxp = list_entry(lp, struct qib_sdma_txreq,\r\nlist);\r\nidx = txp->start_idx;\r\n}\r\n}\r\nprogress = 1;\r\n}\r\nif (progress)\r\nqib_verbs_sdma_desc_avail(ppd, qib_sdma_descq_freecnt(ppd));\r\nreturn progress;\r\n}\r\nvoid qib_sdma_intr(struct qib_pportdata *ppd)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\n__qib_sdma_intr(ppd);\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\n}\r\nvoid __qib_sdma_intr(struct qib_pportdata *ppd)\r\n{\r\nif (__qib_sdma_running(ppd)) {\r\nqib_sdma_make_progress(ppd);\r\nif (!list_empty(&ppd->sdma_userpending))\r\nqib_user_sdma_send_desc(ppd, &ppd->sdma_userpending);\r\n}\r\n}\r\nint qib_setup_sdma(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_devdata *dd = ppd->dd;\r\nunsigned long flags;\r\nint ret = 0;\r\nret = alloc_sdma(ppd);\r\nif (ret)\r\ngoto bail;\r\nppd->dd->f_sdma_init_early(ppd);\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\nsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\nkref_init(&ppd->sdma_state.kref);\r\ninit_completion(&ppd->sdma_state.comp);\r\nppd->sdma_generation = 0;\r\nppd->sdma_descq_head = 0;\r\nppd->sdma_descq_removed = 0;\r\nppd->sdma_descq_added = 0;\r\nppd->sdma_intrequest = 0;\r\nINIT_LIST_HEAD(&ppd->sdma_userpending);\r\nINIT_LIST_HEAD(&ppd->sdma_activelist);\r\ntasklet_init(&ppd->sdma_sw_clean_up_task, sdma_sw_clean_up_task,\r\n(unsigned long)ppd);\r\nret = dd->f_init_sdma_regs(ppd);\r\nif (ret)\r\ngoto bail_alloc;\r\nqib_sdma_process_event(ppd, qib_sdma_event_e10_go_hw_start);\r\nreturn 0;\r\nbail_alloc:\r\nqib_teardown_sdma(ppd);\r\nbail:\r\nreturn ret;\r\n}\r\nvoid qib_teardown_sdma(struct qib_pportdata *ppd)\r\n{\r\nqib_sdma_process_event(ppd, qib_sdma_event_e00_go_hw_down);\r\nsdma_finalput(&ppd->sdma_state);\r\nfree_sdma(ppd);\r\n}\r\nint qib_sdma_running(struct qib_pportdata *ppd)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\nret = __qib_sdma_running(ppd);\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void complete_sdma_err_req(struct qib_pportdata *ppd,\r\nstruct qib_verbs_txreq *tx)\r\n{\r\nstruct qib_qp_priv *priv = tx->qp->priv;\r\natomic_inc(&priv->s_dma_busy);\r\ntx->txreq.start_idx = 0;\r\ntx->txreq.next_descq_idx = 0;\r\nlist_add_tail(&tx->txreq.list, &ppd->sdma_activelist);\r\nclear_sdma_activelist(ppd);\r\n}\r\nint qib_sdma_verbs_send(struct qib_pportdata *ppd,\r\nstruct rvt_sge_state *ss, u32 dwords,\r\nstruct qib_verbs_txreq *tx)\r\n{\r\nunsigned long flags;\r\nstruct rvt_sge *sge;\r\nstruct rvt_qp *qp;\r\nint ret = 0;\r\nu16 tail;\r\n__le64 *descqp;\r\nu64 sdmadesc[2];\r\nu32 dwoffset;\r\ndma_addr_t addr;\r\nstruct qib_qp_priv *priv;\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\nretry:\r\nif (unlikely(!__qib_sdma_running(ppd))) {\r\ncomplete_sdma_err_req(ppd, tx);\r\ngoto unlock;\r\n}\r\nif (tx->txreq.sg_count > qib_sdma_descq_freecnt(ppd)) {\r\nif (qib_sdma_make_progress(ppd))\r\ngoto retry;\r\nif (ppd->dd->flags & QIB_HAS_SDMA_TIMEOUT)\r\nppd->dd->f_sdma_set_desc_cnt(ppd,\r\nppd->sdma_descq_cnt / 2);\r\ngoto busy;\r\n}\r\ndwoffset = tx->hdr_dwords;\r\nmake_sdma_desc(ppd, sdmadesc, (u64) tx->txreq.addr, dwoffset, 0);\r\nsdmadesc[0] |= SDMA_DESC_FIRST;\r\nif (tx->txreq.flags & QIB_SDMA_TXREQ_F_USELARGEBUF)\r\nsdmadesc[0] |= SDMA_DESC_USE_LARGE_BUF;\r\ntail = ppd->sdma_descq_tail;\r\ndescqp = &ppd->sdma_descq[tail].qw[0];\r\n*descqp++ = cpu_to_le64(sdmadesc[0]);\r\n*descqp++ = cpu_to_le64(sdmadesc[1]);\r\nif (++tail == ppd->sdma_descq_cnt) {\r\ntail = 0;\r\ndescqp = &ppd->sdma_descq[0].qw[0];\r\n++ppd->sdma_generation;\r\n}\r\ntx->txreq.start_idx = tail;\r\nsge = &ss->sge;\r\nwhile (dwords) {\r\nu32 dw;\r\nu32 len;\r\nlen = dwords << 2;\r\nif (len > sge->length)\r\nlen = sge->length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\ndw = (len + 3) >> 2;\r\naddr = dma_map_single(&ppd->dd->pcidev->dev, sge->vaddr,\r\ndw << 2, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&ppd->dd->pcidev->dev, addr))\r\ngoto unmap;\r\nsdmadesc[0] = 0;\r\nmake_sdma_desc(ppd, sdmadesc, (u64) addr, dw, dwoffset);\r\nif (tx->txreq.flags & QIB_SDMA_TXREQ_F_USELARGEBUF)\r\nsdmadesc[0] |= SDMA_DESC_USE_LARGE_BUF;\r\n*descqp++ = cpu_to_le64(sdmadesc[0]);\r\n*descqp++ = cpu_to_le64(sdmadesc[1]);\r\nif (++tail == ppd->sdma_descq_cnt) {\r\ntail = 0;\r\ndescqp = &ppd->sdma_descq[0].qw[0];\r\n++ppd->sdma_generation;\r\n}\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= RVT_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\ndwoffset += dw;\r\ndwords -= dw;\r\n}\r\nif (!tail)\r\ndescqp = &ppd->sdma_descq[ppd->sdma_descq_cnt].qw[0];\r\ndescqp -= 2;\r\ndescqp[0] |= cpu_to_le64(SDMA_DESC_LAST);\r\nif (tx->txreq.flags & QIB_SDMA_TXREQ_F_HEADTOHOST)\r\ndescqp[0] |= cpu_to_le64(SDMA_DESC_DMA_HEAD);\r\nif (tx->txreq.flags & QIB_SDMA_TXREQ_F_INTREQ)\r\ndescqp[0] |= cpu_to_le64(SDMA_DESC_INTR);\r\npriv = tx->qp->priv;\r\natomic_inc(&priv->s_dma_busy);\r\ntx->txreq.next_descq_idx = tail;\r\nppd->dd->f_sdma_update_tail(ppd, tail);\r\nppd->sdma_descq_added += tx->txreq.sg_count;\r\nlist_add_tail(&tx->txreq.list, &ppd->sdma_activelist);\r\ngoto unlock;\r\nunmap:\r\nfor (;;) {\r\nif (!tail)\r\ntail = ppd->sdma_descq_cnt - 1;\r\nelse\r\ntail--;\r\nif (tail == ppd->sdma_descq_tail)\r\nbreak;\r\nunmap_desc(ppd, tail);\r\n}\r\nqp = tx->qp;\r\npriv = qp->priv;\r\nqib_put_txreq(tx);\r\nspin_lock(&qp->r_lock);\r\nspin_lock(&qp->s_lock);\r\nif (qp->ibqp.qp_type == IB_QPT_RC) {\r\nif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK)\r\nrvt_error_qp(qp, IB_WC_GENERAL_ERR);\r\n} else if (qp->s_wqe)\r\nqib_send_complete(qp, qp->s_wqe, IB_WC_GENERAL_ERR);\r\nspin_unlock(&qp->s_lock);\r\nspin_unlock(&qp->r_lock);\r\ngoto unlock;\r\nbusy:\r\nqp = tx->qp;\r\npriv = qp->priv;\r\nspin_lock(&qp->s_lock);\r\nif (ib_rvt_state_ops[qp->state] & RVT_PROCESS_RECV_OK) {\r\nstruct qib_ibdev *dev;\r\ntx->ss = ss;\r\ntx->dwords = dwords;\r\npriv->s_tx = tx;\r\ndev = &ppd->dd->verbs_dev;\r\nspin_lock(&dev->rdi.pending_lock);\r\nif (list_empty(&priv->iowait)) {\r\nstruct qib_ibport *ibp;\r\nibp = &ppd->ibport_data;\r\nibp->rvp.n_dmawait++;\r\nqp->s_flags |= RVT_S_WAIT_DMA_DESC;\r\nlist_add_tail(&priv->iowait, &dev->dmawait);\r\n}\r\nspin_unlock(&dev->rdi.pending_lock);\r\nqp->s_flags &= ~RVT_S_BUSY;\r\nspin_unlock(&qp->s_lock);\r\nret = -EBUSY;\r\n} else {\r\nspin_unlock(&qp->s_lock);\r\nqib_put_txreq(tx);\r\n}\r\nunlock:\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\nreturn ret;\r\n}\r\nvoid dump_sdma_state(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_sdma_desc *descq;\r\nstruct qib_sdma_txreq *txp, *txpnext;\r\n__le64 *descqp;\r\nu64 desc[2];\r\nu64 addr;\r\nu16 gen, dwlen, dwoffset;\r\nu16 head, tail, cnt;\r\nhead = ppd->sdma_descq_head;\r\ntail = ppd->sdma_descq_tail;\r\ncnt = qib_sdma_descq_freecnt(ppd);\r\ndescq = ppd->sdma_descq;\r\nqib_dev_porterr(ppd->dd, ppd->port,\r\n"SDMA ppd->sdma_descq_head: %u\n", head);\r\nqib_dev_porterr(ppd->dd, ppd->port,\r\n"SDMA ppd->sdma_descq_tail: %u\n", tail);\r\nqib_dev_porterr(ppd->dd, ppd->port,\r\n"SDMA sdma_descq_freecnt: %u\n", cnt);\r\nwhile (head != tail) {\r\nchar flags[6] = { 'x', 'x', 'x', 'x', 'x', 0 };\r\ndescqp = &descq[head].qw[0];\r\ndesc[0] = le64_to_cpu(descqp[0]);\r\ndesc[1] = le64_to_cpu(descqp[1]);\r\nflags[0] = (desc[0] & 1<<15) ? 'I' : '-';\r\nflags[1] = (desc[0] & 1<<14) ? 'L' : 'S';\r\nflags[2] = (desc[0] & 1<<13) ? 'H' : '-';\r\nflags[3] = (desc[0] & 1<<12) ? 'F' : '-';\r\nflags[4] = (desc[0] & 1<<11) ? 'L' : '-';\r\naddr = (desc[1] << 32) | ((desc[0] >> 32) & 0xfffffffcULL);\r\ngen = (desc[0] >> 30) & 3ULL;\r\ndwlen = (desc[0] >> 14) & (0x7ffULL << 2);\r\ndwoffset = (desc[0] & 0x7ffULL) << 2;\r\nqib_dev_porterr(ppd->dd, ppd->port,\r\n"SDMA sdmadesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes offset:%u bytes\n",\r\nhead, flags, addr, gen, dwlen, dwoffset);\r\nif (++head == ppd->sdma_descq_cnt)\r\nhead = 0;\r\n}\r\nlist_for_each_entry_safe(txp, txpnext, &ppd->sdma_activelist,\r\nlist)\r\nqib_dev_porterr(ppd->dd, ppd->port,\r\n"SDMA txp->start_idx: %u txp->next_descq_idx: %u\n",\r\ntxp->start_idx, txp->next_descq_idx);\r\n}\r\nvoid qib_sdma_process_event(struct qib_pportdata *ppd,\r\nenum qib_sdma_events event)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ppd->sdma_lock, flags);\r\n__qib_sdma_process_event(ppd, event);\r\nif (ppd->sdma_state.current_state == qib_sdma_state_s99_running)\r\nqib_verbs_sdma_desc_avail(ppd, qib_sdma_descq_freecnt(ppd));\r\nspin_unlock_irqrestore(&ppd->sdma_lock, flags);\r\n}\r\nvoid __qib_sdma_process_event(struct qib_pportdata *ppd,\r\nenum qib_sdma_events event)\r\n{\r\nstruct qib_sdma_state *ss = &ppd->sdma_state;\r\nswitch (ss->current_state) {\r\ncase qib_sdma_state_s00_hw_down:\r\nswitch (event) {\r\ncase qib_sdma_event_e00_go_hw_down:\r\nbreak;\r\ncase qib_sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\ncase qib_sdma_event_e10_go_hw_start:\r\nsdma_get(&ppd->sdma_state);\r\nsdma_set_state(ppd,\r\nqib_sdma_state_s10_hw_start_up_wait);\r\nbreak;\r\ncase qib_sdma_event_e20_hw_started:\r\nbreak;\r\ncase qib_sdma_event_e40_sw_cleaned:\r\nsdma_sw_tear_down(ppd);\r\nbreak;\r\ncase qib_sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e60_hw_halted:\r\nbreak;\r\ncase qib_sdma_event_e70_go_idle:\r\nbreak;\r\ncase qib_sdma_event_e7220_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e7322_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e90_timer_tick:\r\nbreak;\r\n}\r\nbreak;\r\ncase qib_sdma_state_s10_hw_start_up_wait:\r\nswitch (event) {\r\ncase qib_sdma_event_e00_go_hw_down:\r\nsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\r\nsdma_sw_tear_down(ppd);\r\nbreak;\r\ncase qib_sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase qib_sdma_event_e20_hw_started:\r\nsdma_set_state(ppd, ss->go_s99_running ?\r\nqib_sdma_state_s99_running :\r\nqib_sdma_state_s20_idle);\r\nbreak;\r\ncase qib_sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase qib_sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e60_hw_halted:\r\nbreak;\r\ncase qib_sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase qib_sdma_event_e7220_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e7322_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e90_timer_tick:\r\nbreak;\r\n}\r\nbreak;\r\ncase qib_sdma_state_s20_idle:\r\nswitch (event) {\r\ncase qib_sdma_event_e00_go_hw_down:\r\nsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\r\nsdma_sw_tear_down(ppd);\r\nbreak;\r\ncase qib_sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase qib_sdma_event_e20_hw_started:\r\nbreak;\r\ncase qib_sdma_event_e30_go_running:\r\nsdma_set_state(ppd, qib_sdma_state_s99_running);\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase qib_sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e60_hw_halted:\r\nbreak;\r\ncase qib_sdma_event_e70_go_idle:\r\nbreak;\r\ncase qib_sdma_event_e7220_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e7322_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e90_timer_tick:\r\nbreak;\r\n}\r\nbreak;\r\ncase qib_sdma_state_s30_sw_clean_up_wait:\r\nswitch (event) {\r\ncase qib_sdma_event_e00_go_hw_down:\r\nsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\r\nbreak;\r\ncase qib_sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase qib_sdma_event_e20_hw_started:\r\nbreak;\r\ncase qib_sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase qib_sdma_event_e40_sw_cleaned:\r\nsdma_set_state(ppd,\r\nqib_sdma_state_s10_hw_start_up_wait);\r\nsdma_hw_start_up(ppd);\r\nbreak;\r\ncase qib_sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e60_hw_halted:\r\nbreak;\r\ncase qib_sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase qib_sdma_event_e7220_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e7322_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e90_timer_tick:\r\nbreak;\r\n}\r\nbreak;\r\ncase qib_sdma_state_s40_hw_clean_up_wait:\r\nswitch (event) {\r\ncase qib_sdma_event_e00_go_hw_down:\r\nsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\r\nsdma_start_sw_clean_up(ppd);\r\nbreak;\r\ncase qib_sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase qib_sdma_event_e20_hw_started:\r\nbreak;\r\ncase qib_sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase qib_sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e50_hw_cleaned:\r\nsdma_set_state(ppd,\r\nqib_sdma_state_s30_sw_clean_up_wait);\r\nsdma_start_sw_clean_up(ppd);\r\nbreak;\r\ncase qib_sdma_event_e60_hw_halted:\r\nbreak;\r\ncase qib_sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase qib_sdma_event_e7220_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e7322_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e90_timer_tick:\r\nbreak;\r\n}\r\nbreak;\r\ncase qib_sdma_state_s50_hw_halt_wait:\r\nswitch (event) {\r\ncase qib_sdma_event_e00_go_hw_down:\r\nsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\r\nsdma_start_sw_clean_up(ppd);\r\nbreak;\r\ncase qib_sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase qib_sdma_event_e20_hw_started:\r\nbreak;\r\ncase qib_sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase qib_sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e60_hw_halted:\r\nsdma_set_state(ppd,\r\nqib_sdma_state_s40_hw_clean_up_wait);\r\nppd->dd->f_sdma_hw_clean_up(ppd);\r\nbreak;\r\ncase qib_sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase qib_sdma_event_e7220_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e7322_err_halted:\r\nbreak;\r\ncase qib_sdma_event_e90_timer_tick:\r\nbreak;\r\n}\r\nbreak;\r\ncase qib_sdma_state_s99_running:\r\nswitch (event) {\r\ncase qib_sdma_event_e00_go_hw_down:\r\nsdma_set_state(ppd, qib_sdma_state_s00_hw_down);\r\nsdma_start_sw_clean_up(ppd);\r\nbreak;\r\ncase qib_sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase qib_sdma_event_e20_hw_started:\r\nbreak;\r\ncase qib_sdma_event_e30_go_running:\r\nbreak;\r\ncase qib_sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase qib_sdma_event_e60_hw_halted:\r\nsdma_set_state(ppd,\r\nqib_sdma_state_s30_sw_clean_up_wait);\r\nsdma_start_sw_clean_up(ppd);\r\nbreak;\r\ncase qib_sdma_event_e70_go_idle:\r\nsdma_set_state(ppd, qib_sdma_state_s50_hw_halt_wait);\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase qib_sdma_event_e7220_err_halted:\r\nsdma_set_state(ppd,\r\nqib_sdma_state_s30_sw_clean_up_wait);\r\nsdma_start_sw_clean_up(ppd);\r\nbreak;\r\ncase qib_sdma_event_e7322_err_halted:\r\nsdma_set_state(ppd, qib_sdma_state_s50_hw_halt_wait);\r\nbreak;\r\ncase qib_sdma_event_e90_timer_tick:\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nss->last_event = event;\r\n}
