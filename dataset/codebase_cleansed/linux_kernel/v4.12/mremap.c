static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)\r\n{\r\npgd_t *pgd;\r\np4d_t *p4d;\r\npud_t *pud;\r\npmd_t *pmd;\r\npgd = pgd_offset(mm, addr);\r\nif (pgd_none_or_clear_bad(pgd))\r\nreturn NULL;\r\np4d = p4d_offset(pgd, addr);\r\nif (p4d_none_or_clear_bad(p4d))\r\nreturn NULL;\r\npud = pud_offset(p4d, addr);\r\nif (pud_none_or_clear_bad(pud))\r\nreturn NULL;\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd))\r\nreturn NULL;\r\nreturn pmd;\r\n}\r\nstatic pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,\r\nunsigned long addr)\r\n{\r\npgd_t *pgd;\r\np4d_t *p4d;\r\npud_t *pud;\r\npmd_t *pmd;\r\npgd = pgd_offset(mm, addr);\r\np4d = p4d_alloc(mm, pgd, addr);\r\nif (!p4d)\r\nreturn NULL;\r\npud = pud_alloc(mm, p4d, addr);\r\nif (!pud)\r\nreturn NULL;\r\npmd = pmd_alloc(mm, pud, addr);\r\nif (!pmd)\r\nreturn NULL;\r\nVM_BUG_ON(pmd_trans_huge(*pmd));\r\nreturn pmd;\r\n}\r\nstatic void take_rmap_locks(struct vm_area_struct *vma)\r\n{\r\nif (vma->vm_file)\r\ni_mmap_lock_write(vma->vm_file->f_mapping);\r\nif (vma->anon_vma)\r\nanon_vma_lock_write(vma->anon_vma);\r\n}\r\nstatic void drop_rmap_locks(struct vm_area_struct *vma)\r\n{\r\nif (vma->anon_vma)\r\nanon_vma_unlock_write(vma->anon_vma);\r\nif (vma->vm_file)\r\ni_mmap_unlock_write(vma->vm_file->f_mapping);\r\n}\r\nstatic pte_t move_soft_dirty_pte(pte_t pte)\r\n{\r\n#ifdef CONFIG_MEM_SOFT_DIRTY\r\nif (pte_present(pte))\r\npte = pte_mksoft_dirty(pte);\r\nelse if (is_swap_pte(pte))\r\npte = pte_swp_mksoft_dirty(pte);\r\n#endif\r\nreturn pte;\r\n}\r\nstatic void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,\r\nunsigned long old_addr, unsigned long old_end,\r\nstruct vm_area_struct *new_vma, pmd_t *new_pmd,\r\nunsigned long new_addr, bool need_rmap_locks, bool *need_flush)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npte_t *old_pte, *new_pte, pte;\r\nspinlock_t *old_ptl, *new_ptl;\r\nbool force_flush = false;\r\nunsigned long len = old_end - old_addr;\r\nif (need_rmap_locks)\r\ntake_rmap_locks(vma);\r\nold_pte = pte_offset_map_lock(mm, old_pmd, old_addr, &old_ptl);\r\nnew_pte = pte_offset_map(new_pmd, new_addr);\r\nnew_ptl = pte_lockptr(mm, new_pmd);\r\nif (new_ptl != old_ptl)\r\nspin_lock_nested(new_ptl, SINGLE_DEPTH_NESTING);\r\narch_enter_lazy_mmu_mode();\r\nfor (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,\r\nnew_pte++, new_addr += PAGE_SIZE) {\r\nif (pte_none(*old_pte))\r\ncontinue;\r\npte = ptep_get_and_clear(mm, old_addr, old_pte);\r\nif (pte_present(pte) && pte_dirty(pte))\r\nforce_flush = true;\r\npte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);\r\npte = move_soft_dirty_pte(pte);\r\nset_pte_at(mm, new_addr, new_pte, pte);\r\n}\r\narch_leave_lazy_mmu_mode();\r\nif (new_ptl != old_ptl)\r\nspin_unlock(new_ptl);\r\npte_unmap(new_pte - 1);\r\nif (force_flush)\r\nflush_tlb_range(vma, old_end - len, old_end);\r\nelse\r\n*need_flush = true;\r\npte_unmap_unlock(old_pte - 1, old_ptl);\r\nif (need_rmap_locks)\r\ndrop_rmap_locks(vma);\r\n}\r\nunsigned long move_page_tables(struct vm_area_struct *vma,\r\nunsigned long old_addr, struct vm_area_struct *new_vma,\r\nunsigned long new_addr, unsigned long len,\r\nbool need_rmap_locks)\r\n{\r\nunsigned long extent, next, old_end;\r\npmd_t *old_pmd, *new_pmd;\r\nbool need_flush = false;\r\nunsigned long mmun_start;\r\nunsigned long mmun_end;\r\nold_end = old_addr + len;\r\nflush_cache_range(vma, old_addr, old_end);\r\nmmun_start = old_addr;\r\nmmun_end = old_end;\r\nmmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);\r\nfor (; old_addr < old_end; old_addr += extent, new_addr += extent) {\r\ncond_resched();\r\nnext = (old_addr + PMD_SIZE) & PMD_MASK;\r\nextent = next - old_addr;\r\nif (extent > old_end - old_addr)\r\nextent = old_end - old_addr;\r\nold_pmd = get_old_pmd(vma->vm_mm, old_addr);\r\nif (!old_pmd)\r\ncontinue;\r\nnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\r\nif (!new_pmd)\r\nbreak;\r\nif (pmd_trans_huge(*old_pmd)) {\r\nif (extent == HPAGE_PMD_SIZE) {\r\nbool moved;\r\nif (need_rmap_locks)\r\ntake_rmap_locks(vma);\r\nmoved = move_huge_pmd(vma, old_addr, new_addr,\r\nold_end, old_pmd, new_pmd,\r\n&need_flush);\r\nif (need_rmap_locks)\r\ndrop_rmap_locks(vma);\r\nif (moved)\r\ncontinue;\r\n}\r\nsplit_huge_pmd(vma, old_pmd, old_addr);\r\nif (pmd_trans_unstable(old_pmd))\r\ncontinue;\r\n}\r\nif (pte_alloc(new_vma->vm_mm, new_pmd, new_addr))\r\nbreak;\r\nnext = (new_addr + PMD_SIZE) & PMD_MASK;\r\nif (extent > next - new_addr)\r\nextent = next - new_addr;\r\nif (extent > LATENCY_LIMIT)\r\nextent = LATENCY_LIMIT;\r\nmove_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,\r\nnew_pmd, new_addr, need_rmap_locks, &need_flush);\r\n}\r\nif (need_flush)\r\nflush_tlb_range(vma, old_end-len, old_addr);\r\nmmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);\r\nreturn len + old_addr - old_end;\r\n}\r\nstatic unsigned long move_vma(struct vm_area_struct *vma,\r\nunsigned long old_addr, unsigned long old_len,\r\nunsigned long new_len, unsigned long new_addr,\r\nbool *locked, struct vm_userfaultfd_ctx *uf,\r\nstruct list_head *uf_unmap)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct vm_area_struct *new_vma;\r\nunsigned long vm_flags = vma->vm_flags;\r\nunsigned long new_pgoff;\r\nunsigned long moved_len;\r\nunsigned long excess = 0;\r\nunsigned long hiwater_vm;\r\nint split = 0;\r\nint err;\r\nbool need_rmap_locks;\r\nif (mm->map_count >= sysctl_max_map_count - 3)\r\nreturn -ENOMEM;\r\nerr = ksm_madvise(vma, old_addr, old_addr + old_len,\r\nMADV_UNMERGEABLE, &vm_flags);\r\nif (err)\r\nreturn err;\r\nnew_pgoff = vma->vm_pgoff + ((old_addr - vma->vm_start) >> PAGE_SHIFT);\r\nnew_vma = copy_vma(&vma, new_addr, new_len, new_pgoff,\r\n&need_rmap_locks);\r\nif (!new_vma)\r\nreturn -ENOMEM;\r\nmoved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len,\r\nneed_rmap_locks);\r\nif (moved_len < old_len) {\r\nerr = -ENOMEM;\r\n} else if (vma->vm_ops && vma->vm_ops->mremap) {\r\nerr = vma->vm_ops->mremap(new_vma);\r\n}\r\nif (unlikely(err)) {\r\nmove_page_tables(new_vma, new_addr, vma, old_addr, moved_len,\r\ntrue);\r\nvma = new_vma;\r\nold_len = new_len;\r\nold_addr = new_addr;\r\nnew_addr = err;\r\n} else {\r\nmremap_userfaultfd_prep(new_vma, uf);\r\narch_remap(mm, old_addr, old_addr + old_len,\r\nnew_addr, new_addr + new_len);\r\n}\r\nif (vm_flags & VM_ACCOUNT) {\r\nvma->vm_flags &= ~VM_ACCOUNT;\r\nexcess = vma->vm_end - vma->vm_start - old_len;\r\nif (old_addr > vma->vm_start &&\r\nold_addr + old_len < vma->vm_end)\r\nsplit = 1;\r\n}\r\nhiwater_vm = mm->hiwater_vm;\r\nvm_stat_account(mm, vma->vm_flags, new_len >> PAGE_SHIFT);\r\nif (unlikely(vma->vm_flags & VM_PFNMAP))\r\nuntrack_pfn_moved(vma);\r\nif (do_munmap(mm, old_addr, old_len, uf_unmap) < 0) {\r\nvm_unacct_memory(excess >> PAGE_SHIFT);\r\nexcess = 0;\r\n}\r\nmm->hiwater_vm = hiwater_vm;\r\nif (excess) {\r\nvma->vm_flags |= VM_ACCOUNT;\r\nif (split)\r\nvma->vm_next->vm_flags |= VM_ACCOUNT;\r\n}\r\nif (vm_flags & VM_LOCKED) {\r\nmm->locked_vm += new_len >> PAGE_SHIFT;\r\n*locked = true;\r\n}\r\nreturn new_addr;\r\n}\r\nstatic struct vm_area_struct *vma_to_resize(unsigned long addr,\r\nunsigned long old_len, unsigned long new_len, unsigned long *p)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma = find_vma(mm, addr);\r\nunsigned long pgoff;\r\nif (!vma || vma->vm_start > addr)\r\nreturn ERR_PTR(-EFAULT);\r\nif (is_vm_hugetlb_page(vma))\r\nreturn ERR_PTR(-EINVAL);\r\nif (old_len > vma->vm_end - addr)\r\nreturn ERR_PTR(-EFAULT);\r\nif (new_len == old_len)\r\nreturn vma;\r\npgoff = (addr - vma->vm_start) >> PAGE_SHIFT;\r\npgoff += vma->vm_pgoff;\r\nif (pgoff + (new_len >> PAGE_SHIFT) < pgoff)\r\nreturn ERR_PTR(-EINVAL);\r\nif (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))\r\nreturn ERR_PTR(-EFAULT);\r\nif (vma->vm_flags & VM_LOCKED) {\r\nunsigned long locked, lock_limit;\r\nlocked = mm->locked_vm << PAGE_SHIFT;\r\nlock_limit = rlimit(RLIMIT_MEMLOCK);\r\nlocked += new_len - old_len;\r\nif (locked > lock_limit && !capable(CAP_IPC_LOCK))\r\nreturn ERR_PTR(-EAGAIN);\r\n}\r\nif (!may_expand_vm(mm, vma->vm_flags,\r\n(new_len - old_len) >> PAGE_SHIFT))\r\nreturn ERR_PTR(-ENOMEM);\r\nif (vma->vm_flags & VM_ACCOUNT) {\r\nunsigned long charged = (new_len - old_len) >> PAGE_SHIFT;\r\nif (security_vm_enough_memory_mm(mm, charged))\r\nreturn ERR_PTR(-ENOMEM);\r\n*p = charged;\r\n}\r\nreturn vma;\r\n}\r\nstatic unsigned long mremap_to(unsigned long addr, unsigned long old_len,\r\nunsigned long new_addr, unsigned long new_len, bool *locked,\r\nstruct vm_userfaultfd_ctx *uf,\r\nstruct list_head *uf_unmap)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nstruct vm_area_struct *vma;\r\nunsigned long ret = -EINVAL;\r\nunsigned long charged = 0;\r\nunsigned long map_flags;\r\nif (offset_in_page(new_addr))\r\ngoto out;\r\nif (new_len > TASK_SIZE || new_addr > TASK_SIZE - new_len)\r\ngoto out;\r\nif (addr + old_len > new_addr && new_addr + new_len > addr)\r\ngoto out;\r\nret = do_munmap(mm, new_addr, new_len, NULL);\r\nif (ret)\r\ngoto out;\r\nif (old_len >= new_len) {\r\nret = do_munmap(mm, addr+new_len, old_len - new_len, uf_unmap);\r\nif (ret && old_len != new_len)\r\ngoto out;\r\nold_len = new_len;\r\n}\r\nvma = vma_to_resize(addr, old_len, new_len, &charged);\r\nif (IS_ERR(vma)) {\r\nret = PTR_ERR(vma);\r\ngoto out;\r\n}\r\nmap_flags = MAP_FIXED;\r\nif (vma->vm_flags & VM_MAYSHARE)\r\nmap_flags |= MAP_SHARED;\r\nret = get_unmapped_area(vma->vm_file, new_addr, new_len, vma->vm_pgoff +\r\n((addr - vma->vm_start) >> PAGE_SHIFT),\r\nmap_flags);\r\nif (offset_in_page(ret))\r\ngoto out1;\r\nret = move_vma(vma, addr, old_len, new_len, new_addr, locked, uf,\r\nuf_unmap);\r\nif (!(offset_in_page(ret)))\r\ngoto out;\r\nout1:\r\nvm_unacct_memory(charged);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int vma_expandable(struct vm_area_struct *vma, unsigned long delta)\r\n{\r\nunsigned long end = vma->vm_end + delta;\r\nif (end < vma->vm_end)\r\nreturn 0;\r\nif (vma->vm_next && vma->vm_next->vm_start < end)\r\nreturn 0;\r\nif (get_unmapped_area(NULL, vma->vm_start, end - vma->vm_start,\r\n0, MAP_FIXED) & ~PAGE_MASK)\r\nreturn 0;\r\nreturn 1;\r\n}
