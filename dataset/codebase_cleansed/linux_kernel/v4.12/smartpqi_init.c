static char *pqi_raid_level_to_string(u8 raid_level)\r\n{\r\nif (raid_level < ARRAY_SIZE(raid_levels))\r\nreturn raid_levels[raid_level];\r\nreturn "";\r\n}\r\nstatic inline void pqi_scsi_done(struct scsi_cmnd *scmd)\r\n{\r\nscmd->scsi_done(scmd);\r\n}\r\nstatic inline bool pqi_scsi3addr_equal(u8 *scsi3addr1, u8 *scsi3addr2)\r\n{\r\nreturn memcmp(scsi3addr1, scsi3addr2, 8) == 0;\r\n}\r\nstatic inline struct pqi_ctrl_info *shost_to_hba(struct Scsi_Host *shost)\r\n{\r\nvoid *hostdata = shost_priv(shost);\r\nreturn *((struct pqi_ctrl_info **)hostdata);\r\n}\r\nstatic inline bool pqi_is_logical_device(struct pqi_scsi_dev *device)\r\n{\r\nreturn !device->is_physical_device;\r\n}\r\nstatic inline bool pqi_ctrl_offline(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nreturn !ctrl_info->controller_online;\r\n}\r\nstatic inline void pqi_check_ctrl_health(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nif (ctrl_info->controller_online)\r\nif (!sis_is_firmware_running(ctrl_info))\r\npqi_take_ctrl_offline(ctrl_info);\r\n}\r\nstatic inline bool pqi_is_hba_lunid(u8 *scsi3addr)\r\n{\r\nreturn pqi_scsi3addr_equal(scsi3addr, RAID_CTLR_LUNID);\r\n}\r\nstatic inline enum pqi_ctrl_mode pqi_get_ctrl_mode(\r\nstruct pqi_ctrl_info *ctrl_info)\r\n{\r\nreturn sis_read_driver_scratch(ctrl_info);\r\n}\r\nstatic inline void pqi_save_ctrl_mode(struct pqi_ctrl_info *ctrl_info,\r\nenum pqi_ctrl_mode mode)\r\n{\r\nsis_write_driver_scratch(ctrl_info, mode);\r\n}\r\nstatic inline void pqi_schedule_rescan_worker(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nschedule_delayed_work(&ctrl_info->rescan_work,\r\nPQI_RESCAN_WORK_INTERVAL);\r\n}\r\nstatic int pqi_map_single(struct pci_dev *pci_dev,\r\nstruct pqi_sg_descriptor *sg_descriptor, void *buffer,\r\nsize_t buffer_length, int data_direction)\r\n{\r\ndma_addr_t bus_address;\r\nif (!buffer || buffer_length == 0 || data_direction == PCI_DMA_NONE)\r\nreturn 0;\r\nbus_address = pci_map_single(pci_dev, buffer, buffer_length,\r\ndata_direction);\r\nif (pci_dma_mapping_error(pci_dev, bus_address))\r\nreturn -ENOMEM;\r\nput_unaligned_le64((u64)bus_address, &sg_descriptor->address);\r\nput_unaligned_le32(buffer_length, &sg_descriptor->length);\r\nput_unaligned_le32(CISS_SG_LAST, &sg_descriptor->flags);\r\nreturn 0;\r\n}\r\nstatic void pqi_pci_unmap(struct pci_dev *pci_dev,\r\nstruct pqi_sg_descriptor *descriptors, int num_descriptors,\r\nint data_direction)\r\n{\r\nint i;\r\nif (data_direction == PCI_DMA_NONE)\r\nreturn;\r\nfor (i = 0; i < num_descriptors; i++)\r\npci_unmap_single(pci_dev,\r\n(dma_addr_t)get_unaligned_le64(&descriptors[i].address),\r\nget_unaligned_le32(&descriptors[i].length),\r\ndata_direction);\r\n}\r\nstatic int pqi_build_raid_path_request(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_raid_path_request *request, u8 cmd,\r\nu8 *scsi3addr, void *buffer, size_t buffer_length,\r\nu16 vpd_page, int *pci_direction)\r\n{\r\nu8 *cdb;\r\nint pci_dir;\r\nmemset(request, 0, sizeof(*request));\r\nrequest->header.iu_type = PQI_REQUEST_IU_RAID_PATH_IO;\r\nput_unaligned_le16(offsetof(struct pqi_raid_path_request,\r\nsg_descriptors[1]) - PQI_REQUEST_HEADER_LENGTH,\r\n&request->header.iu_length);\r\nput_unaligned_le32(buffer_length, &request->buffer_length);\r\nmemcpy(request->lun_number, scsi3addr, sizeof(request->lun_number));\r\nrequest->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\r\nrequest->additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_0;\r\ncdb = request->cdb;\r\nswitch (cmd) {\r\ncase INQUIRY:\r\nrequest->data_direction = SOP_READ_FLAG;\r\ncdb[0] = INQUIRY;\r\nif (vpd_page & VPD_PAGE) {\r\ncdb[1] = 0x1;\r\ncdb[2] = (u8)vpd_page;\r\n}\r\ncdb[4] = (u8)buffer_length;\r\nbreak;\r\ncase CISS_REPORT_LOG:\r\ncase CISS_REPORT_PHYS:\r\nrequest->data_direction = SOP_READ_FLAG;\r\ncdb[0] = cmd;\r\nif (cmd == CISS_REPORT_PHYS)\r\ncdb[1] = CISS_REPORT_PHYS_EXTENDED;\r\nelse\r\ncdb[1] = CISS_REPORT_LOG_EXTENDED;\r\nput_unaligned_be32(buffer_length, &cdb[6]);\r\nbreak;\r\ncase CISS_GET_RAID_MAP:\r\nrequest->data_direction = SOP_READ_FLAG;\r\ncdb[0] = CISS_READ;\r\ncdb[1] = CISS_GET_RAID_MAP;\r\nput_unaligned_be32(buffer_length, &cdb[6]);\r\nbreak;\r\ncase SA_CACHE_FLUSH:\r\nrequest->data_direction = SOP_WRITE_FLAG;\r\ncdb[0] = BMIC_WRITE;\r\ncdb[6] = BMIC_CACHE_FLUSH;\r\nput_unaligned_be16(buffer_length, &cdb[7]);\r\nbreak;\r\ncase BMIC_IDENTIFY_CONTROLLER:\r\ncase BMIC_IDENTIFY_PHYSICAL_DEVICE:\r\nrequest->data_direction = SOP_READ_FLAG;\r\ncdb[0] = BMIC_READ;\r\ncdb[6] = cmd;\r\nput_unaligned_be16(buffer_length, &cdb[7]);\r\nbreak;\r\ncase BMIC_WRITE_HOST_WELLNESS:\r\nrequest->data_direction = SOP_WRITE_FLAG;\r\ncdb[0] = BMIC_WRITE;\r\ncdb[6] = cmd;\r\nput_unaligned_be16(buffer_length, &cdb[7]);\r\nbreak;\r\ndefault:\r\ndev_err(&ctrl_info->pci_dev->dev, "unknown command 0x%c\n",\r\ncmd);\r\nWARN_ON(cmd);\r\nbreak;\r\n}\r\nswitch (request->data_direction) {\r\ncase SOP_READ_FLAG:\r\npci_dir = PCI_DMA_FROMDEVICE;\r\nbreak;\r\ncase SOP_WRITE_FLAG:\r\npci_dir = PCI_DMA_TODEVICE;\r\nbreak;\r\ncase SOP_NO_DIRECTION_FLAG:\r\npci_dir = PCI_DMA_NONE;\r\nbreak;\r\ndefault:\r\npci_dir = PCI_DMA_BIDIRECTIONAL;\r\nbreak;\r\n}\r\n*pci_direction = pci_dir;\r\nreturn pqi_map_single(ctrl_info->pci_dev, &request->sg_descriptors[0],\r\nbuffer, buffer_length, pci_dir);\r\n}\r\nstatic struct pqi_io_request *pqi_alloc_io_request(\r\nstruct pqi_ctrl_info *ctrl_info)\r\n{\r\nstruct pqi_io_request *io_request;\r\nu16 i = ctrl_info->next_io_request_slot;\r\nwhile (1) {\r\nio_request = &ctrl_info->io_request_pool[i];\r\nif (atomic_inc_return(&io_request->refcount) == 1)\r\nbreak;\r\natomic_dec(&io_request->refcount);\r\ni = (i + 1) % ctrl_info->max_io_slots;\r\n}\r\nctrl_info->next_io_request_slot = (i + 1) % ctrl_info->max_io_slots;\r\nio_request->scmd = NULL;\r\nio_request->status = 0;\r\nio_request->error_info = NULL;\r\nreturn io_request;\r\n}\r\nstatic void pqi_free_io_request(struct pqi_io_request *io_request)\r\n{\r\natomic_dec(&io_request->refcount);\r\n}\r\nstatic int pqi_identify_controller(struct pqi_ctrl_info *ctrl_info,\r\nstruct bmic_identify_controller *buffer)\r\n{\r\nint rc;\r\nint pci_direction;\r\nstruct pqi_raid_path_request request;\r\nrc = pqi_build_raid_path_request(ctrl_info, &request,\r\nBMIC_IDENTIFY_CONTROLLER, RAID_CTLR_LUNID, buffer,\r\nsizeof(*buffer), 0, &pci_direction);\r\nif (rc)\r\nreturn rc;\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0,\r\nNULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\r\npci_direction);\r\nreturn rc;\r\n}\r\nstatic int pqi_scsi_inquiry(struct pqi_ctrl_info *ctrl_info,\r\nu8 *scsi3addr, u16 vpd_page, void *buffer, size_t buffer_length)\r\n{\r\nint rc;\r\nint pci_direction;\r\nstruct pqi_raid_path_request request;\r\nrc = pqi_build_raid_path_request(ctrl_info, &request,\r\nINQUIRY, scsi3addr, buffer, buffer_length, vpd_page,\r\n&pci_direction);\r\nif (rc)\r\nreturn rc;\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0,\r\nNULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\r\npci_direction);\r\nreturn rc;\r\n}\r\nstatic int pqi_identify_physical_device(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device,\r\nstruct bmic_identify_physical_device *buffer,\r\nsize_t buffer_length)\r\n{\r\nint rc;\r\nint pci_direction;\r\nu16 bmic_device_index;\r\nstruct pqi_raid_path_request request;\r\nrc = pqi_build_raid_path_request(ctrl_info, &request,\r\nBMIC_IDENTIFY_PHYSICAL_DEVICE, RAID_CTLR_LUNID, buffer,\r\nbuffer_length, 0, &pci_direction);\r\nif (rc)\r\nreturn rc;\r\nbmic_device_index = CISS_GET_DRIVE_NUMBER(device->scsi3addr);\r\nrequest.cdb[2] = (u8)bmic_device_index;\r\nrequest.cdb[9] = (u8)(bmic_device_index >> 8);\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header,\r\n0, NULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\r\npci_direction);\r\nreturn rc;\r\n}\r\nstatic int pqi_flush_cache(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nstruct pqi_raid_path_request request;\r\nint pci_direction;\r\nu8 *buffer;\r\nif (pqi_ctrl_offline(ctrl_info))\r\nreturn -ENXIO;\r\nbuffer = kzalloc(SA_CACHE_FLUSH_BUFFER_LENGTH, GFP_KERNEL);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\nrc = pqi_build_raid_path_request(ctrl_info, &request,\r\nSA_CACHE_FLUSH, RAID_CTLR_LUNID, buffer,\r\nSA_CACHE_FLUSH_BUFFER_LENGTH, 0, &pci_direction);\r\nif (rc)\r\ngoto out;\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header,\r\n0, NULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\r\npci_direction);\r\nout:\r\nkfree(buffer);\r\nreturn rc;\r\n}\r\nstatic int pqi_write_host_wellness(struct pqi_ctrl_info *ctrl_info,\r\nvoid *buffer, size_t buffer_length)\r\n{\r\nint rc;\r\nstruct pqi_raid_path_request request;\r\nint pci_direction;\r\nrc = pqi_build_raid_path_request(ctrl_info, &request,\r\nBMIC_WRITE_HOST_WELLNESS, RAID_CTLR_LUNID, buffer,\r\nbuffer_length, 0, &pci_direction);\r\nif (rc)\r\nreturn rc;\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header,\r\n0, NULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\r\npci_direction);\r\nreturn rc;\r\n}\r\nstatic int pqi_write_driver_version_to_host_wellness(\r\nstruct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nstruct bmic_host_wellness_driver_version *buffer;\r\nsize_t buffer_length;\r\nbuffer_length = sizeof(*buffer);\r\nbuffer = kmalloc(buffer_length, GFP_KERNEL);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\nbuffer->start_tag[0] = '<';\r\nbuffer->start_tag[1] = 'H';\r\nbuffer->start_tag[2] = 'W';\r\nbuffer->start_tag[3] = '>';\r\nbuffer->driver_version_tag[0] = 'D';\r\nbuffer->driver_version_tag[1] = 'V';\r\nput_unaligned_le16(sizeof(buffer->driver_version),\r\n&buffer->driver_version_length);\r\nstrncpy(buffer->driver_version, DRIVER_VERSION,\r\nsizeof(buffer->driver_version) - 1);\r\nbuffer->driver_version[sizeof(buffer->driver_version) - 1] = '\0';\r\nbuffer->end_tag[0] = 'Z';\r\nbuffer->end_tag[1] = 'Z';\r\nrc = pqi_write_host_wellness(ctrl_info, buffer, buffer_length);\r\nkfree(buffer);\r\nreturn rc;\r\n}\r\nstatic int pqi_write_current_time_to_host_wellness(\r\nstruct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nstruct bmic_host_wellness_time *buffer;\r\nsize_t buffer_length;\r\ntime64_t local_time;\r\nunsigned int year;\r\nstruct tm tm;\r\nbuffer_length = sizeof(*buffer);\r\nbuffer = kmalloc(buffer_length, GFP_KERNEL);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\nbuffer->start_tag[0] = '<';\r\nbuffer->start_tag[1] = 'H';\r\nbuffer->start_tag[2] = 'W';\r\nbuffer->start_tag[3] = '>';\r\nbuffer->time_tag[0] = 'T';\r\nbuffer->time_tag[1] = 'D';\r\nput_unaligned_le16(sizeof(buffer->time),\r\n&buffer->time_length);\r\nlocal_time = ktime_get_real_seconds();\r\ntime64_to_tm(local_time, -sys_tz.tz_minuteswest * 60, &tm);\r\nyear = tm.tm_year + 1900;\r\nbuffer->time[0] = bin2bcd(tm.tm_hour);\r\nbuffer->time[1] = bin2bcd(tm.tm_min);\r\nbuffer->time[2] = bin2bcd(tm.tm_sec);\r\nbuffer->time[3] = 0;\r\nbuffer->time[4] = bin2bcd(tm.tm_mon + 1);\r\nbuffer->time[5] = bin2bcd(tm.tm_mday);\r\nbuffer->time[6] = bin2bcd(year / 100);\r\nbuffer->time[7] = bin2bcd(year % 100);\r\nbuffer->dont_write_tag[0] = 'D';\r\nbuffer->dont_write_tag[1] = 'W';\r\nbuffer->end_tag[0] = 'Z';\r\nbuffer->end_tag[1] = 'Z';\r\nrc = pqi_write_host_wellness(ctrl_info, buffer, buffer_length);\r\nkfree(buffer);\r\nreturn rc;\r\n}\r\nstatic void pqi_update_time_worker(struct work_struct *work)\r\n{\r\nint rc;\r\nstruct pqi_ctrl_info *ctrl_info;\r\nctrl_info = container_of(to_delayed_work(work), struct pqi_ctrl_info,\r\nupdate_time_work);\r\nrc = pqi_write_current_time_to_host_wellness(ctrl_info);\r\nif (rc)\r\ndev_warn(&ctrl_info->pci_dev->dev,\r\n"error updating time on controller\n");\r\nschedule_delayed_work(&ctrl_info->update_time_work,\r\nPQI_UPDATE_TIME_WORK_INTERVAL);\r\n}\r\nstatic inline void pqi_schedule_update_time_worker(\r\nstruct pqi_ctrl_info *ctrl_info)\r\n{\r\nschedule_delayed_work(&ctrl_info->update_time_work, 0);\r\n}\r\nstatic int pqi_report_luns(struct pqi_ctrl_info *ctrl_info, u8 cmd,\r\nvoid *buffer, size_t buffer_length)\r\n{\r\nint rc;\r\nint pci_direction;\r\nstruct pqi_raid_path_request request;\r\nrc = pqi_build_raid_path_request(ctrl_info, &request,\r\ncmd, RAID_CTLR_LUNID, buffer, buffer_length, 0, &pci_direction);\r\nif (rc)\r\nreturn rc;\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0,\r\nNULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\r\npci_direction);\r\nreturn rc;\r\n}\r\nstatic int pqi_report_phys_logical_luns(struct pqi_ctrl_info *ctrl_info, u8 cmd,\r\nvoid **buffer)\r\n{\r\nint rc;\r\nsize_t lun_list_length;\r\nsize_t lun_data_length;\r\nsize_t new_lun_list_length;\r\nvoid *lun_data = NULL;\r\nstruct report_lun_header *report_lun_header;\r\nreport_lun_header = kmalloc(sizeof(*report_lun_header), GFP_KERNEL);\r\nif (!report_lun_header) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nrc = pqi_report_luns(ctrl_info, cmd, report_lun_header,\r\nsizeof(*report_lun_header));\r\nif (rc)\r\ngoto out;\r\nlun_list_length = get_unaligned_be32(&report_lun_header->list_length);\r\nagain:\r\nlun_data_length = sizeof(struct report_lun_header) + lun_list_length;\r\nlun_data = kmalloc(lun_data_length, GFP_KERNEL);\r\nif (!lun_data) {\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nif (lun_list_length == 0) {\r\nmemcpy(lun_data, report_lun_header, sizeof(*report_lun_header));\r\ngoto out;\r\n}\r\nrc = pqi_report_luns(ctrl_info, cmd, lun_data, lun_data_length);\r\nif (rc)\r\ngoto out;\r\nnew_lun_list_length = get_unaligned_be32(\r\n&((struct report_lun_header *)lun_data)->list_length);\r\nif (new_lun_list_length > lun_list_length) {\r\nlun_list_length = new_lun_list_length;\r\nkfree(lun_data);\r\ngoto again;\r\n}\r\nout:\r\nkfree(report_lun_header);\r\nif (rc) {\r\nkfree(lun_data);\r\nlun_data = NULL;\r\n}\r\n*buffer = lun_data;\r\nreturn rc;\r\n}\r\nstatic inline int pqi_report_phys_luns(struct pqi_ctrl_info *ctrl_info,\r\nvoid **buffer)\r\n{\r\nreturn pqi_report_phys_logical_luns(ctrl_info, CISS_REPORT_PHYS,\r\nbuffer);\r\n}\r\nstatic inline int pqi_report_logical_luns(struct pqi_ctrl_info *ctrl_info,\r\nvoid **buffer)\r\n{\r\nreturn pqi_report_phys_logical_luns(ctrl_info, CISS_REPORT_LOG, buffer);\r\n}\r\nstatic int pqi_get_device_lists(struct pqi_ctrl_info *ctrl_info,\r\nstruct report_phys_lun_extended **physdev_list,\r\nstruct report_log_lun_extended **logdev_list)\r\n{\r\nint rc;\r\nsize_t logdev_list_length;\r\nsize_t logdev_data_length;\r\nstruct report_log_lun_extended *internal_logdev_list;\r\nstruct report_log_lun_extended *logdev_data;\r\nstruct report_lun_header report_lun_header;\r\nrc = pqi_report_phys_luns(ctrl_info, (void **)physdev_list);\r\nif (rc)\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"report physical LUNs failed\n");\r\nrc = pqi_report_logical_luns(ctrl_info, (void **)logdev_list);\r\nif (rc)\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"report logical LUNs failed\n");\r\nlogdev_data = *logdev_list;\r\nif (logdev_data) {\r\nlogdev_list_length =\r\nget_unaligned_be32(&logdev_data->header.list_length);\r\n} else {\r\nmemset(&report_lun_header, 0, sizeof(report_lun_header));\r\nlogdev_data =\r\n(struct report_log_lun_extended *)&report_lun_header;\r\nlogdev_list_length = 0;\r\n}\r\nlogdev_data_length = sizeof(struct report_lun_header) +\r\nlogdev_list_length;\r\ninternal_logdev_list = kmalloc(logdev_data_length +\r\nsizeof(struct report_log_lun_extended), GFP_KERNEL);\r\nif (!internal_logdev_list) {\r\nkfree(*logdev_list);\r\n*logdev_list = NULL;\r\nreturn -ENOMEM;\r\n}\r\nmemcpy(internal_logdev_list, logdev_data, logdev_data_length);\r\nmemset((u8 *)internal_logdev_list + logdev_data_length, 0,\r\nsizeof(struct report_log_lun_extended_entry));\r\nput_unaligned_be32(logdev_list_length +\r\nsizeof(struct report_log_lun_extended_entry),\r\n&internal_logdev_list->header.list_length);\r\nkfree(*logdev_list);\r\n*logdev_list = internal_logdev_list;\r\nreturn 0;\r\n}\r\nstatic inline void pqi_set_bus_target_lun(struct pqi_scsi_dev *device,\r\nint bus, int target, int lun)\r\n{\r\ndevice->bus = bus;\r\ndevice->target = target;\r\ndevice->lun = lun;\r\n}\r\nstatic void pqi_assign_bus_target_lun(struct pqi_scsi_dev *device)\r\n{\r\nu8 *scsi3addr;\r\nu32 lunid;\r\nscsi3addr = device->scsi3addr;\r\nlunid = get_unaligned_le32(scsi3addr);\r\nif (pqi_is_hba_lunid(scsi3addr)) {\r\npqi_set_bus_target_lun(device, PQI_HBA_BUS, 0, lunid & 0x3fff);\r\ndevice->target_lun_valid = true;\r\nreturn;\r\n}\r\nif (pqi_is_logical_device(device)) {\r\npqi_set_bus_target_lun(device, PQI_RAID_VOLUME_BUS, 0,\r\nlunid & 0x3fff);\r\ndevice->target_lun_valid = true;\r\nreturn;\r\n}\r\npqi_set_bus_target_lun(device, PQI_PHYSICAL_DEVICE_BUS, 0, 0);\r\n}\r\nstatic void pqi_get_raid_level(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nint rc;\r\nu8 raid_level;\r\nu8 *buffer;\r\nraid_level = SA_RAID_UNKNOWN;\r\nbuffer = kmalloc(64, GFP_KERNEL);\r\nif (buffer) {\r\nrc = pqi_scsi_inquiry(ctrl_info, device->scsi3addr,\r\nVPD_PAGE | CISS_VPD_LV_DEVICE_GEOMETRY, buffer, 64);\r\nif (rc == 0) {\r\nraid_level = buffer[8];\r\nif (raid_level > SA_RAID_MAX)\r\nraid_level = SA_RAID_UNKNOWN;\r\n}\r\nkfree(buffer);\r\n}\r\ndevice->raid_level = raid_level;\r\n}\r\nstatic int pqi_validate_raid_map(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device, struct raid_map *raid_map)\r\n{\r\nchar *err_msg;\r\nu32 raid_map_size;\r\nu32 r5or6_blocks_per_row;\r\nunsigned int num_phys_disks;\r\nunsigned int num_raid_map_entries;\r\nraid_map_size = get_unaligned_le32(&raid_map->structure_size);\r\nif (raid_map_size < offsetof(struct raid_map, disk_data)) {\r\nerr_msg = "RAID map too small";\r\ngoto bad_raid_map;\r\n}\r\nif (raid_map_size > sizeof(*raid_map)) {\r\nerr_msg = "RAID map too large";\r\ngoto bad_raid_map;\r\n}\r\nnum_phys_disks = get_unaligned_le16(&raid_map->layout_map_count) *\r\n(get_unaligned_le16(&raid_map->data_disks_per_row) +\r\nget_unaligned_le16(&raid_map->metadata_disks_per_row));\r\nnum_raid_map_entries = num_phys_disks *\r\nget_unaligned_le16(&raid_map->row_cnt);\r\nif (num_raid_map_entries > RAID_MAP_MAX_ENTRIES) {\r\nerr_msg = "invalid number of map entries in RAID map";\r\ngoto bad_raid_map;\r\n}\r\nif (device->raid_level == SA_RAID_1) {\r\nif (get_unaligned_le16(&raid_map->layout_map_count) != 2) {\r\nerr_msg = "invalid RAID-1 map";\r\ngoto bad_raid_map;\r\n}\r\n} else if (device->raid_level == SA_RAID_ADM) {\r\nif (get_unaligned_le16(&raid_map->layout_map_count) != 3) {\r\nerr_msg = "invalid RAID-1(ADM) map";\r\ngoto bad_raid_map;\r\n}\r\n} else if ((device->raid_level == SA_RAID_5 ||\r\ndevice->raid_level == SA_RAID_6) &&\r\nget_unaligned_le16(&raid_map->layout_map_count) > 1) {\r\nr5or6_blocks_per_row =\r\nget_unaligned_le16(&raid_map->strip_size) *\r\nget_unaligned_le16(&raid_map->data_disks_per_row);\r\nif (r5or6_blocks_per_row == 0) {\r\nerr_msg = "invalid RAID-5 or RAID-6 map";\r\ngoto bad_raid_map;\r\n}\r\n}\r\nreturn 0;\r\nbad_raid_map:\r\ndev_warn(&ctrl_info->pci_dev->dev, "%s\n", err_msg);\r\nreturn -EINVAL;\r\n}\r\nstatic int pqi_get_raid_map(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nint rc;\r\nint pci_direction;\r\nstruct pqi_raid_path_request request;\r\nstruct raid_map *raid_map;\r\nraid_map = kmalloc(sizeof(*raid_map), GFP_KERNEL);\r\nif (!raid_map)\r\nreturn -ENOMEM;\r\nrc = pqi_build_raid_path_request(ctrl_info, &request,\r\nCISS_GET_RAID_MAP, device->scsi3addr, raid_map,\r\nsizeof(*raid_map), 0, &pci_direction);\r\nif (rc)\r\ngoto error;\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0,\r\nNULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\r\npci_direction);\r\nif (rc)\r\ngoto error;\r\nrc = pqi_validate_raid_map(ctrl_info, device, raid_map);\r\nif (rc)\r\ngoto error;\r\ndevice->raid_map = raid_map;\r\nreturn 0;\r\nerror:\r\nkfree(raid_map);\r\nreturn rc;\r\n}\r\nstatic void pqi_get_offload_status(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nint rc;\r\nu8 *buffer;\r\nu8 offload_status;\r\nbuffer = kmalloc(64, GFP_KERNEL);\r\nif (!buffer)\r\nreturn;\r\nrc = pqi_scsi_inquiry(ctrl_info, device->scsi3addr,\r\nVPD_PAGE | CISS_VPD_LV_OFFLOAD_STATUS, buffer, 64);\r\nif (rc)\r\ngoto out;\r\n#define OFFLOAD_STATUS_BYTE 4\r\n#define OFFLOAD_CONFIGURED_BIT 0x1\r\n#define OFFLOAD_ENABLED_BIT 0x2\r\noffload_status = buffer[OFFLOAD_STATUS_BYTE];\r\ndevice->offload_configured =\r\n!!(offload_status & OFFLOAD_CONFIGURED_BIT);\r\nif (device->offload_configured) {\r\ndevice->offload_enabled_pending =\r\n!!(offload_status & OFFLOAD_ENABLED_BIT);\r\nif (pqi_get_raid_map(ctrl_info, device))\r\ndevice->offload_enabled_pending = false;\r\n}\r\nout:\r\nkfree(buffer);\r\n}\r\nstatic void pqi_get_volume_status(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nint rc;\r\nsize_t page_length;\r\nu8 volume_status = CISS_LV_STATUS_UNAVAILABLE;\r\nbool volume_offline = true;\r\nu32 volume_flags;\r\nstruct ciss_vpd_logical_volume_status *vpd;\r\nvpd = kmalloc(sizeof(*vpd), GFP_KERNEL);\r\nif (!vpd)\r\ngoto no_buffer;\r\nrc = pqi_scsi_inquiry(ctrl_info, device->scsi3addr,\r\nVPD_PAGE | CISS_VPD_LV_STATUS, vpd, sizeof(*vpd));\r\nif (rc)\r\ngoto out;\r\npage_length = offsetof(struct ciss_vpd_logical_volume_status,\r\nvolume_status) + vpd->page_length;\r\nif (page_length < sizeof(*vpd))\r\ngoto out;\r\nvolume_status = vpd->volume_status;\r\nvolume_flags = get_unaligned_be32(&vpd->flags);\r\nvolume_offline = (volume_flags & CISS_LV_FLAGS_NO_HOST_IO) != 0;\r\nout:\r\nkfree(vpd);\r\nno_buffer:\r\ndevice->volume_status = volume_status;\r\ndevice->volume_offline = volume_offline;\r\n}\r\nstatic int pqi_get_device_info(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nint rc;\r\nu8 *buffer;\r\nbuffer = kmalloc(64, GFP_KERNEL);\r\nif (!buffer)\r\nreturn -ENOMEM;\r\nrc = pqi_scsi_inquiry(ctrl_info, device->scsi3addr, 0, buffer, 64);\r\nif (rc)\r\ngoto out;\r\nscsi_sanitize_inquiry_string(&buffer[8], 8);\r\nscsi_sanitize_inquiry_string(&buffer[16], 16);\r\ndevice->devtype = buffer[0] & 0x1f;\r\nmemcpy(device->vendor, &buffer[8],\r\nsizeof(device->vendor));\r\nmemcpy(device->model, &buffer[16],\r\nsizeof(device->model));\r\nif (pqi_is_logical_device(device) && device->devtype == TYPE_DISK) {\r\npqi_get_raid_level(ctrl_info, device);\r\npqi_get_offload_status(ctrl_info, device);\r\npqi_get_volume_status(ctrl_info, device);\r\n}\r\nout:\r\nkfree(buffer);\r\nreturn rc;\r\n}\r\nstatic void pqi_get_physical_disk_info(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device,\r\nstruct bmic_identify_physical_device *id_phys)\r\n{\r\nint rc;\r\nmemset(id_phys, 0, sizeof(*id_phys));\r\nrc = pqi_identify_physical_device(ctrl_info, device,\r\nid_phys, sizeof(*id_phys));\r\nif (rc) {\r\ndevice->queue_depth = PQI_PHYSICAL_DISK_DEFAULT_MAX_QUEUE_DEPTH;\r\nreturn;\r\n}\r\ndevice->queue_depth =\r\nget_unaligned_le16(&id_phys->current_queue_depth_limit);\r\ndevice->device_type = id_phys->device_type;\r\ndevice->active_path_index = id_phys->active_path_number;\r\ndevice->path_map = id_phys->redundant_path_present_map;\r\nmemcpy(&device->box,\r\n&id_phys->alternate_paths_phys_box_on_port,\r\nsizeof(device->box));\r\nmemcpy(&device->phys_connector,\r\n&id_phys->alternate_paths_phys_connector,\r\nsizeof(device->phys_connector));\r\ndevice->bay = id_phys->phys_bay_in_box;\r\n}\r\nstatic void pqi_show_volume_status(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nchar *status;\r\nstatic const char unknown_state_str[] =\r\n"Volume is in an unknown state (%u)";\r\nchar unknown_state_buffer[sizeof(unknown_state_str) + 10];\r\nswitch (device->volume_status) {\r\ncase CISS_LV_OK:\r\nstatus = "Volume online";\r\nbreak;\r\ncase CISS_LV_FAILED:\r\nstatus = "Volume failed";\r\nbreak;\r\ncase CISS_LV_NOT_CONFIGURED:\r\nstatus = "Volume not configured";\r\nbreak;\r\ncase CISS_LV_DEGRADED:\r\nstatus = "Volume degraded";\r\nbreak;\r\ncase CISS_LV_READY_FOR_RECOVERY:\r\nstatus = "Volume ready for recovery operation";\r\nbreak;\r\ncase CISS_LV_UNDERGOING_RECOVERY:\r\nstatus = "Volume undergoing recovery";\r\nbreak;\r\ncase CISS_LV_WRONG_PHYSICAL_DRIVE_REPLACED:\r\nstatus = "Wrong physical drive was replaced";\r\nbreak;\r\ncase CISS_LV_PHYSICAL_DRIVE_CONNECTION_PROBLEM:\r\nstatus = "A physical drive not properly connected";\r\nbreak;\r\ncase CISS_LV_HARDWARE_OVERHEATING:\r\nstatus = "Hardware is overheating";\r\nbreak;\r\ncase CISS_LV_HARDWARE_HAS_OVERHEATED:\r\nstatus = "Hardware has overheated";\r\nbreak;\r\ncase CISS_LV_UNDERGOING_EXPANSION:\r\nstatus = "Volume undergoing expansion";\r\nbreak;\r\ncase CISS_LV_NOT_AVAILABLE:\r\nstatus = "Volume waiting for transforming volume";\r\nbreak;\r\ncase CISS_LV_QUEUED_FOR_EXPANSION:\r\nstatus = "Volume queued for expansion";\r\nbreak;\r\ncase CISS_LV_DISABLED_SCSI_ID_CONFLICT:\r\nstatus = "Volume disabled due to SCSI ID conflict";\r\nbreak;\r\ncase CISS_LV_EJECTED:\r\nstatus = "Volume has been ejected";\r\nbreak;\r\ncase CISS_LV_UNDERGOING_ERASE:\r\nstatus = "Volume undergoing background erase";\r\nbreak;\r\ncase CISS_LV_READY_FOR_PREDICTIVE_SPARE_REBUILD:\r\nstatus = "Volume ready for predictive spare rebuild";\r\nbreak;\r\ncase CISS_LV_UNDERGOING_RPI:\r\nstatus = "Volume undergoing rapid parity initialization";\r\nbreak;\r\ncase CISS_LV_PENDING_RPI:\r\nstatus = "Volume queued for rapid parity initialization";\r\nbreak;\r\ncase CISS_LV_ENCRYPTED_NO_KEY:\r\nstatus = "Encrypted volume inaccessible - key not present";\r\nbreak;\r\ncase CISS_LV_UNDERGOING_ENCRYPTION:\r\nstatus = "Volume undergoing encryption process";\r\nbreak;\r\ncase CISS_LV_UNDERGOING_ENCRYPTION_REKEYING:\r\nstatus = "Volume undergoing encryption re-keying process";\r\nbreak;\r\ncase CISS_LV_ENCRYPTED_IN_NON_ENCRYPTED_CONTROLLER:\r\nstatus =\r\n"Encrypted volume inaccessible - disabled on ctrl";\r\nbreak;\r\ncase CISS_LV_PENDING_ENCRYPTION:\r\nstatus = "Volume pending migration to encrypted state";\r\nbreak;\r\ncase CISS_LV_PENDING_ENCRYPTION_REKEYING:\r\nstatus = "Volume pending encryption rekeying";\r\nbreak;\r\ncase CISS_LV_NOT_SUPPORTED:\r\nstatus = "Volume not supported on this controller";\r\nbreak;\r\ncase CISS_LV_STATUS_UNAVAILABLE:\r\nstatus = "Volume status not available";\r\nbreak;\r\ndefault:\r\nsnprintf(unknown_state_buffer, sizeof(unknown_state_buffer),\r\nunknown_state_str, device->volume_status);\r\nstatus = unknown_state_buffer;\r\nbreak;\r\n}\r\ndev_info(&ctrl_info->pci_dev->dev,\r\n"scsi %d:%d:%d:%d %s\n",\r\nctrl_info->scsi_host->host_no,\r\ndevice->bus, device->target, device->lun, status);\r\n}\r\nstatic struct pqi_scsi_dev *pqi_find_disk_by_aio_handle(\r\nstruct pqi_ctrl_info *ctrl_info, u32 aio_handle)\r\n{\r\nstruct pqi_scsi_dev *device;\r\nlist_for_each_entry(device, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry) {\r\nif (device->devtype != TYPE_DISK && device->devtype != TYPE_ZBC)\r\ncontinue;\r\nif (pqi_is_logical_device(device))\r\ncontinue;\r\nif (device->aio_handle == aio_handle)\r\nreturn device;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void pqi_update_logical_drive_queue_depth(\r\nstruct pqi_ctrl_info *ctrl_info, struct pqi_scsi_dev *logical_drive)\r\n{\r\nunsigned int i;\r\nstruct raid_map *raid_map;\r\nstruct raid_map_disk_data *disk_data;\r\nstruct pqi_scsi_dev *phys_disk;\r\nunsigned int num_phys_disks;\r\nunsigned int num_raid_map_entries;\r\nunsigned int queue_depth;\r\nlogical_drive->queue_depth = PQI_LOGICAL_DRIVE_DEFAULT_MAX_QUEUE_DEPTH;\r\nraid_map = logical_drive->raid_map;\r\nif (!raid_map)\r\nreturn;\r\ndisk_data = raid_map->disk_data;\r\nnum_phys_disks = get_unaligned_le16(&raid_map->layout_map_count) *\r\n(get_unaligned_le16(&raid_map->data_disks_per_row) +\r\nget_unaligned_le16(&raid_map->metadata_disks_per_row));\r\nnum_raid_map_entries = num_phys_disks *\r\nget_unaligned_le16(&raid_map->row_cnt);\r\nqueue_depth = 0;\r\nfor (i = 0; i < num_raid_map_entries; i++) {\r\nphys_disk = pqi_find_disk_by_aio_handle(ctrl_info,\r\ndisk_data[i].aio_handle);\r\nif (!phys_disk) {\r\ndev_warn(&ctrl_info->pci_dev->dev,\r\n"failed to find physical disk for logical drive %016llx\n",\r\nget_unaligned_be64(logical_drive->scsi3addr));\r\nlogical_drive->offload_enabled = false;\r\nlogical_drive->offload_enabled_pending = false;\r\nkfree(raid_map);\r\nlogical_drive->raid_map = NULL;\r\nreturn;\r\n}\r\nqueue_depth += phys_disk->queue_depth;\r\n}\r\nlogical_drive->queue_depth = queue_depth;\r\n}\r\nstatic void pqi_update_all_logical_drive_queue_depths(\r\nstruct pqi_ctrl_info *ctrl_info)\r\n{\r\nstruct pqi_scsi_dev *device;\r\nlist_for_each_entry(device, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry) {\r\nif (device->devtype != TYPE_DISK && device->devtype != TYPE_ZBC)\r\ncontinue;\r\nif (!pqi_is_logical_device(device))\r\ncontinue;\r\npqi_update_logical_drive_queue_depth(ctrl_info, device);\r\n}\r\n}\r\nstatic void pqi_rescan_worker(struct work_struct *work)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info;\r\nctrl_info = container_of(to_delayed_work(work), struct pqi_ctrl_info,\r\nrescan_work);\r\npqi_scan_scsi_devices(ctrl_info);\r\n}\r\nstatic int pqi_add_device(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nint rc;\r\nif (pqi_is_logical_device(device))\r\nrc = scsi_add_device(ctrl_info->scsi_host, device->bus,\r\ndevice->target, device->lun);\r\nelse\r\nrc = pqi_add_sas_device(ctrl_info->sas_host, device);\r\nreturn rc;\r\n}\r\nstatic inline void pqi_remove_device(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nif (pqi_is_logical_device(device))\r\nscsi_remove_device(device->sdev);\r\nelse\r\npqi_remove_sas_device(device);\r\n}\r\nstatic struct pqi_scsi_dev *pqi_find_scsi_dev(struct pqi_ctrl_info *ctrl_info,\r\nint bus, int target, int lun)\r\n{\r\nstruct pqi_scsi_dev *device;\r\nlist_for_each_entry(device, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry)\r\nif (device->bus == bus && device->target == target &&\r\ndevice->lun == lun)\r\nreturn device;\r\nreturn NULL;\r\n}\r\nstatic inline bool pqi_device_equal(struct pqi_scsi_dev *dev1,\r\nstruct pqi_scsi_dev *dev2)\r\n{\r\nif (dev1->is_physical_device != dev2->is_physical_device)\r\nreturn false;\r\nif (dev1->is_physical_device)\r\nreturn dev1->wwid == dev2->wwid;\r\nreturn memcmp(dev1->volume_id, dev2->volume_id,\r\nsizeof(dev1->volume_id)) == 0;\r\n}\r\nstatic enum pqi_find_result pqi_scsi_find_entry(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device_to_find,\r\nstruct pqi_scsi_dev **matching_device)\r\n{\r\nstruct pqi_scsi_dev *device;\r\nlist_for_each_entry(device, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry) {\r\nif (pqi_scsi3addr_equal(device_to_find->scsi3addr,\r\ndevice->scsi3addr)) {\r\n*matching_device = device;\r\nif (pqi_device_equal(device_to_find, device)) {\r\nif (device_to_find->volume_offline)\r\nreturn DEVICE_CHANGED;\r\nreturn DEVICE_SAME;\r\n}\r\nreturn DEVICE_CHANGED;\r\n}\r\n}\r\nreturn DEVICE_NOT_FOUND;\r\n}\r\nstatic void pqi_dev_info(struct pqi_ctrl_info *ctrl_info,\r\nchar *action, struct pqi_scsi_dev *device)\r\n{\r\ndev_info(&ctrl_info->pci_dev->dev,\r\n"%s scsi %d:%d:%d:%d: %s %.8s %.16s %-12s SSDSmartPathCap%c En%c Exp%c qd=%d\n",\r\naction,\r\nctrl_info->scsi_host->host_no,\r\ndevice->bus,\r\ndevice->target,\r\ndevice->lun,\r\nscsi_device_type(device->devtype),\r\ndevice->vendor,\r\ndevice->model,\r\npqi_raid_level_to_string(device->raid_level),\r\ndevice->offload_configured ? '+' : '-',\r\ndevice->offload_enabled_pending ? '+' : '-',\r\ndevice->expose_device ? '+' : '-',\r\ndevice->queue_depth);\r\n}\r\nstatic void pqi_scsi_update_device(struct pqi_scsi_dev *existing_device,\r\nstruct pqi_scsi_dev *new_device)\r\n{\r\nexisting_device->devtype = new_device->devtype;\r\nexisting_device->device_type = new_device->device_type;\r\nexisting_device->bus = new_device->bus;\r\nif (new_device->target_lun_valid) {\r\nexisting_device->target = new_device->target;\r\nexisting_device->lun = new_device->lun;\r\nexisting_device->target_lun_valid = true;\r\n}\r\nexisting_device->is_physical_device = new_device->is_physical_device;\r\nexisting_device->expose_device = new_device->expose_device;\r\nexisting_device->no_uld_attach = new_device->no_uld_attach;\r\nexisting_device->aio_enabled = new_device->aio_enabled;\r\nmemcpy(existing_device->vendor, new_device->vendor,\r\nsizeof(existing_device->vendor));\r\nmemcpy(existing_device->model, new_device->model,\r\nsizeof(existing_device->model));\r\nexisting_device->sas_address = new_device->sas_address;\r\nexisting_device->raid_level = new_device->raid_level;\r\nexisting_device->queue_depth = new_device->queue_depth;\r\nexisting_device->aio_handle = new_device->aio_handle;\r\nexisting_device->volume_status = new_device->volume_status;\r\nexisting_device->active_path_index = new_device->active_path_index;\r\nexisting_device->path_map = new_device->path_map;\r\nexisting_device->bay = new_device->bay;\r\nmemcpy(existing_device->box, new_device->box,\r\nsizeof(existing_device->box));\r\nmemcpy(existing_device->phys_connector, new_device->phys_connector,\r\nsizeof(existing_device->phys_connector));\r\nexisting_device->offload_configured = new_device->offload_configured;\r\nexisting_device->offload_enabled = false;\r\nexisting_device->offload_enabled_pending =\r\nnew_device->offload_enabled_pending;\r\nexisting_device->offload_to_mirror = 0;\r\nkfree(existing_device->raid_map);\r\nexisting_device->raid_map = new_device->raid_map;\r\nnew_device->raid_map = NULL;\r\n}\r\nstatic inline void pqi_free_device(struct pqi_scsi_dev *device)\r\n{\r\nif (device) {\r\nkfree(device->raid_map);\r\nkfree(device);\r\n}\r\n}\r\nstatic inline void pqi_fixup_botched_add(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\r\nlist_del(&device->scsi_device_list_entry);\r\nspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\r\ndevice->keep_device = false;\r\n}\r\nstatic void pqi_update_device_list(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *new_device_list[], unsigned int num_new_devices)\r\n{\r\nint rc;\r\nunsigned int i;\r\nunsigned long flags;\r\nenum pqi_find_result find_result;\r\nstruct pqi_scsi_dev *device;\r\nstruct pqi_scsi_dev *next;\r\nstruct pqi_scsi_dev *matching_device;\r\nstruct list_head add_list;\r\nstruct list_head delete_list;\r\nINIT_LIST_HEAD(&add_list);\r\nINIT_LIST_HEAD(&delete_list);\r\nspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\r\nlist_for_each_entry(device, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry)\r\ndevice->device_gone = true;\r\nfor (i = 0; i < num_new_devices; i++) {\r\ndevice = new_device_list[i];\r\nfind_result = pqi_scsi_find_entry(ctrl_info, device,\r\n&matching_device);\r\nswitch (find_result) {\r\ncase DEVICE_SAME:\r\ndevice->new_device = false;\r\nmatching_device->device_gone = false;\r\npqi_scsi_update_device(matching_device, device);\r\nbreak;\r\ncase DEVICE_NOT_FOUND:\r\ndevice->new_device = true;\r\nbreak;\r\ncase DEVICE_CHANGED:\r\ndevice->new_device = true;\r\nbreak;\r\ndefault:\r\nWARN_ON(find_result);\r\nbreak;\r\n}\r\n}\r\nlist_for_each_entry_safe(device, next, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry) {\r\nif (device->device_gone) {\r\nlist_del(&device->scsi_device_list_entry);\r\nlist_add_tail(&device->delete_list_entry, &delete_list);\r\n}\r\n}\r\nfor (i = 0; i < num_new_devices; i++) {\r\ndevice = new_device_list[i];\r\nif (!device->new_device)\r\ncontinue;\r\nif (device->volume_offline)\r\ncontinue;\r\nlist_add_tail(&device->scsi_device_list_entry,\r\n&ctrl_info->scsi_device_list);\r\nlist_add_tail(&device->add_list_entry, &add_list);\r\ndevice->keep_device = true;\r\n}\r\npqi_update_all_logical_drive_queue_depths(ctrl_info);\r\nlist_for_each_entry(device, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry)\r\ndevice->offload_enabled =\r\ndevice->offload_enabled_pending;\r\nspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\r\nlist_for_each_entry_safe(device, next, &delete_list,\r\ndelete_list_entry) {\r\nif (device->sdev)\r\npqi_remove_device(ctrl_info, device);\r\nif (device->volume_offline) {\r\npqi_dev_info(ctrl_info, "offline", device);\r\npqi_show_volume_status(ctrl_info, device);\r\n} else {\r\npqi_dev_info(ctrl_info, "removed", device);\r\n}\r\nlist_del(&device->delete_list_entry);\r\npqi_free_device(device);\r\n}\r\nlist_for_each_entry(device, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry) {\r\nif (device->sdev && device->queue_depth !=\r\ndevice->advertised_queue_depth) {\r\ndevice->advertised_queue_depth = device->queue_depth;\r\nscsi_change_queue_depth(device->sdev,\r\ndevice->advertised_queue_depth);\r\n}\r\n}\r\nlist_for_each_entry_safe(device, next, &add_list, add_list_entry) {\r\nif (device->expose_device && !device->sdev) {\r\nrc = pqi_add_device(ctrl_info, device);\r\nif (rc) {\r\ndev_warn(&ctrl_info->pci_dev->dev,\r\n"scsi %d:%d:%d:%d addition failed, device not added\n",\r\nctrl_info->scsi_host->host_no,\r\ndevice->bus, device->target,\r\ndevice->lun);\r\npqi_fixup_botched_add(ctrl_info, device);\r\ncontinue;\r\n}\r\n}\r\npqi_dev_info(ctrl_info, "added", device);\r\n}\r\n}\r\nstatic bool pqi_is_supported_device(struct pqi_scsi_dev *device)\r\n{\r\nbool is_supported = false;\r\nswitch (device->devtype) {\r\ncase TYPE_DISK:\r\ncase TYPE_ZBC:\r\ncase TYPE_TAPE:\r\ncase TYPE_MEDIUM_CHANGER:\r\ncase TYPE_ENCLOSURE:\r\nis_supported = true;\r\nbreak;\r\ncase TYPE_RAID:\r\nif (pqi_is_hba_lunid(device->scsi3addr))\r\nis_supported = true;\r\nbreak;\r\n}\r\nreturn is_supported;\r\n}\r\nstatic inline bool pqi_skip_device(u8 *scsi3addr,\r\nstruct report_phys_lun_extended_entry *phys_lun_ext_entry)\r\n{\r\nu8 device_flags;\r\nif (!MASKED_DEVICE(scsi3addr))\r\nreturn false;\r\ndevice_flags = phys_lun_ext_entry->device_flags;\r\nif (device_flags & REPORT_PHYS_LUN_DEV_FLAG_NON_DISK) {\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic inline bool pqi_expose_device(struct pqi_scsi_dev *device)\r\n{\r\nif (device->is_physical_device && MASKED_DEVICE(device->scsi3addr))\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic int pqi_update_scsi_devices(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint i;\r\nint rc;\r\nstruct list_head new_device_list_head;\r\nstruct report_phys_lun_extended *physdev_list = NULL;\r\nstruct report_log_lun_extended *logdev_list = NULL;\r\nstruct report_phys_lun_extended_entry *phys_lun_ext_entry;\r\nstruct report_log_lun_extended_entry *log_lun_ext_entry;\r\nstruct bmic_identify_physical_device *id_phys = NULL;\r\nu32 num_physicals;\r\nu32 num_logicals;\r\nstruct pqi_scsi_dev **new_device_list = NULL;\r\nstruct pqi_scsi_dev *device;\r\nstruct pqi_scsi_dev *next;\r\nunsigned int num_new_devices;\r\nunsigned int num_valid_devices;\r\nbool is_physical_device;\r\nu8 *scsi3addr;\r\nstatic char *out_of_memory_msg =\r\n"out of memory, device discovery stopped";\r\nINIT_LIST_HEAD(&new_device_list_head);\r\nrc = pqi_get_device_lists(ctrl_info, &physdev_list, &logdev_list);\r\nif (rc)\r\ngoto out;\r\nif (physdev_list)\r\nnum_physicals =\r\nget_unaligned_be32(&physdev_list->header.list_length)\r\n/ sizeof(physdev_list->lun_entries[0]);\r\nelse\r\nnum_physicals = 0;\r\nif (logdev_list)\r\nnum_logicals =\r\nget_unaligned_be32(&logdev_list->header.list_length)\r\n/ sizeof(logdev_list->lun_entries[0]);\r\nelse\r\nnum_logicals = 0;\r\nif (num_physicals) {\r\nid_phys = kmalloc(sizeof(*id_phys), GFP_KERNEL);\r\nif (!id_phys) {\r\ndev_warn(&ctrl_info->pci_dev->dev, "%s\n",\r\nout_of_memory_msg);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\n}\r\nnum_new_devices = num_physicals + num_logicals;\r\nnew_device_list = kmalloc(sizeof(*new_device_list) *\r\nnum_new_devices, GFP_KERNEL);\r\nif (!new_device_list) {\r\ndev_warn(&ctrl_info->pci_dev->dev, "%s\n", out_of_memory_msg);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nfor (i = 0; i < num_new_devices; i++) {\r\ndevice = kzalloc(sizeof(*device), GFP_KERNEL);\r\nif (!device) {\r\ndev_warn(&ctrl_info->pci_dev->dev, "%s\n",\r\nout_of_memory_msg);\r\nrc = -ENOMEM;\r\ngoto out;\r\n}\r\nlist_add_tail(&device->new_device_list_entry,\r\n&new_device_list_head);\r\n}\r\ndevice = NULL;\r\nnum_valid_devices = 0;\r\nfor (i = 0; i < num_new_devices; i++) {\r\nif (i < num_physicals) {\r\nis_physical_device = true;\r\nphys_lun_ext_entry = &physdev_list->lun_entries[i];\r\nlog_lun_ext_entry = NULL;\r\nscsi3addr = phys_lun_ext_entry->lunid;\r\n} else {\r\nis_physical_device = false;\r\nphys_lun_ext_entry = NULL;\r\nlog_lun_ext_entry =\r\n&logdev_list->lun_entries[i - num_physicals];\r\nscsi3addr = log_lun_ext_entry->lunid;\r\n}\r\nif (is_physical_device &&\r\npqi_skip_device(scsi3addr, phys_lun_ext_entry))\r\ncontinue;\r\nif (device)\r\ndevice = list_next_entry(device, new_device_list_entry);\r\nelse\r\ndevice = list_first_entry(&new_device_list_head,\r\nstruct pqi_scsi_dev, new_device_list_entry);\r\nmemcpy(device->scsi3addr, scsi3addr, sizeof(device->scsi3addr));\r\ndevice->is_physical_device = is_physical_device;\r\ndevice->raid_level = SA_RAID_UNKNOWN;\r\nrc = pqi_get_device_info(ctrl_info, device);\r\nif (rc == -ENOMEM) {\r\ndev_warn(&ctrl_info->pci_dev->dev, "%s\n",\r\nout_of_memory_msg);\r\ngoto out;\r\n}\r\nif (rc) {\r\ndev_warn(&ctrl_info->pci_dev->dev,\r\n"obtaining device info failed, skipping device %016llx\n",\r\nget_unaligned_be64(device->scsi3addr));\r\nrc = 0;\r\ncontinue;\r\n}\r\nif (!pqi_is_supported_device(device))\r\ncontinue;\r\npqi_assign_bus_target_lun(device);\r\ndevice->expose_device = pqi_expose_device(device);\r\nif (device->is_physical_device) {\r\ndevice->wwid = phys_lun_ext_entry->wwid;\r\nif ((phys_lun_ext_entry->device_flags &\r\nREPORT_PHYS_LUN_DEV_FLAG_AIO_ENABLED) &&\r\nphys_lun_ext_entry->aio_handle)\r\ndevice->aio_enabled = true;\r\n} else {\r\nmemcpy(device->volume_id, log_lun_ext_entry->volume_id,\r\nsizeof(device->volume_id));\r\n}\r\nswitch (device->devtype) {\r\ncase TYPE_DISK:\r\ncase TYPE_ZBC:\r\ncase TYPE_ENCLOSURE:\r\nif (device->is_physical_device) {\r\ndevice->sas_address =\r\nget_unaligned_be64(&device->wwid);\r\nif (device->devtype == TYPE_DISK ||\r\ndevice->devtype == TYPE_ZBC) {\r\ndevice->aio_handle =\r\nphys_lun_ext_entry->aio_handle;\r\npqi_get_physical_disk_info(ctrl_info,\r\ndevice, id_phys);\r\n}\r\n}\r\nbreak;\r\n}\r\nnew_device_list[num_valid_devices++] = device;\r\n}\r\npqi_update_device_list(ctrl_info, new_device_list, num_valid_devices);\r\nout:\r\nlist_for_each_entry_safe(device, next, &new_device_list_head,\r\nnew_device_list_entry) {\r\nif (device->keep_device)\r\ncontinue;\r\nlist_del(&device->new_device_list_entry);\r\npqi_free_device(device);\r\n}\r\nkfree(new_device_list);\r\nkfree(physdev_list);\r\nkfree(logdev_list);\r\nkfree(id_phys);\r\nreturn rc;\r\n}\r\nstatic void pqi_remove_all_scsi_devices(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nunsigned long flags;\r\nstruct pqi_scsi_dev *device;\r\nstruct pqi_scsi_dev *next;\r\nspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\r\nlist_for_each_entry_safe(device, next, &ctrl_info->scsi_device_list,\r\nscsi_device_list_entry) {\r\nif (device->sdev)\r\npqi_remove_device(ctrl_info, device);\r\nlist_del(&device->scsi_device_list_entry);\r\npqi_free_device(device);\r\n}\r\nspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\r\n}\r\nstatic int pqi_scan_scsi_devices(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nif (pqi_ctrl_offline(ctrl_info))\r\nreturn -ENXIO;\r\nmutex_lock(&ctrl_info->scan_mutex);\r\nrc = pqi_update_scsi_devices(ctrl_info);\r\nif (rc)\r\npqi_schedule_rescan_worker(ctrl_info);\r\nmutex_unlock(&ctrl_info->scan_mutex);\r\nreturn rc;\r\n}\r\nstatic void pqi_scan_start(struct Scsi_Host *shost)\r\n{\r\npqi_scan_scsi_devices(shost_to_hba(shost));\r\n}\r\nstatic int pqi_scan_finished(struct Scsi_Host *shost,\r\nunsigned long elapsed_time)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info;\r\nctrl_info = shost_priv(shost);\r\nreturn !mutex_is_locked(&ctrl_info->scan_mutex);\r\n}\r\nstatic inline void pqi_set_encryption_info(\r\nstruct pqi_encryption_info *encryption_info, struct raid_map *raid_map,\r\nu64 first_block)\r\n{\r\nu32 volume_blk_size;\r\nvolume_blk_size = get_unaligned_le32(&raid_map->volume_blk_size);\r\nif (volume_blk_size != 512)\r\nfirst_block = (first_block * volume_blk_size) / 512;\r\nencryption_info->data_encryption_key_index =\r\nget_unaligned_le16(&raid_map->data_encryption_key_index);\r\nencryption_info->encrypt_tweak_lower = lower_32_bits(first_block);\r\nencryption_info->encrypt_tweak_upper = upper_32_bits(first_block);\r\n}\r\nstatic int pqi_raid_bypass_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device, struct scsi_cmnd *scmd,\r\nstruct pqi_queue_group *queue_group)\r\n{\r\nstruct raid_map *raid_map;\r\nbool is_write = false;\r\nu32 map_index;\r\nu64 first_block;\r\nu64 last_block;\r\nu32 block_cnt;\r\nu32 blocks_per_row;\r\nu64 first_row;\r\nu64 last_row;\r\nu32 first_row_offset;\r\nu32 last_row_offset;\r\nu32 first_column;\r\nu32 last_column;\r\nu64 r0_first_row;\r\nu64 r0_last_row;\r\nu32 r5or6_blocks_per_row;\r\nu64 r5or6_first_row;\r\nu64 r5or6_last_row;\r\nu32 r5or6_first_row_offset;\r\nu32 r5or6_last_row_offset;\r\nu32 r5or6_first_column;\r\nu32 r5or6_last_column;\r\nu16 data_disks_per_row;\r\nu32 total_disks_per_row;\r\nu16 layout_map_count;\r\nu32 stripesize;\r\nu16 strip_size;\r\nu32 first_group;\r\nu32 last_group;\r\nu32 current_group;\r\nu32 map_row;\r\nu32 aio_handle;\r\nu64 disk_block;\r\nu32 disk_block_cnt;\r\nu8 cdb[16];\r\nu8 cdb_length;\r\nint offload_to_mirror;\r\nstruct pqi_encryption_info *encryption_info_ptr;\r\nstruct pqi_encryption_info encryption_info;\r\n#if BITS_PER_LONG == 32\r\nu64 tmpdiv;\r\n#endif\r\nswitch (scmd->cmnd[0]) {\r\ncase WRITE_6:\r\nis_write = true;\r\ncase READ_6:\r\nfirst_block = (u64)(((scmd->cmnd[1] & 0x1f) << 16) |\r\n(scmd->cmnd[2] << 8) | scmd->cmnd[3]);\r\nblock_cnt = (u32)scmd->cmnd[4];\r\nif (block_cnt == 0)\r\nblock_cnt = 256;\r\nbreak;\r\ncase WRITE_10:\r\nis_write = true;\r\ncase READ_10:\r\nfirst_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);\r\nblock_cnt = (u32)get_unaligned_be16(&scmd->cmnd[7]);\r\nbreak;\r\ncase WRITE_12:\r\nis_write = true;\r\ncase READ_12:\r\nfirst_block = (u64)get_unaligned_be32(&scmd->cmnd[2]);\r\nblock_cnt = get_unaligned_be32(&scmd->cmnd[6]);\r\nbreak;\r\ncase WRITE_16:\r\nis_write = true;\r\ncase READ_16:\r\nfirst_block = get_unaligned_be64(&scmd->cmnd[2]);\r\nblock_cnt = get_unaligned_be32(&scmd->cmnd[10]);\r\nbreak;\r\ndefault:\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\n}\r\nif (is_write && device->raid_level != SA_RAID_0)\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\nif (unlikely(block_cnt == 0))\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\nlast_block = first_block + block_cnt - 1;\r\nraid_map = device->raid_map;\r\nif (last_block >= get_unaligned_le64(&raid_map->volume_blk_cnt) ||\r\nlast_block < first_block)\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\ndata_disks_per_row = get_unaligned_le16(&raid_map->data_disks_per_row);\r\nstrip_size = get_unaligned_le16(&raid_map->strip_size);\r\nlayout_map_count = get_unaligned_le16(&raid_map->layout_map_count);\r\nblocks_per_row = data_disks_per_row * strip_size;\r\n#if BITS_PER_LONG == 32\r\ntmpdiv = first_block;\r\ndo_div(tmpdiv, blocks_per_row);\r\nfirst_row = tmpdiv;\r\ntmpdiv = last_block;\r\ndo_div(tmpdiv, blocks_per_row);\r\nlast_row = tmpdiv;\r\nfirst_row_offset = (u32)(first_block - (first_row * blocks_per_row));\r\nlast_row_offset = (u32)(last_block - (last_row * blocks_per_row));\r\ntmpdiv = first_row_offset;\r\ndo_div(tmpdiv, strip_size);\r\nfirst_column = tmpdiv;\r\ntmpdiv = last_row_offset;\r\ndo_div(tmpdiv, strip_size);\r\nlast_column = tmpdiv;\r\n#else\r\nfirst_row = first_block / blocks_per_row;\r\nlast_row = last_block / blocks_per_row;\r\nfirst_row_offset = (u32)(first_block - (first_row * blocks_per_row));\r\nlast_row_offset = (u32)(last_block - (last_row * blocks_per_row));\r\nfirst_column = first_row_offset / strip_size;\r\nlast_column = last_row_offset / strip_size;\r\n#endif\r\nif (first_row != last_row || first_column != last_column)\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\ntotal_disks_per_row = data_disks_per_row +\r\nget_unaligned_le16(&raid_map->metadata_disks_per_row);\r\nmap_row = ((u32)(first_row >> raid_map->parity_rotation_shift)) %\r\nget_unaligned_le16(&raid_map->row_cnt);\r\nmap_index = (map_row * total_disks_per_row) + first_column;\r\nif (device->raid_level == SA_RAID_1) {\r\nif (device->offload_to_mirror)\r\nmap_index += data_disks_per_row;\r\ndevice->offload_to_mirror = !device->offload_to_mirror;\r\n} else if (device->raid_level == SA_RAID_ADM) {\r\noffload_to_mirror = device->offload_to_mirror;\r\nif (offload_to_mirror == 0) {\r\nmap_index %= data_disks_per_row;\r\n} else {\r\ndo {\r\ncurrent_group = map_index / data_disks_per_row;\r\nif (offload_to_mirror != current_group) {\r\nif (current_group <\r\nlayout_map_count - 1) {\r\nmap_index += data_disks_per_row;\r\ncurrent_group++;\r\n} else {\r\nmap_index %= data_disks_per_row;\r\ncurrent_group = 0;\r\n}\r\n}\r\n} while (offload_to_mirror != current_group);\r\n}\r\noffload_to_mirror =\r\n(offload_to_mirror >= layout_map_count - 1) ?\r\n0 : offload_to_mirror + 1;\r\nWARN_ON(offload_to_mirror >= layout_map_count);\r\ndevice->offload_to_mirror = offload_to_mirror;\r\n} else if ((device->raid_level == SA_RAID_5 ||\r\ndevice->raid_level == SA_RAID_6) && layout_map_count > 1) {\r\nr5or6_blocks_per_row = strip_size * data_disks_per_row;\r\nstripesize = r5or6_blocks_per_row * layout_map_count;\r\n#if BITS_PER_LONG == 32\r\ntmpdiv = first_block;\r\nfirst_group = do_div(tmpdiv, stripesize);\r\ntmpdiv = first_group;\r\ndo_div(tmpdiv, r5or6_blocks_per_row);\r\nfirst_group = tmpdiv;\r\ntmpdiv = last_block;\r\nlast_group = do_div(tmpdiv, stripesize);\r\ntmpdiv = last_group;\r\ndo_div(tmpdiv, r5or6_blocks_per_row);\r\nlast_group = tmpdiv;\r\n#else\r\nfirst_group = (first_block % stripesize) / r5or6_blocks_per_row;\r\nlast_group = (last_block % stripesize) / r5or6_blocks_per_row;\r\n#endif\r\nif (first_group != last_group)\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\n#if BITS_PER_LONG == 32\r\ntmpdiv = first_block;\r\ndo_div(tmpdiv, stripesize);\r\nfirst_row = r5or6_first_row = r0_first_row = tmpdiv;\r\ntmpdiv = last_block;\r\ndo_div(tmpdiv, stripesize);\r\nr5or6_last_row = r0_last_row = tmpdiv;\r\n#else\r\nfirst_row = r5or6_first_row = r0_first_row =\r\nfirst_block / stripesize;\r\nr5or6_last_row = r0_last_row = last_block / stripesize;\r\n#endif\r\nif (r5or6_first_row != r5or6_last_row)\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\n#if BITS_PER_LONG == 32\r\ntmpdiv = first_block;\r\nfirst_row_offset = do_div(tmpdiv, stripesize);\r\ntmpdiv = first_row_offset;\r\nfirst_row_offset = (u32)do_div(tmpdiv, r5or6_blocks_per_row);\r\nr5or6_first_row_offset = first_row_offset;\r\ntmpdiv = last_block;\r\nr5or6_last_row_offset = do_div(tmpdiv, stripesize);\r\ntmpdiv = r5or6_last_row_offset;\r\nr5or6_last_row_offset = do_div(tmpdiv, r5or6_blocks_per_row);\r\ntmpdiv = r5or6_first_row_offset;\r\ndo_div(tmpdiv, strip_size);\r\nfirst_column = r5or6_first_column = tmpdiv;\r\ntmpdiv = r5or6_last_row_offset;\r\ndo_div(tmpdiv, strip_size);\r\nr5or6_last_column = tmpdiv;\r\n#else\r\nfirst_row_offset = r5or6_first_row_offset =\r\n(u32)((first_block % stripesize) %\r\nr5or6_blocks_per_row);\r\nr5or6_last_row_offset =\r\n(u32)((last_block % stripesize) %\r\nr5or6_blocks_per_row);\r\nfirst_column = r5or6_first_row_offset / strip_size;\r\nr5or6_first_column = first_column;\r\nr5or6_last_column = r5or6_last_row_offset / strip_size;\r\n#endif\r\nif (r5or6_first_column != r5or6_last_column)\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\nmap_row =\r\n((u32)(first_row >> raid_map->parity_rotation_shift)) %\r\nget_unaligned_le16(&raid_map->row_cnt);\r\nmap_index = (first_group *\r\n(get_unaligned_le16(&raid_map->row_cnt) *\r\ntotal_disks_per_row)) +\r\n(map_row * total_disks_per_row) + first_column;\r\n}\r\nif (unlikely(map_index >= RAID_MAP_MAX_ENTRIES))\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\naio_handle = raid_map->disk_data[map_index].aio_handle;\r\ndisk_block = get_unaligned_le64(&raid_map->disk_starting_blk) +\r\nfirst_row * strip_size +\r\n(first_row_offset - first_column * strip_size);\r\ndisk_block_cnt = block_cnt;\r\nif (raid_map->phys_blk_shift) {\r\ndisk_block <<= raid_map->phys_blk_shift;\r\ndisk_block_cnt <<= raid_map->phys_blk_shift;\r\n}\r\nif (unlikely(disk_block_cnt > 0xffff))\r\nreturn PQI_RAID_BYPASS_INELIGIBLE;\r\nif (disk_block > 0xffffffff) {\r\ncdb[0] = is_write ? WRITE_16 : READ_16;\r\ncdb[1] = 0;\r\nput_unaligned_be64(disk_block, &cdb[2]);\r\nput_unaligned_be32(disk_block_cnt, &cdb[10]);\r\ncdb[14] = 0;\r\ncdb[15] = 0;\r\ncdb_length = 16;\r\n} else {\r\ncdb[0] = is_write ? WRITE_10 : READ_10;\r\ncdb[1] = 0;\r\nput_unaligned_be32((u32)disk_block, &cdb[2]);\r\ncdb[6] = 0;\r\nput_unaligned_be16((u16)disk_block_cnt, &cdb[7]);\r\ncdb[9] = 0;\r\ncdb_length = 10;\r\n}\r\nif (get_unaligned_le16(&raid_map->flags) &\r\nRAID_MAP_ENCRYPTION_ENABLED) {\r\npqi_set_encryption_info(&encryption_info, raid_map,\r\nfirst_block);\r\nencryption_info_ptr = &encryption_info;\r\n} else {\r\nencryption_info_ptr = NULL;\r\n}\r\nreturn pqi_aio_submit_io(ctrl_info, scmd, aio_handle,\r\ncdb, cdb_length, queue_group, encryption_info_ptr);\r\n}\r\nstatic int pqi_wait_for_pqi_mode_ready(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nstruct pqi_device_registers __iomem *pqi_registers;\r\nunsigned long timeout;\r\nu64 signature;\r\nu8 status;\r\npqi_registers = ctrl_info->pqi_registers;\r\ntimeout = (PQI_MODE_READY_TIMEOUT_SECS * HZ) + jiffies;\r\nwhile (1) {\r\nsignature = readq(&pqi_registers->signature);\r\nif (memcmp(&signature, PQI_DEVICE_SIGNATURE,\r\nsizeof(signature)) == 0)\r\nbreak;\r\nif (time_after(jiffies, timeout)) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"timed out waiting for PQI signature\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nmsleep(PQI_MODE_READY_POLL_INTERVAL_MSECS);\r\n}\r\nwhile (1) {\r\nstatus = readb(&pqi_registers->function_and_status_code);\r\nif (status == PQI_STATUS_IDLE)\r\nbreak;\r\nif (time_after(jiffies, timeout)) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"timed out waiting for PQI IDLE\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nmsleep(PQI_MODE_READY_POLL_INTERVAL_MSECS);\r\n}\r\nwhile (1) {\r\nif (readl(&pqi_registers->device_status) ==\r\nPQI_DEVICE_STATE_ALL_REGISTERS_READY)\r\nbreak;\r\nif (time_after(jiffies, timeout)) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"timed out waiting for PQI all registers ready\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nmsleep(PQI_MODE_READY_POLL_INTERVAL_MSECS);\r\n}\r\nreturn 0;\r\n}\r\nstatic inline void pqi_aio_path_disabled(struct pqi_io_request *io_request)\r\n{\r\nstruct pqi_scsi_dev *device;\r\ndevice = io_request->scmd->device->hostdata;\r\ndevice->offload_enabled = false;\r\n}\r\nstatic inline void pqi_take_device_offline(struct scsi_device *sdev)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info;\r\nstruct pqi_scsi_dev *device;\r\nif (scsi_device_online(sdev)) {\r\nscsi_device_set_state(sdev, SDEV_OFFLINE);\r\nctrl_info = shost_to_hba(sdev->host);\r\nschedule_delayed_work(&ctrl_info->rescan_work, 0);\r\ndevice = sdev->hostdata;\r\ndev_err(&ctrl_info->pci_dev->dev, "offlined scsi %d:%d:%d:%d\n",\r\nctrl_info->scsi_host->host_no, device->bus,\r\ndevice->target, device->lun);\r\n}\r\n}\r\nstatic void pqi_process_raid_io_error(struct pqi_io_request *io_request)\r\n{\r\nu8 scsi_status;\r\nu8 host_byte;\r\nstruct scsi_cmnd *scmd;\r\nstruct pqi_raid_error_info *error_info;\r\nsize_t sense_data_length;\r\nint residual_count;\r\nint xfer_count;\r\nstruct scsi_sense_hdr sshdr;\r\nscmd = io_request->scmd;\r\nif (!scmd)\r\nreturn;\r\nerror_info = io_request->error_info;\r\nscsi_status = error_info->status;\r\nhost_byte = DID_OK;\r\nif (error_info->data_out_result == PQI_DATA_IN_OUT_UNDERFLOW) {\r\nxfer_count =\r\nget_unaligned_le32(&error_info->data_out_transferred);\r\nresidual_count = scsi_bufflen(scmd) - xfer_count;\r\nscsi_set_resid(scmd, residual_count);\r\nif (xfer_count < scmd->underflow)\r\nhost_byte = DID_SOFT_ERROR;\r\n}\r\nsense_data_length = get_unaligned_le16(&error_info->sense_data_length);\r\nif (sense_data_length == 0)\r\nsense_data_length =\r\nget_unaligned_le16(&error_info->response_data_length);\r\nif (sense_data_length) {\r\nif (sense_data_length > sizeof(error_info->data))\r\nsense_data_length = sizeof(error_info->data);\r\nif (scsi_status == SAM_STAT_CHECK_CONDITION &&\r\nscsi_normalize_sense(error_info->data,\r\nsense_data_length, &sshdr) &&\r\nsshdr.sense_key == HARDWARE_ERROR &&\r\nsshdr.asc == 0x3e &&\r\nsshdr.ascq == 0x1) {\r\npqi_take_device_offline(scmd->device);\r\nhost_byte = DID_NO_CONNECT;\r\n}\r\nif (sense_data_length > SCSI_SENSE_BUFFERSIZE)\r\nsense_data_length = SCSI_SENSE_BUFFERSIZE;\r\nmemcpy(scmd->sense_buffer, error_info->data,\r\nsense_data_length);\r\n}\r\nscmd->result = scsi_status;\r\nset_host_byte(scmd, host_byte);\r\n}\r\nstatic void pqi_process_aio_io_error(struct pqi_io_request *io_request)\r\n{\r\nu8 scsi_status;\r\nu8 host_byte;\r\nstruct scsi_cmnd *scmd;\r\nstruct pqi_aio_error_info *error_info;\r\nsize_t sense_data_length;\r\nint residual_count;\r\nint xfer_count;\r\nbool device_offline;\r\nscmd = io_request->scmd;\r\nerror_info = io_request->error_info;\r\nhost_byte = DID_OK;\r\nsense_data_length = 0;\r\ndevice_offline = false;\r\nswitch (error_info->service_response) {\r\ncase PQI_AIO_SERV_RESPONSE_COMPLETE:\r\nscsi_status = error_info->status;\r\nbreak;\r\ncase PQI_AIO_SERV_RESPONSE_FAILURE:\r\nswitch (error_info->status) {\r\ncase PQI_AIO_STATUS_IO_ABORTED:\r\nscsi_status = SAM_STAT_TASK_ABORTED;\r\nbreak;\r\ncase PQI_AIO_STATUS_UNDERRUN:\r\nscsi_status = SAM_STAT_GOOD;\r\nresidual_count = get_unaligned_le32(\r\n&error_info->residual_count);\r\nscsi_set_resid(scmd, residual_count);\r\nxfer_count = scsi_bufflen(scmd) - residual_count;\r\nif (xfer_count < scmd->underflow)\r\nhost_byte = DID_SOFT_ERROR;\r\nbreak;\r\ncase PQI_AIO_STATUS_OVERRUN:\r\nscsi_status = SAM_STAT_GOOD;\r\nbreak;\r\ncase PQI_AIO_STATUS_AIO_PATH_DISABLED:\r\npqi_aio_path_disabled(io_request);\r\nscsi_status = SAM_STAT_GOOD;\r\nio_request->status = -EAGAIN;\r\nbreak;\r\ncase PQI_AIO_STATUS_NO_PATH_TO_DEVICE:\r\ncase PQI_AIO_STATUS_INVALID_DEVICE:\r\ndevice_offline = true;\r\npqi_take_device_offline(scmd->device);\r\nhost_byte = DID_NO_CONNECT;\r\nscsi_status = SAM_STAT_CHECK_CONDITION;\r\nbreak;\r\ncase PQI_AIO_STATUS_IO_ERROR:\r\ndefault:\r\nscsi_status = SAM_STAT_CHECK_CONDITION;\r\nbreak;\r\n}\r\nbreak;\r\ncase PQI_AIO_SERV_RESPONSE_TMF_COMPLETE:\r\ncase PQI_AIO_SERV_RESPONSE_TMF_SUCCEEDED:\r\nscsi_status = SAM_STAT_GOOD;\r\nbreak;\r\ncase PQI_AIO_SERV_RESPONSE_TMF_REJECTED:\r\ncase PQI_AIO_SERV_RESPONSE_TMF_INCORRECT_LUN:\r\ndefault:\r\nscsi_status = SAM_STAT_CHECK_CONDITION;\r\nbreak;\r\n}\r\nif (error_info->data_present) {\r\nsense_data_length =\r\nget_unaligned_le16(&error_info->data_length);\r\nif (sense_data_length) {\r\nif (sense_data_length > sizeof(error_info->data))\r\nsense_data_length = sizeof(error_info->data);\r\nif (sense_data_length > SCSI_SENSE_BUFFERSIZE)\r\nsense_data_length = SCSI_SENSE_BUFFERSIZE;\r\nmemcpy(scmd->sense_buffer, error_info->data,\r\nsense_data_length);\r\n}\r\n}\r\nif (device_offline && sense_data_length == 0)\r\nscsi_build_sense_buffer(0, scmd->sense_buffer, HARDWARE_ERROR,\r\n0x3e, 0x1);\r\nscmd->result = scsi_status;\r\nset_host_byte(scmd, host_byte);\r\n}\r\nstatic void pqi_process_io_error(unsigned int iu_type,\r\nstruct pqi_io_request *io_request)\r\n{\r\nswitch (iu_type) {\r\ncase PQI_RESPONSE_IU_RAID_PATH_IO_ERROR:\r\npqi_process_raid_io_error(io_request);\r\nbreak;\r\ncase PQI_RESPONSE_IU_AIO_PATH_IO_ERROR:\r\npqi_process_aio_io_error(io_request);\r\nbreak;\r\n}\r\n}\r\nstatic int pqi_interpret_task_management_response(\r\nstruct pqi_task_management_response *response)\r\n{\r\nint rc;\r\nswitch (response->response_code) {\r\ncase SOP_TMF_COMPLETE:\r\ncase SOP_TMF_FUNCTION_SUCCEEDED:\r\nrc = 0;\r\nbreak;\r\ndefault:\r\nrc = -EIO;\r\nbreak;\r\n}\r\nreturn rc;\r\n}\r\nstatic unsigned int pqi_process_io_intr(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_queue_group *queue_group)\r\n{\r\nunsigned int num_responses;\r\npqi_index_t oq_pi;\r\npqi_index_t oq_ci;\r\nstruct pqi_io_request *io_request;\r\nstruct pqi_io_response *response;\r\nu16 request_id;\r\nnum_responses = 0;\r\noq_ci = queue_group->oq_ci_copy;\r\nwhile (1) {\r\noq_pi = *queue_group->oq_pi;\r\nif (oq_pi == oq_ci)\r\nbreak;\r\nnum_responses++;\r\nresponse = queue_group->oq_element_array +\r\n(oq_ci * PQI_OPERATIONAL_OQ_ELEMENT_LENGTH);\r\nrequest_id = get_unaligned_le16(&response->request_id);\r\nWARN_ON(request_id >= ctrl_info->max_io_slots);\r\nio_request = &ctrl_info->io_request_pool[request_id];\r\nWARN_ON(atomic_read(&io_request->refcount) == 0);\r\nswitch (response->header.iu_type) {\r\ncase PQI_RESPONSE_IU_RAID_PATH_IO_SUCCESS:\r\ncase PQI_RESPONSE_IU_AIO_PATH_IO_SUCCESS:\r\ncase PQI_RESPONSE_IU_GENERAL_MANAGEMENT:\r\nbreak;\r\ncase PQI_RESPONSE_IU_TASK_MANAGEMENT:\r\nio_request->status =\r\npqi_interpret_task_management_response(\r\n(void *)response);\r\nbreak;\r\ncase PQI_RESPONSE_IU_AIO_PATH_DISABLED:\r\npqi_aio_path_disabled(io_request);\r\nio_request->status = -EAGAIN;\r\nbreak;\r\ncase PQI_RESPONSE_IU_RAID_PATH_IO_ERROR:\r\ncase PQI_RESPONSE_IU_AIO_PATH_IO_ERROR:\r\nio_request->error_info = ctrl_info->error_buffer +\r\n(get_unaligned_le16(&response->error_index) *\r\nPQI_ERROR_BUFFER_ELEMENT_LENGTH);\r\npqi_process_io_error(response->header.iu_type,\r\nio_request);\r\nbreak;\r\ndefault:\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"unexpected IU type: 0x%x\n",\r\nresponse->header.iu_type);\r\nWARN_ON(response->header.iu_type);\r\nbreak;\r\n}\r\nio_request->io_complete_callback(io_request,\r\nio_request->context);\r\noq_ci = (oq_ci + 1) % ctrl_info->num_elements_per_oq;\r\n}\r\nif (num_responses) {\r\nqueue_group->oq_ci_copy = oq_ci;\r\nwritel(oq_ci, queue_group->oq_ci);\r\n}\r\nreturn num_responses;\r\n}\r\nstatic inline unsigned int pqi_num_elements_free(unsigned int pi,\r\nunsigned int ci, unsigned int elements_in_queue)\r\n{\r\nunsigned int num_elements_used;\r\nif (pi >= ci)\r\nnum_elements_used = pi - ci;\r\nelse\r\nnum_elements_used = elements_in_queue - ci + pi;\r\nreturn elements_in_queue - num_elements_used - 1;\r\n}\r\nstatic void pqi_start_event_ack(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_event_acknowledge_request *iu, size_t iu_length)\r\n{\r\npqi_index_t iq_pi;\r\npqi_index_t iq_ci;\r\nunsigned long flags;\r\nvoid *next_element;\r\nunsigned long timeout;\r\nstruct pqi_queue_group *queue_group;\r\nqueue_group = &ctrl_info->queue_groups[PQI_DEFAULT_QUEUE_GROUP];\r\nput_unaligned_le16(queue_group->oq_id, &iu->header.response_queue_id);\r\ntimeout = (PQI_EVENT_ACK_TIMEOUT * HZ) + jiffies;\r\nwhile (1) {\r\nspin_lock_irqsave(&queue_group->submit_lock[RAID_PATH], flags);\r\niq_pi = queue_group->iq_pi_copy[RAID_PATH];\r\niq_ci = *queue_group->iq_ci[RAID_PATH];\r\nif (pqi_num_elements_free(iq_pi, iq_ci,\r\nctrl_info->num_elements_per_iq))\r\nbreak;\r\nspin_unlock_irqrestore(\r\n&queue_group->submit_lock[RAID_PATH], flags);\r\nif (time_after(jiffies, timeout)) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"sending event acknowledge timed out\n");\r\nreturn;\r\n}\r\n}\r\nnext_element = queue_group->iq_element_array[RAID_PATH] +\r\n(iq_pi * PQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\r\nmemcpy(next_element, iu, iu_length);\r\niq_pi = (iq_pi + 1) % ctrl_info->num_elements_per_iq;\r\nqueue_group->iq_pi_copy[RAID_PATH] = iq_pi;\r\nwritel(iq_pi, queue_group->iq_pi[RAID_PATH]);\r\nspin_unlock_irqrestore(&queue_group->submit_lock[RAID_PATH], flags);\r\n}\r\nstatic void pqi_acknowledge_event(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_event *event)\r\n{\r\nstruct pqi_event_acknowledge_request request;\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_ACKNOWLEDGE_VENDOR_EVENT;\r\nput_unaligned_le16(sizeof(request) - PQI_REQUEST_HEADER_LENGTH,\r\n&request.header.iu_length);\r\nrequest.event_type = event->event_type;\r\nrequest.event_id = event->event_id;\r\nrequest.additional_event_id = event->additional_event_id;\r\npqi_start_event_ack(ctrl_info, &request, sizeof(request));\r\n}\r\nstatic void pqi_event_worker(struct work_struct *work)\r\n{\r\nunsigned int i;\r\nstruct pqi_ctrl_info *ctrl_info;\r\nstruct pqi_event *pending_event;\r\nbool got_non_heartbeat_event = false;\r\nctrl_info = container_of(work, struct pqi_ctrl_info, event_work);\r\npending_event = ctrl_info->pending_events;\r\nfor (i = 0; i < PQI_NUM_SUPPORTED_EVENTS; i++) {\r\nif (pending_event->pending) {\r\npending_event->pending = false;\r\npqi_acknowledge_event(ctrl_info, pending_event);\r\nif (i != PQI_EVENT_HEARTBEAT)\r\ngot_non_heartbeat_event = true;\r\n}\r\npending_event++;\r\n}\r\nif (got_non_heartbeat_event)\r\npqi_schedule_rescan_worker(ctrl_info);\r\n}\r\nstatic void pqi_take_ctrl_offline(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nunsigned int i;\r\nunsigned int path;\r\nstruct pqi_queue_group *queue_group;\r\nunsigned long flags;\r\nstruct pqi_io_request *io_request;\r\nstruct pqi_io_request *next;\r\nstruct scsi_cmnd *scmd;\r\nctrl_info->controller_online = false;\r\ndev_err(&ctrl_info->pci_dev->dev, "controller offline\n");\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\r\nqueue_group = &ctrl_info->queue_groups[i];\r\nfor (path = 0; path < 2; path++) {\r\nspin_lock_irqsave(\r\n&queue_group->submit_lock[path], flags);\r\nlist_for_each_entry_safe(io_request, next,\r\n&queue_group->request_list[path],\r\nrequest_list_entry) {\r\nscmd = io_request->scmd;\r\nif (scmd) {\r\nset_host_byte(scmd, DID_NO_CONNECT);\r\npqi_scsi_done(scmd);\r\n}\r\nlist_del(&io_request->request_list_entry);\r\n}\r\nspin_unlock_irqrestore(\r\n&queue_group->submit_lock[path], flags);\r\n}\r\n}\r\n}\r\nstatic void pqi_heartbeat_timer_handler(unsigned long data)\r\n{\r\nint num_interrupts;\r\nstruct pqi_ctrl_info *ctrl_info = (struct pqi_ctrl_info *)data;\r\nnum_interrupts = atomic_read(&ctrl_info->num_interrupts);\r\nif (num_interrupts == ctrl_info->previous_num_interrupts) {\r\nctrl_info->num_heartbeats_requested++;\r\nif (ctrl_info->num_heartbeats_requested >\r\nPQI_MAX_HEARTBEAT_REQUESTS) {\r\npqi_take_ctrl_offline(ctrl_info);\r\nreturn;\r\n}\r\nctrl_info->pending_events[PQI_EVENT_HEARTBEAT].pending = true;\r\nschedule_work(&ctrl_info->event_work);\r\n} else {\r\nctrl_info->num_heartbeats_requested = 0;\r\n}\r\nctrl_info->previous_num_interrupts = num_interrupts;\r\nmod_timer(&ctrl_info->heartbeat_timer,\r\njiffies + PQI_HEARTBEAT_TIMER_INTERVAL);\r\n}\r\nstatic void pqi_start_heartbeat_timer(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nctrl_info->previous_num_interrupts =\r\natomic_read(&ctrl_info->num_interrupts);\r\ninit_timer(&ctrl_info->heartbeat_timer);\r\nctrl_info->heartbeat_timer.expires =\r\njiffies + PQI_HEARTBEAT_TIMER_INTERVAL;\r\nctrl_info->heartbeat_timer.data = (unsigned long)ctrl_info;\r\nctrl_info->heartbeat_timer.function = pqi_heartbeat_timer_handler;\r\nadd_timer(&ctrl_info->heartbeat_timer);\r\nctrl_info->heartbeat_timer_started = true;\r\n}\r\nstatic inline void pqi_stop_heartbeat_timer(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nif (ctrl_info->heartbeat_timer_started)\r\ndel_timer_sync(&ctrl_info->heartbeat_timer);\r\n}\r\nstatic int pqi_event_type_to_event_index(unsigned int event_type)\r\n{\r\nint index;\r\nswitch (event_type) {\r\ncase PQI_EVENT_TYPE_HEARTBEAT:\r\nindex = PQI_EVENT_HEARTBEAT;\r\nbreak;\r\ncase PQI_EVENT_TYPE_HOTPLUG:\r\nindex = PQI_EVENT_HOTPLUG;\r\nbreak;\r\ncase PQI_EVENT_TYPE_HARDWARE:\r\nindex = PQI_EVENT_HARDWARE;\r\nbreak;\r\ncase PQI_EVENT_TYPE_PHYSICAL_DEVICE:\r\nindex = PQI_EVENT_PHYSICAL_DEVICE;\r\nbreak;\r\ncase PQI_EVENT_TYPE_LOGICAL_DEVICE:\r\nindex = PQI_EVENT_LOGICAL_DEVICE;\r\nbreak;\r\ncase PQI_EVENT_TYPE_AIO_STATE_CHANGE:\r\nindex = PQI_EVENT_AIO_STATE_CHANGE;\r\nbreak;\r\ncase PQI_EVENT_TYPE_AIO_CONFIG_CHANGE:\r\nindex = PQI_EVENT_AIO_CONFIG_CHANGE;\r\nbreak;\r\ndefault:\r\nindex = -1;\r\nbreak;\r\n}\r\nreturn index;\r\n}\r\nstatic unsigned int pqi_process_event_intr(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nunsigned int num_events;\r\npqi_index_t oq_pi;\r\npqi_index_t oq_ci;\r\nstruct pqi_event_queue *event_queue;\r\nstruct pqi_event_response *response;\r\nstruct pqi_event *pending_event;\r\nbool need_delayed_work;\r\nint event_index;\r\nevent_queue = &ctrl_info->event_queue;\r\nnum_events = 0;\r\nneed_delayed_work = false;\r\noq_ci = event_queue->oq_ci_copy;\r\nwhile (1) {\r\noq_pi = *event_queue->oq_pi;\r\nif (oq_pi == oq_ci)\r\nbreak;\r\nnum_events++;\r\nresponse = event_queue->oq_element_array +\r\n(oq_ci * PQI_EVENT_OQ_ELEMENT_LENGTH);\r\nevent_index =\r\npqi_event_type_to_event_index(response->event_type);\r\nif (event_index >= 0) {\r\nif (response->request_acknowlege) {\r\npending_event =\r\n&ctrl_info->pending_events[event_index];\r\npending_event->event_type =\r\nresponse->event_type;\r\npending_event->event_id = response->event_id;\r\npending_event->additional_event_id =\r\nresponse->additional_event_id;\r\nif (event_index != PQI_EVENT_HEARTBEAT) {\r\npending_event->pending = true;\r\nneed_delayed_work = true;\r\n}\r\n}\r\n}\r\noq_ci = (oq_ci + 1) % PQI_NUM_EVENT_QUEUE_ELEMENTS;\r\n}\r\nif (num_events) {\r\nevent_queue->oq_ci_copy = oq_ci;\r\nwritel(oq_ci, event_queue->oq_ci);\r\nif (need_delayed_work)\r\nschedule_work(&ctrl_info->event_work);\r\n}\r\nreturn num_events;\r\n}\r\nstatic irqreturn_t pqi_irq_handler(int irq, void *data)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info;\r\nstruct pqi_queue_group *queue_group;\r\nunsigned int num_responses_handled;\r\nqueue_group = data;\r\nctrl_info = queue_group->ctrl_info;\r\nif (!ctrl_info || !queue_group->oq_ci)\r\nreturn IRQ_NONE;\r\nnum_responses_handled = pqi_process_io_intr(ctrl_info, queue_group);\r\nif (irq == ctrl_info->event_irq)\r\nnum_responses_handled += pqi_process_event_intr(ctrl_info);\r\nif (num_responses_handled)\r\natomic_inc(&ctrl_info->num_interrupts);\r\npqi_start_io(ctrl_info, queue_group, RAID_PATH, NULL);\r\npqi_start_io(ctrl_info, queue_group, AIO_PATH, NULL);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic int pqi_request_irqs(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nstruct pci_dev *pdev = ctrl_info->pci_dev;\r\nint i;\r\nint rc;\r\nctrl_info->event_irq = pci_irq_vector(pdev, 0);\r\nfor (i = 0; i < ctrl_info->num_msix_vectors_enabled; i++) {\r\nrc = request_irq(pci_irq_vector(pdev, i), pqi_irq_handler, 0,\r\nDRIVER_NAME_SHORT, &ctrl_info->queue_groups[i]);\r\nif (rc) {\r\ndev_err(&pdev->dev,\r\n"irq %u init failed with error %d\n",\r\npci_irq_vector(pdev, i), rc);\r\nreturn rc;\r\n}\r\nctrl_info->num_msix_vectors_initialized++;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pqi_enable_msix_interrupts(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint ret;\r\nret = pci_alloc_irq_vectors(ctrl_info->pci_dev,\r\nPQI_MIN_MSIX_VECTORS, ctrl_info->num_queue_groups,\r\nPCI_IRQ_MSIX | PCI_IRQ_AFFINITY);\r\nif (ret < 0) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"MSI-X init failed with error %d\n", ret);\r\nreturn ret;\r\n}\r\nctrl_info->num_msix_vectors_enabled = ret;\r\nreturn 0;\r\n}\r\nstatic int pqi_alloc_operational_queues(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nunsigned int i;\r\nsize_t alloc_length;\r\nsize_t element_array_length_per_iq;\r\nsize_t element_array_length_per_oq;\r\nvoid *element_array;\r\nvoid *next_queue_index;\r\nvoid *aligned_pointer;\r\nunsigned int num_inbound_queues;\r\nunsigned int num_outbound_queues;\r\nunsigned int num_queue_indexes;\r\nstruct pqi_queue_group *queue_group;\r\nelement_array_length_per_iq =\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH *\r\nctrl_info->num_elements_per_iq;\r\nelement_array_length_per_oq =\r\nPQI_OPERATIONAL_OQ_ELEMENT_LENGTH *\r\nctrl_info->num_elements_per_oq;\r\nnum_inbound_queues = ctrl_info->num_queue_groups * 2;\r\nnum_outbound_queues = ctrl_info->num_queue_groups;\r\nnum_queue_indexes = (ctrl_info->num_queue_groups * 3) + 1;\r\naligned_pointer = NULL;\r\nfor (i = 0; i < num_inbound_queues; i++) {\r\naligned_pointer = PTR_ALIGN(aligned_pointer,\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\r\naligned_pointer += element_array_length_per_iq;\r\n}\r\nfor (i = 0; i < num_outbound_queues; i++) {\r\naligned_pointer = PTR_ALIGN(aligned_pointer,\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\r\naligned_pointer += element_array_length_per_oq;\r\n}\r\naligned_pointer = PTR_ALIGN(aligned_pointer,\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\r\naligned_pointer += PQI_NUM_EVENT_QUEUE_ELEMENTS *\r\nPQI_EVENT_OQ_ELEMENT_LENGTH;\r\nfor (i = 0; i < num_queue_indexes; i++) {\r\naligned_pointer = PTR_ALIGN(aligned_pointer,\r\nPQI_OPERATIONAL_INDEX_ALIGNMENT);\r\naligned_pointer += sizeof(pqi_index_t);\r\n}\r\nalloc_length = (size_t)aligned_pointer +\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT;\r\nctrl_info->queue_memory_base =\r\ndma_zalloc_coherent(&ctrl_info->pci_dev->dev,\r\nalloc_length,\r\n&ctrl_info->queue_memory_base_dma_handle, GFP_KERNEL);\r\nif (!ctrl_info->queue_memory_base) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"failed to allocate memory for PQI admin queues\n");\r\nreturn -ENOMEM;\r\n}\r\nctrl_info->queue_memory_length = alloc_length;\r\nelement_array = PTR_ALIGN(ctrl_info->queue_memory_base,\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\r\nqueue_group = &ctrl_info->queue_groups[i];\r\nqueue_group->iq_element_array[RAID_PATH] = element_array;\r\nqueue_group->iq_element_array_bus_addr[RAID_PATH] =\r\nctrl_info->queue_memory_base_dma_handle +\r\n(element_array - ctrl_info->queue_memory_base);\r\nelement_array += element_array_length_per_iq;\r\nelement_array = PTR_ALIGN(element_array,\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\r\nqueue_group->iq_element_array[AIO_PATH] = element_array;\r\nqueue_group->iq_element_array_bus_addr[AIO_PATH] =\r\nctrl_info->queue_memory_base_dma_handle +\r\n(element_array - ctrl_info->queue_memory_base);\r\nelement_array += element_array_length_per_iq;\r\nelement_array = PTR_ALIGN(element_array,\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\r\n}\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\r\nqueue_group = &ctrl_info->queue_groups[i];\r\nqueue_group->oq_element_array = element_array;\r\nqueue_group->oq_element_array_bus_addr =\r\nctrl_info->queue_memory_base_dma_handle +\r\n(element_array - ctrl_info->queue_memory_base);\r\nelement_array += element_array_length_per_oq;\r\nelement_array = PTR_ALIGN(element_array,\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\r\n}\r\nctrl_info->event_queue.oq_element_array = element_array;\r\nctrl_info->event_queue.oq_element_array_bus_addr =\r\nctrl_info->queue_memory_base_dma_handle +\r\n(element_array - ctrl_info->queue_memory_base);\r\nelement_array += PQI_NUM_EVENT_QUEUE_ELEMENTS *\r\nPQI_EVENT_OQ_ELEMENT_LENGTH;\r\nnext_queue_index = PTR_ALIGN(element_array,\r\nPQI_OPERATIONAL_INDEX_ALIGNMENT);\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\r\nqueue_group = &ctrl_info->queue_groups[i];\r\nqueue_group->iq_ci[RAID_PATH] = next_queue_index;\r\nqueue_group->iq_ci_bus_addr[RAID_PATH] =\r\nctrl_info->queue_memory_base_dma_handle +\r\n(next_queue_index - ctrl_info->queue_memory_base);\r\nnext_queue_index += sizeof(pqi_index_t);\r\nnext_queue_index = PTR_ALIGN(next_queue_index,\r\nPQI_OPERATIONAL_INDEX_ALIGNMENT);\r\nqueue_group->iq_ci[AIO_PATH] = next_queue_index;\r\nqueue_group->iq_ci_bus_addr[AIO_PATH] =\r\nctrl_info->queue_memory_base_dma_handle +\r\n(next_queue_index - ctrl_info->queue_memory_base);\r\nnext_queue_index += sizeof(pqi_index_t);\r\nnext_queue_index = PTR_ALIGN(next_queue_index,\r\nPQI_OPERATIONAL_INDEX_ALIGNMENT);\r\nqueue_group->oq_pi = next_queue_index;\r\nqueue_group->oq_pi_bus_addr =\r\nctrl_info->queue_memory_base_dma_handle +\r\n(next_queue_index - ctrl_info->queue_memory_base);\r\nnext_queue_index += sizeof(pqi_index_t);\r\nnext_queue_index = PTR_ALIGN(next_queue_index,\r\nPQI_OPERATIONAL_INDEX_ALIGNMENT);\r\n}\r\nctrl_info->event_queue.oq_pi = next_queue_index;\r\nctrl_info->event_queue.oq_pi_bus_addr =\r\nctrl_info->queue_memory_base_dma_handle +\r\n(next_queue_index - ctrl_info->queue_memory_base);\r\nreturn 0;\r\n}\r\nstatic void pqi_init_operational_queues(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nunsigned int i;\r\nu16 next_iq_id = PQI_MIN_OPERATIONAL_QUEUE_ID;\r\nu16 next_oq_id = PQI_MIN_OPERATIONAL_QUEUE_ID;\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++)\r\nctrl_info->queue_groups[i].ctrl_info = ctrl_info;\r\nctrl_info->event_queue.oq_id = next_oq_id++;\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\r\nctrl_info->queue_groups[i].iq_id[RAID_PATH] = next_iq_id++;\r\nctrl_info->queue_groups[i].iq_id[AIO_PATH] = next_iq_id++;\r\nctrl_info->queue_groups[i].oq_id = next_oq_id++;\r\n}\r\nctrl_info->event_queue.int_msg_num = 0;\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++)\r\nctrl_info->queue_groups[i].int_msg_num = i;\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\r\nspin_lock_init(&ctrl_info->queue_groups[i].submit_lock[0]);\r\nspin_lock_init(&ctrl_info->queue_groups[i].submit_lock[1]);\r\nINIT_LIST_HEAD(&ctrl_info->queue_groups[i].request_list[0]);\r\nINIT_LIST_HEAD(&ctrl_info->queue_groups[i].request_list[1]);\r\n}\r\n}\r\nstatic int pqi_alloc_admin_queues(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nsize_t alloc_length;\r\nstruct pqi_admin_queues_aligned *admin_queues_aligned;\r\nstruct pqi_admin_queues *admin_queues;\r\nalloc_length = sizeof(struct pqi_admin_queues_aligned) +\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT;\r\nctrl_info->admin_queue_memory_base =\r\ndma_zalloc_coherent(&ctrl_info->pci_dev->dev,\r\nalloc_length,\r\n&ctrl_info->admin_queue_memory_base_dma_handle,\r\nGFP_KERNEL);\r\nif (!ctrl_info->admin_queue_memory_base)\r\nreturn -ENOMEM;\r\nctrl_info->admin_queue_memory_length = alloc_length;\r\nadmin_queues = &ctrl_info->admin_queues;\r\nadmin_queues_aligned = PTR_ALIGN(ctrl_info->admin_queue_memory_base,\r\nPQI_QUEUE_ELEMENT_ARRAY_ALIGNMENT);\r\nadmin_queues->iq_element_array =\r\n&admin_queues_aligned->iq_element_array;\r\nadmin_queues->oq_element_array =\r\n&admin_queues_aligned->oq_element_array;\r\nadmin_queues->iq_ci = &admin_queues_aligned->iq_ci;\r\nadmin_queues->oq_pi = &admin_queues_aligned->oq_pi;\r\nadmin_queues->iq_element_array_bus_addr =\r\nctrl_info->admin_queue_memory_base_dma_handle +\r\n(admin_queues->iq_element_array -\r\nctrl_info->admin_queue_memory_base);\r\nadmin_queues->oq_element_array_bus_addr =\r\nctrl_info->admin_queue_memory_base_dma_handle +\r\n(admin_queues->oq_element_array -\r\nctrl_info->admin_queue_memory_base);\r\nadmin_queues->iq_ci_bus_addr =\r\nctrl_info->admin_queue_memory_base_dma_handle +\r\n((void *)admin_queues->iq_ci -\r\nctrl_info->admin_queue_memory_base);\r\nadmin_queues->oq_pi_bus_addr =\r\nctrl_info->admin_queue_memory_base_dma_handle +\r\n((void *)admin_queues->oq_pi -\r\nctrl_info->admin_queue_memory_base);\r\nreturn 0;\r\n}\r\nstatic int pqi_create_admin_queues(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nstruct pqi_device_registers __iomem *pqi_registers;\r\nstruct pqi_admin_queues *admin_queues;\r\nunsigned long timeout;\r\nu8 status;\r\nu32 reg;\r\npqi_registers = ctrl_info->pqi_registers;\r\nadmin_queues = &ctrl_info->admin_queues;\r\nwriteq((u64)admin_queues->iq_element_array_bus_addr,\r\n&pqi_registers->admin_iq_element_array_addr);\r\nwriteq((u64)admin_queues->oq_element_array_bus_addr,\r\n&pqi_registers->admin_oq_element_array_addr);\r\nwriteq((u64)admin_queues->iq_ci_bus_addr,\r\n&pqi_registers->admin_iq_ci_addr);\r\nwriteq((u64)admin_queues->oq_pi_bus_addr,\r\n&pqi_registers->admin_oq_pi_addr);\r\nreg = PQI_ADMIN_IQ_NUM_ELEMENTS |\r\n(PQI_ADMIN_OQ_NUM_ELEMENTS) << 8 |\r\n(admin_queues->int_msg_num << 16);\r\nwritel(reg, &pqi_registers->admin_iq_num_elements);\r\nwritel(PQI_CREATE_ADMIN_QUEUE_PAIR,\r\n&pqi_registers->function_and_status_code);\r\ntimeout = PQI_ADMIN_QUEUE_CREATE_TIMEOUT_JIFFIES + jiffies;\r\nwhile (1) {\r\nstatus = readb(&pqi_registers->function_and_status_code);\r\nif (status == PQI_STATUS_IDLE)\r\nbreak;\r\nif (time_after(jiffies, timeout))\r\nreturn -ETIMEDOUT;\r\nmsleep(PQI_ADMIN_QUEUE_CREATE_POLL_INTERVAL_MSECS);\r\n}\r\nadmin_queues->iq_pi = ctrl_info->iomem_base +\r\nPQI_DEVICE_REGISTERS_OFFSET +\r\nreadq(&pqi_registers->admin_iq_pi_offset);\r\nadmin_queues->oq_ci = ctrl_info->iomem_base +\r\nPQI_DEVICE_REGISTERS_OFFSET +\r\nreadq(&pqi_registers->admin_oq_ci_offset);\r\nreturn 0;\r\n}\r\nstatic void pqi_submit_admin_request(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_general_admin_request *request)\r\n{\r\nstruct pqi_admin_queues *admin_queues;\r\nvoid *next_element;\r\npqi_index_t iq_pi;\r\nadmin_queues = &ctrl_info->admin_queues;\r\niq_pi = admin_queues->iq_pi_copy;\r\nnext_element = admin_queues->iq_element_array +\r\n(iq_pi * PQI_ADMIN_IQ_ELEMENT_LENGTH);\r\nmemcpy(next_element, request, sizeof(*request));\r\niq_pi = (iq_pi + 1) % PQI_ADMIN_IQ_NUM_ELEMENTS;\r\nadmin_queues->iq_pi_copy = iq_pi;\r\nwritel(iq_pi, admin_queues->iq_pi);\r\n}\r\nstatic int pqi_poll_for_admin_response(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_general_admin_response *response)\r\n{\r\nstruct pqi_admin_queues *admin_queues;\r\npqi_index_t oq_pi;\r\npqi_index_t oq_ci;\r\nunsigned long timeout;\r\nadmin_queues = &ctrl_info->admin_queues;\r\noq_ci = admin_queues->oq_ci_copy;\r\ntimeout = (3 * HZ) + jiffies;\r\nwhile (1) {\r\noq_pi = *admin_queues->oq_pi;\r\nif (oq_pi != oq_ci)\r\nbreak;\r\nif (time_after(jiffies, timeout)) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"timed out waiting for admin response\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nusleep_range(1000, 2000);\r\n}\r\nmemcpy(response, admin_queues->oq_element_array +\r\n(oq_ci * PQI_ADMIN_OQ_ELEMENT_LENGTH), sizeof(*response));\r\noq_ci = (oq_ci + 1) % PQI_ADMIN_OQ_NUM_ELEMENTS;\r\nadmin_queues->oq_ci_copy = oq_ci;\r\nwritel(oq_ci, admin_queues->oq_ci);\r\nreturn 0;\r\n}\r\nstatic void pqi_start_io(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_queue_group *queue_group, enum pqi_io_path path,\r\nstruct pqi_io_request *io_request)\r\n{\r\nstruct pqi_io_request *next;\r\nvoid *next_element;\r\npqi_index_t iq_pi;\r\npqi_index_t iq_ci;\r\nsize_t iu_length;\r\nunsigned long flags;\r\nunsigned int num_elements_needed;\r\nunsigned int num_elements_to_end_of_queue;\r\nsize_t copy_count;\r\nstruct pqi_iu_header *request;\r\nspin_lock_irqsave(&queue_group->submit_lock[path], flags);\r\nif (io_request)\r\nlist_add_tail(&io_request->request_list_entry,\r\n&queue_group->request_list[path]);\r\niq_pi = queue_group->iq_pi_copy[path];\r\nlist_for_each_entry_safe(io_request, next,\r\n&queue_group->request_list[path], request_list_entry) {\r\nrequest = io_request->iu;\r\niu_length = get_unaligned_le16(&request->iu_length) +\r\nPQI_REQUEST_HEADER_LENGTH;\r\nnum_elements_needed =\r\nDIV_ROUND_UP(iu_length,\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\r\niq_ci = *queue_group->iq_ci[path];\r\nif (num_elements_needed > pqi_num_elements_free(iq_pi, iq_ci,\r\nctrl_info->num_elements_per_iq))\r\nbreak;\r\nput_unaligned_le16(queue_group->oq_id,\r\n&request->response_queue_id);\r\nnext_element = queue_group->iq_element_array[path] +\r\n(iq_pi * PQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\r\nnum_elements_to_end_of_queue =\r\nctrl_info->num_elements_per_iq - iq_pi;\r\nif (num_elements_needed <= num_elements_to_end_of_queue) {\r\nmemcpy(next_element, request, iu_length);\r\n} else {\r\ncopy_count = num_elements_to_end_of_queue *\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH;\r\nmemcpy(next_element, request, copy_count);\r\nmemcpy(queue_group->iq_element_array[path],\r\n(u8 *)request + copy_count,\r\niu_length - copy_count);\r\n}\r\niq_pi = (iq_pi + num_elements_needed) %\r\nctrl_info->num_elements_per_iq;\r\nlist_del(&io_request->request_list_entry);\r\n}\r\nif (iq_pi != queue_group->iq_pi_copy[path]) {\r\nqueue_group->iq_pi_copy[path] = iq_pi;\r\nwritel(iq_pi, queue_group->iq_pi[path]);\r\n}\r\nspin_unlock_irqrestore(&queue_group->submit_lock[path], flags);\r\n}\r\nstatic void pqi_raid_synchronous_complete(struct pqi_io_request *io_request,\r\nvoid *context)\r\n{\r\nstruct completion *waiting = context;\r\ncomplete(waiting);\r\n}\r\nstatic int pqi_submit_raid_request_synchronous_with_io_request(\r\nstruct pqi_ctrl_info *ctrl_info, struct pqi_io_request *io_request,\r\nunsigned long timeout_msecs)\r\n{\r\nint rc = 0;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nio_request->io_complete_callback = pqi_raid_synchronous_complete;\r\nio_request->context = &wait;\r\npqi_start_io(ctrl_info,\r\n&ctrl_info->queue_groups[PQI_DEFAULT_QUEUE_GROUP], RAID_PATH,\r\nio_request);\r\nif (timeout_msecs == NO_TIMEOUT) {\r\nwait_for_completion_io(&wait);\r\n} else {\r\nif (!wait_for_completion_io_timeout(&wait,\r\nmsecs_to_jiffies(timeout_msecs))) {\r\ndev_warn(&ctrl_info->pci_dev->dev,\r\n"command timed out\n");\r\nrc = -ETIMEDOUT;\r\n}\r\n}\r\nreturn rc;\r\n}\r\nstatic int pqi_submit_raid_request_synchronous(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_iu_header *request, unsigned int flags,\r\nstruct pqi_raid_error_info *error_info, unsigned long timeout_msecs)\r\n{\r\nint rc;\r\nstruct pqi_io_request *io_request;\r\nunsigned long start_jiffies;\r\nunsigned long msecs_blocked;\r\nsize_t iu_length;\r\nif (flags & PQI_SYNC_FLAGS_INTERRUPTABLE) {\r\nif (down_interruptible(&ctrl_info->sync_request_sem))\r\nreturn -ERESTARTSYS;\r\n} else {\r\nif (timeout_msecs == NO_TIMEOUT) {\r\ndown(&ctrl_info->sync_request_sem);\r\n} else {\r\nstart_jiffies = jiffies;\r\nif (down_timeout(&ctrl_info->sync_request_sem,\r\nmsecs_to_jiffies(timeout_msecs)))\r\nreturn -ETIMEDOUT;\r\nmsecs_blocked =\r\njiffies_to_msecs(jiffies - start_jiffies);\r\nif (msecs_blocked >= timeout_msecs)\r\nreturn -ETIMEDOUT;\r\ntimeout_msecs -= msecs_blocked;\r\n}\r\n}\r\nio_request = pqi_alloc_io_request(ctrl_info);\r\nput_unaligned_le16(io_request->index,\r\n&(((struct pqi_raid_path_request *)request)->request_id));\r\nif (request->iu_type == PQI_REQUEST_IU_RAID_PATH_IO)\r\n((struct pqi_raid_path_request *)request)->error_index =\r\n((struct pqi_raid_path_request *)request)->request_id;\r\niu_length = get_unaligned_le16(&request->iu_length) +\r\nPQI_REQUEST_HEADER_LENGTH;\r\nmemcpy(io_request->iu, request, iu_length);\r\nrc = pqi_submit_raid_request_synchronous_with_io_request(ctrl_info,\r\nio_request, timeout_msecs);\r\nif (error_info) {\r\nif (io_request->error_info)\r\nmemcpy(error_info, io_request->error_info,\r\nsizeof(*error_info));\r\nelse\r\nmemset(error_info, 0, sizeof(*error_info));\r\n} else if (rc == 0 && io_request->error_info) {\r\nu8 scsi_status;\r\nstruct pqi_raid_error_info *raid_error_info;\r\nraid_error_info = io_request->error_info;\r\nscsi_status = raid_error_info->status;\r\nif (scsi_status == SAM_STAT_CHECK_CONDITION &&\r\nraid_error_info->data_out_result ==\r\nPQI_DATA_IN_OUT_UNDERFLOW)\r\nscsi_status = SAM_STAT_GOOD;\r\nif (scsi_status != SAM_STAT_GOOD)\r\nrc = -EIO;\r\n}\r\npqi_free_io_request(io_request);\r\nup(&ctrl_info->sync_request_sem);\r\nreturn rc;\r\n}\r\nstatic int pqi_validate_admin_response(\r\nstruct pqi_general_admin_response *response, u8 expected_function_code)\r\n{\r\nif (response->header.iu_type != PQI_RESPONSE_IU_GENERAL_ADMIN)\r\nreturn -EINVAL;\r\nif (get_unaligned_le16(&response->header.iu_length) !=\r\nPQI_GENERAL_ADMIN_IU_LENGTH)\r\nreturn -EINVAL;\r\nif (response->function_code != expected_function_code)\r\nreturn -EINVAL;\r\nif (response->status != PQI_GENERAL_ADMIN_STATUS_SUCCESS)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int pqi_submit_admin_request_synchronous(\r\nstruct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_general_admin_request *request,\r\nstruct pqi_general_admin_response *response)\r\n{\r\nint rc;\r\npqi_submit_admin_request(ctrl_info, request);\r\nrc = pqi_poll_for_admin_response(ctrl_info, response);\r\nif (rc == 0)\r\nrc = pqi_validate_admin_response(response,\r\nrequest->function_code);\r\nreturn rc;\r\n}\r\nstatic int pqi_report_device_capability(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nstruct pqi_general_admin_request request;\r\nstruct pqi_general_admin_response response;\r\nstruct pqi_device_capability *capability;\r\nstruct pqi_iu_layer_descriptor *sop_iu_layer_descriptor;\r\ncapability = kmalloc(sizeof(*capability), GFP_KERNEL);\r\nif (!capability)\r\nreturn -ENOMEM;\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\r\nput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\r\n&request.header.iu_length);\r\nrequest.function_code =\r\nPQI_GENERAL_ADMIN_FUNCTION_REPORT_DEVICE_CAPABILITY;\r\nput_unaligned_le32(sizeof(*capability),\r\n&request.data.report_device_capability.buffer_length);\r\nrc = pqi_map_single(ctrl_info->pci_dev,\r\n&request.data.report_device_capability.sg_descriptor,\r\ncapability, sizeof(*capability),\r\nPCI_DMA_FROMDEVICE);\r\nif (rc)\r\ngoto out;\r\nrc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\r\n&response);\r\npqi_pci_unmap(ctrl_info->pci_dev,\r\n&request.data.report_device_capability.sg_descriptor, 1,\r\nPCI_DMA_FROMDEVICE);\r\nif (rc)\r\ngoto out;\r\nif (response.status != PQI_GENERAL_ADMIN_STATUS_SUCCESS) {\r\nrc = -EIO;\r\ngoto out;\r\n}\r\nctrl_info->max_inbound_queues =\r\nget_unaligned_le16(&capability->max_inbound_queues);\r\nctrl_info->max_elements_per_iq =\r\nget_unaligned_le16(&capability->max_elements_per_iq);\r\nctrl_info->max_iq_element_length =\r\nget_unaligned_le16(&capability->max_iq_element_length)\r\n* 16;\r\nctrl_info->max_outbound_queues =\r\nget_unaligned_le16(&capability->max_outbound_queues);\r\nctrl_info->max_elements_per_oq =\r\nget_unaligned_le16(&capability->max_elements_per_oq);\r\nctrl_info->max_oq_element_length =\r\nget_unaligned_le16(&capability->max_oq_element_length)\r\n* 16;\r\nsop_iu_layer_descriptor =\r\n&capability->iu_layer_descriptors[PQI_PROTOCOL_SOP];\r\nctrl_info->max_inbound_iu_length_per_firmware =\r\nget_unaligned_le16(\r\n&sop_iu_layer_descriptor->max_inbound_iu_length);\r\nctrl_info->inbound_spanning_supported =\r\nsop_iu_layer_descriptor->inbound_spanning_supported;\r\nctrl_info->outbound_spanning_supported =\r\nsop_iu_layer_descriptor->outbound_spanning_supported;\r\nout:\r\nkfree(capability);\r\nreturn rc;\r\n}\r\nstatic int pqi_validate_device_capability(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nif (ctrl_info->max_iq_element_length <\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"max. inbound queue element length of %d is less than the required length of %d\n",\r\nctrl_info->max_iq_element_length,\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\r\nreturn -EINVAL;\r\n}\r\nif (ctrl_info->max_oq_element_length <\r\nPQI_OPERATIONAL_OQ_ELEMENT_LENGTH) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"max. outbound queue element length of %d is less than the required length of %d\n",\r\nctrl_info->max_oq_element_length,\r\nPQI_OPERATIONAL_OQ_ELEMENT_LENGTH);\r\nreturn -EINVAL;\r\n}\r\nif (ctrl_info->max_inbound_iu_length_per_firmware <\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"max. inbound IU length of %u is less than the min. required length of %d\n",\r\nctrl_info->max_inbound_iu_length_per_firmware,\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\r\nreturn -EINVAL;\r\n}\r\nif (!ctrl_info->inbound_spanning_supported) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"the controller does not support inbound spanning\n");\r\nreturn -EINVAL;\r\n}\r\nif (ctrl_info->outbound_spanning_supported) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"the controller supports outbound spanning but this driver does not\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pqi_delete_operational_queue(struct pqi_ctrl_info *ctrl_info,\r\nbool inbound_queue, u16 queue_id)\r\n{\r\nstruct pqi_general_admin_request request;\r\nstruct pqi_general_admin_response response;\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\r\nput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\r\n&request.header.iu_length);\r\nif (inbound_queue)\r\nrequest.function_code =\r\nPQI_GENERAL_ADMIN_FUNCTION_DELETE_IQ;\r\nelse\r\nrequest.function_code =\r\nPQI_GENERAL_ADMIN_FUNCTION_DELETE_OQ;\r\nput_unaligned_le16(queue_id,\r\n&request.data.delete_operational_queue.queue_id);\r\nreturn pqi_submit_admin_request_synchronous(ctrl_info, &request,\r\n&response);\r\n}\r\nstatic int pqi_create_event_queue(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nstruct pqi_event_queue *event_queue;\r\nstruct pqi_general_admin_request request;\r\nstruct pqi_general_admin_response response;\r\nevent_queue = &ctrl_info->event_queue;\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\r\nput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\r\n&request.header.iu_length);\r\nrequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CREATE_OQ;\r\nput_unaligned_le16(event_queue->oq_id,\r\n&request.data.create_operational_oq.queue_id);\r\nput_unaligned_le64((u64)event_queue->oq_element_array_bus_addr,\r\n&request.data.create_operational_oq.element_array_addr);\r\nput_unaligned_le64((u64)event_queue->oq_pi_bus_addr,\r\n&request.data.create_operational_oq.pi_addr);\r\nput_unaligned_le16(PQI_NUM_EVENT_QUEUE_ELEMENTS,\r\n&request.data.create_operational_oq.num_elements);\r\nput_unaligned_le16(PQI_EVENT_OQ_ELEMENT_LENGTH / 16,\r\n&request.data.create_operational_oq.element_length);\r\nrequest.data.create_operational_oq.queue_protocol = PQI_PROTOCOL_SOP;\r\nput_unaligned_le16(event_queue->int_msg_num,\r\n&request.data.create_operational_oq.int_msg_num);\r\nrc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\r\n&response);\r\nif (rc)\r\nreturn rc;\r\nevent_queue->oq_ci = ctrl_info->iomem_base +\r\nPQI_DEVICE_REGISTERS_OFFSET +\r\nget_unaligned_le64(\r\n&response.data.create_operational_oq.oq_ci_offset);\r\nreturn 0;\r\n}\r\nstatic int pqi_create_queue_group(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nunsigned int i;\r\nint rc;\r\nstruct pqi_queue_group *queue_group;\r\nstruct pqi_general_admin_request request;\r\nstruct pqi_general_admin_response response;\r\ni = ctrl_info->num_active_queue_groups;\r\nqueue_group = &ctrl_info->queue_groups[i];\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\r\nput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\r\n&request.header.iu_length);\r\nrequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CREATE_IQ;\r\nput_unaligned_le16(queue_group->iq_id[RAID_PATH],\r\n&request.data.create_operational_iq.queue_id);\r\nput_unaligned_le64(\r\n(u64)queue_group->iq_element_array_bus_addr[RAID_PATH],\r\n&request.data.create_operational_iq.element_array_addr);\r\nput_unaligned_le64((u64)queue_group->iq_ci_bus_addr[RAID_PATH],\r\n&request.data.create_operational_iq.ci_addr);\r\nput_unaligned_le16(ctrl_info->num_elements_per_iq,\r\n&request.data.create_operational_iq.num_elements);\r\nput_unaligned_le16(PQI_OPERATIONAL_IQ_ELEMENT_LENGTH / 16,\r\n&request.data.create_operational_iq.element_length);\r\nrequest.data.create_operational_iq.queue_protocol = PQI_PROTOCOL_SOP;\r\nrc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\r\n&response);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error creating inbound RAID queue\n");\r\nreturn rc;\r\n}\r\nqueue_group->iq_pi[RAID_PATH] = ctrl_info->iomem_base +\r\nPQI_DEVICE_REGISTERS_OFFSET +\r\nget_unaligned_le64(\r\n&response.data.create_operational_iq.iq_pi_offset);\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\r\nput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\r\n&request.header.iu_length);\r\nrequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CREATE_IQ;\r\nput_unaligned_le16(queue_group->iq_id[AIO_PATH],\r\n&request.data.create_operational_iq.queue_id);\r\nput_unaligned_le64((u64)queue_group->\r\niq_element_array_bus_addr[AIO_PATH],\r\n&request.data.create_operational_iq.element_array_addr);\r\nput_unaligned_le64((u64)queue_group->iq_ci_bus_addr[AIO_PATH],\r\n&request.data.create_operational_iq.ci_addr);\r\nput_unaligned_le16(ctrl_info->num_elements_per_iq,\r\n&request.data.create_operational_iq.num_elements);\r\nput_unaligned_le16(PQI_OPERATIONAL_IQ_ELEMENT_LENGTH / 16,\r\n&request.data.create_operational_iq.element_length);\r\nrequest.data.create_operational_iq.queue_protocol = PQI_PROTOCOL_SOP;\r\nrc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\r\n&response);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error creating inbound AIO queue\n");\r\ngoto delete_inbound_queue_raid;\r\n}\r\nqueue_group->iq_pi[AIO_PATH] = ctrl_info->iomem_base +\r\nPQI_DEVICE_REGISTERS_OFFSET +\r\nget_unaligned_le64(\r\n&response.data.create_operational_iq.iq_pi_offset);\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\r\nput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\r\n&request.header.iu_length);\r\nrequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CHANGE_IQ_PROPERTY;\r\nput_unaligned_le16(queue_group->iq_id[AIO_PATH],\r\n&request.data.change_operational_iq_properties.queue_id);\r\nput_unaligned_le32(PQI_IQ_PROPERTY_IS_AIO_QUEUE,\r\n&request.data.change_operational_iq_properties.vendor_specific);\r\nrc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\r\n&response);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error changing queue property\n");\r\ngoto delete_inbound_queue_aio;\r\n}\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_GENERAL_ADMIN;\r\nput_unaligned_le16(PQI_GENERAL_ADMIN_IU_LENGTH,\r\n&request.header.iu_length);\r\nrequest.function_code = PQI_GENERAL_ADMIN_FUNCTION_CREATE_OQ;\r\nput_unaligned_le16(queue_group->oq_id,\r\n&request.data.create_operational_oq.queue_id);\r\nput_unaligned_le64((u64)queue_group->oq_element_array_bus_addr,\r\n&request.data.create_operational_oq.element_array_addr);\r\nput_unaligned_le64((u64)queue_group->oq_pi_bus_addr,\r\n&request.data.create_operational_oq.pi_addr);\r\nput_unaligned_le16(ctrl_info->num_elements_per_oq,\r\n&request.data.create_operational_oq.num_elements);\r\nput_unaligned_le16(PQI_OPERATIONAL_OQ_ELEMENT_LENGTH / 16,\r\n&request.data.create_operational_oq.element_length);\r\nrequest.data.create_operational_oq.queue_protocol = PQI_PROTOCOL_SOP;\r\nput_unaligned_le16(queue_group->int_msg_num,\r\n&request.data.create_operational_oq.int_msg_num);\r\nrc = pqi_submit_admin_request_synchronous(ctrl_info, &request,\r\n&response);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error creating outbound queue\n");\r\ngoto delete_inbound_queue_aio;\r\n}\r\nqueue_group->oq_ci = ctrl_info->iomem_base +\r\nPQI_DEVICE_REGISTERS_OFFSET +\r\nget_unaligned_le64(\r\n&response.data.create_operational_oq.oq_ci_offset);\r\nctrl_info->num_active_queue_groups++;\r\nreturn 0;\r\ndelete_inbound_queue_aio:\r\npqi_delete_operational_queue(ctrl_info, true,\r\nqueue_group->iq_id[AIO_PATH]);\r\ndelete_inbound_queue_raid:\r\npqi_delete_operational_queue(ctrl_info, true,\r\nqueue_group->iq_id[RAID_PATH]);\r\nreturn rc;\r\n}\r\nstatic int pqi_create_queues(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nunsigned int i;\r\nrc = pqi_create_event_queue(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error creating event queue\n");\r\nreturn rc;\r\n}\r\nfor (i = 0; i < ctrl_info->num_queue_groups; i++) {\r\nrc = pqi_create_queue_group(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error creating queue group number %u/%u\n",\r\ni, ctrl_info->num_queue_groups);\r\nreturn rc;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int pqi_configure_events(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nunsigned int i;\r\nstruct pqi_event_config *event_config;\r\nstruct pqi_general_management_request request;\r\nevent_config = kmalloc(PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\r\nGFP_KERNEL);\r\nif (!event_config)\r\nreturn -ENOMEM;\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_REPORT_VENDOR_EVENT_CONFIG;\r\nput_unaligned_le16(offsetof(struct pqi_general_management_request,\r\ndata.report_event_configuration.sg_descriptors[1]) -\r\nPQI_REQUEST_HEADER_LENGTH, &request.header.iu_length);\r\nput_unaligned_le32(PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\r\n&request.data.report_event_configuration.buffer_length);\r\nrc = pqi_map_single(ctrl_info->pci_dev,\r\nrequest.data.report_event_configuration.sg_descriptors,\r\nevent_config, PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\r\nPCI_DMA_FROMDEVICE);\r\nif (rc)\r\ngoto out;\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header,\r\n0, NULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev,\r\nrequest.data.report_event_configuration.sg_descriptors, 1,\r\nPCI_DMA_FROMDEVICE);\r\nif (rc)\r\ngoto out;\r\nfor (i = 0; i < event_config->num_event_descriptors; i++)\r\nput_unaligned_le16(ctrl_info->event_queue.oq_id,\r\n&event_config->descriptors[i].oq_id);\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_SET_VENDOR_EVENT_CONFIG;\r\nput_unaligned_le16(offsetof(struct pqi_general_management_request,\r\ndata.report_event_configuration.sg_descriptors[1]) -\r\nPQI_REQUEST_HEADER_LENGTH, &request.header.iu_length);\r\nput_unaligned_le32(PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\r\n&request.data.report_event_configuration.buffer_length);\r\nrc = pqi_map_single(ctrl_info->pci_dev,\r\nrequest.data.report_event_configuration.sg_descriptors,\r\nevent_config, PQI_REPORT_EVENT_CONFIG_BUFFER_LENGTH,\r\nPCI_DMA_TODEVICE);\r\nif (rc)\r\ngoto out;\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header, 0,\r\nNULL, NO_TIMEOUT);\r\npqi_pci_unmap(ctrl_info->pci_dev,\r\nrequest.data.report_event_configuration.sg_descriptors, 1,\r\nPCI_DMA_TODEVICE);\r\nout:\r\nkfree(event_config);\r\nreturn rc;\r\n}\r\nstatic void pqi_free_all_io_requests(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nunsigned int i;\r\nstruct device *dev;\r\nsize_t sg_chain_buffer_length;\r\nstruct pqi_io_request *io_request;\r\nif (!ctrl_info->io_request_pool)\r\nreturn;\r\ndev = &ctrl_info->pci_dev->dev;\r\nsg_chain_buffer_length = ctrl_info->sg_chain_buffer_length;\r\nio_request = ctrl_info->io_request_pool;\r\nfor (i = 0; i < ctrl_info->max_io_slots; i++) {\r\nkfree(io_request->iu);\r\nif (!io_request->sg_chain_buffer)\r\nbreak;\r\ndma_free_coherent(dev, sg_chain_buffer_length,\r\nio_request->sg_chain_buffer,\r\nio_request->sg_chain_buffer_dma_handle);\r\nio_request++;\r\n}\r\nkfree(ctrl_info->io_request_pool);\r\nctrl_info->io_request_pool = NULL;\r\n}\r\nstatic inline int pqi_alloc_error_buffer(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nctrl_info->error_buffer = dma_zalloc_coherent(&ctrl_info->pci_dev->dev,\r\nctrl_info->error_buffer_length,\r\n&ctrl_info->error_buffer_dma_handle, GFP_KERNEL);\r\nif (!ctrl_info->error_buffer)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic int pqi_alloc_io_resources(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nunsigned int i;\r\nvoid *sg_chain_buffer;\r\nsize_t sg_chain_buffer_length;\r\ndma_addr_t sg_chain_buffer_dma_handle;\r\nstruct device *dev;\r\nstruct pqi_io_request *io_request;\r\nctrl_info->io_request_pool = kzalloc(ctrl_info->max_io_slots *\r\nsizeof(ctrl_info->io_request_pool[0]), GFP_KERNEL);\r\nif (!ctrl_info->io_request_pool) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"failed to allocate I/O request pool\n");\r\ngoto error;\r\n}\r\ndev = &ctrl_info->pci_dev->dev;\r\nsg_chain_buffer_length = ctrl_info->sg_chain_buffer_length;\r\nio_request = ctrl_info->io_request_pool;\r\nfor (i = 0; i < ctrl_info->max_io_slots; i++) {\r\nio_request->iu =\r\nkmalloc(ctrl_info->max_inbound_iu_length, GFP_KERNEL);\r\nif (!io_request->iu) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"failed to allocate IU buffers\n");\r\ngoto error;\r\n}\r\nsg_chain_buffer = dma_alloc_coherent(dev,\r\nsg_chain_buffer_length, &sg_chain_buffer_dma_handle,\r\nGFP_KERNEL);\r\nif (!sg_chain_buffer) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"failed to allocate PQI scatter-gather chain buffers\n");\r\ngoto error;\r\n}\r\nio_request->index = i;\r\nio_request->sg_chain_buffer = sg_chain_buffer;\r\nio_request->sg_chain_buffer_dma_handle =\r\nsg_chain_buffer_dma_handle;\r\nio_request++;\r\n}\r\nreturn 0;\r\nerror:\r\npqi_free_all_io_requests(ctrl_info);\r\nreturn -ENOMEM;\r\n}\r\nstatic void pqi_calculate_io_resources(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nu32 max_transfer_size;\r\nu32 max_sg_entries;\r\nctrl_info->scsi_ml_can_queue =\r\nctrl_info->max_outstanding_requests - PQI_RESERVED_IO_SLOTS;\r\nctrl_info->max_io_slots = ctrl_info->max_outstanding_requests;\r\nctrl_info->error_buffer_length =\r\nctrl_info->max_io_slots * PQI_ERROR_BUFFER_ELEMENT_LENGTH;\r\nmax_transfer_size =\r\nmin(ctrl_info->max_transfer_size, PQI_MAX_TRANSFER_SIZE);\r\nmax_sg_entries = max_transfer_size / PAGE_SIZE;\r\nmax_sg_entries++;\r\nmax_sg_entries = min(ctrl_info->max_sg_entries, max_sg_entries);\r\nmax_transfer_size = (max_sg_entries - 1) * PAGE_SIZE;\r\nctrl_info->sg_chain_buffer_length =\r\nmax_sg_entries * sizeof(struct pqi_sg_descriptor);\r\nctrl_info->sg_tablesize = max_sg_entries;\r\nctrl_info->max_sectors = max_transfer_size / 512;\r\n}\r\nstatic void pqi_calculate_queue_resources(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint num_cpus;\r\nint max_queue_groups;\r\nint num_queue_groups;\r\nu16 num_elements_per_iq;\r\nu16 num_elements_per_oq;\r\nmax_queue_groups = min(ctrl_info->max_inbound_queues / 2,\r\nctrl_info->max_outbound_queues - 1);\r\nmax_queue_groups = min(max_queue_groups, PQI_MAX_QUEUE_GROUPS);\r\nnum_cpus = num_online_cpus();\r\nnum_queue_groups = min(num_cpus, ctrl_info->max_msix_vectors);\r\nnum_queue_groups = min(num_queue_groups, max_queue_groups);\r\nctrl_info->num_queue_groups = num_queue_groups;\r\nctrl_info->max_inbound_iu_length =\r\n(ctrl_info->max_inbound_iu_length_per_firmware /\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) *\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH;\r\nnum_elements_per_iq =\r\n(ctrl_info->max_inbound_iu_length /\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH);\r\nnum_elements_per_iq++;\r\nnum_elements_per_iq = min(num_elements_per_iq,\r\nctrl_info->max_elements_per_iq);\r\nnum_elements_per_oq = ((num_elements_per_iq - 1) * 2) + 1;\r\nnum_elements_per_oq = min(num_elements_per_oq,\r\nctrl_info->max_elements_per_oq);\r\nctrl_info->num_elements_per_iq = num_elements_per_iq;\r\nctrl_info->num_elements_per_oq = num_elements_per_oq;\r\nctrl_info->max_sg_per_iu =\r\n((ctrl_info->max_inbound_iu_length -\r\nPQI_OPERATIONAL_IQ_ELEMENT_LENGTH) /\r\nsizeof(struct pqi_sg_descriptor)) +\r\nPQI_MAX_EMBEDDED_SG_DESCRIPTORS;\r\n}\r\nstatic inline void pqi_set_sg_descriptor(\r\nstruct pqi_sg_descriptor *sg_descriptor, struct scatterlist *sg)\r\n{\r\nu64 address = (u64)sg_dma_address(sg);\r\nunsigned int length = sg_dma_len(sg);\r\nput_unaligned_le64(address, &sg_descriptor->address);\r\nput_unaligned_le32(length, &sg_descriptor->length);\r\nput_unaligned_le32(0, &sg_descriptor->flags);\r\n}\r\nstatic int pqi_build_raid_sg_list(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_raid_path_request *request, struct scsi_cmnd *scmd,\r\nstruct pqi_io_request *io_request)\r\n{\r\nint i;\r\nu16 iu_length;\r\nint sg_count;\r\nbool chained;\r\nunsigned int num_sg_in_iu;\r\nunsigned int max_sg_per_iu;\r\nstruct scatterlist *sg;\r\nstruct pqi_sg_descriptor *sg_descriptor;\r\nsg_count = scsi_dma_map(scmd);\r\nif (sg_count < 0)\r\nreturn sg_count;\r\niu_length = offsetof(struct pqi_raid_path_request, sg_descriptors) -\r\nPQI_REQUEST_HEADER_LENGTH;\r\nif (sg_count == 0)\r\ngoto out;\r\nsg = scsi_sglist(scmd);\r\nsg_descriptor = request->sg_descriptors;\r\nmax_sg_per_iu = ctrl_info->max_sg_per_iu - 1;\r\nchained = false;\r\nnum_sg_in_iu = 0;\r\ni = 0;\r\nwhile (1) {\r\npqi_set_sg_descriptor(sg_descriptor, sg);\r\nif (!chained)\r\nnum_sg_in_iu++;\r\ni++;\r\nif (i == sg_count)\r\nbreak;\r\nsg_descriptor++;\r\nif (i == max_sg_per_iu) {\r\nput_unaligned_le64(\r\n(u64)io_request->sg_chain_buffer_dma_handle,\r\n&sg_descriptor->address);\r\nput_unaligned_le32((sg_count - num_sg_in_iu)\r\n* sizeof(*sg_descriptor),\r\n&sg_descriptor->length);\r\nput_unaligned_le32(CISS_SG_CHAIN,\r\n&sg_descriptor->flags);\r\nchained = true;\r\nnum_sg_in_iu++;\r\nsg_descriptor = io_request->sg_chain_buffer;\r\n}\r\nsg = sg_next(sg);\r\n}\r\nput_unaligned_le32(CISS_SG_LAST, &sg_descriptor->flags);\r\nrequest->partial = chained;\r\niu_length += num_sg_in_iu * sizeof(*sg_descriptor);\r\nout:\r\nput_unaligned_le16(iu_length, &request->header.iu_length);\r\nreturn 0;\r\n}\r\nstatic int pqi_build_aio_sg_list(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_aio_path_request *request, struct scsi_cmnd *scmd,\r\nstruct pqi_io_request *io_request)\r\n{\r\nint i;\r\nu16 iu_length;\r\nint sg_count;\r\nbool chained;\r\nunsigned int num_sg_in_iu;\r\nunsigned int max_sg_per_iu;\r\nstruct scatterlist *sg;\r\nstruct pqi_sg_descriptor *sg_descriptor;\r\nsg_count = scsi_dma_map(scmd);\r\nif (sg_count < 0)\r\nreturn sg_count;\r\niu_length = offsetof(struct pqi_aio_path_request, sg_descriptors) -\r\nPQI_REQUEST_HEADER_LENGTH;\r\nnum_sg_in_iu = 0;\r\nif (sg_count == 0)\r\ngoto out;\r\nsg = scsi_sglist(scmd);\r\nsg_descriptor = request->sg_descriptors;\r\nmax_sg_per_iu = ctrl_info->max_sg_per_iu - 1;\r\nchained = false;\r\ni = 0;\r\nwhile (1) {\r\npqi_set_sg_descriptor(sg_descriptor, sg);\r\nif (!chained)\r\nnum_sg_in_iu++;\r\ni++;\r\nif (i == sg_count)\r\nbreak;\r\nsg_descriptor++;\r\nif (i == max_sg_per_iu) {\r\nput_unaligned_le64(\r\n(u64)io_request->sg_chain_buffer_dma_handle,\r\n&sg_descriptor->address);\r\nput_unaligned_le32((sg_count - num_sg_in_iu)\r\n* sizeof(*sg_descriptor),\r\n&sg_descriptor->length);\r\nput_unaligned_le32(CISS_SG_CHAIN,\r\n&sg_descriptor->flags);\r\nchained = true;\r\nnum_sg_in_iu++;\r\nsg_descriptor = io_request->sg_chain_buffer;\r\n}\r\nsg = sg_next(sg);\r\n}\r\nput_unaligned_le32(CISS_SG_LAST, &sg_descriptor->flags);\r\nrequest->partial = chained;\r\niu_length += num_sg_in_iu * sizeof(*sg_descriptor);\r\nout:\r\nput_unaligned_le16(iu_length, &request->header.iu_length);\r\nrequest->num_sg_descriptors = num_sg_in_iu;\r\nreturn 0;\r\n}\r\nstatic void pqi_raid_io_complete(struct pqi_io_request *io_request,\r\nvoid *context)\r\n{\r\nstruct scsi_cmnd *scmd;\r\nscmd = io_request->scmd;\r\npqi_free_io_request(io_request);\r\nscsi_dma_unmap(scmd);\r\npqi_scsi_done(scmd);\r\n}\r\nstatic int pqi_raid_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device, struct scsi_cmnd *scmd,\r\nstruct pqi_queue_group *queue_group)\r\n{\r\nint rc;\r\nsize_t cdb_length;\r\nstruct pqi_io_request *io_request;\r\nstruct pqi_raid_path_request *request;\r\nio_request = pqi_alloc_io_request(ctrl_info);\r\nio_request->io_complete_callback = pqi_raid_io_complete;\r\nio_request->scmd = scmd;\r\nscmd->host_scribble = (unsigned char *)io_request;\r\nrequest = io_request->iu;\r\nmemset(request, 0,\r\noffsetof(struct pqi_raid_path_request, sg_descriptors));\r\nrequest->header.iu_type = PQI_REQUEST_IU_RAID_PATH_IO;\r\nput_unaligned_le32(scsi_bufflen(scmd), &request->buffer_length);\r\nrequest->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\r\nput_unaligned_le16(io_request->index, &request->request_id);\r\nrequest->error_index = request->request_id;\r\nmemcpy(request->lun_number, device->scsi3addr,\r\nsizeof(request->lun_number));\r\ncdb_length = min_t(size_t, scmd->cmd_len, sizeof(request->cdb));\r\nmemcpy(request->cdb, scmd->cmnd, cdb_length);\r\nswitch (cdb_length) {\r\ncase 6:\r\ncase 10:\r\ncase 12:\r\ncase 16:\r\nrequest->additional_cdb_bytes_usage =\r\nSOP_ADDITIONAL_CDB_BYTES_0;\r\nbreak;\r\ncase 20:\r\nrequest->additional_cdb_bytes_usage =\r\nSOP_ADDITIONAL_CDB_BYTES_4;\r\nbreak;\r\ncase 24:\r\nrequest->additional_cdb_bytes_usage =\r\nSOP_ADDITIONAL_CDB_BYTES_8;\r\nbreak;\r\ncase 28:\r\nrequest->additional_cdb_bytes_usage =\r\nSOP_ADDITIONAL_CDB_BYTES_12;\r\nbreak;\r\ncase 32:\r\ndefault:\r\nrequest->additional_cdb_bytes_usage =\r\nSOP_ADDITIONAL_CDB_BYTES_16;\r\nbreak;\r\n}\r\nswitch (scmd->sc_data_direction) {\r\ncase DMA_TO_DEVICE:\r\nrequest->data_direction = SOP_READ_FLAG;\r\nbreak;\r\ncase DMA_FROM_DEVICE:\r\nrequest->data_direction = SOP_WRITE_FLAG;\r\nbreak;\r\ncase DMA_NONE:\r\nrequest->data_direction = SOP_NO_DIRECTION_FLAG;\r\nbreak;\r\ncase DMA_BIDIRECTIONAL:\r\nrequest->data_direction = SOP_BIDIRECTIONAL;\r\nbreak;\r\ndefault:\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"unknown data direction: %d\n",\r\nscmd->sc_data_direction);\r\nWARN_ON(scmd->sc_data_direction);\r\nbreak;\r\n}\r\nrc = pqi_build_raid_sg_list(ctrl_info, request, scmd, io_request);\r\nif (rc) {\r\npqi_free_io_request(io_request);\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\npqi_start_io(ctrl_info, queue_group, RAID_PATH, io_request);\r\nreturn 0;\r\n}\r\nstatic void pqi_aio_io_complete(struct pqi_io_request *io_request,\r\nvoid *context)\r\n{\r\nstruct scsi_cmnd *scmd;\r\nscmd = io_request->scmd;\r\nscsi_dma_unmap(scmd);\r\nif (io_request->status == -EAGAIN)\r\nset_host_byte(scmd, DID_IMM_RETRY);\r\npqi_free_io_request(io_request);\r\npqi_scsi_done(scmd);\r\n}\r\nstatic inline int pqi_aio_submit_scsi_cmd(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device, struct scsi_cmnd *scmd,\r\nstruct pqi_queue_group *queue_group)\r\n{\r\nreturn pqi_aio_submit_io(ctrl_info, scmd, device->aio_handle,\r\nscmd->cmnd, scmd->cmd_len, queue_group, NULL);\r\n}\r\nstatic int pqi_aio_submit_io(struct pqi_ctrl_info *ctrl_info,\r\nstruct scsi_cmnd *scmd, u32 aio_handle, u8 *cdb,\r\nunsigned int cdb_length, struct pqi_queue_group *queue_group,\r\nstruct pqi_encryption_info *encryption_info)\r\n{\r\nint rc;\r\nstruct pqi_io_request *io_request;\r\nstruct pqi_aio_path_request *request;\r\nio_request = pqi_alloc_io_request(ctrl_info);\r\nio_request->io_complete_callback = pqi_aio_io_complete;\r\nio_request->scmd = scmd;\r\nscmd->host_scribble = (unsigned char *)io_request;\r\nrequest = io_request->iu;\r\nmemset(request, 0,\r\noffsetof(struct pqi_raid_path_request, sg_descriptors));\r\nrequest->header.iu_type = PQI_REQUEST_IU_AIO_PATH_IO;\r\nput_unaligned_le32(aio_handle, &request->nexus_id);\r\nput_unaligned_le32(scsi_bufflen(scmd), &request->buffer_length);\r\nrequest->task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\r\nput_unaligned_le16(io_request->index, &request->request_id);\r\nrequest->error_index = request->request_id;\r\nif (cdb_length > sizeof(request->cdb))\r\ncdb_length = sizeof(request->cdb);\r\nrequest->cdb_length = cdb_length;\r\nmemcpy(request->cdb, cdb, cdb_length);\r\nswitch (scmd->sc_data_direction) {\r\ncase DMA_TO_DEVICE:\r\nrequest->data_direction = SOP_READ_FLAG;\r\nbreak;\r\ncase DMA_FROM_DEVICE:\r\nrequest->data_direction = SOP_WRITE_FLAG;\r\nbreak;\r\ncase DMA_NONE:\r\nrequest->data_direction = SOP_NO_DIRECTION_FLAG;\r\nbreak;\r\ncase DMA_BIDIRECTIONAL:\r\nrequest->data_direction = SOP_BIDIRECTIONAL;\r\nbreak;\r\ndefault:\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"unknown data direction: %d\n",\r\nscmd->sc_data_direction);\r\nWARN_ON(scmd->sc_data_direction);\r\nbreak;\r\n}\r\nif (encryption_info) {\r\nrequest->encryption_enable = true;\r\nput_unaligned_le16(encryption_info->data_encryption_key_index,\r\n&request->data_encryption_key_index);\r\nput_unaligned_le32(encryption_info->encrypt_tweak_lower,\r\n&request->encrypt_tweak_lower);\r\nput_unaligned_le32(encryption_info->encrypt_tweak_upper,\r\n&request->encrypt_tweak_upper);\r\n}\r\nrc = pqi_build_aio_sg_list(ctrl_info, request, scmd, io_request);\r\nif (rc) {\r\npqi_free_io_request(io_request);\r\nreturn SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\npqi_start_io(ctrl_info, queue_group, AIO_PATH, io_request);\r\nreturn 0;\r\n}\r\nstatic int pqi_scsi_queue_command(struct Scsi_Host *shost,\r\nstruct scsi_cmnd *scmd)\r\n{\r\nint rc;\r\nstruct pqi_ctrl_info *ctrl_info;\r\nstruct pqi_scsi_dev *device;\r\nu16 hwq;\r\nstruct pqi_queue_group *queue_group;\r\nbool raid_bypassed;\r\ndevice = scmd->device->hostdata;\r\nctrl_info = shost_to_hba(shost);\r\nif (pqi_ctrl_offline(ctrl_info)) {\r\nset_host_byte(scmd, DID_NO_CONNECT);\r\npqi_scsi_done(scmd);\r\nreturn 0;\r\n}\r\nscmd->result = 0;\r\nhwq = blk_mq_unique_tag_to_hwq(blk_mq_unique_tag(scmd->request));\r\nif (hwq >= ctrl_info->num_queue_groups)\r\nhwq = 0;\r\nqueue_group = &ctrl_info->queue_groups[hwq];\r\nif (pqi_is_logical_device(device)) {\r\nraid_bypassed = false;\r\nif (device->offload_enabled &&\r\n!blk_rq_is_passthrough(scmd->request)) {\r\nrc = pqi_raid_bypass_submit_scsi_cmd(ctrl_info, device,\r\nscmd, queue_group);\r\nif (rc == 0 ||\r\nrc == SCSI_MLQUEUE_HOST_BUSY ||\r\nrc == SAM_STAT_CHECK_CONDITION ||\r\nrc == SAM_STAT_RESERVATION_CONFLICT)\r\nraid_bypassed = true;\r\n}\r\nif (!raid_bypassed)\r\nrc = pqi_raid_submit_scsi_cmd(ctrl_info, device, scmd,\r\nqueue_group);\r\n} else {\r\nif (device->aio_enabled)\r\nrc = pqi_aio_submit_scsi_cmd(ctrl_info, device, scmd,\r\nqueue_group);\r\nelse\r\nrc = pqi_raid_submit_scsi_cmd(ctrl_info, device, scmd,\r\nqueue_group);\r\n}\r\nreturn rc;\r\n}\r\nstatic void pqi_lun_reset_complete(struct pqi_io_request *io_request,\r\nvoid *context)\r\n{\r\nstruct completion *waiting = context;\r\ncomplete(waiting);\r\n}\r\nstatic int pqi_wait_for_lun_reset_completion(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device, struct completion *wait)\r\n{\r\nint rc;\r\nunsigned int wait_secs = 0;\r\nwhile (1) {\r\nif (wait_for_completion_io_timeout(wait,\r\nPQI_LUN_RESET_TIMEOUT_SECS * HZ)) {\r\nrc = 0;\r\nbreak;\r\n}\r\npqi_check_ctrl_health(ctrl_info);\r\nif (pqi_ctrl_offline(ctrl_info)) {\r\nrc = -ETIMEDOUT;\r\nbreak;\r\n}\r\nwait_secs += PQI_LUN_RESET_TIMEOUT_SECS;\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"resetting scsi %d:%d:%d:%d - waiting %u seconds\n",\r\nctrl_info->scsi_host->host_no, device->bus,\r\ndevice->target, device->lun, wait_secs);\r\n}\r\nreturn rc;\r\n}\r\nstatic int pqi_lun_reset(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nint rc;\r\nstruct pqi_io_request *io_request;\r\nDECLARE_COMPLETION_ONSTACK(wait);\r\nstruct pqi_task_management_request *request;\r\ndown(&ctrl_info->lun_reset_sem);\r\nio_request = pqi_alloc_io_request(ctrl_info);\r\nio_request->io_complete_callback = pqi_lun_reset_complete;\r\nio_request->context = &wait;\r\nrequest = io_request->iu;\r\nmemset(request, 0, sizeof(*request));\r\nrequest->header.iu_type = PQI_REQUEST_IU_TASK_MANAGEMENT;\r\nput_unaligned_le16(sizeof(*request) - PQI_REQUEST_HEADER_LENGTH,\r\n&request->header.iu_length);\r\nput_unaligned_le16(io_request->index, &request->request_id);\r\nmemcpy(request->lun_number, device->scsi3addr,\r\nsizeof(request->lun_number));\r\nrequest->task_management_function = SOP_TASK_MANAGEMENT_LUN_RESET;\r\npqi_start_io(ctrl_info,\r\n&ctrl_info->queue_groups[PQI_DEFAULT_QUEUE_GROUP], RAID_PATH,\r\nio_request);\r\nrc = pqi_wait_for_lun_reset_completion(ctrl_info, device, &wait);\r\nif (rc == 0)\r\nrc = io_request->status;\r\npqi_free_io_request(io_request);\r\nup(&ctrl_info->lun_reset_sem);\r\nreturn rc;\r\n}\r\nstatic int pqi_device_reset(struct pqi_ctrl_info *ctrl_info,\r\nstruct pqi_scsi_dev *device)\r\n{\r\nint rc;\r\npqi_check_ctrl_health(ctrl_info);\r\nif (pqi_ctrl_offline(ctrl_info))\r\nreturn FAILED;\r\nrc = pqi_lun_reset(ctrl_info, device);\r\nreturn rc == 0 ? SUCCESS : FAILED;\r\n}\r\nstatic int pqi_eh_device_reset_handler(struct scsi_cmnd *scmd)\r\n{\r\nint rc;\r\nstruct pqi_ctrl_info *ctrl_info;\r\nstruct pqi_scsi_dev *device;\r\nctrl_info = shost_to_hba(scmd->device->host);\r\ndevice = scmd->device->hostdata;\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"resetting scsi %d:%d:%d:%d\n",\r\nctrl_info->scsi_host->host_no,\r\ndevice->bus, device->target, device->lun);\r\nrc = pqi_device_reset(ctrl_info, device);\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"reset of scsi %d:%d:%d:%d: %s\n",\r\nctrl_info->scsi_host->host_no,\r\ndevice->bus, device->target, device->lun,\r\nrc == SUCCESS ? "SUCCESS" : "FAILED");\r\nreturn rc;\r\n}\r\nstatic int pqi_slave_alloc(struct scsi_device *sdev)\r\n{\r\nstruct pqi_scsi_dev *device;\r\nunsigned long flags;\r\nstruct pqi_ctrl_info *ctrl_info;\r\nstruct scsi_target *starget;\r\nstruct sas_rphy *rphy;\r\nctrl_info = shost_to_hba(sdev->host);\r\nspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\r\nif (sdev_channel(sdev) == PQI_PHYSICAL_DEVICE_BUS) {\r\nstarget = scsi_target(sdev);\r\nrphy = target_to_rphy(starget);\r\ndevice = pqi_find_device_by_sas_rphy(ctrl_info, rphy);\r\nif (device) {\r\ndevice->target = sdev_id(sdev);\r\ndevice->lun = sdev->lun;\r\ndevice->target_lun_valid = true;\r\n}\r\n} else {\r\ndevice = pqi_find_scsi_dev(ctrl_info, sdev_channel(sdev),\r\nsdev_id(sdev), sdev->lun);\r\n}\r\nif (device && device->expose_device) {\r\nsdev->hostdata = device;\r\ndevice->sdev = sdev;\r\nif (device->queue_depth) {\r\ndevice->advertised_queue_depth = device->queue_depth;\r\nscsi_change_queue_depth(sdev,\r\ndevice->advertised_queue_depth);\r\n}\r\n}\r\nspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\r\nreturn 0;\r\n}\r\nstatic int pqi_slave_configure(struct scsi_device *sdev)\r\n{\r\nstruct pqi_scsi_dev *device;\r\ndevice = sdev->hostdata;\r\nif (!device->expose_device)\r\nsdev->no_uld_attach = true;\r\nreturn 0;\r\n}\r\nstatic int pqi_map_queues(struct Scsi_Host *shost)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info = shost_to_hba(shost);\r\nreturn blk_mq_pci_map_queues(&shost->tag_set, ctrl_info->pci_dev);\r\n}\r\nstatic int pqi_getpciinfo_ioctl(struct pqi_ctrl_info *ctrl_info,\r\nvoid __user *arg)\r\n{\r\nstruct pci_dev *pci_dev;\r\nu32 subsystem_vendor;\r\nu32 subsystem_device;\r\ncciss_pci_info_struct pciinfo;\r\nif (!arg)\r\nreturn -EINVAL;\r\npci_dev = ctrl_info->pci_dev;\r\npciinfo.domain = pci_domain_nr(pci_dev->bus);\r\npciinfo.bus = pci_dev->bus->number;\r\npciinfo.dev_fn = pci_dev->devfn;\r\nsubsystem_vendor = pci_dev->subsystem_vendor;\r\nsubsystem_device = pci_dev->subsystem_device;\r\npciinfo.board_id = ((subsystem_device << 16) & 0xffff0000) |\r\nsubsystem_vendor;\r\nif (copy_to_user(arg, &pciinfo, sizeof(pciinfo)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int pqi_getdrivver_ioctl(void __user *arg)\r\n{\r\nu32 version;\r\nif (!arg)\r\nreturn -EINVAL;\r\nversion = (DRIVER_MAJOR << 28) | (DRIVER_MINOR << 24) |\r\n(DRIVER_RELEASE << 16) | DRIVER_REVISION;\r\nif (copy_to_user(arg, &version, sizeof(version)))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic void pqi_error_info_to_ciss(struct pqi_raid_error_info *pqi_error_info,\r\nstruct ciss_error_info *ciss_error_info)\r\n{\r\nint ciss_cmd_status;\r\nsize_t sense_data_length;\r\nswitch (pqi_error_info->data_out_result) {\r\ncase PQI_DATA_IN_OUT_GOOD:\r\nciss_cmd_status = CISS_CMD_STATUS_SUCCESS;\r\nbreak;\r\ncase PQI_DATA_IN_OUT_UNDERFLOW:\r\nciss_cmd_status = CISS_CMD_STATUS_DATA_UNDERRUN;\r\nbreak;\r\ncase PQI_DATA_IN_OUT_BUFFER_OVERFLOW:\r\nciss_cmd_status = CISS_CMD_STATUS_DATA_OVERRUN;\r\nbreak;\r\ncase PQI_DATA_IN_OUT_PROTOCOL_ERROR:\r\ncase PQI_DATA_IN_OUT_BUFFER_ERROR:\r\ncase PQI_DATA_IN_OUT_BUFFER_OVERFLOW_DESCRIPTOR_AREA:\r\ncase PQI_DATA_IN_OUT_BUFFER_OVERFLOW_BRIDGE:\r\ncase PQI_DATA_IN_OUT_ERROR:\r\nciss_cmd_status = CISS_CMD_STATUS_PROTOCOL_ERROR;\r\nbreak;\r\ncase PQI_DATA_IN_OUT_HARDWARE_ERROR:\r\ncase PQI_DATA_IN_OUT_PCIE_FABRIC_ERROR:\r\ncase PQI_DATA_IN_OUT_PCIE_COMPLETION_TIMEOUT:\r\ncase PQI_DATA_IN_OUT_PCIE_COMPLETER_ABORT_RECEIVED:\r\ncase PQI_DATA_IN_OUT_PCIE_UNSUPPORTED_REQUEST_RECEIVED:\r\ncase PQI_DATA_IN_OUT_PCIE_ECRC_CHECK_FAILED:\r\ncase PQI_DATA_IN_OUT_PCIE_UNSUPPORTED_REQUEST:\r\ncase PQI_DATA_IN_OUT_PCIE_ACS_VIOLATION:\r\ncase PQI_DATA_IN_OUT_PCIE_TLP_PREFIX_BLOCKED:\r\ncase PQI_DATA_IN_OUT_PCIE_POISONED_MEMORY_READ:\r\nciss_cmd_status = CISS_CMD_STATUS_HARDWARE_ERROR;\r\nbreak;\r\ncase PQI_DATA_IN_OUT_UNSOLICITED_ABORT:\r\nciss_cmd_status = CISS_CMD_STATUS_UNSOLICITED_ABORT;\r\nbreak;\r\ncase PQI_DATA_IN_OUT_ABORTED:\r\nciss_cmd_status = CISS_CMD_STATUS_ABORTED;\r\nbreak;\r\ncase PQI_DATA_IN_OUT_TIMEOUT:\r\nciss_cmd_status = CISS_CMD_STATUS_TIMEOUT;\r\nbreak;\r\ndefault:\r\nciss_cmd_status = CISS_CMD_STATUS_TARGET_STATUS;\r\nbreak;\r\n}\r\nsense_data_length =\r\nget_unaligned_le16(&pqi_error_info->sense_data_length);\r\nif (sense_data_length == 0)\r\nsense_data_length =\r\nget_unaligned_le16(&pqi_error_info->response_data_length);\r\nif (sense_data_length)\r\nif (sense_data_length > sizeof(pqi_error_info->data))\r\nsense_data_length = sizeof(pqi_error_info->data);\r\nciss_error_info->scsi_status = pqi_error_info->status;\r\nciss_error_info->command_status = ciss_cmd_status;\r\nciss_error_info->sense_data_length = sense_data_length;\r\n}\r\nstatic int pqi_passthru_ioctl(struct pqi_ctrl_info *ctrl_info, void __user *arg)\r\n{\r\nint rc;\r\nchar *kernel_buffer = NULL;\r\nu16 iu_length;\r\nsize_t sense_data_length;\r\nIOCTL_Command_struct iocommand;\r\nstruct pqi_raid_path_request request;\r\nstruct pqi_raid_error_info pqi_error_info;\r\nstruct ciss_error_info ciss_error_info;\r\nif (pqi_ctrl_offline(ctrl_info))\r\nreturn -ENXIO;\r\nif (!arg)\r\nreturn -EINVAL;\r\nif (!capable(CAP_SYS_RAWIO))\r\nreturn -EPERM;\r\nif (copy_from_user(&iocommand, arg, sizeof(iocommand)))\r\nreturn -EFAULT;\r\nif (iocommand.buf_size < 1 &&\r\niocommand.Request.Type.Direction != XFER_NONE)\r\nreturn -EINVAL;\r\nif (iocommand.Request.CDBLen > sizeof(request.cdb))\r\nreturn -EINVAL;\r\nif (iocommand.Request.Type.Type != TYPE_CMD)\r\nreturn -EINVAL;\r\nswitch (iocommand.Request.Type.Direction) {\r\ncase XFER_NONE:\r\ncase XFER_WRITE:\r\ncase XFER_READ:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (iocommand.buf_size > 0) {\r\nkernel_buffer = kmalloc(iocommand.buf_size, GFP_KERNEL);\r\nif (!kernel_buffer)\r\nreturn -ENOMEM;\r\nif (iocommand.Request.Type.Direction & XFER_WRITE) {\r\nif (copy_from_user(kernel_buffer, iocommand.buf,\r\niocommand.buf_size)) {\r\nrc = -EFAULT;\r\ngoto out;\r\n}\r\n} else {\r\nmemset(kernel_buffer, 0, iocommand.buf_size);\r\n}\r\n}\r\nmemset(&request, 0, sizeof(request));\r\nrequest.header.iu_type = PQI_REQUEST_IU_RAID_PATH_IO;\r\niu_length = offsetof(struct pqi_raid_path_request, sg_descriptors) -\r\nPQI_REQUEST_HEADER_LENGTH;\r\nmemcpy(request.lun_number, iocommand.LUN_info.LunAddrBytes,\r\nsizeof(request.lun_number));\r\nmemcpy(request.cdb, iocommand.Request.CDB, iocommand.Request.CDBLen);\r\nrequest.additional_cdb_bytes_usage = SOP_ADDITIONAL_CDB_BYTES_0;\r\nswitch (iocommand.Request.Type.Direction) {\r\ncase XFER_NONE:\r\nrequest.data_direction = SOP_NO_DIRECTION_FLAG;\r\nbreak;\r\ncase XFER_WRITE:\r\nrequest.data_direction = SOP_WRITE_FLAG;\r\nbreak;\r\ncase XFER_READ:\r\nrequest.data_direction = SOP_READ_FLAG;\r\nbreak;\r\n}\r\nrequest.task_attribute = SOP_TASK_ATTRIBUTE_SIMPLE;\r\nif (iocommand.buf_size > 0) {\r\nput_unaligned_le32(iocommand.buf_size, &request.buffer_length);\r\nrc = pqi_map_single(ctrl_info->pci_dev,\r\n&request.sg_descriptors[0], kernel_buffer,\r\niocommand.buf_size, PCI_DMA_BIDIRECTIONAL);\r\nif (rc)\r\ngoto out;\r\niu_length += sizeof(request.sg_descriptors[0]);\r\n}\r\nput_unaligned_le16(iu_length, &request.header.iu_length);\r\nrc = pqi_submit_raid_request_synchronous(ctrl_info, &request.header,\r\nPQI_SYNC_FLAGS_INTERRUPTABLE, &pqi_error_info, NO_TIMEOUT);\r\nif (iocommand.buf_size > 0)\r\npqi_pci_unmap(ctrl_info->pci_dev, request.sg_descriptors, 1,\r\nPCI_DMA_BIDIRECTIONAL);\r\nmemset(&iocommand.error_info, 0, sizeof(iocommand.error_info));\r\nif (rc == 0) {\r\npqi_error_info_to_ciss(&pqi_error_info, &ciss_error_info);\r\niocommand.error_info.ScsiStatus = ciss_error_info.scsi_status;\r\niocommand.error_info.CommandStatus =\r\nciss_error_info.command_status;\r\nsense_data_length = ciss_error_info.sense_data_length;\r\nif (sense_data_length) {\r\nif (sense_data_length >\r\nsizeof(iocommand.error_info.SenseInfo))\r\nsense_data_length =\r\nsizeof(iocommand.error_info.SenseInfo);\r\nmemcpy(iocommand.error_info.SenseInfo,\r\npqi_error_info.data, sense_data_length);\r\niocommand.error_info.SenseLen = sense_data_length;\r\n}\r\n}\r\nif (copy_to_user(arg, &iocommand, sizeof(iocommand))) {\r\nrc = -EFAULT;\r\ngoto out;\r\n}\r\nif (rc == 0 && iocommand.buf_size > 0 &&\r\n(iocommand.Request.Type.Direction & XFER_READ)) {\r\nif (copy_to_user(iocommand.buf, kernel_buffer,\r\niocommand.buf_size)) {\r\nrc = -EFAULT;\r\n}\r\n}\r\nout:\r\nkfree(kernel_buffer);\r\nreturn rc;\r\n}\r\nstatic int pqi_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)\r\n{\r\nint rc;\r\nstruct pqi_ctrl_info *ctrl_info;\r\nctrl_info = shost_to_hba(sdev->host);\r\nswitch (cmd) {\r\ncase CCISS_DEREGDISK:\r\ncase CCISS_REGNEWDISK:\r\ncase CCISS_REGNEWD:\r\nrc = pqi_scan_scsi_devices(ctrl_info);\r\nbreak;\r\ncase CCISS_GETPCIINFO:\r\nrc = pqi_getpciinfo_ioctl(ctrl_info, arg);\r\nbreak;\r\ncase CCISS_GETDRIVVER:\r\nrc = pqi_getdrivver_ioctl(arg);\r\nbreak;\r\ncase CCISS_PASSTHRU:\r\nrc = pqi_passthru_ioctl(ctrl_info, arg);\r\nbreak;\r\ndefault:\r\nrc = -EINVAL;\r\nbreak;\r\n}\r\nreturn rc;\r\n}\r\nstatic ssize_t pqi_version_show(struct device *dev,\r\nstruct device_attribute *attr, char *buffer)\r\n{\r\nssize_t count = 0;\r\nstruct Scsi_Host *shost;\r\nstruct pqi_ctrl_info *ctrl_info;\r\nshost = class_to_shost(dev);\r\nctrl_info = shost_to_hba(shost);\r\ncount += snprintf(buffer + count, PAGE_SIZE - count,\r\n" driver: %s\n", DRIVER_VERSION BUILD_TIMESTAMP);\r\ncount += snprintf(buffer + count, PAGE_SIZE - count,\r\n"firmware: %s\n", ctrl_info->firmware_version);\r\nreturn count;\r\n}\r\nstatic ssize_t pqi_host_rescan_store(struct device *dev,\r\nstruct device_attribute *attr, const char *buffer, size_t count)\r\n{\r\nstruct Scsi_Host *shost = class_to_shost(dev);\r\npqi_scan_start(shost);\r\nreturn count;\r\n}\r\nstatic ssize_t pqi_sas_address_show(struct device *dev,\r\nstruct device_attribute *attr, char *buffer)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info;\r\nstruct scsi_device *sdev;\r\nstruct pqi_scsi_dev *device;\r\nunsigned long flags;\r\nu64 sas_address;\r\nsdev = to_scsi_device(dev);\r\nctrl_info = shost_to_hba(sdev->host);\r\nspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\r\ndevice = sdev->hostdata;\r\nif (pqi_is_logical_device(device)) {\r\nspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock,\r\nflags);\r\nreturn -ENODEV;\r\n}\r\nsas_address = device->sas_address;\r\nspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\r\nreturn snprintf(buffer, PAGE_SIZE, "0x%016llx\n", sas_address);\r\n}\r\nstatic ssize_t pqi_ssd_smart_path_enabled_show(struct device *dev,\r\nstruct device_attribute *attr, char *buffer)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info;\r\nstruct scsi_device *sdev;\r\nstruct pqi_scsi_dev *device;\r\nunsigned long flags;\r\nsdev = to_scsi_device(dev);\r\nctrl_info = shost_to_hba(sdev->host);\r\nspin_lock_irqsave(&ctrl_info->scsi_device_list_lock, flags);\r\ndevice = sdev->hostdata;\r\nbuffer[0] = device->offload_enabled ? '1' : '0';\r\nbuffer[1] = '\n';\r\nbuffer[2] = '\0';\r\nspin_unlock_irqrestore(&ctrl_info->scsi_device_list_lock, flags);\r\nreturn 2;\r\n}\r\nstatic int pqi_register_scsi(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nstruct Scsi_Host *shost;\r\nshost = scsi_host_alloc(&pqi_driver_template, sizeof(ctrl_info));\r\nif (!shost) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"scsi_host_alloc failed for controller %u\n",\r\nctrl_info->ctrl_id);\r\nreturn -ENOMEM;\r\n}\r\nshost->io_port = 0;\r\nshost->n_io_port = 0;\r\nshost->this_id = -1;\r\nshost->max_channel = PQI_MAX_BUS;\r\nshost->max_cmd_len = MAX_COMMAND_SIZE;\r\nshost->max_lun = ~0;\r\nshost->max_id = ~0;\r\nshost->max_sectors = ctrl_info->max_sectors;\r\nshost->can_queue = ctrl_info->scsi_ml_can_queue;\r\nshost->cmd_per_lun = shost->can_queue;\r\nshost->sg_tablesize = ctrl_info->sg_tablesize;\r\nshost->transportt = pqi_sas_transport_template;\r\nshost->irq = pci_irq_vector(ctrl_info->pci_dev, 0);\r\nshost->unique_id = shost->irq;\r\nshost->nr_hw_queues = ctrl_info->num_queue_groups;\r\nshost->hostdata[0] = (unsigned long)ctrl_info;\r\nrc = scsi_add_host(shost, &ctrl_info->pci_dev->dev);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"scsi_add_host failed for controller %u\n",\r\nctrl_info->ctrl_id);\r\ngoto free_host;\r\n}\r\nrc = pqi_add_sas_host(shost, ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"add SAS host failed for controller %u\n",\r\nctrl_info->ctrl_id);\r\ngoto remove_host;\r\n}\r\nctrl_info->scsi_host = shost;\r\nreturn 0;\r\nremove_host:\r\nscsi_remove_host(shost);\r\nfree_host:\r\nscsi_host_put(shost);\r\nreturn rc;\r\n}\r\nstatic void pqi_unregister_scsi(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nstruct Scsi_Host *shost;\r\npqi_delete_sas_host(ctrl_info);\r\nshost = ctrl_info->scsi_host;\r\nif (!shost)\r\nreturn;\r\nscsi_remove_host(shost);\r\nscsi_host_put(shost);\r\n}\r\nstatic int pqi_reset(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nu32 reset_params;\r\nreset_params = (PQI_RESET_ACTION_RESET << 5) |\r\nPQI_RESET_TYPE_HARD_RESET;\r\nwritel(reset_params,\r\n&ctrl_info->pqi_registers->device_reset);\r\nrc = pqi_wait_for_pqi_mode_ready(ctrl_info);\r\nif (rc)\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"PQI reset failed\n");\r\nreturn rc;\r\n}\r\nstatic int pqi_get_ctrl_firmware_version(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nstruct bmic_identify_controller *identify;\r\nidentify = kmalloc(sizeof(*identify), GFP_KERNEL);\r\nif (!identify)\r\nreturn -ENOMEM;\r\nrc = pqi_identify_controller(ctrl_info, identify);\r\nif (rc)\r\ngoto out;\r\nmemcpy(ctrl_info->firmware_version, identify->firmware_version,\r\nsizeof(identify->firmware_version));\r\nctrl_info->firmware_version[sizeof(identify->firmware_version)] = '\0';\r\nsnprintf(ctrl_info->firmware_version +\r\nstrlen(ctrl_info->firmware_version),\r\nsizeof(ctrl_info->firmware_version),\r\n"-%u", get_unaligned_le16(&identify->firmware_build_number));\r\nout:\r\nkfree(identify);\r\nreturn rc;\r\n}\r\nstatic int pqi_kdump_init(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nif (!sis_is_firmware_running(ctrl_info))\r\nreturn -ENXIO;\r\nif (pqi_get_ctrl_mode(ctrl_info) == PQI_MODE) {\r\nsis_disable_msix(ctrl_info);\r\nif (pqi_reset(ctrl_info) == 0)\r\nsis_reenable_sis_mode(ctrl_info);\r\n}\r\nreturn 0;\r\n}\r\nstatic int pqi_ctrl_init(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nif (reset_devices) {\r\nrc = pqi_kdump_init(ctrl_info);\r\nif (rc)\r\nreturn rc;\r\n}\r\nrc = sis_wait_for_ctrl_ready(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error initializing SIS interface\n");\r\nreturn rc;\r\n}\r\nrc = sis_get_ctrl_properties(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error obtaining controller properties\n");\r\nreturn rc;\r\n}\r\nrc = sis_get_pqi_capabilities(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error obtaining controller capabilities\n");\r\nreturn rc;\r\n}\r\nif (ctrl_info->max_outstanding_requests > PQI_MAX_OUTSTANDING_REQUESTS)\r\nctrl_info->max_outstanding_requests =\r\nPQI_MAX_OUTSTANDING_REQUESTS;\r\npqi_calculate_io_resources(ctrl_info);\r\nrc = pqi_alloc_error_buffer(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"failed to allocate PQI error buffer\n");\r\nreturn rc;\r\n}\r\nrc = sis_init_base_struct_addr(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error initializing PQI mode\n");\r\nreturn rc;\r\n}\r\nrc = pqi_wait_for_pqi_mode_ready(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"transition to PQI mode failed\n");\r\nreturn rc;\r\n}\r\nctrl_info->pqi_mode_enabled = true;\r\npqi_save_ctrl_mode(ctrl_info, PQI_MODE);\r\nrc = pqi_alloc_admin_queues(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error allocating admin queues\n");\r\nreturn rc;\r\n}\r\nrc = pqi_create_admin_queues(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error creating admin queues\n");\r\nreturn rc;\r\n}\r\nrc = pqi_report_device_capability(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"obtaining device capability failed\n");\r\nreturn rc;\r\n}\r\nrc = pqi_validate_device_capability(ctrl_info);\r\nif (rc)\r\nreturn rc;\r\npqi_calculate_queue_resources(ctrl_info);\r\nrc = pqi_enable_msix_interrupts(ctrl_info);\r\nif (rc)\r\nreturn rc;\r\nif (ctrl_info->num_msix_vectors_enabled < ctrl_info->num_queue_groups) {\r\nctrl_info->max_msix_vectors =\r\nctrl_info->num_msix_vectors_enabled;\r\npqi_calculate_queue_resources(ctrl_info);\r\n}\r\nrc = pqi_alloc_io_resources(ctrl_info);\r\nif (rc)\r\nreturn rc;\r\nrc = pqi_alloc_operational_queues(ctrl_info);\r\nif (rc)\r\nreturn rc;\r\npqi_init_operational_queues(ctrl_info);\r\nrc = pqi_request_irqs(ctrl_info);\r\nif (rc)\r\nreturn rc;\r\nrc = pqi_create_queues(ctrl_info);\r\nif (rc)\r\nreturn rc;\r\nsis_enable_msix(ctrl_info);\r\nrc = pqi_configure_events(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error configuring events\n");\r\nreturn rc;\r\n}\r\npqi_start_heartbeat_timer(ctrl_info);\r\nctrl_info->controller_online = true;\r\nrc = pqi_register_scsi(ctrl_info);\r\nif (rc)\r\nreturn rc;\r\nrc = pqi_get_ctrl_firmware_version(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error obtaining firmware version\n");\r\nreturn rc;\r\n}\r\nrc = pqi_write_driver_version_to_host_wellness(ctrl_info);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"error updating host wellness\n");\r\nreturn rc;\r\n}\r\npqi_schedule_update_time_worker(ctrl_info);\r\npqi_scan_scsi_devices(ctrl_info);\r\nreturn 0;\r\n}\r\nstatic int pqi_pci_init(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint rc;\r\nu64 mask;\r\nrc = pci_enable_device(ctrl_info->pci_dev);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"failed to enable PCI device\n");\r\nreturn rc;\r\n}\r\nif (sizeof(dma_addr_t) > 4)\r\nmask = DMA_BIT_MASK(64);\r\nelse\r\nmask = DMA_BIT_MASK(32);\r\nrc = dma_set_mask(&ctrl_info->pci_dev->dev, mask);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev, "failed to set DMA mask\n");\r\ngoto disable_device;\r\n}\r\nrc = pci_request_regions(ctrl_info->pci_dev, DRIVER_NAME_SHORT);\r\nif (rc) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"failed to obtain PCI resources\n");\r\ngoto disable_device;\r\n}\r\nctrl_info->iomem_base = ioremap_nocache(pci_resource_start(\r\nctrl_info->pci_dev, 0),\r\nsizeof(struct pqi_ctrl_registers));\r\nif (!ctrl_info->iomem_base) {\r\ndev_err(&ctrl_info->pci_dev->dev,\r\n"failed to map memory for controller registers\n");\r\nrc = -ENOMEM;\r\ngoto release_regions;\r\n}\r\nctrl_info->registers = ctrl_info->iomem_base;\r\nctrl_info->pqi_registers = &ctrl_info->registers->pqi_registers;\r\npci_set_master(ctrl_info->pci_dev);\r\npci_set_drvdata(ctrl_info->pci_dev, ctrl_info);\r\nreturn 0;\r\nrelease_regions:\r\npci_release_regions(ctrl_info->pci_dev);\r\ndisable_device:\r\npci_disable_device(ctrl_info->pci_dev);\r\nreturn rc;\r\n}\r\nstatic void pqi_cleanup_pci_init(struct pqi_ctrl_info *ctrl_info)\r\n{\r\niounmap(ctrl_info->iomem_base);\r\npci_release_regions(ctrl_info->pci_dev);\r\npci_disable_device(ctrl_info->pci_dev);\r\npci_set_drvdata(ctrl_info->pci_dev, NULL);\r\n}\r\nstatic struct pqi_ctrl_info *pqi_alloc_ctrl_info(int numa_node)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info;\r\nctrl_info = kzalloc_node(sizeof(struct pqi_ctrl_info),\r\nGFP_KERNEL, numa_node);\r\nif (!ctrl_info)\r\nreturn NULL;\r\nmutex_init(&ctrl_info->scan_mutex);\r\nINIT_LIST_HEAD(&ctrl_info->scsi_device_list);\r\nspin_lock_init(&ctrl_info->scsi_device_list_lock);\r\nINIT_WORK(&ctrl_info->event_work, pqi_event_worker);\r\natomic_set(&ctrl_info->num_interrupts, 0);\r\nINIT_DELAYED_WORK(&ctrl_info->rescan_work, pqi_rescan_worker);\r\nINIT_DELAYED_WORK(&ctrl_info->update_time_work, pqi_update_time_worker);\r\nsema_init(&ctrl_info->sync_request_sem,\r\nPQI_RESERVED_IO_SLOTS_SYNCHRONOUS_REQUESTS);\r\nsema_init(&ctrl_info->lun_reset_sem, PQI_RESERVED_IO_SLOTS_LUN_RESET);\r\nctrl_info->ctrl_id = atomic_inc_return(&pqi_controller_count) - 1;\r\nctrl_info->max_msix_vectors = PQI_MAX_MSIX_VECTORS;\r\nreturn ctrl_info;\r\n}\r\nstatic inline void pqi_free_ctrl_info(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nkfree(ctrl_info);\r\n}\r\nstatic void pqi_free_interrupts(struct pqi_ctrl_info *ctrl_info)\r\n{\r\nint i;\r\nfor (i = 0; i < ctrl_info->num_msix_vectors_initialized; i++) {\r\nfree_irq(pci_irq_vector(ctrl_info->pci_dev, i),\r\n&ctrl_info->queue_groups[i]);\r\n}\r\npci_free_irq_vectors(ctrl_info->pci_dev);\r\n}\r\nstatic void pqi_free_ctrl_resources(struct pqi_ctrl_info *ctrl_info)\r\n{\r\npqi_stop_heartbeat_timer(ctrl_info);\r\npqi_free_interrupts(ctrl_info);\r\nif (ctrl_info->queue_memory_base)\r\ndma_free_coherent(&ctrl_info->pci_dev->dev,\r\nctrl_info->queue_memory_length,\r\nctrl_info->queue_memory_base,\r\nctrl_info->queue_memory_base_dma_handle);\r\nif (ctrl_info->admin_queue_memory_base)\r\ndma_free_coherent(&ctrl_info->pci_dev->dev,\r\nctrl_info->admin_queue_memory_length,\r\nctrl_info->admin_queue_memory_base,\r\nctrl_info->admin_queue_memory_base_dma_handle);\r\npqi_free_all_io_requests(ctrl_info);\r\nif (ctrl_info->error_buffer)\r\ndma_free_coherent(&ctrl_info->pci_dev->dev,\r\nctrl_info->error_buffer_length,\r\nctrl_info->error_buffer,\r\nctrl_info->error_buffer_dma_handle);\r\nif (ctrl_info->iomem_base)\r\npqi_cleanup_pci_init(ctrl_info);\r\npqi_free_ctrl_info(ctrl_info);\r\n}\r\nstatic void pqi_remove_ctrl(struct pqi_ctrl_info *ctrl_info)\r\n{\r\ncancel_delayed_work_sync(&ctrl_info->rescan_work);\r\ncancel_delayed_work_sync(&ctrl_info->update_time_work);\r\npqi_remove_all_scsi_devices(ctrl_info);\r\npqi_unregister_scsi(ctrl_info);\r\nif (ctrl_info->pqi_mode_enabled) {\r\nsis_disable_msix(ctrl_info);\r\nif (pqi_reset(ctrl_info) == 0)\r\nsis_reenable_sis_mode(ctrl_info);\r\n}\r\npqi_free_ctrl_resources(ctrl_info);\r\n}\r\nstatic void pqi_print_ctrl_info(struct pci_dev *pdev,\r\nconst struct pci_device_id *id)\r\n{\r\nchar *ctrl_description;\r\nif (id->driver_data) {\r\nctrl_description = (char *)id->driver_data;\r\n} else {\r\nswitch (id->subvendor) {\r\ncase PCI_VENDOR_ID_HP:\r\nctrl_description = hpe_branded_controller;\r\nbreak;\r\ncase PCI_VENDOR_ID_ADAPTEC2:\r\ndefault:\r\nctrl_description = microsemi_branded_controller;\r\nbreak;\r\n}\r\n}\r\ndev_info(&pdev->dev, "%s found\n", ctrl_description);\r\n}\r\nstatic int pqi_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\r\n{\r\nint rc;\r\nint node;\r\nstruct pqi_ctrl_info *ctrl_info;\r\npqi_print_ctrl_info(pdev, id);\r\nif (pqi_disable_device_id_wildcards &&\r\nid->subvendor == PCI_ANY_ID &&\r\nid->subdevice == PCI_ANY_ID) {\r\ndev_warn(&pdev->dev,\r\n"controller not probed because device ID wildcards are disabled\n");\r\nreturn -ENODEV;\r\n}\r\nif (id->subvendor == PCI_ANY_ID || id->subdevice == PCI_ANY_ID)\r\ndev_warn(&pdev->dev,\r\n"controller device ID matched using wildcards\n");\r\nnode = dev_to_node(&pdev->dev);\r\nif (node == NUMA_NO_NODE)\r\nset_dev_node(&pdev->dev, 0);\r\nctrl_info = pqi_alloc_ctrl_info(node);\r\nif (!ctrl_info) {\r\ndev_err(&pdev->dev,\r\n"failed to allocate controller info block\n");\r\nreturn -ENOMEM;\r\n}\r\nctrl_info->pci_dev = pdev;\r\nrc = pqi_pci_init(ctrl_info);\r\nif (rc)\r\ngoto error;\r\nrc = pqi_ctrl_init(ctrl_info);\r\nif (rc)\r\ngoto error;\r\nreturn 0;\r\nerror:\r\npqi_remove_ctrl(ctrl_info);\r\nreturn rc;\r\n}\r\nstatic void pqi_pci_remove(struct pci_dev *pdev)\r\n{\r\nstruct pqi_ctrl_info *ctrl_info;\r\nctrl_info = pci_get_drvdata(pdev);\r\nif (!ctrl_info)\r\nreturn;\r\npqi_remove_ctrl(ctrl_info);\r\n}\r\nstatic void pqi_shutdown(struct pci_dev *pdev)\r\n{\r\nint rc;\r\nstruct pqi_ctrl_info *ctrl_info;\r\nctrl_info = pci_get_drvdata(pdev);\r\nif (!ctrl_info)\r\ngoto error;\r\nrc = pqi_flush_cache(ctrl_info);\r\nif (rc == 0)\r\nreturn;\r\nerror:\r\ndev_warn(&pdev->dev,\r\n"unable to flush controller cache\n");\r\n}\r\nstatic int __init pqi_init(void)\r\n{\r\nint rc;\r\npr_info(DRIVER_NAME "\n");\r\npqi_sas_transport_template =\r\nsas_attach_transport(&pqi_sas_transport_functions);\r\nif (!pqi_sas_transport_template)\r\nreturn -ENODEV;\r\nrc = pci_register_driver(&pqi_pci_driver);\r\nif (rc)\r\nsas_release_transport(pqi_sas_transport_template);\r\nreturn rc;\r\n}\r\nstatic void __exit pqi_cleanup(void)\r\n{\r\npci_unregister_driver(&pqi_pci_driver);\r\nsas_release_transport(pqi_sas_transport_template);\r\n}
