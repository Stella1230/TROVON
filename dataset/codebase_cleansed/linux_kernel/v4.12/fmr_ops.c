bool\r\nfmr_is_supported(struct rpcrdma_ia *ia)\r\n{\r\nif (!ia->ri_device->alloc_fmr) {\r\npr_info("rpcrdma: 'fmr' mode is not supported by device %s\n",\r\nia->ri_device->name);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int\r\nfmr_op_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mw *mw)\r\n{\r\nstatic struct ib_fmr_attr fmr_attr = {\r\n.max_pages = RPCRDMA_MAX_FMR_SGES,\r\n.max_maps = 1,\r\n.page_shift = PAGE_SHIFT\r\n};\r\nmw->fmr.fm_physaddrs = kcalloc(RPCRDMA_MAX_FMR_SGES,\r\nsizeof(u64), GFP_KERNEL);\r\nif (!mw->fmr.fm_physaddrs)\r\ngoto out_free;\r\nmw->mw_sg = kcalloc(RPCRDMA_MAX_FMR_SGES,\r\nsizeof(*mw->mw_sg), GFP_KERNEL);\r\nif (!mw->mw_sg)\r\ngoto out_free;\r\nsg_init_table(mw->mw_sg, RPCRDMA_MAX_FMR_SGES);\r\nmw->fmr.fm_mr = ib_alloc_fmr(ia->ri_pd, RPCRDMA_FMR_ACCESS_FLAGS,\r\n&fmr_attr);\r\nif (IS_ERR(mw->fmr.fm_mr))\r\ngoto out_fmr_err;\r\nreturn 0;\r\nout_fmr_err:\r\ndprintk("RPC: %s: ib_alloc_fmr returned %ld\n", __func__,\r\nPTR_ERR(mw->fmr.fm_mr));\r\nout_free:\r\nkfree(mw->mw_sg);\r\nkfree(mw->fmr.fm_physaddrs);\r\nreturn -ENOMEM;\r\n}\r\nstatic int\r\n__fmr_unmap(struct rpcrdma_mw *mw)\r\n{\r\nLIST_HEAD(l);\r\nint rc;\r\nlist_add(&mw->fmr.fm_mr->list, &l);\r\nrc = ib_unmap_fmr(&l);\r\nlist_del_init(&mw->fmr.fm_mr->list);\r\nreturn rc;\r\n}\r\nstatic void\r\nfmr_op_release_mr(struct rpcrdma_mw *r)\r\n{\r\nLIST_HEAD(unmap_list);\r\nint rc;\r\nif (!list_empty(&r->mw_list))\r\nlist_del(&r->mw_list);\r\nkfree(r->fmr.fm_physaddrs);\r\nkfree(r->mw_sg);\r\nrc = __fmr_unmap(r);\r\nif (rc)\r\npr_err("rpcrdma: final ib_unmap_fmr for %p failed %i\n",\r\nr, rc);\r\nrc = ib_dealloc_fmr(r->fmr.fm_mr);\r\nif (rc)\r\npr_err("rpcrdma: final ib_dealloc_fmr for %p returned %i\n",\r\nr, rc);\r\nkfree(r);\r\n}\r\nstatic void\r\nfmr_op_recover_mr(struct rpcrdma_mw *mw)\r\n{\r\nstruct rpcrdma_xprt *r_xprt = mw->mw_xprt;\r\nint rc;\r\nrc = __fmr_unmap(mw);\r\nib_dma_unmap_sg(r_xprt->rx_ia.ri_device,\r\nmw->mw_sg, mw->mw_nents, mw->mw_dir);\r\nif (rc)\r\ngoto out_release;\r\nrpcrdma_put_mw(r_xprt, mw);\r\nr_xprt->rx_stats.mrs_recovered++;\r\nreturn;\r\nout_release:\r\npr_err("rpcrdma: FMR reset failed (%d), %p released\n", rc, mw);\r\nr_xprt->rx_stats.mrs_orphaned++;\r\nspin_lock(&r_xprt->rx_buf.rb_mwlock);\r\nlist_del(&mw->mw_all);\r\nspin_unlock(&r_xprt->rx_buf.rb_mwlock);\r\nfmr_op_release_mr(mw);\r\n}\r\nstatic int\r\nfmr_op_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep,\r\nstruct rpcrdma_create_data_internal *cdata)\r\n{\r\nia->ri_max_segs = max_t(unsigned int, 1, RPCRDMA_MAX_DATA_SEGS /\r\nRPCRDMA_MAX_FMR_SGES);\r\nreturn 0;\r\n}\r\nstatic size_t\r\nfmr_op_maxpages(struct rpcrdma_xprt *r_xprt)\r\n{\r\nreturn min_t(unsigned int, RPCRDMA_MAX_DATA_SEGS,\r\nRPCRDMA_MAX_HDR_SEGS * RPCRDMA_MAX_FMR_SGES);\r\n}\r\nstatic int\r\nfmr_op_map(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr_seg *seg,\r\nint nsegs, bool writing, struct rpcrdma_mw **out)\r\n{\r\nstruct rpcrdma_mr_seg *seg1 = seg;\r\nint len, pageoff, i, rc;\r\nstruct rpcrdma_mw *mw;\r\nu64 *dma_pages;\r\nmw = rpcrdma_get_mw(r_xprt);\r\nif (!mw)\r\nreturn -ENOBUFS;\r\npageoff = offset_in_page(seg1->mr_offset);\r\nseg1->mr_offset -= pageoff;\r\nseg1->mr_len += pageoff;\r\nlen = -pageoff;\r\nif (nsegs > RPCRDMA_MAX_FMR_SGES)\r\nnsegs = RPCRDMA_MAX_FMR_SGES;\r\nfor (i = 0; i < nsegs;) {\r\nif (seg->mr_page)\r\nsg_set_page(&mw->mw_sg[i],\r\nseg->mr_page,\r\nseg->mr_len,\r\noffset_in_page(seg->mr_offset));\r\nelse\r\nsg_set_buf(&mw->mw_sg[i], seg->mr_offset,\r\nseg->mr_len);\r\nlen += seg->mr_len;\r\n++seg;\r\n++i;\r\nif ((i < nsegs && offset_in_page(seg->mr_offset)) ||\r\noffset_in_page((seg-1)->mr_offset + (seg-1)->mr_len))\r\nbreak;\r\n}\r\nmw->mw_nents = i;\r\nmw->mw_dir = rpcrdma_data_dir(writing);\r\nif (i == 0)\r\ngoto out_dmamap_err;\r\nif (!ib_dma_map_sg(r_xprt->rx_ia.ri_device,\r\nmw->mw_sg, mw->mw_nents, mw->mw_dir))\r\ngoto out_dmamap_err;\r\nfor (i = 0, dma_pages = mw->fmr.fm_physaddrs; i < mw->mw_nents; i++)\r\ndma_pages[i] = sg_dma_address(&mw->mw_sg[i]);\r\nrc = ib_map_phys_fmr(mw->fmr.fm_mr, dma_pages, mw->mw_nents,\r\ndma_pages[0]);\r\nif (rc)\r\ngoto out_maperr;\r\nmw->mw_handle = mw->fmr.fm_mr->rkey;\r\nmw->mw_length = len;\r\nmw->mw_offset = dma_pages[0] + pageoff;\r\n*out = mw;\r\nreturn mw->mw_nents;\r\nout_dmamap_err:\r\npr_err("rpcrdma: failed to dma map sg %p sg_nents %u\n",\r\nmw->mw_sg, mw->mw_nents);\r\nrpcrdma_defer_mr_recovery(mw);\r\nreturn -EIO;\r\nout_maperr:\r\npr_err("rpcrdma: ib_map_phys_fmr %u@0x%llx+%i (%d) status %i\n",\r\nlen, (unsigned long long)dma_pages[0],\r\npageoff, mw->mw_nents, rc);\r\nrpcrdma_defer_mr_recovery(mw);\r\nreturn -EIO;\r\n}\r\nstatic void\r\nfmr_op_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req)\r\n{\r\nstruct rpcrdma_mw *mw, *tmp;\r\nLIST_HEAD(unmap_list);\r\nint rc;\r\ndprintk("RPC: %s: req %p\n", __func__, req);\r\nlist_for_each_entry(mw, &req->rl_registered, mw_list)\r\nlist_add_tail(&mw->fmr.fm_mr->list, &unmap_list);\r\nr_xprt->rx_stats.local_inv_needed++;\r\nrc = ib_unmap_fmr(&unmap_list);\r\nif (rc)\r\ngoto out_reset;\r\nlist_for_each_entry_safe(mw, tmp, &req->rl_registered, mw_list) {\r\nlist_del_init(&mw->mw_list);\r\nlist_del_init(&mw->fmr.fm_mr->list);\r\nib_dma_unmap_sg(r_xprt->rx_ia.ri_device,\r\nmw->mw_sg, mw->mw_nents, mw->mw_dir);\r\nrpcrdma_put_mw(r_xprt, mw);\r\n}\r\nreturn;\r\nout_reset:\r\npr_err("rpcrdma: ib_unmap_fmr failed (%i)\n", rc);\r\nlist_for_each_entry_safe(mw, tmp, &req->rl_registered, mw_list) {\r\nlist_del_init(&mw->fmr.fm_mr->list);\r\nfmr_op_recover_mr(mw);\r\n}\r\n}\r\nstatic void\r\nfmr_op_unmap_safe(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,\r\nbool sync)\r\n{\r\nstruct rpcrdma_mw *mw;\r\nwhile (!list_empty(&req->rl_registered)) {\r\nmw = rpcrdma_pop_mw(&req->rl_registered);\r\nif (sync)\r\nfmr_op_recover_mr(mw);\r\nelse\r\nrpcrdma_defer_mr_recovery(mw);\r\n}\r\n}
