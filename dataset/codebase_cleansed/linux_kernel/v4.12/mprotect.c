static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,\r\nunsigned long addr, unsigned long end, pgprot_t newprot,\r\nint dirty_accountable, int prot_numa)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npte_t *pte, oldpte;\r\nspinlock_t *ptl;\r\nunsigned long pages = 0;\r\nint target_node = NUMA_NO_NODE;\r\nif (pmd_trans_unstable(pmd))\r\nreturn 0;\r\npte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\r\nif (!pte)\r\nreturn 0;\r\nif (prot_numa && !(vma->vm_flags & VM_SHARED) &&\r\natomic_read(&vma->vm_mm->mm_users) == 1)\r\ntarget_node = numa_node_id();\r\narch_enter_lazy_mmu_mode();\r\ndo {\r\noldpte = *pte;\r\nif (pte_present(oldpte)) {\r\npte_t ptent;\r\nbool preserve_write = prot_numa && pte_write(oldpte);\r\nif (prot_numa) {\r\nstruct page *page;\r\npage = vm_normal_page(vma, addr, oldpte);\r\nif (!page || PageKsm(page))\r\ncontinue;\r\nif (pte_protnone(oldpte))\r\ncontinue;\r\nif (target_node == page_to_nid(page))\r\ncontinue;\r\n}\r\nptent = ptep_modify_prot_start(mm, addr, pte);\r\nptent = pte_modify(ptent, newprot);\r\nif (preserve_write)\r\nptent = pte_mk_savedwrite(ptent);\r\nif (dirty_accountable && pte_dirty(ptent) &&\r\n(pte_soft_dirty(ptent) ||\r\n!(vma->vm_flags & VM_SOFTDIRTY))) {\r\nptent = pte_mkwrite(ptent);\r\n}\r\nptep_modify_prot_commit(mm, addr, pte, ptent);\r\npages++;\r\n} else if (IS_ENABLED(CONFIG_MIGRATION)) {\r\nswp_entry_t entry = pte_to_swp_entry(oldpte);\r\nif (is_write_migration_entry(entry)) {\r\npte_t newpte;\r\nmake_migration_entry_read(&entry);\r\nnewpte = swp_entry_to_pte(entry);\r\nif (pte_swp_soft_dirty(oldpte))\r\nnewpte = pte_swp_mksoft_dirty(newpte);\r\nset_pte_at(mm, addr, pte, newpte);\r\npages++;\r\n}\r\n}\r\n} while (pte++, addr += PAGE_SIZE, addr != end);\r\narch_leave_lazy_mmu_mode();\r\npte_unmap_unlock(pte - 1, ptl);\r\nreturn pages;\r\n}\r\nstatic inline unsigned long change_pmd_range(struct vm_area_struct *vma,\r\npud_t *pud, unsigned long addr, unsigned long end,\r\npgprot_t newprot, int dirty_accountable, int prot_numa)\r\n{\r\npmd_t *pmd;\r\nstruct mm_struct *mm = vma->vm_mm;\r\nunsigned long next;\r\nunsigned long pages = 0;\r\nunsigned long nr_huge_updates = 0;\r\nunsigned long mni_start = 0;\r\npmd = pmd_offset(pud, addr);\r\ndo {\r\nunsigned long this_pages;\r\nnext = pmd_addr_end(addr, end);\r\nif (!pmd_trans_huge(*pmd) && !pmd_devmap(*pmd)\r\n&& pmd_none_or_clear_bad(pmd))\r\ncontinue;\r\nif (!mni_start) {\r\nmni_start = addr;\r\nmmu_notifier_invalidate_range_start(mm, mni_start, end);\r\n}\r\nif (pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {\r\nif (next - addr != HPAGE_PMD_SIZE) {\r\n__split_huge_pmd(vma, pmd, addr, false, NULL);\r\n} else {\r\nint nr_ptes = change_huge_pmd(vma, pmd, addr,\r\nnewprot, prot_numa);\r\nif (nr_ptes) {\r\nif (nr_ptes == HPAGE_PMD_NR) {\r\npages += HPAGE_PMD_NR;\r\nnr_huge_updates++;\r\n}\r\ncontinue;\r\n}\r\n}\r\n}\r\nthis_pages = change_pte_range(vma, pmd, addr, next, newprot,\r\ndirty_accountable, prot_numa);\r\npages += this_pages;\r\n} while (pmd++, addr = next, addr != end);\r\nif (mni_start)\r\nmmu_notifier_invalidate_range_end(mm, mni_start, end);\r\nif (nr_huge_updates)\r\ncount_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);\r\nreturn pages;\r\n}\r\nstatic inline unsigned long change_pud_range(struct vm_area_struct *vma,\r\np4d_t *p4d, unsigned long addr, unsigned long end,\r\npgprot_t newprot, int dirty_accountable, int prot_numa)\r\n{\r\npud_t *pud;\r\nunsigned long next;\r\nunsigned long pages = 0;\r\npud = pud_offset(p4d, addr);\r\ndo {\r\nnext = pud_addr_end(addr, end);\r\nif (pud_none_or_clear_bad(pud))\r\ncontinue;\r\npages += change_pmd_range(vma, pud, addr, next, newprot,\r\ndirty_accountable, prot_numa);\r\n} while (pud++, addr = next, addr != end);\r\nreturn pages;\r\n}\r\nstatic inline unsigned long change_p4d_range(struct vm_area_struct *vma,\r\npgd_t *pgd, unsigned long addr, unsigned long end,\r\npgprot_t newprot, int dirty_accountable, int prot_numa)\r\n{\r\np4d_t *p4d;\r\nunsigned long next;\r\nunsigned long pages = 0;\r\np4d = p4d_offset(pgd, addr);\r\ndo {\r\nnext = p4d_addr_end(addr, end);\r\nif (p4d_none_or_clear_bad(p4d))\r\ncontinue;\r\npages += change_pud_range(vma, p4d, addr, next, newprot,\r\ndirty_accountable, prot_numa);\r\n} while (p4d++, addr = next, addr != end);\r\nreturn pages;\r\n}\r\nstatic unsigned long change_protection_range(struct vm_area_struct *vma,\r\nunsigned long addr, unsigned long end, pgprot_t newprot,\r\nint dirty_accountable, int prot_numa)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\npgd_t *pgd;\r\nunsigned long next;\r\nunsigned long start = addr;\r\nunsigned long pages = 0;\r\nBUG_ON(addr >= end);\r\npgd = pgd_offset(mm, addr);\r\nflush_cache_range(vma, addr, end);\r\nset_tlb_flush_pending(mm);\r\ndo {\r\nnext = pgd_addr_end(addr, end);\r\nif (pgd_none_or_clear_bad(pgd))\r\ncontinue;\r\npages += change_p4d_range(vma, pgd, addr, next, newprot,\r\ndirty_accountable, prot_numa);\r\n} while (pgd++, addr = next, addr != end);\r\nif (pages)\r\nflush_tlb_range(vma, start, end);\r\nclear_tlb_flush_pending(mm);\r\nreturn pages;\r\n}\r\nunsigned long change_protection(struct vm_area_struct *vma, unsigned long start,\r\nunsigned long end, pgprot_t newprot,\r\nint dirty_accountable, int prot_numa)\r\n{\r\nunsigned long pages;\r\nif (is_vm_hugetlb_page(vma))\r\npages = hugetlb_change_protection(vma, start, end, newprot);\r\nelse\r\npages = change_protection_range(vma, start, end, newprot, dirty_accountable, prot_numa);\r\nreturn pages;\r\n}\r\nint\r\nmprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,\r\nunsigned long start, unsigned long end, unsigned long newflags)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nunsigned long oldflags = vma->vm_flags;\r\nlong nrpages = (end - start) >> PAGE_SHIFT;\r\nunsigned long charged = 0;\r\npgoff_t pgoff;\r\nint error;\r\nint dirty_accountable = 0;\r\nif (newflags == oldflags) {\r\n*pprev = vma;\r\nreturn 0;\r\n}\r\nif (newflags & VM_WRITE) {\r\nif (!may_expand_vm(mm, newflags, nrpages) &&\r\nmay_expand_vm(mm, oldflags, nrpages))\r\nreturn -ENOMEM;\r\nif (!(oldflags & (VM_ACCOUNT|VM_WRITE|VM_HUGETLB|\r\nVM_SHARED|VM_NORESERVE))) {\r\ncharged = nrpages;\r\nif (security_vm_enough_memory_mm(mm, charged))\r\nreturn -ENOMEM;\r\nnewflags |= VM_ACCOUNT;\r\n}\r\n}\r\npgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\r\n*pprev = vma_merge(mm, *pprev, start, end, newflags,\r\nvma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),\r\nvma->vm_userfaultfd_ctx);\r\nif (*pprev) {\r\nvma = *pprev;\r\nVM_WARN_ON((vma->vm_flags ^ newflags) & ~VM_SOFTDIRTY);\r\ngoto success;\r\n}\r\n*pprev = vma;\r\nif (start != vma->vm_start) {\r\nerror = split_vma(mm, vma, start, 1);\r\nif (error)\r\ngoto fail;\r\n}\r\nif (end != vma->vm_end) {\r\nerror = split_vma(mm, vma, end, 0);\r\nif (error)\r\ngoto fail;\r\n}\r\nsuccess:\r\nvma->vm_flags = newflags;\r\ndirty_accountable = vma_wants_writenotify(vma, vma->vm_page_prot);\r\nvma_set_page_prot(vma);\r\nchange_protection(vma, start, end, vma->vm_page_prot,\r\ndirty_accountable, 0);\r\nif ((oldflags & (VM_WRITE | VM_SHARED | VM_LOCKED)) == VM_LOCKED &&\r\n(newflags & VM_WRITE)) {\r\npopulate_vma_page_range(vma, start, end, NULL);\r\n}\r\nvm_stat_account(mm, oldflags, -nrpages);\r\nvm_stat_account(mm, newflags, nrpages);\r\nperf_event_mmap(vma);\r\nreturn 0;\r\nfail:\r\nvm_unacct_memory(charged);\r\nreturn error;\r\n}\r\nstatic int do_mprotect_pkey(unsigned long start, size_t len,\r\nunsigned long prot, int pkey)\r\n{\r\nunsigned long nstart, end, tmp, reqprot;\r\nstruct vm_area_struct *vma, *prev;\r\nint error = -EINVAL;\r\nconst int grows = prot & (PROT_GROWSDOWN|PROT_GROWSUP);\r\nconst bool rier = (current->personality & READ_IMPLIES_EXEC) &&\r\n(prot & PROT_READ);\r\nprot &= ~(PROT_GROWSDOWN|PROT_GROWSUP);\r\nif (grows == (PROT_GROWSDOWN|PROT_GROWSUP))\r\nreturn -EINVAL;\r\nif (start & ~PAGE_MASK)\r\nreturn -EINVAL;\r\nif (!len)\r\nreturn 0;\r\nlen = PAGE_ALIGN(len);\r\nend = start + len;\r\nif (end <= start)\r\nreturn -ENOMEM;\r\nif (!arch_validate_prot(prot))\r\nreturn -EINVAL;\r\nreqprot = prot;\r\nif (down_write_killable(&current->mm->mmap_sem))\r\nreturn -EINTR;\r\nerror = -EINVAL;\r\nif ((pkey != -1) && !mm_pkey_is_allocated(current->mm, pkey))\r\ngoto out;\r\nvma = find_vma(current->mm, start);\r\nerror = -ENOMEM;\r\nif (!vma)\r\ngoto out;\r\nprev = vma->vm_prev;\r\nif (unlikely(grows & PROT_GROWSDOWN)) {\r\nif (vma->vm_start >= end)\r\ngoto out;\r\nstart = vma->vm_start;\r\nerror = -EINVAL;\r\nif (!(vma->vm_flags & VM_GROWSDOWN))\r\ngoto out;\r\n} else {\r\nif (vma->vm_start > start)\r\ngoto out;\r\nif (unlikely(grows & PROT_GROWSUP)) {\r\nend = vma->vm_end;\r\nerror = -EINVAL;\r\nif (!(vma->vm_flags & VM_GROWSUP))\r\ngoto out;\r\n}\r\n}\r\nif (start > vma->vm_start)\r\nprev = vma;\r\nfor (nstart = start ; ; ) {\r\nunsigned long mask_off_old_flags;\r\nunsigned long newflags;\r\nint new_vma_pkey;\r\nif (rier && (vma->vm_flags & VM_MAYEXEC))\r\nprot |= PROT_EXEC;\r\nmask_off_old_flags = VM_READ | VM_WRITE | VM_EXEC |\r\nARCH_VM_PKEY_FLAGS;\r\nnew_vma_pkey = arch_override_mprotect_pkey(vma, prot, pkey);\r\nnewflags = calc_vm_prot_bits(prot, new_vma_pkey);\r\nnewflags |= (vma->vm_flags & ~mask_off_old_flags);\r\nif ((newflags & ~(newflags >> 4)) & (VM_READ | VM_WRITE | VM_EXEC)) {\r\nerror = -EACCES;\r\ngoto out;\r\n}\r\nerror = security_file_mprotect(vma, reqprot, prot);\r\nif (error)\r\ngoto out;\r\ntmp = vma->vm_end;\r\nif (tmp > end)\r\ntmp = end;\r\nerror = mprotect_fixup(vma, &prev, nstart, tmp, newflags);\r\nif (error)\r\ngoto out;\r\nnstart = tmp;\r\nif (nstart < prev->vm_end)\r\nnstart = prev->vm_end;\r\nif (nstart >= end)\r\ngoto out;\r\nvma = prev->vm_next;\r\nif (!vma || vma->vm_start != nstart) {\r\nerror = -ENOMEM;\r\ngoto out;\r\n}\r\nprot = reqprot;\r\n}\r\nout:\r\nup_write(&current->mm->mmap_sem);\r\nreturn error;\r\n}
