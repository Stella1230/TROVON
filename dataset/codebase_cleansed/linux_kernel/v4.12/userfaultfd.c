static int mcopy_atomic_pte(struct mm_struct *dst_mm,\r\npmd_t *dst_pmd,\r\nstruct vm_area_struct *dst_vma,\r\nunsigned long dst_addr,\r\nunsigned long src_addr,\r\nstruct page **pagep)\r\n{\r\nstruct mem_cgroup *memcg;\r\npte_t _dst_pte, *dst_pte;\r\nspinlock_t *ptl;\r\nvoid *page_kaddr;\r\nint ret;\r\nstruct page *page;\r\nif (!*pagep) {\r\nret = -ENOMEM;\r\npage = alloc_page_vma(GFP_HIGHUSER_MOVABLE, dst_vma, dst_addr);\r\nif (!page)\r\ngoto out;\r\npage_kaddr = kmap_atomic(page);\r\nret = copy_from_user(page_kaddr,\r\n(const void __user *) src_addr,\r\nPAGE_SIZE);\r\nkunmap_atomic(page_kaddr);\r\nif (unlikely(ret)) {\r\nret = -EFAULT;\r\n*pagep = page;\r\ngoto out;\r\n}\r\n} else {\r\npage = *pagep;\r\n*pagep = NULL;\r\n}\r\n__SetPageUptodate(page);\r\nret = -ENOMEM;\r\nif (mem_cgroup_try_charge(page, dst_mm, GFP_KERNEL, &memcg, false))\r\ngoto out_release;\r\n_dst_pte = mk_pte(page, dst_vma->vm_page_prot);\r\nif (dst_vma->vm_flags & VM_WRITE)\r\n_dst_pte = pte_mkwrite(pte_mkdirty(_dst_pte));\r\nret = -EEXIST;\r\ndst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\r\nif (!pte_none(*dst_pte))\r\ngoto out_release_uncharge_unlock;\r\ninc_mm_counter(dst_mm, MM_ANONPAGES);\r\npage_add_new_anon_rmap(page, dst_vma, dst_addr, false);\r\nmem_cgroup_commit_charge(page, memcg, false, false);\r\nlru_cache_add_active_or_unevictable(page, dst_vma);\r\nset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\r\nupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\r\npte_unmap_unlock(dst_pte, ptl);\r\nret = 0;\r\nout:\r\nreturn ret;\r\nout_release_uncharge_unlock:\r\npte_unmap_unlock(dst_pte, ptl);\r\nmem_cgroup_cancel_charge(page, memcg, false);\r\nout_release:\r\nput_page(page);\r\ngoto out;\r\n}\r\nstatic int mfill_zeropage_pte(struct mm_struct *dst_mm,\r\npmd_t *dst_pmd,\r\nstruct vm_area_struct *dst_vma,\r\nunsigned long dst_addr)\r\n{\r\npte_t _dst_pte, *dst_pte;\r\nspinlock_t *ptl;\r\nint ret;\r\n_dst_pte = pte_mkspecial(pfn_pte(my_zero_pfn(dst_addr),\r\ndst_vma->vm_page_prot));\r\nret = -EEXIST;\r\ndst_pte = pte_offset_map_lock(dst_mm, dst_pmd, dst_addr, &ptl);\r\nif (!pte_none(*dst_pte))\r\ngoto out_unlock;\r\nset_pte_at(dst_mm, dst_addr, dst_pte, _dst_pte);\r\nupdate_mmu_cache(dst_vma, dst_addr, dst_pte);\r\nret = 0;\r\nout_unlock:\r\npte_unmap_unlock(dst_pte, ptl);\r\nreturn ret;\r\n}\r\nstatic pmd_t *mm_alloc_pmd(struct mm_struct *mm, unsigned long address)\r\n{\r\npgd_t *pgd;\r\np4d_t *p4d;\r\npud_t *pud;\r\npgd = pgd_offset(mm, address);\r\np4d = p4d_alloc(mm, pgd, address);\r\nif (!p4d)\r\nreturn NULL;\r\npud = pud_alloc(mm, p4d, address);\r\nif (!pud)\r\nreturn NULL;\r\nreturn pmd_alloc(mm, pud, address);\r\n}\r\nstatic __always_inline ssize_t __mcopy_atomic_hugetlb(struct mm_struct *dst_mm,\r\nstruct vm_area_struct *dst_vma,\r\nunsigned long dst_start,\r\nunsigned long src_start,\r\nunsigned long len,\r\nbool zeropage)\r\n{\r\nint vm_alloc_shared = dst_vma->vm_flags & VM_SHARED;\r\nint vm_shared = dst_vma->vm_flags & VM_SHARED;\r\nssize_t err;\r\npte_t *dst_pte;\r\nunsigned long src_addr, dst_addr;\r\nlong copied;\r\nstruct page *page;\r\nstruct hstate *h;\r\nunsigned long vma_hpagesize;\r\npgoff_t idx;\r\nu32 hash;\r\nstruct address_space *mapping;\r\nif (zeropage) {\r\nup_read(&dst_mm->mmap_sem);\r\nreturn -EINVAL;\r\n}\r\nsrc_addr = src_start;\r\ndst_addr = dst_start;\r\ncopied = 0;\r\npage = NULL;\r\nvma_hpagesize = vma_kernel_pagesize(dst_vma);\r\nerr = -EINVAL;\r\nif (dst_start & (vma_hpagesize - 1) || len & (vma_hpagesize - 1))\r\ngoto out_unlock;\r\nretry:\r\nif (!dst_vma) {\r\nerr = -ENOENT;\r\ndst_vma = find_vma(dst_mm, dst_start);\r\nif (!dst_vma || !is_vm_hugetlb_page(dst_vma))\r\ngoto out_unlock;\r\nif (!dst_vma->vm_userfaultfd_ctx.ctx)\r\ngoto out_unlock;\r\nif (dst_start < dst_vma->vm_start ||\r\ndst_start + len > dst_vma->vm_end)\r\ngoto out_unlock;\r\nerr = -EINVAL;\r\nif (vma_hpagesize != vma_kernel_pagesize(dst_vma))\r\ngoto out_unlock;\r\nvm_shared = dst_vma->vm_flags & VM_SHARED;\r\n}\r\nif (WARN_ON(dst_addr & (vma_hpagesize - 1) ||\r\n(len - copied) & (vma_hpagesize - 1)))\r\ngoto out_unlock;\r\nerr = -ENOMEM;\r\nif (!vm_shared) {\r\nif (unlikely(anon_vma_prepare(dst_vma)))\r\ngoto out_unlock;\r\n}\r\nh = hstate_vma(dst_vma);\r\nwhile (src_addr < src_start + len) {\r\npte_t dst_pteval;\r\nBUG_ON(dst_addr >= dst_start + len);\r\nVM_BUG_ON(dst_addr & ~huge_page_mask(h));\r\nidx = linear_page_index(dst_vma, dst_addr);\r\nmapping = dst_vma->vm_file->f_mapping;\r\nhash = hugetlb_fault_mutex_hash(h, dst_mm, dst_vma, mapping,\r\nidx, dst_addr);\r\nmutex_lock(&hugetlb_fault_mutex_table[hash]);\r\nerr = -ENOMEM;\r\ndst_pte = huge_pte_alloc(dst_mm, dst_addr, huge_page_size(h));\r\nif (!dst_pte) {\r\nmutex_unlock(&hugetlb_fault_mutex_table[hash]);\r\ngoto out_unlock;\r\n}\r\nerr = -EEXIST;\r\ndst_pteval = huge_ptep_get(dst_pte);\r\nif (!huge_pte_none(dst_pteval)) {\r\nmutex_unlock(&hugetlb_fault_mutex_table[hash]);\r\ngoto out_unlock;\r\n}\r\nerr = hugetlb_mcopy_atomic_pte(dst_mm, dst_pte, dst_vma,\r\ndst_addr, src_addr, &page);\r\nmutex_unlock(&hugetlb_fault_mutex_table[hash]);\r\nvm_alloc_shared = vm_shared;\r\ncond_resched();\r\nif (unlikely(err == -EFAULT)) {\r\nup_read(&dst_mm->mmap_sem);\r\nBUG_ON(!page);\r\nerr = copy_huge_page_from_user(page,\r\n(const void __user *)src_addr,\r\npages_per_huge_page(h), true);\r\nif (unlikely(err)) {\r\nerr = -EFAULT;\r\ngoto out;\r\n}\r\ndown_read(&dst_mm->mmap_sem);\r\ndst_vma = NULL;\r\ngoto retry;\r\n} else\r\nBUG_ON(page);\r\nif (!err) {\r\ndst_addr += vma_hpagesize;\r\nsrc_addr += vma_hpagesize;\r\ncopied += vma_hpagesize;\r\nif (fatal_signal_pending(current))\r\nerr = -EINTR;\r\n}\r\nif (err)\r\nbreak;\r\n}\r\nout_unlock:\r\nup_read(&dst_mm->mmap_sem);\r\nout:\r\nif (page) {\r\nif (vm_alloc_shared)\r\nSetPagePrivate(page);\r\nelse\r\nClearPagePrivate(page);\r\nput_page(page);\r\n}\r\nBUG_ON(copied < 0);\r\nBUG_ON(err > 0);\r\nBUG_ON(!copied && !err);\r\nreturn copied ? copied : err;\r\n}\r\nstatic __always_inline ssize_t __mcopy_atomic(struct mm_struct *dst_mm,\r\nunsigned long dst_start,\r\nunsigned long src_start,\r\nunsigned long len,\r\nbool zeropage)\r\n{\r\nstruct vm_area_struct *dst_vma;\r\nssize_t err;\r\npmd_t *dst_pmd;\r\nunsigned long src_addr, dst_addr;\r\nlong copied;\r\nstruct page *page;\r\nBUG_ON(dst_start & ~PAGE_MASK);\r\nBUG_ON(len & ~PAGE_MASK);\r\nBUG_ON(src_start + len <= src_start);\r\nBUG_ON(dst_start + len <= dst_start);\r\nsrc_addr = src_start;\r\ndst_addr = dst_start;\r\ncopied = 0;\r\npage = NULL;\r\nretry:\r\ndown_read(&dst_mm->mmap_sem);\r\nerr = -ENOENT;\r\ndst_vma = find_vma(dst_mm, dst_start);\r\nif (!dst_vma)\r\ngoto out_unlock;\r\nif (!dst_vma->vm_userfaultfd_ctx.ctx)\r\ngoto out_unlock;\r\nif (dst_start < dst_vma->vm_start ||\r\ndst_start + len > dst_vma->vm_end)\r\ngoto out_unlock;\r\nerr = -EINVAL;\r\nif (WARN_ON_ONCE(vma_is_anonymous(dst_vma) &&\r\ndst_vma->vm_flags & VM_SHARED))\r\ngoto out_unlock;\r\nif (is_vm_hugetlb_page(dst_vma))\r\nreturn __mcopy_atomic_hugetlb(dst_mm, dst_vma, dst_start,\r\nsrc_start, len, zeropage);\r\nif (!vma_is_anonymous(dst_vma) && !vma_is_shmem(dst_vma))\r\ngoto out_unlock;\r\nerr = -ENOMEM;\r\nif (vma_is_anonymous(dst_vma) && unlikely(anon_vma_prepare(dst_vma)))\r\ngoto out_unlock;\r\nwhile (src_addr < src_start + len) {\r\npmd_t dst_pmdval;\r\nBUG_ON(dst_addr >= dst_start + len);\r\ndst_pmd = mm_alloc_pmd(dst_mm, dst_addr);\r\nif (unlikely(!dst_pmd)) {\r\nerr = -ENOMEM;\r\nbreak;\r\n}\r\ndst_pmdval = pmd_read_atomic(dst_pmd);\r\nif (unlikely(pmd_trans_huge(dst_pmdval))) {\r\nerr = -EEXIST;\r\nbreak;\r\n}\r\nif (unlikely(pmd_none(dst_pmdval)) &&\r\nunlikely(__pte_alloc(dst_mm, dst_pmd, dst_addr))) {\r\nerr = -ENOMEM;\r\nbreak;\r\n}\r\nif (unlikely(pmd_trans_huge(*dst_pmd))) {\r\nerr = -EFAULT;\r\nbreak;\r\n}\r\nBUG_ON(pmd_none(*dst_pmd));\r\nBUG_ON(pmd_trans_huge(*dst_pmd));\r\nif (vma_is_anonymous(dst_vma)) {\r\nif (!zeropage)\r\nerr = mcopy_atomic_pte(dst_mm, dst_pmd, dst_vma,\r\ndst_addr, src_addr,\r\n&page);\r\nelse\r\nerr = mfill_zeropage_pte(dst_mm, dst_pmd,\r\ndst_vma, dst_addr);\r\n} else {\r\nerr = -EINVAL;\r\nif (likely(!zeropage))\r\nerr = shmem_mcopy_atomic_pte(dst_mm, dst_pmd,\r\ndst_vma, dst_addr,\r\nsrc_addr, &page);\r\n}\r\ncond_resched();\r\nif (unlikely(err == -EFAULT)) {\r\nvoid *page_kaddr;\r\nup_read(&dst_mm->mmap_sem);\r\nBUG_ON(!page);\r\npage_kaddr = kmap(page);\r\nerr = copy_from_user(page_kaddr,\r\n(const void __user *) src_addr,\r\nPAGE_SIZE);\r\nkunmap(page);\r\nif (unlikely(err)) {\r\nerr = -EFAULT;\r\ngoto out;\r\n}\r\ngoto retry;\r\n} else\r\nBUG_ON(page);\r\nif (!err) {\r\ndst_addr += PAGE_SIZE;\r\nsrc_addr += PAGE_SIZE;\r\ncopied += PAGE_SIZE;\r\nif (fatal_signal_pending(current))\r\nerr = -EINTR;\r\n}\r\nif (err)\r\nbreak;\r\n}\r\nout_unlock:\r\nup_read(&dst_mm->mmap_sem);\r\nout:\r\nif (page)\r\nput_page(page);\r\nBUG_ON(copied < 0);\r\nBUG_ON(err > 0);\r\nBUG_ON(!copied && !err);\r\nreturn copied ? copied : err;\r\n}\r\nssize_t mcopy_atomic(struct mm_struct *dst_mm, unsigned long dst_start,\r\nunsigned long src_start, unsigned long len)\r\n{\r\nreturn __mcopy_atomic(dst_mm, dst_start, src_start, len, false);\r\n}\r\nssize_t mfill_zeropage(struct mm_struct *dst_mm, unsigned long start,\r\nunsigned long len)\r\n{\r\nreturn __mcopy_atomic(dst_mm, start, 0, len, true);\r\n}
