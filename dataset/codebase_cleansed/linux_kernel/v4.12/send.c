void rds_send_path_reset(struct rds_conn_path *cp)\r\n{\r\nstruct rds_message *rm, *tmp;\r\nunsigned long flags;\r\nif (cp->cp_xmit_rm) {\r\nrm = cp->cp_xmit_rm;\r\ncp->cp_xmit_rm = NULL;\r\nrds_message_unmapped(rm);\r\nrds_message_put(rm);\r\n}\r\ncp->cp_xmit_sg = 0;\r\ncp->cp_xmit_hdr_off = 0;\r\ncp->cp_xmit_data_off = 0;\r\ncp->cp_xmit_atomic_sent = 0;\r\ncp->cp_xmit_rdma_sent = 0;\r\ncp->cp_xmit_data_sent = 0;\r\ncp->cp_conn->c_map_queued = 0;\r\ncp->cp_unacked_packets = rds_sysctl_max_unacked_packets;\r\ncp->cp_unacked_bytes = rds_sysctl_max_unacked_bytes;\r\nspin_lock_irqsave(&cp->cp_lock, flags);\r\nlist_for_each_entry_safe(rm, tmp, &cp->cp_retrans, m_conn_item) {\r\nset_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\r\nset_bit(RDS_MSG_RETRANSMITTED, &rm->m_flags);\r\n}\r\nlist_splice_init(&cp->cp_retrans, &cp->cp_send_queue);\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\n}\r\nstatic int acquire_in_xmit(struct rds_conn_path *cp)\r\n{\r\nreturn test_and_set_bit(RDS_IN_XMIT, &cp->cp_flags) == 0;\r\n}\r\nstatic void release_in_xmit(struct rds_conn_path *cp)\r\n{\r\nclear_bit(RDS_IN_XMIT, &cp->cp_flags);\r\nsmp_mb__after_atomic();\r\nif (waitqueue_active(&cp->cp_waitq))\r\nwake_up_all(&cp->cp_waitq);\r\n}\r\nint rds_send_xmit(struct rds_conn_path *cp)\r\n{\r\nstruct rds_connection *conn = cp->cp_conn;\r\nstruct rds_message *rm;\r\nunsigned long flags;\r\nunsigned int tmp;\r\nstruct scatterlist *sg;\r\nint ret = 0;\r\nLIST_HEAD(to_be_dropped);\r\nint batch_count;\r\nunsigned long send_gen = 0;\r\nrestart:\r\nbatch_count = 0;\r\nif (!acquire_in_xmit(cp)) {\r\nrds_stats_inc(s_send_lock_contention);\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ncp->cp_send_gen++;\r\nsend_gen = cp->cp_send_gen;\r\nif (!rds_conn_path_up(cp)) {\r\nrelease_in_xmit(cp);\r\nret = 0;\r\ngoto out;\r\n}\r\nif (conn->c_trans->xmit_path_prepare)\r\nconn->c_trans->xmit_path_prepare(cp);\r\nwhile (1) {\r\nrm = cp->cp_xmit_rm;\r\nif (!rm && test_and_clear_bit(0, &conn->c_map_queued)) {\r\nrm = rds_cong_update_alloc(conn);\r\nif (IS_ERR(rm)) {\r\nret = PTR_ERR(rm);\r\nbreak;\r\n}\r\nrm->data.op_active = 1;\r\nrm->m_inc.i_conn_path = cp;\r\nrm->m_inc.i_conn = cp->cp_conn;\r\ncp->cp_xmit_rm = rm;\r\n}\r\nif (!rm) {\r\nunsigned int len;\r\nbatch_count++;\r\nif (batch_count >= send_batch_count)\r\ngoto over_batch;\r\nspin_lock_irqsave(&cp->cp_lock, flags);\r\nif (!list_empty(&cp->cp_send_queue)) {\r\nrm = list_entry(cp->cp_send_queue.next,\r\nstruct rds_message,\r\nm_conn_item);\r\nrds_message_addref(rm);\r\nlist_move_tail(&rm->m_conn_item,\r\n&cp->cp_retrans);\r\n}\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\nif (!rm)\r\nbreak;\r\nif (test_bit(RDS_MSG_FLUSH, &rm->m_flags) ||\r\n(rm->rdma.op_active &&\r\ntest_bit(RDS_MSG_RETRANSMITTED, &rm->m_flags))) {\r\nspin_lock_irqsave(&cp->cp_lock, flags);\r\nif (test_and_clear_bit(RDS_MSG_ON_CONN, &rm->m_flags))\r\nlist_move(&rm->m_conn_item, &to_be_dropped);\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\ncontinue;\r\n}\r\nlen = ntohl(rm->m_inc.i_hdr.h_len);\r\nif (cp->cp_unacked_packets == 0 ||\r\ncp->cp_unacked_bytes < len) {\r\n__set_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\r\ncp->cp_unacked_packets =\r\nrds_sysctl_max_unacked_packets;\r\ncp->cp_unacked_bytes =\r\nrds_sysctl_max_unacked_bytes;\r\nrds_stats_inc(s_send_ack_required);\r\n} else {\r\ncp->cp_unacked_bytes -= len;\r\ncp->cp_unacked_packets--;\r\n}\r\ncp->cp_xmit_rm = rm;\r\n}\r\nif (rm->rdma.op_active && !cp->cp_xmit_rdma_sent) {\r\nrm->m_final_op = &rm->rdma;\r\nset_bit(RDS_MSG_MAPPED, &rm->m_flags);\r\nret = conn->c_trans->xmit_rdma(conn, &rm->rdma);\r\nif (ret) {\r\nclear_bit(RDS_MSG_MAPPED, &rm->m_flags);\r\nwake_up_interruptible(&rm->m_flush_wait);\r\nbreak;\r\n}\r\ncp->cp_xmit_rdma_sent = 1;\r\n}\r\nif (rm->atomic.op_active && !cp->cp_xmit_atomic_sent) {\r\nrm->m_final_op = &rm->atomic;\r\nset_bit(RDS_MSG_MAPPED, &rm->m_flags);\r\nret = conn->c_trans->xmit_atomic(conn, &rm->atomic);\r\nif (ret) {\r\nclear_bit(RDS_MSG_MAPPED, &rm->m_flags);\r\nwake_up_interruptible(&rm->m_flush_wait);\r\nbreak;\r\n}\r\ncp->cp_xmit_atomic_sent = 1;\r\n}\r\nif (rm->data.op_nents == 0) {\r\nint ops_present;\r\nint all_ops_are_silent = 1;\r\nops_present = (rm->atomic.op_active || rm->rdma.op_active);\r\nif (rm->atomic.op_active && !rm->atomic.op_silent)\r\nall_ops_are_silent = 0;\r\nif (rm->rdma.op_active && !rm->rdma.op_silent)\r\nall_ops_are_silent = 0;\r\nif (ops_present && all_ops_are_silent\r\n&& !rm->m_rdma_cookie)\r\nrm->data.op_active = 0;\r\n}\r\nif (rm->data.op_active && !cp->cp_xmit_data_sent) {\r\nrm->m_final_op = &rm->data;\r\nret = conn->c_trans->xmit(conn, rm,\r\ncp->cp_xmit_hdr_off,\r\ncp->cp_xmit_sg,\r\ncp->cp_xmit_data_off);\r\nif (ret <= 0)\r\nbreak;\r\nif (cp->cp_xmit_hdr_off < sizeof(struct rds_header)) {\r\ntmp = min_t(int, ret,\r\nsizeof(struct rds_header) -\r\ncp->cp_xmit_hdr_off);\r\ncp->cp_xmit_hdr_off += tmp;\r\nret -= tmp;\r\n}\r\nsg = &rm->data.op_sg[cp->cp_xmit_sg];\r\nwhile (ret) {\r\ntmp = min_t(int, ret, sg->length -\r\ncp->cp_xmit_data_off);\r\ncp->cp_xmit_data_off += tmp;\r\nret -= tmp;\r\nif (cp->cp_xmit_data_off == sg->length) {\r\ncp->cp_xmit_data_off = 0;\r\nsg++;\r\ncp->cp_xmit_sg++;\r\nBUG_ON(ret != 0 && cp->cp_xmit_sg ==\r\nrm->data.op_nents);\r\n}\r\n}\r\nif (cp->cp_xmit_hdr_off == sizeof(struct rds_header) &&\r\n(cp->cp_xmit_sg == rm->data.op_nents))\r\ncp->cp_xmit_data_sent = 1;\r\n}\r\nif (!rm->data.op_active || cp->cp_xmit_data_sent) {\r\ncp->cp_xmit_rm = NULL;\r\ncp->cp_xmit_sg = 0;\r\ncp->cp_xmit_hdr_off = 0;\r\ncp->cp_xmit_data_off = 0;\r\ncp->cp_xmit_rdma_sent = 0;\r\ncp->cp_xmit_atomic_sent = 0;\r\ncp->cp_xmit_data_sent = 0;\r\nrds_message_put(rm);\r\n}\r\n}\r\nover_batch:\r\nif (conn->c_trans->xmit_path_complete)\r\nconn->c_trans->xmit_path_complete(cp);\r\nrelease_in_xmit(cp);\r\nif (!list_empty(&to_be_dropped)) {\r\nlist_for_each_entry(rm, &to_be_dropped, m_conn_item)\r\nrds_message_put(rm);\r\nrds_send_remove_from_sock(&to_be_dropped, RDS_RDMA_DROPPED);\r\n}\r\nif (ret == 0) {\r\nsmp_mb();\r\nif ((test_bit(0, &conn->c_map_queued) ||\r\n!list_empty(&cp->cp_send_queue)) &&\r\nsend_gen == cp->cp_send_gen) {\r\nrds_stats_inc(s_send_lock_queue_raced);\r\nif (batch_count < send_batch_count)\r\ngoto restart;\r\nqueue_delayed_work(rds_wq, &cp->cp_send_w, 1);\r\n}\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic void rds_send_sndbuf_remove(struct rds_sock *rs, struct rds_message *rm)\r\n{\r\nu32 len = be32_to_cpu(rm->m_inc.i_hdr.h_len);\r\nassert_spin_locked(&rs->rs_lock);\r\nBUG_ON(rs->rs_snd_bytes < len);\r\nrs->rs_snd_bytes -= len;\r\nif (rs->rs_snd_bytes == 0)\r\nrds_stats_inc(s_send_queue_empty);\r\n}\r\nstatic inline int rds_send_is_acked(struct rds_message *rm, u64 ack,\r\nis_acked_func is_acked)\r\n{\r\nif (is_acked)\r\nreturn is_acked(rm, ack);\r\nreturn be64_to_cpu(rm->m_inc.i_hdr.h_sequence) <= ack;\r\n}\r\nvoid rds_rdma_send_complete(struct rds_message *rm, int status)\r\n{\r\nstruct rds_sock *rs = NULL;\r\nstruct rm_rdma_op *ro;\r\nstruct rds_notifier *notifier;\r\nunsigned long flags;\r\nunsigned int notify = 0;\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nnotify = rm->rdma.op_notify | rm->data.op_notify;\r\nro = &rm->rdma;\r\nif (test_bit(RDS_MSG_ON_SOCK, &rm->m_flags) &&\r\nro->op_active && notify && ro->op_notifier) {\r\nnotifier = ro->op_notifier;\r\nrs = rm->m_rs;\r\nsock_hold(rds_rs_to_sk(rs));\r\nnotifier->n_status = status;\r\nspin_lock(&rs->rs_lock);\r\nlist_add_tail(&notifier->n_list, &rs->rs_notify_queue);\r\nspin_unlock(&rs->rs_lock);\r\nro->op_notifier = NULL;\r\n}\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nif (rs) {\r\nrds_wake_sk_sleep(rs);\r\nsock_put(rds_rs_to_sk(rs));\r\n}\r\n}\r\nvoid rds_atomic_send_complete(struct rds_message *rm, int status)\r\n{\r\nstruct rds_sock *rs = NULL;\r\nstruct rm_atomic_op *ao;\r\nstruct rds_notifier *notifier;\r\nunsigned long flags;\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nao = &rm->atomic;\r\nif (test_bit(RDS_MSG_ON_SOCK, &rm->m_flags)\r\n&& ao->op_active && ao->op_notify && ao->op_notifier) {\r\nnotifier = ao->op_notifier;\r\nrs = rm->m_rs;\r\nsock_hold(rds_rs_to_sk(rs));\r\nnotifier->n_status = status;\r\nspin_lock(&rs->rs_lock);\r\nlist_add_tail(&notifier->n_list, &rs->rs_notify_queue);\r\nspin_unlock(&rs->rs_lock);\r\nao->op_notifier = NULL;\r\n}\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nif (rs) {\r\nrds_wake_sk_sleep(rs);\r\nsock_put(rds_rs_to_sk(rs));\r\n}\r\n}\r\nstatic inline void\r\n__rds_send_complete(struct rds_sock *rs, struct rds_message *rm, int status)\r\n{\r\nstruct rm_rdma_op *ro;\r\nstruct rm_atomic_op *ao;\r\nro = &rm->rdma;\r\nif (ro->op_active && ro->op_notify && ro->op_notifier) {\r\nro->op_notifier->n_status = status;\r\nlist_add_tail(&ro->op_notifier->n_list, &rs->rs_notify_queue);\r\nro->op_notifier = NULL;\r\n}\r\nao = &rm->atomic;\r\nif (ao->op_active && ao->op_notify && ao->op_notifier) {\r\nao->op_notifier->n_status = status;\r\nlist_add_tail(&ao->op_notifier->n_list, &rs->rs_notify_queue);\r\nao->op_notifier = NULL;\r\n}\r\n}\r\nstatic void rds_send_remove_from_sock(struct list_head *messages, int status)\r\n{\r\nunsigned long flags;\r\nstruct rds_sock *rs = NULL;\r\nstruct rds_message *rm;\r\nwhile (!list_empty(messages)) {\r\nint was_on_sock = 0;\r\nrm = list_entry(messages->next, struct rds_message,\r\nm_conn_item);\r\nlist_del_init(&rm->m_conn_item);\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nif (!test_bit(RDS_MSG_ON_SOCK, &rm->m_flags))\r\ngoto unlock_and_drop;\r\nif (rs != rm->m_rs) {\r\nif (rs) {\r\nrds_wake_sk_sleep(rs);\r\nsock_put(rds_rs_to_sk(rs));\r\n}\r\nrs = rm->m_rs;\r\nif (rs)\r\nsock_hold(rds_rs_to_sk(rs));\r\n}\r\nif (!rs)\r\ngoto unlock_and_drop;\r\nspin_lock(&rs->rs_lock);\r\nif (test_and_clear_bit(RDS_MSG_ON_SOCK, &rm->m_flags)) {\r\nstruct rm_rdma_op *ro = &rm->rdma;\r\nstruct rds_notifier *notifier;\r\nlist_del_init(&rm->m_sock_item);\r\nrds_send_sndbuf_remove(rs, rm);\r\nif (ro->op_active && ro->op_notifier &&\r\n(ro->op_notify || (ro->op_recverr && status))) {\r\nnotifier = ro->op_notifier;\r\nlist_add_tail(&notifier->n_list,\r\n&rs->rs_notify_queue);\r\nif (!notifier->n_status)\r\nnotifier->n_status = status;\r\nrm->rdma.op_notifier = NULL;\r\n}\r\nwas_on_sock = 1;\r\nrm->m_rs = NULL;\r\n}\r\nspin_unlock(&rs->rs_lock);\r\nunlock_and_drop:\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nrds_message_put(rm);\r\nif (was_on_sock)\r\nrds_message_put(rm);\r\n}\r\nif (rs) {\r\nrds_wake_sk_sleep(rs);\r\nsock_put(rds_rs_to_sk(rs));\r\n}\r\n}\r\nvoid rds_send_path_drop_acked(struct rds_conn_path *cp, u64 ack,\r\nis_acked_func is_acked)\r\n{\r\nstruct rds_message *rm, *tmp;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\nspin_lock_irqsave(&cp->cp_lock, flags);\r\nlist_for_each_entry_safe(rm, tmp, &cp->cp_retrans, m_conn_item) {\r\nif (!rds_send_is_acked(rm, ack, is_acked))\r\nbreak;\r\nlist_move(&rm->m_conn_item, &list);\r\nclear_bit(RDS_MSG_ON_CONN, &rm->m_flags);\r\n}\r\nif (!list_empty(&list))\r\nsmp_mb__after_atomic();\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\nrds_send_remove_from_sock(&list, RDS_RDMA_SUCCESS);\r\n}\r\nvoid rds_send_drop_acked(struct rds_connection *conn, u64 ack,\r\nis_acked_func is_acked)\r\n{\r\nWARN_ON(conn->c_trans->t_mp_capable);\r\nrds_send_path_drop_acked(&conn->c_path[0], ack, is_acked);\r\n}\r\nvoid rds_send_drop_to(struct rds_sock *rs, struct sockaddr_in *dest)\r\n{\r\nstruct rds_message *rm, *tmp;\r\nstruct rds_connection *conn;\r\nstruct rds_conn_path *cp;\r\nunsigned long flags;\r\nLIST_HEAD(list);\r\nspin_lock_irqsave(&rs->rs_lock, flags);\r\nlist_for_each_entry_safe(rm, tmp, &rs->rs_send_queue, m_sock_item) {\r\nif (dest && (dest->sin_addr.s_addr != rm->m_daddr ||\r\ndest->sin_port != rm->m_inc.i_hdr.h_dport))\r\ncontinue;\r\nlist_move(&rm->m_sock_item, &list);\r\nrds_send_sndbuf_remove(rs, rm);\r\nclear_bit(RDS_MSG_ON_SOCK, &rm->m_flags);\r\n}\r\nsmp_mb__after_atomic();\r\nspin_unlock_irqrestore(&rs->rs_lock, flags);\r\nif (list_empty(&list))\r\nreturn;\r\nlist_for_each_entry(rm, &list, m_sock_item) {\r\nconn = rm->m_inc.i_conn;\r\nif (conn->c_trans->t_mp_capable)\r\ncp = rm->m_inc.i_conn_path;\r\nelse\r\ncp = &conn->c_path[0];\r\nspin_lock_irqsave(&cp->cp_lock, flags);\r\nif (!test_and_clear_bit(RDS_MSG_ON_CONN, &rm->m_flags)) {\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nrm->m_rs = NULL;\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\ncontinue;\r\n}\r\nlist_del_init(&rm->m_conn_item);\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nspin_lock(&rs->rs_lock);\r\n__rds_send_complete(rs, rm, RDS_RDMA_CANCELED);\r\nspin_unlock(&rs->rs_lock);\r\nrm->m_rs = NULL;\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nrds_message_put(rm);\r\n}\r\nrds_wake_sk_sleep(rs);\r\nwhile (!list_empty(&list)) {\r\nrm = list_entry(list.next, struct rds_message, m_sock_item);\r\nlist_del_init(&rm->m_sock_item);\r\nrds_message_wait(rm);\r\nspin_lock_irqsave(&rm->m_rs_lock, flags);\r\nspin_lock(&rs->rs_lock);\r\n__rds_send_complete(rs, rm, RDS_RDMA_CANCELED);\r\nspin_unlock(&rs->rs_lock);\r\nrm->m_rs = NULL;\r\nspin_unlock_irqrestore(&rm->m_rs_lock, flags);\r\nrds_message_put(rm);\r\n}\r\n}\r\nstatic int rds_send_queue_rm(struct rds_sock *rs, struct rds_connection *conn,\r\nstruct rds_conn_path *cp,\r\nstruct rds_message *rm, __be16 sport,\r\n__be16 dport, int *queued)\r\n{\r\nunsigned long flags;\r\nu32 len;\r\nif (*queued)\r\ngoto out;\r\nlen = be32_to_cpu(rm->m_inc.i_hdr.h_len);\r\nspin_lock_irqsave(&rs->rs_lock, flags);\r\nif (rs->rs_snd_bytes < rds_sk_sndbuf(rs)) {\r\nrs->rs_snd_bytes += len;\r\nif (rs->rs_snd_bytes >= rds_sk_sndbuf(rs) / 2)\r\n__set_bit(RDS_MSG_ACK_REQUIRED, &rm->m_flags);\r\nlist_add_tail(&rm->m_sock_item, &rs->rs_send_queue);\r\nset_bit(RDS_MSG_ON_SOCK, &rm->m_flags);\r\nrds_message_addref(rm);\r\nrm->m_rs = rs;\r\nrds_message_populate_header(&rm->m_inc.i_hdr, sport, dport, 0);\r\nrm->m_inc.i_conn = conn;\r\nrm->m_inc.i_conn_path = cp;\r\nrds_message_addref(rm);\r\nspin_lock(&cp->cp_lock);\r\nrm->m_inc.i_hdr.h_sequence = cpu_to_be64(cp->cp_next_tx_seq++);\r\nlist_add_tail(&rm->m_conn_item, &cp->cp_send_queue);\r\nset_bit(RDS_MSG_ON_CONN, &rm->m_flags);\r\nspin_unlock(&cp->cp_lock);\r\nrdsdebug("queued msg %p len %d, rs %p bytes %d seq %llu\n",\r\nrm, len, rs, rs->rs_snd_bytes,\r\n(unsigned long long)be64_to_cpu(rm->m_inc.i_hdr.h_sequence));\r\n*queued = 1;\r\n}\r\nspin_unlock_irqrestore(&rs->rs_lock, flags);\r\nout:\r\nreturn *queued;\r\n}\r\nstatic int rds_rm_size(struct msghdr *msg, int data_len)\r\n{\r\nstruct cmsghdr *cmsg;\r\nint size = 0;\r\nint cmsg_groups = 0;\r\nint retval;\r\nfor_each_cmsghdr(cmsg, msg) {\r\nif (!CMSG_OK(msg, cmsg))\r\nreturn -EINVAL;\r\nif (cmsg->cmsg_level != SOL_RDS)\r\ncontinue;\r\nswitch (cmsg->cmsg_type) {\r\ncase RDS_CMSG_RDMA_ARGS:\r\ncmsg_groups |= 1;\r\nretval = rds_rdma_extra_size(CMSG_DATA(cmsg));\r\nif (retval < 0)\r\nreturn retval;\r\nsize += retval;\r\nbreak;\r\ncase RDS_CMSG_RDMA_DEST:\r\ncase RDS_CMSG_RDMA_MAP:\r\ncmsg_groups |= 2;\r\nbreak;\r\ncase RDS_CMSG_ATOMIC_CSWP:\r\ncase RDS_CMSG_ATOMIC_FADD:\r\ncase RDS_CMSG_MASKED_ATOMIC_CSWP:\r\ncase RDS_CMSG_MASKED_ATOMIC_FADD:\r\ncmsg_groups |= 1;\r\nsize += sizeof(struct scatterlist);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nsize += ceil(data_len, PAGE_SIZE) * sizeof(struct scatterlist);\r\nif (cmsg_groups == 3)\r\nreturn -EINVAL;\r\nreturn size;\r\n}\r\nstatic int rds_cmsg_send(struct rds_sock *rs, struct rds_message *rm,\r\nstruct msghdr *msg, int *allocated_mr)\r\n{\r\nstruct cmsghdr *cmsg;\r\nint ret = 0;\r\nfor_each_cmsghdr(cmsg, msg) {\r\nif (!CMSG_OK(msg, cmsg))\r\nreturn -EINVAL;\r\nif (cmsg->cmsg_level != SOL_RDS)\r\ncontinue;\r\nswitch (cmsg->cmsg_type) {\r\ncase RDS_CMSG_RDMA_ARGS:\r\nret = rds_cmsg_rdma_args(rs, rm, cmsg);\r\nbreak;\r\ncase RDS_CMSG_RDMA_DEST:\r\nret = rds_cmsg_rdma_dest(rs, rm, cmsg);\r\nbreak;\r\ncase RDS_CMSG_RDMA_MAP:\r\nret = rds_cmsg_rdma_map(rs, rm, cmsg);\r\nif (!ret)\r\n*allocated_mr = 1;\r\nelse if (ret == -ENODEV)\r\nret = -EAGAIN;\r\nbreak;\r\ncase RDS_CMSG_ATOMIC_CSWP:\r\ncase RDS_CMSG_ATOMIC_FADD:\r\ncase RDS_CMSG_MASKED_ATOMIC_CSWP:\r\ncase RDS_CMSG_MASKED_ATOMIC_FADD:\r\nret = rds_cmsg_atomic(rs, rm, cmsg);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (ret)\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int rds_send_mprds_hash(struct rds_sock *rs, struct rds_connection *conn)\r\n{\r\nint hash;\r\nif (conn->c_npaths == 0)\r\nhash = RDS_MPATH_HASH(rs, RDS_MPATH_WORKERS);\r\nelse\r\nhash = RDS_MPATH_HASH(rs, conn->c_npaths);\r\nif (conn->c_npaths == 0 && hash != 0) {\r\nrds_send_ping(conn);\r\nif (conn->c_npaths == 0) {\r\nwait_event_interruptible(conn->c_hs_waitq,\r\n(conn->c_npaths != 0));\r\n}\r\nif (conn->c_npaths == 1)\r\nhash = 0;\r\n}\r\nreturn hash;\r\n}\r\nstatic int rds_rdma_bytes(struct msghdr *msg, size_t *rdma_bytes)\r\n{\r\nstruct rds_rdma_args *args;\r\nstruct cmsghdr *cmsg;\r\nfor_each_cmsghdr(cmsg, msg) {\r\nif (!CMSG_OK(msg, cmsg))\r\nreturn -EINVAL;\r\nif (cmsg->cmsg_level != SOL_RDS)\r\ncontinue;\r\nif (cmsg->cmsg_type == RDS_CMSG_RDMA_ARGS) {\r\nargs = CMSG_DATA(cmsg);\r\n*rdma_bytes += args->remote_vec.bytes;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint rds_sendmsg(struct socket *sock, struct msghdr *msg, size_t payload_len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct rds_sock *rs = rds_sk_to_rs(sk);\r\nDECLARE_SOCKADDR(struct sockaddr_in *, usin, msg->msg_name);\r\n__be32 daddr;\r\n__be16 dport;\r\nstruct rds_message *rm = NULL;\r\nstruct rds_connection *conn;\r\nint ret = 0;\r\nint queued = 0, allocated_mr = 0;\r\nint nonblock = msg->msg_flags & MSG_DONTWAIT;\r\nlong timeo = sock_sndtimeo(sk, nonblock);\r\nstruct rds_conn_path *cpath;\r\nsize_t total_payload_len = payload_len, rdma_payload_len = 0;\r\nif (msg->msg_flags & ~(MSG_DONTWAIT | MSG_CMSG_COMPAT)) {\r\nret = -EOPNOTSUPP;\r\ngoto out;\r\n}\r\nif (msg->msg_namelen) {\r\nif (msg->msg_namelen < sizeof(*usin) || usin->sin_family != AF_INET) {\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\ndaddr = usin->sin_addr.s_addr;\r\ndport = usin->sin_port;\r\n} else {\r\nlock_sock(sk);\r\ndaddr = rs->rs_conn_addr;\r\ndport = rs->rs_conn_port;\r\nrelease_sock(sk);\r\n}\r\nlock_sock(sk);\r\nif (daddr == 0 || rs->rs_bound_addr == 0) {\r\nrelease_sock(sk);\r\nret = -ENOTCONN;\r\ngoto out;\r\n}\r\nrelease_sock(sk);\r\nret = rds_rdma_bytes(msg, &rdma_payload_len);\r\nif (ret)\r\ngoto out;\r\ntotal_payload_len += rdma_payload_len;\r\nif (max_t(size_t, payload_len, rdma_payload_len) > RDS_MAX_MSG_SIZE) {\r\nret = -EMSGSIZE;\r\ngoto out;\r\n}\r\nif (payload_len > rds_sk_sndbuf(rs)) {\r\nret = -EMSGSIZE;\r\ngoto out;\r\n}\r\nret = rds_rm_size(msg, payload_len);\r\nif (ret < 0)\r\ngoto out;\r\nrm = rds_message_alloc(ret, GFP_KERNEL);\r\nif (!rm) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (payload_len) {\r\nrm->data.op_sg = rds_message_alloc_sgs(rm, ceil(payload_len, PAGE_SIZE));\r\nif (!rm->data.op_sg) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = rds_message_copy_from_user(rm, &msg->msg_iter);\r\nif (ret)\r\ngoto out;\r\n}\r\nrm->data.op_active = 1;\r\nrm->m_daddr = daddr;\r\nif (rs->rs_conn && rs->rs_conn->c_faddr == daddr)\r\nconn = rs->rs_conn;\r\nelse {\r\nconn = rds_conn_create_outgoing(sock_net(sock->sk),\r\nrs->rs_bound_addr, daddr,\r\nrs->rs_transport,\r\nsock->sk->sk_allocation);\r\nif (IS_ERR(conn)) {\r\nret = PTR_ERR(conn);\r\ngoto out;\r\n}\r\nrs->rs_conn = conn;\r\n}\r\nret = rds_cmsg_send(rs, rm, msg, &allocated_mr);\r\nif (ret) {\r\nif (ret == -EAGAIN)\r\nrds_conn_connect_if_down(conn);\r\ngoto out;\r\n}\r\nif (rm->rdma.op_active && !conn->c_trans->xmit_rdma) {\r\nprintk_ratelimited(KERN_NOTICE "rdma_op %p conn xmit_rdma %p\n",\r\n&rm->rdma, conn->c_trans->xmit_rdma);\r\nret = -EOPNOTSUPP;\r\ngoto out;\r\n}\r\nif (rm->atomic.op_active && !conn->c_trans->xmit_atomic) {\r\nprintk_ratelimited(KERN_NOTICE "atomic_op %p conn xmit_atomic %p\n",\r\n&rm->atomic, conn->c_trans->xmit_atomic);\r\nret = -EOPNOTSUPP;\r\ngoto out;\r\n}\r\nif (conn->c_trans->t_mp_capable)\r\ncpath = &conn->c_path[rds_send_mprds_hash(rs, conn)];\r\nelse\r\ncpath = &conn->c_path[0];\r\nrds_conn_path_connect_if_down(cpath);\r\nret = rds_cong_wait(conn->c_fcong, dport, nonblock, rs);\r\nif (ret) {\r\nrs->rs_seen_congestion = 1;\r\ngoto out;\r\n}\r\nwhile (!rds_send_queue_rm(rs, conn, cpath, rm, rs->rs_bound_port,\r\ndport, &queued)) {\r\nrds_stats_inc(s_send_queue_full);\r\nif (nonblock) {\r\nret = -EAGAIN;\r\ngoto out;\r\n}\r\ntimeo = wait_event_interruptible_timeout(*sk_sleep(sk),\r\nrds_send_queue_rm(rs, conn, cpath, rm,\r\nrs->rs_bound_port,\r\ndport,\r\n&queued),\r\ntimeo);\r\nrdsdebug("sendmsg woke queued %d timeo %ld\n", queued, timeo);\r\nif (timeo > 0 || timeo == MAX_SCHEDULE_TIMEOUT)\r\ncontinue;\r\nret = timeo;\r\nif (ret == 0)\r\nret = -ETIMEDOUT;\r\ngoto out;\r\n}\r\nrds_stats_inc(s_send_queued);\r\nret = rds_send_xmit(cpath);\r\nif (ret == -ENOMEM || ret == -EAGAIN)\r\nqueue_delayed_work(rds_wq, &cpath->cp_send_w, 1);\r\nrds_message_put(rm);\r\nreturn payload_len;\r\nout:\r\nif (allocated_mr)\r\nrds_rdma_unuse(rs, rds_rdma_cookie_key(rm->m_rdma_cookie), 1);\r\nif (rm)\r\nrds_message_put(rm);\r\nreturn ret;\r\n}\r\nstatic int\r\nrds_send_probe(struct rds_conn_path *cp, __be16 sport,\r\n__be16 dport, u8 h_flags)\r\n{\r\nstruct rds_message *rm;\r\nunsigned long flags;\r\nint ret = 0;\r\nrm = rds_message_alloc(0, GFP_ATOMIC);\r\nif (!rm) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nrm->m_daddr = cp->cp_conn->c_faddr;\r\nrm->data.op_active = 1;\r\nrds_conn_path_connect_if_down(cp);\r\nret = rds_cong_wait(cp->cp_conn->c_fcong, dport, 1, NULL);\r\nif (ret)\r\ngoto out;\r\nspin_lock_irqsave(&cp->cp_lock, flags);\r\nlist_add_tail(&rm->m_conn_item, &cp->cp_send_queue);\r\nset_bit(RDS_MSG_ON_CONN, &rm->m_flags);\r\nrds_message_addref(rm);\r\nrm->m_inc.i_conn = cp->cp_conn;\r\nrm->m_inc.i_conn_path = cp;\r\nrds_message_populate_header(&rm->m_inc.i_hdr, sport, dport,\r\ncp->cp_next_tx_seq);\r\nrm->m_inc.i_hdr.h_flags |= h_flags;\r\ncp->cp_next_tx_seq++;\r\nif (RDS_HS_PROBE(sport, dport) && cp->cp_conn->c_trans->t_mp_capable) {\r\nu16 npaths = RDS_MPATH_WORKERS;\r\nrds_message_add_extension(&rm->m_inc.i_hdr,\r\nRDS_EXTHDR_NPATHS, &npaths,\r\nsizeof(npaths));\r\nrds_message_add_extension(&rm->m_inc.i_hdr,\r\nRDS_EXTHDR_GEN_NUM,\r\n&cp->cp_conn->c_my_gen_num,\r\nsizeof(u32));\r\n}\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\nrds_stats_inc(s_send_queued);\r\nrds_stats_inc(s_send_pong);\r\nqueue_delayed_work(rds_wq, &cp->cp_send_w, 1);\r\nrds_message_put(rm);\r\nreturn 0;\r\nout:\r\nif (rm)\r\nrds_message_put(rm);\r\nreturn ret;\r\n}\r\nint\r\nrds_send_pong(struct rds_conn_path *cp, __be16 dport)\r\n{\r\nreturn rds_send_probe(cp, 0, dport, 0);\r\n}\r\nstatic void\r\nrds_send_ping(struct rds_connection *conn)\r\n{\r\nunsigned long flags;\r\nstruct rds_conn_path *cp = &conn->c_path[0];\r\nspin_lock_irqsave(&cp->cp_lock, flags);\r\nif (conn->c_ping_triggered) {\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\nreturn;\r\n}\r\nconn->c_ping_triggered = 1;\r\nspin_unlock_irqrestore(&cp->cp_lock, flags);\r\nrds_send_probe(&conn->c_path[0], RDS_FLAG_PROBE_PORT, 0, 0);\r\n}
