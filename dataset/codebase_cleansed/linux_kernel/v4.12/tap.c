static inline bool tap_legacy_is_little_endian(struct tap_queue *q)\r\n{\r\nreturn q->flags & TAP_VNET_BE ? false :\r\nvirtio_legacy_is_little_endian();\r\n}\r\nstatic long tap_get_vnet_be(struct tap_queue *q, int __user *sp)\r\n{\r\nint s = !!(q->flags & TAP_VNET_BE);\r\nif (put_user(s, sp))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic long tap_set_vnet_be(struct tap_queue *q, int __user *sp)\r\n{\r\nint s;\r\nif (get_user(s, sp))\r\nreturn -EFAULT;\r\nif (s)\r\nq->flags |= TAP_VNET_BE;\r\nelse\r\nq->flags &= ~TAP_VNET_BE;\r\nreturn 0;\r\n}\r\nstatic inline bool tap_legacy_is_little_endian(struct tap_queue *q)\r\n{\r\nreturn virtio_legacy_is_little_endian();\r\n}\r\nstatic long tap_get_vnet_be(struct tap_queue *q, int __user *argp)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic long tap_set_vnet_be(struct tap_queue *q, int __user *argp)\r\n{\r\nreturn -EINVAL;\r\n}\r\nstatic inline bool tap_is_little_endian(struct tap_queue *q)\r\n{\r\nreturn q->flags & TAP_VNET_LE ||\r\ntap_legacy_is_little_endian(q);\r\n}\r\nstatic inline u16 tap16_to_cpu(struct tap_queue *q, __virtio16 val)\r\n{\r\nreturn __virtio16_to_cpu(tap_is_little_endian(q), val);\r\n}\r\nstatic inline __virtio16 cpu_to_tap16(struct tap_queue *q, u16 val)\r\n{\r\nreturn __cpu_to_virtio16(tap_is_little_endian(q), val);\r\n}\r\nstatic struct tap_dev *tap_dev_get_rcu(const struct net_device *dev)\r\n{\r\nreturn rcu_dereference(dev->rx_handler_data);\r\n}\r\nstatic int tap_enable_queue(struct tap_dev *tap, struct file *file,\r\nstruct tap_queue *q)\r\n{\r\nint err = -EINVAL;\r\nASSERT_RTNL();\r\nif (q->enabled)\r\ngoto out;\r\nerr = 0;\r\nrcu_assign_pointer(tap->taps[tap->numvtaps], q);\r\nq->queue_index = tap->numvtaps;\r\nq->enabled = true;\r\ntap->numvtaps++;\r\nout:\r\nreturn err;\r\n}\r\nstatic int tap_set_queue(struct tap_dev *tap, struct file *file,\r\nstruct tap_queue *q)\r\n{\r\nif (tap->numqueues == MAX_TAP_QUEUES)\r\nreturn -EBUSY;\r\nrcu_assign_pointer(q->tap, tap);\r\nrcu_assign_pointer(tap->taps[tap->numvtaps], q);\r\nsock_hold(&q->sk);\r\nq->file = file;\r\nq->queue_index = tap->numvtaps;\r\nq->enabled = true;\r\nfile->private_data = q;\r\nlist_add_tail(&q->next, &tap->queue_list);\r\ntap->numvtaps++;\r\ntap->numqueues++;\r\nreturn 0;\r\n}\r\nstatic int tap_disable_queue(struct tap_queue *q)\r\n{\r\nstruct tap_dev *tap;\r\nstruct tap_queue *nq;\r\nASSERT_RTNL();\r\nif (!q->enabled)\r\nreturn -EINVAL;\r\ntap = rtnl_dereference(q->tap);\r\nif (tap) {\r\nint index = q->queue_index;\r\nBUG_ON(index >= tap->numvtaps);\r\nnq = rtnl_dereference(tap->taps[tap->numvtaps - 1]);\r\nnq->queue_index = index;\r\nrcu_assign_pointer(tap->taps[index], nq);\r\nRCU_INIT_POINTER(tap->taps[tap->numvtaps - 1], NULL);\r\nq->enabled = false;\r\ntap->numvtaps--;\r\n}\r\nreturn 0;\r\n}\r\nstatic void tap_put_queue(struct tap_queue *q)\r\n{\r\nstruct tap_dev *tap;\r\nrtnl_lock();\r\ntap = rtnl_dereference(q->tap);\r\nif (tap) {\r\nif (q->enabled)\r\nBUG_ON(tap_disable_queue(q));\r\ntap->numqueues--;\r\nRCU_INIT_POINTER(q->tap, NULL);\r\nsock_put(&q->sk);\r\nlist_del_init(&q->next);\r\n}\r\nrtnl_unlock();\r\nsynchronize_rcu();\r\nsock_put(&q->sk);\r\n}\r\nstatic struct tap_queue *tap_get_queue(struct tap_dev *tap,\r\nstruct sk_buff *skb)\r\n{\r\nstruct tap_queue *queue = NULL;\r\nint numvtaps = ACCESS_ONCE(tap->numvtaps);\r\n__u32 rxq;\r\nif (!numvtaps)\r\ngoto out;\r\nif (numvtaps == 1)\r\ngoto single;\r\nrxq = skb_get_hash(skb);\r\nif (rxq) {\r\nqueue = rcu_dereference(tap->taps[rxq % numvtaps]);\r\ngoto out;\r\n}\r\nif (likely(skb_rx_queue_recorded(skb))) {\r\nrxq = skb_get_rx_queue(skb);\r\nwhile (unlikely(rxq >= numvtaps))\r\nrxq -= numvtaps;\r\nqueue = rcu_dereference(tap->taps[rxq]);\r\ngoto out;\r\n}\r\nsingle:\r\nqueue = rcu_dereference(tap->taps[0]);\r\nout:\r\nreturn queue;\r\n}\r\nvoid tap_del_queues(struct tap_dev *tap)\r\n{\r\nstruct tap_queue *q, *tmp;\r\nASSERT_RTNL();\r\nlist_for_each_entry_safe(q, tmp, &tap->queue_list, next) {\r\nlist_del_init(&q->next);\r\nRCU_INIT_POINTER(q->tap, NULL);\r\nif (q->enabled)\r\ntap->numvtaps--;\r\ntap->numqueues--;\r\nsock_put(&q->sk);\r\n}\r\nBUG_ON(tap->numvtaps);\r\nBUG_ON(tap->numqueues);\r\ntap->numvtaps = MAX_TAP_QUEUES;\r\n}\r\nrx_handler_result_t tap_handle_frame(struct sk_buff **pskb)\r\n{\r\nstruct sk_buff *skb = *pskb;\r\nstruct net_device *dev = skb->dev;\r\nstruct tap_dev *tap;\r\nstruct tap_queue *q;\r\nnetdev_features_t features = TAP_FEATURES;\r\ntap = tap_dev_get_rcu(dev);\r\nif (!tap)\r\nreturn RX_HANDLER_PASS;\r\nq = tap_get_queue(tap, skb);\r\nif (!q)\r\nreturn RX_HANDLER_PASS;\r\nif (__skb_array_full(&q->skb_array))\r\ngoto drop;\r\nskb_push(skb, ETH_HLEN);\r\nif (q->flags & IFF_VNET_HDR)\r\nfeatures |= tap->tap_features;\r\nif (netif_needs_gso(skb, features)) {\r\nstruct sk_buff *segs = __skb_gso_segment(skb, features, false);\r\nif (IS_ERR(segs))\r\ngoto drop;\r\nif (!segs) {\r\nif (skb_array_produce(&q->skb_array, skb))\r\ngoto drop;\r\ngoto wake_up;\r\n}\r\nconsume_skb(skb);\r\nwhile (segs) {\r\nstruct sk_buff *nskb = segs->next;\r\nsegs->next = NULL;\r\nif (skb_array_produce(&q->skb_array, segs)) {\r\nkfree_skb(segs);\r\nkfree_skb_list(nskb);\r\nbreak;\r\n}\r\nsegs = nskb;\r\n}\r\n} else {\r\nif (skb->ip_summed == CHECKSUM_PARTIAL &&\r\n!(features & NETIF_F_CSUM_MASK) &&\r\nskb_checksum_help(skb))\r\ngoto drop;\r\nif (skb_array_produce(&q->skb_array, skb))\r\ngoto drop;\r\n}\r\nwake_up:\r\nwake_up_interruptible_poll(sk_sleep(&q->sk), POLLIN | POLLRDNORM | POLLRDBAND);\r\nreturn RX_HANDLER_CONSUMED;\r\ndrop:\r\nif (tap->count_rx_dropped)\r\ntap->count_rx_dropped(tap);\r\nkfree_skb(skb);\r\nreturn RX_HANDLER_CONSUMED;\r\n}\r\nstatic struct major_info *tap_get_major(int major)\r\n{\r\nstruct major_info *tap_major;\r\nlist_for_each_entry_rcu(tap_major, &major_list, next) {\r\nif (tap_major->major == major)\r\nreturn tap_major;\r\n}\r\nreturn NULL;\r\n}\r\nint tap_get_minor(dev_t major, struct tap_dev *tap)\r\n{\r\nint retval = -ENOMEM;\r\nstruct major_info *tap_major;\r\nrcu_read_lock();\r\ntap_major = tap_get_major(MAJOR(major));\r\nif (!tap_major) {\r\nretval = -EINVAL;\r\ngoto unlock;\r\n}\r\nmutex_lock(&tap_major->minor_lock);\r\nretval = idr_alloc(&tap_major->minor_idr, tap, 1, TAP_NUM_DEVS, GFP_KERNEL);\r\nif (retval >= 0) {\r\ntap->minor = retval;\r\n} else if (retval == -ENOSPC) {\r\nnetdev_err(tap->dev, "Too many tap devices\n");\r\nretval = -EINVAL;\r\n}\r\nmutex_unlock(&tap_major->minor_lock);\r\nunlock:\r\nrcu_read_unlock();\r\nreturn retval < 0 ? retval : 0;\r\n}\r\nvoid tap_free_minor(dev_t major, struct tap_dev *tap)\r\n{\r\nstruct major_info *tap_major;\r\nrcu_read_lock();\r\ntap_major = tap_get_major(MAJOR(major));\r\nif (!tap_major) {\r\ngoto unlock;\r\n}\r\nmutex_lock(&tap_major->minor_lock);\r\nif (tap->minor) {\r\nidr_remove(&tap_major->minor_idr, tap->minor);\r\ntap->minor = 0;\r\n}\r\nmutex_unlock(&tap_major->minor_lock);\r\nunlock:\r\nrcu_read_unlock();\r\n}\r\nstatic struct tap_dev *dev_get_by_tap_file(int major, int minor)\r\n{\r\nstruct net_device *dev = NULL;\r\nstruct tap_dev *tap;\r\nstruct major_info *tap_major;\r\nrcu_read_lock();\r\ntap_major = tap_get_major(major);\r\nif (!tap_major) {\r\ntap = NULL;\r\ngoto unlock;\r\n}\r\nmutex_lock(&tap_major->minor_lock);\r\ntap = idr_find(&tap_major->minor_idr, minor);\r\nif (tap) {\r\ndev = tap->dev;\r\ndev_hold(dev);\r\n}\r\nmutex_unlock(&tap_major->minor_lock);\r\nunlock:\r\nrcu_read_unlock();\r\nreturn tap;\r\n}\r\nstatic void tap_sock_write_space(struct sock *sk)\r\n{\r\nwait_queue_head_t *wqueue;\r\nif (!sock_writeable(sk) ||\r\n!test_and_clear_bit(SOCKWQ_ASYNC_NOSPACE, &sk->sk_socket->flags))\r\nreturn;\r\nwqueue = sk_sleep(sk);\r\nif (wqueue && waitqueue_active(wqueue))\r\nwake_up_interruptible_poll(wqueue, POLLOUT | POLLWRNORM | POLLWRBAND);\r\n}\r\nstatic void tap_sock_destruct(struct sock *sk)\r\n{\r\nstruct tap_queue *q = container_of(sk, struct tap_queue, sk);\r\nskb_array_cleanup(&q->skb_array);\r\n}\r\nstatic int tap_open(struct inode *inode, struct file *file)\r\n{\r\nstruct net *net = current->nsproxy->net_ns;\r\nstruct tap_dev *tap;\r\nstruct tap_queue *q;\r\nint err = -ENODEV;\r\nrtnl_lock();\r\ntap = dev_get_by_tap_file(imajor(inode), iminor(inode));\r\nif (!tap)\r\ngoto err;\r\nerr = -ENOMEM;\r\nq = (struct tap_queue *)sk_alloc(net, AF_UNSPEC, GFP_KERNEL,\r\n&tap_proto, 0);\r\nif (!q)\r\ngoto err;\r\nRCU_INIT_POINTER(q->sock.wq, &q->wq);\r\ninit_waitqueue_head(&q->wq.wait);\r\nq->sock.type = SOCK_RAW;\r\nq->sock.state = SS_CONNECTED;\r\nq->sock.file = file;\r\nq->sock.ops = &tap_socket_ops;\r\nsock_init_data(&q->sock, &q->sk);\r\nq->sk.sk_write_space = tap_sock_write_space;\r\nq->sk.sk_destruct = tap_sock_destruct;\r\nq->flags = IFF_VNET_HDR | IFF_NO_PI | IFF_TAP;\r\nq->vnet_hdr_sz = sizeof(struct virtio_net_hdr);\r\nif ((tap->dev->features & NETIF_F_HIGHDMA) && (tap->dev->features & NETIF_F_SG))\r\nsock_set_flag(&q->sk, SOCK_ZEROCOPY);\r\nerr = -ENOMEM;\r\nif (skb_array_init(&q->skb_array, tap->dev->tx_queue_len, GFP_KERNEL))\r\ngoto err_array;\r\nerr = tap_set_queue(tap, file, q);\r\nif (err)\r\ngoto err_queue;\r\ndev_put(tap->dev);\r\nrtnl_unlock();\r\nreturn err;\r\nerr_queue:\r\nskb_array_cleanup(&q->skb_array);\r\nerr_array:\r\nsock_put(&q->sk);\r\nerr:\r\nif (tap)\r\ndev_put(tap->dev);\r\nrtnl_unlock();\r\nreturn err;\r\n}\r\nstatic int tap_release(struct inode *inode, struct file *file)\r\n{\r\nstruct tap_queue *q = file->private_data;\r\ntap_put_queue(q);\r\nreturn 0;\r\n}\r\nstatic unsigned int tap_poll(struct file *file, poll_table *wait)\r\n{\r\nstruct tap_queue *q = file->private_data;\r\nunsigned int mask = POLLERR;\r\nif (!q)\r\ngoto out;\r\nmask = 0;\r\npoll_wait(file, &q->wq.wait, wait);\r\nif (!skb_array_empty(&q->skb_array))\r\nmask |= POLLIN | POLLRDNORM;\r\nif (sock_writeable(&q->sk) ||\r\n(!test_and_set_bit(SOCKWQ_ASYNC_NOSPACE, &q->sock.flags) &&\r\nsock_writeable(&q->sk)))\r\nmask |= POLLOUT | POLLWRNORM;\r\nout:\r\nreturn mask;\r\n}\r\nstatic inline struct sk_buff *tap_alloc_skb(struct sock *sk, size_t prepad,\r\nsize_t len, size_t linear,\r\nint noblock, int *err)\r\n{\r\nstruct sk_buff *skb;\r\nif (prepad + len < PAGE_SIZE || !linear)\r\nlinear = len;\r\nskb = sock_alloc_send_pskb(sk, prepad + linear, len - linear, noblock,\r\nerr, 0);\r\nif (!skb)\r\nreturn NULL;\r\nskb_reserve(skb, prepad);\r\nskb_put(skb, linear);\r\nskb->data_len = len - linear;\r\nskb->len += len - linear;\r\nreturn skb;\r\n}\r\nstatic ssize_t tap_get_user(struct tap_queue *q, struct msghdr *m,\r\nstruct iov_iter *from, int noblock)\r\n{\r\nint good_linear = SKB_MAX_HEAD(TAP_RESERVE);\r\nstruct sk_buff *skb;\r\nstruct tap_dev *tap;\r\nunsigned long total_len = iov_iter_count(from);\r\nunsigned long len = total_len;\r\nint err;\r\nstruct virtio_net_hdr vnet_hdr = { 0 };\r\nint vnet_hdr_len = 0;\r\nint copylen = 0;\r\nint depth;\r\nbool zerocopy = false;\r\nsize_t linear;\r\nif (q->flags & IFF_VNET_HDR) {\r\nvnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);\r\nerr = -EINVAL;\r\nif (len < vnet_hdr_len)\r\ngoto err;\r\nlen -= vnet_hdr_len;\r\nerr = -EFAULT;\r\nif (!copy_from_iter_full(&vnet_hdr, sizeof(vnet_hdr), from))\r\ngoto err;\r\niov_iter_advance(from, vnet_hdr_len - sizeof(vnet_hdr));\r\nif ((vnet_hdr.flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\r\ntap16_to_cpu(q, vnet_hdr.csum_start) +\r\ntap16_to_cpu(q, vnet_hdr.csum_offset) + 2 >\r\ntap16_to_cpu(q, vnet_hdr.hdr_len))\r\nvnet_hdr.hdr_len = cpu_to_tap16(q,\r\ntap16_to_cpu(q, vnet_hdr.csum_start) +\r\ntap16_to_cpu(q, vnet_hdr.csum_offset) + 2);\r\nerr = -EINVAL;\r\nif (tap16_to_cpu(q, vnet_hdr.hdr_len) > len)\r\ngoto err;\r\n}\r\nerr = -EINVAL;\r\nif (unlikely(len < ETH_HLEN))\r\ngoto err;\r\nif (m && m->msg_control && sock_flag(&q->sk, SOCK_ZEROCOPY)) {\r\nstruct iov_iter i;\r\ncopylen = vnet_hdr.hdr_len ?\r\ntap16_to_cpu(q, vnet_hdr.hdr_len) : GOODCOPY_LEN;\r\nif (copylen > good_linear)\r\ncopylen = good_linear;\r\nelse if (copylen < ETH_HLEN)\r\ncopylen = ETH_HLEN;\r\nlinear = copylen;\r\ni = *from;\r\niov_iter_advance(&i, copylen);\r\nif (iov_iter_npages(&i, INT_MAX) <= MAX_SKB_FRAGS)\r\nzerocopy = true;\r\n}\r\nif (!zerocopy) {\r\ncopylen = len;\r\nlinear = tap16_to_cpu(q, vnet_hdr.hdr_len);\r\nif (linear > good_linear)\r\nlinear = good_linear;\r\nelse if (linear < ETH_HLEN)\r\nlinear = ETH_HLEN;\r\n}\r\nskb = tap_alloc_skb(&q->sk, TAP_RESERVE, copylen,\r\nlinear, noblock, &err);\r\nif (!skb)\r\ngoto err;\r\nif (zerocopy)\r\nerr = zerocopy_sg_from_iter(skb, from);\r\nelse\r\nerr = skb_copy_datagram_from_iter(skb, 0, from, len);\r\nif (err)\r\ngoto err_kfree;\r\nskb_set_network_header(skb, ETH_HLEN);\r\nskb_reset_mac_header(skb);\r\nskb->protocol = eth_hdr(skb)->h_proto;\r\nif (vnet_hdr_len) {\r\nerr = virtio_net_hdr_to_skb(skb, &vnet_hdr,\r\ntap_is_little_endian(q));\r\nif (err)\r\ngoto err_kfree;\r\n}\r\nskb_probe_transport_header(skb, ETH_HLEN);\r\nif ((skb->protocol == htons(ETH_P_8021Q) ||\r\nskb->protocol == htons(ETH_P_8021AD)) &&\r\n__vlan_get_protocol(skb, skb->protocol, &depth) != 0)\r\nskb_set_network_header(skb, depth);\r\nrcu_read_lock();\r\ntap = rcu_dereference(q->tap);\r\nif (zerocopy) {\r\nskb_shinfo(skb)->destructor_arg = m->msg_control;\r\nskb_shinfo(skb)->tx_flags |= SKBTX_DEV_ZEROCOPY;\r\nskb_shinfo(skb)->tx_flags |= SKBTX_SHARED_FRAG;\r\n} else if (m && m->msg_control) {\r\nstruct ubuf_info *uarg = m->msg_control;\r\nuarg->callback(uarg, false);\r\n}\r\nif (tap) {\r\nskb->dev = tap->dev;\r\ndev_queue_xmit(skb);\r\n} else {\r\nkfree_skb(skb);\r\n}\r\nrcu_read_unlock();\r\nreturn total_len;\r\nerr_kfree:\r\nkfree_skb(skb);\r\nerr:\r\nrcu_read_lock();\r\ntap = rcu_dereference(q->tap);\r\nif (tap && tap->count_tx_dropped)\r\ntap->count_tx_dropped(tap);\r\nrcu_read_unlock();\r\nreturn err;\r\n}\r\nstatic ssize_t tap_write_iter(struct kiocb *iocb, struct iov_iter *from)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct tap_queue *q = file->private_data;\r\nreturn tap_get_user(q, NULL, from, file->f_flags & O_NONBLOCK);\r\n}\r\nstatic ssize_t tap_put_user(struct tap_queue *q,\r\nconst struct sk_buff *skb,\r\nstruct iov_iter *iter)\r\n{\r\nint ret;\r\nint vnet_hdr_len = 0;\r\nint vlan_offset = 0;\r\nint total;\r\nif (q->flags & IFF_VNET_HDR) {\r\nstruct virtio_net_hdr vnet_hdr;\r\nvnet_hdr_len = READ_ONCE(q->vnet_hdr_sz);\r\nif (iov_iter_count(iter) < vnet_hdr_len)\r\nreturn -EINVAL;\r\nif (virtio_net_hdr_from_skb(skb, &vnet_hdr,\r\ntap_is_little_endian(q), true))\r\nBUG();\r\nif (copy_to_iter(&vnet_hdr, sizeof(vnet_hdr), iter) !=\r\nsizeof(vnet_hdr))\r\nreturn -EFAULT;\r\niov_iter_advance(iter, vnet_hdr_len - sizeof(vnet_hdr));\r\n}\r\ntotal = vnet_hdr_len;\r\ntotal += skb->len;\r\nif (skb_vlan_tag_present(skb)) {\r\nstruct {\r\n__be16 h_vlan_proto;\r\n__be16 h_vlan_TCI;\r\n} veth;\r\nveth.h_vlan_proto = skb->vlan_proto;\r\nveth.h_vlan_TCI = htons(skb_vlan_tag_get(skb));\r\nvlan_offset = offsetof(struct vlan_ethhdr, h_vlan_proto);\r\ntotal += VLAN_HLEN;\r\nret = skb_copy_datagram_iter(skb, 0, iter, vlan_offset);\r\nif (ret || !iov_iter_count(iter))\r\ngoto done;\r\nret = copy_to_iter(&veth, sizeof(veth), iter);\r\nif (ret != sizeof(veth) || !iov_iter_count(iter))\r\ngoto done;\r\n}\r\nret = skb_copy_datagram_iter(skb, vlan_offset, iter,\r\nskb->len - vlan_offset);\r\ndone:\r\nreturn ret ? ret : total;\r\n}\r\nstatic ssize_t tap_do_read(struct tap_queue *q,\r\nstruct iov_iter *to,\r\nint noblock)\r\n{\r\nDEFINE_WAIT(wait);\r\nstruct sk_buff *skb;\r\nssize_t ret = 0;\r\nif (!iov_iter_count(to))\r\nreturn 0;\r\nwhile (1) {\r\nif (!noblock)\r\nprepare_to_wait(sk_sleep(&q->sk), &wait,\r\nTASK_INTERRUPTIBLE);\r\nskb = skb_array_consume(&q->skb_array);\r\nif (skb)\r\nbreak;\r\nif (noblock) {\r\nret = -EAGAIN;\r\nbreak;\r\n}\r\nif (signal_pending(current)) {\r\nret = -ERESTARTSYS;\r\nbreak;\r\n}\r\nschedule();\r\n}\r\nif (!noblock)\r\nfinish_wait(sk_sleep(&q->sk), &wait);\r\nif (skb) {\r\nret = tap_put_user(q, skb, to);\r\nif (unlikely(ret < 0))\r\nkfree_skb(skb);\r\nelse\r\nconsume_skb(skb);\r\n}\r\nreturn ret;\r\n}\r\nstatic ssize_t tap_read_iter(struct kiocb *iocb, struct iov_iter *to)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct tap_queue *q = file->private_data;\r\nssize_t len = iov_iter_count(to), ret;\r\nret = tap_do_read(q, to, file->f_flags & O_NONBLOCK);\r\nret = min_t(ssize_t, ret, len);\r\nif (ret > 0)\r\niocb->ki_pos = ret;\r\nreturn ret;\r\n}\r\nstatic struct tap_dev *tap_get_tap_dev(struct tap_queue *q)\r\n{\r\nstruct tap_dev *tap;\r\nASSERT_RTNL();\r\ntap = rtnl_dereference(q->tap);\r\nif (tap)\r\ndev_hold(tap->dev);\r\nreturn tap;\r\n}\r\nstatic void tap_put_tap_dev(struct tap_dev *tap)\r\n{\r\ndev_put(tap->dev);\r\n}\r\nstatic int tap_ioctl_set_queue(struct file *file, unsigned int flags)\r\n{\r\nstruct tap_queue *q = file->private_data;\r\nstruct tap_dev *tap;\r\nint ret;\r\ntap = tap_get_tap_dev(q);\r\nif (!tap)\r\nreturn -EINVAL;\r\nif (flags & IFF_ATTACH_QUEUE)\r\nret = tap_enable_queue(tap, file, q);\r\nelse if (flags & IFF_DETACH_QUEUE)\r\nret = tap_disable_queue(q);\r\nelse\r\nret = -EINVAL;\r\ntap_put_tap_dev(tap);\r\nreturn ret;\r\n}\r\nstatic int set_offload(struct tap_queue *q, unsigned long arg)\r\n{\r\nstruct tap_dev *tap;\r\nnetdev_features_t features;\r\nnetdev_features_t feature_mask = 0;\r\ntap = rtnl_dereference(q->tap);\r\nif (!tap)\r\nreturn -ENOLINK;\r\nfeatures = tap->dev->features;\r\nif (arg & TUN_F_CSUM) {\r\nfeature_mask = NETIF_F_HW_CSUM;\r\nif (arg & (TUN_F_TSO4 | TUN_F_TSO6)) {\r\nif (arg & TUN_F_TSO_ECN)\r\nfeature_mask |= NETIF_F_TSO_ECN;\r\nif (arg & TUN_F_TSO4)\r\nfeature_mask |= NETIF_F_TSO;\r\nif (arg & TUN_F_TSO6)\r\nfeature_mask |= NETIF_F_TSO6;\r\n}\r\nif (arg & TUN_F_UFO)\r\nfeature_mask |= NETIF_F_UFO;\r\n}\r\nif (feature_mask & (NETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_UFO))\r\nfeatures |= RX_OFFLOADS;\r\nelse\r\nfeatures &= ~RX_OFFLOADS;\r\ntap->tap_features = feature_mask;\r\nif (tap->update_features)\r\ntap->update_features(tap, features);\r\nreturn 0;\r\n}\r\nstatic long tap_ioctl(struct file *file, unsigned int cmd,\r\nunsigned long arg)\r\n{\r\nstruct tap_queue *q = file->private_data;\r\nstruct tap_dev *tap;\r\nvoid __user *argp = (void __user *)arg;\r\nstruct ifreq __user *ifr = argp;\r\nunsigned int __user *up = argp;\r\nunsigned short u;\r\nint __user *sp = argp;\r\nstruct sockaddr sa;\r\nint s;\r\nint ret;\r\nswitch (cmd) {\r\ncase TUNSETIFF:\r\nif (get_user(u, &ifr->ifr_flags))\r\nreturn -EFAULT;\r\nret = 0;\r\nif ((u & ~TAP_IFFEATURES) != (IFF_NO_PI | IFF_TAP))\r\nret = -EINVAL;\r\nelse\r\nq->flags = (q->flags & ~TAP_IFFEATURES) | u;\r\nreturn ret;\r\ncase TUNGETIFF:\r\nrtnl_lock();\r\ntap = tap_get_tap_dev(q);\r\nif (!tap) {\r\nrtnl_unlock();\r\nreturn -ENOLINK;\r\n}\r\nret = 0;\r\nu = q->flags;\r\nif (copy_to_user(&ifr->ifr_name, tap->dev->name, IFNAMSIZ) ||\r\nput_user(u, &ifr->ifr_flags))\r\nret = -EFAULT;\r\ntap_put_tap_dev(tap);\r\nrtnl_unlock();\r\nreturn ret;\r\ncase TUNSETQUEUE:\r\nif (get_user(u, &ifr->ifr_flags))\r\nreturn -EFAULT;\r\nrtnl_lock();\r\nret = tap_ioctl_set_queue(file, u);\r\nrtnl_unlock();\r\nreturn ret;\r\ncase TUNGETFEATURES:\r\nif (put_user(IFF_TAP | IFF_NO_PI | TAP_IFFEATURES, up))\r\nreturn -EFAULT;\r\nreturn 0;\r\ncase TUNSETSNDBUF:\r\nif (get_user(s, sp))\r\nreturn -EFAULT;\r\nq->sk.sk_sndbuf = s;\r\nreturn 0;\r\ncase TUNGETVNETHDRSZ:\r\ns = q->vnet_hdr_sz;\r\nif (put_user(s, sp))\r\nreturn -EFAULT;\r\nreturn 0;\r\ncase TUNSETVNETHDRSZ:\r\nif (get_user(s, sp))\r\nreturn -EFAULT;\r\nif (s < (int)sizeof(struct virtio_net_hdr))\r\nreturn -EINVAL;\r\nq->vnet_hdr_sz = s;\r\nreturn 0;\r\ncase TUNGETVNETLE:\r\ns = !!(q->flags & TAP_VNET_LE);\r\nif (put_user(s, sp))\r\nreturn -EFAULT;\r\nreturn 0;\r\ncase TUNSETVNETLE:\r\nif (get_user(s, sp))\r\nreturn -EFAULT;\r\nif (s)\r\nq->flags |= TAP_VNET_LE;\r\nelse\r\nq->flags &= ~TAP_VNET_LE;\r\nreturn 0;\r\ncase TUNGETVNETBE:\r\nreturn tap_get_vnet_be(q, sp);\r\ncase TUNSETVNETBE:\r\nreturn tap_set_vnet_be(q, sp);\r\ncase TUNSETOFFLOAD:\r\nif (arg & ~(TUN_F_CSUM | TUN_F_TSO4 | TUN_F_TSO6 |\r\nTUN_F_TSO_ECN | TUN_F_UFO))\r\nreturn -EINVAL;\r\nrtnl_lock();\r\nret = set_offload(q, arg);\r\nrtnl_unlock();\r\nreturn ret;\r\ncase SIOCGIFHWADDR:\r\nrtnl_lock();\r\ntap = tap_get_tap_dev(q);\r\nif (!tap) {\r\nrtnl_unlock();\r\nreturn -ENOLINK;\r\n}\r\nret = 0;\r\nu = tap->dev->type;\r\nif (copy_to_user(&ifr->ifr_name, tap->dev->name, IFNAMSIZ) ||\r\ncopy_to_user(&ifr->ifr_hwaddr.sa_data, tap->dev->dev_addr, ETH_ALEN) ||\r\nput_user(u, &ifr->ifr_hwaddr.sa_family))\r\nret = -EFAULT;\r\ntap_put_tap_dev(tap);\r\nrtnl_unlock();\r\nreturn ret;\r\ncase SIOCSIFHWADDR:\r\nif (copy_from_user(&sa, &ifr->ifr_hwaddr, sizeof(sa)))\r\nreturn -EFAULT;\r\nrtnl_lock();\r\ntap = tap_get_tap_dev(q);\r\nif (!tap) {\r\nrtnl_unlock();\r\nreturn -ENOLINK;\r\n}\r\nret = dev_set_mac_address(tap->dev, &sa);\r\ntap_put_tap_dev(tap);\r\nrtnl_unlock();\r\nreturn ret;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\n}\r\nstatic long tap_compat_ioctl(struct file *file, unsigned int cmd,\r\nunsigned long arg)\r\n{\r\nreturn tap_ioctl(file, cmd, (unsigned long)compat_ptr(arg));\r\n}\r\nstatic int tap_sendmsg(struct socket *sock, struct msghdr *m,\r\nsize_t total_len)\r\n{\r\nstruct tap_queue *q = container_of(sock, struct tap_queue, sock);\r\nreturn tap_get_user(q, m, &m->msg_iter, m->msg_flags & MSG_DONTWAIT);\r\n}\r\nstatic int tap_recvmsg(struct socket *sock, struct msghdr *m,\r\nsize_t total_len, int flags)\r\n{\r\nstruct tap_queue *q = container_of(sock, struct tap_queue, sock);\r\nint ret;\r\nif (flags & ~(MSG_DONTWAIT|MSG_TRUNC))\r\nreturn -EINVAL;\r\nret = tap_do_read(q, &m->msg_iter, flags & MSG_DONTWAIT);\r\nif (ret > total_len) {\r\nm->msg_flags |= MSG_TRUNC;\r\nret = flags & MSG_TRUNC ? ret : total_len;\r\n}\r\nreturn ret;\r\n}\r\nstatic int tap_peek_len(struct socket *sock)\r\n{\r\nstruct tap_queue *q = container_of(sock, struct tap_queue,\r\nsock);\r\nreturn skb_array_peek_len(&q->skb_array);\r\n}\r\nstruct socket *tap_get_socket(struct file *file)\r\n{\r\nstruct tap_queue *q;\r\nif (file->f_op != &tap_fops)\r\nreturn ERR_PTR(-EINVAL);\r\nq = file->private_data;\r\nif (!q)\r\nreturn ERR_PTR(-EBADFD);\r\nreturn &q->sock;\r\n}\r\nint tap_queue_resize(struct tap_dev *tap)\r\n{\r\nstruct net_device *dev = tap->dev;\r\nstruct tap_queue *q;\r\nstruct skb_array **arrays;\r\nint n = tap->numqueues;\r\nint ret, i = 0;\r\narrays = kmalloc(sizeof *arrays * n, GFP_KERNEL);\r\nif (!arrays)\r\nreturn -ENOMEM;\r\nlist_for_each_entry(q, &tap->queue_list, next)\r\narrays[i++] = &q->skb_array;\r\nret = skb_array_resize_multiple(arrays, n,\r\ndev->tx_queue_len, GFP_KERNEL);\r\nkfree(arrays);\r\nreturn ret;\r\n}\r\nstatic int tap_list_add(dev_t major, const char *device_name)\r\n{\r\nstruct major_info *tap_major;\r\ntap_major = kzalloc(sizeof(*tap_major), GFP_ATOMIC);\r\nif (!tap_major)\r\nreturn -ENOMEM;\r\ntap_major->major = MAJOR(major);\r\nidr_init(&tap_major->minor_idr);\r\nmutex_init(&tap_major->minor_lock);\r\ntap_major->device_name = device_name;\r\nlist_add_tail_rcu(&tap_major->next, &major_list);\r\nreturn 0;\r\n}\r\nint tap_create_cdev(struct cdev *tap_cdev,\r\ndev_t *tap_major, const char *device_name)\r\n{\r\nint err;\r\nerr = alloc_chrdev_region(tap_major, 0, TAP_NUM_DEVS, device_name);\r\nif (err)\r\ngoto out1;\r\ncdev_init(tap_cdev, &tap_fops);\r\nerr = cdev_add(tap_cdev, *tap_major, TAP_NUM_DEVS);\r\nif (err)\r\ngoto out2;\r\nerr = tap_list_add(*tap_major, device_name);\r\nif (err)\r\ngoto out3;\r\nreturn 0;\r\nout3:\r\ncdev_del(tap_cdev);\r\nout2:\r\nunregister_chrdev_region(*tap_major, TAP_NUM_DEVS);\r\nout1:\r\nreturn err;\r\n}\r\nvoid tap_destroy_cdev(dev_t major, struct cdev *tap_cdev)\r\n{\r\nstruct major_info *tap_major, *tmp;\r\ncdev_del(tap_cdev);\r\nunregister_chrdev_region(major, TAP_NUM_DEVS);\r\nlist_for_each_entry_safe(tap_major, tmp, &major_list, next) {\r\nif (tap_major->major == MAJOR(major)) {\r\nidr_destroy(&tap_major->minor_idr);\r\nlist_del_rcu(&tap_major->next);\r\nkfree_rcu(tap_major, rcu);\r\n}\r\n}\r\n}
