static struct zx_dma_chan *to_zx_chan(struct dma_chan *chan)\r\n{\r\nreturn container_of(chan, struct zx_dma_chan, vc.chan);\r\n}\r\nstatic void zx_dma_terminate_chan(struct zx_dma_phy *phy, struct zx_dma_dev *d)\r\n{\r\nu32 val = 0;\r\nval = readl_relaxed(phy->base + REG_ZX_CTRL);\r\nval &= ~ZX_CH_ENABLE;\r\nval |= ZX_FORCE_CLOSE;\r\nwritel_relaxed(val, phy->base + REG_ZX_CTRL);\r\nval = 0x1 << phy->idx;\r\nwritel_relaxed(val, d->base + REG_ZX_TC_IRQ_RAW);\r\nwritel_relaxed(val, d->base + REG_ZX_SRC_ERR_IRQ_RAW);\r\nwritel_relaxed(val, d->base + REG_ZX_DST_ERR_IRQ_RAW);\r\nwritel_relaxed(val, d->base + REG_ZX_CFG_ERR_IRQ_RAW);\r\n}\r\nstatic void zx_dma_set_desc(struct zx_dma_phy *phy, struct zx_desc_hw *hw)\r\n{\r\nwritel_relaxed(hw->saddr, phy->base + REG_ZX_SRC_ADDR);\r\nwritel_relaxed(hw->daddr, phy->base + REG_ZX_DST_ADDR);\r\nwritel_relaxed(hw->src_x, phy->base + REG_ZX_TX_X_COUNT);\r\nwritel_relaxed(0, phy->base + REG_ZX_TX_ZY_COUNT);\r\nwritel_relaxed(0, phy->base + REG_ZX_SRC_ZY_STEP);\r\nwritel_relaxed(0, phy->base + REG_ZX_DST_ZY_STEP);\r\nwritel_relaxed(hw->lli, phy->base + REG_ZX_LLI_ADDR);\r\nwritel_relaxed(hw->ctr, phy->base + REG_ZX_CTRL);\r\n}\r\nstatic u32 zx_dma_get_curr_lli(struct zx_dma_phy *phy)\r\n{\r\nreturn readl_relaxed(phy->base + REG_ZX_LLI_ADDR);\r\n}\r\nstatic u32 zx_dma_get_chan_stat(struct zx_dma_dev *d)\r\n{\r\nreturn readl_relaxed(d->base + REG_ZX_STATUS);\r\n}\r\nstatic void zx_dma_init_state(struct zx_dma_dev *d)\r\n{\r\nwritel_relaxed(0x0, d->base + REG_ZX_DMA_ARB);\r\nwritel_relaxed(0xffffffff, d->base + REG_ZX_TC_IRQ_RAW);\r\nwritel_relaxed(0xffffffff, d->base + REG_ZX_SRC_ERR_IRQ_RAW);\r\nwritel_relaxed(0xffffffff, d->base + REG_ZX_DST_ERR_IRQ_RAW);\r\nwritel_relaxed(0xffffffff, d->base + REG_ZX_CFG_ERR_IRQ_RAW);\r\n}\r\nstatic int zx_dma_start_txd(struct zx_dma_chan *c)\r\n{\r\nstruct zx_dma_dev *d = to_zx_dma(c->vc.chan.device);\r\nstruct virt_dma_desc *vd = vchan_next_desc(&c->vc);\r\nif (!c->phy)\r\nreturn -EAGAIN;\r\nif (BIT(c->phy->idx) & zx_dma_get_chan_stat(d))\r\nreturn -EAGAIN;\r\nif (vd) {\r\nstruct zx_dma_desc_sw *ds =\r\ncontainer_of(vd, struct zx_dma_desc_sw, vd);\r\nlist_del(&ds->vd.node);\r\nc->phy->ds_run = ds;\r\nc->phy->ds_done = NULL;\r\nzx_dma_set_desc(c->phy, ds->desc_hw);\r\nreturn 0;\r\n}\r\nc->phy->ds_done = NULL;\r\nc->phy->ds_run = NULL;\r\nreturn -EAGAIN;\r\n}\r\nstatic void zx_dma_task(struct zx_dma_dev *d)\r\n{\r\nstruct zx_dma_phy *p;\r\nstruct zx_dma_chan *c, *cn;\r\nunsigned pch, pch_alloc = 0;\r\nunsigned long flags;\r\nlist_for_each_entry_safe(c, cn, &d->slave.channels,\r\nvc.chan.device_node) {\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\np = c->phy;\r\nif (p && p->ds_done && zx_dma_start_txd(c)) {\r\ndev_dbg(d->slave.dev, "pchan %u: free\n", p->idx);\r\nc->phy = NULL;\r\np->vchan = NULL;\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\nspin_lock_irqsave(&d->lock, flags);\r\nwhile (!list_empty(&d->chan_pending)) {\r\nc = list_first_entry(&d->chan_pending,\r\nstruct zx_dma_chan, node);\r\np = &d->phy[c->id];\r\nif (!p->vchan) {\r\nlist_del_init(&c->node);\r\npch_alloc |= 1 << c->id;\r\np->vchan = c;\r\nc->phy = p;\r\n} else {\r\ndev_dbg(d->slave.dev, "pchan %u: busy!\n", c->id);\r\n}\r\n}\r\nspin_unlock_irqrestore(&d->lock, flags);\r\nfor (pch = 0; pch < d->dma_channels; pch++) {\r\nif (pch_alloc & (1 << pch)) {\r\np = &d->phy[pch];\r\nc = p->vchan;\r\nif (c) {\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nzx_dma_start_txd(c);\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\n}\r\n}\r\n}\r\n}\r\nstatic irqreturn_t zx_dma_int_handler(int irq, void *dev_id)\r\n{\r\nstruct zx_dma_dev *d = (struct zx_dma_dev *)dev_id;\r\nstruct zx_dma_phy *p;\r\nstruct zx_dma_chan *c;\r\nu32 tc = readl_relaxed(d->base + REG_ZX_TC_IRQ);\r\nu32 serr = readl_relaxed(d->base + REG_ZX_SRC_ERR_IRQ);\r\nu32 derr = readl_relaxed(d->base + REG_ZX_DST_ERR_IRQ);\r\nu32 cfg = readl_relaxed(d->base + REG_ZX_CFG_ERR_IRQ);\r\nu32 i, irq_chan = 0, task = 0;\r\nwhile (tc) {\r\ni = __ffs(tc);\r\ntc &= ~BIT(i);\r\np = &d->phy[i];\r\nc = p->vchan;\r\nif (c) {\r\nunsigned long flags;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nif (c->cyclic) {\r\nvchan_cyclic_callback(&p->ds_run->vd);\r\n} else {\r\nvchan_cookie_complete(&p->ds_run->vd);\r\np->ds_done = p->ds_run;\r\ntask = 1;\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nirq_chan |= BIT(i);\r\n}\r\n}\r\nif (serr || derr || cfg)\r\ndev_warn(d->slave.dev, "DMA ERR src 0x%x, dst 0x%x, cfg 0x%x\n",\r\nserr, derr, cfg);\r\nwritel_relaxed(irq_chan, d->base + REG_ZX_TC_IRQ_RAW);\r\nwritel_relaxed(serr, d->base + REG_ZX_SRC_ERR_IRQ_RAW);\r\nwritel_relaxed(derr, d->base + REG_ZX_DST_ERR_IRQ_RAW);\r\nwritel_relaxed(cfg, d->base + REG_ZX_CFG_ERR_IRQ_RAW);\r\nif (task)\r\nzx_dma_task(d);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void zx_dma_free_chan_resources(struct dma_chan *chan)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nstruct zx_dma_dev *d = to_zx_dma(chan->device);\r\nunsigned long flags;\r\nspin_lock_irqsave(&d->lock, flags);\r\nlist_del_init(&c->node);\r\nspin_unlock_irqrestore(&d->lock, flags);\r\nvchan_free_chan_resources(&c->vc);\r\nc->ccfg = 0;\r\n}\r\nstatic enum dma_status zx_dma_tx_status(struct dma_chan *chan,\r\ndma_cookie_t cookie,\r\nstruct dma_tx_state *state)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nstruct zx_dma_phy *p;\r\nstruct virt_dma_desc *vd;\r\nunsigned long flags;\r\nenum dma_status ret;\r\nsize_t bytes = 0;\r\nret = dma_cookie_status(&c->vc.chan, cookie, state);\r\nif (ret == DMA_COMPLETE || !state)\r\nreturn ret;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\np = c->phy;\r\nret = c->status;\r\nvd = vchan_find_desc(&c->vc, cookie);\r\nif (vd) {\r\nbytes = container_of(vd, struct zx_dma_desc_sw, vd)->size;\r\n} else if ((!p) || (!p->ds_run)) {\r\nbytes = 0;\r\n} else {\r\nstruct zx_dma_desc_sw *ds = p->ds_run;\r\nu32 clli = 0, index = 0;\r\nbytes = 0;\r\nclli = zx_dma_get_curr_lli(p);\r\nindex = (clli - ds->desc_hw_lli) /\r\nsizeof(struct zx_desc_hw) + 1;\r\nfor (; index < ds->desc_num; index++) {\r\nbytes += ds->desc_hw[index].src_x;\r\nif (!ds->desc_hw[index].lli)\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\ndma_set_residue(state, bytes);\r\nreturn ret;\r\n}\r\nstatic void zx_dma_issue_pending(struct dma_chan *chan)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nstruct zx_dma_dev *d = to_zx_dma(chan->device);\r\nunsigned long flags;\r\nint issue = 0;\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nif (vchan_issue_pending(&c->vc)) {\r\nspin_lock(&d->lock);\r\nif (!c->phy && list_empty(&c->node)) {\r\nlist_add_tail(&c->node, &d->chan_pending);\r\nissue = 1;\r\ndev_dbg(d->slave.dev, "vchan %p: issued\n", &c->vc);\r\n}\r\nspin_unlock(&d->lock);\r\n} else {\r\ndev_dbg(d->slave.dev, "vchan %p: nothing to issue\n", &c->vc);\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nif (issue)\r\nzx_dma_task(d);\r\n}\r\nstatic void zx_dma_fill_desc(struct zx_dma_desc_sw *ds, dma_addr_t dst,\r\ndma_addr_t src, size_t len, u32 num, u32 ccfg)\r\n{\r\nif ((num + 1) < ds->desc_num)\r\nds->desc_hw[num].lli = ds->desc_hw_lli + (num + 1) *\r\nsizeof(struct zx_desc_hw);\r\nds->desc_hw[num].saddr = src;\r\nds->desc_hw[num].daddr = dst;\r\nds->desc_hw[num].src_x = len;\r\nds->desc_hw[num].ctr = ccfg;\r\n}\r\nstatic struct zx_dma_desc_sw *zx_alloc_desc_resource(int num,\r\nstruct dma_chan *chan)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nstruct zx_dma_desc_sw *ds;\r\nstruct zx_dma_dev *d = to_zx_dma(chan->device);\r\nint lli_limit = LLI_BLOCK_SIZE / sizeof(struct zx_desc_hw);\r\nif (num > lli_limit) {\r\ndev_dbg(chan->device->dev, "vch %p: sg num %d exceed max %d\n",\r\n&c->vc, num, lli_limit);\r\nreturn NULL;\r\n}\r\nds = kzalloc(sizeof(*ds), GFP_ATOMIC);\r\nif (!ds)\r\nreturn NULL;\r\nds->desc_hw = dma_pool_zalloc(d->pool, GFP_NOWAIT, &ds->desc_hw_lli);\r\nif (!ds->desc_hw) {\r\ndev_dbg(chan->device->dev, "vch %p: dma alloc fail\n", &c->vc);\r\nkfree(ds);\r\nreturn NULL;\r\n}\r\nds->desc_num = num;\r\nreturn ds;\r\n}\r\nstatic enum zx_dma_burst_width zx_dma_burst_width(enum dma_slave_buswidth width)\r\n{\r\nswitch (width) {\r\ncase DMA_SLAVE_BUSWIDTH_1_BYTE:\r\ncase DMA_SLAVE_BUSWIDTH_2_BYTES:\r\ncase DMA_SLAVE_BUSWIDTH_4_BYTES:\r\ncase DMA_SLAVE_BUSWIDTH_8_BYTES:\r\nreturn ffs(width) - 1;\r\ndefault:\r\nreturn ZX_DMA_WIDTH_32BIT;\r\n}\r\n}\r\nstatic int zx_pre_config(struct zx_dma_chan *c, enum dma_transfer_direction dir)\r\n{\r\nstruct dma_slave_config *cfg = &c->slave_cfg;\r\nenum zx_dma_burst_width src_width;\r\nenum zx_dma_burst_width dst_width;\r\nu32 maxburst = 0;\r\nswitch (dir) {\r\ncase DMA_MEM_TO_MEM:\r\nc->ccfg = ZX_CH_ENABLE | ZX_SOFT_REQ\r\n| ZX_SRC_BURST_LEN(ZX_MAX_BURST_LEN - 1)\r\n| ZX_SRC_BURST_WIDTH(ZX_DMA_WIDTH_32BIT)\r\n| ZX_DST_BURST_WIDTH(ZX_DMA_WIDTH_32BIT);\r\nbreak;\r\ncase DMA_MEM_TO_DEV:\r\nc->dev_addr = cfg->dst_addr;\r\ndst_width = zx_dma_burst_width(cfg->dst_addr_width);\r\nmaxburst = cfg->dst_maxburst;\r\nmaxburst = maxburst < ZX_MAX_BURST_LEN ?\r\nmaxburst : ZX_MAX_BURST_LEN;\r\nc->ccfg = ZX_DST_FIFO_MODE | ZX_CH_ENABLE\r\n| ZX_SRC_BURST_LEN(maxburst - 1)\r\n| ZX_SRC_BURST_WIDTH(dst_width)\r\n| ZX_DST_BURST_WIDTH(dst_width);\r\nbreak;\r\ncase DMA_DEV_TO_MEM:\r\nc->dev_addr = cfg->src_addr;\r\nsrc_width = zx_dma_burst_width(cfg->src_addr_width);\r\nmaxburst = cfg->src_maxburst;\r\nmaxburst = maxburst < ZX_MAX_BURST_LEN ?\r\nmaxburst : ZX_MAX_BURST_LEN;\r\nc->ccfg = ZX_SRC_FIFO_MODE | ZX_CH_ENABLE\r\n| ZX_SRC_BURST_LEN(maxburst - 1)\r\n| ZX_SRC_BURST_WIDTH(src_width)\r\n| ZX_DST_BURST_WIDTH(src_width);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct dma_async_tx_descriptor *zx_dma_prep_memcpy(\r\nstruct dma_chan *chan, dma_addr_t dst, dma_addr_t src,\r\nsize_t len, unsigned long flags)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nstruct zx_dma_desc_sw *ds;\r\nsize_t copy = 0;\r\nint num = 0;\r\nif (!len)\r\nreturn NULL;\r\nif (zx_pre_config(c, DMA_MEM_TO_MEM))\r\nreturn NULL;\r\nnum = DIV_ROUND_UP(len, DMA_MAX_SIZE);\r\nds = zx_alloc_desc_resource(num, chan);\r\nif (!ds)\r\nreturn NULL;\r\nds->size = len;\r\nnum = 0;\r\ndo {\r\ncopy = min_t(size_t, len, DMA_MAX_SIZE);\r\nzx_dma_fill_desc(ds, dst, src, copy, num++, c->ccfg);\r\nsrc += copy;\r\ndst += copy;\r\nlen -= copy;\r\n} while (len);\r\nc->cyclic = 0;\r\nds->desc_hw[num - 1].lli = 0;\r\nds->desc_hw[num - 1].ctr |= ZX_IRQ_ENABLE_ALL;\r\nreturn vchan_tx_prep(&c->vc, &ds->vd, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *zx_dma_prep_slave_sg(\r\nstruct dma_chan *chan, struct scatterlist *sgl, unsigned int sglen,\r\nenum dma_transfer_direction dir, unsigned long flags, void *context)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nstruct zx_dma_desc_sw *ds;\r\nsize_t len, avail, total = 0;\r\nstruct scatterlist *sg;\r\ndma_addr_t addr, src = 0, dst = 0;\r\nint num = sglen, i;\r\nif (!sgl)\r\nreturn NULL;\r\nif (zx_pre_config(c, dir))\r\nreturn NULL;\r\nfor_each_sg(sgl, sg, sglen, i) {\r\navail = sg_dma_len(sg);\r\nif (avail > DMA_MAX_SIZE)\r\nnum += DIV_ROUND_UP(avail, DMA_MAX_SIZE) - 1;\r\n}\r\nds = zx_alloc_desc_resource(num, chan);\r\nif (!ds)\r\nreturn NULL;\r\nc->cyclic = 0;\r\nnum = 0;\r\nfor_each_sg(sgl, sg, sglen, i) {\r\naddr = sg_dma_address(sg);\r\navail = sg_dma_len(sg);\r\ntotal += avail;\r\ndo {\r\nlen = min_t(size_t, avail, DMA_MAX_SIZE);\r\nif (dir == DMA_MEM_TO_DEV) {\r\nsrc = addr;\r\ndst = c->dev_addr;\r\n} else if (dir == DMA_DEV_TO_MEM) {\r\nsrc = c->dev_addr;\r\ndst = addr;\r\n}\r\nzx_dma_fill_desc(ds, dst, src, len, num++, c->ccfg);\r\naddr += len;\r\navail -= len;\r\n} while (avail);\r\n}\r\nds->desc_hw[num - 1].lli = 0;\r\nds->desc_hw[num - 1].ctr |= ZX_IRQ_ENABLE_ALL;\r\nds->size = total;\r\nreturn vchan_tx_prep(&c->vc, &ds->vd, flags);\r\n}\r\nstatic struct dma_async_tx_descriptor *zx_dma_prep_dma_cyclic(\r\nstruct dma_chan *chan, dma_addr_t dma_addr, size_t buf_len,\r\nsize_t period_len, enum dma_transfer_direction dir,\r\nunsigned long flags)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nstruct zx_dma_desc_sw *ds;\r\ndma_addr_t src = 0, dst = 0;\r\nint num_periods = buf_len / period_len;\r\nint buf = 0, num = 0;\r\nif (period_len > DMA_MAX_SIZE) {\r\ndev_err(chan->device->dev, "maximum period size exceeded\n");\r\nreturn NULL;\r\n}\r\nif (zx_pre_config(c, dir))\r\nreturn NULL;\r\nds = zx_alloc_desc_resource(num_periods, chan);\r\nif (!ds)\r\nreturn NULL;\r\nc->cyclic = 1;\r\nwhile (buf < buf_len) {\r\nif (dir == DMA_MEM_TO_DEV) {\r\nsrc = dma_addr;\r\ndst = c->dev_addr;\r\n} else if (dir == DMA_DEV_TO_MEM) {\r\nsrc = c->dev_addr;\r\ndst = dma_addr;\r\n}\r\nzx_dma_fill_desc(ds, dst, src, period_len, num++,\r\nc->ccfg | ZX_IRQ_ENABLE_ALL);\r\ndma_addr += period_len;\r\nbuf += period_len;\r\n}\r\nds->desc_hw[num - 1].lli = ds->desc_hw_lli;\r\nds->size = buf_len;\r\nreturn vchan_tx_prep(&c->vc, &ds->vd, flags);\r\n}\r\nstatic int zx_dma_config(struct dma_chan *chan,\r\nstruct dma_slave_config *cfg)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nif (!cfg)\r\nreturn -EINVAL;\r\nmemcpy(&c->slave_cfg, cfg, sizeof(*cfg));\r\nreturn 0;\r\n}\r\nstatic int zx_dma_terminate_all(struct dma_chan *chan)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nstruct zx_dma_dev *d = to_zx_dma(chan->device);\r\nstruct zx_dma_phy *p = c->phy;\r\nunsigned long flags;\r\nLIST_HEAD(head);\r\ndev_dbg(d->slave.dev, "vchan %p: terminate all\n", &c->vc);\r\nspin_lock(&d->lock);\r\nlist_del_init(&c->node);\r\nspin_unlock(&d->lock);\r\nspin_lock_irqsave(&c->vc.lock, flags);\r\nvchan_get_all_descriptors(&c->vc, &head);\r\nif (p) {\r\nzx_dma_terminate_chan(p, d);\r\nc->phy = NULL;\r\np->vchan = NULL;\r\np->ds_run = NULL;\r\np->ds_done = NULL;\r\n}\r\nspin_unlock_irqrestore(&c->vc.lock, flags);\r\nvchan_dma_desc_free_list(&c->vc, &head);\r\nreturn 0;\r\n}\r\nstatic int zx_dma_transfer_pause(struct dma_chan *chan)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nu32 val = 0;\r\nval = readl_relaxed(c->phy->base + REG_ZX_CTRL);\r\nval &= ~ZX_CH_ENABLE;\r\nwritel_relaxed(val, c->phy->base + REG_ZX_CTRL);\r\nreturn 0;\r\n}\r\nstatic int zx_dma_transfer_resume(struct dma_chan *chan)\r\n{\r\nstruct zx_dma_chan *c = to_zx_chan(chan);\r\nu32 val = 0;\r\nval = readl_relaxed(c->phy->base + REG_ZX_CTRL);\r\nval |= ZX_CH_ENABLE;\r\nwritel_relaxed(val, c->phy->base + REG_ZX_CTRL);\r\nreturn 0;\r\n}\r\nstatic void zx_dma_free_desc(struct virt_dma_desc *vd)\r\n{\r\nstruct zx_dma_desc_sw *ds =\r\ncontainer_of(vd, struct zx_dma_desc_sw, vd);\r\nstruct zx_dma_dev *d = to_zx_dma(vd->tx.chan->device);\r\ndma_pool_free(d->pool, ds->desc_hw, ds->desc_hw_lli);\r\nkfree(ds);\r\n}\r\nstatic struct dma_chan *zx_of_dma_simple_xlate(struct of_phandle_args *dma_spec,\r\nstruct of_dma *ofdma)\r\n{\r\nstruct zx_dma_dev *d = ofdma->of_dma_data;\r\nunsigned int request = dma_spec->args[0];\r\nstruct dma_chan *chan;\r\nstruct zx_dma_chan *c;\r\nif (request >= d->dma_requests)\r\nreturn NULL;\r\nchan = dma_get_any_slave_channel(&d->slave);\r\nif (!chan) {\r\ndev_err(d->slave.dev, "get channel fail in %s.\n", __func__);\r\nreturn NULL;\r\n}\r\nc = to_zx_chan(chan);\r\nc->id = request;\r\ndev_info(d->slave.dev, "zx_dma: pchan %u: alloc vchan %p\n",\r\nc->id, &c->vc);\r\nreturn chan;\r\n}\r\nstatic int zx_dma_probe(struct platform_device *op)\r\n{\r\nstruct zx_dma_dev *d;\r\nstruct resource *iores;\r\nint i, ret = 0;\r\niores = platform_get_resource(op, IORESOURCE_MEM, 0);\r\nif (!iores)\r\nreturn -EINVAL;\r\nd = devm_kzalloc(&op->dev, sizeof(*d), GFP_KERNEL);\r\nif (!d)\r\nreturn -ENOMEM;\r\nd->base = devm_ioremap_resource(&op->dev, iores);\r\nif (IS_ERR(d->base))\r\nreturn PTR_ERR(d->base);\r\nof_property_read_u32((&op->dev)->of_node,\r\n"dma-channels", &d->dma_channels);\r\nof_property_read_u32((&op->dev)->of_node,\r\n"dma-requests", &d->dma_requests);\r\nif (!d->dma_requests || !d->dma_channels)\r\nreturn -EINVAL;\r\nd->clk = devm_clk_get(&op->dev, NULL);\r\nif (IS_ERR(d->clk)) {\r\ndev_err(&op->dev, "no dma clk\n");\r\nreturn PTR_ERR(d->clk);\r\n}\r\nd->irq = platform_get_irq(op, 0);\r\nret = devm_request_irq(&op->dev, d->irq, zx_dma_int_handler,\r\n0, DRIVER_NAME, d);\r\nif (ret)\r\nreturn ret;\r\nd->pool = dmam_pool_create(DRIVER_NAME, &op->dev,\r\nLLI_BLOCK_SIZE, 32, 0);\r\nif (!d->pool)\r\nreturn -ENOMEM;\r\nd->phy = devm_kzalloc(&op->dev,\r\nd->dma_channels * sizeof(struct zx_dma_phy), GFP_KERNEL);\r\nif (!d->phy)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < d->dma_channels; i++) {\r\nstruct zx_dma_phy *p = &d->phy[i];\r\np->idx = i;\r\np->base = d->base + i * 0x40;\r\n}\r\nINIT_LIST_HEAD(&d->slave.channels);\r\ndma_cap_set(DMA_SLAVE, d->slave.cap_mask);\r\ndma_cap_set(DMA_MEMCPY, d->slave.cap_mask);\r\ndma_cap_set(DMA_CYCLIC, d->slave.cap_mask);\r\ndma_cap_set(DMA_PRIVATE, d->slave.cap_mask);\r\nd->slave.dev = &op->dev;\r\nd->slave.device_free_chan_resources = zx_dma_free_chan_resources;\r\nd->slave.device_tx_status = zx_dma_tx_status;\r\nd->slave.device_prep_dma_memcpy = zx_dma_prep_memcpy;\r\nd->slave.device_prep_slave_sg = zx_dma_prep_slave_sg;\r\nd->slave.device_prep_dma_cyclic = zx_dma_prep_dma_cyclic;\r\nd->slave.device_issue_pending = zx_dma_issue_pending;\r\nd->slave.device_config = zx_dma_config;\r\nd->slave.device_terminate_all = zx_dma_terminate_all;\r\nd->slave.device_pause = zx_dma_transfer_pause;\r\nd->slave.device_resume = zx_dma_transfer_resume;\r\nd->slave.copy_align = DMA_ALIGN;\r\nd->slave.src_addr_widths = ZX_DMA_BUSWIDTHS;\r\nd->slave.dst_addr_widths = ZX_DMA_BUSWIDTHS;\r\nd->slave.directions = BIT(DMA_MEM_TO_MEM) | BIT(DMA_MEM_TO_DEV)\r\n| BIT(DMA_DEV_TO_MEM);\r\nd->slave.residue_granularity = DMA_RESIDUE_GRANULARITY_SEGMENT;\r\nd->chans = devm_kzalloc(&op->dev,\r\nd->dma_requests * sizeof(struct zx_dma_chan), GFP_KERNEL);\r\nif (!d->chans)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < d->dma_requests; i++) {\r\nstruct zx_dma_chan *c = &d->chans[i];\r\nc->status = DMA_IN_PROGRESS;\r\nINIT_LIST_HEAD(&c->node);\r\nc->vc.desc_free = zx_dma_free_desc;\r\nvchan_init(&c->vc, &d->slave);\r\n}\r\nret = clk_prepare_enable(d->clk);\r\nif (ret < 0) {\r\ndev_err(&op->dev, "clk_prepare_enable failed: %d\n", ret);\r\ngoto zx_dma_out;\r\n}\r\nzx_dma_init_state(d);\r\nspin_lock_init(&d->lock);\r\nINIT_LIST_HEAD(&d->chan_pending);\r\nplatform_set_drvdata(op, d);\r\nret = dma_async_device_register(&d->slave);\r\nif (ret)\r\ngoto clk_dis;\r\nret = of_dma_controller_register((&op->dev)->of_node,\r\nzx_of_dma_simple_xlate, d);\r\nif (ret)\r\ngoto of_dma_register_fail;\r\ndev_info(&op->dev, "initialized\n");\r\nreturn 0;\r\nof_dma_register_fail:\r\ndma_async_device_unregister(&d->slave);\r\nclk_dis:\r\nclk_disable_unprepare(d->clk);\r\nzx_dma_out:\r\nreturn ret;\r\n}\r\nstatic int zx_dma_remove(struct platform_device *op)\r\n{\r\nstruct zx_dma_chan *c, *cn;\r\nstruct zx_dma_dev *d = platform_get_drvdata(op);\r\ndevm_free_irq(&op->dev, d->irq, d);\r\ndma_async_device_unregister(&d->slave);\r\nof_dma_controller_free((&op->dev)->of_node);\r\nlist_for_each_entry_safe(c, cn, &d->slave.channels,\r\nvc.chan.device_node) {\r\nlist_del(&c->vc.chan.device_node);\r\n}\r\nclk_disable_unprepare(d->clk);\r\ndmam_pool_destroy(d->pool);\r\nreturn 0;\r\n}\r\nstatic int zx_dma_suspend_dev(struct device *dev)\r\n{\r\nstruct zx_dma_dev *d = dev_get_drvdata(dev);\r\nu32 stat = 0;\r\nstat = zx_dma_get_chan_stat(d);\r\nif (stat) {\r\ndev_warn(d->slave.dev,\r\n"chan %d is running fail to suspend\n", stat);\r\nreturn -1;\r\n}\r\nclk_disable_unprepare(d->clk);\r\nreturn 0;\r\n}\r\nstatic int zx_dma_resume_dev(struct device *dev)\r\n{\r\nstruct zx_dma_dev *d = dev_get_drvdata(dev);\r\nint ret = 0;\r\nret = clk_prepare_enable(d->clk);\r\nif (ret < 0) {\r\ndev_err(d->slave.dev, "clk_prepare_enable failed: %d\n", ret);\r\nreturn ret;\r\n}\r\nzx_dma_init_state(d);\r\nreturn 0;\r\n}
