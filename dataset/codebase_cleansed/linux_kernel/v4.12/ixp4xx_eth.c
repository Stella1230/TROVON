static inline void memcpy_swab32(u32 *dest, u32 *src, int cnt)\r\n{\r\nint i;\r\nfor (i = 0; i < cnt; i++)\r\ndest[i] = swab32(src[i]);\r\n}\r\nstatic int ixp_ptp_match(struct sk_buff *skb, u16 uid_hi, u32 uid_lo, u16 seqid)\r\n{\r\nu8 *data = skb->data;\r\nunsigned int offset;\r\nu16 *hi, *id;\r\nu32 lo;\r\nif (ptp_classify_raw(skb) != PTP_CLASS_V1_IPV4)\r\nreturn 0;\r\noffset = ETH_HLEN + IPV4_HLEN(data) + UDP_HLEN;\r\nif (skb->len < offset + OFF_PTP_SEQUENCE_ID + sizeof(seqid))\r\nreturn 0;\r\nhi = (u16 *)(data + offset + OFF_PTP_SOURCE_UUID);\r\nid = (u16 *)(data + offset + OFF_PTP_SEQUENCE_ID);\r\nmemcpy(&lo, &hi[1], sizeof(lo));\r\nreturn (uid_hi == ntohs(*hi) &&\r\nuid_lo == ntohl(lo) &&\r\nseqid == ntohs(*id));\r\n}\r\nstatic void ixp_rx_timestamp(struct port *port, struct sk_buff *skb)\r\n{\r\nstruct skb_shared_hwtstamps *shhwtstamps;\r\nstruct ixp46x_ts_regs *regs;\r\nu64 ns;\r\nu32 ch, hi, lo, val;\r\nu16 uid, seq;\r\nif (!port->hwts_rx_en)\r\nreturn;\r\nch = PORT2CHANNEL(port);\r\nregs = (struct ixp46x_ts_regs __iomem *) IXP4XX_TIMESYNC_BASE_VIRT;\r\nval = __raw_readl(&regs->channel[ch].ch_event);\r\nif (!(val & RX_SNAPSHOT_LOCKED))\r\nreturn;\r\nlo = __raw_readl(&regs->channel[ch].src_uuid_lo);\r\nhi = __raw_readl(&regs->channel[ch].src_uuid_hi);\r\nuid = hi & 0xffff;\r\nseq = (hi >> 16) & 0xffff;\r\nif (!ixp_ptp_match(skb, htons(uid), htonl(lo), htons(seq)))\r\ngoto out;\r\nlo = __raw_readl(&regs->channel[ch].rx_snap_lo);\r\nhi = __raw_readl(&regs->channel[ch].rx_snap_hi);\r\nns = ((u64) hi) << 32;\r\nns |= lo;\r\nns <<= TICKS_NS_SHIFT;\r\nshhwtstamps = skb_hwtstamps(skb);\r\nmemset(shhwtstamps, 0, sizeof(*shhwtstamps));\r\nshhwtstamps->hwtstamp = ns_to_ktime(ns);\r\nout:\r\n__raw_writel(RX_SNAPSHOT_LOCKED, &regs->channel[ch].ch_event);\r\n}\r\nstatic void ixp_tx_timestamp(struct port *port, struct sk_buff *skb)\r\n{\r\nstruct skb_shared_hwtstamps shhwtstamps;\r\nstruct ixp46x_ts_regs *regs;\r\nstruct skb_shared_info *shtx;\r\nu64 ns;\r\nu32 ch, cnt, hi, lo, val;\r\nshtx = skb_shinfo(skb);\r\nif (unlikely(shtx->tx_flags & SKBTX_HW_TSTAMP && port->hwts_tx_en))\r\nshtx->tx_flags |= SKBTX_IN_PROGRESS;\r\nelse\r\nreturn;\r\nch = PORT2CHANNEL(port);\r\nregs = (struct ixp46x_ts_regs __iomem *) IXP4XX_TIMESYNC_BASE_VIRT;\r\nfor (cnt = 0; cnt < 100; cnt++) {\r\nval = __raw_readl(&regs->channel[ch].ch_event);\r\nif (val & TX_SNAPSHOT_LOCKED)\r\nbreak;\r\nudelay(1);\r\n}\r\nif (!(val & TX_SNAPSHOT_LOCKED)) {\r\nshtx->tx_flags &= ~SKBTX_IN_PROGRESS;\r\nreturn;\r\n}\r\nlo = __raw_readl(&regs->channel[ch].tx_snap_lo);\r\nhi = __raw_readl(&regs->channel[ch].tx_snap_hi);\r\nns = ((u64) hi) << 32;\r\nns |= lo;\r\nns <<= TICKS_NS_SHIFT;\r\nmemset(&shhwtstamps, 0, sizeof(shhwtstamps));\r\nshhwtstamps.hwtstamp = ns_to_ktime(ns);\r\nskb_tstamp_tx(skb, &shhwtstamps);\r\n__raw_writel(TX_SNAPSHOT_LOCKED, &regs->channel[ch].ch_event);\r\n}\r\nstatic int hwtstamp_set(struct net_device *netdev, struct ifreq *ifr)\r\n{\r\nstruct hwtstamp_config cfg;\r\nstruct ixp46x_ts_regs *regs;\r\nstruct port *port = netdev_priv(netdev);\r\nint ch;\r\nif (copy_from_user(&cfg, ifr->ifr_data, sizeof(cfg)))\r\nreturn -EFAULT;\r\nif (cfg.flags)\r\nreturn -EINVAL;\r\nch = PORT2CHANNEL(port);\r\nregs = (struct ixp46x_ts_regs __iomem *) IXP4XX_TIMESYNC_BASE_VIRT;\r\nif (cfg.tx_type != HWTSTAMP_TX_OFF && cfg.tx_type != HWTSTAMP_TX_ON)\r\nreturn -ERANGE;\r\nswitch (cfg.rx_filter) {\r\ncase HWTSTAMP_FILTER_NONE:\r\nport->hwts_rx_en = 0;\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_SYNC:\r\nport->hwts_rx_en = PTP_SLAVE_MODE;\r\n__raw_writel(0, &regs->channel[ch].ch_control);\r\nbreak;\r\ncase HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ:\r\nport->hwts_rx_en = PTP_MASTER_MODE;\r\n__raw_writel(MASTER_MODE, &regs->channel[ch].ch_control);\r\nbreak;\r\ndefault:\r\nreturn -ERANGE;\r\n}\r\nport->hwts_tx_en = cfg.tx_type == HWTSTAMP_TX_ON;\r\n__raw_writel(TX_SNAPSHOT_LOCKED | RX_SNAPSHOT_LOCKED,\r\n&regs->channel[ch].ch_event);\r\nreturn copy_to_user(ifr->ifr_data, &cfg, sizeof(cfg)) ? -EFAULT : 0;\r\n}\r\nstatic int hwtstamp_get(struct net_device *netdev, struct ifreq *ifr)\r\n{\r\nstruct hwtstamp_config cfg;\r\nstruct port *port = netdev_priv(netdev);\r\ncfg.flags = 0;\r\ncfg.tx_type = port->hwts_tx_en ? HWTSTAMP_TX_ON : HWTSTAMP_TX_OFF;\r\nswitch (port->hwts_rx_en) {\r\ncase 0:\r\ncfg.rx_filter = HWTSTAMP_FILTER_NONE;\r\nbreak;\r\ncase PTP_SLAVE_MODE:\r\ncfg.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_SYNC;\r\nbreak;\r\ncase PTP_MASTER_MODE:\r\ncfg.rx_filter = HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ;\r\nbreak;\r\ndefault:\r\nWARN_ON_ONCE(1);\r\nreturn -ERANGE;\r\n}\r\nreturn copy_to_user(ifr->ifr_data, &cfg, sizeof(cfg)) ? -EFAULT : 0;\r\n}\r\nstatic int ixp4xx_mdio_cmd(struct mii_bus *bus, int phy_id, int location,\r\nint write, u16 cmd)\r\n{\r\nint cycles = 0;\r\nif (__raw_readl(&mdio_regs->mdio_command[3]) & 0x80) {\r\nprintk(KERN_ERR "%s: MII not ready to transmit\n", bus->name);\r\nreturn -1;\r\n}\r\nif (write) {\r\n__raw_writel(cmd & 0xFF, &mdio_regs->mdio_command[0]);\r\n__raw_writel(cmd >> 8, &mdio_regs->mdio_command[1]);\r\n}\r\n__raw_writel(((phy_id << 5) | location) & 0xFF,\r\n&mdio_regs->mdio_command[2]);\r\n__raw_writel((phy_id >> 3) | (write << 2) | 0x80 ,\r\n&mdio_regs->mdio_command[3]);\r\nwhile ((cycles < MAX_MDIO_RETRIES) &&\r\n(__raw_readl(&mdio_regs->mdio_command[3]) & 0x80)) {\r\nudelay(1);\r\ncycles++;\r\n}\r\nif (cycles == MAX_MDIO_RETRIES) {\r\nprintk(KERN_ERR "%s #%i: MII write failed\n", bus->name,\r\nphy_id);\r\nreturn -1;\r\n}\r\n#if DEBUG_MDIO\r\nprintk(KERN_DEBUG "%s #%i: mdio_%s() took %i cycles\n", bus->name,\r\nphy_id, write ? "write" : "read", cycles);\r\n#endif\r\nif (write)\r\nreturn 0;\r\nif (__raw_readl(&mdio_regs->mdio_status[3]) & 0x80) {\r\n#if DEBUG_MDIO\r\nprintk(KERN_DEBUG "%s #%i: MII read failed\n", bus->name,\r\nphy_id);\r\n#endif\r\nreturn 0xFFFF;\r\n}\r\nreturn (__raw_readl(&mdio_regs->mdio_status[0]) & 0xFF) |\r\n((__raw_readl(&mdio_regs->mdio_status[1]) & 0xFF) << 8);\r\n}\r\nstatic int ixp4xx_mdio_read(struct mii_bus *bus, int phy_id, int location)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&mdio_lock, flags);\r\nret = ixp4xx_mdio_cmd(bus, phy_id, location, 0, 0);\r\nspin_unlock_irqrestore(&mdio_lock, flags);\r\n#if DEBUG_MDIO\r\nprintk(KERN_DEBUG "%s #%i: MII read [%i] -> 0x%X\n", bus->name,\r\nphy_id, location, ret);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int ixp4xx_mdio_write(struct mii_bus *bus, int phy_id, int location,\r\nu16 val)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nspin_lock_irqsave(&mdio_lock, flags);\r\nret = ixp4xx_mdio_cmd(bus, phy_id, location, 1, val);\r\nspin_unlock_irqrestore(&mdio_lock, flags);\r\n#if DEBUG_MDIO\r\nprintk(KERN_DEBUG "%s #%i: MII write [%i] <- 0x%X, err = %i\n",\r\nbus->name, phy_id, location, val, ret);\r\n#endif\r\nreturn ret;\r\n}\r\nstatic int ixp4xx_mdio_register(void)\r\n{\r\nint err;\r\nif (!(mdio_bus = mdiobus_alloc()))\r\nreturn -ENOMEM;\r\nif (cpu_is_ixp43x()) {\r\nif (!(ixp4xx_read_feature_bits() & IXP4XX_FEATURE_NPEC_ETH))\r\nreturn -ENODEV;\r\nmdio_regs = (struct eth_regs __iomem *)IXP4XX_EthC_BASE_VIRT;\r\n} else {\r\nif (!(ixp4xx_read_feature_bits() & IXP4XX_FEATURE_NPEB_ETH0))\r\nreturn -ENODEV;\r\nmdio_regs = (struct eth_regs __iomem *)IXP4XX_EthB_BASE_VIRT;\r\n}\r\n__raw_writel(DEFAULT_CORE_CNTRL, &mdio_regs->core_control);\r\nspin_lock_init(&mdio_lock);\r\nmdio_bus->name = "IXP4xx MII Bus";\r\nmdio_bus->read = &ixp4xx_mdio_read;\r\nmdio_bus->write = &ixp4xx_mdio_write;\r\nsnprintf(mdio_bus->id, MII_BUS_ID_SIZE, "ixp4xx-eth-0");\r\nif ((err = mdiobus_register(mdio_bus)))\r\nmdiobus_free(mdio_bus);\r\nreturn err;\r\n}\r\nstatic void ixp4xx_mdio_remove(void)\r\n{\r\nmdiobus_unregister(mdio_bus);\r\nmdiobus_free(mdio_bus);\r\n}\r\nstatic void ixp4xx_adjust_link(struct net_device *dev)\r\n{\r\nstruct port *port = netdev_priv(dev);\r\nstruct phy_device *phydev = dev->phydev;\r\nif (!phydev->link) {\r\nif (port->speed) {\r\nport->speed = 0;\r\nprintk(KERN_INFO "%s: link down\n", dev->name);\r\n}\r\nreturn;\r\n}\r\nif (port->speed == phydev->speed && port->duplex == phydev->duplex)\r\nreturn;\r\nport->speed = phydev->speed;\r\nport->duplex = phydev->duplex;\r\nif (port->duplex)\r\n__raw_writel(DEFAULT_TX_CNTRL0 & ~TX_CNTRL0_HALFDUPLEX,\r\n&port->regs->tx_control[0]);\r\nelse\r\n__raw_writel(DEFAULT_TX_CNTRL0 | TX_CNTRL0_HALFDUPLEX,\r\n&port->regs->tx_control[0]);\r\nprintk(KERN_INFO "%s: link up, speed %u Mb/s, %s duplex\n",\r\ndev->name, port->speed, port->duplex ? "full" : "half");\r\n}\r\nstatic inline void debug_pkt(struct net_device *dev, const char *func,\r\nu8 *data, int len)\r\n{\r\n#if DEBUG_PKT_BYTES\r\nint i;\r\nprintk(KERN_DEBUG "%s: %s(%i) ", dev->name, func, len);\r\nfor (i = 0; i < len; i++) {\r\nif (i >= DEBUG_PKT_BYTES)\r\nbreak;\r\nprintk("%s%02X",\r\n((i == 6) || (i == 12) || (i >= 14)) ? " " : "",\r\ndata[i]);\r\n}\r\nprintk("\n");\r\n#endif\r\n}\r\nstatic inline void debug_desc(u32 phys, struct desc *desc)\r\n{\r\n#if DEBUG_DESC\r\nprintk(KERN_DEBUG "%X: %X %3X %3X %08X %2X < %2X %4X %X"\r\n" %X %X %02X%02X%02X%02X%02X%02X < %02X%02X%02X%02X%02X%02X\n",\r\nphys, desc->next, desc->buf_len, desc->pkt_len,\r\ndesc->data, desc->dest_id, desc->src_id, desc->flags,\r\ndesc->qos, desc->padlen, desc->vlan_tci,\r\ndesc->dst_mac_0, desc->dst_mac_1, desc->dst_mac_2,\r\ndesc->dst_mac_3, desc->dst_mac_4, desc->dst_mac_5,\r\ndesc->src_mac_0, desc->src_mac_1, desc->src_mac_2,\r\ndesc->src_mac_3, desc->src_mac_4, desc->src_mac_5);\r\n#endif\r\n}\r\nstatic inline int queue_get_desc(unsigned int queue, struct port *port,\r\nint is_tx)\r\n{\r\nu32 phys, tab_phys, n_desc;\r\nstruct desc *tab;\r\nif (!(phys = qmgr_get_entry(queue)))\r\nreturn -1;\r\nphys &= ~0x1F;\r\ntab_phys = is_tx ? tx_desc_phys(port, 0) : rx_desc_phys(port, 0);\r\ntab = is_tx ? tx_desc_ptr(port, 0) : rx_desc_ptr(port, 0);\r\nn_desc = (phys - tab_phys) / sizeof(struct desc);\r\nBUG_ON(n_desc >= (is_tx ? TX_DESCS : RX_DESCS));\r\ndebug_desc(phys, &tab[n_desc]);\r\nBUG_ON(tab[n_desc].next);\r\nreturn n_desc;\r\n}\r\nstatic inline void queue_put_desc(unsigned int queue, u32 phys,\r\nstruct desc *desc)\r\n{\r\ndebug_desc(phys, desc);\r\nBUG_ON(phys & 0x1F);\r\nqmgr_put_entry(queue, phys);\r\n}\r\nstatic inline void dma_unmap_tx(struct port *port, struct desc *desc)\r\n{\r\n#ifdef __ARMEB__\r\ndma_unmap_single(&port->netdev->dev, desc->data,\r\ndesc->buf_len, DMA_TO_DEVICE);\r\n#else\r\ndma_unmap_single(&port->netdev->dev, desc->data & ~3,\r\nALIGN((desc->data & 3) + desc->buf_len, 4),\r\nDMA_TO_DEVICE);\r\n#endif\r\n}\r\nstatic void eth_rx_irq(void *pdev)\r\n{\r\nstruct net_device *dev = pdev;\r\nstruct port *port = netdev_priv(dev);\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: eth_rx_irq\n", dev->name);\r\n#endif\r\nqmgr_disable_irq(port->plat->rxq);\r\nnapi_schedule(&port->napi);\r\n}\r\nstatic int eth_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct port *port = container_of(napi, struct port, napi);\r\nstruct net_device *dev = port->netdev;\r\nunsigned int rxq = port->plat->rxq, rxfreeq = RXFREE_QUEUE(port->id);\r\nint received = 0;\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: eth_poll\n", dev->name);\r\n#endif\r\nwhile (received < budget) {\r\nstruct sk_buff *skb;\r\nstruct desc *desc;\r\nint n;\r\n#ifdef __ARMEB__\r\nstruct sk_buff *temp;\r\nu32 phys;\r\n#endif\r\nif ((n = queue_get_desc(rxq, port, 0)) < 0) {\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: eth_poll napi_complete\n",\r\ndev->name);\r\n#endif\r\nnapi_complete(napi);\r\nqmgr_enable_irq(rxq);\r\nif (!qmgr_stat_below_low_watermark(rxq) &&\r\nnapi_reschedule(napi)) {\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: eth_poll napi_reschedule succeeded\n",\r\ndev->name);\r\n#endif\r\nqmgr_disable_irq(rxq);\r\ncontinue;\r\n}\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "%s: eth_poll all done\n",\r\ndev->name);\r\n#endif\r\nreturn received;\r\n}\r\ndesc = rx_desc_ptr(port, n);\r\n#ifdef __ARMEB__\r\nif ((skb = netdev_alloc_skb(dev, RX_BUFF_SIZE))) {\r\nphys = dma_map_single(&dev->dev, skb->data,\r\nRX_BUFF_SIZE, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(&dev->dev, phys)) {\r\ndev_kfree_skb(skb);\r\nskb = NULL;\r\n}\r\n}\r\n#else\r\nskb = netdev_alloc_skb(dev,\r\nALIGN(NET_IP_ALIGN + desc->pkt_len, 4));\r\n#endif\r\nif (!skb) {\r\ndev->stats.rx_dropped++;\r\ndesc->buf_len = MAX_MRU;\r\ndesc->pkt_len = 0;\r\nqueue_put_desc(rxfreeq, rx_desc_phys(port, n), desc);\r\ncontinue;\r\n}\r\n#ifdef __ARMEB__\r\ntemp = skb;\r\nskb = port->rx_buff_tab[n];\r\ndma_unmap_single(&dev->dev, desc->data - NET_IP_ALIGN,\r\nRX_BUFF_SIZE, DMA_FROM_DEVICE);\r\n#else\r\ndma_sync_single_for_cpu(&dev->dev, desc->data - NET_IP_ALIGN,\r\nRX_BUFF_SIZE, DMA_FROM_DEVICE);\r\nmemcpy_swab32((u32 *)skb->data, (u32 *)port->rx_buff_tab[n],\r\nALIGN(NET_IP_ALIGN + desc->pkt_len, 4) / 4);\r\n#endif\r\nskb_reserve(skb, NET_IP_ALIGN);\r\nskb_put(skb, desc->pkt_len);\r\ndebug_pkt(dev, "eth_poll", skb->data, skb->len);\r\nixp_rx_timestamp(port, skb);\r\nskb->protocol = eth_type_trans(skb, dev);\r\ndev->stats.rx_packets++;\r\ndev->stats.rx_bytes += skb->len;\r\nnetif_receive_skb(skb);\r\n#ifdef __ARMEB__\r\nport->rx_buff_tab[n] = temp;\r\ndesc->data = phys + NET_IP_ALIGN;\r\n#endif\r\ndesc->buf_len = MAX_MRU;\r\ndesc->pkt_len = 0;\r\nqueue_put_desc(rxfreeq, rx_desc_phys(port, n), desc);\r\nreceived++;\r\n}\r\n#if DEBUG_RX\r\nprintk(KERN_DEBUG "eth_poll(): end, not all work done\n");\r\n#endif\r\nreturn received;\r\n}\r\nstatic void eth_txdone_irq(void *unused)\r\n{\r\nu32 phys;\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG DRV_NAME ": eth_txdone_irq\n");\r\n#endif\r\nwhile ((phys = qmgr_get_entry(TXDONE_QUEUE)) != 0) {\r\nu32 npe_id, n_desc;\r\nstruct port *port;\r\nstruct desc *desc;\r\nint start;\r\nnpe_id = phys & 3;\r\nBUG_ON(npe_id >= MAX_NPES);\r\nport = npe_port_tab[npe_id];\r\nBUG_ON(!port);\r\nphys &= ~0x1F;\r\nn_desc = (phys - tx_desc_phys(port, 0)) / sizeof(struct desc);\r\nBUG_ON(n_desc >= TX_DESCS);\r\ndesc = tx_desc_ptr(port, n_desc);\r\ndebug_desc(phys, desc);\r\nif (port->tx_buff_tab[n_desc]) {\r\nport->netdev->stats.tx_packets++;\r\nport->netdev->stats.tx_bytes += desc->pkt_len;\r\ndma_unmap_tx(port, desc);\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: eth_txdone_irq free %p\n",\r\nport->netdev->name, port->tx_buff_tab[n_desc]);\r\n#endif\r\nfree_buffer_irq(port->tx_buff_tab[n_desc]);\r\nport->tx_buff_tab[n_desc] = NULL;\r\n}\r\nstart = qmgr_stat_below_low_watermark(port->plat->txreadyq);\r\nqueue_put_desc(port->plat->txreadyq, phys, desc);\r\nif (start) {\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: eth_txdone_irq xmit ready\n",\r\nport->netdev->name);\r\n#endif\r\nnetif_wake_queue(port->netdev);\r\n}\r\n}\r\n}\r\nstatic int eth_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct port *port = netdev_priv(dev);\r\nunsigned int txreadyq = port->plat->txreadyq;\r\nint len, offset, bytes, n;\r\nvoid *mem;\r\nu32 phys;\r\nstruct desc *desc;\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: eth_xmit\n", dev->name);\r\n#endif\r\nif (unlikely(skb->len > MAX_MRU)) {\r\ndev_kfree_skb(skb);\r\ndev->stats.tx_errors++;\r\nreturn NETDEV_TX_OK;\r\n}\r\ndebug_pkt(dev, "eth_xmit", skb->data, skb->len);\r\nlen = skb->len;\r\n#ifdef __ARMEB__\r\noffset = 0;\r\nbytes = len;\r\nmem = skb->data;\r\n#else\r\noffset = (int)skb->data & 3;\r\nbytes = ALIGN(offset + len, 4);\r\nif (!(mem = kmalloc(bytes, GFP_ATOMIC))) {\r\ndev_kfree_skb(skb);\r\ndev->stats.tx_dropped++;\r\nreturn NETDEV_TX_OK;\r\n}\r\nmemcpy_swab32(mem, (u32 *)((int)skb->data & ~3), bytes / 4);\r\n#endif\r\nphys = dma_map_single(&dev->dev, mem, bytes, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&dev->dev, phys)) {\r\ndev_kfree_skb(skb);\r\n#ifndef __ARMEB__\r\nkfree(mem);\r\n#endif\r\ndev->stats.tx_dropped++;\r\nreturn NETDEV_TX_OK;\r\n}\r\nn = queue_get_desc(txreadyq, port, 1);\r\nBUG_ON(n < 0);\r\ndesc = tx_desc_ptr(port, n);\r\n#ifdef __ARMEB__\r\nport->tx_buff_tab[n] = skb;\r\n#else\r\nport->tx_buff_tab[n] = mem;\r\n#endif\r\ndesc->data = phys + offset;\r\ndesc->buf_len = desc->pkt_len = len;\r\nwmb();\r\nqueue_put_desc(TX_QUEUE(port->id), tx_desc_phys(port, n), desc);\r\nif (qmgr_stat_below_low_watermark(txreadyq)) {\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: eth_xmit queue full\n", dev->name);\r\n#endif\r\nnetif_stop_queue(dev);\r\nif (!qmgr_stat_below_low_watermark(txreadyq)) {\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: eth_xmit ready again\n",\r\ndev->name);\r\n#endif\r\nnetif_wake_queue(dev);\r\n}\r\n}\r\n#if DEBUG_TX\r\nprintk(KERN_DEBUG "%s: eth_xmit end\n", dev->name);\r\n#endif\r\nixp_tx_timestamp(port, skb);\r\nskb_tx_timestamp(skb);\r\n#ifndef __ARMEB__\r\ndev_kfree_skb(skb);\r\n#endif\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void eth_set_mcast_list(struct net_device *dev)\r\n{\r\nstruct port *port = netdev_priv(dev);\r\nstruct netdev_hw_addr *ha;\r\nu8 diffs[ETH_ALEN], *addr;\r\nint i;\r\nstatic const u8 allmulti[] = { 0x01, 0x00, 0x00, 0x00, 0x00, 0x00 };\r\nif ((dev->flags & IFF_ALLMULTI) && !(dev->flags & IFF_PROMISC)) {\r\nfor (i = 0; i < ETH_ALEN; i++) {\r\n__raw_writel(allmulti[i], &port->regs->mcast_addr[i]);\r\n__raw_writel(allmulti[i], &port->regs->mcast_mask[i]);\r\n}\r\n__raw_writel(DEFAULT_RX_CNTRL0 | RX_CNTRL0_ADDR_FLTR_EN,\r\n&port->regs->rx_control[0]);\r\nreturn;\r\n}\r\nif ((dev->flags & IFF_PROMISC) || netdev_mc_empty(dev)) {\r\n__raw_writel(DEFAULT_RX_CNTRL0 & ~RX_CNTRL0_ADDR_FLTR_EN,\r\n&port->regs->rx_control[0]);\r\nreturn;\r\n}\r\neth_zero_addr(diffs);\r\naddr = NULL;\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nif (!addr)\r\naddr = ha->addr;\r\nfor (i = 0; i < ETH_ALEN; i++)\r\ndiffs[i] |= addr[i] ^ ha->addr[i];\r\n}\r\nfor (i = 0; i < ETH_ALEN; i++) {\r\n__raw_writel(addr[i], &port->regs->mcast_addr[i]);\r\n__raw_writel(~diffs[i], &port->regs->mcast_mask[i]);\r\n}\r\n__raw_writel(DEFAULT_RX_CNTRL0 | RX_CNTRL0_ADDR_FLTR_EN,\r\n&port->regs->rx_control[0]);\r\n}\r\nstatic int eth_ioctl(struct net_device *dev, struct ifreq *req, int cmd)\r\n{\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nif (cpu_is_ixp46x()) {\r\nif (cmd == SIOCSHWTSTAMP)\r\nreturn hwtstamp_set(dev, req);\r\nif (cmd == SIOCGHWTSTAMP)\r\nreturn hwtstamp_get(dev, req);\r\n}\r\nreturn phy_mii_ioctl(dev->phydev, req, cmd);\r\n}\r\nstatic void ixp4xx_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *info)\r\n{\r\nstruct port *port = netdev_priv(dev);\r\nstrlcpy(info->driver, DRV_NAME, sizeof(info->driver));\r\nsnprintf(info->fw_version, sizeof(info->fw_version), "%u:%u:%u:%u",\r\nport->firmware[0], port->firmware[1],\r\nport->firmware[2], port->firmware[3]);\r\nstrlcpy(info->bus_info, "internal", sizeof(info->bus_info));\r\n}\r\nstatic int ixp4xx_get_ts_info(struct net_device *dev,\r\nstruct ethtool_ts_info *info)\r\n{\r\nif (!cpu_is_ixp46x()) {\r\ninfo->so_timestamping =\r\nSOF_TIMESTAMPING_TX_SOFTWARE |\r\nSOF_TIMESTAMPING_RX_SOFTWARE |\r\nSOF_TIMESTAMPING_SOFTWARE;\r\ninfo->phc_index = -1;\r\nreturn 0;\r\n}\r\ninfo->so_timestamping =\r\nSOF_TIMESTAMPING_TX_HARDWARE |\r\nSOF_TIMESTAMPING_RX_HARDWARE |\r\nSOF_TIMESTAMPING_RAW_HARDWARE;\r\ninfo->phc_index = ixp46x_phc_index;\r\ninfo->tx_types =\r\n(1 << HWTSTAMP_TX_OFF) |\r\n(1 << HWTSTAMP_TX_ON);\r\ninfo->rx_filters =\r\n(1 << HWTSTAMP_FILTER_NONE) |\r\n(1 << HWTSTAMP_FILTER_PTP_V1_L4_SYNC) |\r\n(1 << HWTSTAMP_FILTER_PTP_V1_L4_DELAY_REQ);\r\nreturn 0;\r\n}\r\nstatic int request_queues(struct port *port)\r\n{\r\nint err;\r\nerr = qmgr_request_queue(RXFREE_QUEUE(port->id), RX_DESCS, 0, 0,\r\n"%s:RX-free", port->netdev->name);\r\nif (err)\r\nreturn err;\r\nerr = qmgr_request_queue(port->plat->rxq, RX_DESCS, 0, 0,\r\n"%s:RX", port->netdev->name);\r\nif (err)\r\ngoto rel_rxfree;\r\nerr = qmgr_request_queue(TX_QUEUE(port->id), TX_DESCS, 0, 0,\r\n"%s:TX", port->netdev->name);\r\nif (err)\r\ngoto rel_rx;\r\nerr = qmgr_request_queue(port->plat->txreadyq, TX_DESCS, 0, 0,\r\n"%s:TX-ready", port->netdev->name);\r\nif (err)\r\ngoto rel_tx;\r\nif (!ports_open) {\r\nerr = qmgr_request_queue(TXDONE_QUEUE, TXDONE_QUEUE_LEN, 0, 0,\r\n"%s:TX-done", DRV_NAME);\r\nif (err)\r\ngoto rel_txready;\r\n}\r\nreturn 0;\r\nrel_txready:\r\nqmgr_release_queue(port->plat->txreadyq);\r\nrel_tx:\r\nqmgr_release_queue(TX_QUEUE(port->id));\r\nrel_rx:\r\nqmgr_release_queue(port->plat->rxq);\r\nrel_rxfree:\r\nqmgr_release_queue(RXFREE_QUEUE(port->id));\r\nprintk(KERN_DEBUG "%s: unable to request hardware queues\n",\r\nport->netdev->name);\r\nreturn err;\r\n}\r\nstatic void release_queues(struct port *port)\r\n{\r\nqmgr_release_queue(RXFREE_QUEUE(port->id));\r\nqmgr_release_queue(port->plat->rxq);\r\nqmgr_release_queue(TX_QUEUE(port->id));\r\nqmgr_release_queue(port->plat->txreadyq);\r\nif (!ports_open)\r\nqmgr_release_queue(TXDONE_QUEUE);\r\n}\r\nstatic int init_queues(struct port *port)\r\n{\r\nint i;\r\nif (!ports_open) {\r\ndma_pool = dma_pool_create(DRV_NAME, &port->netdev->dev,\r\nPOOL_ALLOC_SIZE, 32, 0);\r\nif (!dma_pool)\r\nreturn -ENOMEM;\r\n}\r\nif (!(port->desc_tab = dma_pool_alloc(dma_pool, GFP_KERNEL,\r\n&port->desc_tab_phys)))\r\nreturn -ENOMEM;\r\nmemset(port->desc_tab, 0, POOL_ALLOC_SIZE);\r\nmemset(port->rx_buff_tab, 0, sizeof(port->rx_buff_tab));\r\nmemset(port->tx_buff_tab, 0, sizeof(port->tx_buff_tab));\r\nfor (i = 0; i < RX_DESCS; i++) {\r\nstruct desc *desc = rx_desc_ptr(port, i);\r\nbuffer_t *buff;\r\nvoid *data;\r\n#ifdef __ARMEB__\r\nif (!(buff = netdev_alloc_skb(port->netdev, RX_BUFF_SIZE)))\r\nreturn -ENOMEM;\r\ndata = buff->data;\r\n#else\r\nif (!(buff = kmalloc(RX_BUFF_SIZE, GFP_KERNEL)))\r\nreturn -ENOMEM;\r\ndata = buff;\r\n#endif\r\ndesc->buf_len = MAX_MRU;\r\ndesc->data = dma_map_single(&port->netdev->dev, data,\r\nRX_BUFF_SIZE, DMA_FROM_DEVICE);\r\nif (dma_mapping_error(&port->netdev->dev, desc->data)) {\r\nfree_buffer(buff);\r\nreturn -EIO;\r\n}\r\ndesc->data += NET_IP_ALIGN;\r\nport->rx_buff_tab[i] = buff;\r\n}\r\nreturn 0;\r\n}\r\nstatic void destroy_queues(struct port *port)\r\n{\r\nint i;\r\nif (port->desc_tab) {\r\nfor (i = 0; i < RX_DESCS; i++) {\r\nstruct desc *desc = rx_desc_ptr(port, i);\r\nbuffer_t *buff = port->rx_buff_tab[i];\r\nif (buff) {\r\ndma_unmap_single(&port->netdev->dev,\r\ndesc->data - NET_IP_ALIGN,\r\nRX_BUFF_SIZE, DMA_FROM_DEVICE);\r\nfree_buffer(buff);\r\n}\r\n}\r\nfor (i = 0; i < TX_DESCS; i++) {\r\nstruct desc *desc = tx_desc_ptr(port, i);\r\nbuffer_t *buff = port->tx_buff_tab[i];\r\nif (buff) {\r\ndma_unmap_tx(port, desc);\r\nfree_buffer(buff);\r\n}\r\n}\r\ndma_pool_free(dma_pool, port->desc_tab, port->desc_tab_phys);\r\nport->desc_tab = NULL;\r\n}\r\nif (!ports_open && dma_pool) {\r\ndma_pool_destroy(dma_pool);\r\ndma_pool = NULL;\r\n}\r\n}\r\nstatic int eth_open(struct net_device *dev)\r\n{\r\nstruct port *port = netdev_priv(dev);\r\nstruct npe *npe = port->npe;\r\nstruct msg msg;\r\nint i, err;\r\nif (!npe_running(npe)) {\r\nerr = npe_load_firmware(npe, npe_name(npe), &dev->dev);\r\nif (err)\r\nreturn err;\r\nif (npe_recv_message(npe, &msg, "ETH_GET_STATUS")) {\r\nprintk(KERN_ERR "%s: %s not responding\n", dev->name,\r\nnpe_name(npe));\r\nreturn -EIO;\r\n}\r\nport->firmware[0] = msg.byte4;\r\nport->firmware[1] = msg.byte5;\r\nport->firmware[2] = msg.byte6;\r\nport->firmware[3] = msg.byte7;\r\n}\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = NPE_VLAN_SETRXQOSENTRY;\r\nmsg.eth_id = port->id;\r\nmsg.byte5 = port->plat->rxq | 0x80;\r\nmsg.byte7 = port->plat->rxq << 4;\r\nfor (i = 0; i < 8; i++) {\r\nmsg.byte3 = i;\r\nif (npe_send_recv_message(port->npe, &msg, "ETH_SET_RXQ"))\r\nreturn -EIO;\r\n}\r\nmsg.cmd = NPE_EDB_SETPORTADDRESS;\r\nmsg.eth_id = PHYSICAL_ID(port->id);\r\nmsg.byte2 = dev->dev_addr[0];\r\nmsg.byte3 = dev->dev_addr[1];\r\nmsg.byte4 = dev->dev_addr[2];\r\nmsg.byte5 = dev->dev_addr[3];\r\nmsg.byte6 = dev->dev_addr[4];\r\nmsg.byte7 = dev->dev_addr[5];\r\nif (npe_send_recv_message(port->npe, &msg, "ETH_SET_MAC"))\r\nreturn -EIO;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = NPE_FW_SETFIREWALLMODE;\r\nmsg.eth_id = port->id;\r\nif (npe_send_recv_message(port->npe, &msg, "ETH_SET_FIREWALL_MODE"))\r\nreturn -EIO;\r\nif ((err = request_queues(port)) != 0)\r\nreturn err;\r\nif ((err = init_queues(port)) != 0) {\r\ndestroy_queues(port);\r\nrelease_queues(port);\r\nreturn err;\r\n}\r\nport->speed = 0;\r\nphy_start(dev->phydev);\r\nfor (i = 0; i < ETH_ALEN; i++)\r\n__raw_writel(dev->dev_addr[i], &port->regs->hw_addr[i]);\r\n__raw_writel(0x08, &port->regs->random_seed);\r\n__raw_writel(0x12, &port->regs->partial_empty_threshold);\r\n__raw_writel(0x30, &port->regs->partial_full_threshold);\r\n__raw_writel(0x08, &port->regs->tx_start_bytes);\r\n__raw_writel(0x15, &port->regs->tx_deferral);\r\n__raw_writel(0x08, &port->regs->tx_2part_deferral[0]);\r\n__raw_writel(0x07, &port->regs->tx_2part_deferral[1]);\r\n__raw_writel(0x80, &port->regs->slot_time);\r\n__raw_writel(0x01, &port->regs->int_clock_threshold);\r\nfor (i = 0; i < TX_DESCS; i++)\r\nqueue_put_desc(port->plat->txreadyq,\r\ntx_desc_phys(port, i), tx_desc_ptr(port, i));\r\nfor (i = 0; i < RX_DESCS; i++)\r\nqueue_put_desc(RXFREE_QUEUE(port->id),\r\nrx_desc_phys(port, i), rx_desc_ptr(port, i));\r\n__raw_writel(TX_CNTRL1_RETRIES, &port->regs->tx_control[1]);\r\n__raw_writel(DEFAULT_TX_CNTRL0, &port->regs->tx_control[0]);\r\n__raw_writel(0, &port->regs->rx_control[1]);\r\n__raw_writel(DEFAULT_RX_CNTRL0, &port->regs->rx_control[0]);\r\nnapi_enable(&port->napi);\r\neth_set_mcast_list(dev);\r\nnetif_start_queue(dev);\r\nqmgr_set_irq(port->plat->rxq, QUEUE_IRQ_SRC_NOT_EMPTY,\r\neth_rx_irq, dev);\r\nif (!ports_open) {\r\nqmgr_set_irq(TXDONE_QUEUE, QUEUE_IRQ_SRC_NOT_EMPTY,\r\neth_txdone_irq, NULL);\r\nqmgr_enable_irq(TXDONE_QUEUE);\r\n}\r\nports_open++;\r\nnapi_schedule(&port->napi);\r\nreturn 0;\r\n}\r\nstatic int eth_close(struct net_device *dev)\r\n{\r\nstruct port *port = netdev_priv(dev);\r\nstruct msg msg;\r\nint buffs = RX_DESCS;\r\nint i;\r\nports_open--;\r\nqmgr_disable_irq(port->plat->rxq);\r\nnapi_disable(&port->napi);\r\nnetif_stop_queue(dev);\r\nwhile (queue_get_desc(RXFREE_QUEUE(port->id), port, 0) >= 0)\r\nbuffs--;\r\nmemset(&msg, 0, sizeof(msg));\r\nmsg.cmd = NPE_SETLOOPBACK_MODE;\r\nmsg.eth_id = port->id;\r\nmsg.byte3 = 1;\r\nif (npe_send_recv_message(port->npe, &msg, "ETH_ENABLE_LOOPBACK"))\r\nprintk(KERN_CRIT "%s: unable to enable loopback\n", dev->name);\r\ni = 0;\r\ndo {\r\nwhile (queue_get_desc(port->plat->rxq, port, 0) >= 0)\r\nbuffs--;\r\nif (!buffs)\r\nbreak;\r\nif (qmgr_stat_empty(TX_QUEUE(port->id))) {\r\nstruct desc *desc;\r\nu32 phys;\r\nint n = queue_get_desc(port->plat->txreadyq, port, 1);\r\nBUG_ON(n < 0);\r\ndesc = tx_desc_ptr(port, n);\r\nphys = tx_desc_phys(port, n);\r\ndesc->buf_len = desc->pkt_len = 1;\r\nwmb();\r\nqueue_put_desc(TX_QUEUE(port->id), phys, desc);\r\n}\r\nudelay(1);\r\n} while (++i < MAX_CLOSE_WAIT);\r\nif (buffs)\r\nprintk(KERN_CRIT "%s: unable to drain RX queue, %i buffer(s)"\r\n" left in NPE\n", dev->name, buffs);\r\n#if DEBUG_CLOSE\r\nif (!buffs)\r\nprintk(KERN_DEBUG "Draining RX queue took %i cycles\n", i);\r\n#endif\r\nbuffs = TX_DESCS;\r\nwhile (queue_get_desc(TX_QUEUE(port->id), port, 1) >= 0)\r\nbuffs--;\r\ni = 0;\r\ndo {\r\nwhile (queue_get_desc(port->plat->txreadyq, port, 1) >= 0)\r\nbuffs--;\r\nif (!buffs)\r\nbreak;\r\n} while (++i < MAX_CLOSE_WAIT);\r\nif (buffs)\r\nprintk(KERN_CRIT "%s: unable to drain TX queue, %i buffer(s) "\r\n"left in NPE\n", dev->name, buffs);\r\n#if DEBUG_CLOSE\r\nif (!buffs)\r\nprintk(KERN_DEBUG "Draining TX queues took %i cycles\n", i);\r\n#endif\r\nmsg.byte3 = 0;\r\nif (npe_send_recv_message(port->npe, &msg, "ETH_DISABLE_LOOPBACK"))\r\nprintk(KERN_CRIT "%s: unable to disable loopback\n",\r\ndev->name);\r\nphy_stop(dev->phydev);\r\nif (!ports_open)\r\nqmgr_disable_irq(TXDONE_QUEUE);\r\ndestroy_queues(port);\r\nrelease_queues(port);\r\nreturn 0;\r\n}\r\nstatic int eth_init_one(struct platform_device *pdev)\r\n{\r\nstruct port *port;\r\nstruct net_device *dev;\r\nstruct eth_plat_info *plat = dev_get_platdata(&pdev->dev);\r\nstruct phy_device *phydev = NULL;\r\nu32 regs_phys;\r\nchar phy_id[MII_BUS_ID_SIZE + 3];\r\nint err;\r\nif (!(dev = alloc_etherdev(sizeof(struct port))))\r\nreturn -ENOMEM;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nport = netdev_priv(dev);\r\nport->netdev = dev;\r\nport->id = pdev->id;\r\nswitch (port->id) {\r\ncase IXP4XX_ETH_NPEA:\r\nport->regs = (struct eth_regs __iomem *)IXP4XX_EthA_BASE_VIRT;\r\nregs_phys = IXP4XX_EthA_BASE_PHYS;\r\nbreak;\r\ncase IXP4XX_ETH_NPEB:\r\nport->regs = (struct eth_regs __iomem *)IXP4XX_EthB_BASE_VIRT;\r\nregs_phys = IXP4XX_EthB_BASE_PHYS;\r\nbreak;\r\ncase IXP4XX_ETH_NPEC:\r\nport->regs = (struct eth_regs __iomem *)IXP4XX_EthC_BASE_VIRT;\r\nregs_phys = IXP4XX_EthC_BASE_PHYS;\r\nbreak;\r\ndefault:\r\nerr = -ENODEV;\r\ngoto err_free;\r\n}\r\ndev->netdev_ops = &ixp4xx_netdev_ops;\r\ndev->ethtool_ops = &ixp4xx_ethtool_ops;\r\ndev->tx_queue_len = 100;\r\nnetif_napi_add(dev, &port->napi, eth_poll, NAPI_WEIGHT);\r\nif (!(port->npe = npe_request(NPE_ID(port->id)))) {\r\nerr = -EIO;\r\ngoto err_free;\r\n}\r\nport->mem_res = request_mem_region(regs_phys, REGS_SIZE, dev->name);\r\nif (!port->mem_res) {\r\nerr = -EBUSY;\r\ngoto err_npe_rel;\r\n}\r\nport->plat = plat;\r\nnpe_port_tab[NPE_ID(port->id)] = port;\r\nmemcpy(dev->dev_addr, plat->hwaddr, ETH_ALEN);\r\nplatform_set_drvdata(pdev, dev);\r\n__raw_writel(DEFAULT_CORE_CNTRL | CORE_RESET,\r\n&port->regs->core_control);\r\nudelay(50);\r\n__raw_writel(DEFAULT_CORE_CNTRL, &port->regs->core_control);\r\nudelay(50);\r\nsnprintf(phy_id, MII_BUS_ID_SIZE + 3, PHY_ID_FMT,\r\nmdio_bus->id, plat->phy);\r\nphydev = phy_connect(dev, phy_id, &ixp4xx_adjust_link,\r\nPHY_INTERFACE_MODE_MII);\r\nif (IS_ERR(phydev)) {\r\nerr = PTR_ERR(phydev);\r\ngoto err_free_mem;\r\n}\r\nphydev->irq = PHY_POLL;\r\nif ((err = register_netdev(dev)))\r\ngoto err_phy_dis;\r\nprintk(KERN_INFO "%s: MII PHY %i on %s\n", dev->name, plat->phy,\r\nnpe_name(port->npe));\r\nreturn 0;\r\nerr_phy_dis:\r\nphy_disconnect(phydev);\r\nerr_free_mem:\r\nnpe_port_tab[NPE_ID(port->id)] = NULL;\r\nrelease_resource(port->mem_res);\r\nerr_npe_rel:\r\nnpe_release(port->npe);\r\nerr_free:\r\nfree_netdev(dev);\r\nreturn err;\r\n}\r\nstatic int eth_remove_one(struct platform_device *pdev)\r\n{\r\nstruct net_device *dev = platform_get_drvdata(pdev);\r\nstruct phy_device *phydev = dev->phydev;\r\nstruct port *port = netdev_priv(dev);\r\nunregister_netdev(dev);\r\nphy_disconnect(phydev);\r\nnpe_port_tab[NPE_ID(port->id)] = NULL;\r\nnpe_release(port->npe);\r\nrelease_resource(port->mem_res);\r\nfree_netdev(dev);\r\nreturn 0;\r\n}\r\nstatic int __init eth_init_module(void)\r\n{\r\nint err;\r\nif ((err = ixp4xx_mdio_register()))\r\nreturn err;\r\nreturn platform_driver_register(&ixp4xx_eth_driver);\r\n}\r\nstatic void __exit eth_cleanup_module(void)\r\n{\r\nplatform_driver_unregister(&ixp4xx_eth_driver);\r\nixp4xx_mdio_remove();\r\n}
