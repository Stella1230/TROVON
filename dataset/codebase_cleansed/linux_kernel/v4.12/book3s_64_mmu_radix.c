int kvmppc_mmu_radix_xlate(struct kvm_vcpu *vcpu, gva_t eaddr,\r\nstruct kvmppc_pte *gpte, bool data, bool iswrite)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nu32 pid;\r\nint ret, level, ps;\r\n__be64 prte, rpte;\r\nunsigned long ptbl;\r\nunsigned long root, pte, index;\r\nunsigned long rts, bits, offset;\r\nunsigned long gpa;\r\nunsigned long proc_tbl_size;\r\nswitch (eaddr >> 62) {\r\ncase 0:\r\npid = vcpu->arch.pid;\r\nbreak;\r\ncase 3:\r\npid = 0;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nproc_tbl_size = 1 << ((kvm->arch.process_table & PRTS_MASK) + 12);\r\nif (pid * 16 >= proc_tbl_size)\r\nreturn -EINVAL;\r\nptbl = (kvm->arch.process_table & PRTB_MASK) + (pid * 16);\r\nret = kvm_read_guest(kvm, ptbl, &prte, sizeof(prte));\r\nif (ret)\r\nreturn ret;\r\nroot = be64_to_cpu(prte);\r\nrts = ((root & RTS1_MASK) >> (RTS1_SHIFT - 3)) |\r\n((root & RTS2_MASK) >> RTS2_SHIFT);\r\nbits = root & RPDS_MASK;\r\nroot = root & RPDB_MASK;\r\noffset = rts + 31;\r\nif (cpu_has_feature(CPU_FTR_POWER9_DD1))\r\noffset -= 3;\r\nif (offset != 52)\r\nreturn -EINVAL;\r\nfor (level = 3; level >= 0; --level) {\r\nif (level && bits != p9_supported_radix_bits[level])\r\nreturn -EINVAL;\r\nif (level == 0 && !(bits == 5 || bits == 9))\r\nreturn -EINVAL;\r\noffset -= bits;\r\nindex = (eaddr >> offset) & ((1UL << bits) - 1);\r\nif (root & ((1UL << (bits + 3)) - 1))\r\nreturn -EINVAL;\r\nret = kvm_read_guest(kvm, root + index * 8,\r\n&rpte, sizeof(rpte));\r\nif (ret)\r\nreturn ret;\r\npte = __be64_to_cpu(rpte);\r\nif (!(pte & _PAGE_PRESENT))\r\nreturn -ENOENT;\r\nif (pte & _PAGE_PTE)\r\nbreak;\r\nbits = pte & 0x1f;\r\nroot = pte & 0x0fffffffffffff00ul;\r\n}\r\nif (level < 0 || level == 3)\r\nreturn -EINVAL;\r\ngpa = pte & 0x01fffffffffff000ul;\r\nif (gpa & ((1ul << offset) - 1))\r\nreturn -EINVAL;\r\ngpa += eaddr & ((1ul << offset) - 1);\r\nfor (ps = MMU_PAGE_4K; ps < MMU_PAGE_COUNT; ++ps)\r\nif (offset == mmu_psize_defs[ps].shift)\r\nbreak;\r\ngpte->page_size = ps;\r\ngpte->eaddr = eaddr;\r\ngpte->raddr = gpa;\r\ngpte->may_read = !!(pte & _PAGE_READ);\r\ngpte->may_write = !!(pte & _PAGE_WRITE);\r\ngpte->may_execute = !!(pte & _PAGE_EXEC);\r\nif (kvmppc_get_msr(vcpu) & MSR_PR) {\r\nif (pte & _PAGE_PRIVILEGED) {\r\ngpte->may_read = 0;\r\ngpte->may_write = 0;\r\ngpte->may_execute = 0;\r\n}\r\n} else {\r\nif (!(pte & _PAGE_PRIVILEGED)) {\r\nif (vcpu->arch.amr & (1ul << 62))\r\ngpte->may_read = 0;\r\nif (vcpu->arch.amr & (1ul << 63))\r\ngpte->may_write = 0;\r\nif (vcpu->arch.iamr & (1ul << 62))\r\ngpte->may_execute = 0;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic void kvmppc_radix_tlbie_page(struct kvm *kvm, unsigned long addr,\r\nunsigned int pshift)\r\n{\r\nint psize = MMU_BASE_PSIZE;\r\nif (pshift >= PMD_SHIFT)\r\npsize = MMU_PAGE_2M;\r\naddr &= ~0xfffUL;\r\naddr |= mmu_psize_defs[psize].ap << 5;\r\nasm volatile("ptesync": : :"memory");\r\nasm volatile(PPC_TLBIE_5(%0, %1, 0, 0, 1)\r\n: : "r" (addr), "r" (kvm->arch.lpid) : "memory");\r\nasm volatile("ptesync": : :"memory");\r\n}\r\nunsigned long kvmppc_radix_update_pte(struct kvm *kvm, pte_t *ptep,\r\nunsigned long clr, unsigned long set,\r\nunsigned long addr, unsigned int shift)\r\n{\r\nunsigned long old = 0;\r\nif (!(clr & _PAGE_PRESENT) && cpu_has_feature(CPU_FTR_POWER9_DD1) &&\r\npte_present(*ptep)) {\r\nold = __radix_pte_update(ptep, _PAGE_PRESENT, 0);\r\nkvmppc_radix_tlbie_page(kvm, addr, shift);\r\nset |= _PAGE_PRESENT;\r\nold &= _PAGE_PRESENT;\r\n}\r\nreturn __radix_pte_update(ptep, clr, set) | old;\r\n}\r\nvoid kvmppc_radix_set_pte_at(struct kvm *kvm, unsigned long addr,\r\npte_t *ptep, pte_t pte)\r\n{\r\nradix__set_pte_at(kvm->mm, addr, ptep, pte, 0);\r\n}\r\nstatic pte_t *kvmppc_pte_alloc(void)\r\n{\r\nreturn kmem_cache_alloc(kvm_pte_cache, GFP_KERNEL);\r\n}\r\nstatic void kvmppc_pte_free(pte_t *ptep)\r\n{\r\nkmem_cache_free(kvm_pte_cache, ptep);\r\n}\r\nstatic int kvmppc_create_pte(struct kvm *kvm, pte_t pte, unsigned long gpa,\r\nunsigned int level, unsigned long mmu_seq)\r\n{\r\npgd_t *pgd;\r\npud_t *pud, *new_pud = NULL;\r\npmd_t *pmd, *new_pmd = NULL;\r\npte_t *ptep, *new_ptep = NULL;\r\nunsigned long old;\r\nint ret;\r\npgd = kvm->arch.pgtable + pgd_index(gpa);\r\npud = NULL;\r\nif (pgd_present(*pgd))\r\npud = pud_offset(pgd, gpa);\r\nelse\r\nnew_pud = pud_alloc_one(kvm->mm, gpa);\r\npmd = NULL;\r\nif (pud && pud_present(*pud))\r\npmd = pmd_offset(pud, gpa);\r\nelse\r\nnew_pmd = pmd_alloc_one(kvm->mm, gpa);\r\nif (level == 0 && !(pmd && pmd_present(*pmd)))\r\nnew_ptep = kvmppc_pte_alloc();\r\nspin_lock(&kvm->mmu_lock);\r\nret = -EAGAIN;\r\nif (mmu_notifier_retry(kvm, mmu_seq))\r\ngoto out_unlock;\r\nret = -ENOMEM;\r\nif (pgd_none(*pgd)) {\r\nif (!new_pud)\r\ngoto out_unlock;\r\npgd_populate(kvm->mm, pgd, new_pud);\r\nnew_pud = NULL;\r\n}\r\npud = pud_offset(pgd, gpa);\r\nif (pud_none(*pud)) {\r\nif (!new_pmd)\r\ngoto out_unlock;\r\npud_populate(kvm->mm, pud, new_pmd);\r\nnew_pmd = NULL;\r\n}\r\npmd = pmd_offset(pud, gpa);\r\nif (pmd_large(*pmd)) {\r\nret = -EAGAIN;\r\ngoto out_unlock;\r\n}\r\nif (level == 1 && !pmd_none(*pmd)) {\r\nret = -EBUSY;\r\ngoto out_unlock;\r\n}\r\nif (level == 0) {\r\nif (pmd_none(*pmd)) {\r\nif (!new_ptep)\r\ngoto out_unlock;\r\npmd_populate(kvm->mm, pmd, new_ptep);\r\nnew_ptep = NULL;\r\n}\r\nptep = pte_offset_kernel(pmd, gpa);\r\nif (pte_present(*ptep)) {\r\nold = kvmppc_radix_update_pte(kvm, ptep, _PAGE_PRESENT,\r\n0, gpa, 0);\r\nkvmppc_radix_tlbie_page(kvm, gpa, 0);\r\nif (old & _PAGE_DIRTY)\r\nmark_page_dirty(kvm, gpa >> PAGE_SHIFT);\r\n}\r\nkvmppc_radix_set_pte_at(kvm, gpa, ptep, pte);\r\n} else {\r\nkvmppc_radix_set_pte_at(kvm, gpa, pmdp_ptep(pmd), pte);\r\n}\r\nret = 0;\r\nout_unlock:\r\nspin_unlock(&kvm->mmu_lock);\r\nif (new_pud)\r\npud_free(kvm->mm, new_pud);\r\nif (new_pmd)\r\npmd_free(kvm->mm, new_pmd);\r\nif (new_ptep)\r\nkvmppc_pte_free(new_ptep);\r\nreturn ret;\r\n}\r\nint kvmppc_book3s_radix_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned long ea, unsigned long dsisr)\r\n{\r\nstruct kvm *kvm = vcpu->kvm;\r\nunsigned long mmu_seq, pte_size;\r\nunsigned long gpa, gfn, hva, pfn;\r\nstruct kvm_memory_slot *memslot;\r\nstruct page *page = NULL, *pages[1];\r\nlong ret, npages, ok;\r\nunsigned int writing;\r\nstruct vm_area_struct *vma;\r\nunsigned long flags;\r\npte_t pte, *ptep;\r\nunsigned long pgflags;\r\nunsigned int shift, level;\r\nif (dsisr & DSISR_UNSUPP_MMU) {\r\npr_err("KVM: Got unsupported MMU fault\n");\r\nreturn -EFAULT;\r\n}\r\nif (dsisr & DSISR_BADACCESS) {\r\npr_err("KVM: Got radix HV page fault with DSISR=%lx\n", dsisr);\r\nkvmppc_core_queue_data_storage(vcpu, ea, dsisr);\r\nreturn RESUME_GUEST;\r\n}\r\ngpa = vcpu->arch.fault_gpa & ~0xfffUL;\r\ngpa &= ~0xF000000000000000ul;\r\ngfn = gpa >> PAGE_SHIFT;\r\nif (!(dsisr & DSISR_PGDIRFAULT))\r\ngpa |= ea & 0xfff;\r\nmemslot = gfn_to_memslot(kvm, gfn);\r\nif (!memslot || (memslot->flags & KVM_MEMSLOT_INVALID)) {\r\nif (dsisr & (DSISR_PGDIRFAULT | DSISR_BADACCESS |\r\nDSISR_SET_RC)) {\r\nkvmppc_core_queue_data_storage(vcpu, ea, dsisr);\r\nreturn RESUME_GUEST;\r\n}\r\nreturn kvmppc_hv_emulate_mmio(run, vcpu, gpa, ea,\r\ndsisr & DSISR_ISSTORE);\r\n}\r\nmmu_seq = kvm->mmu_notifier_seq;\r\nsmp_rmb();\r\nwriting = (dsisr & DSISR_ISSTORE) != 0;\r\nhva = gfn_to_hva_memslot(memslot, gfn);\r\nif (dsisr & DSISR_SET_RC) {\r\nok = 0;\r\npgflags = _PAGE_ACCESSED;\r\nif (writing)\r\npgflags |= _PAGE_DIRTY;\r\nlocal_irq_save(flags);\r\nptep = __find_linux_pte_or_hugepte(current->mm->pgd, hva,\r\nNULL, NULL);\r\nif (ptep) {\r\npte = READ_ONCE(*ptep);\r\nif (pte_present(pte) &&\r\n(pte_val(pte) & pgflags) == pgflags)\r\nok = 1;\r\n}\r\nlocal_irq_restore(flags);\r\nif (ok) {\r\nspin_lock(&kvm->mmu_lock);\r\nif (mmu_notifier_retry(vcpu->kvm, mmu_seq)) {\r\nspin_unlock(&kvm->mmu_lock);\r\nreturn RESUME_GUEST;\r\n}\r\nptep = __find_linux_pte_or_hugepte(kvm->arch.pgtable,\r\ngpa, NULL, &shift);\r\nif (ptep && pte_present(*ptep)) {\r\nkvmppc_radix_update_pte(kvm, ptep, 0, pgflags,\r\ngpa, shift);\r\nspin_unlock(&kvm->mmu_lock);\r\nreturn RESUME_GUEST;\r\n}\r\nspin_unlock(&kvm->mmu_lock);\r\n}\r\n}\r\nret = -EFAULT;\r\npfn = 0;\r\npte_size = PAGE_SIZE;\r\npgflags = _PAGE_READ | _PAGE_EXEC;\r\nlevel = 0;\r\nnpages = get_user_pages_fast(hva, 1, writing, pages);\r\nif (npages < 1) {\r\ndown_read(&current->mm->mmap_sem);\r\nvma = find_vma(current->mm, hva);\r\nif (vma && vma->vm_start <= hva && hva < vma->vm_end &&\r\n(vma->vm_flags & VM_PFNMAP)) {\r\npfn = vma->vm_pgoff +\r\n((hva - vma->vm_start) >> PAGE_SHIFT);\r\npgflags = pgprot_val(vma->vm_page_prot);\r\n}\r\nup_read(&current->mm->mmap_sem);\r\nif (!pfn)\r\nreturn -EFAULT;\r\n} else {\r\npage = pages[0];\r\npfn = page_to_pfn(page);\r\nif (PageHuge(page)) {\r\npage = compound_head(page);\r\npte_size <<= compound_order(page);\r\nif (pte_size >= PMD_SIZE &&\r\n(gpa & PMD_MASK & PAGE_MASK) ==\r\n(hva & PMD_MASK & PAGE_MASK)) {\r\nlevel = 1;\r\npfn &= ~((PMD_SIZE >> PAGE_SHIFT) - 1);\r\n}\r\n}\r\nif (writing) {\r\npgflags |= _PAGE_WRITE;\r\n} else {\r\nlocal_irq_save(flags);\r\nptep = __find_linux_pte_or_hugepte(current->mm->pgd,\r\nhva, NULL, NULL);\r\nif (ptep && pte_write(*ptep) && pte_dirty(*ptep))\r\npgflags |= _PAGE_WRITE;\r\nlocal_irq_restore(flags);\r\n}\r\n}\r\npgflags |= _PAGE_PRESENT | _PAGE_PTE | _PAGE_ACCESSED;\r\nif (pgflags & _PAGE_WRITE)\r\npgflags |= _PAGE_DIRTY;\r\npte = pfn_pte(pfn, __pgprot(pgflags));\r\nret = kvmppc_create_pte(kvm, pte, gpa, level, mmu_seq);\r\nif (ret == -EBUSY) {\r\nlevel = 0;\r\npfn |= gfn & ((PMD_SIZE >> PAGE_SHIFT) - 1);\r\npte = pfn_pte(pfn, __pgprot(pgflags));\r\nret = kvmppc_create_pte(kvm, pte, gpa, level, mmu_seq);\r\n}\r\nif (ret == 0 || ret == -EAGAIN)\r\nret = RESUME_GUEST;\r\nif (page) {\r\nput_page(pages[0]);\r\n}\r\nreturn ret;\r\n}\r\nstatic void mark_pages_dirty(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long gfn, unsigned int order)\r\n{\r\nunsigned long i, limit;\r\nunsigned long *dp;\r\nif (!memslot->dirty_bitmap)\r\nreturn;\r\nlimit = 1ul << order;\r\nif (limit < BITS_PER_LONG) {\r\nfor (i = 0; i < limit; ++i)\r\nmark_page_dirty(kvm, gfn + i);\r\nreturn;\r\n}\r\ndp = memslot->dirty_bitmap + (gfn - memslot->base_gfn);\r\nlimit /= BITS_PER_LONG;\r\nfor (i = 0; i < limit; ++i)\r\n*dp++ = ~0ul;\r\n}\r\nint kvm_unmap_radix(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long gfn)\r\n{\r\npte_t *ptep;\r\nunsigned long gpa = gfn << PAGE_SHIFT;\r\nunsigned int shift;\r\nunsigned long old;\r\nptep = __find_linux_pte_or_hugepte(kvm->arch.pgtable, gpa,\r\nNULL, &shift);\r\nif (ptep && pte_present(*ptep)) {\r\nold = kvmppc_radix_update_pte(kvm, ptep, _PAGE_PRESENT, 0,\r\ngpa, shift);\r\nkvmppc_radix_tlbie_page(kvm, gpa, shift);\r\nif (old & _PAGE_DIRTY) {\r\nif (!shift)\r\nmark_page_dirty(kvm, gfn);\r\nelse\r\nmark_pages_dirty(kvm, memslot,\r\ngfn, shift - PAGE_SHIFT);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint kvm_age_radix(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long gfn)\r\n{\r\npte_t *ptep;\r\nunsigned long gpa = gfn << PAGE_SHIFT;\r\nunsigned int shift;\r\nint ref = 0;\r\nptep = __find_linux_pte_or_hugepte(kvm->arch.pgtable, gpa,\r\nNULL, &shift);\r\nif (ptep && pte_present(*ptep) && pte_young(*ptep)) {\r\nkvmppc_radix_update_pte(kvm, ptep, _PAGE_ACCESSED, 0,\r\ngpa, shift);\r\nref = 1;\r\n}\r\nreturn ref;\r\n}\r\nint kvm_test_age_radix(struct kvm *kvm, struct kvm_memory_slot *memslot,\r\nunsigned long gfn)\r\n{\r\npte_t *ptep;\r\nunsigned long gpa = gfn << PAGE_SHIFT;\r\nunsigned int shift;\r\nint ref = 0;\r\nptep = __find_linux_pte_or_hugepte(kvm->arch.pgtable, gpa,\r\nNULL, &shift);\r\nif (ptep && pte_present(*ptep) && pte_young(*ptep))\r\nref = 1;\r\nreturn ref;\r\n}\r\nstatic int kvm_radix_test_clear_dirty(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot, int pagenum)\r\n{\r\nunsigned long gfn = memslot->base_gfn + pagenum;\r\nunsigned long gpa = gfn << PAGE_SHIFT;\r\npte_t *ptep;\r\nunsigned int shift;\r\nint ret = 0;\r\nptep = __find_linux_pte_or_hugepte(kvm->arch.pgtable, gpa,\r\nNULL, &shift);\r\nif (ptep && pte_present(*ptep) && pte_dirty(*ptep)) {\r\nret = 1;\r\nif (shift)\r\nret = 1 << (shift - PAGE_SHIFT);\r\nkvmppc_radix_update_pte(kvm, ptep, _PAGE_DIRTY, 0,\r\ngpa, shift);\r\nkvmppc_radix_tlbie_page(kvm, gpa, shift);\r\n}\r\nreturn ret;\r\n}\r\nlong kvmppc_hv_get_dirty_log_radix(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot, unsigned long *map)\r\n{\r\nunsigned long i, j;\r\nunsigned long n, *p;\r\nint npages;\r\nn = kvm_dirty_bitmap_bytes(memslot) / sizeof(long);\r\np = memslot->dirty_bitmap;\r\nfor (i = 0; i < n; ++i)\r\nmap[i] |= xchg(&p[i], 0);\r\nfor (i = 0; i < memslot->npages; i = j) {\r\nnpages = kvm_radix_test_clear_dirty(kvm, memslot, i);\r\nj = i + 1;\r\nif (npages)\r\nfor (j = i; npages; ++j, --npages)\r\n__set_bit_le(j, map);\r\n}\r\nreturn 0;\r\n}\r\nstatic void add_rmmu_ap_encoding(struct kvm_ppc_rmmu_info *info,\r\nint psize, int *indexp)\r\n{\r\nif (!mmu_psize_defs[psize].shift)\r\nreturn;\r\ninfo->ap_encodings[*indexp] = mmu_psize_defs[psize].shift |\r\n(mmu_psize_defs[psize].ap << 29);\r\n++(*indexp);\r\n}\r\nint kvmhv_get_rmmu_info(struct kvm *kvm, struct kvm_ppc_rmmu_info *info)\r\n{\r\nint i;\r\nif (!radix_enabled())\r\nreturn -EINVAL;\r\nmemset(info, 0, sizeof(*info));\r\ninfo->geometries[0].page_shift = 12;\r\ninfo->geometries[0].level_bits[0] = 9;\r\nfor (i = 1; i < 4; ++i)\r\ninfo->geometries[0].level_bits[i] = p9_supported_radix_bits[i];\r\ninfo->geometries[1].page_shift = 16;\r\nfor (i = 0; i < 4; ++i)\r\ninfo->geometries[1].level_bits[i] = p9_supported_radix_bits[i];\r\ni = 0;\r\nadd_rmmu_ap_encoding(info, MMU_PAGE_4K, &i);\r\nadd_rmmu_ap_encoding(info, MMU_PAGE_64K, &i);\r\nadd_rmmu_ap_encoding(info, MMU_PAGE_2M, &i);\r\nadd_rmmu_ap_encoding(info, MMU_PAGE_1G, &i);\r\nreturn 0;\r\n}\r\nint kvmppc_init_vm_radix(struct kvm *kvm)\r\n{\r\nkvm->arch.pgtable = pgd_alloc(kvm->mm);\r\nif (!kvm->arch.pgtable)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid kvmppc_free_radix(struct kvm *kvm)\r\n{\r\nunsigned long ig, iu, im;\r\npte_t *pte;\r\npmd_t *pmd;\r\npud_t *pud;\r\npgd_t *pgd;\r\nif (!kvm->arch.pgtable)\r\nreturn;\r\npgd = kvm->arch.pgtable;\r\nfor (ig = 0; ig < PTRS_PER_PGD; ++ig, ++pgd) {\r\nif (!pgd_present(*pgd))\r\ncontinue;\r\npud = pud_offset(pgd, 0);\r\nfor (iu = 0; iu < PTRS_PER_PUD; ++iu, ++pud) {\r\nif (!pud_present(*pud))\r\ncontinue;\r\npmd = pmd_offset(pud, 0);\r\nfor (im = 0; im < PTRS_PER_PMD; ++im, ++pmd) {\r\nif (pmd_huge(*pmd)) {\r\npmd_clear(pmd);\r\ncontinue;\r\n}\r\nif (!pmd_present(*pmd))\r\ncontinue;\r\npte = pte_offset_map(pmd, 0);\r\nmemset(pte, 0, sizeof(long) << PTE_INDEX_SIZE);\r\nkvmppc_pte_free(pte);\r\npmd_clear(pmd);\r\n}\r\npmd_free(kvm->mm, pmd_offset(pud, 0));\r\npud_clear(pud);\r\n}\r\npud_free(kvm->mm, pud_offset(pgd, 0));\r\npgd_clear(pgd);\r\n}\r\npgd_free(kvm->mm, kvm->arch.pgtable);\r\n}\r\nstatic void pte_ctor(void *addr)\r\n{\r\nmemset(addr, 0, PTE_TABLE_SIZE);\r\n}\r\nint kvmppc_radix_init(void)\r\n{\r\nunsigned long size = sizeof(void *) << PTE_INDEX_SIZE;\r\nkvm_pte_cache = kmem_cache_create("kvm-pte", size, size, 0, pte_ctor);\r\nif (!kvm_pte_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid kvmppc_radix_exit(void)\r\n{\r\nkmem_cache_destroy(kvm_pte_cache);\r\n}
