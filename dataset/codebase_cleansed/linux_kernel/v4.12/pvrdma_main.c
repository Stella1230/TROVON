static ssize_t show_hca(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "VMW_PVRDMA-%s\n", DRV_VERSION);\r\n}\r\nstatic ssize_t show_rev(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%d\n", PVRDMA_REV_ID);\r\n}\r\nstatic ssize_t show_board(struct device *device, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nreturn sprintf(buf, "%d\n", PVRDMA_BOARD_ID);\r\n}\r\nstatic void pvrdma_get_fw_ver_str(struct ib_device *device, char *str,\r\nsize_t str_len)\r\n{\r\nstruct pvrdma_dev *dev =\r\ncontainer_of(device, struct pvrdma_dev, ib_dev);\r\nsnprintf(str, str_len, "%d.%d.%d\n",\r\n(int) (dev->dsr->caps.fw_ver >> 32),\r\n(int) (dev->dsr->caps.fw_ver >> 16) & 0xffff,\r\n(int) dev->dsr->caps.fw_ver & 0xffff);\r\n}\r\nstatic int pvrdma_init_device(struct pvrdma_dev *dev)\r\n{\r\nspin_lock_init(&dev->cmd_lock);\r\nsema_init(&dev->cmd_sema, 1);\r\natomic_set(&dev->num_qps, 0);\r\natomic_set(&dev->num_cqs, 0);\r\natomic_set(&dev->num_pds, 0);\r\natomic_set(&dev->num_ahs, 0);\r\nreturn 0;\r\n}\r\nstatic int pvrdma_port_immutable(struct ib_device *ibdev, u8 port_num,\r\nstruct ib_port_immutable *immutable)\r\n{\r\nstruct ib_port_attr attr;\r\nint err;\r\nimmutable->core_cap_flags = RDMA_CORE_PORT_IBA_ROCE;\r\nerr = ib_query_port(ibdev, port_num, &attr);\r\nif (err)\r\nreturn err;\r\nimmutable->pkey_tbl_len = attr.pkey_tbl_len;\r\nimmutable->gid_tbl_len = attr.gid_tbl_len;\r\nimmutable->max_mad_size = IB_MGMT_MAD_SIZE;\r\nreturn 0;\r\n}\r\nstatic struct net_device *pvrdma_get_netdev(struct ib_device *ibdev,\r\nu8 port_num)\r\n{\r\nstruct net_device *netdev;\r\nstruct pvrdma_dev *dev = to_vdev(ibdev);\r\nif (port_num != 1)\r\nreturn NULL;\r\nrcu_read_lock();\r\nnetdev = dev->netdev;\r\nif (netdev)\r\ndev_hold(netdev);\r\nrcu_read_unlock();\r\nreturn netdev;\r\n}\r\nstatic int pvrdma_register_device(struct pvrdma_dev *dev)\r\n{\r\nint ret = -1;\r\nint i = 0;\r\nstrlcpy(dev->ib_dev.name, "vmw_pvrdma%d", IB_DEVICE_NAME_MAX);\r\ndev->ib_dev.node_guid = dev->dsr->caps.node_guid;\r\ndev->sys_image_guid = dev->dsr->caps.sys_image_guid;\r\ndev->flags = 0;\r\ndev->ib_dev.owner = THIS_MODULE;\r\ndev->ib_dev.num_comp_vectors = 1;\r\ndev->ib_dev.dev.parent = &dev->pdev->dev;\r\ndev->ib_dev.uverbs_abi_ver = PVRDMA_UVERBS_ABI_VERSION;\r\ndev->ib_dev.uverbs_cmd_mask =\r\n(1ull << IB_USER_VERBS_CMD_GET_CONTEXT) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_DEVICE) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_PORT) |\r\n(1ull << IB_USER_VERBS_CMD_ALLOC_PD) |\r\n(1ull << IB_USER_VERBS_CMD_DEALLOC_PD) |\r\n(1ull << IB_USER_VERBS_CMD_REG_MR) |\r\n(1ull << IB_USER_VERBS_CMD_DEREG_MR) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_POLL_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_REQ_NOTIFY_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_QP) |\r\n(1ull << IB_USER_VERBS_CMD_MODIFY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_POST_SEND) |\r\n(1ull << IB_USER_VERBS_CMD_POST_RECV) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_AH) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_AH);\r\ndev->ib_dev.node_type = RDMA_NODE_IB_CA;\r\ndev->ib_dev.phys_port_cnt = dev->dsr->caps.phys_port_cnt;\r\ndev->ib_dev.query_device = pvrdma_query_device;\r\ndev->ib_dev.query_port = pvrdma_query_port;\r\ndev->ib_dev.query_gid = pvrdma_query_gid;\r\ndev->ib_dev.query_pkey = pvrdma_query_pkey;\r\ndev->ib_dev.modify_port = pvrdma_modify_port;\r\ndev->ib_dev.alloc_ucontext = pvrdma_alloc_ucontext;\r\ndev->ib_dev.dealloc_ucontext = pvrdma_dealloc_ucontext;\r\ndev->ib_dev.mmap = pvrdma_mmap;\r\ndev->ib_dev.alloc_pd = pvrdma_alloc_pd;\r\ndev->ib_dev.dealloc_pd = pvrdma_dealloc_pd;\r\ndev->ib_dev.create_ah = pvrdma_create_ah;\r\ndev->ib_dev.destroy_ah = pvrdma_destroy_ah;\r\ndev->ib_dev.create_qp = pvrdma_create_qp;\r\ndev->ib_dev.modify_qp = pvrdma_modify_qp;\r\ndev->ib_dev.query_qp = pvrdma_query_qp;\r\ndev->ib_dev.destroy_qp = pvrdma_destroy_qp;\r\ndev->ib_dev.post_send = pvrdma_post_send;\r\ndev->ib_dev.post_recv = pvrdma_post_recv;\r\ndev->ib_dev.create_cq = pvrdma_create_cq;\r\ndev->ib_dev.modify_cq = pvrdma_modify_cq;\r\ndev->ib_dev.resize_cq = pvrdma_resize_cq;\r\ndev->ib_dev.destroy_cq = pvrdma_destroy_cq;\r\ndev->ib_dev.poll_cq = pvrdma_poll_cq;\r\ndev->ib_dev.req_notify_cq = pvrdma_req_notify_cq;\r\ndev->ib_dev.get_dma_mr = pvrdma_get_dma_mr;\r\ndev->ib_dev.reg_user_mr = pvrdma_reg_user_mr;\r\ndev->ib_dev.dereg_mr = pvrdma_dereg_mr;\r\ndev->ib_dev.alloc_mr = pvrdma_alloc_mr;\r\ndev->ib_dev.map_mr_sg = pvrdma_map_mr_sg;\r\ndev->ib_dev.add_gid = pvrdma_add_gid;\r\ndev->ib_dev.del_gid = pvrdma_del_gid;\r\ndev->ib_dev.get_netdev = pvrdma_get_netdev;\r\ndev->ib_dev.get_port_immutable = pvrdma_port_immutable;\r\ndev->ib_dev.get_link_layer = pvrdma_port_link_layer;\r\ndev->ib_dev.get_dev_fw_str = pvrdma_get_fw_ver_str;\r\nmutex_init(&dev->port_mutex);\r\nspin_lock_init(&dev->desc_lock);\r\ndev->cq_tbl = kcalloc(dev->dsr->caps.max_cq, sizeof(void *),\r\nGFP_KERNEL);\r\nif (!dev->cq_tbl)\r\nreturn ret;\r\nspin_lock_init(&dev->cq_tbl_lock);\r\ndev->qp_tbl = kcalloc(dev->dsr->caps.max_qp, sizeof(void *),\r\nGFP_KERNEL);\r\nif (!dev->qp_tbl)\r\ngoto err_cq_free;\r\nspin_lock_init(&dev->qp_tbl_lock);\r\nret = ib_register_device(&dev->ib_dev, NULL);\r\nif (ret)\r\ngoto err_qp_free;\r\nfor (i = 0; i < ARRAY_SIZE(pvrdma_class_attributes); ++i) {\r\nret = device_create_file(&dev->ib_dev.dev,\r\npvrdma_class_attributes[i]);\r\nif (ret)\r\ngoto err_class;\r\n}\r\ndev->ib_active = true;\r\nreturn 0;\r\nerr_class:\r\nib_unregister_device(&dev->ib_dev);\r\nerr_qp_free:\r\nkfree(dev->qp_tbl);\r\nerr_cq_free:\r\nkfree(dev->cq_tbl);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t pvrdma_intr0_handler(int irq, void *dev_id)\r\n{\r\nu32 icr = PVRDMA_INTR_CAUSE_RESPONSE;\r\nstruct pvrdma_dev *dev = dev_id;\r\ndev_dbg(&dev->pdev->dev, "interrupt 0 (response) handler\n");\r\nif (!dev->pdev->msix_enabled) {\r\nicr = pvrdma_read_reg(dev, PVRDMA_REG_ICR);\r\nif (icr == 0)\r\nreturn IRQ_NONE;\r\n}\r\nif (icr == PVRDMA_INTR_CAUSE_RESPONSE)\r\ncomplete(&dev->cmd_done);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void pvrdma_qp_event(struct pvrdma_dev *dev, u32 qpn, int type)\r\n{\r\nstruct pvrdma_qp *qp;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->qp_tbl_lock, flags);\r\nqp = dev->qp_tbl[qpn % dev->dsr->caps.max_qp];\r\nif (qp)\r\natomic_inc(&qp->refcnt);\r\nspin_unlock_irqrestore(&dev->qp_tbl_lock, flags);\r\nif (qp && qp->ibqp.event_handler) {\r\nstruct ib_qp *ibqp = &qp->ibqp;\r\nstruct ib_event e;\r\ne.device = ibqp->device;\r\ne.element.qp = ibqp;\r\ne.event = type;\r\nibqp->event_handler(&e, ibqp->qp_context);\r\n}\r\nif (qp) {\r\natomic_dec(&qp->refcnt);\r\nif (atomic_read(&qp->refcnt) == 0)\r\nwake_up(&qp->wait);\r\n}\r\n}\r\nstatic void pvrdma_cq_event(struct pvrdma_dev *dev, u32 cqn, int type)\r\n{\r\nstruct pvrdma_cq *cq;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->cq_tbl_lock, flags);\r\ncq = dev->cq_tbl[cqn % dev->dsr->caps.max_cq];\r\nif (cq)\r\natomic_inc(&cq->refcnt);\r\nspin_unlock_irqrestore(&dev->cq_tbl_lock, flags);\r\nif (cq && cq->ibcq.event_handler) {\r\nstruct ib_cq *ibcq = &cq->ibcq;\r\nstruct ib_event e;\r\ne.device = ibcq->device;\r\ne.element.cq = ibcq;\r\ne.event = type;\r\nibcq->event_handler(&e, ibcq->cq_context);\r\n}\r\nif (cq) {\r\natomic_dec(&cq->refcnt);\r\nif (atomic_read(&cq->refcnt) == 0)\r\nwake_up(&cq->wait);\r\n}\r\n}\r\nstatic void pvrdma_dispatch_event(struct pvrdma_dev *dev, int port,\r\nenum ib_event_type event)\r\n{\r\nstruct ib_event ib_event;\r\nmemset(&ib_event, 0, sizeof(ib_event));\r\nib_event.device = &dev->ib_dev;\r\nib_event.element.port_num = port;\r\nib_event.event = event;\r\nib_dispatch_event(&ib_event);\r\n}\r\nstatic void pvrdma_dev_event(struct pvrdma_dev *dev, u8 port, int type)\r\n{\r\nif (port < 1 || port > dev->dsr->caps.phys_port_cnt) {\r\ndev_warn(&dev->pdev->dev, "event on port %d\n", port);\r\nreturn;\r\n}\r\npvrdma_dispatch_event(dev, port, type);\r\n}\r\nstatic inline struct pvrdma_eqe *get_eqe(struct pvrdma_dev *dev, unsigned int i)\r\n{\r\nreturn (struct pvrdma_eqe *)pvrdma_page_dir_get_ptr(\r\n&dev->async_pdir,\r\nPAGE_SIZE +\r\nsizeof(struct pvrdma_eqe) * i);\r\n}\r\nstatic irqreturn_t pvrdma_intr1_handler(int irq, void *dev_id)\r\n{\r\nstruct pvrdma_dev *dev = dev_id;\r\nstruct pvrdma_ring *ring = &dev->async_ring_state->rx;\r\nint ring_slots = (dev->dsr->async_ring_pages.num_pages - 1) *\r\nPAGE_SIZE / sizeof(struct pvrdma_eqe);\r\nunsigned int head;\r\ndev_dbg(&dev->pdev->dev, "interrupt 1 (async event) handler\n");\r\nif (!dev->ib_active)\r\nreturn IRQ_HANDLED;\r\nwhile (pvrdma_idx_ring_has_data(ring, ring_slots, &head) > 0) {\r\nstruct pvrdma_eqe *eqe;\r\neqe = get_eqe(dev, head);\r\nswitch (eqe->type) {\r\ncase PVRDMA_EVENT_QP_FATAL:\r\ncase PVRDMA_EVENT_QP_REQ_ERR:\r\ncase PVRDMA_EVENT_QP_ACCESS_ERR:\r\ncase PVRDMA_EVENT_COMM_EST:\r\ncase PVRDMA_EVENT_SQ_DRAINED:\r\ncase PVRDMA_EVENT_PATH_MIG:\r\ncase PVRDMA_EVENT_PATH_MIG_ERR:\r\ncase PVRDMA_EVENT_QP_LAST_WQE_REACHED:\r\npvrdma_qp_event(dev, eqe->info, eqe->type);\r\nbreak;\r\ncase PVRDMA_EVENT_CQ_ERR:\r\npvrdma_cq_event(dev, eqe->info, eqe->type);\r\nbreak;\r\ncase PVRDMA_EVENT_SRQ_ERR:\r\ncase PVRDMA_EVENT_SRQ_LIMIT_REACHED:\r\nbreak;\r\ncase PVRDMA_EVENT_PORT_ACTIVE:\r\ncase PVRDMA_EVENT_PORT_ERR:\r\ncase PVRDMA_EVENT_LID_CHANGE:\r\ncase PVRDMA_EVENT_PKEY_CHANGE:\r\ncase PVRDMA_EVENT_SM_CHANGE:\r\ncase PVRDMA_EVENT_CLIENT_REREGISTER:\r\ncase PVRDMA_EVENT_GID_CHANGE:\r\npvrdma_dev_event(dev, eqe->info, eqe->type);\r\nbreak;\r\ncase PVRDMA_EVENT_DEVICE_FATAL:\r\npvrdma_dev_event(dev, 1, eqe->type);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\npvrdma_idx_ring_inc(&ring->cons_head, ring_slots);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline struct pvrdma_cqne *get_cqne(struct pvrdma_dev *dev,\r\nunsigned int i)\r\n{\r\nreturn (struct pvrdma_cqne *)pvrdma_page_dir_get_ptr(\r\n&dev->cq_pdir,\r\nPAGE_SIZE +\r\nsizeof(struct pvrdma_cqne) * i);\r\n}\r\nstatic irqreturn_t pvrdma_intrx_handler(int irq, void *dev_id)\r\n{\r\nstruct pvrdma_dev *dev = dev_id;\r\nstruct pvrdma_ring *ring = &dev->cq_ring_state->rx;\r\nint ring_slots = (dev->dsr->cq_ring_pages.num_pages - 1) * PAGE_SIZE /\r\nsizeof(struct pvrdma_cqne);\r\nunsigned int head;\r\nunsigned long flags;\r\ndev_dbg(&dev->pdev->dev, "interrupt x (completion) handler\n");\r\nwhile (pvrdma_idx_ring_has_data(ring, ring_slots, &head) > 0) {\r\nstruct pvrdma_cqne *cqne;\r\nstruct pvrdma_cq *cq;\r\ncqne = get_cqne(dev, head);\r\nspin_lock_irqsave(&dev->cq_tbl_lock, flags);\r\ncq = dev->cq_tbl[cqne->info % dev->dsr->caps.max_cq];\r\nif (cq)\r\natomic_inc(&cq->refcnt);\r\nspin_unlock_irqrestore(&dev->cq_tbl_lock, flags);\r\nif (cq && cq->ibcq.comp_handler)\r\ncq->ibcq.comp_handler(&cq->ibcq, cq->ibcq.cq_context);\r\nif (cq) {\r\natomic_dec(&cq->refcnt);\r\nif (atomic_read(&cq->refcnt))\r\nwake_up(&cq->wait);\r\n}\r\npvrdma_idx_ring_inc(&ring->cons_head, ring_slots);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void pvrdma_free_irq(struct pvrdma_dev *dev)\r\n{\r\nint i;\r\ndev_dbg(&dev->pdev->dev, "freeing interrupts\n");\r\nfor (i = 0; i < dev->nr_vectors; i++)\r\nfree_irq(pci_irq_vector(dev->pdev, i), dev);\r\n}\r\nstatic void pvrdma_enable_intrs(struct pvrdma_dev *dev)\r\n{\r\ndev_dbg(&dev->pdev->dev, "enable interrupts\n");\r\npvrdma_write_reg(dev, PVRDMA_REG_IMR, 0);\r\n}\r\nstatic void pvrdma_disable_intrs(struct pvrdma_dev *dev)\r\n{\r\ndev_dbg(&dev->pdev->dev, "disable interrupts\n");\r\npvrdma_write_reg(dev, PVRDMA_REG_IMR, ~0);\r\n}\r\nstatic int pvrdma_alloc_intrs(struct pvrdma_dev *dev)\r\n{\r\nstruct pci_dev *pdev = dev->pdev;\r\nint ret = 0, i;\r\nret = pci_alloc_irq_vectors(pdev, 1, PVRDMA_MAX_INTERRUPTS,\r\nPCI_IRQ_MSIX);\r\nif (ret < 0) {\r\nret = pci_alloc_irq_vectors(pdev, 1, 1,\r\nPCI_IRQ_MSI | PCI_IRQ_LEGACY);\r\nif (ret < 0)\r\nreturn ret;\r\n}\r\ndev->nr_vectors = ret;\r\nret = request_irq(pci_irq_vector(dev->pdev, 0), pvrdma_intr0_handler,\r\npdev->msix_enabled ? 0 : IRQF_SHARED, DRV_NAME, dev);\r\nif (ret) {\r\ndev_err(&dev->pdev->dev,\r\n"failed to request interrupt 0\n");\r\ngoto out_free_vectors;\r\n}\r\nfor (i = 1; i < dev->nr_vectors; i++) {\r\nret = request_irq(pci_irq_vector(dev->pdev, i),\r\ni == 1 ? pvrdma_intr1_handler :\r\npvrdma_intrx_handler,\r\n0, DRV_NAME, dev);\r\nif (ret) {\r\ndev_err(&dev->pdev->dev,\r\n"failed to request interrupt %d\n", i);\r\ngoto free_irqs;\r\n}\r\n}\r\nreturn 0;\r\nfree_irqs:\r\nwhile (--i >= 0)\r\nfree_irq(pci_irq_vector(dev->pdev, i), dev);\r\nout_free_vectors:\r\npci_free_irq_vectors(pdev);\r\nreturn ret;\r\n}\r\nstatic void pvrdma_free_slots(struct pvrdma_dev *dev)\r\n{\r\nstruct pci_dev *pdev = dev->pdev;\r\nif (dev->resp_slot)\r\ndma_free_coherent(&pdev->dev, PAGE_SIZE, dev->resp_slot,\r\ndev->dsr->resp_slot_dma);\r\nif (dev->cmd_slot)\r\ndma_free_coherent(&pdev->dev, PAGE_SIZE, dev->cmd_slot,\r\ndev->dsr->cmd_slot_dma);\r\n}\r\nstatic int pvrdma_add_gid_at_index(struct pvrdma_dev *dev,\r\nconst union ib_gid *gid,\r\nint index)\r\n{\r\nint ret;\r\nunion pvrdma_cmd_req req;\r\nstruct pvrdma_cmd_create_bind *cmd_bind = &req.create_bind;\r\nif (!dev->sgid_tbl) {\r\ndev_warn(&dev->pdev->dev, "sgid table not initialized\n");\r\nreturn -EINVAL;\r\n}\r\nmemset(cmd_bind, 0, sizeof(*cmd_bind));\r\ncmd_bind->hdr.cmd = PVRDMA_CMD_CREATE_BIND;\r\nmemcpy(cmd_bind->new_gid, gid->raw, 16);\r\ncmd_bind->mtu = ib_mtu_enum_to_int(IB_MTU_1024);\r\ncmd_bind->vlan = 0xfff;\r\ncmd_bind->index = index;\r\ncmd_bind->gid_type = PVRDMA_GID_TYPE_FLAG_ROCE_V1;\r\nret = pvrdma_cmd_post(dev, &req, NULL, 0);\r\nif (ret < 0) {\r\ndev_warn(&dev->pdev->dev,\r\n"could not create binding, error: %d\n", ret);\r\nreturn -EFAULT;\r\n}\r\nmemcpy(&dev->sgid_tbl[index], gid, sizeof(*gid));\r\nreturn 0;\r\n}\r\nstatic int pvrdma_add_gid(struct ib_device *ibdev,\r\nu8 port_num,\r\nunsigned int index,\r\nconst union ib_gid *gid,\r\nconst struct ib_gid_attr *attr,\r\nvoid **context)\r\n{\r\nstruct pvrdma_dev *dev = to_vdev(ibdev);\r\nreturn pvrdma_add_gid_at_index(dev, gid, index);\r\n}\r\nstatic int pvrdma_del_gid_at_index(struct pvrdma_dev *dev, int index)\r\n{\r\nint ret;\r\nunion pvrdma_cmd_req req;\r\nstruct pvrdma_cmd_destroy_bind *cmd_dest = &req.destroy_bind;\r\nif (!dev->sgid_tbl) {\r\ndev_warn(&dev->pdev->dev, "sgid table not initialized\n");\r\nreturn -EINVAL;\r\n}\r\nmemset(cmd_dest, 0, sizeof(*cmd_dest));\r\ncmd_dest->hdr.cmd = PVRDMA_CMD_DESTROY_BIND;\r\nmemcpy(cmd_dest->dest_gid, &dev->sgid_tbl[index], 16);\r\ncmd_dest->index = index;\r\nret = pvrdma_cmd_post(dev, &req, NULL, 0);\r\nif (ret < 0) {\r\ndev_warn(&dev->pdev->dev,\r\n"could not destroy binding, error: %d\n", ret);\r\nreturn ret;\r\n}\r\nmemset(&dev->sgid_tbl[index], 0, 16);\r\nreturn 0;\r\n}\r\nstatic int pvrdma_del_gid(struct ib_device *ibdev,\r\nu8 port_num,\r\nunsigned int index,\r\nvoid **context)\r\n{\r\nstruct pvrdma_dev *dev = to_vdev(ibdev);\r\ndev_dbg(&dev->pdev->dev, "removing gid at index %u from %s",\r\nindex, dev->netdev->name);\r\nreturn pvrdma_del_gid_at_index(dev, index);\r\n}\r\nstatic void pvrdma_netdevice_event_handle(struct pvrdma_dev *dev,\r\nunsigned long event)\r\n{\r\nswitch (event) {\r\ncase NETDEV_REBOOT:\r\ncase NETDEV_DOWN:\r\npvrdma_dispatch_event(dev, 1, IB_EVENT_PORT_ERR);\r\nbreak;\r\ncase NETDEV_UP:\r\npvrdma_write_reg(dev, PVRDMA_REG_CTL,\r\nPVRDMA_DEVICE_CTL_UNQUIESCE);\r\nmb();\r\nif (pvrdma_read_reg(dev, PVRDMA_REG_ERR))\r\ndev_err(&dev->pdev->dev,\r\n"failed to activate device during link up\n");\r\nelse\r\npvrdma_dispatch_event(dev, 1, IB_EVENT_PORT_ACTIVE);\r\nbreak;\r\ndefault:\r\ndev_dbg(&dev->pdev->dev, "ignore netdevice event %ld on %s\n",\r\nevent, dev->ib_dev.name);\r\nbreak;\r\n}\r\n}\r\nstatic void pvrdma_netdevice_event_work(struct work_struct *work)\r\n{\r\nstruct pvrdma_netdevice_work *netdev_work;\r\nstruct pvrdma_dev *dev;\r\nnetdev_work = container_of(work, struct pvrdma_netdevice_work, work);\r\nmutex_lock(&pvrdma_device_list_lock);\r\nlist_for_each_entry(dev, &pvrdma_device_list, device_link) {\r\nif (dev->netdev == netdev_work->event_netdev) {\r\npvrdma_netdevice_event_handle(dev, netdev_work->event);\r\nbreak;\r\n}\r\n}\r\nmutex_unlock(&pvrdma_device_list_lock);\r\nkfree(netdev_work);\r\n}\r\nstatic int pvrdma_netdevice_event(struct notifier_block *this,\r\nunsigned long event, void *ptr)\r\n{\r\nstruct net_device *event_netdev = netdev_notifier_info_to_dev(ptr);\r\nstruct pvrdma_netdevice_work *netdev_work;\r\nnetdev_work = kmalloc(sizeof(*netdev_work), GFP_ATOMIC);\r\nif (!netdev_work)\r\nreturn NOTIFY_BAD;\r\nINIT_WORK(&netdev_work->work, pvrdma_netdevice_event_work);\r\nnetdev_work->event_netdev = event_netdev;\r\nnetdev_work->event = event;\r\nqueue_work(event_wq, &netdev_work->work);\r\nreturn NOTIFY_DONE;\r\n}\r\nstatic int pvrdma_pci_probe(struct pci_dev *pdev,\r\nconst struct pci_device_id *id)\r\n{\r\nstruct pci_dev *pdev_net;\r\nstruct pvrdma_dev *dev;\r\nint ret;\r\nunsigned long start;\r\nunsigned long len;\r\nunsigned int version;\r\ndma_addr_t slot_dma = 0;\r\ndev_dbg(&pdev->dev, "initializing driver %s\n", pci_name(pdev));\r\ndev = (struct pvrdma_dev *)ib_alloc_device(sizeof(*dev));\r\nif (!dev) {\r\ndev_err(&pdev->dev, "failed to allocate IB device\n");\r\nreturn -ENOMEM;\r\n}\r\nmutex_lock(&pvrdma_device_list_lock);\r\nlist_add(&dev->device_link, &pvrdma_device_list);\r\nmutex_unlock(&pvrdma_device_list_lock);\r\nret = pvrdma_init_device(dev);\r\nif (ret)\r\ngoto err_free_device;\r\ndev->pdev = pdev;\r\npci_set_drvdata(pdev, dev);\r\nret = pci_enable_device(pdev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "cannot enable PCI device\n");\r\ngoto err_free_device;\r\n}\r\ndev_dbg(&pdev->dev, "PCI resource flags BAR0 %#lx\n",\r\npci_resource_flags(pdev, 0));\r\ndev_dbg(&pdev->dev, "PCI resource len %#llx\n",\r\n(unsigned long long)pci_resource_len(pdev, 0));\r\ndev_dbg(&pdev->dev, "PCI resource start %#llx\n",\r\n(unsigned long long)pci_resource_start(pdev, 0));\r\ndev_dbg(&pdev->dev, "PCI resource flags BAR1 %#lx\n",\r\npci_resource_flags(pdev, 1));\r\ndev_dbg(&pdev->dev, "PCI resource len %#llx\n",\r\n(unsigned long long)pci_resource_len(pdev, 1));\r\ndev_dbg(&pdev->dev, "PCI resource start %#llx\n",\r\n(unsigned long long)pci_resource_start(pdev, 1));\r\nif (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM) ||\r\n!(pci_resource_flags(pdev, 1) & IORESOURCE_MEM)) {\r\ndev_err(&pdev->dev, "PCI BAR region not MMIO\n");\r\nret = -ENOMEM;\r\ngoto err_free_device;\r\n}\r\nret = pci_request_regions(pdev, DRV_NAME);\r\nif (ret) {\r\ndev_err(&pdev->dev, "cannot request PCI resources\n");\r\ngoto err_disable_pdev;\r\n}\r\nif (pci_set_dma_mask(pdev, DMA_BIT_MASK(64)) == 0) {\r\nret = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));\r\nif (ret != 0) {\r\ndev_err(&pdev->dev,\r\n"pci_set_consistent_dma_mask failed\n");\r\ngoto err_free_resource;\r\n}\r\n} else {\r\nret = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));\r\nif (ret != 0) {\r\ndev_err(&pdev->dev,\r\n"pci_set_dma_mask failed\n");\r\ngoto err_free_resource;\r\n}\r\n}\r\npci_set_master(pdev);\r\nstart = pci_resource_start(dev->pdev, PVRDMA_PCI_RESOURCE_REG);\r\nlen = pci_resource_len(dev->pdev, PVRDMA_PCI_RESOURCE_REG);\r\ndev->regs = ioremap(start, len);\r\nif (!dev->regs) {\r\ndev_err(&pdev->dev, "register mapping failed\n");\r\nret = -ENOMEM;\r\ngoto err_free_resource;\r\n}\r\ndev->driver_uar.index = 0;\r\ndev->driver_uar.pfn =\r\npci_resource_start(dev->pdev, PVRDMA_PCI_RESOURCE_UAR) >>\r\nPAGE_SHIFT;\r\ndev->driver_uar.map =\r\nioremap(dev->driver_uar.pfn << PAGE_SHIFT, PAGE_SIZE);\r\nif (!dev->driver_uar.map) {\r\ndev_err(&pdev->dev, "failed to remap UAR pages\n");\r\nret = -ENOMEM;\r\ngoto err_unmap_regs;\r\n}\r\nversion = pvrdma_read_reg(dev, PVRDMA_REG_VERSION);\r\ndev_info(&pdev->dev, "device version %d, driver version %d\n",\r\nversion, PVRDMA_VERSION);\r\nif (version < PVRDMA_VERSION) {\r\ndev_err(&pdev->dev, "incompatible device version\n");\r\ngoto err_uar_unmap;\r\n}\r\ndev->dsr = dma_alloc_coherent(&pdev->dev, sizeof(*dev->dsr),\r\n&dev->dsrbase, GFP_KERNEL);\r\nif (!dev->dsr) {\r\ndev_err(&pdev->dev, "failed to allocate shared region\n");\r\nret = -ENOMEM;\r\ngoto err_uar_unmap;\r\n}\r\nmemset(dev->dsr, 0, sizeof(*dev->dsr));\r\ndev->dsr->driver_version = PVRDMA_VERSION;\r\ndev->dsr->gos_info.gos_bits = sizeof(void *) == 4 ?\r\nPVRDMA_GOS_BITS_32 :\r\nPVRDMA_GOS_BITS_64;\r\ndev->dsr->gos_info.gos_type = PVRDMA_GOS_TYPE_LINUX;\r\ndev->dsr->gos_info.gos_ver = 1;\r\ndev->dsr->uar_pfn = dev->driver_uar.pfn;\r\ndev->cmd_slot = dma_alloc_coherent(&pdev->dev, PAGE_SIZE,\r\n&slot_dma, GFP_KERNEL);\r\nif (!dev->cmd_slot) {\r\nret = -ENOMEM;\r\ngoto err_free_dsr;\r\n}\r\ndev->dsr->cmd_slot_dma = (u64)slot_dma;\r\ndev->resp_slot = dma_alloc_coherent(&pdev->dev, PAGE_SIZE,\r\n&slot_dma, GFP_KERNEL);\r\nif (!dev->resp_slot) {\r\nret = -ENOMEM;\r\ngoto err_free_slots;\r\n}\r\ndev->dsr->resp_slot_dma = (u64)slot_dma;\r\ndev->dsr->async_ring_pages.num_pages = PVRDMA_NUM_RING_PAGES;\r\nret = pvrdma_page_dir_init(dev, &dev->async_pdir,\r\ndev->dsr->async_ring_pages.num_pages, true);\r\nif (ret)\r\ngoto err_free_slots;\r\ndev->async_ring_state = dev->async_pdir.pages[0];\r\ndev->dsr->async_ring_pages.pdir_dma = dev->async_pdir.dir_dma;\r\ndev->dsr->cq_ring_pages.num_pages = PVRDMA_NUM_RING_PAGES;\r\nret = pvrdma_page_dir_init(dev, &dev->cq_pdir,\r\ndev->dsr->cq_ring_pages.num_pages, true);\r\nif (ret)\r\ngoto err_free_async_ring;\r\ndev->cq_ring_state = dev->cq_pdir.pages[0];\r\ndev->dsr->cq_ring_pages.pdir_dma = dev->cq_pdir.dir_dma;\r\npvrdma_write_reg(dev, PVRDMA_REG_DSRLOW, (u32)dev->dsrbase);\r\npvrdma_write_reg(dev, PVRDMA_REG_DSRHIGH,\r\n(u32)((u64)(dev->dsrbase) >> 32));\r\nmb();\r\nif (dev->dsr->caps.mode != PVRDMA_DEVICE_MODE_ROCE) {\r\ndev_err(&pdev->dev, "unsupported transport %d\n",\r\ndev->dsr->caps.mode);\r\nret = -EFAULT;\r\ngoto err_free_cq_ring;\r\n}\r\nif (!(dev->dsr->caps.gid_types & PVRDMA_GID_TYPE_FLAG_ROCE_V1)) {\r\ndev_err(&pdev->dev, "driver needs RoCE v1 support\n");\r\nret = -EFAULT;\r\ngoto err_free_cq_ring;\r\n}\r\npdev_net = pci_get_slot(pdev->bus, PCI_DEVFN(PCI_SLOT(pdev->devfn), 0));\r\nif (!pdev_net) {\r\ndev_err(&pdev->dev, "failed to find paired net device\n");\r\nret = -ENODEV;\r\ngoto err_free_cq_ring;\r\n}\r\nif (pdev_net->vendor != PCI_VENDOR_ID_VMWARE ||\r\npdev_net->device != PCI_DEVICE_ID_VMWARE_VMXNET3) {\r\ndev_err(&pdev->dev, "failed to find paired vmxnet3 device\n");\r\npci_dev_put(pdev_net);\r\nret = -ENODEV;\r\ngoto err_free_cq_ring;\r\n}\r\ndev->netdev = pci_get_drvdata(pdev_net);\r\npci_dev_put(pdev_net);\r\nif (!dev->netdev) {\r\ndev_err(&pdev->dev, "failed to get vmxnet3 device\n");\r\nret = -ENODEV;\r\ngoto err_free_cq_ring;\r\n}\r\ndev_info(&pdev->dev, "paired device to %s\n", dev->netdev->name);\r\nret = pvrdma_alloc_intrs(dev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "failed to allocate interrupts\n");\r\nret = -ENOMEM;\r\ngoto err_free_cq_ring;\r\n}\r\nret = pvrdma_uar_table_init(dev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "failed to allocate UAR table\n");\r\nret = -ENOMEM;\r\ngoto err_free_intrs;\r\n}\r\ndev->sgid_tbl = kcalloc(dev->dsr->caps.gid_tbl_len,\r\nsizeof(union ib_gid), GFP_KERNEL);\r\nif (!dev->sgid_tbl) {\r\nret = -ENOMEM;\r\ngoto err_free_uar_table;\r\n}\r\ndev_dbg(&pdev->dev, "gid table len %d\n", dev->dsr->caps.gid_tbl_len);\r\npvrdma_enable_intrs(dev);\r\npvrdma_write_reg(dev, PVRDMA_REG_CTL, PVRDMA_DEVICE_CTL_ACTIVATE);\r\nmb();\r\nret = pvrdma_read_reg(dev, PVRDMA_REG_ERR);\r\nif (ret != 0) {\r\ndev_err(&pdev->dev, "failed to activate device\n");\r\nret = -EFAULT;\r\ngoto err_disable_intr;\r\n}\r\nret = pvrdma_register_device(dev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "failed to register IB device\n");\r\ngoto err_disable_intr;\r\n}\r\ndev->nb_netdev.notifier_call = pvrdma_netdevice_event;\r\nret = register_netdevice_notifier(&dev->nb_netdev);\r\nif (ret) {\r\ndev_err(&pdev->dev, "failed to register netdevice events\n");\r\ngoto err_unreg_ibdev;\r\n}\r\ndev_info(&pdev->dev, "attached to device\n");\r\nreturn 0;\r\nerr_unreg_ibdev:\r\nib_unregister_device(&dev->ib_dev);\r\nerr_disable_intr:\r\npvrdma_disable_intrs(dev);\r\nkfree(dev->sgid_tbl);\r\nerr_free_uar_table:\r\npvrdma_uar_table_cleanup(dev);\r\nerr_free_intrs:\r\npvrdma_free_irq(dev);\r\npci_free_irq_vectors(pdev);\r\nerr_free_cq_ring:\r\npvrdma_page_dir_cleanup(dev, &dev->cq_pdir);\r\nerr_free_async_ring:\r\npvrdma_page_dir_cleanup(dev, &dev->async_pdir);\r\nerr_free_slots:\r\npvrdma_free_slots(dev);\r\nerr_free_dsr:\r\ndma_free_coherent(&pdev->dev, sizeof(*dev->dsr), dev->dsr,\r\ndev->dsrbase);\r\nerr_uar_unmap:\r\niounmap(dev->driver_uar.map);\r\nerr_unmap_regs:\r\niounmap(dev->regs);\r\nerr_free_resource:\r\npci_release_regions(pdev);\r\nerr_disable_pdev:\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\nerr_free_device:\r\nmutex_lock(&pvrdma_device_list_lock);\r\nlist_del(&dev->device_link);\r\nmutex_unlock(&pvrdma_device_list_lock);\r\nib_dealloc_device(&dev->ib_dev);\r\nreturn ret;\r\n}\r\nstatic void pvrdma_pci_remove(struct pci_dev *pdev)\r\n{\r\nstruct pvrdma_dev *dev = pci_get_drvdata(pdev);\r\nif (!dev)\r\nreturn;\r\ndev_info(&pdev->dev, "detaching from device\n");\r\nunregister_netdevice_notifier(&dev->nb_netdev);\r\ndev->nb_netdev.notifier_call = NULL;\r\nflush_workqueue(event_wq);\r\nib_unregister_device(&dev->ib_dev);\r\nmutex_lock(&pvrdma_device_list_lock);\r\nlist_del(&dev->device_link);\r\nmutex_unlock(&pvrdma_device_list_lock);\r\npvrdma_disable_intrs(dev);\r\npvrdma_free_irq(dev);\r\npci_free_irq_vectors(pdev);\r\npvrdma_write_reg(dev, PVRDMA_REG_CTL, PVRDMA_DEVICE_CTL_RESET);\r\npvrdma_page_dir_cleanup(dev, &dev->cq_pdir);\r\npvrdma_page_dir_cleanup(dev, &dev->async_pdir);\r\npvrdma_free_slots(dev);\r\niounmap(dev->regs);\r\nkfree(dev->sgid_tbl);\r\nkfree(dev->cq_tbl);\r\nkfree(dev->qp_tbl);\r\npvrdma_uar_table_cleanup(dev);\r\niounmap(dev->driver_uar.map);\r\nib_dealloc_device(&dev->ib_dev);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\n}\r\nstatic int __init pvrdma_init(void)\r\n{\r\nint err;\r\nevent_wq = alloc_ordered_workqueue("pvrdma_event_wq", WQ_MEM_RECLAIM);\r\nif (!event_wq)\r\nreturn -ENOMEM;\r\nerr = pci_register_driver(&pvrdma_driver);\r\nif (err)\r\ndestroy_workqueue(event_wq);\r\nreturn err;\r\n}\r\nstatic void __exit pvrdma_cleanup(void)\r\n{\r\npci_unregister_driver(&pvrdma_driver);\r\ndestroy_workqueue(event_wq);\r\n}
