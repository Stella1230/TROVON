static void cpt_disable_cores(struct cpt_device *cpt, u64 coremask,\r\nu8 type, u8 grp)\r\n{\r\nu64 pf_exe_ctl;\r\nu32 timeout = 100;\r\nu64 grpmask = 0;\r\nstruct device *dev = &cpt->pdev->dev;\r\nif (type == AE_TYPES)\r\ncoremask = (coremask << cpt->max_se_cores);\r\ngrpmask = cpt_read_csr64(cpt->reg_base, CPTX_PF_GX_EN(0, grp));\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_GX_EN(0, grp),\r\n(grpmask & ~coremask));\r\nudelay(CSR_DELAY);\r\ngrp = cpt_read_csr64(cpt->reg_base, CPTX_PF_EXEC_BUSY(0));\r\nwhile (grp & coremask) {\r\ndev_err(dev, "Cores still busy %llx", coremask);\r\ngrp = cpt_read_csr64(cpt->reg_base,\r\nCPTX_PF_EXEC_BUSY(0));\r\nif (timeout--)\r\nbreak;\r\nudelay(CSR_DELAY);\r\n}\r\npf_exe_ctl = cpt_read_csr64(cpt->reg_base, CPTX_PF_EXE_CTL(0));\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_EXE_CTL(0),\r\n(pf_exe_ctl & ~coremask));\r\nudelay(CSR_DELAY);\r\n}\r\nstatic void cpt_enable_cores(struct cpt_device *cpt, u64 coremask,\r\nu8 type)\r\n{\r\nu64 pf_exe_ctl;\r\nif (type == AE_TYPES)\r\ncoremask = (coremask << cpt->max_se_cores);\r\npf_exe_ctl = cpt_read_csr64(cpt->reg_base, CPTX_PF_EXE_CTL(0));\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_EXE_CTL(0),\r\n(pf_exe_ctl | coremask));\r\nudelay(CSR_DELAY);\r\n}\r\nstatic void cpt_configure_group(struct cpt_device *cpt, u8 grp,\r\nu64 coremask, u8 type)\r\n{\r\nu64 pf_gx_en = 0;\r\nif (type == AE_TYPES)\r\ncoremask = (coremask << cpt->max_se_cores);\r\npf_gx_en = cpt_read_csr64(cpt->reg_base, CPTX_PF_GX_EN(0, grp));\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_GX_EN(0, grp),\r\n(pf_gx_en | coremask));\r\nudelay(CSR_DELAY);\r\n}\r\nstatic void cpt_disable_mbox_interrupts(struct cpt_device *cpt)\r\n{\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_MBOX_ENA_W1CX(0, 0), ~0ull);\r\n}\r\nstatic void cpt_disable_ecc_interrupts(struct cpt_device *cpt)\r\n{\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_ECC0_ENA_W1C(0), ~0ull);\r\n}\r\nstatic void cpt_disable_exec_interrupts(struct cpt_device *cpt)\r\n{\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_EXEC_ENA_W1C(0), ~0ull);\r\n}\r\nstatic void cpt_disable_all_interrupts(struct cpt_device *cpt)\r\n{\r\ncpt_disable_mbox_interrupts(cpt);\r\ncpt_disable_ecc_interrupts(cpt);\r\ncpt_disable_exec_interrupts(cpt);\r\n}\r\nstatic void cpt_enable_mbox_interrupts(struct cpt_device *cpt)\r\n{\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_MBOX_ENA_W1SX(0, 0), ~0ull);\r\n}\r\nstatic int cpt_load_microcode(struct cpt_device *cpt, struct microcode *mcode)\r\n{\r\nint ret = 0, core = 0, shift = 0;\r\nu32 total_cores = 0;\r\nstruct device *dev = &cpt->pdev->dev;\r\nif (!mcode || !mcode->code) {\r\ndev_err(dev, "Either the mcode is null or data is NULL\n");\r\nreturn -EINVAL;\r\n}\r\nif (mcode->code_size == 0) {\r\ndev_err(dev, "microcode size is 0\n");\r\nreturn -EINVAL;\r\n}\r\nif (mcode->is_ae) {\r\ncore = CPT_MAX_SE_CORES;\r\ntotal_cores = CPT_MAX_TOTAL_CORES;\r\n} else {\r\ncore = 0;\r\ntotal_cores = CPT_MAX_SE_CORES;\r\n}\r\nfor (; core < total_cores ; core++, shift++) {\r\nif (mcode->core_mask & (1 << shift)) {\r\ncpt_write_csr64(cpt->reg_base,\r\nCPTX_PF_ENGX_UCODE_BASE(0, core),\r\n(u64)mcode->phys_base);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nstatic int do_cpt_init(struct cpt_device *cpt, struct microcode *mcode)\r\n{\r\nint ret = 0;\r\nstruct device *dev = &cpt->pdev->dev;\r\ncpt->flags &= ~CPT_FLAG_DEVICE_READY;\r\ncpt_disable_all_interrupts(cpt);\r\nif (mcode->is_ae) {\r\nif (mcode->num_cores > cpt->max_ae_cores) {\r\ndev_err(dev, "Requested for more cores than available AE cores\n");\r\nret = -EINVAL;\r\ngoto cpt_init_fail;\r\n}\r\nif (cpt->next_group >= CPT_MAX_CORE_GROUPS) {\r\ndev_err(dev, "Can't load, all eight microcode groups in use");\r\nreturn -ENFILE;\r\n}\r\nmcode->group = cpt->next_group;\r\nmcode->core_mask = GENMASK(mcode->num_cores, 0);\r\ncpt_disable_cores(cpt, mcode->core_mask, AE_TYPES,\r\nmcode->group);\r\nret = cpt_load_microcode(cpt, mcode);\r\nif (ret) {\r\ndev_err(dev, "Microcode load Failed for %s\n",\r\nmcode->version);\r\ngoto cpt_init_fail;\r\n}\r\ncpt->next_group++;\r\ncpt_configure_group(cpt, mcode->group, mcode->core_mask,\r\nAE_TYPES);\r\ncpt_enable_cores(cpt, mcode->core_mask, AE_TYPES);\r\n} else {\r\nif (mcode->num_cores > cpt->max_se_cores) {\r\ndev_err(dev, "Requested for more cores than available SE cores\n");\r\nret = -EINVAL;\r\ngoto cpt_init_fail;\r\n}\r\nif (cpt->next_group >= CPT_MAX_CORE_GROUPS) {\r\ndev_err(dev, "Can't load, all eight microcode groups in use");\r\nreturn -ENFILE;\r\n}\r\nmcode->group = cpt->next_group;\r\nmcode->core_mask = GENMASK(mcode->num_cores, 0);\r\ncpt_disable_cores(cpt, mcode->core_mask, SE_TYPES,\r\nmcode->group);\r\nret = cpt_load_microcode(cpt, mcode);\r\nif (ret) {\r\ndev_err(dev, "Microcode load Failed for %s\n",\r\nmcode->version);\r\ngoto cpt_init_fail;\r\n}\r\ncpt->next_group++;\r\ncpt_configure_group(cpt, mcode->group, mcode->core_mask,\r\nSE_TYPES);\r\ncpt_enable_cores(cpt, mcode->core_mask, SE_TYPES);\r\n}\r\ncpt_enable_mbox_interrupts(cpt);\r\ncpt->flags |= CPT_FLAG_DEVICE_READY;\r\nreturn ret;\r\ncpt_init_fail:\r\ncpt_enable_mbox_interrupts(cpt);\r\nreturn ret;\r\n}\r\nstatic int cpt_ucode_load_fw(struct cpt_device *cpt, const u8 *fw, bool is_ae)\r\n{\r\nconst struct firmware *fw_entry;\r\nstruct device *dev = &cpt->pdev->dev;\r\nstruct ucode_header *ucode;\r\nstruct microcode *mcode;\r\nint j, ret = 0;\r\nret = request_firmware(&fw_entry, fw, dev);\r\nif (ret)\r\nreturn ret;\r\nucode = (struct ucode_header *)fw_entry->data;\r\nmcode = &cpt->mcode[cpt->next_mc_idx];\r\nmemcpy(mcode->version, (u8 *)fw_entry->data, CPT_UCODE_VERSION_SZ);\r\nmcode->code_size = ntohl(ucode->code_length) * 2;\r\nif (!mcode->code_size)\r\nreturn -EINVAL;\r\nmcode->is_ae = is_ae;\r\nmcode->core_mask = 0ULL;\r\nmcode->num_cores = is_ae ? 6 : 10;\r\nmcode->code = dma_zalloc_coherent(&cpt->pdev->dev, mcode->code_size,\r\n&mcode->phys_base, GFP_KERNEL);\r\nif (!mcode->code) {\r\ndev_err(dev, "Unable to allocate space for microcode");\r\nreturn -ENOMEM;\r\n}\r\nmemcpy((void *)mcode->code, (void *)(fw_entry->data + sizeof(*ucode)),\r\nmcode->code_size);\r\nfor (j = 0; j < (mcode->code_size / 8); j++)\r\n((u64 *)mcode->code)[j] = cpu_to_be64(((u64 *)mcode->code)[j]);\r\nfor (j = 0; j < (mcode->code_size / 2); j++)\r\n((u16 *)mcode->code)[j] = cpu_to_be16(((u16 *)mcode->code)[j]);\r\ndev_dbg(dev, "mcode->code_size = %u\n", mcode->code_size);\r\ndev_dbg(dev, "mcode->is_ae = %u\n", mcode->is_ae);\r\ndev_dbg(dev, "mcode->num_cores = %u\n", mcode->num_cores);\r\ndev_dbg(dev, "mcode->code = %llx\n", (u64)mcode->code);\r\ndev_dbg(dev, "mcode->phys_base = %llx\n", mcode->phys_base);\r\nret = do_cpt_init(cpt, mcode);\r\nif (ret) {\r\ndev_err(dev, "do_cpt_init failed with ret: %d\n", ret);\r\nreturn ret;\r\n}\r\ndev_info(dev, "Microcode Loaded %s\n", mcode->version);\r\nmcode->is_mc_valid = 1;\r\ncpt->next_mc_idx++;\r\nrelease_firmware(fw_entry);\r\nreturn ret;\r\n}\r\nstatic int cpt_ucode_load(struct cpt_device *cpt)\r\n{\r\nint ret = 0;\r\nstruct device *dev = &cpt->pdev->dev;\r\nret = cpt_ucode_load_fw(cpt, "cpt8x-mc-ae.out", true);\r\nif (ret) {\r\ndev_err(dev, "ae:cpt_ucode_load failed with ret: %d\n", ret);\r\nreturn ret;\r\n}\r\nret = cpt_ucode_load_fw(cpt, "cpt8x-mc-se.out", false);\r\nif (ret) {\r\ndev_err(dev, "se:cpt_ucode_load failed with ret: %d\n", ret);\r\nreturn ret;\r\n}\r\nreturn ret;\r\n}\r\nstatic irqreturn_t cpt_mbx0_intr_handler(int irq, void *cpt_irq)\r\n{\r\nstruct cpt_device *cpt = (struct cpt_device *)cpt_irq;\r\ncpt_mbox_intr_handler(cpt, 0);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void cpt_reset(struct cpt_device *cpt)\r\n{\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_RESET(0), 1);\r\n}\r\nstatic void cpt_find_max_enabled_cores(struct cpt_device *cpt)\r\n{\r\nunion cptx_pf_constants pf_cnsts = {0};\r\npf_cnsts.u = cpt_read_csr64(cpt->reg_base, CPTX_PF_CONSTANTS(0));\r\ncpt->max_se_cores = pf_cnsts.s.se;\r\ncpt->max_ae_cores = pf_cnsts.s.ae;\r\n}\r\nstatic u32 cpt_check_bist_status(struct cpt_device *cpt)\r\n{\r\nunion cptx_pf_bist_status bist_sts = {0};\r\nbist_sts.u = cpt_read_csr64(cpt->reg_base,\r\nCPTX_PF_BIST_STATUS(0));\r\nreturn bist_sts.u;\r\n}\r\nstatic u64 cpt_check_exe_bist_status(struct cpt_device *cpt)\r\n{\r\nunion cptx_pf_exe_bist_status bist_sts = {0};\r\nbist_sts.u = cpt_read_csr64(cpt->reg_base,\r\nCPTX_PF_EXE_BIST_STATUS(0));\r\nreturn bist_sts.u;\r\n}\r\nstatic void cpt_disable_all_cores(struct cpt_device *cpt)\r\n{\r\nu32 grp, timeout = 100;\r\nstruct device *dev = &cpt->pdev->dev;\r\nfor (grp = 0; grp < CPT_MAX_CORE_GROUPS; grp++) {\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_GX_EN(0, grp), 0);\r\nudelay(CSR_DELAY);\r\n}\r\ngrp = cpt_read_csr64(cpt->reg_base, CPTX_PF_EXEC_BUSY(0));\r\nwhile (grp) {\r\ndev_err(dev, "Cores still busy");\r\ngrp = cpt_read_csr64(cpt->reg_base,\r\nCPTX_PF_EXEC_BUSY(0));\r\nif (timeout--)\r\nbreak;\r\nudelay(CSR_DELAY);\r\n}\r\ncpt_write_csr64(cpt->reg_base, CPTX_PF_EXE_CTL(0), 0);\r\n}\r\nstatic void cpt_unload_microcode(struct cpt_device *cpt)\r\n{\r\nu32 grp = 0, core;\r\nfor (grp = 0; grp < CPT_MAX_CORE_GROUPS; grp++) {\r\nstruct microcode *mcode = &cpt->mcode[grp];\r\nif (cpt->mcode[grp].code)\r\ndma_free_coherent(&cpt->pdev->dev, mcode->code_size,\r\nmcode->code, mcode->phys_base);\r\nmcode->code = NULL;\r\n}\r\nfor (core = 0; core < CPT_MAX_TOTAL_CORES; core++)\r\ncpt_write_csr64(cpt->reg_base,\r\nCPTX_PF_ENGX_UCODE_BASE(0, core), 0ull);\r\n}\r\nstatic int cpt_device_init(struct cpt_device *cpt)\r\n{\r\nu64 bist;\r\nstruct device *dev = &cpt->pdev->dev;\r\ncpt_reset(cpt);\r\nmdelay(100);\r\nbist = (u64)cpt_check_bist_status(cpt);\r\nif (bist) {\r\ndev_err(dev, "RAM BIST failed with code 0x%llx", bist);\r\nreturn -ENODEV;\r\n}\r\nbist = cpt_check_exe_bist_status(cpt);\r\nif (bist) {\r\ndev_err(dev, "Engine BIST failed with code 0x%llx", bist);\r\nreturn -ENODEV;\r\n}\r\ncpt_find_max_enabled_cores(cpt);\r\ncpt_disable_all_cores(cpt);\r\ncpt->next_mc_idx = 0;\r\ncpt->next_group = 0;\r\ncpt->flags |= CPT_FLAG_DEVICE_READY;\r\nreturn 0;\r\n}\r\nstatic int cpt_register_interrupts(struct cpt_device *cpt)\r\n{\r\nint ret;\r\nstruct device *dev = &cpt->pdev->dev;\r\nret = pci_alloc_irq_vectors(cpt->pdev, CPT_PF_MSIX_VECTORS,\r\nCPT_PF_MSIX_VECTORS, PCI_IRQ_MSIX);\r\nif (ret < 0) {\r\ndev_err(&cpt->pdev->dev, "Request for #%d msix vectors failed\n",\r\nCPT_PF_MSIX_VECTORS);\r\nreturn ret;\r\n}\r\nret = request_irq(pci_irq_vector(cpt->pdev, CPT_PF_INT_VEC_E_MBOXX(0)),\r\ncpt_mbx0_intr_handler, 0, "CPT Mbox0", cpt);\r\nif (ret)\r\ngoto fail;\r\ncpt_enable_mbox_interrupts(cpt);\r\nreturn 0;\r\nfail:\r\ndev_err(dev, "Request irq failed\n");\r\npci_disable_msix(cpt->pdev);\r\nreturn ret;\r\n}\r\nstatic void cpt_unregister_interrupts(struct cpt_device *cpt)\r\n{\r\nfree_irq(pci_irq_vector(cpt->pdev, CPT_PF_INT_VEC_E_MBOXX(0)), cpt);\r\npci_disable_msix(cpt->pdev);\r\n}\r\nstatic int cpt_sriov_init(struct cpt_device *cpt, int num_vfs)\r\n{\r\nint pos = 0;\r\nint err;\r\nu16 total_vf_cnt;\r\nstruct pci_dev *pdev = cpt->pdev;\r\npos = pci_find_ext_capability(pdev, PCI_EXT_CAP_ID_SRIOV);\r\nif (!pos) {\r\ndev_err(&pdev->dev, "SRIOV capability is not found in PCIe config space\n");\r\nreturn -ENODEV;\r\n}\r\ncpt->num_vf_en = num_vfs;\r\npci_read_config_word(pdev, (pos + PCI_SRIOV_TOTAL_VF), &total_vf_cnt);\r\nif (total_vf_cnt < cpt->num_vf_en)\r\ncpt->num_vf_en = total_vf_cnt;\r\nif (!total_vf_cnt)\r\nreturn 0;\r\nerr = pci_enable_sriov(pdev, cpt->num_vf_en);\r\nif (err) {\r\ndev_err(&pdev->dev, "SRIOV enable failed, num VF is %d\n",\r\ncpt->num_vf_en);\r\ncpt->num_vf_en = 0;\r\nreturn err;\r\n}\r\ndev_info(&pdev->dev, "SRIOV enabled, number of VF available %d\n",\r\ncpt->num_vf_en);\r\ncpt->flags |= CPT_FLAG_SRIOV_ENABLED;\r\nreturn 0;\r\n}\r\nstatic int cpt_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\r\n{\r\nstruct device *dev = &pdev->dev;\r\nstruct cpt_device *cpt;\r\nint err;\r\nif (num_vfs > 16 || num_vfs < 4) {\r\ndev_warn(dev, "Invalid vf count %d, Resetting it to 4(default)\n",\r\nnum_vfs);\r\nnum_vfs = 4;\r\n}\r\ncpt = devm_kzalloc(dev, sizeof(*cpt), GFP_KERNEL);\r\nif (!cpt)\r\nreturn -ENOMEM;\r\npci_set_drvdata(pdev, cpt);\r\ncpt->pdev = pdev;\r\nerr = pci_enable_device(pdev);\r\nif (err) {\r\ndev_err(dev, "Failed to enable PCI device\n");\r\npci_set_drvdata(pdev, NULL);\r\nreturn err;\r\n}\r\nerr = pci_request_regions(pdev, DRV_NAME);\r\nif (err) {\r\ndev_err(dev, "PCI request regions failed 0x%x\n", err);\r\ngoto cpt_err_disable_device;\r\n}\r\nerr = pci_set_dma_mask(pdev, DMA_BIT_MASK(48));\r\nif (err) {\r\ndev_err(dev, "Unable to get usable DMA configuration\n");\r\ngoto cpt_err_release_regions;\r\n}\r\nerr = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(48));\r\nif (err) {\r\ndev_err(dev, "Unable to get 48-bit DMA for consistent allocations\n");\r\ngoto cpt_err_release_regions;\r\n}\r\ncpt->reg_base = pcim_iomap(pdev, 0, 0);\r\nif (!cpt->reg_base) {\r\ndev_err(dev, "Cannot map config register space, aborting\n");\r\nerr = -ENOMEM;\r\ngoto cpt_err_release_regions;\r\n}\r\ncpt_device_init(cpt);\r\nerr = cpt_register_interrupts(cpt);\r\nif (err)\r\ngoto cpt_err_release_regions;\r\nerr = cpt_ucode_load(cpt);\r\nif (err)\r\ngoto cpt_err_unregister_interrupts;\r\nerr = cpt_sriov_init(cpt, num_vfs);\r\nif (err)\r\ngoto cpt_err_unregister_interrupts;\r\nreturn 0;\r\ncpt_err_unregister_interrupts:\r\ncpt_unregister_interrupts(cpt);\r\ncpt_err_release_regions:\r\npci_release_regions(pdev);\r\ncpt_err_disable_device:\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\nreturn err;\r\n}\r\nstatic void cpt_remove(struct pci_dev *pdev)\r\n{\r\nstruct cpt_device *cpt = pci_get_drvdata(pdev);\r\ncpt_disable_all_cores(cpt);\r\ncpt_unload_microcode(cpt);\r\ncpt_unregister_interrupts(cpt);\r\npci_disable_sriov(pdev);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\n}\r\nstatic void cpt_shutdown(struct pci_dev *pdev)\r\n{\r\nstruct cpt_device *cpt = pci_get_drvdata(pdev);\r\nif (!cpt)\r\nreturn;\r\ndev_info(&pdev->dev, "Shutdown device %x:%x.\n",\r\n(u32)pdev->vendor, (u32)pdev->device);\r\ncpt_unregister_interrupts(cpt);\r\npci_release_regions(pdev);\r\npci_disable_device(pdev);\r\npci_set_drvdata(pdev, NULL);\r\n}
