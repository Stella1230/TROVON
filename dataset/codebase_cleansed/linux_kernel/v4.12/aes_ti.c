static u32 mul_by_x(u32 w)\r\n{\r\nu32 x = w & 0x7f7f7f7f;\r\nu32 y = w & 0x80808080;\r\nreturn (x << 1) ^ (y >> 7) * 0x1b;\r\n}\r\nstatic u32 mul_by_x2(u32 w)\r\n{\r\nu32 x = w & 0x3f3f3f3f;\r\nu32 y = w & 0x80808080;\r\nu32 z = w & 0x40404040;\r\nreturn (x << 2) ^ (y >> 7) * 0x36 ^ (z >> 6) * 0x1b;\r\n}\r\nstatic u32 mix_columns(u32 x)\r\n{\r\nu32 y = mul_by_x(x) ^ ror32(x, 16);\r\nreturn y ^ ror32(x ^ y, 8);\r\n}\r\nstatic u32 inv_mix_columns(u32 x)\r\n{\r\nu32 y = mul_by_x2(x);\r\nreturn mix_columns(x ^ y ^ ror32(y, 16));\r\n}\r\nstatic __always_inline u32 subshift(u32 in[], int pos)\r\n{\r\nreturn (__aesti_sbox[in[pos] & 0xff]) ^\r\n(__aesti_sbox[(in[(pos + 1) % 4] >> 8) & 0xff] << 8) ^\r\n(__aesti_sbox[(in[(pos + 2) % 4] >> 16) & 0xff] << 16) ^\r\n(__aesti_sbox[(in[(pos + 3) % 4] >> 24) & 0xff] << 24);\r\n}\r\nstatic __always_inline u32 inv_subshift(u32 in[], int pos)\r\n{\r\nreturn (__aesti_inv_sbox[in[pos] & 0xff]) ^\r\n(__aesti_inv_sbox[(in[(pos + 3) % 4] >> 8) & 0xff] << 8) ^\r\n(__aesti_inv_sbox[(in[(pos + 2) % 4] >> 16) & 0xff] << 16) ^\r\n(__aesti_inv_sbox[(in[(pos + 1) % 4] >> 24) & 0xff] << 24);\r\n}\r\nstatic u32 subw(u32 in)\r\n{\r\nreturn (__aesti_sbox[in & 0xff]) ^\r\n(__aesti_sbox[(in >> 8) & 0xff] << 8) ^\r\n(__aesti_sbox[(in >> 16) & 0xff] << 16) ^\r\n(__aesti_sbox[(in >> 24) & 0xff] << 24);\r\n}\r\nstatic int aesti_expand_key(struct crypto_aes_ctx *ctx, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nu32 kwords = key_len / sizeof(u32);\r\nu32 rc, i, j;\r\nif (key_len != AES_KEYSIZE_128 &&\r\nkey_len != AES_KEYSIZE_192 &&\r\nkey_len != AES_KEYSIZE_256)\r\nreturn -EINVAL;\r\nctx->key_length = key_len;\r\nfor (i = 0; i < kwords; i++)\r\nctx->key_enc[i] = get_unaligned_le32(in_key + i * sizeof(u32));\r\nfor (i = 0, rc = 1; i < 10; i++, rc = mul_by_x(rc)) {\r\nu32 *rki = ctx->key_enc + (i * kwords);\r\nu32 *rko = rki + kwords;\r\nrko[0] = ror32(subw(rki[kwords - 1]), 8) ^ rc ^ rki[0];\r\nrko[1] = rko[0] ^ rki[1];\r\nrko[2] = rko[1] ^ rki[2];\r\nrko[3] = rko[2] ^ rki[3];\r\nif (key_len == 24) {\r\nif (i >= 7)\r\nbreak;\r\nrko[4] = rko[3] ^ rki[4];\r\nrko[5] = rko[4] ^ rki[5];\r\n} else if (key_len == 32) {\r\nif (i >= 6)\r\nbreak;\r\nrko[4] = subw(rko[3]) ^ rki[4];\r\nrko[5] = rko[4] ^ rki[5];\r\nrko[6] = rko[5] ^ rki[6];\r\nrko[7] = rko[6] ^ rki[7];\r\n}\r\n}\r\nctx->key_dec[0] = ctx->key_enc[key_len + 24];\r\nctx->key_dec[1] = ctx->key_enc[key_len + 25];\r\nctx->key_dec[2] = ctx->key_enc[key_len + 26];\r\nctx->key_dec[3] = ctx->key_enc[key_len + 27];\r\nfor (i = 4, j = key_len + 20; j > 0; i += 4, j -= 4) {\r\nctx->key_dec[i] = inv_mix_columns(ctx->key_enc[j]);\r\nctx->key_dec[i + 1] = inv_mix_columns(ctx->key_enc[j + 1]);\r\nctx->key_dec[i + 2] = inv_mix_columns(ctx->key_enc[j + 2]);\r\nctx->key_dec[i + 3] = inv_mix_columns(ctx->key_enc[j + 3]);\r\n}\r\nctx->key_dec[i] = ctx->key_enc[0];\r\nctx->key_dec[i + 1] = ctx->key_enc[1];\r\nctx->key_dec[i + 2] = ctx->key_enc[2];\r\nctx->key_dec[i + 3] = ctx->key_enc[3];\r\nreturn 0;\r\n}\r\nstatic int aesti_set_key(struct crypto_tfm *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint err;\r\nerr = aesti_expand_key(ctx, in_key, key_len);\r\nif (err)\r\nreturn err;\r\nctx->key_enc[0] ^= __aesti_sbox[ 0] ^ __aesti_sbox[128];\r\nctx->key_enc[1] ^= __aesti_sbox[32] ^ __aesti_sbox[160];\r\nctx->key_enc[2] ^= __aesti_sbox[64] ^ __aesti_sbox[192];\r\nctx->key_enc[3] ^= __aesti_sbox[96] ^ __aesti_sbox[224];\r\nctx->key_dec[0] ^= __aesti_inv_sbox[ 0] ^ __aesti_inv_sbox[128];\r\nctx->key_dec[1] ^= __aesti_inv_sbox[32] ^ __aesti_inv_sbox[160];\r\nctx->key_dec[2] ^= __aesti_inv_sbox[64] ^ __aesti_inv_sbox[192];\r\nctx->key_dec[3] ^= __aesti_inv_sbox[96] ^ __aesti_inv_sbox[224];\r\nreturn 0;\r\n}\r\nstatic void aesti_encrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\r\n{\r\nconst struct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nconst u32 *rkp = ctx->key_enc + 4;\r\nint rounds = 6 + ctx->key_length / 4;\r\nu32 st0[4], st1[4];\r\nint round;\r\nst0[0] = ctx->key_enc[0] ^ get_unaligned_le32(in);\r\nst0[1] = ctx->key_enc[1] ^ get_unaligned_le32(in + 4);\r\nst0[2] = ctx->key_enc[2] ^ get_unaligned_le32(in + 8);\r\nst0[3] = ctx->key_enc[3] ^ get_unaligned_le32(in + 12);\r\nst0[0] ^= __aesti_sbox[ 0] ^ __aesti_sbox[128];\r\nst0[1] ^= __aesti_sbox[32] ^ __aesti_sbox[160];\r\nst0[2] ^= __aesti_sbox[64] ^ __aesti_sbox[192];\r\nst0[3] ^= __aesti_sbox[96] ^ __aesti_sbox[224];\r\nfor (round = 0;; round += 2, rkp += 8) {\r\nst1[0] = mix_columns(subshift(st0, 0)) ^ rkp[0];\r\nst1[1] = mix_columns(subshift(st0, 1)) ^ rkp[1];\r\nst1[2] = mix_columns(subshift(st0, 2)) ^ rkp[2];\r\nst1[3] = mix_columns(subshift(st0, 3)) ^ rkp[3];\r\nif (round == rounds - 2)\r\nbreak;\r\nst0[0] = mix_columns(subshift(st1, 0)) ^ rkp[4];\r\nst0[1] = mix_columns(subshift(st1, 1)) ^ rkp[5];\r\nst0[2] = mix_columns(subshift(st1, 2)) ^ rkp[6];\r\nst0[3] = mix_columns(subshift(st1, 3)) ^ rkp[7];\r\n}\r\nput_unaligned_le32(subshift(st1, 0) ^ rkp[4], out);\r\nput_unaligned_le32(subshift(st1, 1) ^ rkp[5], out + 4);\r\nput_unaligned_le32(subshift(st1, 2) ^ rkp[6], out + 8);\r\nput_unaligned_le32(subshift(st1, 3) ^ rkp[7], out + 12);\r\n}\r\nstatic void aesti_decrypt(struct crypto_tfm *tfm, u8 *out, const u8 *in)\r\n{\r\nconst struct crypto_aes_ctx *ctx = crypto_tfm_ctx(tfm);\r\nconst u32 *rkp = ctx->key_dec + 4;\r\nint rounds = 6 + ctx->key_length / 4;\r\nu32 st0[4], st1[4];\r\nint round;\r\nst0[0] = ctx->key_dec[0] ^ get_unaligned_le32(in);\r\nst0[1] = ctx->key_dec[1] ^ get_unaligned_le32(in + 4);\r\nst0[2] = ctx->key_dec[2] ^ get_unaligned_le32(in + 8);\r\nst0[3] = ctx->key_dec[3] ^ get_unaligned_le32(in + 12);\r\nst0[0] ^= __aesti_inv_sbox[ 0] ^ __aesti_inv_sbox[128];\r\nst0[1] ^= __aesti_inv_sbox[32] ^ __aesti_inv_sbox[160];\r\nst0[2] ^= __aesti_inv_sbox[64] ^ __aesti_inv_sbox[192];\r\nst0[3] ^= __aesti_inv_sbox[96] ^ __aesti_inv_sbox[224];\r\nfor (round = 0;; round += 2, rkp += 8) {\r\nst1[0] = inv_mix_columns(inv_subshift(st0, 0)) ^ rkp[0];\r\nst1[1] = inv_mix_columns(inv_subshift(st0, 1)) ^ rkp[1];\r\nst1[2] = inv_mix_columns(inv_subshift(st0, 2)) ^ rkp[2];\r\nst1[3] = inv_mix_columns(inv_subshift(st0, 3)) ^ rkp[3];\r\nif (round == rounds - 2)\r\nbreak;\r\nst0[0] = inv_mix_columns(inv_subshift(st1, 0)) ^ rkp[4];\r\nst0[1] = inv_mix_columns(inv_subshift(st1, 1)) ^ rkp[5];\r\nst0[2] = inv_mix_columns(inv_subshift(st1, 2)) ^ rkp[6];\r\nst0[3] = inv_mix_columns(inv_subshift(st1, 3)) ^ rkp[7];\r\n}\r\nput_unaligned_le32(inv_subshift(st1, 0) ^ rkp[4], out);\r\nput_unaligned_le32(inv_subshift(st1, 1) ^ rkp[5], out + 4);\r\nput_unaligned_le32(inv_subshift(st1, 2) ^ rkp[6], out + 8);\r\nput_unaligned_le32(inv_subshift(st1, 3) ^ rkp[7], out + 12);\r\n}\r\nstatic int __init aes_init(void)\r\n{\r\nreturn crypto_register_alg(&aes_alg);\r\n}\r\nstatic void __exit aes_fini(void)\r\n{\r\ncrypto_unregister_alg(&aes_alg);\r\n}
