void lock_vector_lock(void)\r\n{\r\nraw_spin_lock(&vector_lock);\r\n}\r\nvoid unlock_vector_lock(void)\r\n{\r\nraw_spin_unlock(&vector_lock);\r\n}\r\nstatic struct apic_chip_data *apic_chip_data(struct irq_data *irq_data)\r\n{\r\nif (!irq_data)\r\nreturn NULL;\r\nwhile (irq_data->parent_data)\r\nirq_data = irq_data->parent_data;\r\nreturn irq_data->chip_data;\r\n}\r\nstruct irq_cfg *irqd_cfg(struct irq_data *irq_data)\r\n{\r\nstruct apic_chip_data *data = apic_chip_data(irq_data);\r\nreturn data ? &data->cfg : NULL;\r\n}\r\nstruct irq_cfg *irq_cfg(unsigned int irq)\r\n{\r\nreturn irqd_cfg(irq_get_irq_data(irq));\r\n}\r\nstatic struct apic_chip_data *alloc_apic_chip_data(int node)\r\n{\r\nstruct apic_chip_data *data;\r\ndata = kzalloc_node(sizeof(*data), GFP_KERNEL, node);\r\nif (!data)\r\nreturn NULL;\r\nif (!zalloc_cpumask_var_node(&data->domain, GFP_KERNEL, node))\r\ngoto out_data;\r\nif (!zalloc_cpumask_var_node(&data->old_domain, GFP_KERNEL, node))\r\ngoto out_domain;\r\nreturn data;\r\nout_domain:\r\nfree_cpumask_var(data->domain);\r\nout_data:\r\nkfree(data);\r\nreturn NULL;\r\n}\r\nstatic void free_apic_chip_data(struct apic_chip_data *data)\r\n{\r\nif (data) {\r\nfree_cpumask_var(data->domain);\r\nfree_cpumask_var(data->old_domain);\r\nkfree(data);\r\n}\r\n}\r\nstatic int __assign_irq_vector(int irq, struct apic_chip_data *d,\r\nconst struct cpumask *mask)\r\n{\r\nstatic int current_vector = FIRST_EXTERNAL_VECTOR + VECTOR_OFFSET_START;\r\nstatic int current_offset = VECTOR_OFFSET_START % 16;\r\nint cpu, vector;\r\nif (d->move_in_progress ||\r\ncpumask_intersects(d->old_domain, cpu_online_mask))\r\nreturn -EBUSY;\r\ncpumask_clear(d->old_domain);\r\ncpumask_clear(searched_cpumask);\r\ncpu = cpumask_first_and(mask, cpu_online_mask);\r\nwhile (cpu < nr_cpu_ids) {\r\nint new_cpu, offset;\r\napic->vector_allocation_domain(cpu, vector_cpumask, mask);\r\ncpumask_and(vector_searchmask, vector_cpumask, cpu_online_mask);\r\nif (!cpumask_intersects(vector_searchmask, mask))\r\ngoto next_cpu;\r\nif (cpumask_subset(vector_cpumask, d->domain)) {\r\nif (cpumask_equal(vector_cpumask, d->domain))\r\ngoto success;\r\ncpumask_andnot(d->old_domain, d->domain, vector_cpumask);\r\nvector = d->cfg.vector;\r\ngoto update;\r\n}\r\nvector = current_vector;\r\noffset = current_offset;\r\nnext:\r\nvector += 16;\r\nif (vector >= first_system_vector) {\r\noffset = (offset + 1) % 16;\r\nvector = FIRST_EXTERNAL_VECTOR + offset;\r\n}\r\nif (unlikely(current_vector == vector))\r\ngoto next_cpu;\r\nif (test_bit(vector, used_vectors))\r\ngoto next;\r\nfor_each_cpu(new_cpu, vector_searchmask) {\r\nif (!IS_ERR_OR_NULL(per_cpu(vector_irq, new_cpu)[vector]))\r\ngoto next;\r\n}\r\ncurrent_vector = vector;\r\ncurrent_offset = offset;\r\nif (d->cfg.vector)\r\ncpumask_copy(d->old_domain, d->domain);\r\nfor_each_cpu(new_cpu, vector_searchmask)\r\nper_cpu(vector_irq, new_cpu)[vector] = irq_to_desc(irq);\r\ngoto update;\r\nnext_cpu:\r\ncpumask_or(searched_cpumask, searched_cpumask, vector_cpumask);\r\ncpumask_andnot(vector_cpumask, mask, searched_cpumask);\r\ncpu = cpumask_first_and(vector_cpumask, cpu_online_mask);\r\ncontinue;\r\n}\r\nreturn -ENOSPC;\r\nupdate:\r\ncpumask_and(d->old_domain, d->old_domain, cpu_online_mask);\r\nd->move_in_progress = !cpumask_empty(d->old_domain);\r\nd->cfg.old_vector = d->move_in_progress ? d->cfg.vector : 0;\r\nd->cfg.vector = vector;\r\ncpumask_copy(d->domain, vector_cpumask);\r\nsuccess:\r\nBUG_ON(apic->cpu_mask_to_apicid_and(mask, d->domain,\r\n&d->cfg.dest_apicid));\r\nreturn 0;\r\n}\r\nstatic int assign_irq_vector(int irq, struct apic_chip_data *data,\r\nconst struct cpumask *mask)\r\n{\r\nint err;\r\nunsigned long flags;\r\nraw_spin_lock_irqsave(&vector_lock, flags);\r\nerr = __assign_irq_vector(irq, data, mask);\r\nraw_spin_unlock_irqrestore(&vector_lock, flags);\r\nreturn err;\r\n}\r\nstatic int assign_irq_vector_policy(int irq, int node,\r\nstruct apic_chip_data *data,\r\nstruct irq_alloc_info *info)\r\n{\r\nif (info && info->mask)\r\nreturn assign_irq_vector(irq, data, info->mask);\r\nif (node != NUMA_NO_NODE &&\r\nassign_irq_vector(irq, data, cpumask_of_node(node)) == 0)\r\nreturn 0;\r\nreturn assign_irq_vector(irq, data, apic->target_cpus());\r\n}\r\nstatic void clear_irq_vector(int irq, struct apic_chip_data *data)\r\n{\r\nstruct irq_desc *desc;\r\nint cpu, vector;\r\nif (!data->cfg.vector)\r\nreturn;\r\nvector = data->cfg.vector;\r\nfor_each_cpu_and(cpu, data->domain, cpu_online_mask)\r\nper_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;\r\ndata->cfg.vector = 0;\r\ncpumask_clear(data->domain);\r\nif (!data->move_in_progress && cpumask_empty(data->old_domain))\r\nreturn;\r\ndesc = irq_to_desc(irq);\r\nfor_each_cpu_and(cpu, data->old_domain, cpu_online_mask) {\r\nfor (vector = FIRST_EXTERNAL_VECTOR; vector < NR_VECTORS;\r\nvector++) {\r\nif (per_cpu(vector_irq, cpu)[vector] != desc)\r\ncontinue;\r\nper_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;\r\nbreak;\r\n}\r\n}\r\ndata->move_in_progress = 0;\r\n}\r\nvoid init_irq_alloc_info(struct irq_alloc_info *info,\r\nconst struct cpumask *mask)\r\n{\r\nmemset(info, 0, sizeof(*info));\r\ninfo->mask = mask;\r\n}\r\nvoid copy_irq_alloc_info(struct irq_alloc_info *dst, struct irq_alloc_info *src)\r\n{\r\nif (src)\r\n*dst = *src;\r\nelse\r\nmemset(dst, 0, sizeof(*dst));\r\n}\r\nstatic void x86_vector_free_irqs(struct irq_domain *domain,\r\nunsigned int virq, unsigned int nr_irqs)\r\n{\r\nstruct apic_chip_data *apic_data;\r\nstruct irq_data *irq_data;\r\nunsigned long flags;\r\nint i;\r\nfor (i = 0; i < nr_irqs; i++) {\r\nirq_data = irq_domain_get_irq_data(x86_vector_domain, virq + i);\r\nif (irq_data && irq_data->chip_data) {\r\nraw_spin_lock_irqsave(&vector_lock, flags);\r\nclear_irq_vector(virq + i, irq_data->chip_data);\r\napic_data = irq_data->chip_data;\r\nirq_domain_reset_irq_data(irq_data);\r\nraw_spin_unlock_irqrestore(&vector_lock, flags);\r\nfree_apic_chip_data(apic_data);\r\n#ifdef CONFIG_X86_IO_APIC\r\nif (virq + i < nr_legacy_irqs())\r\nlegacy_irq_data[virq + i] = NULL;\r\n#endif\r\n}\r\n}\r\n}\r\nstatic int x86_vector_alloc_irqs(struct irq_domain *domain, unsigned int virq,\r\nunsigned int nr_irqs, void *arg)\r\n{\r\nstruct irq_alloc_info *info = arg;\r\nstruct apic_chip_data *data;\r\nstruct irq_data *irq_data;\r\nint i, err, node;\r\nif (disable_apic)\r\nreturn -ENXIO;\r\nif ((info->flags & X86_IRQ_ALLOC_CONTIGUOUS_VECTORS) && nr_irqs > 1)\r\nreturn -ENOSYS;\r\nfor (i = 0; i < nr_irqs; i++) {\r\nirq_data = irq_domain_get_irq_data(domain, virq + i);\r\nBUG_ON(!irq_data);\r\nnode = irq_data_get_node(irq_data);\r\n#ifdef CONFIG_X86_IO_APIC\r\nif (virq + i < nr_legacy_irqs() && legacy_irq_data[virq + i])\r\ndata = legacy_irq_data[virq + i];\r\nelse\r\n#endif\r\ndata = alloc_apic_chip_data(node);\r\nif (!data) {\r\nerr = -ENOMEM;\r\ngoto error;\r\n}\r\nirq_data->chip = &lapic_controller;\r\nirq_data->chip_data = data;\r\nirq_data->hwirq = virq + i;\r\nerr = assign_irq_vector_policy(virq + i, node, data, info);\r\nif (err)\r\ngoto error;\r\n}\r\nreturn 0;\r\nerror:\r\nx86_vector_free_irqs(domain, virq, i + 1);\r\nreturn err;\r\n}\r\nint __init arch_probe_nr_irqs(void)\r\n{\r\nint nr;\r\nif (nr_irqs > (NR_VECTORS * nr_cpu_ids))\r\nnr_irqs = NR_VECTORS * nr_cpu_ids;\r\nnr = (gsi_top + nr_legacy_irqs()) + 8 * nr_cpu_ids;\r\n#if defined(CONFIG_PCI_MSI) || defined(CONFIG_HT_IRQ)\r\nif (gsi_top <= NR_IRQS_LEGACY)\r\nnr += 8 * nr_cpu_ids;\r\nelse\r\nnr += gsi_top * 16;\r\n#endif\r\nif (nr < nr_irqs)\r\nnr_irqs = nr;\r\nreturn legacy_pic->probe();\r\n}\r\nstatic void init_legacy_irqs(void)\r\n{\r\nint i, node = cpu_to_node(0);\r\nstruct apic_chip_data *data;\r\nfor (i = 0; i < nr_legacy_irqs(); i++) {\r\ndata = legacy_irq_data[i] = alloc_apic_chip_data(node);\r\nBUG_ON(!data);\r\ndata->cfg.vector = ISA_IRQ_VECTOR(i);\r\ncpumask_setall(data->domain);\r\nirq_set_chip_data(i, data);\r\n}\r\n}\r\nstatic void init_legacy_irqs(void) { }\r\nint __init arch_early_irq_init(void)\r\n{\r\ninit_legacy_irqs();\r\nx86_vector_domain = irq_domain_add_tree(NULL, &x86_vector_domain_ops,\r\nNULL);\r\nBUG_ON(x86_vector_domain == NULL);\r\nirq_set_default_host(x86_vector_domain);\r\narch_init_msi_domain(x86_vector_domain);\r\narch_init_htirq_domain(x86_vector_domain);\r\nBUG_ON(!alloc_cpumask_var(&vector_cpumask, GFP_KERNEL));\r\nBUG_ON(!alloc_cpumask_var(&vector_searchmask, GFP_KERNEL));\r\nBUG_ON(!alloc_cpumask_var(&searched_cpumask, GFP_KERNEL));\r\nreturn arch_early_ioapic_init();\r\n}\r\nstatic void __setup_vector_irq(int cpu)\r\n{\r\nstruct apic_chip_data *data;\r\nstruct irq_desc *desc;\r\nint irq, vector;\r\nfor_each_irq_desc(irq, desc) {\r\nstruct irq_data *idata = irq_desc_get_irq_data(desc);\r\ndata = apic_chip_data(idata);\r\nif (!data || !cpumask_test_cpu(cpu, data->domain))\r\ncontinue;\r\nvector = data->cfg.vector;\r\nper_cpu(vector_irq, cpu)[vector] = desc;\r\n}\r\nfor (vector = 0; vector < NR_VECTORS; ++vector) {\r\ndesc = per_cpu(vector_irq, cpu)[vector];\r\nif (IS_ERR_OR_NULL(desc))\r\ncontinue;\r\ndata = apic_chip_data(irq_desc_get_irq_data(desc));\r\nif (!cpumask_test_cpu(cpu, data->domain))\r\nper_cpu(vector_irq, cpu)[vector] = VECTOR_UNUSED;\r\n}\r\n}\r\nvoid setup_vector_irq(int cpu)\r\n{\r\nint irq;\r\nlockdep_assert_held(&vector_lock);\r\nfor (irq = 0; irq < nr_legacy_irqs(); irq++)\r\nper_cpu(vector_irq, cpu)[ISA_IRQ_VECTOR(irq)] = irq_to_desc(irq);\r\n__setup_vector_irq(cpu);\r\n}\r\nstatic int apic_retrigger_irq(struct irq_data *irq_data)\r\n{\r\nstruct apic_chip_data *data = apic_chip_data(irq_data);\r\nunsigned long flags;\r\nint cpu;\r\nraw_spin_lock_irqsave(&vector_lock, flags);\r\ncpu = cpumask_first_and(data->domain, cpu_online_mask);\r\napic->send_IPI_mask(cpumask_of(cpu), data->cfg.vector);\r\nraw_spin_unlock_irqrestore(&vector_lock, flags);\r\nreturn 1;\r\n}\r\nvoid apic_ack_edge(struct irq_data *data)\r\n{\r\nirq_complete_move(irqd_cfg(data));\r\nirq_move_irq(data);\r\nack_APIC_irq();\r\n}\r\nstatic int apic_set_affinity(struct irq_data *irq_data,\r\nconst struct cpumask *dest, bool force)\r\n{\r\nstruct apic_chip_data *data = irq_data->chip_data;\r\nint err, irq = irq_data->irq;\r\nif (!IS_ENABLED(CONFIG_SMP))\r\nreturn -EPERM;\r\nif (!cpumask_intersects(dest, cpu_online_mask))\r\nreturn -EINVAL;\r\nerr = assign_irq_vector(irq, data, dest);\r\nreturn err ? err : IRQ_SET_MASK_OK;\r\n}\r\nstatic void __send_cleanup_vector(struct apic_chip_data *data)\r\n{\r\nraw_spin_lock(&vector_lock);\r\ncpumask_and(data->old_domain, data->old_domain, cpu_online_mask);\r\ndata->move_in_progress = 0;\r\nif (!cpumask_empty(data->old_domain))\r\napic->send_IPI_mask(data->old_domain, IRQ_MOVE_CLEANUP_VECTOR);\r\nraw_spin_unlock(&vector_lock);\r\n}\r\nvoid send_cleanup_vector(struct irq_cfg *cfg)\r\n{\r\nstruct apic_chip_data *data;\r\ndata = container_of(cfg, struct apic_chip_data, cfg);\r\nif (data->move_in_progress)\r\n__send_cleanup_vector(data);\r\n}\r\nasmlinkage __visible void __irq_entry smp_irq_move_cleanup_interrupt(void)\r\n{\r\nunsigned vector, me;\r\nentering_ack_irq();\r\nraw_spin_lock(&vector_lock);\r\nme = smp_processor_id();\r\nfor (vector = FIRST_EXTERNAL_VECTOR; vector < NR_VECTORS; vector++) {\r\nstruct apic_chip_data *data;\r\nstruct irq_desc *desc;\r\nunsigned int irr;\r\nretry:\r\ndesc = __this_cpu_read(vector_irq[vector]);\r\nif (IS_ERR_OR_NULL(desc))\r\ncontinue;\r\nif (!raw_spin_trylock(&desc->lock)) {\r\nraw_spin_unlock(&vector_lock);\r\ncpu_relax();\r\nraw_spin_lock(&vector_lock);\r\ngoto retry;\r\n}\r\ndata = apic_chip_data(irq_desc_get_irq_data(desc));\r\nif (!data)\r\ngoto unlock;\r\nif (data->move_in_progress ||\r\n!cpumask_test_cpu(me, data->old_domain))\r\ngoto unlock;\r\nif (vector == data->cfg.vector &&\r\ncpumask_test_cpu(me, data->domain))\r\ngoto unlock;\r\nirr = apic_read(APIC_IRR + (vector / 32 * 0x10));\r\nif (irr & (1 << (vector % 32))) {\r\napic->send_IPI_self(IRQ_MOVE_CLEANUP_VECTOR);\r\ngoto unlock;\r\n}\r\n__this_cpu_write(vector_irq[vector], VECTOR_UNUSED);\r\ncpumask_clear_cpu(me, data->old_domain);\r\nunlock:\r\nraw_spin_unlock(&desc->lock);\r\n}\r\nraw_spin_unlock(&vector_lock);\r\nexiting_irq();\r\n}\r\nstatic void __irq_complete_move(struct irq_cfg *cfg, unsigned vector)\r\n{\r\nunsigned me;\r\nstruct apic_chip_data *data;\r\ndata = container_of(cfg, struct apic_chip_data, cfg);\r\nif (likely(!data->move_in_progress))\r\nreturn;\r\nme = smp_processor_id();\r\nif (vector == data->cfg.vector && cpumask_test_cpu(me, data->domain))\r\n__send_cleanup_vector(data);\r\n}\r\nvoid irq_complete_move(struct irq_cfg *cfg)\r\n{\r\n__irq_complete_move(cfg, ~get_irq_regs()->orig_ax);\r\n}\r\nvoid irq_force_complete_move(struct irq_desc *desc)\r\n{\r\nstruct irq_data *irqdata;\r\nstruct apic_chip_data *data;\r\nstruct irq_cfg *cfg;\r\nunsigned int cpu;\r\nirqdata = irq_domain_get_irq_data(x86_vector_domain,\r\nirq_desc_get_irq(desc));\r\nif (!irqdata)\r\nreturn;\r\ndata = apic_chip_data(irqdata);\r\ncfg = data ? &data->cfg : NULL;\r\nif (!cfg)\r\nreturn;\r\nraw_spin_lock(&vector_lock);\r\ncpumask_and(data->old_domain, data->old_domain, cpu_online_mask);\r\nif (!data->move_in_progress && cpumask_empty(data->old_domain)) {\r\nraw_spin_unlock(&vector_lock);\r\nreturn;\r\n}\r\nif (data->move_in_progress) {\r\npr_warn("IRQ fixup: irq %d move in progress, old vector %d\n",\r\nirqdata->irq, cfg->old_vector);\r\n}\r\nfor_each_cpu(cpu, data->old_domain)\r\nper_cpu(vector_irq, cpu)[cfg->old_vector] = VECTOR_UNUSED;\r\ncpumask_clear(data->old_domain);\r\ndata->move_in_progress = 0;\r\nraw_spin_unlock(&vector_lock);\r\n}\r\nstatic void __init print_APIC_field(int base)\r\n{\r\nint i;\r\nprintk(KERN_DEBUG);\r\nfor (i = 0; i < 8; i++)\r\npr_cont("%08x", apic_read(base + i*0x10));\r\npr_cont("\n");\r\n}\r\nstatic void __init print_local_APIC(void *dummy)\r\n{\r\nunsigned int i, v, ver, maxlvt;\r\nu64 icr;\r\npr_debug("printing local APIC contents on CPU#%d/%d:\n",\r\nsmp_processor_id(), hard_smp_processor_id());\r\nv = apic_read(APIC_ID);\r\npr_info("... APIC ID: %08x (%01x)\n", v, read_apic_id());\r\nv = apic_read(APIC_LVR);\r\npr_info("... APIC VERSION: %08x\n", v);\r\nver = GET_APIC_VERSION(v);\r\nmaxlvt = lapic_get_maxlvt();\r\nv = apic_read(APIC_TASKPRI);\r\npr_debug("... APIC TASKPRI: %08x (%02x)\n", v, v & APIC_TPRI_MASK);\r\nif (APIC_INTEGRATED(ver)) {\r\nif (!APIC_XAPIC(ver)) {\r\nv = apic_read(APIC_ARBPRI);\r\npr_debug("... APIC ARBPRI: %08x (%02x)\n",\r\nv, v & APIC_ARBPRI_MASK);\r\n}\r\nv = apic_read(APIC_PROCPRI);\r\npr_debug("... APIC PROCPRI: %08x\n", v);\r\n}\r\nif (!APIC_INTEGRATED(ver) || maxlvt == 3) {\r\nv = apic_read(APIC_RRR);\r\npr_debug("... APIC RRR: %08x\n", v);\r\n}\r\nv = apic_read(APIC_LDR);\r\npr_debug("... APIC LDR: %08x\n", v);\r\nif (!x2apic_enabled()) {\r\nv = apic_read(APIC_DFR);\r\npr_debug("... APIC DFR: %08x\n", v);\r\n}\r\nv = apic_read(APIC_SPIV);\r\npr_debug("... APIC SPIV: %08x\n", v);\r\npr_debug("... APIC ISR field:\n");\r\nprint_APIC_field(APIC_ISR);\r\npr_debug("... APIC TMR field:\n");\r\nprint_APIC_field(APIC_TMR);\r\npr_debug("... APIC IRR field:\n");\r\nprint_APIC_field(APIC_IRR);\r\nif (APIC_INTEGRATED(ver)) {\r\nif (maxlvt > 3)\r\napic_write(APIC_ESR, 0);\r\nv = apic_read(APIC_ESR);\r\npr_debug("... APIC ESR: %08x\n", v);\r\n}\r\nicr = apic_icr_read();\r\npr_debug("... APIC ICR: %08x\n", (u32)icr);\r\npr_debug("... APIC ICR2: %08x\n", (u32)(icr >> 32));\r\nv = apic_read(APIC_LVTT);\r\npr_debug("... APIC LVTT: %08x\n", v);\r\nif (maxlvt > 3) {\r\nv = apic_read(APIC_LVTPC);\r\npr_debug("... APIC LVTPC: %08x\n", v);\r\n}\r\nv = apic_read(APIC_LVT0);\r\npr_debug("... APIC LVT0: %08x\n", v);\r\nv = apic_read(APIC_LVT1);\r\npr_debug("... APIC LVT1: %08x\n", v);\r\nif (maxlvt > 2) {\r\nv = apic_read(APIC_LVTERR);\r\npr_debug("... APIC LVTERR: %08x\n", v);\r\n}\r\nv = apic_read(APIC_TMICT);\r\npr_debug("... APIC TMICT: %08x\n", v);\r\nv = apic_read(APIC_TMCCT);\r\npr_debug("... APIC TMCCT: %08x\n", v);\r\nv = apic_read(APIC_TDCR);\r\npr_debug("... APIC TDCR: %08x\n", v);\r\nif (boot_cpu_has(X86_FEATURE_EXTAPIC)) {\r\nv = apic_read(APIC_EFEAT);\r\nmaxlvt = (v >> 16) & 0xff;\r\npr_debug("... APIC EFEAT: %08x\n", v);\r\nv = apic_read(APIC_ECTRL);\r\npr_debug("... APIC ECTRL: %08x\n", v);\r\nfor (i = 0; i < maxlvt; i++) {\r\nv = apic_read(APIC_EILVTn(i));\r\npr_debug("... APIC EILVT%d: %08x\n", i, v);\r\n}\r\n}\r\npr_cont("\n");\r\n}\r\nstatic void __init print_local_APICs(int maxcpu)\r\n{\r\nint cpu;\r\nif (!maxcpu)\r\nreturn;\r\npreempt_disable();\r\nfor_each_online_cpu(cpu) {\r\nif (cpu >= maxcpu)\r\nbreak;\r\nsmp_call_function_single(cpu, print_local_APIC, NULL, 1);\r\n}\r\npreempt_enable();\r\n}\r\nstatic void __init print_PIC(void)\r\n{\r\nunsigned int v;\r\nunsigned long flags;\r\nif (!nr_legacy_irqs())\r\nreturn;\r\npr_debug("\nprinting PIC contents\n");\r\nraw_spin_lock_irqsave(&i8259A_lock, flags);\r\nv = inb(0xa1) << 8 | inb(0x21);\r\npr_debug("... PIC IMR: %04x\n", v);\r\nv = inb(0xa0) << 8 | inb(0x20);\r\npr_debug("... PIC IRR: %04x\n", v);\r\noutb(0x0b, 0xa0);\r\noutb(0x0b, 0x20);\r\nv = inb(0xa0) << 8 | inb(0x20);\r\noutb(0x0a, 0xa0);\r\noutb(0x0a, 0x20);\r\nraw_spin_unlock_irqrestore(&i8259A_lock, flags);\r\npr_debug("... PIC ISR: %04x\n", v);\r\nv = inb(0x4d1) << 8 | inb(0x4d0);\r\npr_debug("... PIC ELCR: %04x\n", v);\r\n}\r\nstatic __init int setup_show_lapic(char *arg)\r\n{\r\nint num = -1;\r\nif (strcmp(arg, "all") == 0) {\r\nshow_lapic = CONFIG_NR_CPUS;\r\n} else {\r\nget_option(&arg, &num);\r\nif (num >= 0)\r\nshow_lapic = num;\r\n}\r\nreturn 1;\r\n}\r\nstatic int __init print_ICs(void)\r\n{\r\nif (apic_verbosity == APIC_QUIET)\r\nreturn 0;\r\nprint_PIC();\r\nif (!boot_cpu_has(X86_FEATURE_APIC) && !apic_from_smp_config())\r\nreturn 0;\r\nprint_local_APICs(show_lapic);\r\nprint_IO_APICs();\r\nreturn 0;\r\n}
