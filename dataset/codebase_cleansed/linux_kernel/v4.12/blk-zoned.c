static inline sector_t blk_zone_start(struct request_queue *q,\r\nsector_t sector)\r\n{\r\nsector_t zone_mask = blk_queue_zone_sectors(q) - 1;\r\nreturn sector & ~zone_mask;\r\n}\r\nstatic bool blkdev_report_zone(struct block_device *bdev,\r\nstruct blk_zone *rep,\r\nstruct blk_zone *zone)\r\n{\r\nsector_t offset = get_start_sect(bdev);\r\nif (rep->start < offset)\r\nreturn false;\r\nrep->start -= offset;\r\nif (rep->start + rep->len > bdev->bd_part->nr_sects)\r\nreturn false;\r\nif (rep->type == BLK_ZONE_TYPE_CONVENTIONAL)\r\nrep->wp = rep->start + rep->len;\r\nelse\r\nrep->wp -= offset;\r\nmemcpy(zone, rep, sizeof(struct blk_zone));\r\nreturn true;\r\n}\r\nint blkdev_report_zones(struct block_device *bdev,\r\nsector_t sector,\r\nstruct blk_zone *zones,\r\nunsigned int *nr_zones,\r\ngfp_t gfp_mask)\r\n{\r\nstruct request_queue *q = bdev_get_queue(bdev);\r\nstruct blk_zone_report_hdr *hdr;\r\nunsigned int nrz = *nr_zones;\r\nstruct page *page;\r\nunsigned int nr_rep;\r\nsize_t rep_bytes;\r\nunsigned int nr_pages;\r\nstruct bio *bio;\r\nstruct bio_vec *bv;\r\nunsigned int i, n, nz;\r\nunsigned int ofst;\r\nvoid *addr;\r\nint ret;\r\nif (!q)\r\nreturn -ENXIO;\r\nif (!blk_queue_is_zoned(q))\r\nreturn -EOPNOTSUPP;\r\nif (!nrz)\r\nreturn 0;\r\nif (sector > bdev->bd_part->nr_sects) {\r\n*nr_zones = 0;\r\nreturn 0;\r\n}\r\nrep_bytes = sizeof(struct blk_zone_report_hdr) +\r\nsizeof(struct blk_zone) * nrz;\r\nrep_bytes = (rep_bytes + PAGE_SIZE - 1) & PAGE_MASK;\r\nif (rep_bytes > (queue_max_sectors(q) << 9))\r\nrep_bytes = queue_max_sectors(q) << 9;\r\nnr_pages = min_t(unsigned int, BIO_MAX_PAGES,\r\nrep_bytes >> PAGE_SHIFT);\r\nnr_pages = min_t(unsigned int, nr_pages,\r\nqueue_max_segments(q));\r\nbio = bio_alloc(gfp_mask, nr_pages);\r\nif (!bio)\r\nreturn -ENOMEM;\r\nbio->bi_bdev = bdev;\r\nbio->bi_iter.bi_sector = blk_zone_start(q, sector);\r\nbio_set_op_attrs(bio, REQ_OP_ZONE_REPORT, 0);\r\nfor (i = 0; i < nr_pages; i++) {\r\npage = alloc_page(gfp_mask);\r\nif (!page) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (!bio_add_page(bio, page, PAGE_SIZE, 0)) {\r\n__free_page(page);\r\nbreak;\r\n}\r\n}\r\nif (i == 0)\r\nret = -ENOMEM;\r\nelse\r\nret = submit_bio_wait(bio);\r\nif (ret)\r\ngoto out;\r\nn = 0;\r\nnz = 0;\r\nnr_rep = 0;\r\nbio_for_each_segment_all(bv, bio, i) {\r\nif (!bv->bv_page)\r\nbreak;\r\naddr = kmap_atomic(bv->bv_page);\r\nofst = 0;\r\nif (!nr_rep) {\r\nhdr = (struct blk_zone_report_hdr *) addr;\r\nnr_rep = hdr->nr_zones;\r\nofst = sizeof(struct blk_zone_report_hdr);\r\n}\r\nwhile (ofst < bv->bv_len &&\r\nn < nr_rep && nz < nrz) {\r\nif (blkdev_report_zone(bdev, addr + ofst, &zones[nz]))\r\nnz++;\r\nofst += sizeof(struct blk_zone);\r\nn++;\r\n}\r\nkunmap_atomic(addr);\r\nif (n >= nr_rep || nz >= nrz)\r\nbreak;\r\n}\r\n*nr_zones = nz;\r\nout:\r\nbio_for_each_segment_all(bv, bio, i)\r\n__free_page(bv->bv_page);\r\nbio_put(bio);\r\nreturn ret;\r\n}\r\nint blkdev_reset_zones(struct block_device *bdev,\r\nsector_t sector, sector_t nr_sectors,\r\ngfp_t gfp_mask)\r\n{\r\nstruct request_queue *q = bdev_get_queue(bdev);\r\nsector_t zone_sectors;\r\nsector_t end_sector = sector + nr_sectors;\r\nstruct bio *bio;\r\nint ret;\r\nif (!q)\r\nreturn -ENXIO;\r\nif (!blk_queue_is_zoned(q))\r\nreturn -EOPNOTSUPP;\r\nif (end_sector > bdev->bd_part->nr_sects)\r\nreturn -EINVAL;\r\nzone_sectors = blk_queue_zone_sectors(q);\r\nif (sector & (zone_sectors - 1))\r\nreturn -EINVAL;\r\nif ((nr_sectors & (zone_sectors - 1)) &&\r\nend_sector != bdev->bd_part->nr_sects)\r\nreturn -EINVAL;\r\nwhile (sector < end_sector) {\r\nbio = bio_alloc(gfp_mask, 0);\r\nbio->bi_iter.bi_sector = sector;\r\nbio->bi_bdev = bdev;\r\nbio_set_op_attrs(bio, REQ_OP_ZONE_RESET, 0);\r\nret = submit_bio_wait(bio);\r\nbio_put(bio);\r\nif (ret)\r\nreturn ret;\r\nsector += zone_sectors;\r\ncond_resched();\r\n}\r\nreturn 0;\r\n}\r\nint blkdev_report_zones_ioctl(struct block_device *bdev, fmode_t mode,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nvoid __user *argp = (void __user *)arg;\r\nstruct request_queue *q;\r\nstruct blk_zone_report rep;\r\nstruct blk_zone *zones;\r\nint ret;\r\nif (!argp)\r\nreturn -EINVAL;\r\nq = bdev_get_queue(bdev);\r\nif (!q)\r\nreturn -ENXIO;\r\nif (!blk_queue_is_zoned(q))\r\nreturn -ENOTTY;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nif (copy_from_user(&rep, argp, sizeof(struct blk_zone_report)))\r\nreturn -EFAULT;\r\nif (!rep.nr_zones)\r\nreturn -EINVAL;\r\nzones = kcalloc(rep.nr_zones, sizeof(struct blk_zone), GFP_KERNEL);\r\nif (!zones)\r\nreturn -ENOMEM;\r\nret = blkdev_report_zones(bdev, rep.sector,\r\nzones, &rep.nr_zones,\r\nGFP_KERNEL);\r\nif (ret)\r\ngoto out;\r\nif (copy_to_user(argp, &rep, sizeof(struct blk_zone_report))) {\r\nret = -EFAULT;\r\ngoto out;\r\n}\r\nif (rep.nr_zones) {\r\nif (copy_to_user(argp + sizeof(struct blk_zone_report), zones,\r\nsizeof(struct blk_zone) * rep.nr_zones))\r\nret = -EFAULT;\r\n}\r\nout:\r\nkfree(zones);\r\nreturn ret;\r\n}\r\nint blkdev_reset_zones_ioctl(struct block_device *bdev, fmode_t mode,\r\nunsigned int cmd, unsigned long arg)\r\n{\r\nvoid __user *argp = (void __user *)arg;\r\nstruct request_queue *q;\r\nstruct blk_zone_range zrange;\r\nif (!argp)\r\nreturn -EINVAL;\r\nq = bdev_get_queue(bdev);\r\nif (!q)\r\nreturn -ENXIO;\r\nif (!blk_queue_is_zoned(q))\r\nreturn -ENOTTY;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EACCES;\r\nif (!(mode & FMODE_WRITE))\r\nreturn -EBADF;\r\nif (copy_from_user(&zrange, argp, sizeof(struct blk_zone_range)))\r\nreturn -EFAULT;\r\nreturn blkdev_reset_zones(bdev, zrange.sector, zrange.nr_sectors,\r\nGFP_KERNEL);\r\n}
