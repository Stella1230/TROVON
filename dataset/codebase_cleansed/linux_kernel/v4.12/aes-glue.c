static int skcipher_aes_setkey(struct crypto_skcipher *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nreturn aes_setkey(crypto_skcipher_tfm(tfm), in_key, key_len);\r\n}\r\nstatic int xts_set_key(struct crypto_skcipher *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct crypto_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint ret;\r\nret = xts_verify_key(tfm, in_key, key_len);\r\nif (ret)\r\nreturn ret;\r\nret = aes_expandkey(&ctx->key1, in_key, key_len / 2);\r\nif (!ret)\r\nret = aes_expandkey(&ctx->key2, &in_key[key_len / 2],\r\nkey_len / 2);\r\nif (!ret)\r\nreturn 0;\r\ncrypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn -EINVAL;\r\n}\r\nstatic int ecb_encrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = 6 + ctx->key_length / 4;\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\naes_ecb_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, rounds, blocks, first);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int ecb_decrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = 6 + ctx->key_length / 4;\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\naes_ecb_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_dec, rounds, blocks, first);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int cbc_encrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = 6 + ctx->key_length / 4;\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\naes_cbc_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, rounds, blocks, walk.iv,\r\nfirst);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int cbc_decrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = 6 + ctx->key_length / 4;\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\naes_cbc_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_dec, rounds, blocks, walk.iv,\r\nfirst);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int ctr_encrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = 6 + ctx->key_length / 4;\r\nstruct skcipher_walk walk;\r\nint blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nfirst = 1;\r\nkernel_neon_begin();\r\nwhile ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {\r\naes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key_enc, rounds, blocks, walk.iv,\r\nfirst);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\nfirst = 0;\r\n}\r\nif (walk.nbytes) {\r\nu8 __aligned(8) tail[AES_BLOCK_SIZE];\r\nunsigned int nbytes = walk.nbytes;\r\nu8 *tdst = walk.dst.virt.addr;\r\nu8 *tsrc = walk.src.virt.addr;\r\nblocks = -1;\r\naes_ctr_encrypt(tail, NULL, (u8 *)ctx->key_enc, rounds,\r\nblocks, walk.iv, first);\r\nif (tdst != tsrc)\r\nmemcpy(tdst, tsrc, nbytes);\r\ncrypto_xor(tdst, tail, nbytes);\r\nerr = skcipher_walk_done(&walk, 0);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int xts_encrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = 6 + ctx->key1.key_length / 4;\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\naes_xts_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key1.key_enc, rounds, blocks,\r\n(u8 *)ctx->key2.key_enc, walk.iv, first);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int xts_decrypt(struct skcipher_request *req)\r\n{\r\nstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\r\nstruct crypto_aes_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\r\nint err, first, rounds = 6 + ctx->key1.key_length / 4;\r\nstruct skcipher_walk walk;\r\nunsigned int blocks;\r\nerr = skcipher_walk_virt(&walk, req, true);\r\nkernel_neon_begin();\r\nfor (first = 1; (blocks = (walk.nbytes / AES_BLOCK_SIZE)); first = 0) {\r\naes_xts_decrypt(walk.dst.virt.addr, walk.src.virt.addr,\r\n(u8 *)ctx->key1.key_dec, rounds, blocks,\r\n(u8 *)ctx->key2.key_enc, walk.iv, first);\r\nerr = skcipher_walk_done(&walk, walk.nbytes % AES_BLOCK_SIZE);\r\n}\r\nkernel_neon_end();\r\nreturn err;\r\n}\r\nstatic int cbcmac_setkey(struct crypto_shash *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct mac_tfm_ctx *ctx = crypto_shash_ctx(tfm);\r\nint err;\r\nerr = aes_expandkey(&ctx->key, in_key, key_len);\r\nif (err)\r\ncrypto_shash_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);\r\nreturn err;\r\n}\r\nstatic void cmac_gf128_mul_by_x(be128 *y, const be128 *x)\r\n{\r\nu64 a = be64_to_cpu(x->a);\r\nu64 b = be64_to_cpu(x->b);\r\ny->a = cpu_to_be64((a << 1) | (b >> 63));\r\ny->b = cpu_to_be64((b << 1) ^ ((a >> 63) ? 0x87 : 0));\r\n}\r\nstatic int cmac_setkey(struct crypto_shash *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstruct mac_tfm_ctx *ctx = crypto_shash_ctx(tfm);\r\nbe128 *consts = (be128 *)ctx->consts;\r\nu8 *rk = (u8 *)ctx->key.key_enc;\r\nint rounds = 6 + key_len / 4;\r\nint err;\r\nerr = cbcmac_setkey(tfm, in_key, key_len);\r\nif (err)\r\nreturn err;\r\nkernel_neon_begin();\r\naes_ecb_encrypt(ctx->consts, (u8[AES_BLOCK_SIZE]){}, rk, rounds, 1, 1);\r\nkernel_neon_end();\r\ncmac_gf128_mul_by_x(consts, consts);\r\ncmac_gf128_mul_by_x(consts + 1, consts);\r\nreturn 0;\r\n}\r\nstatic int xcbc_setkey(struct crypto_shash *tfm, const u8 *in_key,\r\nunsigned int key_len)\r\n{\r\nstatic u8 const ks[3][AES_BLOCK_SIZE] = {\r\n{ [0 ... AES_BLOCK_SIZE - 1] = 0x1 },\r\n{ [0 ... AES_BLOCK_SIZE - 1] = 0x2 },\r\n{ [0 ... AES_BLOCK_SIZE - 1] = 0x3 },\r\n};\r\nstruct mac_tfm_ctx *ctx = crypto_shash_ctx(tfm);\r\nu8 *rk = (u8 *)ctx->key.key_enc;\r\nint rounds = 6 + key_len / 4;\r\nu8 key[AES_BLOCK_SIZE];\r\nint err;\r\nerr = cbcmac_setkey(tfm, in_key, key_len);\r\nif (err)\r\nreturn err;\r\nkernel_neon_begin();\r\naes_ecb_encrypt(key, ks[0], rk, rounds, 1, 1);\r\naes_ecb_encrypt(ctx->consts, ks[1], rk, rounds, 2, 0);\r\nkernel_neon_end();\r\nreturn cbcmac_setkey(tfm, key, sizeof(key));\r\n}\r\nstatic int mac_init(struct shash_desc *desc)\r\n{\r\nstruct mac_desc_ctx *ctx = shash_desc_ctx(desc);\r\nmemset(ctx->dg, 0, AES_BLOCK_SIZE);\r\nctx->len = 0;\r\nreturn 0;\r\n}\r\nstatic int mac_update(struct shash_desc *desc, const u8 *p, unsigned int len)\r\n{\r\nstruct mac_tfm_ctx *tctx = crypto_shash_ctx(desc->tfm);\r\nstruct mac_desc_ctx *ctx = shash_desc_ctx(desc);\r\nint rounds = 6 + tctx->key.key_length / 4;\r\nwhile (len > 0) {\r\nunsigned int l;\r\nif ((ctx->len % AES_BLOCK_SIZE) == 0 &&\r\n(ctx->len + len) > AES_BLOCK_SIZE) {\r\nint blocks = len / AES_BLOCK_SIZE;\r\nlen %= AES_BLOCK_SIZE;\r\nkernel_neon_begin();\r\naes_mac_update(p, tctx->key.key_enc, rounds, blocks,\r\nctx->dg, (ctx->len != 0), (len != 0));\r\nkernel_neon_end();\r\np += blocks * AES_BLOCK_SIZE;\r\nif (!len) {\r\nctx->len = AES_BLOCK_SIZE;\r\nbreak;\r\n}\r\nctx->len = 0;\r\n}\r\nl = min(len, AES_BLOCK_SIZE - ctx->len);\r\nif (l <= AES_BLOCK_SIZE) {\r\ncrypto_xor(ctx->dg + ctx->len, p, l);\r\nctx->len += l;\r\nlen -= l;\r\np += l;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cbcmac_final(struct shash_desc *desc, u8 *out)\r\n{\r\nstruct mac_tfm_ctx *tctx = crypto_shash_ctx(desc->tfm);\r\nstruct mac_desc_ctx *ctx = shash_desc_ctx(desc);\r\nint rounds = 6 + tctx->key.key_length / 4;\r\nkernel_neon_begin();\r\naes_mac_update(NULL, tctx->key.key_enc, rounds, 0, ctx->dg, 1, 0);\r\nkernel_neon_end();\r\nmemcpy(out, ctx->dg, AES_BLOCK_SIZE);\r\nreturn 0;\r\n}\r\nstatic int cmac_final(struct shash_desc *desc, u8 *out)\r\n{\r\nstruct mac_tfm_ctx *tctx = crypto_shash_ctx(desc->tfm);\r\nstruct mac_desc_ctx *ctx = shash_desc_ctx(desc);\r\nint rounds = 6 + tctx->key.key_length / 4;\r\nu8 *consts = tctx->consts;\r\nif (ctx->len != AES_BLOCK_SIZE) {\r\nctx->dg[ctx->len] ^= 0x80;\r\nconsts += AES_BLOCK_SIZE;\r\n}\r\nkernel_neon_begin();\r\naes_mac_update(consts, tctx->key.key_enc, rounds, 1, ctx->dg, 0, 1);\r\nkernel_neon_end();\r\nmemcpy(out, ctx->dg, AES_BLOCK_SIZE);\r\nreturn 0;\r\n}\r\nstatic void aes_exit(void)\r\n{\r\nint i;\r\nfor (i = 0; i < ARRAY_SIZE(aes_simd_algs); i++)\r\nif (aes_simd_algs[i])\r\nsimd_skcipher_free(aes_simd_algs[i]);\r\ncrypto_unregister_shashes(mac_algs, ARRAY_SIZE(mac_algs));\r\ncrypto_unregister_skciphers(aes_algs, ARRAY_SIZE(aes_algs));\r\n}\r\nstatic int __init aes_init(void)\r\n{\r\nstruct simd_skcipher_alg *simd;\r\nconst char *basename;\r\nconst char *algname;\r\nconst char *drvname;\r\nint err;\r\nint i;\r\nerr = crypto_register_skciphers(aes_algs, ARRAY_SIZE(aes_algs));\r\nif (err)\r\nreturn err;\r\nerr = crypto_register_shashes(mac_algs, ARRAY_SIZE(mac_algs));\r\nif (err)\r\ngoto unregister_ciphers;\r\nfor (i = 0; i < ARRAY_SIZE(aes_algs); i++) {\r\nif (!(aes_algs[i].base.cra_flags & CRYPTO_ALG_INTERNAL))\r\ncontinue;\r\nalgname = aes_algs[i].base.cra_name + 2;\r\ndrvname = aes_algs[i].base.cra_driver_name + 2;\r\nbasename = aes_algs[i].base.cra_driver_name;\r\nsimd = simd_skcipher_create_compat(algname, drvname, basename);\r\nerr = PTR_ERR(simd);\r\nif (IS_ERR(simd))\r\ngoto unregister_simds;\r\naes_simd_algs[i] = simd;\r\n}\r\nreturn 0;\r\nunregister_simds:\r\naes_exit();\r\nunregister_ciphers:\r\ncrypto_unregister_skciphers(aes_algs, ARRAY_SIZE(aes_algs));\r\nreturn err;\r\n}
