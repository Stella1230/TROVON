void mlx4_ib_update_cache_on_guid_change(struct mlx4_ib_dev *dev, int block_num,\r\nu8 port_num, u8 *p_data)\r\n{\r\nint i;\r\nu64 guid_indexes;\r\nint slave_id;\r\nint port_index = port_num - 1;\r\nif (!mlx4_is_master(dev->dev))\r\nreturn;\r\nguid_indexes = be64_to_cpu((__force __be64) dev->sriov.alias_guid.\r\nports_guid[port_num - 1].\r\nall_rec_per_port[block_num].guid_indexes);\r\npr_debug("port: %d, guid_indexes: 0x%llx\n", port_num, guid_indexes);\r\nfor (i = 0; i < NUM_ALIAS_GUID_IN_REC; i++) {\r\nif (test_bit(i + 4, (unsigned long *)&guid_indexes)) {\r\nslave_id = (block_num * NUM_ALIAS_GUID_IN_REC) + i ;\r\nif (slave_id >= dev->dev->num_slaves) {\r\npr_debug("The last slave: %d\n", slave_id);\r\nreturn;\r\n}\r\nmemcpy(&dev->sriov.demux[port_index].guid_cache[slave_id],\r\n&p_data[i * GUID_REC_SIZE],\r\nGUID_REC_SIZE);\r\n} else\r\npr_debug("Guid number: %d in block: %d"\r\n" was not updated\n", i, block_num);\r\n}\r\n}\r\nstatic __be64 get_cached_alias_guid(struct mlx4_ib_dev *dev, int port, int index)\r\n{\r\nif (index >= NUM_ALIAS_GUID_PER_PORT) {\r\npr_err("%s: ERROR: asked for index:%d\n", __func__, index);\r\nreturn (__force __be64) -1;\r\n}\r\nreturn *(__be64 *)&dev->sriov.demux[port - 1].guid_cache[index];\r\n}\r\nib_sa_comp_mask mlx4_ib_get_aguid_comp_mask_from_ix(int index)\r\n{\r\nreturn IB_SA_COMP_MASK(4 + index);\r\n}\r\nvoid mlx4_ib_slave_alias_guid_event(struct mlx4_ib_dev *dev, int slave,\r\nint port, int slave_init)\r\n{\r\n__be64 curr_guid, required_guid;\r\nint record_num = slave / 8;\r\nint index = slave % 8;\r\nint port_index = port - 1;\r\nunsigned long flags;\r\nint do_work = 0;\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags);\r\nif (dev->sriov.alias_guid.ports_guid[port_index].state_flags &\r\nGUID_STATE_NEED_PORT_INIT)\r\ngoto unlock;\r\nif (!slave_init) {\r\ncurr_guid = *(__be64 *)&dev->sriov.\r\nalias_guid.ports_guid[port_index].\r\nall_rec_per_port[record_num].\r\nall_recs[GUID_REC_SIZE * index];\r\nif (curr_guid == cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL) ||\r\n!curr_guid)\r\ngoto unlock;\r\nrequired_guid = cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL);\r\n} else {\r\nrequired_guid = mlx4_get_admin_guid(dev->dev, slave, port);\r\nif (required_guid == cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL))\r\ngoto unlock;\r\n}\r\n*(__be64 *)&dev->sriov.alias_guid.ports_guid[port_index].\r\nall_rec_per_port[record_num].\r\nall_recs[GUID_REC_SIZE * index] = required_guid;\r\ndev->sriov.alias_guid.ports_guid[port_index].\r\nall_rec_per_port[record_num].guid_indexes\r\n|= mlx4_ib_get_aguid_comp_mask_from_ix(index);\r\ndev->sriov.alias_guid.ports_guid[port_index].\r\nall_rec_per_port[record_num].status\r\n= MLX4_GUID_INFO_STATUS_IDLE;\r\ndev->sriov.alias_guid.ports_guid[port_index].\r\nall_rec_per_port[record_num].time_to_run = 0;\r\ndev->sriov.alias_guid.ports_guid[port_index].\r\nall_rec_per_port[record_num].\r\nguids_retry_schedule[index] = 0;\r\ndo_work = 1;\r\nunlock:\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags);\r\nif (do_work)\r\nmlx4_ib_init_alias_guid_work(dev, port_index);\r\n}\r\nvoid mlx4_ib_notify_slaves_on_guid_change(struct mlx4_ib_dev *dev,\r\nint block_num, u8 port_num,\r\nu8 *p_data)\r\n{\r\nint i;\r\nu64 guid_indexes;\r\nint slave_id, slave_port;\r\nenum slave_port_state new_state;\r\nenum slave_port_state prev_state;\r\n__be64 tmp_cur_ag, form_cache_ag;\r\nenum slave_port_gen_event gen_event;\r\nstruct mlx4_sriov_alias_guid_info_rec_det *rec;\r\nunsigned long flags;\r\n__be64 required_value;\r\nif (!mlx4_is_master(dev->dev))\r\nreturn;\r\nrec = &dev->sriov.alias_guid.ports_guid[port_num - 1].\r\nall_rec_per_port[block_num];\r\nguid_indexes = be64_to_cpu((__force __be64) dev->sriov.alias_guid.\r\nports_guid[port_num - 1].\r\nall_rec_per_port[block_num].guid_indexes);\r\npr_debug("port: %d, guid_indexes: 0x%llx\n", port_num, guid_indexes);\r\nfor (i = 0; i < NUM_ALIAS_GUID_IN_REC; i++) {\r\nif (!(test_bit(i + 4, (unsigned long *)&guid_indexes)))\r\ncontinue;\r\nslave_id = (block_num * NUM_ALIAS_GUID_IN_REC) + i ;\r\nif (slave_id >= dev->dev->persist->num_vfs + 1)\r\nreturn;\r\nslave_port = mlx4_phys_to_slave_port(dev->dev, slave_id, port_num);\r\nif (slave_port < 0)\r\ncontinue;\r\ntmp_cur_ag = *(__be64 *)&p_data[i * GUID_REC_SIZE];\r\nform_cache_ag = get_cached_alias_guid(dev, port_num,\r\n(NUM_ALIAS_GUID_IN_REC * block_num) + i);\r\nif (tmp_cur_ag != form_cache_ag)\r\ncontinue;\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags);\r\nrequired_value = *(__be64 *)&rec->all_recs[i * GUID_REC_SIZE];\r\nif (required_value == cpu_to_be64(MLX4_GUID_FOR_DELETE_VAL))\r\nrequired_value = 0;\r\nif (tmp_cur_ag == required_value) {\r\nrec->guid_indexes = rec->guid_indexes &\r\n~mlx4_ib_get_aguid_comp_mask_from_ix(i);\r\n} else {\r\nif (tmp_cur_ag != MLX4_NOT_SET_GUID) {\r\nspin_unlock_irqrestore(&dev->sriov.\r\nalias_guid.ag_work_lock, flags);\r\ncontinue;\r\n}\r\n}\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock,\r\nflags);\r\nmlx4_gen_guid_change_eqe(dev->dev, slave_id, port_num);\r\nif (tmp_cur_ag != MLX4_NOT_SET_GUID) {\r\nprev_state = mlx4_get_slave_port_state(dev->dev, slave_id, port_num);\r\nnew_state = set_and_calc_slave_port_state(dev->dev, slave_id, port_num,\r\nMLX4_PORT_STATE_IB_PORT_STATE_EVENT_GID_VALID,\r\n&gen_event);\r\npr_debug("slave: %d, port: %d prev_port_state: %d,"\r\n" new_port_state: %d, gen_event: %d\n",\r\nslave_id, port_num, prev_state, new_state, gen_event);\r\nif (gen_event == SLAVE_PORT_GEN_EVENT_UP) {\r\npr_debug("sending PORT_UP event to slave: %d, port: %d\n",\r\nslave_id, port_num);\r\nmlx4_gen_port_state_change_eqe(dev->dev, slave_id,\r\nport_num, MLX4_PORT_CHANGE_SUBTYPE_ACTIVE);\r\n}\r\n} else {\r\nset_and_calc_slave_port_state(dev->dev, slave_id, port_num,\r\nMLX4_PORT_STATE_IB_EVENT_GID_INVALID,\r\n&gen_event);\r\nif (gen_event == SLAVE_PORT_GEN_EVENT_DOWN) {\r\npr_debug("sending PORT DOWN event to slave: %d, port: %d\n",\r\nslave_id, port_num);\r\nmlx4_gen_port_state_change_eqe(dev->dev,\r\nslave_id,\r\nport_num,\r\nMLX4_PORT_CHANGE_SUBTYPE_DOWN);\r\n}\r\n}\r\n}\r\n}\r\nstatic void aliasguid_query_handler(int status,\r\nstruct ib_sa_guidinfo_rec *guid_rec,\r\nvoid *context)\r\n{\r\nstruct mlx4_ib_dev *dev;\r\nstruct mlx4_alias_guid_work_context *cb_ctx = context;\r\nu8 port_index ;\r\nint i;\r\nstruct mlx4_sriov_alias_guid_info_rec_det *rec;\r\nunsigned long flags, flags1;\r\nib_sa_comp_mask declined_guid_indexes = 0;\r\nib_sa_comp_mask applied_guid_indexes = 0;\r\nunsigned int resched_delay_sec = 0;\r\nif (!context)\r\nreturn;\r\ndev = cb_ctx->dev;\r\nport_index = cb_ctx->port - 1;\r\nrec = &dev->sriov.alias_guid.ports_guid[port_index].\r\nall_rec_per_port[cb_ctx->block_num];\r\nif (status) {\r\npr_debug("(port: %d) failed: status = %d\n",\r\ncb_ctx->port, status);\r\nrec->time_to_run = ktime_get_boot_ns() + 1 * NSEC_PER_SEC;\r\ngoto out;\r\n}\r\nif (guid_rec->block_num != cb_ctx->block_num) {\r\npr_err("block num mismatch: %d != %d\n",\r\ncb_ctx->block_num, guid_rec->block_num);\r\ngoto out;\r\n}\r\npr_debug("lid/port: %d/%d, block_num: %d\n",\r\nbe16_to_cpu(guid_rec->lid), cb_ctx->port,\r\nguid_rec->block_num);\r\nrec = &dev->sriov.alias_guid.ports_guid[port_index].\r\nall_rec_per_port[guid_rec->block_num];\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags);\r\nfor (i = 0 ; i < NUM_ALIAS_GUID_IN_REC; i++) {\r\n__be64 sm_response, required_val;\r\nif (!(cb_ctx->guid_indexes &\r\nmlx4_ib_get_aguid_comp_mask_from_ix(i)))\r\ncontinue;\r\nsm_response = *(__be64 *)&guid_rec->guid_info_list\r\n[i * GUID_REC_SIZE];\r\nrequired_val = *(__be64 *)&rec->all_recs[i * GUID_REC_SIZE];\r\nif (cb_ctx->method == MLX4_GUID_INFO_RECORD_DELETE) {\r\nif (required_val ==\r\ncpu_to_be64(MLX4_GUID_FOR_DELETE_VAL))\r\ngoto next_entry;\r\npr_debug("need to set new value %llx, record num %d, block_num:%d\n",\r\nbe64_to_cpu(required_val),\r\ni, guid_rec->block_num);\r\ngoto entry_declined;\r\n}\r\nif (sm_response == MLX4_NOT_SET_GUID) {\r\nif (rec->guids_retry_schedule[i] == 0)\r\nmlx4_ib_warn(&dev->ib_dev,\r\n"%s:Record num %d in block_num: %d was declined by SM\n",\r\n__func__, i,\r\nguid_rec->block_num);\r\ngoto entry_declined;\r\n} else {\r\nif (required_val &&\r\nsm_response != required_val) {\r\nif (rec->guids_retry_schedule[i] == 0)\r\nmlx4_ib_warn(&dev->ib_dev, "%s: Failed to set"\r\n" admin guid after SysAdmin "\r\n"configuration. "\r\n"Record num %d in block_num:%d "\r\n"was declined by SM, "\r\n"new val(0x%llx) was kept, SM returned (0x%llx)\n",\r\n__func__, i,\r\nguid_rec->block_num,\r\nbe64_to_cpu(required_val),\r\nbe64_to_cpu(sm_response));\r\ngoto entry_declined;\r\n} else {\r\n*(__be64 *)&rec->all_recs[i * GUID_REC_SIZE] =\r\nsm_response;\r\nif (required_val == 0)\r\nmlx4_set_admin_guid(dev->dev,\r\nsm_response,\r\n(guid_rec->block_num\r\n* NUM_ALIAS_GUID_IN_REC) + i,\r\ncb_ctx->port);\r\ngoto next_entry;\r\n}\r\n}\r\nentry_declined:\r\ndeclined_guid_indexes |= mlx4_ib_get_aguid_comp_mask_from_ix(i);\r\nrec->guids_retry_schedule[i] =\r\n(rec->guids_retry_schedule[i] == 0) ? 1 :\r\nmin((unsigned int)60,\r\nrec->guids_retry_schedule[i] * 2);\r\nresched_delay_sec = (resched_delay_sec == 0) ?\r\nrec->guids_retry_schedule[i] :\r\nmin(resched_delay_sec,\r\nrec->guids_retry_schedule[i]);\r\ncontinue;\r\nnext_entry:\r\nrec->guids_retry_schedule[i] = 0;\r\n}\r\napplied_guid_indexes = cb_ctx->guid_indexes & ~declined_guid_indexes;\r\nif (declined_guid_indexes ||\r\nrec->guid_indexes & ~(applied_guid_indexes)) {\r\npr_debug("record=%d wasn't fully set, guid_indexes=0x%llx applied_indexes=0x%llx, declined_indexes=0x%llx\n",\r\nguid_rec->block_num,\r\nbe64_to_cpu((__force __be64)rec->guid_indexes),\r\nbe64_to_cpu((__force __be64)applied_guid_indexes),\r\nbe64_to_cpu((__force __be64)declined_guid_indexes));\r\nrec->time_to_run = ktime_get_boot_ns() +\r\nresched_delay_sec * NSEC_PER_SEC;\r\n} else {\r\nrec->status = MLX4_GUID_INFO_STATUS_SET;\r\n}\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags);\r\nmlx4_ib_notify_slaves_on_guid_change(dev, guid_rec->block_num,\r\ncb_ctx->port,\r\nguid_rec->guid_info_list);\r\nout:\r\nspin_lock_irqsave(&dev->sriov.going_down_lock, flags);\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nif (!dev->sriov.is_going_down) {\r\nget_low_record_time_index(dev, port_index, &resched_delay_sec);\r\nqueue_delayed_work(dev->sriov.alias_guid.ports_guid[port_index].wq,\r\n&dev->sriov.alias_guid.ports_guid[port_index].\r\nalias_guid_work,\r\nmsecs_to_jiffies(resched_delay_sec * 1000));\r\n}\r\nif (cb_ctx->sa_query) {\r\nlist_del(&cb_ctx->list);\r\nkfree(cb_ctx);\r\n} else\r\ncomplete(&cb_ctx->done);\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nspin_unlock_irqrestore(&dev->sriov.going_down_lock, flags);\r\n}\r\nstatic void invalidate_guid_record(struct mlx4_ib_dev *dev, u8 port, int index)\r\n{\r\nint i;\r\nu64 cur_admin_val;\r\nib_sa_comp_mask comp_mask = 0;\r\ndev->sriov.alias_guid.ports_guid[port - 1].all_rec_per_port[index].status\r\n= MLX4_GUID_INFO_STATUS_SET;\r\nfor (i = 0; i < NUM_ALIAS_GUID_IN_REC; i++) {\r\ncur_admin_val =\r\n*(u64 *)&dev->sriov.alias_guid.ports_guid[port - 1].\r\nall_rec_per_port[index].all_recs[GUID_REC_SIZE * i];\r\nif (MLX4_GUID_FOR_DELETE_VAL == cur_admin_val ||\r\n(!index && !i))\r\ncontinue;\r\ncomp_mask |= mlx4_ib_get_aguid_comp_mask_from_ix(i);\r\n}\r\ndev->sriov.alias_guid.ports_guid[port - 1].\r\nall_rec_per_port[index].guid_indexes |= comp_mask;\r\nif (dev->sriov.alias_guid.ports_guid[port - 1].\r\nall_rec_per_port[index].guid_indexes)\r\ndev->sriov.alias_guid.ports_guid[port - 1].\r\nall_rec_per_port[index].status = MLX4_GUID_INFO_STATUS_IDLE;\r\n}\r\nstatic int set_guid_rec(struct ib_device *ibdev,\r\nstruct mlx4_next_alias_guid_work *rec)\r\n{\r\nint err;\r\nstruct mlx4_ib_dev *dev = to_mdev(ibdev);\r\nstruct ib_sa_guidinfo_rec guid_info_rec;\r\nib_sa_comp_mask comp_mask;\r\nstruct ib_port_attr attr;\r\nstruct mlx4_alias_guid_work_context *callback_context;\r\nunsigned long resched_delay, flags, flags1;\r\nu8 port = rec->port + 1;\r\nint index = rec->block_num;\r\nstruct mlx4_sriov_alias_guid_info_rec_det *rec_det = &rec->rec_det;\r\nstruct list_head *head =\r\n&dev->sriov.alias_guid.ports_guid[port - 1].cb_list;\r\nmemset(&attr, 0, sizeof(attr));\r\nerr = __mlx4_ib_query_port(ibdev, port, &attr, 1);\r\nif (err) {\r\npr_debug("mlx4_ib_query_port failed (err: %d), port: %d\n",\r\nerr, port);\r\nreturn err;\r\n}\r\nif (attr.state != IB_PORT_ACTIVE) {\r\npr_debug("port %d not active...rescheduling\n", port);\r\nresched_delay = 5 * HZ;\r\nerr = -EAGAIN;\r\ngoto new_schedule;\r\n}\r\ncallback_context = kmalloc(sizeof *callback_context, GFP_KERNEL);\r\nif (!callback_context) {\r\nerr = -ENOMEM;\r\nresched_delay = HZ * 5;\r\ngoto new_schedule;\r\n}\r\ncallback_context->port = port;\r\ncallback_context->dev = dev;\r\ncallback_context->block_num = index;\r\ncallback_context->guid_indexes = rec_det->guid_indexes;\r\ncallback_context->method = rec->method;\r\nmemset(&guid_info_rec, 0, sizeof (struct ib_sa_guidinfo_rec));\r\nguid_info_rec.lid = cpu_to_be16(attr.lid);\r\nguid_info_rec.block_num = index;\r\nmemcpy(guid_info_rec.guid_info_list, rec_det->all_recs,\r\nGUID_REC_SIZE * NUM_ALIAS_GUID_IN_REC);\r\ncomp_mask = IB_SA_GUIDINFO_REC_LID | IB_SA_GUIDINFO_REC_BLOCK_NUM |\r\nrec_det->guid_indexes;\r\ninit_completion(&callback_context->done);\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nlist_add_tail(&callback_context->list, head);\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\ncallback_context->query_id =\r\nib_sa_guid_info_rec_query(dev->sriov.alias_guid.sa_client,\r\nibdev, port, &guid_info_rec,\r\ncomp_mask, rec->method, 1000,\r\nGFP_KERNEL, aliasguid_query_handler,\r\ncallback_context,\r\n&callback_context->sa_query);\r\nif (callback_context->query_id < 0) {\r\npr_debug("ib_sa_guid_info_rec_query failed, query_id: "\r\n"%d. will reschedule to the next 1 sec.\n",\r\ncallback_context->query_id);\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nlist_del(&callback_context->list);\r\nkfree(callback_context);\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nresched_delay = 1 * HZ;\r\nerr = -EAGAIN;\r\ngoto new_schedule;\r\n}\r\nerr = 0;\r\ngoto out;\r\nnew_schedule:\r\nspin_lock_irqsave(&dev->sriov.going_down_lock, flags);\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\ninvalidate_guid_record(dev, port, index);\r\nif (!dev->sriov.is_going_down) {\r\nqueue_delayed_work(dev->sriov.alias_guid.ports_guid[port - 1].wq,\r\n&dev->sriov.alias_guid.ports_guid[port - 1].alias_guid_work,\r\nresched_delay);\r\n}\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nspin_unlock_irqrestore(&dev->sriov.going_down_lock, flags);\r\nout:\r\nreturn err;\r\n}\r\nstatic void mlx4_ib_guid_port_init(struct mlx4_ib_dev *dev, int port)\r\n{\r\nint j, k, entry;\r\n__be64 guid;\r\nfor (j = 0; j < NUM_ALIAS_GUID_REC_IN_PORT; j++) {\r\nfor (k = 0; k < NUM_ALIAS_GUID_IN_REC; k++) {\r\nentry = j * NUM_ALIAS_GUID_IN_REC + k;\r\nif (!entry || entry > dev->dev->persist->num_vfs ||\r\n!mlx4_is_slave_active(dev->dev, entry))\r\ncontinue;\r\nguid = mlx4_get_admin_guid(dev->dev, entry, port);\r\n*(__be64 *)&dev->sriov.alias_guid.ports_guid[port - 1].\r\nall_rec_per_port[j].all_recs\r\n[GUID_REC_SIZE * k] = guid;\r\npr_debug("guid was set, entry=%d, val=0x%llx, port=%d\n",\r\nentry,\r\nbe64_to_cpu(guid),\r\nport);\r\n}\r\n}\r\n}\r\nvoid mlx4_ib_invalidate_all_guid_record(struct mlx4_ib_dev *dev, int port)\r\n{\r\nint i;\r\nunsigned long flags, flags1;\r\npr_debug("port %d\n", port);\r\nspin_lock_irqsave(&dev->sriov.going_down_lock, flags);\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nif (dev->sriov.alias_guid.ports_guid[port - 1].state_flags &\r\nGUID_STATE_NEED_PORT_INIT) {\r\nmlx4_ib_guid_port_init(dev, port);\r\ndev->sriov.alias_guid.ports_guid[port - 1].state_flags &=\r\n(~GUID_STATE_NEED_PORT_INIT);\r\n}\r\nfor (i = 0; i < NUM_ALIAS_GUID_REC_IN_PORT; i++)\r\ninvalidate_guid_record(dev, port, i);\r\nif (mlx4_is_master(dev->dev) && !dev->sriov.is_going_down) {\r\ncancel_delayed_work(&dev->sriov.alias_guid.\r\nports_guid[port - 1].alias_guid_work);\r\nqueue_delayed_work(dev->sriov.alias_guid.ports_guid[port - 1].wq,\r\n&dev->sriov.alias_guid.ports_guid[port - 1].alias_guid_work,\r\n0);\r\n}\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nspin_unlock_irqrestore(&dev->sriov.going_down_lock, flags);\r\n}\r\nstatic void set_required_record(struct mlx4_ib_dev *dev, u8 port,\r\nstruct mlx4_next_alias_guid_work *next_rec,\r\nint record_index)\r\n{\r\nint i;\r\nint lowset_time_entry = -1;\r\nint lowest_time = 0;\r\nib_sa_comp_mask delete_guid_indexes = 0;\r\nib_sa_comp_mask set_guid_indexes = 0;\r\nstruct mlx4_sriov_alias_guid_info_rec_det *rec =\r\n&dev->sriov.alias_guid.ports_guid[port].\r\nall_rec_per_port[record_index];\r\nfor (i = 0; i < NUM_ALIAS_GUID_IN_REC; i++) {\r\nif (!(rec->guid_indexes &\r\nmlx4_ib_get_aguid_comp_mask_from_ix(i)))\r\ncontinue;\r\nif (*(__be64 *)&rec->all_recs[i * GUID_REC_SIZE] ==\r\ncpu_to_be64(MLX4_GUID_FOR_DELETE_VAL))\r\ndelete_guid_indexes |=\r\nmlx4_ib_get_aguid_comp_mask_from_ix(i);\r\nelse\r\nset_guid_indexes |=\r\nmlx4_ib_get_aguid_comp_mask_from_ix(i);\r\nif (lowset_time_entry == -1 || rec->guids_retry_schedule[i] <=\r\nlowest_time) {\r\nlowset_time_entry = i;\r\nlowest_time = rec->guids_retry_schedule[i];\r\n}\r\n}\r\nmemcpy(&next_rec->rec_det, rec, sizeof(*rec));\r\nnext_rec->port = port;\r\nnext_rec->block_num = record_index;\r\nif (*(__be64 *)&rec->all_recs[lowset_time_entry * GUID_REC_SIZE] ==\r\ncpu_to_be64(MLX4_GUID_FOR_DELETE_VAL)) {\r\nnext_rec->rec_det.guid_indexes = delete_guid_indexes;\r\nnext_rec->method = MLX4_GUID_INFO_RECORD_DELETE;\r\n} else {\r\nnext_rec->rec_det.guid_indexes = set_guid_indexes;\r\nnext_rec->method = MLX4_GUID_INFO_RECORD_SET;\r\n}\r\n}\r\nstatic int get_low_record_time_index(struct mlx4_ib_dev *dev, u8 port,\r\nint *resched_delay_sec)\r\n{\r\nint record_index = -1;\r\nu64 low_record_time = 0;\r\nstruct mlx4_sriov_alias_guid_info_rec_det rec;\r\nint j;\r\nfor (j = 0; j < NUM_ALIAS_GUID_REC_IN_PORT; j++) {\r\nrec = dev->sriov.alias_guid.ports_guid[port].\r\nall_rec_per_port[j];\r\nif (rec.status == MLX4_GUID_INFO_STATUS_IDLE &&\r\nrec.guid_indexes) {\r\nif (record_index == -1 ||\r\nrec.time_to_run < low_record_time) {\r\nrecord_index = j;\r\nlow_record_time = rec.time_to_run;\r\n}\r\n}\r\n}\r\nif (resched_delay_sec) {\r\nu64 curr_time = ktime_get_boot_ns();\r\n*resched_delay_sec = (low_record_time < curr_time) ? 0 :\r\ndiv_u64((low_record_time - curr_time), NSEC_PER_SEC);\r\n}\r\nreturn record_index;\r\n}\r\nstatic int get_next_record_to_update(struct mlx4_ib_dev *dev, u8 port,\r\nstruct mlx4_next_alias_guid_work *rec)\r\n{\r\nunsigned long flags;\r\nint record_index;\r\nint ret = 0;\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags);\r\nrecord_index = get_low_record_time_index(dev, port, NULL);\r\nif (record_index < 0) {\r\nret = -ENOENT;\r\ngoto out;\r\n}\r\nset_required_record(dev, port, rec, record_index);\r\nout:\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags);\r\nreturn ret;\r\n}\r\nstatic void alias_guid_work(struct work_struct *work)\r\n{\r\nstruct delayed_work *delay = to_delayed_work(work);\r\nint ret = 0;\r\nstruct mlx4_next_alias_guid_work *rec;\r\nstruct mlx4_sriov_alias_guid_port_rec_det *sriov_alias_port =\r\ncontainer_of(delay, struct mlx4_sriov_alias_guid_port_rec_det,\r\nalias_guid_work);\r\nstruct mlx4_sriov_alias_guid *sriov_alias_guid = sriov_alias_port->parent;\r\nstruct mlx4_ib_sriov *ib_sriov = container_of(sriov_alias_guid,\r\nstruct mlx4_ib_sriov,\r\nalias_guid);\r\nstruct mlx4_ib_dev *dev = container_of(ib_sriov, struct mlx4_ib_dev, sriov);\r\nrec = kzalloc(sizeof *rec, GFP_KERNEL);\r\nif (!rec)\r\nreturn;\r\npr_debug("starting [port: %d]...\n", sriov_alias_port->port + 1);\r\nret = get_next_record_to_update(dev, sriov_alias_port->port, rec);\r\nif (ret) {\r\npr_debug("No more records to update.\n");\r\ngoto out;\r\n}\r\nset_guid_rec(&dev->ib_dev, rec);\r\nout:\r\nkfree(rec);\r\n}\r\nvoid mlx4_ib_init_alias_guid_work(struct mlx4_ib_dev *dev, int port)\r\n{\r\nunsigned long flags, flags1;\r\nif (!mlx4_is_master(dev->dev))\r\nreturn;\r\nspin_lock_irqsave(&dev->sriov.going_down_lock, flags);\r\nspin_lock_irqsave(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nif (!dev->sriov.is_going_down) {\r\ncancel_delayed_work(&dev->sriov.alias_guid.ports_guid[port].\r\nalias_guid_work);\r\nqueue_delayed_work(dev->sriov.alias_guid.ports_guid[port].wq,\r\n&dev->sriov.alias_guid.ports_guid[port].alias_guid_work, 0);\r\n}\r\nspin_unlock_irqrestore(&dev->sriov.alias_guid.ag_work_lock, flags1);\r\nspin_unlock_irqrestore(&dev->sriov.going_down_lock, flags);\r\n}\r\nvoid mlx4_ib_destroy_alias_guid_service(struct mlx4_ib_dev *dev)\r\n{\r\nint i;\r\nstruct mlx4_ib_sriov *sriov = &dev->sriov;\r\nstruct mlx4_alias_guid_work_context *cb_ctx;\r\nstruct mlx4_sriov_alias_guid_port_rec_det *det;\r\nstruct ib_sa_query *sa_query;\r\nunsigned long flags;\r\nfor (i = 0 ; i < dev->num_ports; i++) {\r\ncancel_delayed_work(&dev->sriov.alias_guid.ports_guid[i].alias_guid_work);\r\ndet = &sriov->alias_guid.ports_guid[i];\r\nspin_lock_irqsave(&sriov->alias_guid.ag_work_lock, flags);\r\nwhile (!list_empty(&det->cb_list)) {\r\ncb_ctx = list_entry(det->cb_list.next,\r\nstruct mlx4_alias_guid_work_context,\r\nlist);\r\nsa_query = cb_ctx->sa_query;\r\ncb_ctx->sa_query = NULL;\r\nlist_del(&cb_ctx->list);\r\nspin_unlock_irqrestore(&sriov->alias_guid.ag_work_lock, flags);\r\nib_sa_cancel_query(cb_ctx->query_id, sa_query);\r\nwait_for_completion(&cb_ctx->done);\r\nkfree(cb_ctx);\r\nspin_lock_irqsave(&sriov->alias_guid.ag_work_lock, flags);\r\n}\r\nspin_unlock_irqrestore(&sriov->alias_guid.ag_work_lock, flags);\r\n}\r\nfor (i = 0 ; i < dev->num_ports; i++) {\r\nflush_workqueue(dev->sriov.alias_guid.ports_guid[i].wq);\r\ndestroy_workqueue(dev->sriov.alias_guid.ports_guid[i].wq);\r\n}\r\nib_sa_unregister_client(dev->sriov.alias_guid.sa_client);\r\nkfree(dev->sriov.alias_guid.sa_client);\r\n}\r\nint mlx4_ib_init_alias_guid_service(struct mlx4_ib_dev *dev)\r\n{\r\nchar alias_wq_name[15];\r\nint ret = 0;\r\nint i, j;\r\nunion ib_gid gid;\r\nif (!mlx4_is_master(dev->dev))\r\nreturn 0;\r\ndev->sriov.alias_guid.sa_client =\r\nkzalloc(sizeof *dev->sriov.alias_guid.sa_client, GFP_KERNEL);\r\nif (!dev->sriov.alias_guid.sa_client)\r\nreturn -ENOMEM;\r\nib_sa_register_client(dev->sriov.alias_guid.sa_client);\r\nspin_lock_init(&dev->sriov.alias_guid.ag_work_lock);\r\nfor (i = 1; i <= dev->num_ports; ++i) {\r\nif (dev->ib_dev.query_gid(&dev->ib_dev , i, 0, &gid)) {\r\nret = -EFAULT;\r\ngoto err_unregister;\r\n}\r\n}\r\nfor (i = 0 ; i < dev->num_ports; i++) {\r\nmemset(&dev->sriov.alias_guid.ports_guid[i], 0,\r\nsizeof (struct mlx4_sriov_alias_guid_port_rec_det));\r\ndev->sriov.alias_guid.ports_guid[i].state_flags |=\r\nGUID_STATE_NEED_PORT_INIT;\r\nfor (j = 0; j < NUM_ALIAS_GUID_REC_IN_PORT; j++) {\r\nmemset(dev->sriov.alias_guid.ports_guid[i].\r\nall_rec_per_port[j].all_recs, 0xFF,\r\nsizeof(dev->sriov.alias_guid.ports_guid[i].\r\nall_rec_per_port[j].all_recs));\r\n}\r\nINIT_LIST_HEAD(&dev->sriov.alias_guid.ports_guid[i].cb_list);\r\nif (mlx4_ib_sm_guid_assign)\r\nfor (j = 1; j < NUM_ALIAS_GUID_PER_PORT; j++)\r\nmlx4_set_admin_guid(dev->dev, 0, j, i + 1);\r\nfor (j = 0 ; j < NUM_ALIAS_GUID_REC_IN_PORT; j++)\r\ninvalidate_guid_record(dev, i + 1, j);\r\ndev->sriov.alias_guid.ports_guid[i].parent = &dev->sriov.alias_guid;\r\ndev->sriov.alias_guid.ports_guid[i].port = i;\r\nsnprintf(alias_wq_name, sizeof alias_wq_name, "alias_guid%d", i);\r\ndev->sriov.alias_guid.ports_guid[i].wq =\r\nalloc_ordered_workqueue(alias_wq_name, WQ_MEM_RECLAIM);\r\nif (!dev->sriov.alias_guid.ports_guid[i].wq) {\r\nret = -ENOMEM;\r\ngoto err_thread;\r\n}\r\nINIT_DELAYED_WORK(&dev->sriov.alias_guid.ports_guid[i].alias_guid_work,\r\nalias_guid_work);\r\n}\r\nreturn 0;\r\nerr_thread:\r\nfor (--i; i >= 0; i--) {\r\ndestroy_workqueue(dev->sriov.alias_guid.ports_guid[i].wq);\r\ndev->sriov.alias_guid.ports_guid[i].wq = NULL;\r\n}\r\nerr_unregister:\r\nib_sa_unregister_client(dev->sriov.alias_guid.sa_client);\r\nkfree(dev->sriov.alias_guid.sa_client);\r\ndev->sriov.alias_guid.sa_client = NULL;\r\npr_err("init_alias_guid_service: Failed. (ret:%d)\n", ret);\r\nreturn ret;\r\n}
