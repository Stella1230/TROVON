static int early_page_owner_param(char *buf)\r\n{\r\nif (!buf)\r\nreturn -EINVAL;\r\nif (strcmp(buf, "on") == 0)\r\npage_owner_disabled = false;\r\nreturn 0;\r\n}\r\nstatic bool need_page_owner(void)\r\n{\r\nif (page_owner_disabled)\r\nreturn false;\r\nreturn true;\r\n}\r\nstatic noinline void register_dummy_stack(void)\r\n{\r\nunsigned long entries[4];\r\nstruct stack_trace dummy;\r\ndummy.nr_entries = 0;\r\ndummy.max_entries = ARRAY_SIZE(entries);\r\ndummy.entries = &entries[0];\r\ndummy.skip = 0;\r\nsave_stack_trace(&dummy);\r\ndummy_handle = depot_save_stack(&dummy, GFP_KERNEL);\r\n}\r\nstatic noinline void register_failure_stack(void)\r\n{\r\nunsigned long entries[4];\r\nstruct stack_trace failure;\r\nfailure.nr_entries = 0;\r\nfailure.max_entries = ARRAY_SIZE(entries);\r\nfailure.entries = &entries[0];\r\nfailure.skip = 0;\r\nsave_stack_trace(&failure);\r\nfailure_handle = depot_save_stack(&failure, GFP_KERNEL);\r\n}\r\nstatic void init_page_owner(void)\r\n{\r\nif (page_owner_disabled)\r\nreturn;\r\nregister_dummy_stack();\r\nregister_failure_stack();\r\nstatic_branch_enable(&page_owner_inited);\r\ninit_early_allocated_pages();\r\n}\r\nstatic inline struct page_owner *get_page_owner(struct page_ext *page_ext)\r\n{\r\nreturn (void *)page_ext + page_owner_ops.offset;\r\n}\r\nvoid __reset_page_owner(struct page *page, unsigned int order)\r\n{\r\nint i;\r\nstruct page_ext *page_ext;\r\nfor (i = 0; i < (1 << order); i++) {\r\npage_ext = lookup_page_ext(page + i);\r\nif (unlikely(!page_ext))\r\ncontinue;\r\n__clear_bit(PAGE_EXT_OWNER, &page_ext->flags);\r\n}\r\n}\r\nstatic inline bool check_recursive_alloc(struct stack_trace *trace,\r\nunsigned long ip)\r\n{\r\nint i, count;\r\nif (!trace->nr_entries)\r\nreturn false;\r\nfor (i = 0, count = 0; i < trace->nr_entries; i++) {\r\nif (trace->entries[i] == ip && ++count == 2)\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic noinline depot_stack_handle_t save_stack(gfp_t flags)\r\n{\r\nunsigned long entries[PAGE_OWNER_STACK_DEPTH];\r\nstruct stack_trace trace = {\r\n.nr_entries = 0,\r\n.entries = entries,\r\n.max_entries = PAGE_OWNER_STACK_DEPTH,\r\n.skip = 0\r\n};\r\ndepot_stack_handle_t handle;\r\nsave_stack_trace(&trace);\r\nif (trace.nr_entries != 0 &&\r\ntrace.entries[trace.nr_entries-1] == ULONG_MAX)\r\ntrace.nr_entries--;\r\nif (check_recursive_alloc(&trace, _RET_IP_))\r\nreturn dummy_handle;\r\nhandle = depot_save_stack(&trace, flags);\r\nif (!handle)\r\nhandle = failure_handle;\r\nreturn handle;\r\n}\r\nnoinline void __set_page_owner(struct page *page, unsigned int order,\r\ngfp_t gfp_mask)\r\n{\r\nstruct page_ext *page_ext = lookup_page_ext(page);\r\nstruct page_owner *page_owner;\r\nif (unlikely(!page_ext))\r\nreturn;\r\npage_owner = get_page_owner(page_ext);\r\npage_owner->handle = save_stack(gfp_mask);\r\npage_owner->order = order;\r\npage_owner->gfp_mask = gfp_mask;\r\npage_owner->last_migrate_reason = -1;\r\n__set_bit(PAGE_EXT_OWNER, &page_ext->flags);\r\n}\r\nvoid __set_page_owner_migrate_reason(struct page *page, int reason)\r\n{\r\nstruct page_ext *page_ext = lookup_page_ext(page);\r\nstruct page_owner *page_owner;\r\nif (unlikely(!page_ext))\r\nreturn;\r\npage_owner = get_page_owner(page_ext);\r\npage_owner->last_migrate_reason = reason;\r\n}\r\nvoid __split_page_owner(struct page *page, unsigned int order)\r\n{\r\nint i;\r\nstruct page_ext *page_ext = lookup_page_ext(page);\r\nstruct page_owner *page_owner;\r\nif (unlikely(!page_ext))\r\nreturn;\r\npage_owner = get_page_owner(page_ext);\r\npage_owner->order = 0;\r\nfor (i = 1; i < (1 << order); i++)\r\n__copy_page_owner(page, page + i);\r\n}\r\nvoid __copy_page_owner(struct page *oldpage, struct page *newpage)\r\n{\r\nstruct page_ext *old_ext = lookup_page_ext(oldpage);\r\nstruct page_ext *new_ext = lookup_page_ext(newpage);\r\nstruct page_owner *old_page_owner, *new_page_owner;\r\nif (unlikely(!old_ext || !new_ext))\r\nreturn;\r\nold_page_owner = get_page_owner(old_ext);\r\nnew_page_owner = get_page_owner(new_ext);\r\nnew_page_owner->order = old_page_owner->order;\r\nnew_page_owner->gfp_mask = old_page_owner->gfp_mask;\r\nnew_page_owner->last_migrate_reason =\r\nold_page_owner->last_migrate_reason;\r\nnew_page_owner->handle = old_page_owner->handle;\r\n__set_bit(PAGE_EXT_OWNER, &new_ext->flags);\r\n}\r\nvoid pagetypeinfo_showmixedcount_print(struct seq_file *m,\r\npg_data_t *pgdat, struct zone *zone)\r\n{\r\nstruct page *page;\r\nstruct page_ext *page_ext;\r\nstruct page_owner *page_owner;\r\nunsigned long pfn = zone->zone_start_pfn, block_end_pfn;\r\nunsigned long end_pfn = pfn + zone->spanned_pages;\r\nunsigned long count[MIGRATE_TYPES] = { 0, };\r\nint pageblock_mt, page_mt;\r\nint i;\r\npfn = zone->zone_start_pfn;\r\nfor (; pfn < end_pfn; ) {\r\nif (!pfn_valid(pfn)) {\r\npfn = ALIGN(pfn + 1, MAX_ORDER_NR_PAGES);\r\ncontinue;\r\n}\r\nblock_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nblock_end_pfn = min(block_end_pfn, end_pfn);\r\npage = pfn_to_page(pfn);\r\npageblock_mt = get_pageblock_migratetype(page);\r\nfor (; pfn < block_end_pfn; pfn++) {\r\nif (!pfn_valid_within(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (page_zone(page) != zone)\r\ncontinue;\r\nif (PageBuddy(page)) {\r\npfn += (1UL << page_order(page)) - 1;\r\ncontinue;\r\n}\r\nif (PageReserved(page))\r\ncontinue;\r\npage_ext = lookup_page_ext(page);\r\nif (unlikely(!page_ext))\r\ncontinue;\r\nif (!test_bit(PAGE_EXT_OWNER, &page_ext->flags))\r\ncontinue;\r\npage_owner = get_page_owner(page_ext);\r\npage_mt = gfpflags_to_migratetype(\r\npage_owner->gfp_mask);\r\nif (pageblock_mt != page_mt) {\r\nif (is_migrate_cma(pageblock_mt))\r\ncount[MIGRATE_MOVABLE]++;\r\nelse\r\ncount[pageblock_mt]++;\r\npfn = block_end_pfn;\r\nbreak;\r\n}\r\npfn += (1UL << page_owner->order) - 1;\r\n}\r\n}\r\nseq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);\r\nfor (i = 0; i < MIGRATE_TYPES; i++)\r\nseq_printf(m, "%12lu ", count[i]);\r\nseq_putc(m, '\n');\r\n}\r\nstatic ssize_t\r\nprint_page_owner(char __user *buf, size_t count, unsigned long pfn,\r\nstruct page *page, struct page_owner *page_owner,\r\ndepot_stack_handle_t handle)\r\n{\r\nint ret;\r\nint pageblock_mt, page_mt;\r\nchar *kbuf;\r\nunsigned long entries[PAGE_OWNER_STACK_DEPTH];\r\nstruct stack_trace trace = {\r\n.nr_entries = 0,\r\n.entries = entries,\r\n.max_entries = PAGE_OWNER_STACK_DEPTH,\r\n.skip = 0\r\n};\r\nkbuf = kmalloc(count, GFP_KERNEL);\r\nif (!kbuf)\r\nreturn -ENOMEM;\r\nret = snprintf(kbuf, count,\r\n"Page allocated via order %u, mask %#x(%pGg)\n",\r\npage_owner->order, page_owner->gfp_mask,\r\n&page_owner->gfp_mask);\r\nif (ret >= count)\r\ngoto err;\r\npageblock_mt = get_pageblock_migratetype(page);\r\npage_mt = gfpflags_to_migratetype(page_owner->gfp_mask);\r\nret += snprintf(kbuf + ret, count - ret,\r\n"PFN %lu type %s Block %lu type %s Flags %#lx(%pGp)\n",\r\npfn,\r\nmigratetype_names[page_mt],\r\npfn >> pageblock_order,\r\nmigratetype_names[pageblock_mt],\r\npage->flags, &page->flags);\r\nif (ret >= count)\r\ngoto err;\r\ndepot_fetch_stack(handle, &trace);\r\nret += snprint_stack_trace(kbuf + ret, count - ret, &trace, 0);\r\nif (ret >= count)\r\ngoto err;\r\nif (page_owner->last_migrate_reason != -1) {\r\nret += snprintf(kbuf + ret, count - ret,\r\n"Page has been migrated, last migrate reason: %s\n",\r\nmigrate_reason_names[page_owner->last_migrate_reason]);\r\nif (ret >= count)\r\ngoto err;\r\n}\r\nret += snprintf(kbuf + ret, count - ret, "\n");\r\nif (ret >= count)\r\ngoto err;\r\nif (copy_to_user(buf, kbuf, ret))\r\nret = -EFAULT;\r\nkfree(kbuf);\r\nreturn ret;\r\nerr:\r\nkfree(kbuf);\r\nreturn -ENOMEM;\r\n}\r\nvoid __dump_page_owner(struct page *page)\r\n{\r\nstruct page_ext *page_ext = lookup_page_ext(page);\r\nstruct page_owner *page_owner;\r\nunsigned long entries[PAGE_OWNER_STACK_DEPTH];\r\nstruct stack_trace trace = {\r\n.nr_entries = 0,\r\n.entries = entries,\r\n.max_entries = PAGE_OWNER_STACK_DEPTH,\r\n.skip = 0\r\n};\r\ndepot_stack_handle_t handle;\r\ngfp_t gfp_mask;\r\nint mt;\r\nif (unlikely(!page_ext)) {\r\npr_alert("There is not page extension available.\n");\r\nreturn;\r\n}\r\npage_owner = get_page_owner(page_ext);\r\ngfp_mask = page_owner->gfp_mask;\r\nmt = gfpflags_to_migratetype(gfp_mask);\r\nif (!test_bit(PAGE_EXT_OWNER, &page_ext->flags)) {\r\npr_alert("page_owner info is not active (free page?)\n");\r\nreturn;\r\n}\r\nhandle = READ_ONCE(page_owner->handle);\r\nif (!handle) {\r\npr_alert("page_owner info is not active (free page?)\n");\r\nreturn;\r\n}\r\ndepot_fetch_stack(handle, &trace);\r\npr_alert("page allocated via order %u, migratetype %s, gfp_mask %#x(%pGg)\n",\r\npage_owner->order, migratetype_names[mt], gfp_mask, &gfp_mask);\r\nprint_stack_trace(&trace, 0);\r\nif (page_owner->last_migrate_reason != -1)\r\npr_alert("page has been migrated, last migrate reason: %s\n",\r\nmigrate_reason_names[page_owner->last_migrate_reason]);\r\n}\r\nstatic ssize_t\r\nread_page_owner(struct file *file, char __user *buf, size_t count, loff_t *ppos)\r\n{\r\nunsigned long pfn;\r\nstruct page *page;\r\nstruct page_ext *page_ext;\r\nstruct page_owner *page_owner;\r\ndepot_stack_handle_t handle;\r\nif (!static_branch_unlikely(&page_owner_inited))\r\nreturn -EINVAL;\r\npage = NULL;\r\npfn = min_low_pfn + *ppos;\r\nwhile (!pfn_valid(pfn) && (pfn & (MAX_ORDER_NR_PAGES - 1)) != 0)\r\npfn++;\r\ndrain_all_pages(NULL);\r\nfor (; pfn < max_pfn; pfn++) {\r\nif ((pfn & (MAX_ORDER_NR_PAGES - 1)) == 0 && !pfn_valid(pfn)) {\r\npfn += MAX_ORDER_NR_PAGES - 1;\r\ncontinue;\r\n}\r\nif (!pfn_valid_within(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (PageBuddy(page)) {\r\nunsigned long freepage_order = page_order_unsafe(page);\r\nif (freepage_order < MAX_ORDER)\r\npfn += (1UL << freepage_order) - 1;\r\ncontinue;\r\n}\r\npage_ext = lookup_page_ext(page);\r\nif (unlikely(!page_ext))\r\ncontinue;\r\nif (!test_bit(PAGE_EXT_OWNER, &page_ext->flags))\r\ncontinue;\r\npage_owner = get_page_owner(page_ext);\r\nhandle = READ_ONCE(page_owner->handle);\r\nif (!handle)\r\ncontinue;\r\n*ppos = (pfn - min_low_pfn) + 1;\r\nreturn print_page_owner(buf, count, pfn, page,\r\npage_owner, handle);\r\n}\r\nreturn 0;\r\n}\r\nstatic void init_pages_in_zone(pg_data_t *pgdat, struct zone *zone)\r\n{\r\nstruct page *page;\r\nstruct page_ext *page_ext;\r\nunsigned long pfn = zone->zone_start_pfn, block_end_pfn;\r\nunsigned long end_pfn = pfn + zone->spanned_pages;\r\nunsigned long count = 0;\r\npfn = zone->zone_start_pfn;\r\nfor (; pfn < end_pfn; ) {\r\nif (!pfn_valid(pfn)) {\r\npfn = ALIGN(pfn + 1, MAX_ORDER_NR_PAGES);\r\ncontinue;\r\n}\r\nblock_end_pfn = ALIGN(pfn + 1, pageblock_nr_pages);\r\nblock_end_pfn = min(block_end_pfn, end_pfn);\r\npage = pfn_to_page(pfn);\r\nfor (; pfn < block_end_pfn; pfn++) {\r\nif (!pfn_valid_within(pfn))\r\ncontinue;\r\npage = pfn_to_page(pfn);\r\nif (page_zone(page) != zone)\r\ncontinue;\r\nif (PageBuddy(page)) {\r\npfn += (1UL << page_order(page)) - 1;\r\ncontinue;\r\n}\r\nif (PageReserved(page))\r\ncontinue;\r\npage_ext = lookup_page_ext(page);\r\nif (unlikely(!page_ext))\r\ncontinue;\r\nif (test_bit(PAGE_EXT_OWNER, &page_ext->flags))\r\ncontinue;\r\nset_page_owner(page, 0, 0);\r\ncount++;\r\n}\r\n}\r\npr_info("Node %d, zone %8s: page owner found early allocated %lu pages\n",\r\npgdat->node_id, zone->name, count);\r\n}\r\nstatic void init_zones_in_node(pg_data_t *pgdat)\r\n{\r\nstruct zone *zone;\r\nstruct zone *node_zones = pgdat->node_zones;\r\nunsigned long flags;\r\nfor (zone = node_zones; zone - node_zones < MAX_NR_ZONES; ++zone) {\r\nif (!populated_zone(zone))\r\ncontinue;\r\nspin_lock_irqsave(&zone->lock, flags);\r\ninit_pages_in_zone(pgdat, zone);\r\nspin_unlock_irqrestore(&zone->lock, flags);\r\n}\r\n}\r\nstatic void init_early_allocated_pages(void)\r\n{\r\npg_data_t *pgdat;\r\ndrain_all_pages(NULL);\r\nfor_each_online_pgdat(pgdat)\r\ninit_zones_in_node(pgdat);\r\n}\r\nstatic int __init pageowner_init(void)\r\n{\r\nstruct dentry *dentry;\r\nif (!static_branch_unlikely(&page_owner_inited)) {\r\npr_info("page_owner is disabled\n");\r\nreturn 0;\r\n}\r\ndentry = debugfs_create_file("page_owner", S_IRUSR, NULL,\r\nNULL, &proc_page_owner_operations);\r\nif (IS_ERR(dentry))\r\nreturn PTR_ERR(dentry);\r\nreturn 0;\r\n}
