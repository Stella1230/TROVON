void _cxgbit_free_cdev(struct kref *kref)\r\n{\r\nstruct cxgbit_device *cdev;\r\ncdev = container_of(kref, struct cxgbit_device, kref);\r\ncxgbi_ppm_release(cdev2ppm(cdev));\r\nkfree(cdev);\r\n}\r\nstatic void cxgbit_set_mdsl(struct cxgbit_device *cdev)\r\n{\r\nstruct cxgb4_lld_info *lldi = &cdev->lldi;\r\nu32 mdsl;\r\n#define ULP2_MAX_PKT_LEN 16224\r\n#define ISCSI_PDU_NONPAYLOAD_LEN 312\r\nmdsl = min_t(u32, lldi->iscsi_iolen - ISCSI_PDU_NONPAYLOAD_LEN,\r\nULP2_MAX_PKT_LEN - ISCSI_PDU_NONPAYLOAD_LEN);\r\nmdsl = min_t(u32, mdsl, 8192);\r\nmdsl = min_t(u32, mdsl, (MAX_SKB_FRAGS - 1) * PAGE_SIZE);\r\ncdev->mdsl = mdsl;\r\n}\r\nstatic void *cxgbit_uld_add(const struct cxgb4_lld_info *lldi)\r\n{\r\nstruct cxgbit_device *cdev;\r\nif (is_t4(lldi->adapter_type))\r\nreturn ERR_PTR(-ENODEV);\r\ncdev = kzalloc(sizeof(*cdev), GFP_KERNEL);\r\nif (!cdev)\r\nreturn ERR_PTR(-ENOMEM);\r\nkref_init(&cdev->kref);\r\ncdev->lldi = *lldi;\r\ncxgbit_set_mdsl(cdev);\r\nif (cxgbit_ddp_init(cdev) < 0) {\r\nkfree(cdev);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nif (!test_bit(CDEV_DDP_ENABLE, &cdev->flags))\r\npr_info("cdev %s ddp init failed\n",\r\npci_name(lldi->pdev));\r\nif (lldi->fw_vers >= 0x10d2b00)\r\nset_bit(CDEV_ISO_ENABLE, &cdev->flags);\r\nspin_lock_init(&cdev->cskq.lock);\r\nINIT_LIST_HEAD(&cdev->cskq.list);\r\nmutex_lock(&cdev_list_lock);\r\nlist_add_tail(&cdev->list, &cdev_list_head);\r\nmutex_unlock(&cdev_list_lock);\r\npr_info("cdev %s added for iSCSI target transport\n",\r\npci_name(lldi->pdev));\r\nreturn cdev;\r\n}\r\nstatic void cxgbit_close_conn(struct cxgbit_device *cdev)\r\n{\r\nstruct cxgbit_sock *csk;\r\nstruct sk_buff *skb;\r\nbool wakeup_thread = false;\r\nspin_lock_bh(&cdev->cskq.lock);\r\nlist_for_each_entry(csk, &cdev->cskq.list, list) {\r\nskb = alloc_skb(0, GFP_ATOMIC);\r\nif (!skb)\r\ncontinue;\r\nspin_lock_bh(&csk->rxq.lock);\r\n__skb_queue_tail(&csk->rxq, skb);\r\nif (skb_queue_len(&csk->rxq) == 1)\r\nwakeup_thread = true;\r\nspin_unlock_bh(&csk->rxq.lock);\r\nif (wakeup_thread) {\r\nwake_up(&csk->waitq);\r\nwakeup_thread = false;\r\n}\r\n}\r\nspin_unlock_bh(&cdev->cskq.lock);\r\n}\r\nstatic void cxgbit_detach_cdev(struct cxgbit_device *cdev)\r\n{\r\nbool free_cdev = false;\r\nspin_lock_bh(&cdev->cskq.lock);\r\nif (list_empty(&cdev->cskq.list))\r\nfree_cdev = true;\r\nspin_unlock_bh(&cdev->cskq.lock);\r\nif (free_cdev) {\r\nmutex_lock(&cdev_list_lock);\r\nlist_del(&cdev->list);\r\nmutex_unlock(&cdev_list_lock);\r\ncxgbit_put_cdev(cdev);\r\n} else {\r\ncxgbit_close_conn(cdev);\r\n}\r\n}\r\nstatic int cxgbit_uld_state_change(void *handle, enum cxgb4_state state)\r\n{\r\nstruct cxgbit_device *cdev = handle;\r\nswitch (state) {\r\ncase CXGB4_STATE_UP:\r\nset_bit(CDEV_STATE_UP, &cdev->flags);\r\npr_info("cdev %s state UP.\n", pci_name(cdev->lldi.pdev));\r\nbreak;\r\ncase CXGB4_STATE_START_RECOVERY:\r\nclear_bit(CDEV_STATE_UP, &cdev->flags);\r\ncxgbit_close_conn(cdev);\r\npr_info("cdev %s state RECOVERY.\n", pci_name(cdev->lldi.pdev));\r\nbreak;\r\ncase CXGB4_STATE_DOWN:\r\npr_info("cdev %s state DOWN.\n", pci_name(cdev->lldi.pdev));\r\nbreak;\r\ncase CXGB4_STATE_DETACH:\r\nclear_bit(CDEV_STATE_UP, &cdev->flags);\r\npr_info("cdev %s state DETACH.\n", pci_name(cdev->lldi.pdev));\r\ncxgbit_detach_cdev(cdev);\r\nbreak;\r\ndefault:\r\npr_info("cdev %s unknown state %d.\n",\r\npci_name(cdev->lldi.pdev), state);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic void\r\ncxgbit_process_ddpvld(struct cxgbit_sock *csk, struct cxgbit_lro_pdu_cb *pdu_cb,\r\nu32 ddpvld)\r\n{\r\nif (ddpvld & (1 << CPL_RX_ISCSI_DDP_STATUS_HCRC_SHIFT)) {\r\npr_info("tid 0x%x, status 0x%x, hcrc bad.\n", csk->tid, ddpvld);\r\npdu_cb->flags |= PDUCBF_RX_HCRC_ERR;\r\n}\r\nif (ddpvld & (1 << CPL_RX_ISCSI_DDP_STATUS_DCRC_SHIFT)) {\r\npr_info("tid 0x%x, status 0x%x, dcrc bad.\n", csk->tid, ddpvld);\r\npdu_cb->flags |= PDUCBF_RX_DCRC_ERR;\r\n}\r\nif (ddpvld & (1 << CPL_RX_ISCSI_DDP_STATUS_PAD_SHIFT))\r\npr_info("tid 0x%x, status 0x%x, pad bad.\n", csk->tid, ddpvld);\r\nif ((ddpvld & (1 << CPL_RX_ISCSI_DDP_STATUS_DDP_SHIFT)) &&\r\n(!(pdu_cb->flags & PDUCBF_RX_DATA))) {\r\npdu_cb->flags |= PDUCBF_RX_DATA_DDPD;\r\n}\r\n}\r\nstatic void\r\ncxgbit_lro_add_packet_rsp(struct sk_buff *skb, u8 op, const __be64 *rsp)\r\n{\r\nstruct cxgbit_lro_cb *lro_cb = cxgbit_skb_lro_cb(skb);\r\nstruct cxgbit_lro_pdu_cb *pdu_cb = cxgbit_skb_lro_pdu_cb(skb,\r\nlro_cb->pdu_idx);\r\nstruct cpl_rx_iscsi_ddp *cpl = (struct cpl_rx_iscsi_ddp *)(rsp + 1);\r\ncxgbit_process_ddpvld(lro_cb->csk, pdu_cb, be32_to_cpu(cpl->ddpvld));\r\npdu_cb->flags |= PDUCBF_RX_STATUS;\r\npdu_cb->ddigest = ntohl(cpl->ulp_crc);\r\npdu_cb->pdulen = ntohs(cpl->len);\r\nif (pdu_cb->flags & PDUCBF_RX_HDR)\r\npdu_cb->complete = true;\r\nlro_cb->pdu_totallen += pdu_cb->pdulen;\r\nlro_cb->complete = true;\r\nlro_cb->pdu_idx++;\r\n}\r\nstatic void\r\ncxgbit_copy_frags(struct sk_buff *skb, const struct pkt_gl *gl,\r\nunsigned int offset)\r\n{\r\nu8 skb_frag_idx = skb_shinfo(skb)->nr_frags;\r\nu8 i;\r\n__skb_fill_page_desc(skb, skb_frag_idx, gl->frags[0].page,\r\ngl->frags[0].offset + offset,\r\ngl->frags[0].size - offset);\r\nfor (i = 1; i < gl->nfrags; i++)\r\n__skb_fill_page_desc(skb, skb_frag_idx + i,\r\ngl->frags[i].page,\r\ngl->frags[i].offset,\r\ngl->frags[i].size);\r\nskb_shinfo(skb)->nr_frags += gl->nfrags;\r\nget_page(gl->frags[gl->nfrags - 1].page);\r\n}\r\nstatic void\r\ncxgbit_lro_add_packet_gl(struct sk_buff *skb, u8 op, const struct pkt_gl *gl)\r\n{\r\nstruct cxgbit_lro_cb *lro_cb = cxgbit_skb_lro_cb(skb);\r\nstruct cxgbit_lro_pdu_cb *pdu_cb = cxgbit_skb_lro_pdu_cb(skb,\r\nlro_cb->pdu_idx);\r\nu32 len, offset;\r\nif (op == CPL_ISCSI_HDR) {\r\nstruct cpl_iscsi_hdr *cpl = (struct cpl_iscsi_hdr *)gl->va;\r\noffset = sizeof(struct cpl_iscsi_hdr);\r\npdu_cb->flags |= PDUCBF_RX_HDR;\r\npdu_cb->seq = ntohl(cpl->seq);\r\nlen = ntohs(cpl->len);\r\npdu_cb->hdr = gl->va + offset;\r\npdu_cb->hlen = len;\r\npdu_cb->hfrag_idx = skb_shinfo(skb)->nr_frags;\r\nif (unlikely(gl->nfrags > 1))\r\ncxgbit_skcb_flags(skb) = 0;\r\nlro_cb->complete = false;\r\n} else if (op == CPL_ISCSI_DATA) {\r\nstruct cpl_iscsi_data *cpl = (struct cpl_iscsi_data *)gl->va;\r\noffset = sizeof(struct cpl_iscsi_data);\r\npdu_cb->flags |= PDUCBF_RX_DATA;\r\nlen = ntohs(cpl->len);\r\npdu_cb->dlen = len;\r\npdu_cb->doffset = lro_cb->offset;\r\npdu_cb->nr_dfrags = gl->nfrags;\r\npdu_cb->dfrag_idx = skb_shinfo(skb)->nr_frags;\r\nlro_cb->complete = false;\r\n} else {\r\nstruct cpl_rx_iscsi_cmp *cpl;\r\ncpl = (struct cpl_rx_iscsi_cmp *)gl->va;\r\noffset = sizeof(struct cpl_rx_iscsi_cmp);\r\npdu_cb->flags |= (PDUCBF_RX_HDR | PDUCBF_RX_STATUS);\r\nlen = be16_to_cpu(cpl->len);\r\npdu_cb->hdr = gl->va + offset;\r\npdu_cb->hlen = len;\r\npdu_cb->hfrag_idx = skb_shinfo(skb)->nr_frags;\r\npdu_cb->ddigest = be32_to_cpu(cpl->ulp_crc);\r\npdu_cb->pdulen = ntohs(cpl->len);\r\nif (unlikely(gl->nfrags > 1))\r\ncxgbit_skcb_flags(skb) = 0;\r\ncxgbit_process_ddpvld(lro_cb->csk, pdu_cb,\r\nbe32_to_cpu(cpl->ddpvld));\r\nif (pdu_cb->flags & PDUCBF_RX_DATA_DDPD) {\r\npdu_cb->flags |= PDUCBF_RX_DDP_CMP;\r\npdu_cb->complete = true;\r\n} else if (pdu_cb->flags & PDUCBF_RX_DATA) {\r\npdu_cb->complete = true;\r\n}\r\nlro_cb->pdu_totallen += pdu_cb->hlen + pdu_cb->dlen;\r\nlro_cb->complete = true;\r\nlro_cb->pdu_idx++;\r\n}\r\ncxgbit_copy_frags(skb, gl, offset);\r\npdu_cb->frags += gl->nfrags;\r\nlro_cb->offset += len;\r\nskb->len += len;\r\nskb->data_len += len;\r\nskb->truesize += len;\r\n}\r\nstatic struct sk_buff *\r\ncxgbit_lro_init_skb(struct cxgbit_sock *csk, u8 op, const struct pkt_gl *gl,\r\nconst __be64 *rsp, struct napi_struct *napi)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cxgbit_lro_cb *lro_cb;\r\nskb = napi_alloc_skb(napi, LRO_SKB_MAX_HEADROOM);\r\nif (unlikely(!skb))\r\nreturn NULL;\r\nmemset(skb->data, 0, LRO_SKB_MAX_HEADROOM);\r\ncxgbit_skcb_flags(skb) |= SKCBF_RX_LRO;\r\nlro_cb = cxgbit_skb_lro_cb(skb);\r\ncxgbit_get_csk(csk);\r\nlro_cb->csk = csk;\r\nreturn skb;\r\n}\r\nstatic void cxgbit_queue_lro_skb(struct cxgbit_sock *csk, struct sk_buff *skb)\r\n{\r\nbool wakeup_thread = false;\r\nspin_lock(&csk->rxq.lock);\r\n__skb_queue_tail(&csk->rxq, skb);\r\nif (skb_queue_len(&csk->rxq) == 1)\r\nwakeup_thread = true;\r\nspin_unlock(&csk->rxq.lock);\r\nif (wakeup_thread)\r\nwake_up(&csk->waitq);\r\n}\r\nstatic void cxgbit_lro_flush(struct t4_lro_mgr *lro_mgr, struct sk_buff *skb)\r\n{\r\nstruct cxgbit_lro_cb *lro_cb = cxgbit_skb_lro_cb(skb);\r\nstruct cxgbit_sock *csk = lro_cb->csk;\r\ncsk->lro_skb = NULL;\r\n__skb_unlink(skb, &lro_mgr->lroq);\r\ncxgbit_queue_lro_skb(csk, skb);\r\ncxgbit_put_csk(csk);\r\nlro_mgr->lro_pkts++;\r\nlro_mgr->lro_session_cnt--;\r\n}\r\nstatic void cxgbit_uld_lro_flush(struct t4_lro_mgr *lro_mgr)\r\n{\r\nstruct sk_buff *skb;\r\nwhile ((skb = skb_peek(&lro_mgr->lroq)))\r\ncxgbit_lro_flush(lro_mgr, skb);\r\n}\r\nstatic int\r\ncxgbit_lro_receive(struct cxgbit_sock *csk, u8 op, const __be64 *rsp,\r\nconst struct pkt_gl *gl, struct t4_lro_mgr *lro_mgr,\r\nstruct napi_struct *napi)\r\n{\r\nstruct sk_buff *skb;\r\nstruct cxgbit_lro_cb *lro_cb;\r\nif (!csk) {\r\npr_err("%s: csk NULL, op 0x%x.\n", __func__, op);\r\ngoto out;\r\n}\r\nif (csk->lro_skb)\r\ngoto add_packet;\r\nstart_lro:\r\nif (lro_mgr->lro_session_cnt >= MAX_LRO_SESSIONS) {\r\ncxgbit_uld_lro_flush(lro_mgr);\r\ngoto start_lro;\r\n}\r\nskb = cxgbit_lro_init_skb(csk, op, gl, rsp, napi);\r\nif (unlikely(!skb))\r\ngoto out;\r\ncsk->lro_skb = skb;\r\n__skb_queue_tail(&lro_mgr->lroq, skb);\r\nlro_mgr->lro_session_cnt++;\r\nadd_packet:\r\nskb = csk->lro_skb;\r\nlro_cb = cxgbit_skb_lro_cb(skb);\r\nif ((gl && (((skb_shinfo(skb)->nr_frags + gl->nfrags) >\r\nMAX_SKB_FRAGS) || (lro_cb->pdu_totallen >= LRO_FLUSH_LEN_MAX))) ||\r\n(lro_cb->pdu_idx >= MAX_SKB_FRAGS)) {\r\ncxgbit_lro_flush(lro_mgr, skb);\r\ngoto start_lro;\r\n}\r\nif (gl)\r\ncxgbit_lro_add_packet_gl(skb, op, gl);\r\nelse\r\ncxgbit_lro_add_packet_rsp(skb, op, rsp);\r\nlro_mgr->lro_merged++;\r\nreturn 0;\r\nout:\r\nreturn -1;\r\n}\r\nstatic int\r\ncxgbit_uld_lro_rx_handler(void *hndl, const __be64 *rsp,\r\nconst struct pkt_gl *gl, struct t4_lro_mgr *lro_mgr,\r\nstruct napi_struct *napi)\r\n{\r\nstruct cxgbit_device *cdev = hndl;\r\nstruct cxgb4_lld_info *lldi = &cdev->lldi;\r\nstruct cpl_tx_data *rpl = NULL;\r\nstruct cxgbit_sock *csk = NULL;\r\nunsigned int tid = 0;\r\nstruct sk_buff *skb;\r\nunsigned int op = *(u8 *)rsp;\r\nbool lro_flush = true;\r\nswitch (op) {\r\ncase CPL_ISCSI_HDR:\r\ncase CPL_ISCSI_DATA:\r\ncase CPL_RX_ISCSI_CMP:\r\ncase CPL_RX_ISCSI_DDP:\r\ncase CPL_FW4_ACK:\r\nlro_flush = false;\r\ncase CPL_ABORT_RPL_RSS:\r\ncase CPL_PASS_ESTABLISH:\r\ncase CPL_PEER_CLOSE:\r\ncase CPL_CLOSE_CON_RPL:\r\ncase CPL_ABORT_REQ_RSS:\r\ncase CPL_SET_TCB_RPL:\r\ncase CPL_RX_DATA:\r\nrpl = gl ? (struct cpl_tx_data *)gl->va :\r\n(struct cpl_tx_data *)(rsp + 1);\r\ntid = GET_TID(rpl);\r\ncsk = lookup_tid(lldi->tids, tid);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nif (csk && csk->lro_skb && lro_flush)\r\ncxgbit_lro_flush(lro_mgr, csk->lro_skb);\r\nif (!gl) {\r\nunsigned int len;\r\nif (op == CPL_RX_ISCSI_DDP) {\r\nif (!cxgbit_lro_receive(csk, op, rsp, NULL, lro_mgr,\r\nnapi))\r\nreturn 0;\r\n}\r\nlen = 64 - sizeof(struct rsp_ctrl) - 8;\r\nskb = napi_alloc_skb(napi, len);\r\nif (!skb)\r\ngoto nomem;\r\n__skb_put(skb, len);\r\nskb_copy_to_linear_data(skb, &rsp[1], len);\r\n} else {\r\nif (unlikely(op != *(u8 *)gl->va)) {\r\npr_info("? FL 0x%p,RSS%#llx,FL %#llx,len %u.\n",\r\ngl->va, be64_to_cpu(*rsp),\r\nget_unaligned_be64(gl->va),\r\ngl->tot_len);\r\nreturn 0;\r\n}\r\nif ((op == CPL_ISCSI_HDR) || (op == CPL_ISCSI_DATA) ||\r\n(op == CPL_RX_ISCSI_CMP)) {\r\nif (!cxgbit_lro_receive(csk, op, rsp, gl, lro_mgr,\r\nnapi))\r\nreturn 0;\r\n}\r\n#define RX_PULL_LEN 128\r\nskb = cxgb4_pktgl_to_skb(gl, RX_PULL_LEN, RX_PULL_LEN);\r\nif (unlikely(!skb))\r\ngoto nomem;\r\n}\r\nrpl = (struct cpl_tx_data *)skb->data;\r\nop = rpl->ot.opcode;\r\ncxgbit_skcb_rx_opcode(skb) = op;\r\npr_debug("cdev %p, opcode 0x%x(0x%x,0x%x), skb %p.\n",\r\ncdev, op, rpl->ot.opcode_tid,\r\nntohl(rpl->ot.opcode_tid), skb);\r\nif (op < NUM_CPL_CMDS && cxgbit_cplhandlers[op]) {\r\ncxgbit_cplhandlers[op](cdev, skb);\r\n} else {\r\npr_err("No handler for opcode 0x%x.\n", op);\r\n__kfree_skb(skb);\r\n}\r\nreturn 0;\r\nnomem:\r\npr_err("%s OOM bailing out.\n", __func__);\r\nreturn 1;\r\n}\r\nstatic void\r\ncxgbit_update_dcb_priority(struct cxgbit_device *cdev, u8 port_id,\r\nu8 dcb_priority, u16 port_num)\r\n{\r\nstruct cxgbit_sock *csk;\r\nstruct sk_buff *skb;\r\nu16 local_port;\r\nbool wakeup_thread = false;\r\nspin_lock_bh(&cdev->cskq.lock);\r\nlist_for_each_entry(csk, &cdev->cskq.list, list) {\r\nif (csk->port_id != port_id)\r\ncontinue;\r\nif (csk->com.local_addr.ss_family == AF_INET6) {\r\nstruct sockaddr_in6 *sock_in6;\r\nsock_in6 = (struct sockaddr_in6 *)&csk->com.local_addr;\r\nlocal_port = ntohs(sock_in6->sin6_port);\r\n} else {\r\nstruct sockaddr_in *sock_in;\r\nsock_in = (struct sockaddr_in *)&csk->com.local_addr;\r\nlocal_port = ntohs(sock_in->sin_port);\r\n}\r\nif (local_port != port_num)\r\ncontinue;\r\nif (csk->dcb_priority == dcb_priority)\r\ncontinue;\r\nskb = alloc_skb(0, GFP_ATOMIC);\r\nif (!skb)\r\ncontinue;\r\nspin_lock(&csk->rxq.lock);\r\n__skb_queue_tail(&csk->rxq, skb);\r\nif (skb_queue_len(&csk->rxq) == 1)\r\nwakeup_thread = true;\r\nspin_unlock(&csk->rxq.lock);\r\nif (wakeup_thread) {\r\nwake_up(&csk->waitq);\r\nwakeup_thread = false;\r\n}\r\n}\r\nspin_unlock_bh(&cdev->cskq.lock);\r\n}\r\nstatic void cxgbit_dcb_workfn(struct work_struct *work)\r\n{\r\nstruct cxgbit_dcb_work *dcb_work;\r\nstruct net_device *ndev;\r\nstruct cxgbit_device *cdev = NULL;\r\nstruct dcb_app_type *iscsi_app;\r\nu8 priority, port_id = 0xff;\r\ndcb_work = container_of(work, struct cxgbit_dcb_work, work);\r\niscsi_app = &dcb_work->dcb_app;\r\nif (iscsi_app->dcbx & DCB_CAP_DCBX_VER_IEEE) {\r\nif (iscsi_app->app.selector != IEEE_8021QAZ_APP_SEL_ANY)\r\ngoto out;\r\npriority = iscsi_app->app.priority;\r\n} else if (iscsi_app->dcbx & DCB_CAP_DCBX_VER_CEE) {\r\nif (iscsi_app->app.selector != DCB_APP_IDTYPE_PORTNUM)\r\ngoto out;\r\nif (!iscsi_app->app.priority)\r\ngoto out;\r\npriority = ffs(iscsi_app->app.priority) - 1;\r\n} else {\r\ngoto out;\r\n}\r\npr_debug("priority for ifid %d is %u\n",\r\niscsi_app->ifindex, priority);\r\nndev = dev_get_by_index(&init_net, iscsi_app->ifindex);\r\nif (!ndev)\r\ngoto out;\r\nmutex_lock(&cdev_list_lock);\r\ncdev = cxgbit_find_device(ndev, &port_id);\r\ndev_put(ndev);\r\nif (!cdev) {\r\nmutex_unlock(&cdev_list_lock);\r\ngoto out;\r\n}\r\ncxgbit_update_dcb_priority(cdev, port_id, priority,\r\niscsi_app->app.protocol);\r\nmutex_unlock(&cdev_list_lock);\r\nout:\r\nkfree(dcb_work);\r\n}\r\nstatic int\r\ncxgbit_dcbevent_notify(struct notifier_block *nb, unsigned long action,\r\nvoid *data)\r\n{\r\nstruct cxgbit_dcb_work *dcb_work;\r\nstruct dcb_app_type *dcb_app = data;\r\ndcb_work = kzalloc(sizeof(*dcb_work), GFP_ATOMIC);\r\nif (!dcb_work)\r\nreturn NOTIFY_DONE;\r\ndcb_work->dcb_app = *dcb_app;\r\nINIT_WORK(&dcb_work->work, cxgbit_dcb_workfn);\r\nschedule_work(&dcb_work->work);\r\nreturn NOTIFY_OK;\r\n}\r\nstatic enum target_prot_op cxgbit_get_sup_prot_ops(struct iscsi_conn *conn)\r\n{\r\nreturn TARGET_PROT_NORMAL;\r\n}\r\nstatic int __init cxgbit_init(void)\r\n{\r\ncxgb4_register_uld(CXGB4_ULD_ISCSIT, &cxgbit_uld_info);\r\niscsit_register_transport(&cxgbit_transport);\r\n#ifdef CONFIG_CHELSIO_T4_DCB\r\npr_info("%s dcb enabled.\n", DRV_NAME);\r\nregister_dcbevent_notifier(&cxgbit_dcbevent_nb);\r\n#endif\r\nBUILD_BUG_ON(FIELD_SIZEOF(struct sk_buff, cb) <\r\nsizeof(union cxgbit_skb_cb));\r\nreturn 0;\r\n}\r\nstatic void __exit cxgbit_exit(void)\r\n{\r\nstruct cxgbit_device *cdev, *tmp;\r\n#ifdef CONFIG_CHELSIO_T4_DCB\r\nunregister_dcbevent_notifier(&cxgbit_dcbevent_nb);\r\n#endif\r\nmutex_lock(&cdev_list_lock);\r\nlist_for_each_entry_safe(cdev, tmp, &cdev_list_head, list) {\r\nlist_del(&cdev->list);\r\ncxgbit_put_cdev(cdev);\r\n}\r\nmutex_unlock(&cdev_list_lock);\r\niscsit_unregister_transport(&cxgbit_transport);\r\ncxgb4_unregister_uld(CXGB4_ULD_ISCSIT);\r\n}
