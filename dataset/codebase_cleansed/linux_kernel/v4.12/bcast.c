static struct tipc_bc_base *tipc_bc_base(struct net *net)\r\n{\r\nreturn tipc_net(net)->bcbase;\r\n}\r\nint tipc_bcast_get_mtu(struct net *net)\r\n{\r\nreturn tipc_link_mtu(tipc_bc_sndlink(net)) - INT_H_SIZE;\r\n}\r\nvoid tipc_bcast_disable_rcast(struct net *net)\r\n{\r\ntipc_bc_base(net)->rcast_support = false;\r\n}\r\nstatic void tipc_bcbase_calc_bc_threshold(struct net *net)\r\n{\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\nint cluster_size = tipc_link_bc_peers(tipc_bc_sndlink(net));\r\nbb->bc_threshold = 1 + (cluster_size * bb->rc_ratio / 100);\r\n}\r\nstatic void tipc_bcbase_select_primary(struct net *net)\r\n{\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\nint all_dests = tipc_link_bc_peers(bb->link);\r\nint i, mtu, prim;\r\nbb->primary_bearer = INVALID_BEARER_ID;\r\nbb->bcast_support = true;\r\nif (!all_dests)\r\nreturn;\r\nfor (i = 0; i < MAX_BEARERS; i++) {\r\nif (!bb->dests[i])\r\ncontinue;\r\nmtu = tipc_bearer_mtu(net, i);\r\nif (mtu < tipc_link_mtu(bb->link))\r\ntipc_link_set_mtu(bb->link, mtu);\r\nbb->bcast_support &= tipc_bearer_bcast_support(net, i);\r\nif (bb->dests[i] < all_dests)\r\ncontinue;\r\nbb->primary_bearer = i;\r\nif ((i ^ tipc_own_addr(net)) & 1)\r\nbreak;\r\n}\r\nprim = bb->primary_bearer;\r\nif (prim != INVALID_BEARER_ID)\r\nbb->bcast_support = tipc_bearer_bcast_support(net, prim);\r\n}\r\nvoid tipc_bcast_inc_bearer_dst_cnt(struct net *net, int bearer_id)\r\n{\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\ntipc_bcast_lock(net);\r\nbb->dests[bearer_id]++;\r\ntipc_bcbase_select_primary(net);\r\ntipc_bcast_unlock(net);\r\n}\r\nvoid tipc_bcast_dec_bearer_dst_cnt(struct net *net, int bearer_id)\r\n{\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\ntipc_bcast_lock(net);\r\nbb->dests[bearer_id]--;\r\ntipc_bcbase_select_primary(net);\r\ntipc_bcast_unlock(net);\r\n}\r\nstatic void tipc_bcbase_xmit(struct net *net, struct sk_buff_head *xmitq)\r\n{\r\nint bearer_id;\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\nstruct sk_buff *skb, *_skb;\r\nstruct sk_buff_head _xmitq;\r\nif (skb_queue_empty(xmitq))\r\nreturn;\r\nbearer_id = bb->primary_bearer;\r\nif (bearer_id >= 0) {\r\ntipc_bearer_bc_xmit(net, bearer_id, xmitq);\r\nreturn;\r\n}\r\nskb_queue_head_init(&_xmitq);\r\nfor (bearer_id = 0; bearer_id < MAX_BEARERS; bearer_id++) {\r\nif (!bb->dests[bearer_id])\r\ncontinue;\r\nskb_queue_walk(xmitq, skb) {\r\n_skb = pskb_copy_for_clone(skb, GFP_ATOMIC);\r\nif (!_skb)\r\nbreak;\r\n__skb_queue_tail(&_xmitq, _skb);\r\n}\r\ntipc_bearer_bc_xmit(net, bearer_id, &_xmitq);\r\n}\r\n__skb_queue_purge(xmitq);\r\n__skb_queue_purge(&_xmitq);\r\n}\r\nstatic void tipc_bcast_select_xmit_method(struct net *net, int dests,\r\nstruct tipc_mc_method *method)\r\n{\r\nstruct tipc_bc_base *bb = tipc_bc_base(net);\r\nunsigned long exp = method->expires;\r\nif (!bb->bcast_support) {\r\nmethod->rcast = true;\r\nreturn;\r\n}\r\nif (!bb->rcast_support) {\r\nmethod->rcast = false;\r\nreturn;\r\n}\r\nmethod->expires = jiffies + TIPC_METHOD_EXPIRE;\r\nif (method->mandatory || time_before(jiffies, exp))\r\nreturn;\r\nmethod->rcast = dests <= bb->bc_threshold;\r\n}\r\nstatic int tipc_bcast_xmit(struct net *net, struct sk_buff_head *pkts,\r\nu16 *cong_link_cnt)\r\n{\r\nstruct tipc_link *l = tipc_bc_sndlink(net);\r\nstruct sk_buff_head xmitq;\r\nint rc = 0;\r\n__skb_queue_head_init(&xmitq);\r\ntipc_bcast_lock(net);\r\nif (tipc_link_bc_peers(l))\r\nrc = tipc_link_xmit(l, pkts, &xmitq);\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\n__skb_queue_purge(pkts);\r\nif (rc == -ELINKCONG) {\r\n*cong_link_cnt = 1;\r\nrc = 0;\r\n}\r\nreturn rc;\r\n}\r\nstatic int tipc_rcast_xmit(struct net *net, struct sk_buff_head *pkts,\r\nstruct tipc_nlist *dests, u16 *cong_link_cnt)\r\n{\r\nstruct sk_buff_head _pkts;\r\nstruct u32_item *n, *tmp;\r\nu32 dst, selector;\r\nselector = msg_link_selector(buf_msg(skb_peek(pkts)));\r\n__skb_queue_head_init(&_pkts);\r\nlist_for_each_entry_safe(n, tmp, &dests->list, list) {\r\ndst = n->value;\r\nif (!tipc_msg_pskb_copy(dst, pkts, &_pkts))\r\nreturn -ENOMEM;\r\nif (tipc_node_xmit(net, &_pkts, dst, selector) == -ELINKCONG)\r\n(*cong_link_cnt)++;\r\n}\r\nreturn 0;\r\n}\r\nint tipc_mcast_xmit(struct net *net, struct sk_buff_head *pkts,\r\nstruct tipc_mc_method *method, struct tipc_nlist *dests,\r\nu16 *cong_link_cnt)\r\n{\r\nstruct sk_buff_head inputq, localq;\r\nint rc = 0;\r\nskb_queue_head_init(&inputq);\r\nskb_queue_head_init(&localq);\r\nif (dests->local && !tipc_msg_reassemble(pkts, &localq)) {\r\nrc = -ENOMEM;\r\ngoto exit;\r\n}\r\nif (dests->remote) {\r\ntipc_bcast_select_xmit_method(net, dests->remote, method);\r\nif (method->rcast)\r\nrc = tipc_rcast_xmit(net, pkts, dests, cong_link_cnt);\r\nelse\r\nrc = tipc_bcast_xmit(net, pkts, cong_link_cnt);\r\n}\r\nif (dests->local)\r\ntipc_sk_mcast_rcv(net, &localq, &inputq);\r\nexit:\r\n__skb_queue_purge(pkts);\r\nreturn rc;\r\n}\r\nint tipc_bcast_rcv(struct net *net, struct tipc_link *l, struct sk_buff *skb)\r\n{\r\nstruct tipc_msg *hdr = buf_msg(skb);\r\nstruct sk_buff_head *inputq = &tipc_bc_base(net)->inputq;\r\nstruct sk_buff_head xmitq;\r\nint rc;\r\n__skb_queue_head_init(&xmitq);\r\nif (msg_mc_netid(hdr) != tipc_netid(net) || !tipc_link_is_up(l)) {\r\nkfree_skb(skb);\r\nreturn 0;\r\n}\r\ntipc_bcast_lock(net);\r\nif (msg_user(hdr) == BCAST_PROTOCOL)\r\nrc = tipc_link_bc_nack_rcv(l, skb, &xmitq);\r\nelse\r\nrc = tipc_link_rcv(l, skb, NULL);\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\nif (!skb_queue_empty(inputq))\r\ntipc_sk_rcv(net, inputq);\r\nreturn rc;\r\n}\r\nvoid tipc_bcast_ack_rcv(struct net *net, struct tipc_link *l,\r\nstruct tipc_msg *hdr)\r\n{\r\nstruct sk_buff_head *inputq = &tipc_bc_base(net)->inputq;\r\nu16 acked = msg_bcast_ack(hdr);\r\nstruct sk_buff_head xmitq;\r\nif (msg_bc_ack_invalid(hdr))\r\nreturn;\r\n__skb_queue_head_init(&xmitq);\r\ntipc_bcast_lock(net);\r\ntipc_link_bc_ack_rcv(l, acked, &xmitq);\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\nif (!skb_queue_empty(inputq))\r\ntipc_sk_rcv(net, inputq);\r\n}\r\nint tipc_bcast_sync_rcv(struct net *net, struct tipc_link *l,\r\nstruct tipc_msg *hdr)\r\n{\r\nstruct sk_buff_head *inputq = &tipc_bc_base(net)->inputq;\r\nstruct sk_buff_head xmitq;\r\nint rc = 0;\r\n__skb_queue_head_init(&xmitq);\r\ntipc_bcast_lock(net);\r\nif (msg_type(hdr) != STATE_MSG) {\r\ntipc_link_bc_init_rcv(l, hdr);\r\n} else if (!msg_bc_ack_invalid(hdr)) {\r\ntipc_link_bc_ack_rcv(l, msg_bcast_ack(hdr), &xmitq);\r\nrc = tipc_link_bc_sync_rcv(l, hdr, &xmitq);\r\n}\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\nif (!skb_queue_empty(inputq))\r\ntipc_sk_rcv(net, inputq);\r\nreturn rc;\r\n}\r\nvoid tipc_bcast_add_peer(struct net *net, struct tipc_link *uc_l,\r\nstruct sk_buff_head *xmitq)\r\n{\r\nstruct tipc_link *snd_l = tipc_bc_sndlink(net);\r\ntipc_bcast_lock(net);\r\ntipc_link_add_bc_peer(snd_l, uc_l, xmitq);\r\ntipc_bcbase_select_primary(net);\r\ntipc_bcbase_calc_bc_threshold(net);\r\ntipc_bcast_unlock(net);\r\n}\r\nvoid tipc_bcast_remove_peer(struct net *net, struct tipc_link *rcv_l)\r\n{\r\nstruct tipc_link *snd_l = tipc_bc_sndlink(net);\r\nstruct sk_buff_head *inputq = &tipc_bc_base(net)->inputq;\r\nstruct sk_buff_head xmitq;\r\n__skb_queue_head_init(&xmitq);\r\ntipc_bcast_lock(net);\r\ntipc_link_remove_bc_peer(snd_l, rcv_l, &xmitq);\r\ntipc_bcbase_select_primary(net);\r\ntipc_bcbase_calc_bc_threshold(net);\r\ntipc_bcast_unlock(net);\r\ntipc_bcbase_xmit(net, &xmitq);\r\nif (!skb_queue_empty(inputq))\r\ntipc_sk_rcv(net, inputq);\r\n}\r\nint tipc_bclink_reset_stats(struct net *net)\r\n{\r\nstruct tipc_link *l = tipc_bc_sndlink(net);\r\nif (!l)\r\nreturn -ENOPROTOOPT;\r\ntipc_bcast_lock(net);\r\ntipc_link_reset_stats(l);\r\ntipc_bcast_unlock(net);\r\nreturn 0;\r\n}\r\nstatic int tipc_bc_link_set_queue_limits(struct net *net, u32 limit)\r\n{\r\nstruct tipc_link *l = tipc_bc_sndlink(net);\r\nif (!l)\r\nreturn -ENOPROTOOPT;\r\nif (limit < BCLINK_WIN_MIN)\r\nlimit = BCLINK_WIN_MIN;\r\nif (limit > TIPC_MAX_LINK_WIN)\r\nreturn -EINVAL;\r\ntipc_bcast_lock(net);\r\ntipc_link_set_queue_limits(l, limit);\r\ntipc_bcast_unlock(net);\r\nreturn 0;\r\n}\r\nint tipc_nl_bc_link_set(struct net *net, struct nlattr *attrs[])\r\n{\r\nint err;\r\nu32 win;\r\nstruct nlattr *props[TIPC_NLA_PROP_MAX + 1];\r\nif (!attrs[TIPC_NLA_LINK_PROP])\r\nreturn -EINVAL;\r\nerr = tipc_nl_parse_link_prop(attrs[TIPC_NLA_LINK_PROP], props);\r\nif (err)\r\nreturn err;\r\nif (!props[TIPC_NLA_PROP_WIN])\r\nreturn -EOPNOTSUPP;\r\nwin = nla_get_u32(props[TIPC_NLA_PROP_WIN]);\r\nreturn tipc_bc_link_set_queue_limits(net, win);\r\n}\r\nint tipc_bcast_init(struct net *net)\r\n{\r\nstruct tipc_net *tn = tipc_net(net);\r\nstruct tipc_bc_base *bb = NULL;\r\nstruct tipc_link *l = NULL;\r\nbb = kzalloc(sizeof(*bb), GFP_ATOMIC);\r\nif (!bb)\r\ngoto enomem;\r\ntn->bcbase = bb;\r\nspin_lock_init(&tipc_net(net)->bclock);\r\nif (!tipc_link_bc_create(net, 0, 0,\r\nU16_MAX,\r\nBCLINK_WIN_DEFAULT,\r\n0,\r\n&bb->inputq,\r\nNULL,\r\nNULL,\r\n&l))\r\ngoto enomem;\r\nbb->link = l;\r\ntn->bcl = l;\r\nbb->rc_ratio = 25;\r\nbb->rcast_support = true;\r\nreturn 0;\r\nenomem:\r\nkfree(bb);\r\nkfree(l);\r\nreturn -ENOMEM;\r\n}\r\nvoid tipc_bcast_stop(struct net *net)\r\n{\r\nstruct tipc_net *tn = net_generic(net, tipc_net_id);\r\nsynchronize_net();\r\nkfree(tn->bcbase);\r\nkfree(tn->bcl);\r\n}\r\nvoid tipc_nlist_init(struct tipc_nlist *nl, u32 self)\r\n{\r\nmemset(nl, 0, sizeof(*nl));\r\nINIT_LIST_HEAD(&nl->list);\r\nnl->self = self;\r\n}\r\nvoid tipc_nlist_add(struct tipc_nlist *nl, u32 node)\r\n{\r\nif (node == nl->self)\r\nnl->local = true;\r\nelse if (u32_push(&nl->list, node))\r\nnl->remote++;\r\n}\r\nvoid tipc_nlist_del(struct tipc_nlist *nl, u32 node)\r\n{\r\nif (node == nl->self)\r\nnl->local = false;\r\nelse if (u32_del(&nl->list, node))\r\nnl->remote--;\r\n}\r\nvoid tipc_nlist_purge(struct tipc_nlist *nl)\r\n{\r\nu32_list_purge(&nl->list);\r\nnl->remote = 0;\r\nnl->local = 0;\r\n}
