static void __uniphier_cache_sync(struct uniphier_cache_data *data)\r\n{\r\nwritel_relaxed(UNIPHIER_SSCOPE_CM_SYNC,\r\ndata->op_base + UNIPHIER_SSCOPE);\r\nreadl_relaxed(data->op_base + UNIPHIER_SSCOPE);\r\n}\r\nstatic void __uniphier_cache_maint_common(struct uniphier_cache_data *data,\r\nunsigned long start,\r\nunsigned long size,\r\nu32 operation)\r\n{\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nwritel_relaxed(UNIPHIER_SSCOLPQS_EF, data->op_base + UNIPHIER_SSCOLPQS);\r\ndo {\r\nwritel_relaxed(UNIPHIER_SSCOQM_CE | operation,\r\ndata->op_base + UNIPHIER_SSCOQM);\r\nif (likely(UNIPHIER_SSCOQM_S_IS_RANGE(operation))) {\r\nwritel_relaxed(start, data->op_base + UNIPHIER_SSCOQAD);\r\nwritel_relaxed(size, data->op_base + UNIPHIER_SSCOQSZ);\r\n}\r\nif (unlikely(UNIPHIER_SSCOQM_TID_IS_WAY(operation)))\r\nwritel_relaxed(data->way_locked_mask,\r\ndata->op_base + UNIPHIER_SSCOQWN);\r\n} while (unlikely(readl_relaxed(data->op_base + UNIPHIER_SSCOPPQSEF) &\r\n(UNIPHIER_SSCOPPQSEF_FE | UNIPHIER_SSCOPPQSEF_OE)));\r\nwhile (likely(readl_relaxed(data->op_base + UNIPHIER_SSCOLPQS) !=\r\nUNIPHIER_SSCOLPQS_EF))\r\ncpu_relax();\r\nlocal_irq_restore(flags);\r\n}\r\nstatic void __uniphier_cache_maint_all(struct uniphier_cache_data *data,\r\nu32 operation)\r\n{\r\n__uniphier_cache_maint_common(data, 0, 0,\r\nUNIPHIER_SSCOQM_S_ALL | operation);\r\n__uniphier_cache_sync(data);\r\n}\r\nstatic void __uniphier_cache_maint_range(struct uniphier_cache_data *data,\r\nunsigned long start, unsigned long end,\r\nu32 operation)\r\n{\r\nunsigned long size;\r\nstart = start & ~(data->line_size - 1);\r\nsize = end - start;\r\nif (unlikely(size >= (unsigned long)(-data->line_size))) {\r\n__uniphier_cache_maint_all(data, operation);\r\nreturn;\r\n}\r\nsize = ALIGN(size, data->line_size);\r\nwhile (size) {\r\nunsigned long chunk_size = min_t(unsigned long, size,\r\ndata->range_op_max_size);\r\n__uniphier_cache_maint_common(data, start, chunk_size,\r\nUNIPHIER_SSCOQM_S_RANGE | operation);\r\nstart += chunk_size;\r\nsize -= chunk_size;\r\n}\r\n__uniphier_cache_sync(data);\r\n}\r\nstatic void __uniphier_cache_enable(struct uniphier_cache_data *data, bool on)\r\n{\r\nu32 val = 0;\r\nif (on)\r\nval = UNIPHIER_SSCC_WTG | UNIPHIER_SSCC_PRD | UNIPHIER_SSCC_ON;\r\nwritel_relaxed(val, data->ctrl_base + UNIPHIER_SSCC);\r\n}\r\nstatic void __init __uniphier_cache_set_locked_ways(\r\nstruct uniphier_cache_data *data,\r\nu32 way_mask)\r\n{\r\nunsigned int cpu;\r\ndata->way_locked_mask = way_mask & data->way_present_mask;\r\nfor_each_possible_cpu(cpu)\r\nwritel_relaxed(~data->way_locked_mask & data->way_present_mask,\r\ndata->way_ctrl_base + 4 * cpu);\r\n}\r\nstatic void uniphier_cache_maint_range(unsigned long start, unsigned long end,\r\nu32 operation)\r\n{\r\nstruct uniphier_cache_data *data;\r\nlist_for_each_entry(data, &uniphier_cache_list, list)\r\n__uniphier_cache_maint_range(data, start, end, operation);\r\n}\r\nstatic void uniphier_cache_maint_all(u32 operation)\r\n{\r\nstruct uniphier_cache_data *data;\r\nlist_for_each_entry(data, &uniphier_cache_list, list)\r\n__uniphier_cache_maint_all(data, operation);\r\n}\r\nstatic void uniphier_cache_inv_range(unsigned long start, unsigned long end)\r\n{\r\nuniphier_cache_maint_range(start, end, UNIPHIER_SSCOQM_CM_INV);\r\n}\r\nstatic void uniphier_cache_clean_range(unsigned long start, unsigned long end)\r\n{\r\nuniphier_cache_maint_range(start, end, UNIPHIER_SSCOQM_CM_CLEAN);\r\n}\r\nstatic void uniphier_cache_flush_range(unsigned long start, unsigned long end)\r\n{\r\nuniphier_cache_maint_range(start, end, UNIPHIER_SSCOQM_CM_FLUSH);\r\n}\r\nstatic void __init uniphier_cache_inv_all(void)\r\n{\r\nuniphier_cache_maint_all(UNIPHIER_SSCOQM_CM_INV);\r\n}\r\nstatic void uniphier_cache_flush_all(void)\r\n{\r\nuniphier_cache_maint_all(UNIPHIER_SSCOQM_CM_FLUSH);\r\n}\r\nstatic void uniphier_cache_disable(void)\r\n{\r\nstruct uniphier_cache_data *data;\r\nlist_for_each_entry_reverse(data, &uniphier_cache_list, list)\r\n__uniphier_cache_enable(data, false);\r\nuniphier_cache_flush_all();\r\n}\r\nstatic void __init uniphier_cache_enable(void)\r\n{\r\nstruct uniphier_cache_data *data;\r\nuniphier_cache_inv_all();\r\nlist_for_each_entry(data, &uniphier_cache_list, list) {\r\n__uniphier_cache_enable(data, true);\r\n__uniphier_cache_set_locked_ways(data, 0);\r\n}\r\n}\r\nstatic void uniphier_cache_sync(void)\r\n{\r\nstruct uniphier_cache_data *data;\r\nlist_for_each_entry(data, &uniphier_cache_list, list)\r\n__uniphier_cache_sync(data);\r\n}\r\nint __init uniphier_cache_l2_is_enabled(void)\r\n{\r\nstruct uniphier_cache_data *data;\r\ndata = list_first_entry_or_null(&uniphier_cache_list,\r\nstruct uniphier_cache_data, list);\r\nif (!data)\r\nreturn 0;\r\nreturn !!(readl_relaxed(data->ctrl_base + UNIPHIER_SSCC) &\r\nUNIPHIER_SSCC_ON);\r\n}\r\nvoid __init uniphier_cache_l2_touch_range(unsigned long start,\r\nunsigned long end)\r\n{\r\nstruct uniphier_cache_data *data;\r\ndata = list_first_entry_or_null(&uniphier_cache_list,\r\nstruct uniphier_cache_data, list);\r\nif (data)\r\n__uniphier_cache_maint_range(data, start, end,\r\nUNIPHIER_SSCOQM_TID_WAY |\r\nUNIPHIER_SSCOQM_CM_TOUCH);\r\n}\r\nvoid __init uniphier_cache_l2_set_locked_ways(u32 way_mask)\r\n{\r\nstruct uniphier_cache_data *data;\r\ndata = list_first_entry_or_null(&uniphier_cache_list,\r\nstruct uniphier_cache_data, list);\r\nif (data)\r\n__uniphier_cache_set_locked_ways(data, way_mask);\r\n}\r\nstatic int __init __uniphier_cache_init(struct device_node *np,\r\nunsigned int *cache_level)\r\n{\r\nstruct uniphier_cache_data *data;\r\nu32 level, cache_size;\r\nstruct device_node *next_np;\r\nint ret = 0;\r\nif (!of_match_node(uniphier_cache_match, np)) {\r\npr_err("L%d: not compatible with uniphier cache\n",\r\n*cache_level);\r\nreturn -EINVAL;\r\n}\r\nif (of_property_read_u32(np, "cache-level", &level)) {\r\npr_err("L%d: cache-level is not specified\n", *cache_level);\r\nreturn -EINVAL;\r\n}\r\nif (level != *cache_level) {\r\npr_err("L%d: cache-level is unexpected value %d\n",\r\n*cache_level, level);\r\nreturn -EINVAL;\r\n}\r\nif (!of_property_read_bool(np, "cache-unified")) {\r\npr_err("L%d: cache-unified is not specified\n", *cache_level);\r\nreturn -EINVAL;\r\n}\r\ndata = kzalloc(sizeof(*data), GFP_KERNEL);\r\nif (!data)\r\nreturn -ENOMEM;\r\nif (of_property_read_u32(np, "cache-line-size", &data->line_size) ||\r\n!is_power_of_2(data->line_size)) {\r\npr_err("L%d: cache-line-size is unspecified or invalid\n",\r\n*cache_level);\r\nret = -EINVAL;\r\ngoto err;\r\n}\r\nif (of_property_read_u32(np, "cache-sets", &data->nsets) ||\r\n!is_power_of_2(data->nsets)) {\r\npr_err("L%d: cache-sets is unspecified or invalid\n",\r\n*cache_level);\r\nret = -EINVAL;\r\ngoto err;\r\n}\r\nif (of_property_read_u32(np, "cache-size", &cache_size) ||\r\ncache_size == 0 || cache_size % (data->nsets * data->line_size)) {\r\npr_err("L%d: cache-size is unspecified or invalid\n",\r\n*cache_level);\r\nret = -EINVAL;\r\ngoto err;\r\n}\r\ndata->way_present_mask =\r\n((u32)1 << cache_size / data->nsets / data->line_size) - 1;\r\ndata->ctrl_base = of_iomap(np, 0);\r\nif (!data->ctrl_base) {\r\npr_err("L%d: failed to map control register\n", *cache_level);\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndata->rev_base = of_iomap(np, 1);\r\nif (!data->rev_base) {\r\npr_err("L%d: failed to map revision register\n", *cache_level);\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndata->op_base = of_iomap(np, 2);\r\nif (!data->op_base) {\r\npr_err("L%d: failed to map operation register\n", *cache_level);\r\nret = -ENOMEM;\r\ngoto err;\r\n}\r\ndata->way_ctrl_base = data->ctrl_base + 0xc00;\r\nif (*cache_level == 2) {\r\nu32 revision = readl(data->rev_base + UNIPHIER_SSCID);\r\nif (revision <= 0x16)\r\ndata->range_op_max_size = (u32)1 << 22;\r\nswitch (revision) {\r\ncase 0x11:\r\ndata->way_ctrl_base = data->ctrl_base + 0x870;\r\nbreak;\r\ncase 0x12:\r\ncase 0x16:\r\ndata->way_ctrl_base = data->ctrl_base + 0x840;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\ndata->range_op_max_size -= data->line_size;\r\nINIT_LIST_HEAD(&data->list);\r\nlist_add_tail(&data->list, &uniphier_cache_list);\r\nnext_np = of_find_next_cache_node(np);\r\nif (next_np) {\r\n(*cache_level)++;\r\nret = __uniphier_cache_init(next_np, cache_level);\r\n}\r\nof_node_put(next_np);\r\nreturn ret;\r\nerr:\r\niounmap(data->op_base);\r\niounmap(data->rev_base);\r\niounmap(data->ctrl_base);\r\nkfree(data);\r\nreturn ret;\r\n}\r\nint __init uniphier_cache_init(void)\r\n{\r\nstruct device_node *np = NULL;\r\nunsigned int cache_level;\r\nint ret = 0;\r\nwhile ((np = of_find_matching_node(np, uniphier_cache_match)))\r\nif (!of_property_read_u32(np, "cache-level", &cache_level) &&\r\ncache_level == 2)\r\nbreak;\r\nif (!np)\r\nreturn -ENODEV;\r\nret = __uniphier_cache_init(np, &cache_level);\r\nof_node_put(np);\r\nif (ret) {\r\nif (cache_level == 2) {\r\npr_err("failed to initialize L2 cache\n");\r\nreturn ret;\r\n}\r\ncache_level--;\r\nret = 0;\r\n}\r\nouter_cache.inv_range = uniphier_cache_inv_range;\r\nouter_cache.clean_range = uniphier_cache_clean_range;\r\nouter_cache.flush_range = uniphier_cache_flush_range;\r\nouter_cache.flush_all = uniphier_cache_flush_all;\r\nouter_cache.disable = uniphier_cache_disable;\r\nouter_cache.sync = uniphier_cache_sync;\r\nuniphier_cache_enable();\r\npr_info("enabled outer cache (cache level: %d)\n", cache_level);\r\nreturn ret;\r\n}
