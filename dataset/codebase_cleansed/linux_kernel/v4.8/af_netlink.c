static inline int netlink_is_kernel(struct sock *sk)\r\n{\r\nreturn nlk_sk(sk)->flags & NETLINK_F_KERNEL_SOCKET;\r\n}\r\nstatic inline u32 netlink_group_mask(u32 group)\r\n{\r\nreturn group ? 1 << (group - 1) : 0;\r\n}\r\nstatic struct sk_buff *netlink_to_full_skb(const struct sk_buff *skb,\r\ngfp_t gfp_mask)\r\n{\r\nunsigned int len = skb_end_offset(skb);\r\nstruct sk_buff *new;\r\nnew = alloc_skb(len, gfp_mask);\r\nif (new == NULL)\r\nreturn NULL;\r\nNETLINK_CB(new).portid = NETLINK_CB(skb).portid;\r\nNETLINK_CB(new).dst_group = NETLINK_CB(skb).dst_group;\r\nNETLINK_CB(new).creds = NETLINK_CB(skb).creds;\r\nmemcpy(skb_put(new, len), skb->data, len);\r\nreturn new;\r\n}\r\nint netlink_add_tap(struct netlink_tap *nt)\r\n{\r\nif (unlikely(nt->dev->type != ARPHRD_NETLINK))\r\nreturn -EINVAL;\r\nspin_lock(&netlink_tap_lock);\r\nlist_add_rcu(&nt->list, &netlink_tap_all);\r\nspin_unlock(&netlink_tap_lock);\r\n__module_get(nt->module);\r\nreturn 0;\r\n}\r\nstatic int __netlink_remove_tap(struct netlink_tap *nt)\r\n{\r\nbool found = false;\r\nstruct netlink_tap *tmp;\r\nspin_lock(&netlink_tap_lock);\r\nlist_for_each_entry(tmp, &netlink_tap_all, list) {\r\nif (nt == tmp) {\r\nlist_del_rcu(&nt->list);\r\nfound = true;\r\ngoto out;\r\n}\r\n}\r\npr_warn("__netlink_remove_tap: %p not found\n", nt);\r\nout:\r\nspin_unlock(&netlink_tap_lock);\r\nif (found)\r\nmodule_put(nt->module);\r\nreturn found ? 0 : -ENODEV;\r\n}\r\nint netlink_remove_tap(struct netlink_tap *nt)\r\n{\r\nint ret;\r\nret = __netlink_remove_tap(nt);\r\nsynchronize_net();\r\nreturn ret;\r\n}\r\nstatic bool netlink_filter_tap(const struct sk_buff *skb)\r\n{\r\nstruct sock *sk = skb->sk;\r\nswitch (sk->sk_protocol) {\r\ncase NETLINK_ROUTE:\r\ncase NETLINK_USERSOCK:\r\ncase NETLINK_SOCK_DIAG:\r\ncase NETLINK_NFLOG:\r\ncase NETLINK_XFRM:\r\ncase NETLINK_FIB_LOOKUP:\r\ncase NETLINK_NETFILTER:\r\ncase NETLINK_GENERIC:\r\nreturn true;\r\n}\r\nreturn false;\r\n}\r\nstatic int __netlink_deliver_tap_skb(struct sk_buff *skb,\r\nstruct net_device *dev)\r\n{\r\nstruct sk_buff *nskb;\r\nstruct sock *sk = skb->sk;\r\nint ret = -ENOMEM;\r\ndev_hold(dev);\r\nif (is_vmalloc_addr(skb->head))\r\nnskb = netlink_to_full_skb(skb, GFP_ATOMIC);\r\nelse\r\nnskb = skb_clone(skb, GFP_ATOMIC);\r\nif (nskb) {\r\nnskb->dev = dev;\r\nnskb->protocol = htons((u16) sk->sk_protocol);\r\nnskb->pkt_type = netlink_is_kernel(sk) ?\r\nPACKET_KERNEL : PACKET_USER;\r\nskb_reset_network_header(nskb);\r\nret = dev_queue_xmit(nskb);\r\nif (unlikely(ret > 0))\r\nret = net_xmit_errno(ret);\r\n}\r\ndev_put(dev);\r\nreturn ret;\r\n}\r\nstatic void __netlink_deliver_tap(struct sk_buff *skb)\r\n{\r\nint ret;\r\nstruct netlink_tap *tmp;\r\nif (!netlink_filter_tap(skb))\r\nreturn;\r\nlist_for_each_entry_rcu(tmp, &netlink_tap_all, list) {\r\nret = __netlink_deliver_tap_skb(skb, tmp->dev);\r\nif (unlikely(ret))\r\nbreak;\r\n}\r\n}\r\nstatic void netlink_deliver_tap(struct sk_buff *skb)\r\n{\r\nrcu_read_lock();\r\nif (unlikely(!list_empty(&netlink_tap_all)))\r\n__netlink_deliver_tap(skb);\r\nrcu_read_unlock();\r\n}\r\nstatic void netlink_deliver_tap_kernel(struct sock *dst, struct sock *src,\r\nstruct sk_buff *skb)\r\n{\r\nif (!(netlink_is_kernel(dst) && netlink_is_kernel(src)))\r\nnetlink_deliver_tap(skb);\r\n}\r\nstatic void netlink_overrun(struct sock *sk)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nif (!(nlk->flags & NETLINK_F_RECV_NO_ENOBUFS)) {\r\nif (!test_and_set_bit(NETLINK_S_CONGESTED,\r\n&nlk_sk(sk)->state)) {\r\nsk->sk_err = ENOBUFS;\r\nsk->sk_error_report(sk);\r\n}\r\n}\r\natomic_inc(&sk->sk_drops);\r\n}\r\nstatic void netlink_rcv_wake(struct sock *sk)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nif (skb_queue_empty(&sk->sk_receive_queue))\r\nclear_bit(NETLINK_S_CONGESTED, &nlk->state);\r\nif (!test_bit(NETLINK_S_CONGESTED, &nlk->state))\r\nwake_up_interruptible(&nlk->wait);\r\n}\r\nstatic void netlink_skb_destructor(struct sk_buff *skb)\r\n{\r\nif (is_vmalloc_addr(skb->head)) {\r\nif (!skb->cloned ||\r\n!atomic_dec_return(&(skb_shinfo(skb)->dataref)))\r\nvfree(skb->head);\r\nskb->head = NULL;\r\n}\r\nif (skb->sk != NULL)\r\nsock_rfree(skb);\r\n}\r\nstatic void netlink_skb_set_owner_r(struct sk_buff *skb, struct sock *sk)\r\n{\r\nWARN_ON(skb->sk != NULL);\r\nskb->sk = sk;\r\nskb->destructor = netlink_skb_destructor;\r\natomic_add(skb->truesize, &sk->sk_rmem_alloc);\r\nsk_mem_charge(sk, skb->truesize);\r\n}\r\nstatic void netlink_sock_destruct(struct sock *sk)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nif (nlk->cb_running) {\r\nif (nlk->cb.done)\r\nnlk->cb.done(&nlk->cb);\r\nmodule_put(nlk->cb.module);\r\nkfree_skb(nlk->cb.skb);\r\n}\r\nskb_queue_purge(&sk->sk_receive_queue);\r\nif (!sock_flag(sk, SOCK_DEAD)) {\r\nprintk(KERN_ERR "Freeing alive netlink socket %p\n", sk);\r\nreturn;\r\n}\r\nWARN_ON(atomic_read(&sk->sk_rmem_alloc));\r\nWARN_ON(atomic_read(&sk->sk_wmem_alloc));\r\nWARN_ON(nlk_sk(sk)->groups);\r\n}\r\nvoid netlink_table_grab(void)\r\n__acquires(nl_table_lock)\r\n{\r\nmight_sleep();\r\nwrite_lock_irq(&nl_table_lock);\r\nif (atomic_read(&nl_table_users)) {\r\nDECLARE_WAITQUEUE(wait, current);\r\nadd_wait_queue_exclusive(&nl_table_wait, &wait);\r\nfor (;;) {\r\nset_current_state(TASK_UNINTERRUPTIBLE);\r\nif (atomic_read(&nl_table_users) == 0)\r\nbreak;\r\nwrite_unlock_irq(&nl_table_lock);\r\nschedule();\r\nwrite_lock_irq(&nl_table_lock);\r\n}\r\n__set_current_state(TASK_RUNNING);\r\nremove_wait_queue(&nl_table_wait, &wait);\r\n}\r\n}\r\nvoid netlink_table_ungrab(void)\r\n__releases(nl_table_lock)\r\n{\r\nwrite_unlock_irq(&nl_table_lock);\r\nwake_up(&nl_table_wait);\r\n}\r\nstatic inline void\r\nnetlink_lock_table(void)\r\n{\r\nread_lock(&nl_table_lock);\r\natomic_inc(&nl_table_users);\r\nread_unlock(&nl_table_lock);\r\n}\r\nstatic inline void\r\nnetlink_unlock_table(void)\r\n{\r\nif (atomic_dec_and_test(&nl_table_users))\r\nwake_up(&nl_table_wait);\r\n}\r\nstatic inline int netlink_compare(struct rhashtable_compare_arg *arg,\r\nconst void *ptr)\r\n{\r\nconst struct netlink_compare_arg *x = arg->key;\r\nconst struct netlink_sock *nlk = ptr;\r\nreturn nlk->portid != x->portid ||\r\n!net_eq(sock_net(&nlk->sk), read_pnet(&x->pnet));\r\n}\r\nstatic void netlink_compare_arg_init(struct netlink_compare_arg *arg,\r\nstruct net *net, u32 portid)\r\n{\r\nmemset(arg, 0, sizeof(*arg));\r\nwrite_pnet(&arg->pnet, net);\r\narg->portid = portid;\r\n}\r\nstatic struct sock *__netlink_lookup(struct netlink_table *table, u32 portid,\r\nstruct net *net)\r\n{\r\nstruct netlink_compare_arg arg;\r\nnetlink_compare_arg_init(&arg, net, portid);\r\nreturn rhashtable_lookup_fast(&table->hash, &arg,\r\nnetlink_rhashtable_params);\r\n}\r\nstatic int __netlink_insert(struct netlink_table *table, struct sock *sk)\r\n{\r\nstruct netlink_compare_arg arg;\r\nnetlink_compare_arg_init(&arg, sock_net(sk), nlk_sk(sk)->portid);\r\nreturn rhashtable_lookup_insert_key(&table->hash, &arg,\r\n&nlk_sk(sk)->node,\r\nnetlink_rhashtable_params);\r\n}\r\nstatic struct sock *netlink_lookup(struct net *net, int protocol, u32 portid)\r\n{\r\nstruct netlink_table *table = &nl_table[protocol];\r\nstruct sock *sk;\r\nrcu_read_lock();\r\nsk = __netlink_lookup(table, portid, net);\r\nif (sk)\r\nsock_hold(sk);\r\nrcu_read_unlock();\r\nreturn sk;\r\n}\r\nstatic void\r\nnetlink_update_listeners(struct sock *sk)\r\n{\r\nstruct netlink_table *tbl = &nl_table[sk->sk_protocol];\r\nunsigned long mask;\r\nunsigned int i;\r\nstruct listeners *listeners;\r\nlisteners = nl_deref_protected(tbl->listeners);\r\nif (!listeners)\r\nreturn;\r\nfor (i = 0; i < NLGRPLONGS(tbl->groups); i++) {\r\nmask = 0;\r\nsk_for_each_bound(sk, &tbl->mc_list) {\r\nif (i < NLGRPLONGS(nlk_sk(sk)->ngroups))\r\nmask |= nlk_sk(sk)->groups[i];\r\n}\r\nlisteners->masks[i] = mask;\r\n}\r\n}\r\nstatic int netlink_insert(struct sock *sk, u32 portid)\r\n{\r\nstruct netlink_table *table = &nl_table[sk->sk_protocol];\r\nint err;\r\nlock_sock(sk);\r\nerr = nlk_sk(sk)->portid == portid ? 0 : -EBUSY;\r\nif (nlk_sk(sk)->bound)\r\ngoto err;\r\nerr = -ENOMEM;\r\nif (BITS_PER_LONG > 32 &&\r\nunlikely(atomic_read(&table->hash.nelems) >= UINT_MAX))\r\ngoto err;\r\nnlk_sk(sk)->portid = portid;\r\nsock_hold(sk);\r\nerr = __netlink_insert(table, sk);\r\nif (err) {\r\nif (unlikely(err == -EBUSY))\r\nerr = -EOVERFLOW;\r\nif (err == -EEXIST)\r\nerr = -EADDRINUSE;\r\nsock_put(sk);\r\ngoto err;\r\n}\r\nsmp_wmb();\r\nnlk_sk(sk)->bound = portid;\r\nerr:\r\nrelease_sock(sk);\r\nreturn err;\r\n}\r\nstatic void netlink_remove(struct sock *sk)\r\n{\r\nstruct netlink_table *table;\r\ntable = &nl_table[sk->sk_protocol];\r\nif (!rhashtable_remove_fast(&table->hash, &nlk_sk(sk)->node,\r\nnetlink_rhashtable_params)) {\r\nWARN_ON(atomic_read(&sk->sk_refcnt) == 1);\r\n__sock_put(sk);\r\n}\r\nnetlink_table_grab();\r\nif (nlk_sk(sk)->subscriptions) {\r\n__sk_del_bind_node(sk);\r\nnetlink_update_listeners(sk);\r\n}\r\nif (sk->sk_protocol == NETLINK_GENERIC)\r\natomic_inc(&genl_sk_destructing_cnt);\r\nnetlink_table_ungrab();\r\n}\r\nstatic int __netlink_create(struct net *net, struct socket *sock,\r\nstruct mutex *cb_mutex, int protocol,\r\nint kern)\r\n{\r\nstruct sock *sk;\r\nstruct netlink_sock *nlk;\r\nsock->ops = &netlink_ops;\r\nsk = sk_alloc(net, PF_NETLINK, GFP_KERNEL, &netlink_proto, kern);\r\nif (!sk)\r\nreturn -ENOMEM;\r\nsock_init_data(sock, sk);\r\nnlk = nlk_sk(sk);\r\nif (cb_mutex) {\r\nnlk->cb_mutex = cb_mutex;\r\n} else {\r\nnlk->cb_mutex = &nlk->cb_def_mutex;\r\nmutex_init(nlk->cb_mutex);\r\n}\r\ninit_waitqueue_head(&nlk->wait);\r\nsk->sk_destruct = netlink_sock_destruct;\r\nsk->sk_protocol = protocol;\r\nreturn 0;\r\n}\r\nstatic int netlink_create(struct net *net, struct socket *sock, int protocol,\r\nint kern)\r\n{\r\nstruct module *module = NULL;\r\nstruct mutex *cb_mutex;\r\nstruct netlink_sock *nlk;\r\nint (*bind)(struct net *net, int group);\r\nvoid (*unbind)(struct net *net, int group);\r\nint err = 0;\r\nsock->state = SS_UNCONNECTED;\r\nif (sock->type != SOCK_RAW && sock->type != SOCK_DGRAM)\r\nreturn -ESOCKTNOSUPPORT;\r\nif (protocol < 0 || protocol >= MAX_LINKS)\r\nreturn -EPROTONOSUPPORT;\r\nnetlink_lock_table();\r\n#ifdef CONFIG_MODULES\r\nif (!nl_table[protocol].registered) {\r\nnetlink_unlock_table();\r\nrequest_module("net-pf-%d-proto-%d", PF_NETLINK, protocol);\r\nnetlink_lock_table();\r\n}\r\n#endif\r\nif (nl_table[protocol].registered &&\r\ntry_module_get(nl_table[protocol].module))\r\nmodule = nl_table[protocol].module;\r\nelse\r\nerr = -EPROTONOSUPPORT;\r\ncb_mutex = nl_table[protocol].cb_mutex;\r\nbind = nl_table[protocol].bind;\r\nunbind = nl_table[protocol].unbind;\r\nnetlink_unlock_table();\r\nif (err < 0)\r\ngoto out;\r\nerr = __netlink_create(net, sock, cb_mutex, protocol, kern);\r\nif (err < 0)\r\ngoto out_module;\r\nlocal_bh_disable();\r\nsock_prot_inuse_add(net, &netlink_proto, 1);\r\nlocal_bh_enable();\r\nnlk = nlk_sk(sock->sk);\r\nnlk->module = module;\r\nnlk->netlink_bind = bind;\r\nnlk->netlink_unbind = unbind;\r\nout:\r\nreturn err;\r\nout_module:\r\nmodule_put(module);\r\ngoto out;\r\n}\r\nstatic void deferred_put_nlk_sk(struct rcu_head *head)\r\n{\r\nstruct netlink_sock *nlk = container_of(head, struct netlink_sock, rcu);\r\nsock_put(&nlk->sk);\r\n}\r\nstatic int netlink_release(struct socket *sock)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct netlink_sock *nlk;\r\nif (!sk)\r\nreturn 0;\r\nnetlink_remove(sk);\r\nsock_orphan(sk);\r\nnlk = nlk_sk(sk);\r\nif (nlk->netlink_unbind) {\r\nint i;\r\nfor (i = 0; i < nlk->ngroups; i++)\r\nif (test_bit(i, nlk->groups))\r\nnlk->netlink_unbind(sock_net(sk), i + 1);\r\n}\r\nif (sk->sk_protocol == NETLINK_GENERIC &&\r\natomic_dec_return(&genl_sk_destructing_cnt) == 0)\r\nwake_up(&genl_sk_destructing_waitq);\r\nsock->sk = NULL;\r\nwake_up_interruptible_all(&nlk->wait);\r\nskb_queue_purge(&sk->sk_write_queue);\r\nif (nlk->portid && nlk->bound) {\r\nstruct netlink_notify n = {\r\n.net = sock_net(sk),\r\n.protocol = sk->sk_protocol,\r\n.portid = nlk->portid,\r\n};\r\natomic_notifier_call_chain(&netlink_chain,\r\nNETLINK_URELEASE, &n);\r\n}\r\nmodule_put(nlk->module);\r\nif (netlink_is_kernel(sk)) {\r\nnetlink_table_grab();\r\nBUG_ON(nl_table[sk->sk_protocol].registered == 0);\r\nif (--nl_table[sk->sk_protocol].registered == 0) {\r\nstruct listeners *old;\r\nold = nl_deref_protected(nl_table[sk->sk_protocol].listeners);\r\nRCU_INIT_POINTER(nl_table[sk->sk_protocol].listeners, NULL);\r\nkfree_rcu(old, rcu);\r\nnl_table[sk->sk_protocol].module = NULL;\r\nnl_table[sk->sk_protocol].bind = NULL;\r\nnl_table[sk->sk_protocol].unbind = NULL;\r\nnl_table[sk->sk_protocol].flags = 0;\r\nnl_table[sk->sk_protocol].registered = 0;\r\n}\r\nnetlink_table_ungrab();\r\n}\r\nkfree(nlk->groups);\r\nnlk->groups = NULL;\r\nlocal_bh_disable();\r\nsock_prot_inuse_add(sock_net(sk), &netlink_proto, -1);\r\nlocal_bh_enable();\r\ncall_rcu(&nlk->rcu, deferred_put_nlk_sk);\r\nreturn 0;\r\n}\r\nstatic int netlink_autobind(struct socket *sock)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct net *net = sock_net(sk);\r\nstruct netlink_table *table = &nl_table[sk->sk_protocol];\r\ns32 portid = task_tgid_vnr(current);\r\nint err;\r\ns32 rover = -4096;\r\nbool ok;\r\nretry:\r\ncond_resched();\r\nrcu_read_lock();\r\nok = !__netlink_lookup(table, portid, net);\r\nrcu_read_unlock();\r\nif (!ok) {\r\nif (rover == -4096)\r\nrover = S32_MIN + prandom_u32_max(-4096 - S32_MIN);\r\nelse if (rover >= -4096)\r\nrover = -4097;\r\nportid = rover--;\r\ngoto retry;\r\n}\r\nerr = netlink_insert(sk, portid);\r\nif (err == -EADDRINUSE)\r\ngoto retry;\r\nif (err == -EBUSY)\r\nerr = 0;\r\nreturn err;\r\n}\r\nbool __netlink_ns_capable(const struct netlink_skb_parms *nsp,\r\nstruct user_namespace *user_ns, int cap)\r\n{\r\nreturn ((nsp->flags & NETLINK_SKB_DST) ||\r\nfile_ns_capable(nsp->sk->sk_socket->file, user_ns, cap)) &&\r\nns_capable(user_ns, cap);\r\n}\r\nbool netlink_ns_capable(const struct sk_buff *skb,\r\nstruct user_namespace *user_ns, int cap)\r\n{\r\nreturn __netlink_ns_capable(&NETLINK_CB(skb), user_ns, cap);\r\n}\r\nbool netlink_capable(const struct sk_buff *skb, int cap)\r\n{\r\nreturn netlink_ns_capable(skb, &init_user_ns, cap);\r\n}\r\nbool netlink_net_capable(const struct sk_buff *skb, int cap)\r\n{\r\nreturn netlink_ns_capable(skb, sock_net(skb->sk)->user_ns, cap);\r\n}\r\nstatic inline int netlink_allowed(const struct socket *sock, unsigned int flag)\r\n{\r\nreturn (nl_table[sock->sk->sk_protocol].flags & flag) ||\r\nns_capable(sock_net(sock->sk)->user_ns, CAP_NET_ADMIN);\r\n}\r\nstatic void\r\nnetlink_update_subscriptions(struct sock *sk, unsigned int subscriptions)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nif (nlk->subscriptions && !subscriptions)\r\n__sk_del_bind_node(sk);\r\nelse if (!nlk->subscriptions && subscriptions)\r\nsk_add_bind_node(sk, &nl_table[sk->sk_protocol].mc_list);\r\nnlk->subscriptions = subscriptions;\r\n}\r\nstatic int netlink_realloc_groups(struct sock *sk)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nunsigned int groups;\r\nunsigned long *new_groups;\r\nint err = 0;\r\nnetlink_table_grab();\r\ngroups = nl_table[sk->sk_protocol].groups;\r\nif (!nl_table[sk->sk_protocol].registered) {\r\nerr = -ENOENT;\r\ngoto out_unlock;\r\n}\r\nif (nlk->ngroups >= groups)\r\ngoto out_unlock;\r\nnew_groups = krealloc(nlk->groups, NLGRPSZ(groups), GFP_ATOMIC);\r\nif (new_groups == NULL) {\r\nerr = -ENOMEM;\r\ngoto out_unlock;\r\n}\r\nmemset((char *)new_groups + NLGRPSZ(nlk->ngroups), 0,\r\nNLGRPSZ(groups) - NLGRPSZ(nlk->ngroups));\r\nnlk->groups = new_groups;\r\nnlk->ngroups = groups;\r\nout_unlock:\r\nnetlink_table_ungrab();\r\nreturn err;\r\n}\r\nstatic void netlink_undo_bind(int group, long unsigned int groups,\r\nstruct sock *sk)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nint undo;\r\nif (!nlk->netlink_unbind)\r\nreturn;\r\nfor (undo = 0; undo < group; undo++)\r\nif (test_bit(undo, &groups))\r\nnlk->netlink_unbind(sock_net(sk), undo + 1);\r\n}\r\nstatic int netlink_bind(struct socket *sock, struct sockaddr *addr,\r\nint addr_len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct net *net = sock_net(sk);\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nstruct sockaddr_nl *nladdr = (struct sockaddr_nl *)addr;\r\nint err;\r\nlong unsigned int groups = nladdr->nl_groups;\r\nbool bound;\r\nif (addr_len < sizeof(struct sockaddr_nl))\r\nreturn -EINVAL;\r\nif (nladdr->nl_family != AF_NETLINK)\r\nreturn -EINVAL;\r\nif (groups) {\r\nif (!netlink_allowed(sock, NL_CFG_F_NONROOT_RECV))\r\nreturn -EPERM;\r\nerr = netlink_realloc_groups(sk);\r\nif (err)\r\nreturn err;\r\n}\r\nbound = nlk->bound;\r\nif (bound) {\r\nsmp_rmb();\r\nif (nladdr->nl_pid != nlk->portid)\r\nreturn -EINVAL;\r\n}\r\nif (nlk->netlink_bind && groups) {\r\nint group;\r\nfor (group = 0; group < nlk->ngroups; group++) {\r\nif (!test_bit(group, &groups))\r\ncontinue;\r\nerr = nlk->netlink_bind(net, group + 1);\r\nif (!err)\r\ncontinue;\r\nnetlink_undo_bind(group, groups, sk);\r\nreturn err;\r\n}\r\n}\r\nif (!bound) {\r\nerr = nladdr->nl_pid ?\r\nnetlink_insert(sk, nladdr->nl_pid) :\r\nnetlink_autobind(sock);\r\nif (err) {\r\nnetlink_undo_bind(nlk->ngroups, groups, sk);\r\nreturn err;\r\n}\r\n}\r\nif (!groups && (nlk->groups == NULL || !(u32)nlk->groups[0]))\r\nreturn 0;\r\nnetlink_table_grab();\r\nnetlink_update_subscriptions(sk, nlk->subscriptions +\r\nhweight32(groups) -\r\nhweight32(nlk->groups[0]));\r\nnlk->groups[0] = (nlk->groups[0] & ~0xffffffffUL) | groups;\r\nnetlink_update_listeners(sk);\r\nnetlink_table_ungrab();\r\nreturn 0;\r\n}\r\nstatic int netlink_connect(struct socket *sock, struct sockaddr *addr,\r\nint alen, int flags)\r\n{\r\nint err = 0;\r\nstruct sock *sk = sock->sk;\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nstruct sockaddr_nl *nladdr = (struct sockaddr_nl *)addr;\r\nif (alen < sizeof(addr->sa_family))\r\nreturn -EINVAL;\r\nif (addr->sa_family == AF_UNSPEC) {\r\nsk->sk_state = NETLINK_UNCONNECTED;\r\nnlk->dst_portid = 0;\r\nnlk->dst_group = 0;\r\nreturn 0;\r\n}\r\nif (addr->sa_family != AF_NETLINK)\r\nreturn -EINVAL;\r\nif ((nladdr->nl_groups || nladdr->nl_pid) &&\r\n!netlink_allowed(sock, NL_CFG_F_NONROOT_SEND))\r\nreturn -EPERM;\r\nif (!nlk->bound)\r\nerr = netlink_autobind(sock);\r\nif (err == 0) {\r\nsk->sk_state = NETLINK_CONNECTED;\r\nnlk->dst_portid = nladdr->nl_pid;\r\nnlk->dst_group = ffs(nladdr->nl_groups);\r\n}\r\nreturn err;\r\n}\r\nstatic int netlink_getname(struct socket *sock, struct sockaddr *addr,\r\nint *addr_len, int peer)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nDECLARE_SOCKADDR(struct sockaddr_nl *, nladdr, addr);\r\nnladdr->nl_family = AF_NETLINK;\r\nnladdr->nl_pad = 0;\r\n*addr_len = sizeof(*nladdr);\r\nif (peer) {\r\nnladdr->nl_pid = nlk->dst_portid;\r\nnladdr->nl_groups = netlink_group_mask(nlk->dst_group);\r\n} else {\r\nnladdr->nl_pid = nlk->portid;\r\nnladdr->nl_groups = nlk->groups ? nlk->groups[0] : 0;\r\n}\r\nreturn 0;\r\n}\r\nstatic int netlink_ioctl(struct socket *sock, unsigned int cmd,\r\nunsigned long arg)\r\n{\r\nreturn -ENOIOCTLCMD;\r\n}\r\nstatic struct sock *netlink_getsockbyportid(struct sock *ssk, u32 portid)\r\n{\r\nstruct sock *sock;\r\nstruct netlink_sock *nlk;\r\nsock = netlink_lookup(sock_net(ssk), ssk->sk_protocol, portid);\r\nif (!sock)\r\nreturn ERR_PTR(-ECONNREFUSED);\r\nnlk = nlk_sk(sock);\r\nif (sock->sk_state == NETLINK_CONNECTED &&\r\nnlk->dst_portid != nlk_sk(ssk)->portid) {\r\nsock_put(sock);\r\nreturn ERR_PTR(-ECONNREFUSED);\r\n}\r\nreturn sock;\r\n}\r\nstruct sock *netlink_getsockbyfilp(struct file *filp)\r\n{\r\nstruct inode *inode = file_inode(filp);\r\nstruct sock *sock;\r\nif (!S_ISSOCK(inode->i_mode))\r\nreturn ERR_PTR(-ENOTSOCK);\r\nsock = SOCKET_I(inode)->sk;\r\nif (sock->sk_family != AF_NETLINK)\r\nreturn ERR_PTR(-EINVAL);\r\nsock_hold(sock);\r\nreturn sock;\r\n}\r\nstatic struct sk_buff *netlink_alloc_large_skb(unsigned int size,\r\nint broadcast)\r\n{\r\nstruct sk_buff *skb;\r\nvoid *data;\r\nif (size <= NLMSG_GOODSIZE || broadcast)\r\nreturn alloc_skb(size, GFP_KERNEL);\r\nsize = SKB_DATA_ALIGN(size) +\r\nSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\r\ndata = vmalloc(size);\r\nif (data == NULL)\r\nreturn NULL;\r\nskb = __build_skb(data, size);\r\nif (skb == NULL)\r\nvfree(data);\r\nelse\r\nskb->destructor = netlink_skb_destructor;\r\nreturn skb;\r\n}\r\nint netlink_attachskb(struct sock *sk, struct sk_buff *skb,\r\nlong *timeo, struct sock *ssk)\r\n{\r\nstruct netlink_sock *nlk;\r\nnlk = nlk_sk(sk);\r\nif ((atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\r\ntest_bit(NETLINK_S_CONGESTED, &nlk->state))) {\r\nDECLARE_WAITQUEUE(wait, current);\r\nif (!*timeo) {\r\nif (!ssk || netlink_is_kernel(ssk))\r\nnetlink_overrun(sk);\r\nsock_put(sk);\r\nkfree_skb(skb);\r\nreturn -EAGAIN;\r\n}\r\n__set_current_state(TASK_INTERRUPTIBLE);\r\nadd_wait_queue(&nlk->wait, &wait);\r\nif ((atomic_read(&sk->sk_rmem_alloc) > sk->sk_rcvbuf ||\r\ntest_bit(NETLINK_S_CONGESTED, &nlk->state)) &&\r\n!sock_flag(sk, SOCK_DEAD))\r\n*timeo = schedule_timeout(*timeo);\r\n__set_current_state(TASK_RUNNING);\r\nremove_wait_queue(&nlk->wait, &wait);\r\nsock_put(sk);\r\nif (signal_pending(current)) {\r\nkfree_skb(skb);\r\nreturn sock_intr_errno(*timeo);\r\n}\r\nreturn 1;\r\n}\r\nnetlink_skb_set_owner_r(skb, sk);\r\nreturn 0;\r\n}\r\nstatic int __netlink_sendskb(struct sock *sk, struct sk_buff *skb)\r\n{\r\nint len = skb->len;\r\nnetlink_deliver_tap(skb);\r\nskb_queue_tail(&sk->sk_receive_queue, skb);\r\nsk->sk_data_ready(sk);\r\nreturn len;\r\n}\r\nint netlink_sendskb(struct sock *sk, struct sk_buff *skb)\r\n{\r\nint len = __netlink_sendskb(sk, skb);\r\nsock_put(sk);\r\nreturn len;\r\n}\r\nvoid netlink_detachskb(struct sock *sk, struct sk_buff *skb)\r\n{\r\nkfree_skb(skb);\r\nsock_put(sk);\r\n}\r\nstatic struct sk_buff *netlink_trim(struct sk_buff *skb, gfp_t allocation)\r\n{\r\nint delta;\r\nWARN_ON(skb->sk != NULL);\r\ndelta = skb->end - skb->tail;\r\nif (is_vmalloc_addr(skb->head) || delta * 2 < skb->truesize)\r\nreturn skb;\r\nif (skb_shared(skb)) {\r\nstruct sk_buff *nskb = skb_clone(skb, allocation);\r\nif (!nskb)\r\nreturn skb;\r\nconsume_skb(skb);\r\nskb = nskb;\r\n}\r\nif (!pskb_expand_head(skb, 0, -delta, allocation))\r\nskb->truesize -= delta;\r\nreturn skb;\r\n}\r\nstatic int netlink_unicast_kernel(struct sock *sk, struct sk_buff *skb,\r\nstruct sock *ssk)\r\n{\r\nint ret;\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nret = -ECONNREFUSED;\r\nif (nlk->netlink_rcv != NULL) {\r\nret = skb->len;\r\nnetlink_skb_set_owner_r(skb, sk);\r\nNETLINK_CB(skb).sk = ssk;\r\nnetlink_deliver_tap_kernel(sk, ssk, skb);\r\nnlk->netlink_rcv(skb);\r\nconsume_skb(skb);\r\n} else {\r\nkfree_skb(skb);\r\n}\r\nsock_put(sk);\r\nreturn ret;\r\n}\r\nint netlink_unicast(struct sock *ssk, struct sk_buff *skb,\r\nu32 portid, int nonblock)\r\n{\r\nstruct sock *sk;\r\nint err;\r\nlong timeo;\r\nskb = netlink_trim(skb, gfp_any());\r\ntimeo = sock_sndtimeo(ssk, nonblock);\r\nretry:\r\nsk = netlink_getsockbyportid(ssk, portid);\r\nif (IS_ERR(sk)) {\r\nkfree_skb(skb);\r\nreturn PTR_ERR(sk);\r\n}\r\nif (netlink_is_kernel(sk))\r\nreturn netlink_unicast_kernel(sk, skb, ssk);\r\nif (sk_filter(sk, skb)) {\r\nerr = skb->len;\r\nkfree_skb(skb);\r\nsock_put(sk);\r\nreturn err;\r\n}\r\nerr = netlink_attachskb(sk, skb, &timeo, ssk);\r\nif (err == 1)\r\ngoto retry;\r\nif (err)\r\nreturn err;\r\nreturn netlink_sendskb(sk, skb);\r\n}\r\nint netlink_has_listeners(struct sock *sk, unsigned int group)\r\n{\r\nint res = 0;\r\nstruct listeners *listeners;\r\nBUG_ON(!netlink_is_kernel(sk));\r\nrcu_read_lock();\r\nlisteners = rcu_dereference(nl_table[sk->sk_protocol].listeners);\r\nif (listeners && group - 1 < nl_table[sk->sk_protocol].groups)\r\nres = test_bit(group - 1, listeners->masks);\r\nrcu_read_unlock();\r\nreturn res;\r\n}\r\nstatic int netlink_broadcast_deliver(struct sock *sk, struct sk_buff *skb)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nif (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf &&\r\n!test_bit(NETLINK_S_CONGESTED, &nlk->state)) {\r\nnetlink_skb_set_owner_r(skb, sk);\r\n__netlink_sendskb(sk, skb);\r\nreturn atomic_read(&sk->sk_rmem_alloc) > (sk->sk_rcvbuf >> 1);\r\n}\r\nreturn -1;\r\n}\r\nstatic void do_one_broadcast(struct sock *sk,\r\nstruct netlink_broadcast_data *p)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nint val;\r\nif (p->exclude_sk == sk)\r\nreturn;\r\nif (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||\r\n!test_bit(p->group - 1, nlk->groups))\r\nreturn;\r\nif (!net_eq(sock_net(sk), p->net)) {\r\nif (!(nlk->flags & NETLINK_F_LISTEN_ALL_NSID))\r\nreturn;\r\nif (!peernet_has_id(sock_net(sk), p->net))\r\nreturn;\r\nif (!file_ns_capable(sk->sk_socket->file, p->net->user_ns,\r\nCAP_NET_BROADCAST))\r\nreturn;\r\n}\r\nif (p->failure) {\r\nnetlink_overrun(sk);\r\nreturn;\r\n}\r\nsock_hold(sk);\r\nif (p->skb2 == NULL) {\r\nif (skb_shared(p->skb)) {\r\np->skb2 = skb_clone(p->skb, p->allocation);\r\n} else {\r\np->skb2 = skb_get(p->skb);\r\nskb_orphan(p->skb2);\r\n}\r\n}\r\nif (p->skb2 == NULL) {\r\nnetlink_overrun(sk);\r\np->failure = 1;\r\nif (nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR)\r\np->delivery_failure = 1;\r\ngoto out;\r\n}\r\nif (p->tx_filter && p->tx_filter(sk, p->skb2, p->tx_data)) {\r\nkfree_skb(p->skb2);\r\np->skb2 = NULL;\r\ngoto out;\r\n}\r\nif (sk_filter(sk, p->skb2)) {\r\nkfree_skb(p->skb2);\r\np->skb2 = NULL;\r\ngoto out;\r\n}\r\nNETLINK_CB(p->skb2).nsid = peernet2id(sock_net(sk), p->net);\r\nNETLINK_CB(p->skb2).nsid_is_set = true;\r\nval = netlink_broadcast_deliver(sk, p->skb2);\r\nif (val < 0) {\r\nnetlink_overrun(sk);\r\nif (nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR)\r\np->delivery_failure = 1;\r\n} else {\r\np->congested |= val;\r\np->delivered = 1;\r\np->skb2 = NULL;\r\n}\r\nout:\r\nsock_put(sk);\r\n}\r\nint netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid,\r\nu32 group, gfp_t allocation,\r\nint (*filter)(struct sock *dsk, struct sk_buff *skb, void *data),\r\nvoid *filter_data)\r\n{\r\nstruct net *net = sock_net(ssk);\r\nstruct netlink_broadcast_data info;\r\nstruct sock *sk;\r\nskb = netlink_trim(skb, allocation);\r\ninfo.exclude_sk = ssk;\r\ninfo.net = net;\r\ninfo.portid = portid;\r\ninfo.group = group;\r\ninfo.failure = 0;\r\ninfo.delivery_failure = 0;\r\ninfo.congested = 0;\r\ninfo.delivered = 0;\r\ninfo.allocation = allocation;\r\ninfo.skb = skb;\r\ninfo.skb2 = NULL;\r\ninfo.tx_filter = filter;\r\ninfo.tx_data = filter_data;\r\nnetlink_lock_table();\r\nsk_for_each_bound(sk, &nl_table[ssk->sk_protocol].mc_list)\r\ndo_one_broadcast(sk, &info);\r\nconsume_skb(skb);\r\nnetlink_unlock_table();\r\nif (info.delivery_failure) {\r\nkfree_skb(info.skb2);\r\nreturn -ENOBUFS;\r\n}\r\nconsume_skb(info.skb2);\r\nif (info.delivered) {\r\nif (info.congested && gfpflags_allow_blocking(allocation))\r\nyield();\r\nreturn 0;\r\n}\r\nreturn -ESRCH;\r\n}\r\nint netlink_broadcast(struct sock *ssk, struct sk_buff *skb, u32 portid,\r\nu32 group, gfp_t allocation)\r\n{\r\nreturn netlink_broadcast_filtered(ssk, skb, portid, group, allocation,\r\nNULL, NULL);\r\n}\r\nstatic int do_one_set_err(struct sock *sk, struct netlink_set_err_data *p)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nint ret = 0;\r\nif (sk == p->exclude_sk)\r\ngoto out;\r\nif (!net_eq(sock_net(sk), sock_net(p->exclude_sk)))\r\ngoto out;\r\nif (nlk->portid == p->portid || p->group - 1 >= nlk->ngroups ||\r\n!test_bit(p->group - 1, nlk->groups))\r\ngoto out;\r\nif (p->code == ENOBUFS && nlk->flags & NETLINK_F_RECV_NO_ENOBUFS) {\r\nret = 1;\r\ngoto out;\r\n}\r\nsk->sk_err = p->code;\r\nsk->sk_error_report(sk);\r\nout:\r\nreturn ret;\r\n}\r\nint netlink_set_err(struct sock *ssk, u32 portid, u32 group, int code)\r\n{\r\nstruct netlink_set_err_data info;\r\nstruct sock *sk;\r\nint ret = 0;\r\ninfo.exclude_sk = ssk;\r\ninfo.portid = portid;\r\ninfo.group = group;\r\ninfo.code = -code;\r\nread_lock(&nl_table_lock);\r\nsk_for_each_bound(sk, &nl_table[ssk->sk_protocol].mc_list)\r\nret += do_one_set_err(sk, &info);\r\nread_unlock(&nl_table_lock);\r\nreturn ret;\r\n}\r\nstatic void netlink_update_socket_mc(struct netlink_sock *nlk,\r\nunsigned int group,\r\nint is_new)\r\n{\r\nint old, new = !!is_new, subscriptions;\r\nold = test_bit(group - 1, nlk->groups);\r\nsubscriptions = nlk->subscriptions - old + new;\r\nif (new)\r\n__set_bit(group - 1, nlk->groups);\r\nelse\r\n__clear_bit(group - 1, nlk->groups);\r\nnetlink_update_subscriptions(&nlk->sk, subscriptions);\r\nnetlink_update_listeners(&nlk->sk);\r\n}\r\nstatic int netlink_setsockopt(struct socket *sock, int level, int optname,\r\nchar __user *optval, unsigned int optlen)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nunsigned int val = 0;\r\nint err;\r\nif (level != SOL_NETLINK)\r\nreturn -ENOPROTOOPT;\r\nif (optlen >= sizeof(int) &&\r\nget_user(val, (unsigned int __user *)optval))\r\nreturn -EFAULT;\r\nswitch (optname) {\r\ncase NETLINK_PKTINFO:\r\nif (val)\r\nnlk->flags |= NETLINK_F_RECV_PKTINFO;\r\nelse\r\nnlk->flags &= ~NETLINK_F_RECV_PKTINFO;\r\nerr = 0;\r\nbreak;\r\ncase NETLINK_ADD_MEMBERSHIP:\r\ncase NETLINK_DROP_MEMBERSHIP: {\r\nif (!netlink_allowed(sock, NL_CFG_F_NONROOT_RECV))\r\nreturn -EPERM;\r\nerr = netlink_realloc_groups(sk);\r\nif (err)\r\nreturn err;\r\nif (!val || val - 1 >= nlk->ngroups)\r\nreturn -EINVAL;\r\nif (optname == NETLINK_ADD_MEMBERSHIP && nlk->netlink_bind) {\r\nerr = nlk->netlink_bind(sock_net(sk), val);\r\nif (err)\r\nreturn err;\r\n}\r\nnetlink_table_grab();\r\nnetlink_update_socket_mc(nlk, val,\r\noptname == NETLINK_ADD_MEMBERSHIP);\r\nnetlink_table_ungrab();\r\nif (optname == NETLINK_DROP_MEMBERSHIP && nlk->netlink_unbind)\r\nnlk->netlink_unbind(sock_net(sk), val);\r\nerr = 0;\r\nbreak;\r\n}\r\ncase NETLINK_BROADCAST_ERROR:\r\nif (val)\r\nnlk->flags |= NETLINK_F_BROADCAST_SEND_ERROR;\r\nelse\r\nnlk->flags &= ~NETLINK_F_BROADCAST_SEND_ERROR;\r\nerr = 0;\r\nbreak;\r\ncase NETLINK_NO_ENOBUFS:\r\nif (val) {\r\nnlk->flags |= NETLINK_F_RECV_NO_ENOBUFS;\r\nclear_bit(NETLINK_S_CONGESTED, &nlk->state);\r\nwake_up_interruptible(&nlk->wait);\r\n} else {\r\nnlk->flags &= ~NETLINK_F_RECV_NO_ENOBUFS;\r\n}\r\nerr = 0;\r\nbreak;\r\ncase NETLINK_LISTEN_ALL_NSID:\r\nif (!ns_capable(sock_net(sk)->user_ns, CAP_NET_BROADCAST))\r\nreturn -EPERM;\r\nif (val)\r\nnlk->flags |= NETLINK_F_LISTEN_ALL_NSID;\r\nelse\r\nnlk->flags &= ~NETLINK_F_LISTEN_ALL_NSID;\r\nerr = 0;\r\nbreak;\r\ncase NETLINK_CAP_ACK:\r\nif (val)\r\nnlk->flags |= NETLINK_F_CAP_ACK;\r\nelse\r\nnlk->flags &= ~NETLINK_F_CAP_ACK;\r\nerr = 0;\r\nbreak;\r\ndefault:\r\nerr = -ENOPROTOOPT;\r\n}\r\nreturn err;\r\n}\r\nstatic int netlink_getsockopt(struct socket *sock, int level, int optname,\r\nchar __user *optval, int __user *optlen)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nint len, val, err;\r\nif (level != SOL_NETLINK)\r\nreturn -ENOPROTOOPT;\r\nif (get_user(len, optlen))\r\nreturn -EFAULT;\r\nif (len < 0)\r\nreturn -EINVAL;\r\nswitch (optname) {\r\ncase NETLINK_PKTINFO:\r\nif (len < sizeof(int))\r\nreturn -EINVAL;\r\nlen = sizeof(int);\r\nval = nlk->flags & NETLINK_F_RECV_PKTINFO ? 1 : 0;\r\nif (put_user(len, optlen) ||\r\nput_user(val, optval))\r\nreturn -EFAULT;\r\nerr = 0;\r\nbreak;\r\ncase NETLINK_BROADCAST_ERROR:\r\nif (len < sizeof(int))\r\nreturn -EINVAL;\r\nlen = sizeof(int);\r\nval = nlk->flags & NETLINK_F_BROADCAST_SEND_ERROR ? 1 : 0;\r\nif (put_user(len, optlen) ||\r\nput_user(val, optval))\r\nreturn -EFAULT;\r\nerr = 0;\r\nbreak;\r\ncase NETLINK_NO_ENOBUFS:\r\nif (len < sizeof(int))\r\nreturn -EINVAL;\r\nlen = sizeof(int);\r\nval = nlk->flags & NETLINK_F_RECV_NO_ENOBUFS ? 1 : 0;\r\nif (put_user(len, optlen) ||\r\nput_user(val, optval))\r\nreturn -EFAULT;\r\nerr = 0;\r\nbreak;\r\ncase NETLINK_LIST_MEMBERSHIPS: {\r\nint pos, idx, shift;\r\nerr = 0;\r\nnetlink_lock_table();\r\nfor (pos = 0; pos * 8 < nlk->ngroups; pos += sizeof(u32)) {\r\nif (len - pos < sizeof(u32))\r\nbreak;\r\nidx = pos / sizeof(unsigned long);\r\nshift = (pos % sizeof(unsigned long)) * 8;\r\nif (put_user((u32)(nlk->groups[idx] >> shift),\r\n(u32 __user *)(optval + pos))) {\r\nerr = -EFAULT;\r\nbreak;\r\n}\r\n}\r\nif (put_user(ALIGN(nlk->ngroups / 8, sizeof(u32)), optlen))\r\nerr = -EFAULT;\r\nnetlink_unlock_table();\r\nbreak;\r\n}\r\ncase NETLINK_CAP_ACK:\r\nif (len < sizeof(int))\r\nreturn -EINVAL;\r\nlen = sizeof(int);\r\nval = nlk->flags & NETLINK_F_CAP_ACK ? 1 : 0;\r\nif (put_user(len, optlen) ||\r\nput_user(val, optval))\r\nreturn -EFAULT;\r\nerr = 0;\r\nbreak;\r\ndefault:\r\nerr = -ENOPROTOOPT;\r\n}\r\nreturn err;\r\n}\r\nstatic void netlink_cmsg_recv_pktinfo(struct msghdr *msg, struct sk_buff *skb)\r\n{\r\nstruct nl_pktinfo info;\r\ninfo.group = NETLINK_CB(skb).dst_group;\r\nput_cmsg(msg, SOL_NETLINK, NETLINK_PKTINFO, sizeof(info), &info);\r\n}\r\nstatic void netlink_cmsg_listen_all_nsid(struct sock *sk, struct msghdr *msg,\r\nstruct sk_buff *skb)\r\n{\r\nif (!NETLINK_CB(skb).nsid_is_set)\r\nreturn;\r\nput_cmsg(msg, SOL_NETLINK, NETLINK_LISTEN_ALL_NSID, sizeof(int),\r\n&NETLINK_CB(skb).nsid);\r\n}\r\nstatic int netlink_sendmsg(struct socket *sock, struct msghdr *msg, size_t len)\r\n{\r\nstruct sock *sk = sock->sk;\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nDECLARE_SOCKADDR(struct sockaddr_nl *, addr, msg->msg_name);\r\nu32 dst_portid;\r\nu32 dst_group;\r\nstruct sk_buff *skb;\r\nint err;\r\nstruct scm_cookie scm;\r\nu32 netlink_skb_flags = 0;\r\nif (msg->msg_flags&MSG_OOB)\r\nreturn -EOPNOTSUPP;\r\nerr = scm_send(sock, msg, &scm, true);\r\nif (err < 0)\r\nreturn err;\r\nif (msg->msg_namelen) {\r\nerr = -EINVAL;\r\nif (addr->nl_family != AF_NETLINK)\r\ngoto out;\r\ndst_portid = addr->nl_pid;\r\ndst_group = ffs(addr->nl_groups);\r\nerr = -EPERM;\r\nif ((dst_group || dst_portid) &&\r\n!netlink_allowed(sock, NL_CFG_F_NONROOT_SEND))\r\ngoto out;\r\nnetlink_skb_flags |= NETLINK_SKB_DST;\r\n} else {\r\ndst_portid = nlk->dst_portid;\r\ndst_group = nlk->dst_group;\r\n}\r\nif (!nlk->bound) {\r\nerr = netlink_autobind(sock);\r\nif (err)\r\ngoto out;\r\n} else {\r\nsmp_rmb();\r\n}\r\nerr = -EMSGSIZE;\r\nif (len > sk->sk_sndbuf - 32)\r\ngoto out;\r\nerr = -ENOBUFS;\r\nskb = netlink_alloc_large_skb(len, dst_group);\r\nif (skb == NULL)\r\ngoto out;\r\nNETLINK_CB(skb).portid = nlk->portid;\r\nNETLINK_CB(skb).dst_group = dst_group;\r\nNETLINK_CB(skb).creds = scm.creds;\r\nNETLINK_CB(skb).flags = netlink_skb_flags;\r\nerr = -EFAULT;\r\nif (memcpy_from_msg(skb_put(skb, len), msg, len)) {\r\nkfree_skb(skb);\r\ngoto out;\r\n}\r\nerr = security_netlink_send(sk, skb);\r\nif (err) {\r\nkfree_skb(skb);\r\ngoto out;\r\n}\r\nif (dst_group) {\r\natomic_inc(&skb->users);\r\nnetlink_broadcast(sk, skb, dst_portid, dst_group, GFP_KERNEL);\r\n}\r\nerr = netlink_unicast(sk, skb, dst_portid, msg->msg_flags&MSG_DONTWAIT);\r\nout:\r\nscm_destroy(&scm);\r\nreturn err;\r\n}\r\nstatic int netlink_recvmsg(struct socket *sock, struct msghdr *msg, size_t len,\r\nint flags)\r\n{\r\nstruct scm_cookie scm;\r\nstruct sock *sk = sock->sk;\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nint noblock = flags&MSG_DONTWAIT;\r\nsize_t copied;\r\nstruct sk_buff *skb, *data_skb;\r\nint err, ret;\r\nif (flags&MSG_OOB)\r\nreturn -EOPNOTSUPP;\r\ncopied = 0;\r\nskb = skb_recv_datagram(sk, flags, noblock, &err);\r\nif (skb == NULL)\r\ngoto out;\r\ndata_skb = skb;\r\n#ifdef CONFIG_COMPAT_NETLINK_MESSAGES\r\nif (unlikely(skb_shinfo(skb)->frag_list)) {\r\nif (flags & MSG_CMSG_COMPAT)\r\ndata_skb = skb_shinfo(skb)->frag_list;\r\n}\r\n#endif\r\nnlk->max_recvmsg_len = max(nlk->max_recvmsg_len, len);\r\nnlk->max_recvmsg_len = min_t(size_t, nlk->max_recvmsg_len,\r\n16384);\r\ncopied = data_skb->len;\r\nif (len < copied) {\r\nmsg->msg_flags |= MSG_TRUNC;\r\ncopied = len;\r\n}\r\nskb_reset_transport_header(data_skb);\r\nerr = skb_copy_datagram_msg(data_skb, 0, msg, copied);\r\nif (msg->msg_name) {\r\nDECLARE_SOCKADDR(struct sockaddr_nl *, addr, msg->msg_name);\r\naddr->nl_family = AF_NETLINK;\r\naddr->nl_pad = 0;\r\naddr->nl_pid = NETLINK_CB(skb).portid;\r\naddr->nl_groups = netlink_group_mask(NETLINK_CB(skb).dst_group);\r\nmsg->msg_namelen = sizeof(*addr);\r\n}\r\nif (nlk->flags & NETLINK_F_RECV_PKTINFO)\r\nnetlink_cmsg_recv_pktinfo(msg, skb);\r\nif (nlk->flags & NETLINK_F_LISTEN_ALL_NSID)\r\nnetlink_cmsg_listen_all_nsid(sk, msg, skb);\r\nmemset(&scm, 0, sizeof(scm));\r\nscm.creds = *NETLINK_CREDS(skb);\r\nif (flags & MSG_TRUNC)\r\ncopied = data_skb->len;\r\nskb_free_datagram(sk, skb);\r\nif (nlk->cb_running &&\r\natomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf / 2) {\r\nret = netlink_dump(sk);\r\nif (ret) {\r\nsk->sk_err = -ret;\r\nsk->sk_error_report(sk);\r\n}\r\n}\r\nscm_recv(sock, msg, &scm, flags);\r\nout:\r\nnetlink_rcv_wake(sk);\r\nreturn err ? : copied;\r\n}\r\nstatic void netlink_data_ready(struct sock *sk)\r\n{\r\nBUG();\r\n}\r\nstruct sock *\r\n__netlink_kernel_create(struct net *net, int unit, struct module *module,\r\nstruct netlink_kernel_cfg *cfg)\r\n{\r\nstruct socket *sock;\r\nstruct sock *sk;\r\nstruct netlink_sock *nlk;\r\nstruct listeners *listeners = NULL;\r\nstruct mutex *cb_mutex = cfg ? cfg->cb_mutex : NULL;\r\nunsigned int groups;\r\nBUG_ON(!nl_table);\r\nif (unit < 0 || unit >= MAX_LINKS)\r\nreturn NULL;\r\nif (sock_create_lite(PF_NETLINK, SOCK_DGRAM, unit, &sock))\r\nreturn NULL;\r\nif (__netlink_create(net, sock, cb_mutex, unit, 1) < 0)\r\ngoto out_sock_release_nosk;\r\nsk = sock->sk;\r\nif (!cfg || cfg->groups < 32)\r\ngroups = 32;\r\nelse\r\ngroups = cfg->groups;\r\nlisteners = kzalloc(sizeof(*listeners) + NLGRPSZ(groups), GFP_KERNEL);\r\nif (!listeners)\r\ngoto out_sock_release;\r\nsk->sk_data_ready = netlink_data_ready;\r\nif (cfg && cfg->input)\r\nnlk_sk(sk)->netlink_rcv = cfg->input;\r\nif (netlink_insert(sk, 0))\r\ngoto out_sock_release;\r\nnlk = nlk_sk(sk);\r\nnlk->flags |= NETLINK_F_KERNEL_SOCKET;\r\nnetlink_table_grab();\r\nif (!nl_table[unit].registered) {\r\nnl_table[unit].groups = groups;\r\nrcu_assign_pointer(nl_table[unit].listeners, listeners);\r\nnl_table[unit].cb_mutex = cb_mutex;\r\nnl_table[unit].module = module;\r\nif (cfg) {\r\nnl_table[unit].bind = cfg->bind;\r\nnl_table[unit].unbind = cfg->unbind;\r\nnl_table[unit].flags = cfg->flags;\r\nif (cfg->compare)\r\nnl_table[unit].compare = cfg->compare;\r\n}\r\nnl_table[unit].registered = 1;\r\n} else {\r\nkfree(listeners);\r\nnl_table[unit].registered++;\r\n}\r\nnetlink_table_ungrab();\r\nreturn sk;\r\nout_sock_release:\r\nkfree(listeners);\r\nnetlink_kernel_release(sk);\r\nreturn NULL;\r\nout_sock_release_nosk:\r\nsock_release(sock);\r\nreturn NULL;\r\n}\r\nvoid\r\nnetlink_kernel_release(struct sock *sk)\r\n{\r\nif (sk == NULL || sk->sk_socket == NULL)\r\nreturn;\r\nsock_release(sk->sk_socket);\r\n}\r\nint __netlink_change_ngroups(struct sock *sk, unsigned int groups)\r\n{\r\nstruct listeners *new, *old;\r\nstruct netlink_table *tbl = &nl_table[sk->sk_protocol];\r\nif (groups < 32)\r\ngroups = 32;\r\nif (NLGRPSZ(tbl->groups) < NLGRPSZ(groups)) {\r\nnew = kzalloc(sizeof(*new) + NLGRPSZ(groups), GFP_ATOMIC);\r\nif (!new)\r\nreturn -ENOMEM;\r\nold = nl_deref_protected(tbl->listeners);\r\nmemcpy(new->masks, old->masks, NLGRPSZ(tbl->groups));\r\nrcu_assign_pointer(tbl->listeners, new);\r\nkfree_rcu(old, rcu);\r\n}\r\ntbl->groups = groups;\r\nreturn 0;\r\n}\r\nint netlink_change_ngroups(struct sock *sk, unsigned int groups)\r\n{\r\nint err;\r\nnetlink_table_grab();\r\nerr = __netlink_change_ngroups(sk, groups);\r\nnetlink_table_ungrab();\r\nreturn err;\r\n}\r\nvoid __netlink_clear_multicast_users(struct sock *ksk, unsigned int group)\r\n{\r\nstruct sock *sk;\r\nstruct netlink_table *tbl = &nl_table[ksk->sk_protocol];\r\nsk_for_each_bound(sk, &tbl->mc_list)\r\nnetlink_update_socket_mc(nlk_sk(sk), group, 0);\r\n}\r\nstruct nlmsghdr *\r\n__nlmsg_put(struct sk_buff *skb, u32 portid, u32 seq, int type, int len, int flags)\r\n{\r\nstruct nlmsghdr *nlh;\r\nint size = nlmsg_msg_size(len);\r\nnlh = (struct nlmsghdr *)skb_put(skb, NLMSG_ALIGN(size));\r\nnlh->nlmsg_type = type;\r\nnlh->nlmsg_len = size;\r\nnlh->nlmsg_flags = flags;\r\nnlh->nlmsg_pid = portid;\r\nnlh->nlmsg_seq = seq;\r\nif (!__builtin_constant_p(size) || NLMSG_ALIGN(size) - size != 0)\r\nmemset(nlmsg_data(nlh) + len, 0, NLMSG_ALIGN(size) - size);\r\nreturn nlh;\r\n}\r\nstatic int netlink_dump(struct sock *sk)\r\n{\r\nstruct netlink_sock *nlk = nlk_sk(sk);\r\nstruct netlink_callback *cb;\r\nstruct sk_buff *skb = NULL;\r\nstruct nlmsghdr *nlh;\r\nstruct module *module;\r\nint len, err = -ENOBUFS;\r\nint alloc_min_size;\r\nint alloc_size;\r\nmutex_lock(nlk->cb_mutex);\r\nif (!nlk->cb_running) {\r\nerr = -EINVAL;\r\ngoto errout_skb;\r\n}\r\nif (atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\r\ngoto errout_skb;\r\ncb = &nlk->cb;\r\nalloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\r\nif (alloc_min_size < nlk->max_recvmsg_len) {\r\nalloc_size = nlk->max_recvmsg_len;\r\nskb = alloc_skb(alloc_size, GFP_KERNEL |\r\n__GFP_NOWARN | __GFP_NORETRY);\r\n}\r\nif (!skb) {\r\nalloc_size = alloc_min_size;\r\nskb = alloc_skb(alloc_size, GFP_KERNEL);\r\n}\r\nif (!skb)\r\ngoto errout_skb;\r\nskb_reserve(skb, skb_tailroom(skb) - alloc_size);\r\nnetlink_skb_set_owner_r(skb, sk);\r\nlen = cb->dump(skb, cb);\r\nif (len > 0) {\r\nmutex_unlock(nlk->cb_mutex);\r\nif (sk_filter(sk, skb))\r\nkfree_skb(skb);\r\nelse\r\n__netlink_sendskb(sk, skb);\r\nreturn 0;\r\n}\r\nnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\r\nif (!nlh)\r\ngoto errout_skb;\r\nnl_dump_check_consistent(cb, nlh);\r\nmemcpy(nlmsg_data(nlh), &len, sizeof(len));\r\nif (sk_filter(sk, skb))\r\nkfree_skb(skb);\r\nelse\r\n__netlink_sendskb(sk, skb);\r\nif (cb->done)\r\ncb->done(cb);\r\nnlk->cb_running = false;\r\nmodule = cb->module;\r\nskb = cb->skb;\r\nmutex_unlock(nlk->cb_mutex);\r\nmodule_put(module);\r\nconsume_skb(skb);\r\nreturn 0;\r\nerrout_skb:\r\nmutex_unlock(nlk->cb_mutex);\r\nkfree_skb(skb);\r\nreturn err;\r\n}\r\nint __netlink_dump_start(struct sock *ssk, struct sk_buff *skb,\r\nconst struct nlmsghdr *nlh,\r\nstruct netlink_dump_control *control)\r\n{\r\nstruct netlink_callback *cb;\r\nstruct sock *sk;\r\nstruct netlink_sock *nlk;\r\nint ret;\r\natomic_inc(&skb->users);\r\nsk = netlink_lookup(sock_net(ssk), ssk->sk_protocol, NETLINK_CB(skb).portid);\r\nif (sk == NULL) {\r\nret = -ECONNREFUSED;\r\ngoto error_free;\r\n}\r\nnlk = nlk_sk(sk);\r\nmutex_lock(nlk->cb_mutex);\r\nif (nlk->cb_running) {\r\nret = -EBUSY;\r\ngoto error_unlock;\r\n}\r\nif (!try_module_get(control->module)) {\r\nret = -EPROTONOSUPPORT;\r\ngoto error_unlock;\r\n}\r\ncb = &nlk->cb;\r\nmemset(cb, 0, sizeof(*cb));\r\ncb->start = control->start;\r\ncb->dump = control->dump;\r\ncb->done = control->done;\r\ncb->nlh = nlh;\r\ncb->data = control->data;\r\ncb->module = control->module;\r\ncb->min_dump_alloc = control->min_dump_alloc;\r\ncb->skb = skb;\r\nnlk->cb_running = true;\r\nmutex_unlock(nlk->cb_mutex);\r\nif (cb->start)\r\ncb->start(cb);\r\nret = netlink_dump(sk);\r\nsock_put(sk);\r\nif (ret)\r\nreturn ret;\r\nreturn -EINTR;\r\nerror_unlock:\r\nsock_put(sk);\r\nmutex_unlock(nlk->cb_mutex);\r\nerror_free:\r\nkfree_skb(skb);\r\nreturn ret;\r\n}\r\nvoid netlink_ack(struct sk_buff *in_skb, struct nlmsghdr *nlh, int err)\r\n{\r\nstruct sk_buff *skb;\r\nstruct nlmsghdr *rep;\r\nstruct nlmsgerr *errmsg;\r\nsize_t payload = sizeof(*errmsg);\r\nstruct netlink_sock *nlk = nlk_sk(NETLINK_CB(in_skb).sk);\r\nif (!(nlk->flags & NETLINK_F_CAP_ACK) && err)\r\npayload += nlmsg_len(nlh);\r\nskb = nlmsg_new(payload, GFP_KERNEL);\r\nif (!skb) {\r\nstruct sock *sk;\r\nsk = netlink_lookup(sock_net(in_skb->sk),\r\nin_skb->sk->sk_protocol,\r\nNETLINK_CB(in_skb).portid);\r\nif (sk) {\r\nsk->sk_err = ENOBUFS;\r\nsk->sk_error_report(sk);\r\nsock_put(sk);\r\n}\r\nreturn;\r\n}\r\nrep = __nlmsg_put(skb, NETLINK_CB(in_skb).portid, nlh->nlmsg_seq,\r\nNLMSG_ERROR, payload, 0);\r\nerrmsg = nlmsg_data(rep);\r\nerrmsg->error = err;\r\nmemcpy(&errmsg->msg, nlh, payload > sizeof(*errmsg) ? nlh->nlmsg_len : sizeof(*nlh));\r\nnetlink_unicast(in_skb->sk, skb, NETLINK_CB(in_skb).portid, MSG_DONTWAIT);\r\n}\r\nint netlink_rcv_skb(struct sk_buff *skb, int (*cb)(struct sk_buff *,\r\nstruct nlmsghdr *))\r\n{\r\nstruct nlmsghdr *nlh;\r\nint err;\r\nwhile (skb->len >= nlmsg_total_size(0)) {\r\nint msglen;\r\nnlh = nlmsg_hdr(skb);\r\nerr = 0;\r\nif (nlh->nlmsg_len < NLMSG_HDRLEN || skb->len < nlh->nlmsg_len)\r\nreturn 0;\r\nif (!(nlh->nlmsg_flags & NLM_F_REQUEST))\r\ngoto ack;\r\nif (nlh->nlmsg_type < NLMSG_MIN_TYPE)\r\ngoto ack;\r\nerr = cb(skb, nlh);\r\nif (err == -EINTR)\r\ngoto skip;\r\nack:\r\nif (nlh->nlmsg_flags & NLM_F_ACK || err)\r\nnetlink_ack(skb, nlh, err);\r\nskip:\r\nmsglen = NLMSG_ALIGN(nlh->nlmsg_len);\r\nif (msglen > skb->len)\r\nmsglen = skb->len;\r\nskb_pull(skb, msglen);\r\n}\r\nreturn 0;\r\n}\r\nint nlmsg_notify(struct sock *sk, struct sk_buff *skb, u32 portid,\r\nunsigned int group, int report, gfp_t flags)\r\n{\r\nint err = 0;\r\nif (group) {\r\nint exclude_portid = 0;\r\nif (report) {\r\natomic_inc(&skb->users);\r\nexclude_portid = portid;\r\n}\r\nerr = nlmsg_multicast(sk, skb, exclude_portid, group, flags);\r\n}\r\nif (report) {\r\nint err2;\r\nerr2 = nlmsg_unicast(sk, skb, portid);\r\nif (!err || err == -ESRCH)\r\nerr = err2;\r\n}\r\nreturn err;\r\n}\r\nstatic int netlink_walk_start(struct nl_seq_iter *iter)\r\n{\r\nint err;\r\nerr = rhashtable_walk_init(&nl_table[iter->link].hash, &iter->hti,\r\nGFP_KERNEL);\r\nif (err) {\r\niter->link = MAX_LINKS;\r\nreturn err;\r\n}\r\nerr = rhashtable_walk_start(&iter->hti);\r\nreturn err == -EAGAIN ? 0 : err;\r\n}\r\nstatic void netlink_walk_stop(struct nl_seq_iter *iter)\r\n{\r\nrhashtable_walk_stop(&iter->hti);\r\nrhashtable_walk_exit(&iter->hti);\r\n}\r\nstatic void *__netlink_seq_next(struct seq_file *seq)\r\n{\r\nstruct nl_seq_iter *iter = seq->private;\r\nstruct netlink_sock *nlk;\r\ndo {\r\nfor (;;) {\r\nint err;\r\nnlk = rhashtable_walk_next(&iter->hti);\r\nif (IS_ERR(nlk)) {\r\nif (PTR_ERR(nlk) == -EAGAIN)\r\ncontinue;\r\nreturn nlk;\r\n}\r\nif (nlk)\r\nbreak;\r\nnetlink_walk_stop(iter);\r\nif (++iter->link >= MAX_LINKS)\r\nreturn NULL;\r\nerr = netlink_walk_start(iter);\r\nif (err)\r\nreturn ERR_PTR(err);\r\n}\r\n} while (sock_net(&nlk->sk) != seq_file_net(seq));\r\nreturn nlk;\r\n}\r\nstatic void *netlink_seq_start(struct seq_file *seq, loff_t *posp)\r\n{\r\nstruct nl_seq_iter *iter = seq->private;\r\nvoid *obj = SEQ_START_TOKEN;\r\nloff_t pos;\r\nint err;\r\niter->link = 0;\r\nerr = netlink_walk_start(iter);\r\nif (err)\r\nreturn ERR_PTR(err);\r\nfor (pos = *posp; pos && obj && !IS_ERR(obj); pos--)\r\nobj = __netlink_seq_next(seq);\r\nreturn obj;\r\n}\r\nstatic void *netlink_seq_next(struct seq_file *seq, void *v, loff_t *pos)\r\n{\r\n++*pos;\r\nreturn __netlink_seq_next(seq);\r\n}\r\nstatic void netlink_seq_stop(struct seq_file *seq, void *v)\r\n{\r\nstruct nl_seq_iter *iter = seq->private;\r\nif (iter->link >= MAX_LINKS)\r\nreturn;\r\nnetlink_walk_stop(iter);\r\n}\r\nstatic int netlink_seq_show(struct seq_file *seq, void *v)\r\n{\r\nif (v == SEQ_START_TOKEN) {\r\nseq_puts(seq,\r\n"sk Eth Pid Groups "\r\n"Rmem Wmem Dump Locks Drops Inode\n");\r\n} else {\r\nstruct sock *s = v;\r\nstruct netlink_sock *nlk = nlk_sk(s);\r\nseq_printf(seq, "%pK %-3d %-6u %08x %-8d %-8d %d %-8d %-8d %-8lu\n",\r\ns,\r\ns->sk_protocol,\r\nnlk->portid,\r\nnlk->groups ? (u32)nlk->groups[0] : 0,\r\nsk_rmem_alloc_get(s),\r\nsk_wmem_alloc_get(s),\r\nnlk->cb_running,\r\natomic_read(&s->sk_refcnt),\r\natomic_read(&s->sk_drops),\r\nsock_i_ino(s)\r\n);\r\n}\r\nreturn 0;\r\n}\r\nstatic int netlink_seq_open(struct inode *inode, struct file *file)\r\n{\r\nreturn seq_open_net(inode, file, &netlink_seq_ops,\r\nsizeof(struct nl_seq_iter));\r\n}\r\nint netlink_register_notifier(struct notifier_block *nb)\r\n{\r\nreturn atomic_notifier_chain_register(&netlink_chain, nb);\r\n}\r\nint netlink_unregister_notifier(struct notifier_block *nb)\r\n{\r\nreturn atomic_notifier_chain_unregister(&netlink_chain, nb);\r\n}\r\nstatic int __net_init netlink_net_init(struct net *net)\r\n{\r\n#ifdef CONFIG_PROC_FS\r\nif (!proc_create("netlink", 0, net->proc_net, &netlink_seq_fops))\r\nreturn -ENOMEM;\r\n#endif\r\nreturn 0;\r\n}\r\nstatic void __net_exit netlink_net_exit(struct net *net)\r\n{\r\n#ifdef CONFIG_PROC_FS\r\nremove_proc_entry("netlink", net->proc_net);\r\n#endif\r\n}\r\nstatic void __init netlink_add_usersock_entry(void)\r\n{\r\nstruct listeners *listeners;\r\nint groups = 32;\r\nlisteners = kzalloc(sizeof(*listeners) + NLGRPSZ(groups), GFP_KERNEL);\r\nif (!listeners)\r\npanic("netlink_add_usersock_entry: Cannot allocate listeners\n");\r\nnetlink_table_grab();\r\nnl_table[NETLINK_USERSOCK].groups = groups;\r\nrcu_assign_pointer(nl_table[NETLINK_USERSOCK].listeners, listeners);\r\nnl_table[NETLINK_USERSOCK].module = THIS_MODULE;\r\nnl_table[NETLINK_USERSOCK].registered = 1;\r\nnl_table[NETLINK_USERSOCK].flags = NL_CFG_F_NONROOT_SEND;\r\nnetlink_table_ungrab();\r\n}\r\nstatic inline u32 netlink_hash(const void *data, u32 len, u32 seed)\r\n{\r\nconst struct netlink_sock *nlk = data;\r\nstruct netlink_compare_arg arg;\r\nnetlink_compare_arg_init(&arg, sock_net(&nlk->sk), nlk->portid);\r\nreturn jhash2((u32 *)&arg, netlink_compare_arg_len / sizeof(u32), seed);\r\n}\r\nstatic int __init netlink_proto_init(void)\r\n{\r\nint i;\r\nint err = proto_register(&netlink_proto, 0);\r\nif (err != 0)\r\ngoto out;\r\nBUILD_BUG_ON(sizeof(struct netlink_skb_parms) > FIELD_SIZEOF(struct sk_buff, cb));\r\nnl_table = kcalloc(MAX_LINKS, sizeof(*nl_table), GFP_KERNEL);\r\nif (!nl_table)\r\ngoto panic;\r\nfor (i = 0; i < MAX_LINKS; i++) {\r\nif (rhashtable_init(&nl_table[i].hash,\r\n&netlink_rhashtable_params) < 0) {\r\nwhile (--i > 0)\r\nrhashtable_destroy(&nl_table[i].hash);\r\nkfree(nl_table);\r\ngoto panic;\r\n}\r\n}\r\nINIT_LIST_HEAD(&netlink_tap_all);\r\nnetlink_add_usersock_entry();\r\nsock_register(&netlink_family_ops);\r\nregister_pernet_subsys(&netlink_net_ops);\r\nrtnetlink_init();\r\nout:\r\nreturn err;\r\npanic:\r\npanic("netlink_init: Cannot allocate nl_table\n");\r\n}
