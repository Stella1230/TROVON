const char *xenbus_strstate(enum xenbus_state state)\r\n{\r\nstatic const char *const name[] = {\r\n[ XenbusStateUnknown ] = "Unknown",\r\n[ XenbusStateInitialising ] = "Initialising",\r\n[ XenbusStateInitWait ] = "InitWait",\r\n[ XenbusStateInitialised ] = "Initialised",\r\n[ XenbusStateConnected ] = "Connected",\r\n[ XenbusStateClosing ] = "Closing",\r\n[ XenbusStateClosed ] = "Closed",\r\n[XenbusStateReconfiguring] = "Reconfiguring",\r\n[XenbusStateReconfigured] = "Reconfigured",\r\n};\r\nreturn (state < ARRAY_SIZE(name)) ? name[state] : "INVALID";\r\n}\r\nint xenbus_watch_path(struct xenbus_device *dev, const char *path,\r\nstruct xenbus_watch *watch,\r\nvoid (*callback)(struct xenbus_watch *,\r\nconst char **, unsigned int))\r\n{\r\nint err;\r\nwatch->node = path;\r\nwatch->callback = callback;\r\nerr = register_xenbus_watch(watch);\r\nif (err) {\r\nwatch->node = NULL;\r\nwatch->callback = NULL;\r\nxenbus_dev_fatal(dev, err, "adding watch on %s", path);\r\n}\r\nreturn err;\r\n}\r\nint xenbus_watch_pathfmt(struct xenbus_device *dev,\r\nstruct xenbus_watch *watch,\r\nvoid (*callback)(struct xenbus_watch *,\r\nconst char **, unsigned int),\r\nconst char *pathfmt, ...)\r\n{\r\nint err;\r\nva_list ap;\r\nchar *path;\r\nva_start(ap, pathfmt);\r\npath = kvasprintf(GFP_NOIO | __GFP_HIGH, pathfmt, ap);\r\nva_end(ap);\r\nif (!path) {\r\nxenbus_dev_fatal(dev, -ENOMEM, "allocating path for watch");\r\nreturn -ENOMEM;\r\n}\r\nerr = xenbus_watch_path(dev, path, watch, callback);\r\nif (err)\r\nkfree(path);\r\nreturn err;\r\n}\r\nstatic int\r\n__xenbus_switch_state(struct xenbus_device *dev,\r\nenum xenbus_state state, int depth)\r\n{\r\nstruct xenbus_transaction xbt;\r\nint current_state;\r\nint err, abort;\r\nif (state == dev->state)\r\nreturn 0;\r\nagain:\r\nabort = 1;\r\nerr = xenbus_transaction_start(&xbt);\r\nif (err) {\r\nxenbus_switch_fatal(dev, depth, err, "starting transaction");\r\nreturn 0;\r\n}\r\nerr = xenbus_scanf(xbt, dev->nodename, "state", "%d", &current_state);\r\nif (err != 1)\r\ngoto abort;\r\nerr = xenbus_printf(xbt, dev->nodename, "state", "%d", state);\r\nif (err) {\r\nxenbus_switch_fatal(dev, depth, err, "writing new state");\r\ngoto abort;\r\n}\r\nabort = 0;\r\nabort:\r\nerr = xenbus_transaction_end(xbt, abort);\r\nif (err) {\r\nif (err == -EAGAIN && !abort)\r\ngoto again;\r\nxenbus_switch_fatal(dev, depth, err, "ending transaction");\r\n} else\r\ndev->state = state;\r\nreturn 0;\r\n}\r\nint xenbus_switch_state(struct xenbus_device *dev, enum xenbus_state state)\r\n{\r\nreturn __xenbus_switch_state(dev, state, 0);\r\n}\r\nint xenbus_frontend_closed(struct xenbus_device *dev)\r\n{\r\nxenbus_switch_state(dev, XenbusStateClosed);\r\ncomplete(&dev->down);\r\nreturn 0;\r\n}\r\nstatic char *error_path(struct xenbus_device *dev)\r\n{\r\nreturn kasprintf(GFP_KERNEL, "error/%s", dev->nodename);\r\n}\r\nstatic void xenbus_va_dev_error(struct xenbus_device *dev, int err,\r\nconst char *fmt, va_list ap)\r\n{\r\nunsigned int len;\r\nchar *printf_buffer = NULL;\r\nchar *path_buffer = NULL;\r\n#define PRINTF_BUFFER_SIZE 4096\r\nprintf_buffer = kmalloc(PRINTF_BUFFER_SIZE, GFP_KERNEL);\r\nif (printf_buffer == NULL)\r\ngoto fail;\r\nlen = sprintf(printf_buffer, "%i ", -err);\r\nvsnprintf(printf_buffer+len, PRINTF_BUFFER_SIZE-len, fmt, ap);\r\ndev_err(&dev->dev, "%s\n", printf_buffer);\r\npath_buffer = error_path(dev);\r\nif (path_buffer == NULL) {\r\ndev_err(&dev->dev, "failed to write error node for %s (%s)\n",\r\ndev->nodename, printf_buffer);\r\ngoto fail;\r\n}\r\nif (xenbus_write(XBT_NIL, path_buffer, "error", printf_buffer) != 0) {\r\ndev_err(&dev->dev, "failed to write error node for %s (%s)\n",\r\ndev->nodename, printf_buffer);\r\ngoto fail;\r\n}\r\nfail:\r\nkfree(printf_buffer);\r\nkfree(path_buffer);\r\n}\r\nvoid xenbus_dev_error(struct xenbus_device *dev, int err, const char *fmt, ...)\r\n{\r\nva_list ap;\r\nva_start(ap, fmt);\r\nxenbus_va_dev_error(dev, err, fmt, ap);\r\nva_end(ap);\r\n}\r\nvoid xenbus_dev_fatal(struct xenbus_device *dev, int err, const char *fmt, ...)\r\n{\r\nva_list ap;\r\nva_start(ap, fmt);\r\nxenbus_va_dev_error(dev, err, fmt, ap);\r\nva_end(ap);\r\nxenbus_switch_state(dev, XenbusStateClosing);\r\n}\r\nstatic void xenbus_switch_fatal(struct xenbus_device *dev, int depth, int err,\r\nconst char *fmt, ...)\r\n{\r\nva_list ap;\r\nva_start(ap, fmt);\r\nxenbus_va_dev_error(dev, err, fmt, ap);\r\nva_end(ap);\r\nif (!depth)\r\n__xenbus_switch_state(dev, XenbusStateClosing, 1);\r\n}\r\nint xenbus_grant_ring(struct xenbus_device *dev, void *vaddr,\r\nunsigned int nr_pages, grant_ref_t *grefs)\r\n{\r\nint err;\r\nint i, j;\r\nfor (i = 0; i < nr_pages; i++) {\r\nerr = gnttab_grant_foreign_access(dev->otherend_id,\r\nvirt_to_gfn(vaddr), 0);\r\nif (err < 0) {\r\nxenbus_dev_fatal(dev, err,\r\n"granting access to ring page");\r\ngoto fail;\r\n}\r\ngrefs[i] = err;\r\nvaddr = vaddr + XEN_PAGE_SIZE;\r\n}\r\nreturn 0;\r\nfail:\r\nfor (j = 0; j < i; j++)\r\ngnttab_end_foreign_access_ref(grefs[j], 0);\r\nreturn err;\r\n}\r\nint xenbus_alloc_evtchn(struct xenbus_device *dev, int *port)\r\n{\r\nstruct evtchn_alloc_unbound alloc_unbound;\r\nint err;\r\nalloc_unbound.dom = DOMID_SELF;\r\nalloc_unbound.remote_dom = dev->otherend_id;\r\nerr = HYPERVISOR_event_channel_op(EVTCHNOP_alloc_unbound,\r\n&alloc_unbound);\r\nif (err)\r\nxenbus_dev_fatal(dev, err, "allocating event channel");\r\nelse\r\n*port = alloc_unbound.port;\r\nreturn err;\r\n}\r\nint xenbus_free_evtchn(struct xenbus_device *dev, int port)\r\n{\r\nstruct evtchn_close close;\r\nint err;\r\nclose.port = port;\r\nerr = HYPERVISOR_event_channel_op(EVTCHNOP_close, &close);\r\nif (err)\r\nxenbus_dev_error(dev, err, "freeing event channel %d", port);\r\nreturn err;\r\n}\r\nint xenbus_map_ring_valloc(struct xenbus_device *dev, grant_ref_t *gnt_refs,\r\nunsigned int nr_grefs, void **vaddr)\r\n{\r\nreturn ring_ops->map(dev, gnt_refs, nr_grefs, vaddr);\r\n}\r\nstatic int __xenbus_map_ring(struct xenbus_device *dev,\r\ngrant_ref_t *gnt_refs,\r\nunsigned int nr_grefs,\r\ngrant_handle_t *handles,\r\nphys_addr_t *addrs,\r\nunsigned int flags,\r\nbool *leaked)\r\n{\r\nstruct gnttab_map_grant_ref map[XENBUS_MAX_RING_GRANTS];\r\nstruct gnttab_unmap_grant_ref unmap[XENBUS_MAX_RING_GRANTS];\r\nint i, j;\r\nint err = GNTST_okay;\r\nif (nr_grefs > XENBUS_MAX_RING_GRANTS)\r\nreturn -EINVAL;\r\nfor (i = 0; i < nr_grefs; i++) {\r\nmemset(&map[i], 0, sizeof(map[i]));\r\ngnttab_set_map_op(&map[i], addrs[i], flags, gnt_refs[i],\r\ndev->otherend_id);\r\nhandles[i] = INVALID_GRANT_HANDLE;\r\n}\r\ngnttab_batch_map(map, i);\r\nfor (i = 0; i < nr_grefs; i++) {\r\nif (map[i].status != GNTST_okay) {\r\nerr = map[i].status;\r\nxenbus_dev_fatal(dev, map[i].status,\r\n"mapping in shared page %d from domain %d",\r\ngnt_refs[i], dev->otherend_id);\r\ngoto fail;\r\n} else\r\nhandles[i] = map[i].handle;\r\n}\r\nreturn GNTST_okay;\r\nfail:\r\nfor (i = j = 0; i < nr_grefs; i++) {\r\nif (handles[i] != INVALID_GRANT_HANDLE) {\r\nmemset(&unmap[j], 0, sizeof(unmap[j]));\r\ngnttab_set_unmap_op(&unmap[j], (phys_addr_t)addrs[i],\r\nGNTMAP_host_map, handles[i]);\r\nj++;\r\n}\r\n}\r\nif (HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, unmap, j))\r\nBUG();\r\n*leaked = false;\r\nfor (i = 0; i < j; i++) {\r\nif (unmap[i].status != GNTST_okay) {\r\n*leaked = true;\r\nbreak;\r\n}\r\n}\r\nreturn err;\r\n}\r\nstatic int xenbus_map_ring_valloc_pv(struct xenbus_device *dev,\r\ngrant_ref_t *gnt_refs,\r\nunsigned int nr_grefs,\r\nvoid **vaddr)\r\n{\r\nstruct xenbus_map_node *node;\r\nstruct vm_struct *area;\r\npte_t *ptes[XENBUS_MAX_RING_GRANTS];\r\nphys_addr_t phys_addrs[XENBUS_MAX_RING_GRANTS];\r\nint err = GNTST_okay;\r\nint i;\r\nbool leaked;\r\n*vaddr = NULL;\r\nif (nr_grefs > XENBUS_MAX_RING_GRANTS)\r\nreturn -EINVAL;\r\nnode = kzalloc(sizeof(*node), GFP_KERNEL);\r\nif (!node)\r\nreturn -ENOMEM;\r\narea = alloc_vm_area(XEN_PAGE_SIZE * nr_grefs, ptes);\r\nif (!area) {\r\nkfree(node);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < nr_grefs; i++)\r\nphys_addrs[i] = arbitrary_virt_to_machine(ptes[i]).maddr;\r\nerr = __xenbus_map_ring(dev, gnt_refs, nr_grefs, node->handles,\r\nphys_addrs,\r\nGNTMAP_host_map | GNTMAP_contains_pte,\r\n&leaked);\r\nif (err)\r\ngoto failed;\r\nnode->nr_handles = nr_grefs;\r\nnode->pv.area = area;\r\nspin_lock(&xenbus_valloc_lock);\r\nlist_add(&node->next, &xenbus_valloc_pages);\r\nspin_unlock(&xenbus_valloc_lock);\r\n*vaddr = area->addr;\r\nreturn 0;\r\nfailed:\r\nif (!leaked)\r\nfree_vm_area(area);\r\nelse\r\npr_alert("leaking VM area %p size %u page(s)", area, nr_grefs);\r\nkfree(node);\r\nreturn err;\r\n}\r\nstatic void xenbus_map_ring_setup_grant_hvm(unsigned long gfn,\r\nunsigned int goffset,\r\nunsigned int len,\r\nvoid *data)\r\n{\r\nstruct map_ring_valloc_hvm *info = data;\r\nunsigned long vaddr = (unsigned long)gfn_to_virt(gfn);\r\ninfo->phys_addrs[info->idx] = vaddr;\r\ninfo->addrs[info->idx] = vaddr;\r\ninfo->idx++;\r\n}\r\nstatic int xenbus_map_ring_valloc_hvm(struct xenbus_device *dev,\r\ngrant_ref_t *gnt_ref,\r\nunsigned int nr_grefs,\r\nvoid **vaddr)\r\n{\r\nstruct xenbus_map_node *node;\r\nint err;\r\nvoid *addr;\r\nbool leaked = false;\r\nstruct map_ring_valloc_hvm info = {\r\n.idx = 0,\r\n};\r\nunsigned int nr_pages = XENBUS_PAGES(nr_grefs);\r\nif (nr_grefs > XENBUS_MAX_RING_GRANTS)\r\nreturn -EINVAL;\r\n*vaddr = NULL;\r\nnode = kzalloc(sizeof(*node), GFP_KERNEL);\r\nif (!node)\r\nreturn -ENOMEM;\r\nerr = alloc_xenballooned_pages(nr_pages, node->hvm.pages);\r\nif (err)\r\ngoto out_err;\r\ngnttab_foreach_grant(node->hvm.pages, nr_grefs,\r\nxenbus_map_ring_setup_grant_hvm,\r\n&info);\r\nerr = __xenbus_map_ring(dev, gnt_ref, nr_grefs, node->handles,\r\ninfo.phys_addrs, GNTMAP_host_map, &leaked);\r\nnode->nr_handles = nr_grefs;\r\nif (err)\r\ngoto out_free_ballooned_pages;\r\naddr = vmap(node->hvm.pages, nr_pages, VM_MAP | VM_IOREMAP,\r\nPAGE_KERNEL);\r\nif (!addr) {\r\nerr = -ENOMEM;\r\ngoto out_xenbus_unmap_ring;\r\n}\r\nnode->hvm.addr = addr;\r\nspin_lock(&xenbus_valloc_lock);\r\nlist_add(&node->next, &xenbus_valloc_pages);\r\nspin_unlock(&xenbus_valloc_lock);\r\n*vaddr = addr;\r\nreturn 0;\r\nout_xenbus_unmap_ring:\r\nif (!leaked)\r\nxenbus_unmap_ring(dev, node->handles, nr_grefs, info.addrs);\r\nelse\r\npr_alert("leaking %p size %u page(s)",\r\naddr, nr_pages);\r\nout_free_ballooned_pages:\r\nif (!leaked)\r\nfree_xenballooned_pages(nr_pages, node->hvm.pages);\r\nout_err:\r\nkfree(node);\r\nreturn err;\r\n}\r\nint xenbus_map_ring(struct xenbus_device *dev, grant_ref_t *gnt_refs,\r\nunsigned int nr_grefs, grant_handle_t *handles,\r\nunsigned long *vaddrs, bool *leaked)\r\n{\r\nphys_addr_t phys_addrs[XENBUS_MAX_RING_GRANTS];\r\nint i;\r\nif (nr_grefs > XENBUS_MAX_RING_GRANTS)\r\nreturn -EINVAL;\r\nfor (i = 0; i < nr_grefs; i++)\r\nphys_addrs[i] = (unsigned long)vaddrs[i];\r\nreturn __xenbus_map_ring(dev, gnt_refs, nr_grefs, handles,\r\nphys_addrs, GNTMAP_host_map, leaked);\r\n}\r\nint xenbus_unmap_ring_vfree(struct xenbus_device *dev, void *vaddr)\r\n{\r\nreturn ring_ops->unmap(dev, vaddr);\r\n}\r\nstatic int xenbus_unmap_ring_vfree_pv(struct xenbus_device *dev, void *vaddr)\r\n{\r\nstruct xenbus_map_node *node;\r\nstruct gnttab_unmap_grant_ref unmap[XENBUS_MAX_RING_GRANTS];\r\nunsigned int level;\r\nint i;\r\nbool leaked = false;\r\nint err;\r\nspin_lock(&xenbus_valloc_lock);\r\nlist_for_each_entry(node, &xenbus_valloc_pages, next) {\r\nif (node->pv.area->addr == vaddr) {\r\nlist_del(&node->next);\r\ngoto found;\r\n}\r\n}\r\nnode = NULL;\r\nfound:\r\nspin_unlock(&xenbus_valloc_lock);\r\nif (!node) {\r\nxenbus_dev_error(dev, -ENOENT,\r\n"can't find mapped virtual address %p", vaddr);\r\nreturn GNTST_bad_virt_addr;\r\n}\r\nfor (i = 0; i < node->nr_handles; i++) {\r\nunsigned long addr;\r\nmemset(&unmap[i], 0, sizeof(unmap[i]));\r\naddr = (unsigned long)vaddr + (XEN_PAGE_SIZE * i);\r\nunmap[i].host_addr = arbitrary_virt_to_machine(\r\nlookup_address(addr, &level)).maddr;\r\nunmap[i].dev_bus_addr = 0;\r\nunmap[i].handle = node->handles[i];\r\n}\r\nif (HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, unmap, i))\r\nBUG();\r\nerr = GNTST_okay;\r\nleaked = false;\r\nfor (i = 0; i < node->nr_handles; i++) {\r\nif (unmap[i].status != GNTST_okay) {\r\nleaked = true;\r\nxenbus_dev_error(dev, unmap[i].status,\r\n"unmapping page at handle %d error %d",\r\nnode->handles[i], unmap[i].status);\r\nerr = unmap[i].status;\r\nbreak;\r\n}\r\n}\r\nif (!leaked)\r\nfree_vm_area(node->pv.area);\r\nelse\r\npr_alert("leaking VM area %p size %u page(s)",\r\nnode->pv.area, node->nr_handles);\r\nkfree(node);\r\nreturn err;\r\n}\r\nstatic void xenbus_unmap_ring_setup_grant_hvm(unsigned long gfn,\r\nunsigned int goffset,\r\nunsigned int len,\r\nvoid *data)\r\n{\r\nstruct unmap_ring_vfree_hvm *info = data;\r\ninfo->addrs[info->idx] = (unsigned long)gfn_to_virt(gfn);\r\ninfo->idx++;\r\n}\r\nstatic int xenbus_unmap_ring_vfree_hvm(struct xenbus_device *dev, void *vaddr)\r\n{\r\nint rv;\r\nstruct xenbus_map_node *node;\r\nvoid *addr;\r\nstruct unmap_ring_vfree_hvm info = {\r\n.idx = 0,\r\n};\r\nunsigned int nr_pages;\r\nspin_lock(&xenbus_valloc_lock);\r\nlist_for_each_entry(node, &xenbus_valloc_pages, next) {\r\naddr = node->hvm.addr;\r\nif (addr == vaddr) {\r\nlist_del(&node->next);\r\ngoto found;\r\n}\r\n}\r\nnode = addr = NULL;\r\nfound:\r\nspin_unlock(&xenbus_valloc_lock);\r\nif (!node) {\r\nxenbus_dev_error(dev, -ENOENT,\r\n"can't find mapped virtual address %p", vaddr);\r\nreturn GNTST_bad_virt_addr;\r\n}\r\nnr_pages = XENBUS_PAGES(node->nr_handles);\r\ngnttab_foreach_grant(node->hvm.pages, node->nr_handles,\r\nxenbus_unmap_ring_setup_grant_hvm,\r\n&info);\r\nrv = xenbus_unmap_ring(dev, node->handles, node->nr_handles,\r\ninfo.addrs);\r\nif (!rv) {\r\nvunmap(vaddr);\r\nfree_xenballooned_pages(nr_pages, node->hvm.pages);\r\n}\r\nelse\r\nWARN(1, "Leaking %p, size %u page(s)\n", vaddr, nr_pages);\r\nkfree(node);\r\nreturn rv;\r\n}\r\nint xenbus_unmap_ring(struct xenbus_device *dev,\r\ngrant_handle_t *handles, unsigned int nr_handles,\r\nunsigned long *vaddrs)\r\n{\r\nstruct gnttab_unmap_grant_ref unmap[XENBUS_MAX_RING_GRANTS];\r\nint i;\r\nint err;\r\nif (nr_handles > XENBUS_MAX_RING_GRANTS)\r\nreturn -EINVAL;\r\nfor (i = 0; i < nr_handles; i++)\r\ngnttab_set_unmap_op(&unmap[i], vaddrs[i],\r\nGNTMAP_host_map, handles[i]);\r\nif (HYPERVISOR_grant_table_op(GNTTABOP_unmap_grant_ref, unmap, i))\r\nBUG();\r\nerr = GNTST_okay;\r\nfor (i = 0; i < nr_handles; i++) {\r\nif (unmap[i].status != GNTST_okay) {\r\nxenbus_dev_error(dev, unmap[i].status,\r\n"unmapping page at handle %d error %d",\r\nhandles[i], unmap[i].status);\r\nerr = unmap[i].status;\r\nbreak;\r\n}\r\n}\r\nreturn err;\r\n}\r\nenum xenbus_state xenbus_read_driver_state(const char *path)\r\n{\r\nenum xenbus_state result;\r\nint err = xenbus_gather(XBT_NIL, path, "state", "%d", &result, NULL);\r\nif (err)\r\nresult = XenbusStateUnknown;\r\nreturn result;\r\n}\r\nvoid __init xenbus_ring_ops_init(void)\r\n{\r\nif (!xen_feature(XENFEAT_auto_translated_physmap))\r\nring_ops = &ring_ops_pv;\r\nelse\r\nring_ops = &ring_ops_hvm;\r\n}
