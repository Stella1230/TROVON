static bool spr_allowed(struct kvm_vcpu *vcpu, enum priv_level level)\r\n{\r\nif (vcpu->arch.papr_enabled && (level > PRIV_SUPER))\r\nreturn false;\r\nif ((kvmppc_get_msr(vcpu) & MSR_PR) && level > PRIV_PROBLEM)\r\nreturn false;\r\nreturn true;\r\n}\r\nint kvmppc_core_emulate_op_pr(struct kvm_run *run, struct kvm_vcpu *vcpu,\r\nunsigned int inst, int *advance)\r\n{\r\nint emulated = EMULATE_DONE;\r\nint rt = get_rt(inst);\r\nint rs = get_rs(inst);\r\nint ra = get_ra(inst);\r\nint rb = get_rb(inst);\r\nu32 inst_sc = 0x44000002;\r\nswitch (get_op(inst)) {\r\ncase 0:\r\nemulated = EMULATE_FAIL;\r\nif ((kvmppc_get_msr(vcpu) & MSR_LE) &&\r\n(inst == swab32(inst_sc))) {\r\nkvmppc_set_gpr(vcpu, 3, EV_UNIMPLEMENTED);\r\nkvmppc_set_pc(vcpu, kvmppc_get_pc(vcpu) + 4);\r\nemulated = EMULATE_DONE;\r\n}\r\nbreak;\r\ncase 19:\r\nswitch (get_xop(inst)) {\r\ncase OP_19_XOP_RFID:\r\ncase OP_19_XOP_RFI:\r\nkvmppc_set_pc(vcpu, kvmppc_get_srr0(vcpu));\r\nkvmppc_set_msr(vcpu, kvmppc_get_srr1(vcpu));\r\n*advance = 0;\r\nbreak;\r\ndefault:\r\nemulated = EMULATE_FAIL;\r\nbreak;\r\n}\r\nbreak;\r\ncase 31:\r\nswitch (get_xop(inst)) {\r\ncase OP_31_XOP_MFMSR:\r\nkvmppc_set_gpr(vcpu, rt, kvmppc_get_msr(vcpu));\r\nbreak;\r\ncase OP_31_XOP_MTMSRD:\r\n{\r\nulong rs_val = kvmppc_get_gpr(vcpu, rs);\r\nif (inst & 0x10000) {\r\nulong new_msr = kvmppc_get_msr(vcpu);\r\nnew_msr &= ~(MSR_RI | MSR_EE);\r\nnew_msr |= rs_val & (MSR_RI | MSR_EE);\r\nkvmppc_set_msr_fast(vcpu, new_msr);\r\n} else\r\nkvmppc_set_msr(vcpu, rs_val);\r\nbreak;\r\n}\r\ncase OP_31_XOP_MTMSR:\r\nkvmppc_set_msr(vcpu, kvmppc_get_gpr(vcpu, rs));\r\nbreak;\r\ncase OP_31_XOP_MFSR:\r\n{\r\nint srnum;\r\nsrnum = kvmppc_get_field(inst, 12 + 32, 15 + 32);\r\nif (vcpu->arch.mmu.mfsrin) {\r\nu32 sr;\r\nsr = vcpu->arch.mmu.mfsrin(vcpu, srnum);\r\nkvmppc_set_gpr(vcpu, rt, sr);\r\n}\r\nbreak;\r\n}\r\ncase OP_31_XOP_MFSRIN:\r\n{\r\nint srnum;\r\nsrnum = (kvmppc_get_gpr(vcpu, rb) >> 28) & 0xf;\r\nif (vcpu->arch.mmu.mfsrin) {\r\nu32 sr;\r\nsr = vcpu->arch.mmu.mfsrin(vcpu, srnum);\r\nkvmppc_set_gpr(vcpu, rt, sr);\r\n}\r\nbreak;\r\n}\r\ncase OP_31_XOP_MTSR:\r\nvcpu->arch.mmu.mtsrin(vcpu,\r\n(inst >> 16) & 0xf,\r\nkvmppc_get_gpr(vcpu, rs));\r\nbreak;\r\ncase OP_31_XOP_MTSRIN:\r\nvcpu->arch.mmu.mtsrin(vcpu,\r\n(kvmppc_get_gpr(vcpu, rb) >> 28) & 0xf,\r\nkvmppc_get_gpr(vcpu, rs));\r\nbreak;\r\ncase OP_31_XOP_TLBIE:\r\ncase OP_31_XOP_TLBIEL:\r\n{\r\nbool large = (inst & 0x00200000) ? true : false;\r\nulong addr = kvmppc_get_gpr(vcpu, rb);\r\nvcpu->arch.mmu.tlbie(vcpu, addr, large);\r\nbreak;\r\n}\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase OP_31_XOP_FAKE_SC1:\r\n{\r\nulong cmd = kvmppc_get_gpr(vcpu, 3);\r\nint i;\r\nif ((kvmppc_get_msr(vcpu) & MSR_PR) ||\r\n!vcpu->arch.papr_enabled) {\r\nemulated = EMULATE_FAIL;\r\nbreak;\r\n}\r\nif (kvmppc_h_pr(vcpu, cmd) == EMULATE_DONE)\r\nbreak;\r\nrun->papr_hcall.nr = cmd;\r\nfor (i = 0; i < 9; ++i) {\r\nulong gpr = kvmppc_get_gpr(vcpu, 4 + i);\r\nrun->papr_hcall.args[i] = gpr;\r\n}\r\nrun->exit_reason = KVM_EXIT_PAPR_HCALL;\r\nvcpu->arch.hcall_needed = 1;\r\nemulated = EMULATE_EXIT_USER;\r\nbreak;\r\n}\r\n#endif\r\ncase OP_31_XOP_EIOIO:\r\nbreak;\r\ncase OP_31_XOP_SLBMTE:\r\nif (!vcpu->arch.mmu.slbmte)\r\nreturn EMULATE_FAIL;\r\nvcpu->arch.mmu.slbmte(vcpu,\r\nkvmppc_get_gpr(vcpu, rs),\r\nkvmppc_get_gpr(vcpu, rb));\r\nbreak;\r\ncase OP_31_XOP_SLBIE:\r\nif (!vcpu->arch.mmu.slbie)\r\nreturn EMULATE_FAIL;\r\nvcpu->arch.mmu.slbie(vcpu,\r\nkvmppc_get_gpr(vcpu, rb));\r\nbreak;\r\ncase OP_31_XOP_SLBIA:\r\nif (!vcpu->arch.mmu.slbia)\r\nreturn EMULATE_FAIL;\r\nvcpu->arch.mmu.slbia(vcpu);\r\nbreak;\r\ncase OP_31_XOP_SLBMFEE:\r\nif (!vcpu->arch.mmu.slbmfee) {\r\nemulated = EMULATE_FAIL;\r\n} else {\r\nulong t, rb_val;\r\nrb_val = kvmppc_get_gpr(vcpu, rb);\r\nt = vcpu->arch.mmu.slbmfee(vcpu, rb_val);\r\nkvmppc_set_gpr(vcpu, rt, t);\r\n}\r\nbreak;\r\ncase OP_31_XOP_SLBMFEV:\r\nif (!vcpu->arch.mmu.slbmfev) {\r\nemulated = EMULATE_FAIL;\r\n} else {\r\nulong t, rb_val;\r\nrb_val = kvmppc_get_gpr(vcpu, rb);\r\nt = vcpu->arch.mmu.slbmfev(vcpu, rb_val);\r\nkvmppc_set_gpr(vcpu, rt, t);\r\n}\r\nbreak;\r\ncase OP_31_XOP_DCBA:\r\nbreak;\r\ncase OP_31_XOP_DCBZ:\r\n{\r\nulong rb_val = kvmppc_get_gpr(vcpu, rb);\r\nulong ra_val = 0;\r\nulong addr, vaddr;\r\nu32 zeros[8] = { 0, 0, 0, 0, 0, 0, 0, 0 };\r\nu32 dsisr;\r\nint r;\r\nif (ra)\r\nra_val = kvmppc_get_gpr(vcpu, ra);\r\naddr = (ra_val + rb_val) & ~31ULL;\r\nif (!(kvmppc_get_msr(vcpu) & MSR_SF))\r\naddr &= 0xffffffff;\r\nvaddr = addr;\r\nr = kvmppc_st(vcpu, &addr, 32, zeros, true);\r\nif ((r == -ENOENT) || (r == -EPERM)) {\r\n*advance = 0;\r\nkvmppc_set_dar(vcpu, vaddr);\r\nvcpu->arch.fault_dar = vaddr;\r\ndsisr = DSISR_ISSTORE;\r\nif (r == -ENOENT)\r\ndsisr |= DSISR_NOHPTE;\r\nelse if (r == -EPERM)\r\ndsisr |= DSISR_PROTFAULT;\r\nkvmppc_set_dsisr(vcpu, dsisr);\r\nvcpu->arch.fault_dsisr = dsisr;\r\nkvmppc_book3s_queue_irqprio(vcpu,\r\nBOOK3S_INTERRUPT_DATA_STORAGE);\r\n}\r\nbreak;\r\n}\r\ndefault:\r\nemulated = EMULATE_FAIL;\r\n}\r\nbreak;\r\ndefault:\r\nemulated = EMULATE_FAIL;\r\n}\r\nif (emulated == EMULATE_FAIL)\r\nemulated = kvmppc_emulate_paired_single(run, vcpu);\r\nreturn emulated;\r\n}\r\nvoid kvmppc_set_bat(struct kvm_vcpu *vcpu, struct kvmppc_bat *bat, bool upper,\r\nu32 val)\r\n{\r\nif (upper) {\r\nu32 bl = (val >> 2) & 0x7ff;\r\nbat->bepi_mask = (~bl << 17);\r\nbat->bepi = val & 0xfffe0000;\r\nbat->vs = (val & 2) ? 1 : 0;\r\nbat->vp = (val & 1) ? 1 : 0;\r\nbat->raw = (bat->raw & 0xffffffff00000000ULL) | val;\r\n} else {\r\nbat->brpn = val & 0xfffe0000;\r\nbat->wimg = (val >> 3) & 0xf;\r\nbat->pp = val & 3;\r\nbat->raw = (bat->raw & 0x00000000ffffffffULL) | ((u64)val << 32);\r\n}\r\n}\r\nstatic struct kvmppc_bat *kvmppc_find_bat(struct kvm_vcpu *vcpu, int sprn)\r\n{\r\nstruct kvmppc_vcpu_book3s *vcpu_book3s = to_book3s(vcpu);\r\nstruct kvmppc_bat *bat;\r\nswitch (sprn) {\r\ncase SPRN_IBAT0U ... SPRN_IBAT3L:\r\nbat = &vcpu_book3s->ibat[(sprn - SPRN_IBAT0U) / 2];\r\nbreak;\r\ncase SPRN_IBAT4U ... SPRN_IBAT7L:\r\nbat = &vcpu_book3s->ibat[4 + ((sprn - SPRN_IBAT4U) / 2)];\r\nbreak;\r\ncase SPRN_DBAT0U ... SPRN_DBAT3L:\r\nbat = &vcpu_book3s->dbat[(sprn - SPRN_DBAT0U) / 2];\r\nbreak;\r\ncase SPRN_DBAT4U ... SPRN_DBAT7L:\r\nbat = &vcpu_book3s->dbat[4 + ((sprn - SPRN_DBAT4U) / 2)];\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn bat;\r\n}\r\nint kvmppc_core_emulate_mtspr_pr(struct kvm_vcpu *vcpu, int sprn, ulong spr_val)\r\n{\r\nint emulated = EMULATE_DONE;\r\nswitch (sprn) {\r\ncase SPRN_SDR1:\r\nif (!spr_allowed(vcpu, PRIV_HYPER))\r\ngoto unprivileged;\r\nto_book3s(vcpu)->sdr1 = spr_val;\r\nbreak;\r\ncase SPRN_DSISR:\r\nkvmppc_set_dsisr(vcpu, spr_val);\r\nbreak;\r\ncase SPRN_DAR:\r\nkvmppc_set_dar(vcpu, spr_val);\r\nbreak;\r\ncase SPRN_HIOR:\r\nto_book3s(vcpu)->hior = spr_val;\r\nbreak;\r\ncase SPRN_IBAT0U ... SPRN_IBAT3L:\r\ncase SPRN_IBAT4U ... SPRN_IBAT7L:\r\ncase SPRN_DBAT0U ... SPRN_DBAT3L:\r\ncase SPRN_DBAT4U ... SPRN_DBAT7L:\r\n{\r\nstruct kvmppc_bat *bat = kvmppc_find_bat(vcpu, sprn);\r\nkvmppc_set_bat(vcpu, bat, !(sprn % 2), (u32)spr_val);\r\nkvmppc_mmu_pte_flush(vcpu, 0, 0);\r\nkvmppc_mmu_flush_segments(vcpu);\r\nbreak;\r\n}\r\ncase SPRN_HID0:\r\nto_book3s(vcpu)->hid[0] = spr_val;\r\nbreak;\r\ncase SPRN_HID1:\r\nto_book3s(vcpu)->hid[1] = spr_val;\r\nbreak;\r\ncase SPRN_HID2:\r\nto_book3s(vcpu)->hid[2] = spr_val;\r\nbreak;\r\ncase SPRN_HID2_GEKKO:\r\nto_book3s(vcpu)->hid[2] = spr_val;\r\nswitch (vcpu->arch.pvr) {\r\ncase 0x00080200:\r\ncase 0x00088202:\r\ncase 0x70000100:\r\ncase 0x00080100:\r\ncase 0x00083203:\r\ncase 0x00083213:\r\ncase 0x00083204:\r\ncase 0x00083214:\r\ncase 0x00087200:\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_NATIVE_PS) {\r\n} else if (spr_val & (1 << 29)) {\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_PAIRED_SINGLE;\r\nkvmppc_giveup_ext(vcpu, MSR_FP);\r\n} else {\r\nvcpu->arch.hflags &= ~BOOK3S_HFLAG_PAIRED_SINGLE;\r\n}\r\nbreak;\r\n}\r\nbreak;\r\ncase SPRN_HID4:\r\ncase SPRN_HID4_GEKKO:\r\nto_book3s(vcpu)->hid[4] = spr_val;\r\nbreak;\r\ncase SPRN_HID5:\r\nto_book3s(vcpu)->hid[5] = spr_val;\r\nif (vcpu->arch.mmu.is_dcbz32(vcpu) &&\r\n(mfmsr() & MSR_HV))\r\nvcpu->arch.hflags |= BOOK3S_HFLAG_DCBZ32;\r\nbreak;\r\ncase SPRN_GQR0:\r\ncase SPRN_GQR1:\r\ncase SPRN_GQR2:\r\ncase SPRN_GQR3:\r\ncase SPRN_GQR4:\r\ncase SPRN_GQR5:\r\ncase SPRN_GQR6:\r\ncase SPRN_GQR7:\r\nto_book3s(vcpu)->gqr[sprn - SPRN_GQR0] = spr_val;\r\nbreak;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase SPRN_FSCR:\r\nkvmppc_set_fscr(vcpu, spr_val);\r\nbreak;\r\ncase SPRN_BESCR:\r\nvcpu->arch.bescr = spr_val;\r\nbreak;\r\ncase SPRN_EBBHR:\r\nvcpu->arch.ebbhr = spr_val;\r\nbreak;\r\ncase SPRN_EBBRR:\r\nvcpu->arch.ebbrr = spr_val;\r\nbreak;\r\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\r\ncase SPRN_TFHAR:\r\nvcpu->arch.tfhar = spr_val;\r\nbreak;\r\ncase SPRN_TEXASR:\r\nvcpu->arch.texasr = spr_val;\r\nbreak;\r\ncase SPRN_TFIAR:\r\nvcpu->arch.tfiar = spr_val;\r\nbreak;\r\n#endif\r\n#endif\r\ncase SPRN_ICTC:\r\ncase SPRN_THRM1:\r\ncase SPRN_THRM2:\r\ncase SPRN_THRM3:\r\ncase SPRN_CTRLF:\r\ncase SPRN_CTRLT:\r\ncase SPRN_L2CR:\r\ncase SPRN_DSCR:\r\ncase SPRN_MMCR0_GEKKO:\r\ncase SPRN_MMCR1_GEKKO:\r\ncase SPRN_PMC1_GEKKO:\r\ncase SPRN_PMC2_GEKKO:\r\ncase SPRN_PMC3_GEKKO:\r\ncase SPRN_PMC4_GEKKO:\r\ncase SPRN_WPAR_GEKKO:\r\ncase SPRN_MSSSR0:\r\ncase SPRN_DABR:\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase SPRN_MMCRS:\r\ncase SPRN_MMCRA:\r\ncase SPRN_MMCR0:\r\ncase SPRN_MMCR1:\r\ncase SPRN_MMCR2:\r\n#endif\r\nbreak;\r\nunprivileged:\r\ndefault:\r\nprintk(KERN_INFO "KVM: invalid SPR write: %d\n", sprn);\r\n#ifndef DEBUG_SPR\r\nemulated = EMULATE_FAIL;\r\n#endif\r\nbreak;\r\n}\r\nreturn emulated;\r\n}\r\nint kvmppc_core_emulate_mfspr_pr(struct kvm_vcpu *vcpu, int sprn, ulong *spr_val)\r\n{\r\nint emulated = EMULATE_DONE;\r\nswitch (sprn) {\r\ncase SPRN_IBAT0U ... SPRN_IBAT3L:\r\ncase SPRN_IBAT4U ... SPRN_IBAT7L:\r\ncase SPRN_DBAT0U ... SPRN_DBAT3L:\r\ncase SPRN_DBAT4U ... SPRN_DBAT7L:\r\n{\r\nstruct kvmppc_bat *bat = kvmppc_find_bat(vcpu, sprn);\r\nif (sprn % 2)\r\n*spr_val = bat->raw >> 32;\r\nelse\r\n*spr_val = bat->raw;\r\nbreak;\r\n}\r\ncase SPRN_SDR1:\r\nif (!spr_allowed(vcpu, PRIV_HYPER))\r\ngoto unprivileged;\r\n*spr_val = to_book3s(vcpu)->sdr1;\r\nbreak;\r\ncase SPRN_DSISR:\r\n*spr_val = kvmppc_get_dsisr(vcpu);\r\nbreak;\r\ncase SPRN_DAR:\r\n*spr_val = kvmppc_get_dar(vcpu);\r\nbreak;\r\ncase SPRN_HIOR:\r\n*spr_val = to_book3s(vcpu)->hior;\r\nbreak;\r\ncase SPRN_HID0:\r\n*spr_val = to_book3s(vcpu)->hid[0];\r\nbreak;\r\ncase SPRN_HID1:\r\n*spr_val = to_book3s(vcpu)->hid[1];\r\nbreak;\r\ncase SPRN_HID2:\r\ncase SPRN_HID2_GEKKO:\r\n*spr_val = to_book3s(vcpu)->hid[2];\r\nbreak;\r\ncase SPRN_HID4:\r\ncase SPRN_HID4_GEKKO:\r\n*spr_val = to_book3s(vcpu)->hid[4];\r\nbreak;\r\ncase SPRN_HID5:\r\n*spr_val = to_book3s(vcpu)->hid[5];\r\nbreak;\r\ncase SPRN_CFAR:\r\ncase SPRN_DSCR:\r\n*spr_val = 0;\r\nbreak;\r\ncase SPRN_PURR:\r\n*spr_val = vcpu->arch.purr;\r\nbreak;\r\ncase SPRN_SPURR:\r\n*spr_val = vcpu->arch.spurr;\r\nbreak;\r\ncase SPRN_VTB:\r\n*spr_val = vcpu->arch.vtb;\r\nbreak;\r\ncase SPRN_IC:\r\n*spr_val = vcpu->arch.ic;\r\nbreak;\r\ncase SPRN_GQR0:\r\ncase SPRN_GQR1:\r\ncase SPRN_GQR2:\r\ncase SPRN_GQR3:\r\ncase SPRN_GQR4:\r\ncase SPRN_GQR5:\r\ncase SPRN_GQR6:\r\ncase SPRN_GQR7:\r\n*spr_val = to_book3s(vcpu)->gqr[sprn - SPRN_GQR0];\r\nbreak;\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase SPRN_FSCR:\r\n*spr_val = vcpu->arch.fscr;\r\nbreak;\r\ncase SPRN_BESCR:\r\n*spr_val = vcpu->arch.bescr;\r\nbreak;\r\ncase SPRN_EBBHR:\r\n*spr_val = vcpu->arch.ebbhr;\r\nbreak;\r\ncase SPRN_EBBRR:\r\n*spr_val = vcpu->arch.ebbrr;\r\nbreak;\r\n#ifdef CONFIG_PPC_TRANSACTIONAL_MEM\r\ncase SPRN_TFHAR:\r\n*spr_val = vcpu->arch.tfhar;\r\nbreak;\r\ncase SPRN_TEXASR:\r\n*spr_val = vcpu->arch.texasr;\r\nbreak;\r\ncase SPRN_TFIAR:\r\n*spr_val = vcpu->arch.tfiar;\r\nbreak;\r\n#endif\r\n#endif\r\ncase SPRN_THRM1:\r\ncase SPRN_THRM2:\r\ncase SPRN_THRM3:\r\ncase SPRN_CTRLF:\r\ncase SPRN_CTRLT:\r\ncase SPRN_L2CR:\r\ncase SPRN_MMCR0_GEKKO:\r\ncase SPRN_MMCR1_GEKKO:\r\ncase SPRN_PMC1_GEKKO:\r\ncase SPRN_PMC2_GEKKO:\r\ncase SPRN_PMC3_GEKKO:\r\ncase SPRN_PMC4_GEKKO:\r\ncase SPRN_WPAR_GEKKO:\r\ncase SPRN_MSSSR0:\r\ncase SPRN_DABR:\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\ncase SPRN_MMCRS:\r\ncase SPRN_MMCRA:\r\ncase SPRN_MMCR0:\r\ncase SPRN_MMCR1:\r\ncase SPRN_MMCR2:\r\ncase SPRN_TIR:\r\n#endif\r\n*spr_val = 0;\r\nbreak;\r\ndefault:\r\nunprivileged:\r\nprintk(KERN_INFO "KVM: invalid SPR read: %d\n", sprn);\r\n#ifndef DEBUG_SPR\r\nemulated = EMULATE_FAIL;\r\n#endif\r\nbreak;\r\n}\r\nreturn emulated;\r\n}\r\nu32 kvmppc_alignment_dsisr(struct kvm_vcpu *vcpu, unsigned int inst)\r\n{\r\nreturn make_dsisr(inst);\r\n}\r\nulong kvmppc_alignment_dar(struct kvm_vcpu *vcpu, unsigned int inst)\r\n{\r\n#ifdef CONFIG_PPC_BOOK3S_64\r\nreturn vcpu->arch.fault_dar;\r\n#else\r\nulong dar = 0;\r\nulong ra = get_ra(inst);\r\nulong rb = get_rb(inst);\r\nswitch (get_op(inst)) {\r\ncase OP_LFS:\r\ncase OP_LFD:\r\ncase OP_STFD:\r\ncase OP_STFS:\r\nif (ra)\r\ndar = kvmppc_get_gpr(vcpu, ra);\r\ndar += (s32)((s16)inst);\r\nbreak;\r\ncase 31:\r\nif (ra)\r\ndar = kvmppc_get_gpr(vcpu, ra);\r\ndar += kvmppc_get_gpr(vcpu, rb);\r\nbreak;\r\ndefault:\r\nprintk(KERN_INFO "KVM: Unaligned instruction 0x%x\n", inst);\r\nbreak;\r\n}\r\nreturn dar;\r\n#endif\r\n}
