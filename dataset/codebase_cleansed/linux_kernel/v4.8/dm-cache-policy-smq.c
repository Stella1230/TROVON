static unsigned safe_div(unsigned n, unsigned d)\r\n{\r\nreturn d ? n / d : 0u;\r\n}\r\nstatic unsigned safe_mod(unsigned n, unsigned d)\r\n{\r\nreturn d ? n % d : 0u;\r\n}\r\nstatic int space_init(struct entry_space *es, unsigned nr_entries)\r\n{\r\nif (!nr_entries) {\r\nes->begin = es->end = NULL;\r\nreturn 0;\r\n}\r\nes->begin = vzalloc(sizeof(struct entry) * nr_entries);\r\nif (!es->begin)\r\nreturn -ENOMEM;\r\nes->end = es->begin + nr_entries;\r\nreturn 0;\r\n}\r\nstatic void space_exit(struct entry_space *es)\r\n{\r\nvfree(es->begin);\r\n}\r\nstatic struct entry *__get_entry(struct entry_space *es, unsigned block)\r\n{\r\nstruct entry *e;\r\ne = es->begin + block;\r\nBUG_ON(e >= es->end);\r\nreturn e;\r\n}\r\nstatic unsigned to_index(struct entry_space *es, struct entry *e)\r\n{\r\nBUG_ON(e < es->begin || e >= es->end);\r\nreturn e - es->begin;\r\n}\r\nstatic struct entry *to_entry(struct entry_space *es, unsigned block)\r\n{\r\nif (block == INDEXER_NULL)\r\nreturn NULL;\r\nreturn __get_entry(es, block);\r\n}\r\nstatic void l_init(struct ilist *l)\r\n{\r\nl->nr_elts = 0;\r\nl->head = l->tail = INDEXER_NULL;\r\n}\r\nstatic struct entry *l_head(struct entry_space *es, struct ilist *l)\r\n{\r\nreturn to_entry(es, l->head);\r\n}\r\nstatic struct entry *l_tail(struct entry_space *es, struct ilist *l)\r\n{\r\nreturn to_entry(es, l->tail);\r\n}\r\nstatic struct entry *l_next(struct entry_space *es, struct entry *e)\r\n{\r\nreturn to_entry(es, e->next);\r\n}\r\nstatic struct entry *l_prev(struct entry_space *es, struct entry *e)\r\n{\r\nreturn to_entry(es, e->prev);\r\n}\r\nstatic bool l_empty(struct ilist *l)\r\n{\r\nreturn l->head == INDEXER_NULL;\r\n}\r\nstatic void l_add_head(struct entry_space *es, struct ilist *l, struct entry *e)\r\n{\r\nstruct entry *head = l_head(es, l);\r\ne->next = l->head;\r\ne->prev = INDEXER_NULL;\r\nif (head)\r\nhead->prev = l->head = to_index(es, e);\r\nelse\r\nl->head = l->tail = to_index(es, e);\r\nif (!e->sentinel)\r\nl->nr_elts++;\r\n}\r\nstatic void l_add_tail(struct entry_space *es, struct ilist *l, struct entry *e)\r\n{\r\nstruct entry *tail = l_tail(es, l);\r\ne->next = INDEXER_NULL;\r\ne->prev = l->tail;\r\nif (tail)\r\ntail->next = l->tail = to_index(es, e);\r\nelse\r\nl->head = l->tail = to_index(es, e);\r\nif (!e->sentinel)\r\nl->nr_elts++;\r\n}\r\nstatic void l_add_before(struct entry_space *es, struct ilist *l,\r\nstruct entry *old, struct entry *e)\r\n{\r\nstruct entry *prev = l_prev(es, old);\r\nif (!prev)\r\nl_add_head(es, l, e);\r\nelse {\r\ne->prev = old->prev;\r\ne->next = to_index(es, old);\r\nprev->next = old->prev = to_index(es, e);\r\nif (!e->sentinel)\r\nl->nr_elts++;\r\n}\r\n}\r\nstatic void l_del(struct entry_space *es, struct ilist *l, struct entry *e)\r\n{\r\nstruct entry *prev = l_prev(es, e);\r\nstruct entry *next = l_next(es, e);\r\nif (prev)\r\nprev->next = e->next;\r\nelse\r\nl->head = e->next;\r\nif (next)\r\nnext->prev = e->prev;\r\nelse\r\nl->tail = e->prev;\r\nif (!e->sentinel)\r\nl->nr_elts--;\r\n}\r\nstatic struct entry *l_pop_tail(struct entry_space *es, struct ilist *l)\r\n{\r\nstruct entry *e;\r\nfor (e = l_tail(es, l); e; e = l_prev(es, e))\r\nif (!e->sentinel) {\r\nl_del(es, l, e);\r\nreturn e;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void q_init(struct queue *q, struct entry_space *es, unsigned nr_levels)\r\n{\r\nunsigned i;\r\nq->es = es;\r\nq->nr_elts = 0;\r\nq->nr_levels = nr_levels;\r\nfor (i = 0; i < q->nr_levels; i++) {\r\nl_init(q->qs + i);\r\nq->target_count[i] = 0u;\r\n}\r\nq->last_target_nr_elts = 0u;\r\nq->nr_top_levels = 0u;\r\nq->nr_in_top_levels = 0u;\r\n}\r\nstatic unsigned q_size(struct queue *q)\r\n{\r\nreturn q->nr_elts;\r\n}\r\nstatic void q_push(struct queue *q, struct entry *e)\r\n{\r\nif (!e->sentinel)\r\nq->nr_elts++;\r\nl_add_tail(q->es, q->qs + e->level, e);\r\n}\r\nstatic void q_push_before(struct queue *q, struct entry *old, struct entry *e)\r\n{\r\nif (!e->sentinel)\r\nq->nr_elts++;\r\nl_add_before(q->es, q->qs + e->level, old, e);\r\n}\r\nstatic void q_del(struct queue *q, struct entry *e)\r\n{\r\nl_del(q->es, q->qs + e->level, e);\r\nif (!e->sentinel)\r\nq->nr_elts--;\r\n}\r\nstatic struct entry *q_peek(struct queue *q, unsigned max_level, bool can_cross_sentinel)\r\n{\r\nunsigned level;\r\nstruct entry *e;\r\nmax_level = min(max_level, q->nr_levels);\r\nfor (level = 0; level < max_level; level++)\r\nfor (e = l_head(q->es, q->qs + level); e; e = l_next(q->es, e)) {\r\nif (e->sentinel) {\r\nif (can_cross_sentinel)\r\ncontinue;\r\nelse\r\nbreak;\r\n}\r\nreturn e;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct entry *q_pop(struct queue *q)\r\n{\r\nstruct entry *e = q_peek(q, q->nr_levels, true);\r\nif (e)\r\nq_del(q, e);\r\nreturn e;\r\n}\r\nstatic struct entry *q_pop_old(struct queue *q, unsigned max_level)\r\n{\r\nstruct entry *e = q_peek(q, max_level, false);\r\nif (e)\r\nq_del(q, e);\r\nreturn e;\r\n}\r\nstatic struct entry *__redist_pop_from(struct queue *q, unsigned level)\r\n{\r\nstruct entry *e;\r\nfor (; level < q->nr_levels; level++)\r\nfor (e = l_head(q->es, q->qs + level); e; e = l_next(q->es, e))\r\nif (!e->sentinel) {\r\nl_del(q->es, q->qs + e->level, e);\r\nreturn e;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void q_set_targets_subrange_(struct queue *q, unsigned nr_elts, unsigned lbegin, unsigned lend)\r\n{\r\nunsigned level, nr_levels, entries_per_level, remainder;\r\nBUG_ON(lbegin > lend);\r\nBUG_ON(lend > q->nr_levels);\r\nnr_levels = lend - lbegin;\r\nentries_per_level = safe_div(nr_elts, nr_levels);\r\nremainder = safe_mod(nr_elts, nr_levels);\r\nfor (level = lbegin; level < lend; level++)\r\nq->target_count[level] =\r\n(level < (lbegin + remainder)) ? entries_per_level + 1u : entries_per_level;\r\n}\r\nstatic void q_set_targets(struct queue *q)\r\n{\r\nif (q->last_target_nr_elts == q->nr_elts)\r\nreturn;\r\nq->last_target_nr_elts = q->nr_elts;\r\nif (q->nr_top_levels > q->nr_levels)\r\nq_set_targets_subrange_(q, q->nr_elts, 0, q->nr_levels);\r\nelse {\r\nq_set_targets_subrange_(q, q->nr_in_top_levels,\r\nq->nr_levels - q->nr_top_levels, q->nr_levels);\r\nif (q->nr_in_top_levels < q->nr_elts)\r\nq_set_targets_subrange_(q, q->nr_elts - q->nr_in_top_levels,\r\n0, q->nr_levels - q->nr_top_levels);\r\nelse\r\nq_set_targets_subrange_(q, 0, 0, q->nr_levels - q->nr_top_levels);\r\n}\r\n}\r\nstatic void q_redistribute(struct queue *q)\r\n{\r\nunsigned target, level;\r\nstruct ilist *l, *l_above;\r\nstruct entry *e;\r\nq_set_targets(q);\r\nfor (level = 0u; level < q->nr_levels - 1u; level++) {\r\nl = q->qs + level;\r\ntarget = q->target_count[level];\r\nwhile (l->nr_elts < target) {\r\ne = __redist_pop_from(q, level + 1u);\r\nif (!e) {\r\nbreak;\r\n}\r\ne->level = level;\r\nl_add_tail(q->es, l, e);\r\n}\r\nl_above = q->qs + level + 1u;\r\nwhile (l->nr_elts > target) {\r\ne = l_pop_tail(q->es, l);\r\nif (!e)\r\nbreak;\r\ne->level = level + 1u;\r\nl_add_head(q->es, l_above, e);\r\n}\r\n}\r\n}\r\nstatic void q_requeue_before(struct queue *q, struct entry *dest, struct entry *e, unsigned extra_levels)\r\n{\r\nstruct entry *de;\r\nunsigned new_level;\r\nq_del(q, e);\r\nif (extra_levels && (e->level < q->nr_levels - 1u)) {\r\nnew_level = min(q->nr_levels - 1u, e->level + extra_levels);\r\nfor (de = l_head(q->es, q->qs + new_level); de; de = l_next(q->es, de)) {\r\nif (de->sentinel)\r\ncontinue;\r\nq_del(q, de);\r\nde->level = e->level;\r\nif (dest)\r\nq_push_before(q, dest, de);\r\nelse\r\nq_push(q, de);\r\nbreak;\r\n}\r\ne->level = new_level;\r\n}\r\nq_push(q, e);\r\n}\r\nstatic void q_requeue(struct queue *q, struct entry *e, unsigned extra_levels)\r\n{\r\nq_requeue_before(q, NULL, e, extra_levels);\r\n}\r\nstatic void stats_init(struct stats *s, unsigned nr_levels)\r\n{\r\ns->hit_threshold = (nr_levels * 3u) / 4u;\r\ns->hits = 0u;\r\ns->misses = 0u;\r\n}\r\nstatic void stats_reset(struct stats *s)\r\n{\r\ns->hits = s->misses = 0u;\r\n}\r\nstatic void stats_level_accessed(struct stats *s, unsigned level)\r\n{\r\nif (level >= s->hit_threshold)\r\ns->hits++;\r\nelse\r\ns->misses++;\r\n}\r\nstatic void stats_miss(struct stats *s)\r\n{\r\ns->misses++;\r\n}\r\nstatic enum performance stats_assess(struct stats *s)\r\n{\r\nunsigned confidence = safe_div(s->hits << FP_SHIFT, s->hits + s->misses);\r\nif (confidence < SIXTEENTH)\r\nreturn Q_POOR;\r\nelse if (confidence < EIGHTH)\r\nreturn Q_FAIR;\r\nelse\r\nreturn Q_WELL;\r\n}\r\nstatic int h_init(struct hash_table *ht, struct entry_space *es, unsigned nr_entries)\r\n{\r\nunsigned i, nr_buckets;\r\nht->es = es;\r\nnr_buckets = roundup_pow_of_two(max(nr_entries / 4u, 16u));\r\nht->hash_bits = __ffs(nr_buckets);\r\nht->buckets = vmalloc(sizeof(*ht->buckets) * nr_buckets);\r\nif (!ht->buckets)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < nr_buckets; i++)\r\nht->buckets[i] = INDEXER_NULL;\r\nreturn 0;\r\n}\r\nstatic void h_exit(struct hash_table *ht)\r\n{\r\nvfree(ht->buckets);\r\n}\r\nstatic struct entry *h_head(struct hash_table *ht, unsigned bucket)\r\n{\r\nreturn to_entry(ht->es, ht->buckets[bucket]);\r\n}\r\nstatic struct entry *h_next(struct hash_table *ht, struct entry *e)\r\n{\r\nreturn to_entry(ht->es, e->hash_next);\r\n}\r\nstatic void __h_insert(struct hash_table *ht, unsigned bucket, struct entry *e)\r\n{\r\ne->hash_next = ht->buckets[bucket];\r\nht->buckets[bucket] = to_index(ht->es, e);\r\n}\r\nstatic void h_insert(struct hash_table *ht, struct entry *e)\r\n{\r\nunsigned h = hash_64(from_oblock(e->oblock), ht->hash_bits);\r\n__h_insert(ht, h, e);\r\n}\r\nstatic struct entry *__h_lookup(struct hash_table *ht, unsigned h, dm_oblock_t oblock,\r\nstruct entry **prev)\r\n{\r\nstruct entry *e;\r\n*prev = NULL;\r\nfor (e = h_head(ht, h); e; e = h_next(ht, e)) {\r\nif (e->oblock == oblock)\r\nreturn e;\r\n*prev = e;\r\n}\r\nreturn NULL;\r\n}\r\nstatic void __h_unlink(struct hash_table *ht, unsigned h,\r\nstruct entry *e, struct entry *prev)\r\n{\r\nif (prev)\r\nprev->hash_next = e->hash_next;\r\nelse\r\nht->buckets[h] = e->hash_next;\r\n}\r\nstatic struct entry *h_lookup(struct hash_table *ht, dm_oblock_t oblock)\r\n{\r\nstruct entry *e, *prev;\r\nunsigned h = hash_64(from_oblock(oblock), ht->hash_bits);\r\ne = __h_lookup(ht, h, oblock, &prev);\r\nif (e && prev) {\r\n__h_unlink(ht, h, e, prev);\r\n__h_insert(ht, h, e);\r\n}\r\nreturn e;\r\n}\r\nstatic void h_remove(struct hash_table *ht, struct entry *e)\r\n{\r\nunsigned h = hash_64(from_oblock(e->oblock), ht->hash_bits);\r\nstruct entry *prev;\r\ne = __h_lookup(ht, h, e->oblock, &prev);\r\nif (e)\r\n__h_unlink(ht, h, e, prev);\r\n}\r\nstatic void init_allocator(struct entry_alloc *ea, struct entry_space *es,\r\nunsigned begin, unsigned end)\r\n{\r\nunsigned i;\r\nea->es = es;\r\nea->nr_allocated = 0u;\r\nea->begin = begin;\r\nl_init(&ea->free);\r\nfor (i = begin; i != end; i++)\r\nl_add_tail(ea->es, &ea->free, __get_entry(ea->es, i));\r\n}\r\nstatic void init_entry(struct entry *e)\r\n{\r\ne->hash_next = INDEXER_NULL;\r\ne->next = INDEXER_NULL;\r\ne->prev = INDEXER_NULL;\r\ne->level = 0u;\r\ne->allocated = true;\r\n}\r\nstatic struct entry *alloc_entry(struct entry_alloc *ea)\r\n{\r\nstruct entry *e;\r\nif (l_empty(&ea->free))\r\nreturn NULL;\r\ne = l_pop_tail(ea->es, &ea->free);\r\ninit_entry(e);\r\nea->nr_allocated++;\r\nreturn e;\r\n}\r\nstatic struct entry *alloc_particular_entry(struct entry_alloc *ea, unsigned i)\r\n{\r\nstruct entry *e = __get_entry(ea->es, ea->begin + i);\r\nBUG_ON(e->allocated);\r\nl_del(ea->es, &ea->free, e);\r\ninit_entry(e);\r\nea->nr_allocated++;\r\nreturn e;\r\n}\r\nstatic void free_entry(struct entry_alloc *ea, struct entry *e)\r\n{\r\nBUG_ON(!ea->nr_allocated);\r\nBUG_ON(!e->allocated);\r\nea->nr_allocated--;\r\ne->allocated = false;\r\nl_add_tail(ea->es, &ea->free, e);\r\n}\r\nstatic bool allocator_empty(struct entry_alloc *ea)\r\n{\r\nreturn l_empty(&ea->free);\r\n}\r\nstatic unsigned get_index(struct entry_alloc *ea, struct entry *e)\r\n{\r\nreturn to_index(ea->es, e) - ea->begin;\r\n}\r\nstatic struct entry *get_entry(struct entry_alloc *ea, unsigned index)\r\n{\r\nreturn __get_entry(ea->es, ea->begin + index);\r\n}\r\nstatic struct entry *get_sentinel(struct entry_alloc *ea, unsigned level, bool which)\r\n{\r\nreturn get_entry(ea, which ? level : NR_CACHE_LEVELS + level);\r\n}\r\nstatic struct entry *writeback_sentinel(struct smq_policy *mq, unsigned level)\r\n{\r\nreturn get_sentinel(&mq->writeback_sentinel_alloc, level, mq->current_writeback_sentinels);\r\n}\r\nstatic struct entry *demote_sentinel(struct smq_policy *mq, unsigned level)\r\n{\r\nreturn get_sentinel(&mq->demote_sentinel_alloc, level, mq->current_demote_sentinels);\r\n}\r\nstatic void __update_writeback_sentinels(struct smq_policy *mq)\r\n{\r\nunsigned level;\r\nstruct queue *q = &mq->dirty;\r\nstruct entry *sentinel;\r\nfor (level = 0; level < q->nr_levels; level++) {\r\nsentinel = writeback_sentinel(mq, level);\r\nq_del(q, sentinel);\r\nq_push(q, sentinel);\r\n}\r\n}\r\nstatic void __update_demote_sentinels(struct smq_policy *mq)\r\n{\r\nunsigned level;\r\nstruct queue *q = &mq->clean;\r\nstruct entry *sentinel;\r\nfor (level = 0; level < q->nr_levels; level++) {\r\nsentinel = demote_sentinel(mq, level);\r\nq_del(q, sentinel);\r\nq_push(q, sentinel);\r\n}\r\n}\r\nstatic void update_sentinels(struct smq_policy *mq)\r\n{\r\nif (time_after(jiffies, mq->next_writeback_period)) {\r\n__update_writeback_sentinels(mq);\r\nmq->next_writeback_period = jiffies + WRITEBACK_PERIOD;\r\nmq->current_writeback_sentinels = !mq->current_writeback_sentinels;\r\n}\r\nif (time_after(jiffies, mq->next_demote_period)) {\r\n__update_demote_sentinels(mq);\r\nmq->next_demote_period = jiffies + DEMOTE_PERIOD;\r\nmq->current_demote_sentinels = !mq->current_demote_sentinels;\r\n}\r\n}\r\nstatic void __sentinels_init(struct smq_policy *mq)\r\n{\r\nunsigned level;\r\nstruct entry *sentinel;\r\nfor (level = 0; level < NR_CACHE_LEVELS; level++) {\r\nsentinel = writeback_sentinel(mq, level);\r\nsentinel->level = level;\r\nq_push(&mq->dirty, sentinel);\r\nsentinel = demote_sentinel(mq, level);\r\nsentinel->level = level;\r\nq_push(&mq->clean, sentinel);\r\n}\r\n}\r\nstatic void sentinels_init(struct smq_policy *mq)\r\n{\r\nmq->next_writeback_period = jiffies + WRITEBACK_PERIOD;\r\nmq->next_demote_period = jiffies + DEMOTE_PERIOD;\r\nmq->current_writeback_sentinels = false;\r\nmq->current_demote_sentinels = false;\r\n__sentinels_init(mq);\r\nmq->current_writeback_sentinels = !mq->current_writeback_sentinels;\r\nmq->current_demote_sentinels = !mq->current_demote_sentinels;\r\n__sentinels_init(mq);\r\n}\r\nstatic void push_new(struct smq_policy *mq, struct entry *e)\r\n{\r\nstruct queue *q = e->dirty ? &mq->dirty : &mq->clean;\r\nh_insert(&mq->table, e);\r\nq_push(q, e);\r\n}\r\nstatic void push(struct smq_policy *mq, struct entry *e)\r\n{\r\nstruct entry *sentinel;\r\nh_insert(&mq->table, e);\r\nif (e->dirty) {\r\nsentinel = writeback_sentinel(mq, e->level);\r\nq_push_before(&mq->dirty, sentinel, e);\r\n} else {\r\nsentinel = demote_sentinel(mq, e->level);\r\nq_push_before(&mq->clean, sentinel, e);\r\n}\r\n}\r\nstatic void __del(struct smq_policy *mq, struct queue *q, struct entry *e)\r\n{\r\nq_del(q, e);\r\nh_remove(&mq->table, e);\r\n}\r\nstatic void del(struct smq_policy *mq, struct entry *e)\r\n{\r\n__del(mq, e->dirty ? &mq->dirty : &mq->clean, e);\r\n}\r\nstatic struct entry *pop_old(struct smq_policy *mq, struct queue *q, unsigned max_level)\r\n{\r\nstruct entry *e = q_pop_old(q, max_level);\r\nif (e)\r\nh_remove(&mq->table, e);\r\nreturn e;\r\n}\r\nstatic dm_cblock_t infer_cblock(struct smq_policy *mq, struct entry *e)\r\n{\r\nreturn to_cblock(get_index(&mq->cache_alloc, e));\r\n}\r\nstatic void requeue(struct smq_policy *mq, struct entry *e)\r\n{\r\nstruct entry *sentinel;\r\nif (!test_and_set_bit(from_cblock(infer_cblock(mq, e)), mq->cache_hit_bits)) {\r\nif (e->dirty) {\r\nsentinel = writeback_sentinel(mq, e->level);\r\nq_requeue_before(&mq->dirty, sentinel, e, 1u);\r\n} else {\r\nsentinel = demote_sentinel(mq, e->level);\r\nq_requeue_before(&mq->clean, sentinel, e, 1u);\r\n}\r\n}\r\n}\r\nstatic unsigned default_promote_level(struct smq_policy *mq)\r\n{\r\nstatic unsigned table[] = {1, 1, 1, 2, 4, 6, 7, 8, 7, 6, 4, 4, 3, 3, 2, 2, 1};\r\nunsigned hits = mq->cache_stats.hits;\r\nunsigned misses = mq->cache_stats.misses;\r\nunsigned index = safe_div(hits << 4u, hits + misses);\r\nreturn table[index];\r\n}\r\nstatic void update_promote_levels(struct smq_policy *mq)\r\n{\r\nunsigned threshold_level = allocator_empty(&mq->cache_alloc) ?\r\ndefault_promote_level(mq) : (NR_HOTSPOT_LEVELS / 2u);\r\nswitch (stats_assess(&mq->hotspot_stats)) {\r\ncase Q_POOR:\r\nthreshold_level /= 4u;\r\nbreak;\r\ncase Q_FAIR:\r\nthreshold_level /= 2u;\r\nbreak;\r\ncase Q_WELL:\r\nbreak;\r\n}\r\nmq->read_promote_level = NR_HOTSPOT_LEVELS - threshold_level;\r\nmq->write_promote_level = (NR_HOTSPOT_LEVELS - threshold_level) + 2u;\r\n}\r\nstatic void update_level_jump(struct smq_policy *mq)\r\n{\r\nswitch (stats_assess(&mq->hotspot_stats)) {\r\ncase Q_POOR:\r\nmq->hotspot_level_jump = 4u;\r\nbreak;\r\ncase Q_FAIR:\r\nmq->hotspot_level_jump = 2u;\r\nbreak;\r\ncase Q_WELL:\r\nmq->hotspot_level_jump = 1u;\r\nbreak;\r\n}\r\n}\r\nstatic void end_hotspot_period(struct smq_policy *mq)\r\n{\r\nclear_bitset(mq->hotspot_hit_bits, mq->nr_hotspot_blocks);\r\nupdate_promote_levels(mq);\r\nif (time_after(jiffies, mq->next_hotspot_period)) {\r\nupdate_level_jump(mq);\r\nq_redistribute(&mq->hotspot);\r\nstats_reset(&mq->hotspot_stats);\r\nmq->next_hotspot_period = jiffies + HOTSPOT_UPDATE_PERIOD;\r\n}\r\n}\r\nstatic void end_cache_period(struct smq_policy *mq)\r\n{\r\nif (time_after(jiffies, mq->next_cache_period)) {\r\nclear_bitset(mq->cache_hit_bits, from_cblock(mq->cache_size));\r\nq_redistribute(&mq->dirty);\r\nq_redistribute(&mq->clean);\r\nstats_reset(&mq->cache_stats);\r\nmq->next_cache_period = jiffies + CACHE_UPDATE_PERIOD;\r\n}\r\n}\r\nstatic int demote_cblock(struct smq_policy *mq,\r\nstruct policy_locker *locker,\r\ndm_oblock_t *oblock)\r\n{\r\nstruct entry *demoted = q_peek(&mq->clean, mq->clean.nr_levels, false);\r\nif (!demoted)\r\nreturn -ENOSPC;\r\nif (locker->fn(locker, demoted->oblock))\r\nreturn -EBUSY;\r\ndel(mq, demoted);\r\n*oblock = demoted->oblock;\r\nfree_entry(&mq->cache_alloc, demoted);\r\nreturn 0;\r\n}\r\nstatic enum promote_result maybe_promote(bool promote)\r\n{\r\nreturn promote ? PROMOTE_PERMANENT : PROMOTE_NOT;\r\n}\r\nstatic enum promote_result should_promote(struct smq_policy *mq, struct entry *hs_e, struct bio *bio,\r\nbool fast_promote)\r\n{\r\nif (bio_data_dir(bio) == WRITE) {\r\nif (!allocator_empty(&mq->cache_alloc) && fast_promote)\r\nreturn PROMOTE_TEMPORARY;\r\nelse\r\nreturn maybe_promote(hs_e->level >= mq->write_promote_level);\r\n} else\r\nreturn maybe_promote(hs_e->level >= mq->read_promote_level);\r\n}\r\nstatic void insert_in_cache(struct smq_policy *mq, dm_oblock_t oblock,\r\nstruct policy_locker *locker,\r\nstruct policy_result *result, enum promote_result pr)\r\n{\r\nint r;\r\nstruct entry *e;\r\nif (allocator_empty(&mq->cache_alloc)) {\r\nresult->op = POLICY_REPLACE;\r\nr = demote_cblock(mq, locker, &result->old_oblock);\r\nif (r) {\r\nresult->op = POLICY_MISS;\r\nreturn;\r\n}\r\n} else\r\nresult->op = POLICY_NEW;\r\ne = alloc_entry(&mq->cache_alloc);\r\nBUG_ON(!e);\r\ne->oblock = oblock;\r\nif (pr == PROMOTE_TEMPORARY)\r\npush(mq, e);\r\nelse\r\npush_new(mq, e);\r\nresult->cblock = infer_cblock(mq, e);\r\n}\r\nstatic dm_oblock_t to_hblock(struct smq_policy *mq, dm_oblock_t b)\r\n{\r\nsector_t r = from_oblock(b);\r\n(void) sector_div(r, mq->cache_blocks_per_hotspot_block);\r\nreturn to_oblock(r);\r\n}\r\nstatic struct entry *update_hotspot_queue(struct smq_policy *mq, dm_oblock_t b, struct bio *bio)\r\n{\r\nunsigned hi;\r\ndm_oblock_t hb = to_hblock(mq, b);\r\nstruct entry *e = h_lookup(&mq->hotspot_table, hb);\r\nif (e) {\r\nstats_level_accessed(&mq->hotspot_stats, e->level);\r\nhi = get_index(&mq->hotspot_alloc, e);\r\nq_requeue(&mq->hotspot, e,\r\ntest_and_set_bit(hi, mq->hotspot_hit_bits) ?\r\n0u : mq->hotspot_level_jump);\r\n} else {\r\nstats_miss(&mq->hotspot_stats);\r\ne = alloc_entry(&mq->hotspot_alloc);\r\nif (!e) {\r\ne = q_pop(&mq->hotspot);\r\nif (e) {\r\nh_remove(&mq->hotspot_table, e);\r\nhi = get_index(&mq->hotspot_alloc, e);\r\nclear_bit(hi, mq->hotspot_hit_bits);\r\n}\r\n}\r\nif (e) {\r\ne->oblock = hb;\r\nq_push(&mq->hotspot, e);\r\nh_insert(&mq->hotspot_table, e);\r\n}\r\n}\r\nreturn e;\r\n}\r\nstatic int map(struct smq_policy *mq, struct bio *bio, dm_oblock_t oblock,\r\nbool can_migrate, bool fast_promote,\r\nstruct policy_locker *locker, struct policy_result *result)\r\n{\r\nstruct entry *e, *hs_e;\r\nenum promote_result pr;\r\nhs_e = update_hotspot_queue(mq, oblock, bio);\r\ne = h_lookup(&mq->table, oblock);\r\nif (e) {\r\nstats_level_accessed(&mq->cache_stats, e->level);\r\nrequeue(mq, e);\r\nresult->op = POLICY_HIT;\r\nresult->cblock = infer_cblock(mq, e);\r\n} else {\r\nstats_miss(&mq->cache_stats);\r\npr = should_promote(mq, hs_e, bio, fast_promote);\r\nif (pr == PROMOTE_NOT)\r\nresult->op = POLICY_MISS;\r\nelse {\r\nif (!can_migrate) {\r\nresult->op = POLICY_MISS;\r\nreturn -EWOULDBLOCK;\r\n}\r\ninsert_in_cache(mq, oblock, locker, result, pr);\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic struct smq_policy *to_smq_policy(struct dm_cache_policy *p)\r\n{\r\nreturn container_of(p, struct smq_policy, policy);\r\n}\r\nstatic void smq_destroy(struct dm_cache_policy *p)\r\n{\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nh_exit(&mq->hotspot_table);\r\nh_exit(&mq->table);\r\nfree_bitset(mq->hotspot_hit_bits);\r\nfree_bitset(mq->cache_hit_bits);\r\nspace_exit(&mq->es);\r\nkfree(mq);\r\n}\r\nstatic int smq_map(struct dm_cache_policy *p, dm_oblock_t oblock,\r\nbool can_block, bool can_migrate, bool fast_promote,\r\nstruct bio *bio, struct policy_locker *locker,\r\nstruct policy_result *result)\r\n{\r\nint r;\r\nunsigned long flags;\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nresult->op = POLICY_MISS;\r\nspin_lock_irqsave(&mq->lock, flags);\r\nr = map(mq, bio, oblock, can_migrate, fast_promote, locker, result);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\nreturn r;\r\n}\r\nstatic int smq_lookup(struct dm_cache_policy *p, dm_oblock_t oblock, dm_cblock_t *cblock)\r\n{\r\nint r;\r\nunsigned long flags;\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nstruct entry *e;\r\nspin_lock_irqsave(&mq->lock, flags);\r\ne = h_lookup(&mq->table, oblock);\r\nif (e) {\r\n*cblock = infer_cblock(mq, e);\r\nr = 0;\r\n} else\r\nr = -ENOENT;\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\nreturn r;\r\n}\r\nstatic void __smq_set_clear_dirty(struct smq_policy *mq, dm_oblock_t oblock, bool set)\r\n{\r\nstruct entry *e;\r\ne = h_lookup(&mq->table, oblock);\r\nBUG_ON(!e);\r\ndel(mq, e);\r\ne->dirty = set;\r\npush(mq, e);\r\n}\r\nstatic void smq_set_dirty(struct dm_cache_policy *p, dm_oblock_t oblock)\r\n{\r\nunsigned long flags;\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nspin_lock_irqsave(&mq->lock, flags);\r\n__smq_set_clear_dirty(mq, oblock, true);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\n}\r\nstatic void smq_clear_dirty(struct dm_cache_policy *p, dm_oblock_t oblock)\r\n{\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mq->lock, flags);\r\n__smq_set_clear_dirty(mq, oblock, false);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\n}\r\nstatic int smq_load_mapping(struct dm_cache_policy *p,\r\ndm_oblock_t oblock, dm_cblock_t cblock,\r\nuint32_t hint, bool hint_valid)\r\n{\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nstruct entry *e;\r\ne = alloc_particular_entry(&mq->cache_alloc, from_cblock(cblock));\r\ne->oblock = oblock;\r\ne->dirty = false;\r\ne->level = hint_valid ? min(hint, NR_CACHE_LEVELS - 1) : 1;\r\npush(mq, e);\r\nreturn 0;\r\n}\r\nstatic int smq_save_hints(struct smq_policy *mq, struct queue *q,\r\npolicy_walk_fn fn, void *context)\r\n{\r\nint r;\r\nunsigned level;\r\nstruct entry *e;\r\nfor (level = 0; level < q->nr_levels; level++)\r\nfor (e = l_head(q->es, q->qs + level); e; e = l_next(q->es, e)) {\r\nif (!e->sentinel) {\r\nr = fn(context, infer_cblock(mq, e),\r\ne->oblock, e->level);\r\nif (r)\r\nreturn r;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int smq_walk_mappings(struct dm_cache_policy *p, policy_walk_fn fn,\r\nvoid *context)\r\n{\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nint r = 0;\r\nr = smq_save_hints(mq, &mq->clean, fn, context);\r\nif (!r)\r\nr = smq_save_hints(mq, &mq->dirty, fn, context);\r\nreturn r;\r\n}\r\nstatic void __remove_mapping(struct smq_policy *mq, dm_oblock_t oblock)\r\n{\r\nstruct entry *e;\r\ne = h_lookup(&mq->table, oblock);\r\nBUG_ON(!e);\r\ndel(mq, e);\r\nfree_entry(&mq->cache_alloc, e);\r\n}\r\nstatic void smq_remove_mapping(struct dm_cache_policy *p, dm_oblock_t oblock)\r\n{\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mq->lock, flags);\r\n__remove_mapping(mq, oblock);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\n}\r\nstatic int __remove_cblock(struct smq_policy *mq, dm_cblock_t cblock)\r\n{\r\nstruct entry *e = get_entry(&mq->cache_alloc, from_cblock(cblock));\r\nif (!e || !e->allocated)\r\nreturn -ENODATA;\r\ndel(mq, e);\r\nfree_entry(&mq->cache_alloc, e);\r\nreturn 0;\r\n}\r\nstatic int smq_remove_cblock(struct dm_cache_policy *p, dm_cblock_t cblock)\r\n{\r\nint r;\r\nunsigned long flags;\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nspin_lock_irqsave(&mq->lock, flags);\r\nr = __remove_cblock(mq, cblock);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\nreturn r;\r\n}\r\nstatic bool clean_target_met(struct smq_policy *mq, bool critical)\r\n{\r\nif (critical) {\r\nunsigned nr_clean = from_cblock(mq->cache_size) - q_size(&mq->dirty);\r\nunsigned target = from_cblock(mq->cache_size) * CLEAN_TARGET_CRITICAL / 100u;\r\nreturn nr_clean >= target;\r\n} else\r\nreturn !q_size(&mq->dirty);\r\n}\r\nstatic int __smq_writeback_work(struct smq_policy *mq, dm_oblock_t *oblock,\r\ndm_cblock_t *cblock, bool critical_only)\r\n{\r\nstruct entry *e = NULL;\r\nbool target_met = clean_target_met(mq, critical_only);\r\nif (critical_only)\r\ne = pop_old(mq, &mq->dirty, target_met ? 1u : mq->dirty.nr_levels);\r\nelse\r\ne = pop_old(mq, &mq->dirty, mq->dirty.nr_levels);\r\nif (!e)\r\nreturn -ENODATA;\r\n*oblock = e->oblock;\r\n*cblock = infer_cblock(mq, e);\r\ne->dirty = false;\r\npush_new(mq, e);\r\nreturn 0;\r\n}\r\nstatic int smq_writeback_work(struct dm_cache_policy *p, dm_oblock_t *oblock,\r\ndm_cblock_t *cblock, bool critical_only)\r\n{\r\nint r;\r\nunsigned long flags;\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nspin_lock_irqsave(&mq->lock, flags);\r\nr = __smq_writeback_work(mq, oblock, cblock, critical_only);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\nreturn r;\r\n}\r\nstatic void __force_mapping(struct smq_policy *mq,\r\ndm_oblock_t current_oblock, dm_oblock_t new_oblock)\r\n{\r\nstruct entry *e = h_lookup(&mq->table, current_oblock);\r\nif (e) {\r\ndel(mq, e);\r\ne->oblock = new_oblock;\r\ne->dirty = true;\r\npush(mq, e);\r\n}\r\n}\r\nstatic void smq_force_mapping(struct dm_cache_policy *p,\r\ndm_oblock_t current_oblock, dm_oblock_t new_oblock)\r\n{\r\nunsigned long flags;\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nspin_lock_irqsave(&mq->lock, flags);\r\n__force_mapping(mq, current_oblock, new_oblock);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\n}\r\nstatic dm_cblock_t smq_residency(struct dm_cache_policy *p)\r\n{\r\ndm_cblock_t r;\r\nunsigned long flags;\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nspin_lock_irqsave(&mq->lock, flags);\r\nr = to_cblock(mq->cache_alloc.nr_allocated);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\nreturn r;\r\n}\r\nstatic void smq_tick(struct dm_cache_policy *p, bool can_block)\r\n{\r\nstruct smq_policy *mq = to_smq_policy(p);\r\nunsigned long flags;\r\nspin_lock_irqsave(&mq->lock, flags);\r\nmq->tick++;\r\nupdate_sentinels(mq);\r\nend_hotspot_period(mq);\r\nend_cache_period(mq);\r\nspin_unlock_irqrestore(&mq->lock, flags);\r\n}\r\nstatic int mq_set_config_value(struct dm_cache_policy *p,\r\nconst char *key, const char *value)\r\n{\r\nunsigned long tmp;\r\nif (kstrtoul(value, 10, &tmp))\r\nreturn -EINVAL;\r\nif (!strcasecmp(key, "random_threshold") ||\r\n!strcasecmp(key, "sequential_threshold") ||\r\n!strcasecmp(key, "discard_promote_adjustment") ||\r\n!strcasecmp(key, "read_promote_adjustment") ||\r\n!strcasecmp(key, "write_promote_adjustment")) {\r\nDMWARN("tunable '%s' no longer has any effect, mq policy is now an alias for smq", key);\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic int mq_emit_config_values(struct dm_cache_policy *p, char *result,\r\nunsigned maxlen, ssize_t *sz_ptr)\r\n{\r\nssize_t sz = *sz_ptr;\r\nDMEMIT("10 random_threshold 0 "\r\n"sequential_threshold 0 "\r\n"discard_promote_adjustment 0 "\r\n"read_promote_adjustment 0 "\r\n"write_promote_adjustment 0 ");\r\n*sz_ptr = sz;\r\nreturn 0;\r\n}\r\nstatic void init_policy_functions(struct smq_policy *mq, bool mimic_mq)\r\n{\r\nmq->policy.destroy = smq_destroy;\r\nmq->policy.map = smq_map;\r\nmq->policy.lookup = smq_lookup;\r\nmq->policy.set_dirty = smq_set_dirty;\r\nmq->policy.clear_dirty = smq_clear_dirty;\r\nmq->policy.load_mapping = smq_load_mapping;\r\nmq->policy.walk_mappings = smq_walk_mappings;\r\nmq->policy.remove_mapping = smq_remove_mapping;\r\nmq->policy.remove_cblock = smq_remove_cblock;\r\nmq->policy.writeback_work = smq_writeback_work;\r\nmq->policy.force_mapping = smq_force_mapping;\r\nmq->policy.residency = smq_residency;\r\nmq->policy.tick = smq_tick;\r\nif (mimic_mq) {\r\nmq->policy.set_config_value = mq_set_config_value;\r\nmq->policy.emit_config_values = mq_emit_config_values;\r\n}\r\n}\r\nstatic bool too_many_hotspot_blocks(sector_t origin_size,\r\nsector_t hotspot_block_size,\r\nunsigned nr_hotspot_blocks)\r\n{\r\nreturn (hotspot_block_size * nr_hotspot_blocks) > origin_size;\r\n}\r\nstatic void calc_hotspot_params(sector_t origin_size,\r\nsector_t cache_block_size,\r\nunsigned nr_cache_blocks,\r\nsector_t *hotspot_block_size,\r\nunsigned *nr_hotspot_blocks)\r\n{\r\n*hotspot_block_size = cache_block_size * 16u;\r\n*nr_hotspot_blocks = max(nr_cache_blocks / 4u, 1024u);\r\nwhile ((*hotspot_block_size > cache_block_size) &&\r\ntoo_many_hotspot_blocks(origin_size, *hotspot_block_size, *nr_hotspot_blocks))\r\n*hotspot_block_size /= 2u;\r\n}\r\nstatic struct dm_cache_policy *__smq_create(dm_cblock_t cache_size,\r\nsector_t origin_size,\r\nsector_t cache_block_size,\r\nbool mimic_mq)\r\n{\r\nunsigned i;\r\nunsigned nr_sentinels_per_queue = 2u * NR_CACHE_LEVELS;\r\nunsigned total_sentinels = 2u * nr_sentinels_per_queue;\r\nstruct smq_policy *mq = kzalloc(sizeof(*mq), GFP_KERNEL);\r\nif (!mq)\r\nreturn NULL;\r\ninit_policy_functions(mq, mimic_mq);\r\nmq->cache_size = cache_size;\r\nmq->cache_block_size = cache_block_size;\r\ncalc_hotspot_params(origin_size, cache_block_size, from_cblock(cache_size),\r\n&mq->hotspot_block_size, &mq->nr_hotspot_blocks);\r\nmq->cache_blocks_per_hotspot_block = div64_u64(mq->hotspot_block_size, mq->cache_block_size);\r\nmq->hotspot_level_jump = 1u;\r\nif (space_init(&mq->es, total_sentinels + mq->nr_hotspot_blocks + from_cblock(cache_size))) {\r\nDMERR("couldn't initialize entry space");\r\ngoto bad_pool_init;\r\n}\r\ninit_allocator(&mq->writeback_sentinel_alloc, &mq->es, 0, nr_sentinels_per_queue);\r\nfor (i = 0; i < nr_sentinels_per_queue; i++)\r\nget_entry(&mq->writeback_sentinel_alloc, i)->sentinel = true;\r\ninit_allocator(&mq->demote_sentinel_alloc, &mq->es, nr_sentinels_per_queue, total_sentinels);\r\nfor (i = 0; i < nr_sentinels_per_queue; i++)\r\nget_entry(&mq->demote_sentinel_alloc, i)->sentinel = true;\r\ninit_allocator(&mq->hotspot_alloc, &mq->es, total_sentinels,\r\ntotal_sentinels + mq->nr_hotspot_blocks);\r\ninit_allocator(&mq->cache_alloc, &mq->es,\r\ntotal_sentinels + mq->nr_hotspot_blocks,\r\ntotal_sentinels + mq->nr_hotspot_blocks + from_cblock(cache_size));\r\nmq->hotspot_hit_bits = alloc_bitset(mq->nr_hotspot_blocks);\r\nif (!mq->hotspot_hit_bits) {\r\nDMERR("couldn't allocate hotspot hit bitset");\r\ngoto bad_hotspot_hit_bits;\r\n}\r\nclear_bitset(mq->hotspot_hit_bits, mq->nr_hotspot_blocks);\r\nif (from_cblock(cache_size)) {\r\nmq->cache_hit_bits = alloc_bitset(from_cblock(cache_size));\r\nif (!mq->cache_hit_bits) {\r\nDMERR("couldn't allocate cache hit bitset");\r\ngoto bad_cache_hit_bits;\r\n}\r\nclear_bitset(mq->cache_hit_bits, from_cblock(mq->cache_size));\r\n} else\r\nmq->cache_hit_bits = NULL;\r\nmq->tick = 0;\r\nspin_lock_init(&mq->lock);\r\nq_init(&mq->hotspot, &mq->es, NR_HOTSPOT_LEVELS);\r\nmq->hotspot.nr_top_levels = 8;\r\nmq->hotspot.nr_in_top_levels = min(mq->nr_hotspot_blocks / NR_HOTSPOT_LEVELS,\r\nfrom_cblock(mq->cache_size) / mq->cache_blocks_per_hotspot_block);\r\nq_init(&mq->clean, &mq->es, NR_CACHE_LEVELS);\r\nq_init(&mq->dirty, &mq->es, NR_CACHE_LEVELS);\r\nstats_init(&mq->hotspot_stats, NR_HOTSPOT_LEVELS);\r\nstats_init(&mq->cache_stats, NR_CACHE_LEVELS);\r\nif (h_init(&mq->table, &mq->es, from_cblock(cache_size)))\r\ngoto bad_alloc_table;\r\nif (h_init(&mq->hotspot_table, &mq->es, mq->nr_hotspot_blocks))\r\ngoto bad_alloc_hotspot_table;\r\nsentinels_init(mq);\r\nmq->write_promote_level = mq->read_promote_level = NR_HOTSPOT_LEVELS;\r\nmq->next_hotspot_period = jiffies;\r\nmq->next_cache_period = jiffies;\r\nreturn &mq->policy;\r\nbad_alloc_hotspot_table:\r\nh_exit(&mq->table);\r\nbad_alloc_table:\r\nfree_bitset(mq->cache_hit_bits);\r\nbad_cache_hit_bits:\r\nfree_bitset(mq->hotspot_hit_bits);\r\nbad_hotspot_hit_bits:\r\nspace_exit(&mq->es);\r\nbad_pool_init:\r\nkfree(mq);\r\nreturn NULL;\r\n}\r\nstatic struct dm_cache_policy *smq_create(dm_cblock_t cache_size,\r\nsector_t origin_size,\r\nsector_t cache_block_size)\r\n{\r\nreturn __smq_create(cache_size, origin_size, cache_block_size, false);\r\n}\r\nstatic struct dm_cache_policy *mq_create(dm_cblock_t cache_size,\r\nsector_t origin_size,\r\nsector_t cache_block_size)\r\n{\r\nreturn __smq_create(cache_size, origin_size, cache_block_size, true);\r\n}\r\nstatic int __init smq_init(void)\r\n{\r\nint r;\r\nr = dm_cache_policy_register(&smq_policy_type);\r\nif (r) {\r\nDMERR("register failed %d", r);\r\nreturn -ENOMEM;\r\n}\r\nr = dm_cache_policy_register(&mq_policy_type);\r\nif (r) {\r\nDMERR("register failed (as mq) %d", r);\r\ndm_cache_policy_unregister(&smq_policy_type);\r\nreturn -ENOMEM;\r\n}\r\nr = dm_cache_policy_register(&default_policy_type);\r\nif (r) {\r\nDMERR("register failed (as default) %d", r);\r\ndm_cache_policy_unregister(&mq_policy_type);\r\ndm_cache_policy_unregister(&smq_policy_type);\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __exit smq_exit(void)\r\n{\r\ndm_cache_policy_unregister(&smq_policy_type);\r\ndm_cache_policy_unregister(&mq_policy_type);\r\ndm_cache_policy_unregister(&default_policy_type);\r\n}
