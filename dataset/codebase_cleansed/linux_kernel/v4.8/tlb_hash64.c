void hpte_need_flush(struct mm_struct *mm, unsigned long addr,\r\npte_t *ptep, unsigned long pte, int huge)\r\n{\r\nunsigned long vpn;\r\nstruct ppc64_tlb_batch *batch = &get_cpu_var(ppc64_tlb_batch);\r\nunsigned long vsid;\r\nunsigned int psize;\r\nint ssize;\r\nreal_pte_t rpte;\r\nint i;\r\ni = batch->index;\r\nif (huge) {\r\n#ifdef CONFIG_HUGETLB_PAGE\r\npsize = get_slice_psize(mm, addr);\r\naddr &= ~((1UL << mmu_psize_defs[psize].shift) - 1);\r\n#else\r\nBUG();\r\npsize = pte_pagesize_index(mm, addr, pte);\r\n#endif\r\n} else {\r\npsize = pte_pagesize_index(mm, addr, pte);\r\naddr &= PAGE_MASK;\r\n}\r\nif (!is_kernel_addr(addr)) {\r\nssize = user_segment_size(addr);\r\nvsid = get_vsid(mm->context.id, addr, ssize);\r\n} else {\r\nvsid = get_kernel_vsid(addr, mmu_kernel_ssize);\r\nssize = mmu_kernel_ssize;\r\n}\r\nWARN_ON(vsid == 0);\r\nvpn = hpt_vpn(addr, vsid, ssize);\r\nrpte = __real_pte(__pte(pte), ptep);\r\nif (!batch->active) {\r\nflush_hash_page(vpn, rpte, psize, ssize, 0);\r\nput_cpu_var(ppc64_tlb_batch);\r\nreturn;\r\n}\r\nif (i != 0 && (mm != batch->mm || batch->psize != psize ||\r\nbatch->ssize != ssize)) {\r\n__flush_tlb_pending(batch);\r\ni = 0;\r\n}\r\nif (i == 0) {\r\nbatch->mm = mm;\r\nbatch->psize = psize;\r\nbatch->ssize = ssize;\r\n}\r\nbatch->pte[i] = rpte;\r\nbatch->vpn[i] = vpn;\r\nbatch->index = ++i;\r\nif (i >= PPC64_TLB_BATCH_NR)\r\n__flush_tlb_pending(batch);\r\nput_cpu_var(ppc64_tlb_batch);\r\n}\r\nvoid __flush_tlb_pending(struct ppc64_tlb_batch *batch)\r\n{\r\nconst struct cpumask *tmp;\r\nint i, local = 0;\r\ni = batch->index;\r\ntmp = cpumask_of(smp_processor_id());\r\nif (cpumask_equal(mm_cpumask(batch->mm), tmp))\r\nlocal = 1;\r\nif (i == 1)\r\nflush_hash_page(batch->vpn[0], batch->pte[0],\r\nbatch->psize, batch->ssize, local);\r\nelse\r\nflush_hash_range(i, local);\r\nbatch->index = 0;\r\n}\r\nvoid hash__tlb_flush(struct mmu_gather *tlb)\r\n{\r\nstruct ppc64_tlb_batch *tlbbatch = &get_cpu_var(ppc64_tlb_batch);\r\nif (tlbbatch->index)\r\n__flush_tlb_pending(tlbbatch);\r\nput_cpu_var(ppc64_tlb_batch);\r\n}\r\nvoid __flush_hash_table_range(struct mm_struct *mm, unsigned long start,\r\nunsigned long end)\r\n{\r\nbool is_thp;\r\nint hugepage_shift;\r\nunsigned long flags;\r\nstart = _ALIGN_DOWN(start, PAGE_SIZE);\r\nend = _ALIGN_UP(end, PAGE_SIZE);\r\nBUG_ON(!mm->pgd);\r\nlocal_irq_save(flags);\r\narch_enter_lazy_mmu_mode();\r\nfor (; start < end; start += PAGE_SIZE) {\r\npte_t *ptep = find_linux_pte_or_hugepte(mm->pgd, start, &is_thp,\r\n&hugepage_shift);\r\nunsigned long pte;\r\nif (ptep == NULL)\r\ncontinue;\r\npte = pte_val(*ptep);\r\nif (is_thp)\r\ntrace_hugepage_invalidate(start, pte);\r\nif (!(pte & H_PAGE_HASHPTE))\r\ncontinue;\r\nif (unlikely(is_thp))\r\nhpte_do_hugepage_flush(mm, start, (pmd_t *)ptep, pte);\r\nelse\r\nhpte_need_flush(mm, start, ptep, pte, hugepage_shift);\r\n}\r\narch_leave_lazy_mmu_mode();\r\nlocal_irq_restore(flags);\r\n}\r\nvoid flush_tlb_pmd_range(struct mm_struct *mm, pmd_t *pmd, unsigned long addr)\r\n{\r\npte_t *pte;\r\npte_t *start_pte;\r\nunsigned long flags;\r\naddr = _ALIGN_DOWN(addr, PMD_SIZE);\r\nlocal_irq_save(flags);\r\narch_enter_lazy_mmu_mode();\r\nstart_pte = pte_offset_map(pmd, addr);\r\nfor (pte = start_pte; pte < start_pte + PTRS_PER_PTE; pte++) {\r\nunsigned long pteval = pte_val(*pte);\r\nif (pteval & H_PAGE_HASHPTE)\r\nhpte_need_flush(mm, addr, pte, pteval, 0);\r\naddr += PAGE_SIZE;\r\n}\r\narch_leave_lazy_mmu_mode();\r\nlocal_irq_restore(flags);\r\n}
