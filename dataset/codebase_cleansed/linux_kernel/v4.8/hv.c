static int query_hypervisor_info(void)\r\n{\r\nunsigned int eax;\r\nunsigned int ebx;\r\nunsigned int ecx;\r\nunsigned int edx;\r\nunsigned int max_leaf;\r\nunsigned int op;\r\neax = 0;\r\nebx = 0;\r\necx = 0;\r\nedx = 0;\r\nop = HVCPUID_VENDOR_MAXFUNCTION;\r\ncpuid(op, &eax, &ebx, &ecx, &edx);\r\nmax_leaf = eax;\r\nif (max_leaf >= HVCPUID_VERSION) {\r\neax = 0;\r\nebx = 0;\r\necx = 0;\r\nedx = 0;\r\nop = HVCPUID_VERSION;\r\ncpuid(op, &eax, &ebx, &ecx, &edx);\r\nhost_info_eax = eax;\r\nhost_info_ebx = ebx;\r\nhost_info_ecx = ecx;\r\nhost_info_edx = edx;\r\n}\r\nreturn max_leaf;\r\n}\r\nu64 hv_do_hypercall(u64 control, void *input, void *output)\r\n{\r\nu64 input_address = (input) ? virt_to_phys(input) : 0;\r\nu64 output_address = (output) ? virt_to_phys(output) : 0;\r\nvoid *hypercall_page = hv_context.hypercall_page;\r\n#ifdef CONFIG_X86_64\r\nu64 hv_status = 0;\r\nif (!hypercall_page)\r\nreturn (u64)ULLONG_MAX;\r\n__asm__ __volatile__("mov %0, %%r8" : : "r" (output_address) : "r8");\r\n__asm__ __volatile__("call *%3" : "=a" (hv_status) :\r\n"c" (control), "d" (input_address),\r\n"m" (hypercall_page));\r\nreturn hv_status;\r\n#else\r\nu32 control_hi = control >> 32;\r\nu32 control_lo = control & 0xFFFFFFFF;\r\nu32 hv_status_hi = 1;\r\nu32 hv_status_lo = 1;\r\nu32 input_address_hi = input_address >> 32;\r\nu32 input_address_lo = input_address & 0xFFFFFFFF;\r\nu32 output_address_hi = output_address >> 32;\r\nu32 output_address_lo = output_address & 0xFFFFFFFF;\r\nif (!hypercall_page)\r\nreturn (u64)ULLONG_MAX;\r\n__asm__ __volatile__ ("call *%8" : "=d"(hv_status_hi),\r\n"=a"(hv_status_lo) : "d" (control_hi),\r\n"a" (control_lo), "b" (input_address_hi),\r\n"c" (input_address_lo), "D"(output_address_hi),\r\n"S"(output_address_lo), "m" (hypercall_page));\r\nreturn hv_status_lo | ((u64)hv_status_hi << 32);\r\n#endif\r\n}\r\nstatic cycle_t read_hv_clock_tsc(struct clocksource *arg)\r\n{\r\ncycle_t current_tick;\r\nstruct ms_hyperv_tsc_page *tsc_pg = hv_context.tsc_page;\r\nif (tsc_pg->tsc_sequence != 0) {\r\nwhile (1) {\r\ncycle_t tmp;\r\nu32 sequence = tsc_pg->tsc_sequence;\r\nu64 cur_tsc;\r\nu64 scale = tsc_pg->tsc_scale;\r\ns64 offset = tsc_pg->tsc_offset;\r\nrdtscll(cur_tsc);\r\nasm("mulq %3"\r\n: "=d" (current_tick), "=a" (tmp)\r\n: "a" (cur_tsc), "r" (scale));\r\ncurrent_tick += offset;\r\nif (tsc_pg->tsc_sequence == sequence)\r\nreturn current_tick;\r\nif (tsc_pg->tsc_sequence != 0)\r\ncontinue;\r\nbreak;\r\n}\r\n}\r\nrdmsrl(HV_X64_MSR_TIME_REF_COUNT, current_tick);\r\nreturn current_tick;\r\n}\r\nint hv_init(void)\r\n{\r\nint max_leaf;\r\nunion hv_x64_msr_hypercall_contents hypercall_msr;\r\nvoid *virtaddr = NULL;\r\nmemset(hv_context.synic_event_page, 0, sizeof(void *) * NR_CPUS);\r\nmemset(hv_context.synic_message_page, 0,\r\nsizeof(void *) * NR_CPUS);\r\nmemset(hv_context.post_msg_page, 0,\r\nsizeof(void *) * NR_CPUS);\r\nmemset(hv_context.vp_index, 0,\r\nsizeof(int) * NR_CPUS);\r\nmemset(hv_context.event_dpc, 0,\r\nsizeof(void *) * NR_CPUS);\r\nmemset(hv_context.msg_dpc, 0,\r\nsizeof(void *) * NR_CPUS);\r\nmemset(hv_context.clk_evt, 0,\r\nsizeof(void *) * NR_CPUS);\r\nmax_leaf = query_hypervisor_info();\r\nhv_context.guestid = generate_guest_id(0, LINUX_VERSION_CODE, 0);\r\nwrmsrl(HV_X64_MSR_GUEST_OS_ID, hv_context.guestid);\r\nrdmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);\r\nvirtaddr = __vmalloc(PAGE_SIZE, GFP_KERNEL, PAGE_KERNEL_EXEC);\r\nif (!virtaddr)\r\ngoto cleanup;\r\nhypercall_msr.enable = 1;\r\nhypercall_msr.guest_physical_address = vmalloc_to_pfn(virtaddr);\r\nwrmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);\r\nhypercall_msr.as_uint64 = 0;\r\nrdmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);\r\nif (!hypercall_msr.enable)\r\ngoto cleanup;\r\nhv_context.hypercall_page = virtaddr;\r\n#ifdef CONFIG_X86_64\r\nif (ms_hyperv.features & HV_X64_MSR_REFERENCE_TSC_AVAILABLE) {\r\nunion hv_x64_msr_hypercall_contents tsc_msr;\r\nvoid *va_tsc;\r\nva_tsc = __vmalloc(PAGE_SIZE, GFP_KERNEL, PAGE_KERNEL);\r\nif (!va_tsc)\r\ngoto cleanup;\r\nhv_context.tsc_page = va_tsc;\r\nrdmsrl(HV_X64_MSR_REFERENCE_TSC, tsc_msr.as_uint64);\r\ntsc_msr.enable = 1;\r\ntsc_msr.guest_physical_address = vmalloc_to_pfn(va_tsc);\r\nwrmsrl(HV_X64_MSR_REFERENCE_TSC, tsc_msr.as_uint64);\r\nclocksource_register_hz(&hyperv_cs_tsc, NSEC_PER_SEC/100);\r\n}\r\n#endif\r\nreturn 0;\r\ncleanup:\r\nif (virtaddr) {\r\nif (hypercall_msr.enable) {\r\nhypercall_msr.as_uint64 = 0;\r\nwrmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);\r\n}\r\nvfree(virtaddr);\r\n}\r\nreturn -ENOTSUPP;\r\n}\r\nvoid hv_cleanup(void)\r\n{\r\nunion hv_x64_msr_hypercall_contents hypercall_msr;\r\nwrmsrl(HV_X64_MSR_GUEST_OS_ID, 0);\r\nif (hv_context.hypercall_page) {\r\nhypercall_msr.as_uint64 = 0;\r\nwrmsrl(HV_X64_MSR_HYPERCALL, hypercall_msr.as_uint64);\r\nvfree(hv_context.hypercall_page);\r\nhv_context.hypercall_page = NULL;\r\n}\r\n#ifdef CONFIG_X86_64\r\nif (ms_hyperv.features & HV_X64_MSR_REFERENCE_TSC_AVAILABLE) {\r\nif (!oops_in_progress) {\r\nclocksource_change_rating(&hyperv_cs_tsc, 10);\r\nclocksource_unregister(&hyperv_cs_tsc);\r\n}\r\nhypercall_msr.as_uint64 = 0;\r\nwrmsrl(HV_X64_MSR_REFERENCE_TSC, hypercall_msr.as_uint64);\r\nvfree(hv_context.tsc_page);\r\nhv_context.tsc_page = NULL;\r\n}\r\n#endif\r\n}\r\nint hv_post_message(union hv_connection_id connection_id,\r\nenum hv_message_type message_type,\r\nvoid *payload, size_t payload_size)\r\n{\r\nstruct hv_input_post_message *aligned_msg;\r\nu64 status;\r\nif (payload_size > HV_MESSAGE_PAYLOAD_BYTE_COUNT)\r\nreturn -EMSGSIZE;\r\naligned_msg = (struct hv_input_post_message *)\r\nhv_context.post_msg_page[get_cpu()];\r\naligned_msg->connectionid = connection_id;\r\naligned_msg->reserved = 0;\r\naligned_msg->message_type = message_type;\r\naligned_msg->payload_size = payload_size;\r\nmemcpy((void *)aligned_msg->payload, payload, payload_size);\r\nstatus = hv_do_hypercall(HVCALL_POST_MESSAGE, aligned_msg, NULL);\r\nput_cpu();\r\nreturn status & 0xFFFF;\r\n}\r\nstatic int hv_ce_set_next_event(unsigned long delta,\r\nstruct clock_event_device *evt)\r\n{\r\ncycle_t current_tick;\r\nWARN_ON(!clockevent_state_oneshot(evt));\r\nrdmsrl(HV_X64_MSR_TIME_REF_COUNT, current_tick);\r\ncurrent_tick += delta;\r\nwrmsrl(HV_X64_MSR_STIMER0_COUNT, current_tick);\r\nreturn 0;\r\n}\r\nstatic int hv_ce_shutdown(struct clock_event_device *evt)\r\n{\r\nwrmsrl(HV_X64_MSR_STIMER0_COUNT, 0);\r\nwrmsrl(HV_X64_MSR_STIMER0_CONFIG, 0);\r\nreturn 0;\r\n}\r\nstatic int hv_ce_set_oneshot(struct clock_event_device *evt)\r\n{\r\nunion hv_timer_config timer_cfg;\r\ntimer_cfg.enable = 1;\r\ntimer_cfg.auto_enable = 1;\r\ntimer_cfg.sintx = VMBUS_MESSAGE_SINT;\r\nwrmsrl(HV_X64_MSR_STIMER0_CONFIG, timer_cfg.as_uint64);\r\nreturn 0;\r\n}\r\nstatic void hv_init_clockevent_device(struct clock_event_device *dev, int cpu)\r\n{\r\ndev->name = "Hyper-V clockevent";\r\ndev->features = CLOCK_EVT_FEAT_ONESHOT;\r\ndev->cpumask = cpumask_of(cpu);\r\ndev->rating = 1000;\r\ndev->set_state_shutdown = hv_ce_shutdown;\r\ndev->set_state_oneshot = hv_ce_set_oneshot;\r\ndev->set_next_event = hv_ce_set_next_event;\r\n}\r\nint hv_synic_alloc(void)\r\n{\r\nsize_t size = sizeof(struct tasklet_struct);\r\nsize_t ced_size = sizeof(struct clock_event_device);\r\nint cpu;\r\nhv_context.hv_numa_map = kzalloc(sizeof(struct cpumask) * nr_node_ids,\r\nGFP_ATOMIC);\r\nif (hv_context.hv_numa_map == NULL) {\r\npr_err("Unable to allocate NUMA map\n");\r\ngoto err;\r\n}\r\nfor_each_online_cpu(cpu) {\r\nhv_context.event_dpc[cpu] = kmalloc(size, GFP_ATOMIC);\r\nif (hv_context.event_dpc[cpu] == NULL) {\r\npr_err("Unable to allocate event dpc\n");\r\ngoto err;\r\n}\r\ntasklet_init(hv_context.event_dpc[cpu], vmbus_on_event, cpu);\r\nhv_context.msg_dpc[cpu] = kmalloc(size, GFP_ATOMIC);\r\nif (hv_context.msg_dpc[cpu] == NULL) {\r\npr_err("Unable to allocate event dpc\n");\r\ngoto err;\r\n}\r\ntasklet_init(hv_context.msg_dpc[cpu], vmbus_on_msg_dpc, cpu);\r\nhv_context.clk_evt[cpu] = kzalloc(ced_size, GFP_ATOMIC);\r\nif (hv_context.clk_evt[cpu] == NULL) {\r\npr_err("Unable to allocate clock event device\n");\r\ngoto err;\r\n}\r\nhv_init_clockevent_device(hv_context.clk_evt[cpu], cpu);\r\nhv_context.synic_message_page[cpu] =\r\n(void *)get_zeroed_page(GFP_ATOMIC);\r\nif (hv_context.synic_message_page[cpu] == NULL) {\r\npr_err("Unable to allocate SYNIC message page\n");\r\ngoto err;\r\n}\r\nhv_context.synic_event_page[cpu] =\r\n(void *)get_zeroed_page(GFP_ATOMIC);\r\nif (hv_context.synic_event_page[cpu] == NULL) {\r\npr_err("Unable to allocate SYNIC event page\n");\r\ngoto err;\r\n}\r\nhv_context.post_msg_page[cpu] =\r\n(void *)get_zeroed_page(GFP_ATOMIC);\r\nif (hv_context.post_msg_page[cpu] == NULL) {\r\npr_err("Unable to allocate post msg page\n");\r\ngoto err;\r\n}\r\n}\r\nreturn 0;\r\nerr:\r\nreturn -ENOMEM;\r\n}\r\nstatic void hv_synic_free_cpu(int cpu)\r\n{\r\nkfree(hv_context.event_dpc[cpu]);\r\nkfree(hv_context.msg_dpc[cpu]);\r\nkfree(hv_context.clk_evt[cpu]);\r\nif (hv_context.synic_event_page[cpu])\r\nfree_page((unsigned long)hv_context.synic_event_page[cpu]);\r\nif (hv_context.synic_message_page[cpu])\r\nfree_page((unsigned long)hv_context.synic_message_page[cpu]);\r\nif (hv_context.post_msg_page[cpu])\r\nfree_page((unsigned long)hv_context.post_msg_page[cpu]);\r\n}\r\nvoid hv_synic_free(void)\r\n{\r\nint cpu;\r\nkfree(hv_context.hv_numa_map);\r\nfor_each_online_cpu(cpu)\r\nhv_synic_free_cpu(cpu);\r\n}\r\nvoid hv_synic_init(void *arg)\r\n{\r\nu64 version;\r\nunion hv_synic_simp simp;\r\nunion hv_synic_siefp siefp;\r\nunion hv_synic_sint shared_sint;\r\nunion hv_synic_scontrol sctrl;\r\nu64 vp_index;\r\nint cpu = smp_processor_id();\r\nif (!hv_context.hypercall_page)\r\nreturn;\r\nrdmsrl(HV_X64_MSR_SVERSION, version);\r\nrdmsrl(HV_X64_MSR_SIMP, simp.as_uint64);\r\nsimp.simp_enabled = 1;\r\nsimp.base_simp_gpa = virt_to_phys(hv_context.synic_message_page[cpu])\r\n>> PAGE_SHIFT;\r\nwrmsrl(HV_X64_MSR_SIMP, simp.as_uint64);\r\nrdmsrl(HV_X64_MSR_SIEFP, siefp.as_uint64);\r\nsiefp.siefp_enabled = 1;\r\nsiefp.base_siefp_gpa = virt_to_phys(hv_context.synic_event_page[cpu])\r\n>> PAGE_SHIFT;\r\nwrmsrl(HV_X64_MSR_SIEFP, siefp.as_uint64);\r\nrdmsrl(HV_X64_MSR_SINT0 + VMBUS_MESSAGE_SINT, shared_sint.as_uint64);\r\nshared_sint.as_uint64 = 0;\r\nshared_sint.vector = HYPERVISOR_CALLBACK_VECTOR;\r\nshared_sint.masked = false;\r\nshared_sint.auto_eoi = true;\r\nwrmsrl(HV_X64_MSR_SINT0 + VMBUS_MESSAGE_SINT, shared_sint.as_uint64);\r\nrdmsrl(HV_X64_MSR_SCONTROL, sctrl.as_uint64);\r\nsctrl.enable = 1;\r\nwrmsrl(HV_X64_MSR_SCONTROL, sctrl.as_uint64);\r\nhv_context.synic_initialized = true;\r\nrdmsrl(HV_X64_MSR_VP_INDEX, vp_index);\r\nhv_context.vp_index[cpu] = (u32)vp_index;\r\nINIT_LIST_HEAD(&hv_context.percpu_list[cpu]);\r\nif (ms_hyperv.features & HV_X64_MSR_SYNTIMER_AVAILABLE)\r\nclockevents_config_and_register(hv_context.clk_evt[cpu],\r\nHV_TIMER_FREQUENCY,\r\nHV_MIN_DELTA_TICKS,\r\nHV_MAX_MAX_DELTA_TICKS);\r\nreturn;\r\n}\r\nvoid hv_synic_clockevents_cleanup(void)\r\n{\r\nint cpu;\r\nif (!(ms_hyperv.features & HV_X64_MSR_SYNTIMER_AVAILABLE))\r\nreturn;\r\nfor_each_online_cpu(cpu)\r\nclockevents_unbind_device(hv_context.clk_evt[cpu], cpu);\r\n}\r\nvoid hv_synic_cleanup(void *arg)\r\n{\r\nunion hv_synic_sint shared_sint;\r\nunion hv_synic_simp simp;\r\nunion hv_synic_siefp siefp;\r\nunion hv_synic_scontrol sctrl;\r\nint cpu = smp_processor_id();\r\nif (!hv_context.synic_initialized)\r\nreturn;\r\nif (ms_hyperv.features & HV_X64_MSR_SYNTIMER_AVAILABLE)\r\nhv_ce_shutdown(hv_context.clk_evt[cpu]);\r\nrdmsrl(HV_X64_MSR_SINT0 + VMBUS_MESSAGE_SINT, shared_sint.as_uint64);\r\nshared_sint.masked = 1;\r\nwrmsrl(HV_X64_MSR_SINT0 + VMBUS_MESSAGE_SINT, shared_sint.as_uint64);\r\nrdmsrl(HV_X64_MSR_SIMP, simp.as_uint64);\r\nsimp.simp_enabled = 0;\r\nsimp.base_simp_gpa = 0;\r\nwrmsrl(HV_X64_MSR_SIMP, simp.as_uint64);\r\nrdmsrl(HV_X64_MSR_SIEFP, siefp.as_uint64);\r\nsiefp.siefp_enabled = 0;\r\nsiefp.base_siefp_gpa = 0;\r\nwrmsrl(HV_X64_MSR_SIEFP, siefp.as_uint64);\r\nrdmsrl(HV_X64_MSR_SCONTROL, sctrl.as_uint64);\r\nsctrl.enable = 0;\r\nwrmsrl(HV_X64_MSR_SCONTROL, sctrl.as_uint64);\r\n}
