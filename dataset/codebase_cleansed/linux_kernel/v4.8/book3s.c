void kvmppc_unfixup_split_real(struct kvm_vcpu *vcpu)\r\n{\r\nif (vcpu->arch.hflags & BOOK3S_HFLAG_SPLIT_HACK) {\r\nulong pc = kvmppc_get_pc(vcpu);\r\nif ((pc & SPLIT_HACK_MASK) == SPLIT_HACK_OFFS)\r\nkvmppc_set_pc(vcpu, pc & ~SPLIT_HACK_MASK);\r\nvcpu->arch.hflags &= ~BOOK3S_HFLAG_SPLIT_HACK;\r\n}\r\n}\r\nstatic inline unsigned long kvmppc_interrupt_offset(struct kvm_vcpu *vcpu)\r\n{\r\nif (!is_kvmppc_hv_enabled(vcpu->kvm))\r\nreturn to_book3s(vcpu)->hior;\r\nreturn 0;\r\n}\r\nstatic inline void kvmppc_update_int_pending(struct kvm_vcpu *vcpu,\r\nunsigned long pending_now, unsigned long old_pending)\r\n{\r\nif (is_kvmppc_hv_enabled(vcpu->kvm))\r\nreturn;\r\nif (pending_now)\r\nkvmppc_set_int_pending(vcpu, 1);\r\nelse if (old_pending)\r\nkvmppc_set_int_pending(vcpu, 0);\r\n}\r\nstatic inline bool kvmppc_critical_section(struct kvm_vcpu *vcpu)\r\n{\r\nulong crit_raw;\r\nulong crit_r1;\r\nbool crit;\r\nif (is_kvmppc_hv_enabled(vcpu->kvm))\r\nreturn false;\r\ncrit_raw = kvmppc_get_critical(vcpu);\r\ncrit_r1 = kvmppc_get_gpr(vcpu, 1);\r\nif (!(kvmppc_get_msr(vcpu) & MSR_SF)) {\r\ncrit_raw &= 0xffffffff;\r\ncrit_r1 &= 0xffffffff;\r\n}\r\ncrit = (crit_raw == crit_r1);\r\ncrit = crit && !(kvmppc_get_msr(vcpu) & MSR_PR);\r\nreturn crit;\r\n}\r\nvoid kvmppc_inject_interrupt(struct kvm_vcpu *vcpu, int vec, u64 flags)\r\n{\r\nkvmppc_unfixup_split_real(vcpu);\r\nkvmppc_set_srr0(vcpu, kvmppc_get_pc(vcpu));\r\nkvmppc_set_srr1(vcpu, kvmppc_get_msr(vcpu) | flags);\r\nkvmppc_set_pc(vcpu, kvmppc_interrupt_offset(vcpu) + vec);\r\nvcpu->arch.mmu.reset_msr(vcpu);\r\n}\r\nstatic int kvmppc_book3s_vec2irqprio(unsigned int vec)\r\n{\r\nunsigned int prio;\r\nswitch (vec) {\r\ncase 0x100: prio = BOOK3S_IRQPRIO_SYSTEM_RESET; break;\r\ncase 0x200: prio = BOOK3S_IRQPRIO_MACHINE_CHECK; break;\r\ncase 0x300: prio = BOOK3S_IRQPRIO_DATA_STORAGE; break;\r\ncase 0x380: prio = BOOK3S_IRQPRIO_DATA_SEGMENT; break;\r\ncase 0x400: prio = BOOK3S_IRQPRIO_INST_STORAGE; break;\r\ncase 0x480: prio = BOOK3S_IRQPRIO_INST_SEGMENT; break;\r\ncase 0x500: prio = BOOK3S_IRQPRIO_EXTERNAL; break;\r\ncase 0x501: prio = BOOK3S_IRQPRIO_EXTERNAL_LEVEL; break;\r\ncase 0x600: prio = BOOK3S_IRQPRIO_ALIGNMENT; break;\r\ncase 0x700: prio = BOOK3S_IRQPRIO_PROGRAM; break;\r\ncase 0x800: prio = BOOK3S_IRQPRIO_FP_UNAVAIL; break;\r\ncase 0x900: prio = BOOK3S_IRQPRIO_DECREMENTER; break;\r\ncase 0xc00: prio = BOOK3S_IRQPRIO_SYSCALL; break;\r\ncase 0xd00: prio = BOOK3S_IRQPRIO_DEBUG; break;\r\ncase 0xf20: prio = BOOK3S_IRQPRIO_ALTIVEC; break;\r\ncase 0xf40: prio = BOOK3S_IRQPRIO_VSX; break;\r\ncase 0xf60: prio = BOOK3S_IRQPRIO_FAC_UNAVAIL; break;\r\ndefault: prio = BOOK3S_IRQPRIO_MAX; break;\r\n}\r\nreturn prio;\r\n}\r\nvoid kvmppc_book3s_dequeue_irqprio(struct kvm_vcpu *vcpu,\r\nunsigned int vec)\r\n{\r\nunsigned long old_pending = vcpu->arch.pending_exceptions;\r\nclear_bit(kvmppc_book3s_vec2irqprio(vec),\r\n&vcpu->arch.pending_exceptions);\r\nkvmppc_update_int_pending(vcpu, vcpu->arch.pending_exceptions,\r\nold_pending);\r\n}\r\nvoid kvmppc_book3s_queue_irqprio(struct kvm_vcpu *vcpu, unsigned int vec)\r\n{\r\nvcpu->stat.queue_intr++;\r\nset_bit(kvmppc_book3s_vec2irqprio(vec),\r\n&vcpu->arch.pending_exceptions);\r\n#ifdef EXIT_DEBUG\r\nprintk(KERN_INFO "Queueing interrupt %x\n", vec);\r\n#endif\r\n}\r\nvoid kvmppc_core_queue_program(struct kvm_vcpu *vcpu, ulong flags)\r\n{\r\nkvmppc_inject_interrupt(vcpu, BOOK3S_INTERRUPT_PROGRAM, flags);\r\n}\r\nvoid kvmppc_core_queue_dec(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_book3s_queue_irqprio(vcpu, BOOK3S_INTERRUPT_DECREMENTER);\r\n}\r\nint kvmppc_core_pending_dec(struct kvm_vcpu *vcpu)\r\n{\r\nreturn test_bit(BOOK3S_IRQPRIO_DECREMENTER, &vcpu->arch.pending_exceptions);\r\n}\r\nvoid kvmppc_core_dequeue_dec(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_book3s_dequeue_irqprio(vcpu, BOOK3S_INTERRUPT_DECREMENTER);\r\n}\r\nvoid kvmppc_core_queue_external(struct kvm_vcpu *vcpu,\r\nstruct kvm_interrupt *irq)\r\n{\r\nunsigned int vec = BOOK3S_INTERRUPT_EXTERNAL;\r\nif (irq->irq == KVM_INTERRUPT_SET_LEVEL)\r\nvec = BOOK3S_INTERRUPT_EXTERNAL_LEVEL;\r\nkvmppc_book3s_queue_irqprio(vcpu, vec);\r\n}\r\nvoid kvmppc_core_dequeue_external(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_book3s_dequeue_irqprio(vcpu, BOOK3S_INTERRUPT_EXTERNAL);\r\nkvmppc_book3s_dequeue_irqprio(vcpu, BOOK3S_INTERRUPT_EXTERNAL_LEVEL);\r\n}\r\nvoid kvmppc_core_queue_data_storage(struct kvm_vcpu *vcpu, ulong dar,\r\nulong flags)\r\n{\r\nkvmppc_set_dar(vcpu, dar);\r\nkvmppc_set_dsisr(vcpu, flags);\r\nkvmppc_book3s_queue_irqprio(vcpu, BOOK3S_INTERRUPT_DATA_STORAGE);\r\n}\r\nvoid kvmppc_core_queue_inst_storage(struct kvm_vcpu *vcpu, ulong flags)\r\n{\r\nu64 msr = kvmppc_get_msr(vcpu);\r\nmsr &= ~(SRR1_ISI_NOPT | SRR1_ISI_N_OR_G | SRR1_ISI_PROT);\r\nmsr |= flags & (SRR1_ISI_NOPT | SRR1_ISI_N_OR_G | SRR1_ISI_PROT);\r\nkvmppc_set_msr_fast(vcpu, msr);\r\nkvmppc_book3s_queue_irqprio(vcpu, BOOK3S_INTERRUPT_INST_STORAGE);\r\n}\r\nstatic int kvmppc_book3s_irqprio_deliver(struct kvm_vcpu *vcpu,\r\nunsigned int priority)\r\n{\r\nint deliver = 1;\r\nint vec = 0;\r\nbool crit = kvmppc_critical_section(vcpu);\r\nswitch (priority) {\r\ncase BOOK3S_IRQPRIO_DECREMENTER:\r\ndeliver = (kvmppc_get_msr(vcpu) & MSR_EE) && !crit;\r\nvec = BOOK3S_INTERRUPT_DECREMENTER;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_EXTERNAL:\r\ncase BOOK3S_IRQPRIO_EXTERNAL_LEVEL:\r\ndeliver = (kvmppc_get_msr(vcpu) & MSR_EE) && !crit;\r\nvec = BOOK3S_INTERRUPT_EXTERNAL;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_SYSTEM_RESET:\r\nvec = BOOK3S_INTERRUPT_SYSTEM_RESET;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_MACHINE_CHECK:\r\nvec = BOOK3S_INTERRUPT_MACHINE_CHECK;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_DATA_STORAGE:\r\nvec = BOOK3S_INTERRUPT_DATA_STORAGE;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_INST_STORAGE:\r\nvec = BOOK3S_INTERRUPT_INST_STORAGE;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_DATA_SEGMENT:\r\nvec = BOOK3S_INTERRUPT_DATA_SEGMENT;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_INST_SEGMENT:\r\nvec = BOOK3S_INTERRUPT_INST_SEGMENT;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_ALIGNMENT:\r\nvec = BOOK3S_INTERRUPT_ALIGNMENT;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_PROGRAM:\r\nvec = BOOK3S_INTERRUPT_PROGRAM;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_VSX:\r\nvec = BOOK3S_INTERRUPT_VSX;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_ALTIVEC:\r\nvec = BOOK3S_INTERRUPT_ALTIVEC;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_FP_UNAVAIL:\r\nvec = BOOK3S_INTERRUPT_FP_UNAVAIL;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_SYSCALL:\r\nvec = BOOK3S_INTERRUPT_SYSCALL;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_DEBUG:\r\nvec = BOOK3S_INTERRUPT_TRACE;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_PERFORMANCE_MONITOR:\r\nvec = BOOK3S_INTERRUPT_PERFMON;\r\nbreak;\r\ncase BOOK3S_IRQPRIO_FAC_UNAVAIL:\r\nvec = BOOK3S_INTERRUPT_FAC_UNAVAIL;\r\nbreak;\r\ndefault:\r\ndeliver = 0;\r\nprintk(KERN_ERR "KVM: Unknown interrupt: 0x%x\n", priority);\r\nbreak;\r\n}\r\n#if 0\r\nprintk(KERN_INFO "Deliver interrupt 0x%x? %x\n", vec, deliver);\r\n#endif\r\nif (deliver)\r\nkvmppc_inject_interrupt(vcpu, vec, 0);\r\nreturn deliver;\r\n}\r\nstatic bool clear_irqprio(struct kvm_vcpu *vcpu, unsigned int priority)\r\n{\r\nswitch (priority) {\r\ncase BOOK3S_IRQPRIO_DECREMENTER:\r\nreturn false;\r\ncase BOOK3S_IRQPRIO_EXTERNAL_LEVEL:\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nint kvmppc_core_prepare_to_enter(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long *pending = &vcpu->arch.pending_exceptions;\r\nunsigned long old_pending = vcpu->arch.pending_exceptions;\r\nunsigned int priority;\r\n#ifdef EXIT_DEBUG\r\nif (vcpu->arch.pending_exceptions)\r\nprintk(KERN_EMERG "KVM: Check pending: %lx\n", vcpu->arch.pending_exceptions);\r\n#endif\r\npriority = __ffs(*pending);\r\nwhile (priority < BOOK3S_IRQPRIO_MAX) {\r\nif (kvmppc_book3s_irqprio_deliver(vcpu, priority) &&\r\nclear_irqprio(vcpu, priority)) {\r\nclear_bit(priority, &vcpu->arch.pending_exceptions);\r\nbreak;\r\n}\r\npriority = find_next_bit(pending,\r\nBITS_PER_BYTE * sizeof(*pending),\r\npriority + 1);\r\n}\r\nkvmppc_update_int_pending(vcpu, *pending, old_pending);\r\nreturn 0;\r\n}\r\nkvm_pfn_t kvmppc_gpa_to_pfn(struct kvm_vcpu *vcpu, gpa_t gpa, bool writing,\r\nbool *writable)\r\n{\r\nulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM;\r\ngfn_t gfn = gpa >> PAGE_SHIFT;\r\nif (!(kvmppc_get_msr(vcpu) & MSR_SF))\r\nmp_pa = (uint32_t)mp_pa;\r\ngpa &= ~0xFFFULL;\r\nif (unlikely(mp_pa) && unlikely((gpa & KVM_PAM) == mp_pa)) {\r\nulong shared_page = ((ulong)vcpu->arch.shared) & PAGE_MASK;\r\nkvm_pfn_t pfn;\r\npfn = (kvm_pfn_t)virt_to_phys((void*)shared_page) >> PAGE_SHIFT;\r\nget_page(pfn_to_page(pfn));\r\nif (writable)\r\n*writable = true;\r\nreturn pfn;\r\n}\r\nreturn gfn_to_pfn_prot(vcpu->kvm, gfn, writing, writable);\r\n}\r\nint kvmppc_xlate(struct kvm_vcpu *vcpu, ulong eaddr, enum xlate_instdata xlid,\r\nenum xlate_readwrite xlrw, struct kvmppc_pte *pte)\r\n{\r\nbool data = (xlid == XLATE_DATA);\r\nbool iswrite = (xlrw == XLATE_WRITE);\r\nint relocated = (kvmppc_get_msr(vcpu) & (data ? MSR_DR : MSR_IR));\r\nint r;\r\nif (relocated) {\r\nr = vcpu->arch.mmu.xlate(vcpu, eaddr, pte, data, iswrite);\r\n} else {\r\npte->eaddr = eaddr;\r\npte->raddr = eaddr & KVM_PAM;\r\npte->vpage = VSID_REAL | eaddr >> 12;\r\npte->may_read = true;\r\npte->may_write = true;\r\npte->may_execute = true;\r\nr = 0;\r\nif ((kvmppc_get_msr(vcpu) & (MSR_IR | MSR_DR)) == MSR_DR &&\r\n!data) {\r\nif ((vcpu->arch.hflags & BOOK3S_HFLAG_SPLIT_HACK) &&\r\n((eaddr & SPLIT_HACK_MASK) == SPLIT_HACK_OFFS))\r\npte->raddr &= ~SPLIT_HACK_MASK;\r\n}\r\n}\r\nreturn r;\r\n}\r\nint kvmppc_load_last_inst(struct kvm_vcpu *vcpu, enum instruction_type type,\r\nu32 *inst)\r\n{\r\nulong pc = kvmppc_get_pc(vcpu);\r\nint r;\r\nif (type == INST_SC)\r\npc -= 4;\r\nr = kvmppc_ld(vcpu, &pc, sizeof(u32), inst, false);\r\nif (r == EMULATE_DONE)\r\nreturn r;\r\nelse\r\nreturn EMULATE_AGAIN;\r\n}\r\nint kvm_arch_vcpu_setup(struct kvm_vcpu *vcpu)\r\n{\r\nreturn 0;\r\n}\r\nint kvmppc_subarch_vcpu_init(struct kvm_vcpu *vcpu)\r\n{\r\nreturn 0;\r\n}\r\nvoid kvmppc_subarch_vcpu_uninit(struct kvm_vcpu *vcpu)\r\n{\r\n}\r\nint kvm_arch_vcpu_ioctl_get_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nreturn vcpu->kvm->arch.kvm_ops->get_sregs(vcpu, sregs);\r\n}\r\nint kvm_arch_vcpu_ioctl_set_sregs(struct kvm_vcpu *vcpu,\r\nstruct kvm_sregs *sregs)\r\n{\r\nreturn vcpu->kvm->arch.kvm_ops->set_sregs(vcpu, sregs);\r\n}\r\nint kvm_arch_vcpu_ioctl_get_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\r\n{\r\nint i;\r\nregs->pc = kvmppc_get_pc(vcpu);\r\nregs->cr = kvmppc_get_cr(vcpu);\r\nregs->ctr = kvmppc_get_ctr(vcpu);\r\nregs->lr = kvmppc_get_lr(vcpu);\r\nregs->xer = kvmppc_get_xer(vcpu);\r\nregs->msr = kvmppc_get_msr(vcpu);\r\nregs->srr0 = kvmppc_get_srr0(vcpu);\r\nregs->srr1 = kvmppc_get_srr1(vcpu);\r\nregs->pid = vcpu->arch.pid;\r\nregs->sprg0 = kvmppc_get_sprg0(vcpu);\r\nregs->sprg1 = kvmppc_get_sprg1(vcpu);\r\nregs->sprg2 = kvmppc_get_sprg2(vcpu);\r\nregs->sprg3 = kvmppc_get_sprg3(vcpu);\r\nregs->sprg4 = kvmppc_get_sprg4(vcpu);\r\nregs->sprg5 = kvmppc_get_sprg5(vcpu);\r\nregs->sprg6 = kvmppc_get_sprg6(vcpu);\r\nregs->sprg7 = kvmppc_get_sprg7(vcpu);\r\nfor (i = 0; i < ARRAY_SIZE(regs->gpr); i++)\r\nregs->gpr[i] = kvmppc_get_gpr(vcpu, i);\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_regs(struct kvm_vcpu *vcpu, struct kvm_regs *regs)\r\n{\r\nint i;\r\nkvmppc_set_pc(vcpu, regs->pc);\r\nkvmppc_set_cr(vcpu, regs->cr);\r\nkvmppc_set_ctr(vcpu, regs->ctr);\r\nkvmppc_set_lr(vcpu, regs->lr);\r\nkvmppc_set_xer(vcpu, regs->xer);\r\nkvmppc_set_msr(vcpu, regs->msr);\r\nkvmppc_set_srr0(vcpu, regs->srr0);\r\nkvmppc_set_srr1(vcpu, regs->srr1);\r\nkvmppc_set_sprg0(vcpu, regs->sprg0);\r\nkvmppc_set_sprg1(vcpu, regs->sprg1);\r\nkvmppc_set_sprg2(vcpu, regs->sprg2);\r\nkvmppc_set_sprg3(vcpu, regs->sprg3);\r\nkvmppc_set_sprg4(vcpu, regs->sprg4);\r\nkvmppc_set_sprg5(vcpu, regs->sprg5);\r\nkvmppc_set_sprg6(vcpu, regs->sprg6);\r\nkvmppc_set_sprg7(vcpu, regs->sprg7);\r\nfor (i = 0; i < ARRAY_SIZE(regs->gpr); i++)\r\nkvmppc_set_gpr(vcpu, i, regs->gpr[i]);\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_get_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\r\n{\r\nreturn -ENOTSUPP;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_fpu(struct kvm_vcpu *vcpu, struct kvm_fpu *fpu)\r\n{\r\nreturn -ENOTSUPP;\r\n}\r\nint kvmppc_get_one_reg(struct kvm_vcpu *vcpu, u64 id,\r\nunion kvmppc_one_reg *val)\r\n{\r\nint r = 0;\r\nlong int i;\r\nr = vcpu->kvm->arch.kvm_ops->get_one_reg(vcpu, id, val);\r\nif (r == -EINVAL) {\r\nr = 0;\r\nswitch (id) {\r\ncase KVM_REG_PPC_DAR:\r\n*val = get_reg_val(id, kvmppc_get_dar(vcpu));\r\nbreak;\r\ncase KVM_REG_PPC_DSISR:\r\n*val = get_reg_val(id, kvmppc_get_dsisr(vcpu));\r\nbreak;\r\ncase KVM_REG_PPC_FPR0 ... KVM_REG_PPC_FPR31:\r\ni = id - KVM_REG_PPC_FPR0;\r\n*val = get_reg_val(id, VCPU_FPR(vcpu, i));\r\nbreak;\r\ncase KVM_REG_PPC_FPSCR:\r\n*val = get_reg_val(id, vcpu->arch.fp.fpscr);\r\nbreak;\r\n#ifdef CONFIG_VSX\r\ncase KVM_REG_PPC_VSR0 ... KVM_REG_PPC_VSR31:\r\nif (cpu_has_feature(CPU_FTR_VSX)) {\r\ni = id - KVM_REG_PPC_VSR0;\r\nval->vsxval[0] = vcpu->arch.fp.fpr[i][0];\r\nval->vsxval[1] = vcpu->arch.fp.fpr[i][1];\r\n} else {\r\nr = -ENXIO;\r\n}\r\nbreak;\r\n#endif\r\ncase KVM_REG_PPC_DEBUG_INST:\r\n*val = get_reg_val(id, INS_TW);\r\nbreak;\r\n#ifdef CONFIG_KVM_XICS\r\ncase KVM_REG_PPC_ICP_STATE:\r\nif (!vcpu->arch.icp) {\r\nr = -ENXIO;\r\nbreak;\r\n}\r\n*val = get_reg_val(id, kvmppc_xics_get_icp(vcpu));\r\nbreak;\r\n#endif\r\ncase KVM_REG_PPC_FSCR:\r\n*val = get_reg_val(id, vcpu->arch.fscr);\r\nbreak;\r\ncase KVM_REG_PPC_TAR:\r\n*val = get_reg_val(id, vcpu->arch.tar);\r\nbreak;\r\ncase KVM_REG_PPC_EBBHR:\r\n*val = get_reg_val(id, vcpu->arch.ebbhr);\r\nbreak;\r\ncase KVM_REG_PPC_EBBRR:\r\n*val = get_reg_val(id, vcpu->arch.ebbrr);\r\nbreak;\r\ncase KVM_REG_PPC_BESCR:\r\n*val = get_reg_val(id, vcpu->arch.bescr);\r\nbreak;\r\ncase KVM_REG_PPC_VTB:\r\n*val = get_reg_val(id, vcpu->arch.vtb);\r\nbreak;\r\ncase KVM_REG_PPC_IC:\r\n*val = get_reg_val(id, vcpu->arch.ic);\r\nbreak;\r\ndefault:\r\nr = -EINVAL;\r\nbreak;\r\n}\r\n}\r\nreturn r;\r\n}\r\nint kvmppc_set_one_reg(struct kvm_vcpu *vcpu, u64 id,\r\nunion kvmppc_one_reg *val)\r\n{\r\nint r = 0;\r\nlong int i;\r\nr = vcpu->kvm->arch.kvm_ops->set_one_reg(vcpu, id, val);\r\nif (r == -EINVAL) {\r\nr = 0;\r\nswitch (id) {\r\ncase KVM_REG_PPC_DAR:\r\nkvmppc_set_dar(vcpu, set_reg_val(id, *val));\r\nbreak;\r\ncase KVM_REG_PPC_DSISR:\r\nkvmppc_set_dsisr(vcpu, set_reg_val(id, *val));\r\nbreak;\r\ncase KVM_REG_PPC_FPR0 ... KVM_REG_PPC_FPR31:\r\ni = id - KVM_REG_PPC_FPR0;\r\nVCPU_FPR(vcpu, i) = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_FPSCR:\r\nvcpu->arch.fp.fpscr = set_reg_val(id, *val);\r\nbreak;\r\n#ifdef CONFIG_VSX\r\ncase KVM_REG_PPC_VSR0 ... KVM_REG_PPC_VSR31:\r\nif (cpu_has_feature(CPU_FTR_VSX)) {\r\ni = id - KVM_REG_PPC_VSR0;\r\nvcpu->arch.fp.fpr[i][0] = val->vsxval[0];\r\nvcpu->arch.fp.fpr[i][1] = val->vsxval[1];\r\n} else {\r\nr = -ENXIO;\r\n}\r\nbreak;\r\n#endif\r\n#ifdef CONFIG_KVM_XICS\r\ncase KVM_REG_PPC_ICP_STATE:\r\nif (!vcpu->arch.icp) {\r\nr = -ENXIO;\r\nbreak;\r\n}\r\nr = kvmppc_xics_set_icp(vcpu,\r\nset_reg_val(id, *val));\r\nbreak;\r\n#endif\r\ncase KVM_REG_PPC_FSCR:\r\nvcpu->arch.fscr = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_TAR:\r\nvcpu->arch.tar = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_EBBHR:\r\nvcpu->arch.ebbhr = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_EBBRR:\r\nvcpu->arch.ebbrr = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_BESCR:\r\nvcpu->arch.bescr = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_VTB:\r\nvcpu->arch.vtb = set_reg_val(id, *val);\r\nbreak;\r\ncase KVM_REG_PPC_IC:\r\nvcpu->arch.ic = set_reg_val(id, *val);\r\nbreak;\r\ndefault:\r\nr = -EINVAL;\r\nbreak;\r\n}\r\n}\r\nreturn r;\r\n}\r\nvoid kvmppc_core_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\r\n{\r\nvcpu->kvm->arch.kvm_ops->vcpu_load(vcpu, cpu);\r\n}\r\nvoid kvmppc_core_vcpu_put(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->kvm->arch.kvm_ops->vcpu_put(vcpu);\r\n}\r\nvoid kvmppc_set_msr(struct kvm_vcpu *vcpu, u64 msr)\r\n{\r\nvcpu->kvm->arch.kvm_ops->set_msr(vcpu, msr);\r\n}\r\nint kvmppc_vcpu_run(struct kvm_run *kvm_run, struct kvm_vcpu *vcpu)\r\n{\r\nreturn vcpu->kvm->arch.kvm_ops->vcpu_run(kvm_run, vcpu);\r\n}\r\nint kvm_arch_vcpu_ioctl_translate(struct kvm_vcpu *vcpu,\r\nstruct kvm_translation *tr)\r\n{\r\nreturn 0;\r\n}\r\nint kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,\r\nstruct kvm_guest_debug *dbg)\r\n{\r\nvcpu->guest_debug = dbg->control;\r\nreturn 0;\r\n}\r\nvoid kvmppc_decrementer_func(struct kvm_vcpu *vcpu)\r\n{\r\nkvmppc_core_queue_dec(vcpu);\r\nkvm_vcpu_kick(vcpu);\r\n}\r\nstruct kvm_vcpu *kvmppc_core_vcpu_create(struct kvm *kvm, unsigned int id)\r\n{\r\nreturn kvm->arch.kvm_ops->vcpu_create(kvm, id);\r\n}\r\nvoid kvmppc_core_vcpu_free(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->kvm->arch.kvm_ops->vcpu_free(vcpu);\r\n}\r\nint kvmppc_core_check_requests(struct kvm_vcpu *vcpu)\r\n{\r\nreturn vcpu->kvm->arch.kvm_ops->check_requests(vcpu);\r\n}\r\nint kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)\r\n{\r\nreturn kvm->arch.kvm_ops->get_dirty_log(kvm, log);\r\n}\r\nvoid kvmppc_core_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,\r\nstruct kvm_memory_slot *dont)\r\n{\r\nkvm->arch.kvm_ops->free_memslot(free, dont);\r\n}\r\nint kvmppc_core_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,\r\nunsigned long npages)\r\n{\r\nreturn kvm->arch.kvm_ops->create_memslot(slot, npages);\r\n}\r\nvoid kvmppc_core_flush_memslot(struct kvm *kvm, struct kvm_memory_slot *memslot)\r\n{\r\nkvm->arch.kvm_ops->flush_memslot(kvm, memslot);\r\n}\r\nint kvmppc_core_prepare_memory_region(struct kvm *kvm,\r\nstruct kvm_memory_slot *memslot,\r\nconst struct kvm_userspace_memory_region *mem)\r\n{\r\nreturn kvm->arch.kvm_ops->prepare_memory_region(kvm, memslot, mem);\r\n}\r\nvoid kvmppc_core_commit_memory_region(struct kvm *kvm,\r\nconst struct kvm_userspace_memory_region *mem,\r\nconst struct kvm_memory_slot *old,\r\nconst struct kvm_memory_slot *new)\r\n{\r\nkvm->arch.kvm_ops->commit_memory_region(kvm, mem, old, new);\r\n}\r\nint kvm_unmap_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nreturn kvm->arch.kvm_ops->unmap_hva(kvm, hva);\r\n}\r\nint kvm_unmap_hva_range(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nreturn kvm->arch.kvm_ops->unmap_hva_range(kvm, start, end);\r\n}\r\nint kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end)\r\n{\r\nreturn kvm->arch.kvm_ops->age_hva(kvm, start, end);\r\n}\r\nint kvm_test_age_hva(struct kvm *kvm, unsigned long hva)\r\n{\r\nreturn kvm->arch.kvm_ops->test_age_hva(kvm, hva);\r\n}\r\nvoid kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)\r\n{\r\nkvm->arch.kvm_ops->set_spte_hva(kvm, hva, pte);\r\n}\r\nvoid kvmppc_mmu_destroy(struct kvm_vcpu *vcpu)\r\n{\r\nvcpu->kvm->arch.kvm_ops->mmu_destroy(vcpu);\r\n}\r\nint kvmppc_core_init_vm(struct kvm *kvm)\r\n{\r\n#ifdef CONFIG_PPC64\r\nINIT_LIST_HEAD_RCU(&kvm->arch.spapr_tce_tables);\r\nINIT_LIST_HEAD(&kvm->arch.rtas_tokens);\r\n#endif\r\nreturn kvm->arch.kvm_ops->init_vm(kvm);\r\n}\r\nvoid kvmppc_core_destroy_vm(struct kvm *kvm)\r\n{\r\nkvm->arch.kvm_ops->destroy_vm(kvm);\r\n#ifdef CONFIG_PPC64\r\nkvmppc_rtas_tokens_free(kvm);\r\nWARN_ON(!list_empty(&kvm->arch.spapr_tce_tables));\r\n#endif\r\n}\r\nint kvmppc_h_logical_ci_load(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long size = kvmppc_get_gpr(vcpu, 4);\r\nunsigned long addr = kvmppc_get_gpr(vcpu, 5);\r\nu64 buf;\r\nint srcu_idx;\r\nint ret;\r\nif (!is_power_of_2(size) || (size > sizeof(buf)))\r\nreturn H_TOO_HARD;\r\nsrcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\r\nret = kvm_io_bus_read(vcpu, KVM_MMIO_BUS, addr, size, &buf);\r\nsrcu_read_unlock(&vcpu->kvm->srcu, srcu_idx);\r\nif (ret != 0)\r\nreturn H_TOO_HARD;\r\nswitch (size) {\r\ncase 1:\r\nkvmppc_set_gpr(vcpu, 4, *(u8 *)&buf);\r\nbreak;\r\ncase 2:\r\nkvmppc_set_gpr(vcpu, 4, be16_to_cpu(*(__be16 *)&buf));\r\nbreak;\r\ncase 4:\r\nkvmppc_set_gpr(vcpu, 4, be32_to_cpu(*(__be32 *)&buf));\r\nbreak;\r\ncase 8:\r\nkvmppc_set_gpr(vcpu, 4, be64_to_cpu(*(__be64 *)&buf));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn H_SUCCESS;\r\n}\r\nint kvmppc_h_logical_ci_store(struct kvm_vcpu *vcpu)\r\n{\r\nunsigned long size = kvmppc_get_gpr(vcpu, 4);\r\nunsigned long addr = kvmppc_get_gpr(vcpu, 5);\r\nunsigned long val = kvmppc_get_gpr(vcpu, 6);\r\nu64 buf;\r\nint srcu_idx;\r\nint ret;\r\nswitch (size) {\r\ncase 1:\r\n*(u8 *)&buf = val;\r\nbreak;\r\ncase 2:\r\n*(__be16 *)&buf = cpu_to_be16(val);\r\nbreak;\r\ncase 4:\r\n*(__be32 *)&buf = cpu_to_be32(val);\r\nbreak;\r\ncase 8:\r\n*(__be64 *)&buf = cpu_to_be64(val);\r\nbreak;\r\ndefault:\r\nreturn H_TOO_HARD;\r\n}\r\nsrcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\r\nret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, addr, size, &buf);\r\nsrcu_read_unlock(&vcpu->kvm->srcu, srcu_idx);\r\nif (ret != 0)\r\nreturn H_TOO_HARD;\r\nreturn H_SUCCESS;\r\n}\r\nint kvmppc_core_check_processor_compat(void)\r\n{\r\nreturn 0;\r\n}\r\nint kvmppc_book3s_hcall_implemented(struct kvm *kvm, unsigned long hcall)\r\n{\r\nreturn kvm->arch.kvm_ops->hcall_implemented(hcall);\r\n}\r\nstatic int kvmppc_book3s_init(void)\r\n{\r\nint r;\r\nr = kvm_init(NULL, sizeof(struct kvm_vcpu), 0, THIS_MODULE);\r\nif (r)\r\nreturn r;\r\n#ifdef CONFIG_KVM_BOOK3S_32_HANDLER\r\nr = kvmppc_book3s_init_pr();\r\n#endif\r\nreturn r;\r\n}\r\nstatic void kvmppc_book3s_exit(void)\r\n{\r\n#ifdef CONFIG_KVM_BOOK3S_32_HANDLER\r\nkvmppc_book3s_exit_pr();\r\n#endif\r\nkvm_exit();\r\n}
