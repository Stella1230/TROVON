static struct svc_xprt *svc_rdma_bc_create(struct svc_serv *serv,\r\nstruct net *net,\r\nstruct sockaddr *sa, int salen,\r\nint flags)\r\n{\r\nstruct svcxprt_rdma *cma_xprt;\r\nstruct svc_xprt *xprt;\r\ncma_xprt = rdma_create_xprt(serv, 0);\r\nif (!cma_xprt)\r\nreturn ERR_PTR(-ENOMEM);\r\nxprt = &cma_xprt->sc_xprt;\r\nsvc_xprt_init(net, &svc_rdma_bc_class, xprt, serv);\r\nserv->sv_bc_xprt = xprt;\r\ndprintk("svcrdma: %s(%p)\n", __func__, xprt);\r\nreturn xprt;\r\n}\r\nstatic void svc_rdma_bc_detach(struct svc_xprt *xprt)\r\n{\r\ndprintk("svcrdma: %s(%p)\n", __func__, xprt);\r\n}\r\nstatic void svc_rdma_bc_free(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\ndprintk("svcrdma: %s(%p)\n", __func__, xprt);\r\nif (xprt)\r\nkfree(rdma);\r\n}\r\nstatic struct svc_rdma_op_ctxt *alloc_ctxt(struct svcxprt_rdma *xprt,\r\ngfp_t flags)\r\n{\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nctxt = kmalloc(sizeof(*ctxt), flags);\r\nif (ctxt) {\r\nctxt->xprt = xprt;\r\nINIT_LIST_HEAD(&ctxt->free);\r\nINIT_LIST_HEAD(&ctxt->dto_q);\r\n}\r\nreturn ctxt;\r\n}\r\nstatic bool svc_rdma_prealloc_ctxts(struct svcxprt_rdma *xprt)\r\n{\r\nunsigned int i;\r\ni = xprt->sc_sq_depth + xprt->sc_rq_depth;\r\nwhile (i--) {\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nctxt = alloc_ctxt(xprt, GFP_KERNEL);\r\nif (!ctxt) {\r\ndprintk("svcrdma: No memory for RDMA ctxt\n");\r\nreturn false;\r\n}\r\nlist_add(&ctxt->free, &xprt->sc_ctxts);\r\n}\r\nreturn true;\r\n}\r\nstruct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *xprt)\r\n{\r\nstruct svc_rdma_op_ctxt *ctxt = NULL;\r\nspin_lock_bh(&xprt->sc_ctxt_lock);\r\nxprt->sc_ctxt_used++;\r\nif (list_empty(&xprt->sc_ctxts))\r\ngoto out_empty;\r\nctxt = list_first_entry(&xprt->sc_ctxts,\r\nstruct svc_rdma_op_ctxt, free);\r\nlist_del_init(&ctxt->free);\r\nspin_unlock_bh(&xprt->sc_ctxt_lock);\r\nout:\r\nctxt->count = 0;\r\nctxt->frmr = NULL;\r\nreturn ctxt;\r\nout_empty:\r\nspin_unlock_bh(&xprt->sc_ctxt_lock);\r\nctxt = alloc_ctxt(xprt, GFP_NOIO);\r\nif (ctxt)\r\ngoto out;\r\nspin_lock_bh(&xprt->sc_ctxt_lock);\r\nxprt->sc_ctxt_used--;\r\nspin_unlock_bh(&xprt->sc_ctxt_lock);\r\nWARN_ONCE(1, "svcrdma: empty RDMA ctxt list?\n");\r\nreturn NULL;\r\n}\r\nvoid svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt)\r\n{\r\nstruct svcxprt_rdma *xprt = ctxt->xprt;\r\nint i;\r\nfor (i = 0; i < ctxt->count && ctxt->sge[i].length; i++) {\r\nif (ctxt->sge[i].lkey == xprt->sc_pd->local_dma_lkey) {\r\natomic_dec(&xprt->sc_dma_used);\r\nib_dma_unmap_page(xprt->sc_cm_id->device,\r\nctxt->sge[i].addr,\r\nctxt->sge[i].length,\r\nctxt->direction);\r\n}\r\n}\r\n}\r\nvoid svc_rdma_put_context(struct svc_rdma_op_ctxt *ctxt, int free_pages)\r\n{\r\nstruct svcxprt_rdma *xprt = ctxt->xprt;\r\nint i;\r\nif (free_pages)\r\nfor (i = 0; i < ctxt->count; i++)\r\nput_page(ctxt->pages[i]);\r\nspin_lock_bh(&xprt->sc_ctxt_lock);\r\nxprt->sc_ctxt_used--;\r\nlist_add(&ctxt->free, &xprt->sc_ctxts);\r\nspin_unlock_bh(&xprt->sc_ctxt_lock);\r\n}\r\nstatic void svc_rdma_destroy_ctxts(struct svcxprt_rdma *xprt)\r\n{\r\nwhile (!list_empty(&xprt->sc_ctxts)) {\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nctxt = list_first_entry(&xprt->sc_ctxts,\r\nstruct svc_rdma_op_ctxt, free);\r\nlist_del(&ctxt->free);\r\nkfree(ctxt);\r\n}\r\n}\r\nstatic struct svc_rdma_req_map *alloc_req_map(gfp_t flags)\r\n{\r\nstruct svc_rdma_req_map *map;\r\nmap = kmalloc(sizeof(*map), flags);\r\nif (map)\r\nINIT_LIST_HEAD(&map->free);\r\nreturn map;\r\n}\r\nstatic bool svc_rdma_prealloc_maps(struct svcxprt_rdma *xprt)\r\n{\r\nunsigned int i;\r\ni = xprt->sc_max_requests;\r\nwhile (i--) {\r\nstruct svc_rdma_req_map *map;\r\nmap = alloc_req_map(GFP_KERNEL);\r\nif (!map) {\r\ndprintk("svcrdma: No memory for request map\n");\r\nreturn false;\r\n}\r\nlist_add(&map->free, &xprt->sc_maps);\r\n}\r\nreturn true;\r\n}\r\nstruct svc_rdma_req_map *svc_rdma_get_req_map(struct svcxprt_rdma *xprt)\r\n{\r\nstruct svc_rdma_req_map *map = NULL;\r\nspin_lock(&xprt->sc_map_lock);\r\nif (list_empty(&xprt->sc_maps))\r\ngoto out_empty;\r\nmap = list_first_entry(&xprt->sc_maps,\r\nstruct svc_rdma_req_map, free);\r\nlist_del_init(&map->free);\r\nspin_unlock(&xprt->sc_map_lock);\r\nout:\r\nmap->count = 0;\r\nreturn map;\r\nout_empty:\r\nspin_unlock(&xprt->sc_map_lock);\r\nmap = alloc_req_map(GFP_NOIO);\r\nif (map)\r\ngoto out;\r\nWARN_ONCE(1, "svcrdma: empty request map list?\n");\r\nreturn NULL;\r\n}\r\nvoid svc_rdma_put_req_map(struct svcxprt_rdma *xprt,\r\nstruct svc_rdma_req_map *map)\r\n{\r\nspin_lock(&xprt->sc_map_lock);\r\nlist_add(&map->free, &xprt->sc_maps);\r\nspin_unlock(&xprt->sc_map_lock);\r\n}\r\nstatic void svc_rdma_destroy_maps(struct svcxprt_rdma *xprt)\r\n{\r\nwhile (!list_empty(&xprt->sc_maps)) {\r\nstruct svc_rdma_req_map *map;\r\nmap = list_first_entry(&xprt->sc_maps,\r\nstruct svc_rdma_req_map, free);\r\nlist_del(&map->free);\r\nkfree(map);\r\n}\r\n}\r\nstatic void qp_event_handler(struct ib_event *event, void *context)\r\n{\r\nstruct svc_xprt *xprt = context;\r\nswitch (event->event) {\r\ncase IB_EVENT_PATH_MIG:\r\ncase IB_EVENT_COMM_EST:\r\ncase IB_EVENT_SQ_DRAINED:\r\ncase IB_EVENT_QP_LAST_WQE_REACHED:\r\ndprintk("svcrdma: QP event %s (%d) received for QP=%p\n",\r\nib_event_msg(event->event), event->event,\r\nevent->element.qp);\r\nbreak;\r\ncase IB_EVENT_PATH_MIG_ERR:\r\ncase IB_EVENT_QP_FATAL:\r\ncase IB_EVENT_QP_REQ_ERR:\r\ncase IB_EVENT_QP_ACCESS_ERR:\r\ncase IB_EVENT_DEVICE_FATAL:\r\ndefault:\r\ndprintk("svcrdma: QP ERROR event %s (%d) received for QP=%p, "\r\n"closing transport\n",\r\nib_event_msg(event->event), event->event,\r\nevent->element.qp);\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\nbreak;\r\n}\r\n}\r\nstatic void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nstruct svcxprt_rdma *xprt = cq->cq_context;\r\nstruct ib_cqe *cqe = wc->wr_cqe;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\r\nctxt->wc_status = wc->status;\r\nsvc_rdma_unmap_dma(ctxt);\r\nif (wc->status != IB_WC_SUCCESS)\r\ngoto flushed;\r\nctxt->byte_len = wc->byte_len;\r\nspin_lock(&xprt->sc_rq_dto_lock);\r\nlist_add_tail(&ctxt->dto_q, &xprt->sc_rq_dto_q);\r\nspin_unlock(&xprt->sc_rq_dto_lock);\r\nset_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);\r\nif (test_bit(RDMAXPRT_CONN_PENDING, &xprt->sc_flags))\r\ngoto out;\r\nsvc_xprt_enqueue(&xprt->sc_xprt);\r\ngoto out;\r\nflushed:\r\nif (wc->status != IB_WC_WR_FLUSH_ERR)\r\npr_warn("svcrdma: receive: %s (%u/0x%x)\n",\r\nib_wc_status_msg(wc->status),\r\nwc->status, wc->vendor_err);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\nsvc_rdma_put_context(ctxt, 1);\r\nout:\r\nsvc_xprt_put(&xprt->sc_xprt);\r\n}\r\nstatic void svc_rdma_send_wc_common(struct svcxprt_rdma *xprt,\r\nstruct ib_wc *wc,\r\nconst char *opname)\r\n{\r\nif (wc->status != IB_WC_SUCCESS)\r\ngoto err;\r\nout:\r\natomic_dec(&xprt->sc_sq_count);\r\nwake_up(&xprt->sc_send_wait);\r\nreturn;\r\nerr:\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\nif (wc->status != IB_WC_WR_FLUSH_ERR)\r\npr_err("svcrdma: %s: %s (%u/0x%x)\n",\r\nopname, ib_wc_status_msg(wc->status),\r\nwc->status, wc->vendor_err);\r\ngoto out;\r\n}\r\nstatic void svc_rdma_send_wc_common_put(struct ib_cq *cq, struct ib_wc *wc,\r\nconst char *opname)\r\n{\r\nstruct svcxprt_rdma *xprt = cq->cq_context;\r\nsvc_rdma_send_wc_common(xprt, wc, opname);\r\nsvc_xprt_put(&xprt->sc_xprt);\r\n}\r\nvoid svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nstruct ib_cqe *cqe = wc->wr_cqe;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nsvc_rdma_send_wc_common_put(cq, wc, "send");\r\nctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\n}\r\nvoid svc_rdma_wc_write(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nstruct ib_cqe *cqe = wc->wr_cqe;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nsvc_rdma_send_wc_common_put(cq, wc, "write");\r\nctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 0);\r\n}\r\nvoid svc_rdma_wc_reg(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nsvc_rdma_send_wc_common_put(cq, wc, "fastreg");\r\n}\r\nvoid svc_rdma_wc_read(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nstruct svcxprt_rdma *xprt = cq->cq_context;\r\nstruct ib_cqe *cqe = wc->wr_cqe;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nsvc_rdma_send_wc_common(xprt, wc, "read");\r\nctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_frmr(xprt, ctxt->frmr);\r\nif (test_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags)) {\r\nstruct svc_rdma_op_ctxt *read_hdr;\r\nread_hdr = ctxt->read_hdr;\r\nspin_lock(&xprt->sc_rq_dto_lock);\r\nlist_add_tail(&read_hdr->dto_q,\r\n&xprt->sc_read_complete_q);\r\nspin_unlock(&xprt->sc_rq_dto_lock);\r\nset_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);\r\nsvc_xprt_enqueue(&xprt->sc_xprt);\r\n}\r\nsvc_rdma_put_context(ctxt, 0);\r\nsvc_xprt_put(&xprt->sc_xprt);\r\n}\r\nvoid svc_rdma_wc_inv(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nsvc_rdma_send_wc_common_put(cq, wc, "localInv");\r\n}\r\nstatic struct svcxprt_rdma *rdma_create_xprt(struct svc_serv *serv,\r\nint listener)\r\n{\r\nstruct svcxprt_rdma *cma_xprt = kzalloc(sizeof *cma_xprt, GFP_KERNEL);\r\nif (!cma_xprt)\r\nreturn NULL;\r\nsvc_xprt_init(&init_net, &svc_rdma_class, &cma_xprt->sc_xprt, serv);\r\nINIT_LIST_HEAD(&cma_xprt->sc_accept_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_dto_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_rq_dto_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_read_complete_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_frmr_q);\r\nINIT_LIST_HEAD(&cma_xprt->sc_ctxts);\r\nINIT_LIST_HEAD(&cma_xprt->sc_maps);\r\ninit_waitqueue_head(&cma_xprt->sc_send_wait);\r\nspin_lock_init(&cma_xprt->sc_lock);\r\nspin_lock_init(&cma_xprt->sc_rq_dto_lock);\r\nspin_lock_init(&cma_xprt->sc_frmr_q_lock);\r\nspin_lock_init(&cma_xprt->sc_ctxt_lock);\r\nspin_lock_init(&cma_xprt->sc_map_lock);\r\nif (listener)\r\nset_bit(XPT_LISTENER, &cma_xprt->sc_xprt.xpt_flags);\r\nreturn cma_xprt;\r\n}\r\nint svc_rdma_post_recv(struct svcxprt_rdma *xprt, gfp_t flags)\r\n{\r\nstruct ib_recv_wr recv_wr, *bad_recv_wr;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nstruct page *page;\r\ndma_addr_t pa;\r\nint sge_no;\r\nint buflen;\r\nint ret;\r\nctxt = svc_rdma_get_context(xprt);\r\nbuflen = 0;\r\nctxt->direction = DMA_FROM_DEVICE;\r\nctxt->cqe.done = svc_rdma_wc_receive;\r\nfor (sge_no = 0; buflen < xprt->sc_max_req_size; sge_no++) {\r\nif (sge_no >= xprt->sc_max_sge) {\r\npr_err("svcrdma: Too many sges (%d)\n", sge_no);\r\ngoto err_put_ctxt;\r\n}\r\npage = alloc_page(flags);\r\nif (!page)\r\ngoto err_put_ctxt;\r\nctxt->pages[sge_no] = page;\r\npa = ib_dma_map_page(xprt->sc_cm_id->device,\r\npage, 0, PAGE_SIZE,\r\nDMA_FROM_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device, pa))\r\ngoto err_put_ctxt;\r\natomic_inc(&xprt->sc_dma_used);\r\nctxt->sge[sge_no].addr = pa;\r\nctxt->sge[sge_no].length = PAGE_SIZE;\r\nctxt->sge[sge_no].lkey = xprt->sc_pd->local_dma_lkey;\r\nctxt->count = sge_no + 1;\r\nbuflen += PAGE_SIZE;\r\n}\r\nrecv_wr.next = NULL;\r\nrecv_wr.sg_list = &ctxt->sge[0];\r\nrecv_wr.num_sge = ctxt->count;\r\nrecv_wr.wr_cqe = &ctxt->cqe;\r\nsvc_xprt_get(&xprt->sc_xprt);\r\nret = ib_post_recv(xprt->sc_qp, &recv_wr, &bad_recv_wr);\r\nif (ret) {\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\nsvc_xprt_put(&xprt->sc_xprt);\r\n}\r\nreturn ret;\r\nerr_put_ctxt:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn -ENOMEM;\r\n}\r\nint svc_rdma_repost_recv(struct svcxprt_rdma *xprt, gfp_t flags)\r\n{\r\nint ret = 0;\r\nret = svc_rdma_post_recv(xprt, flags);\r\nif (ret) {\r\npr_err("svcrdma: could not post a receive buffer, err=%d.\n",\r\nret);\r\npr_err("svcrdma: closing transport %p.\n", xprt);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\nret = -ENOTCONN;\r\n}\r\nreturn ret;\r\n}\r\nstatic void handle_connect_req(struct rdma_cm_id *new_cma_id, size_t client_ird)\r\n{\r\nstruct svcxprt_rdma *listen_xprt = new_cma_id->context;\r\nstruct svcxprt_rdma *newxprt;\r\nstruct sockaddr *sa;\r\nnewxprt = rdma_create_xprt(listen_xprt->sc_xprt.xpt_server, 0);\r\nif (!newxprt) {\r\ndprintk("svcrdma: failed to create new transport\n");\r\nreturn;\r\n}\r\nnewxprt->sc_cm_id = new_cma_id;\r\nnew_cma_id->context = newxprt;\r\ndprintk("svcrdma: Creating newxprt=%p, cm_id=%p, listenxprt=%p\n",\r\nnewxprt, newxprt->sc_cm_id, listen_xprt);\r\nnewxprt->sc_ord = client_ird;\r\nsa = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.dst_addr;\r\nsvc_xprt_set_remote(&newxprt->sc_xprt, sa, svc_addr_len(sa));\r\nsa = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.src_addr;\r\nsvc_xprt_set_local(&newxprt->sc_xprt, sa, svc_addr_len(sa));\r\nspin_lock_bh(&listen_xprt->sc_lock);\r\nlist_add_tail(&newxprt->sc_accept_q, &listen_xprt->sc_accept_q);\r\nspin_unlock_bh(&listen_xprt->sc_lock);\r\nset_bit(XPT_CONN, &listen_xprt->sc_xprt.xpt_flags);\r\nsvc_xprt_enqueue(&listen_xprt->sc_xprt);\r\n}\r\nstatic int rdma_listen_handler(struct rdma_cm_id *cma_id,\r\nstruct rdma_cm_event *event)\r\n{\r\nstruct svcxprt_rdma *xprt = cma_id->context;\r\nint ret = 0;\r\nswitch (event->event) {\r\ncase RDMA_CM_EVENT_CONNECT_REQUEST:\r\ndprintk("svcrdma: Connect request on cma_id=%p, xprt = %p, "\r\n"event = %s (%d)\n", cma_id, cma_id->context,\r\nrdma_event_msg(event->event), event->event);\r\nhandle_connect_req(cma_id,\r\nevent->param.conn.initiator_depth);\r\nbreak;\r\ncase RDMA_CM_EVENT_ESTABLISHED:\r\ndprintk("svcrdma: Connection completed on LISTEN xprt=%p, "\r\n"cm_id=%p\n", xprt, cma_id);\r\nbreak;\r\ncase RDMA_CM_EVENT_DEVICE_REMOVAL:\r\ndprintk("svcrdma: Device removal xprt=%p, cm_id=%p\n",\r\nxprt, cma_id);\r\nif (xprt)\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\nbreak;\r\ndefault:\r\ndprintk("svcrdma: Unexpected event on listening endpoint %p, "\r\n"event = %s (%d)\n", cma_id,\r\nrdma_event_msg(event->event), event->event);\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nstatic int rdma_cma_handler(struct rdma_cm_id *cma_id,\r\nstruct rdma_cm_event *event)\r\n{\r\nstruct svc_xprt *xprt = cma_id->context;\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nswitch (event->event) {\r\ncase RDMA_CM_EVENT_ESTABLISHED:\r\nsvc_xprt_get(xprt);\r\ndprintk("svcrdma: Connection completed on DTO xprt=%p, "\r\n"cm_id=%p\n", xprt, cma_id);\r\nclear_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags);\r\nsvc_xprt_enqueue(xprt);\r\nbreak;\r\ncase RDMA_CM_EVENT_DISCONNECTED:\r\ndprintk("svcrdma: Disconnect on DTO xprt=%p, cm_id=%p\n",\r\nxprt, cma_id);\r\nif (xprt) {\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\nsvc_xprt_enqueue(xprt);\r\nsvc_xprt_put(xprt);\r\n}\r\nbreak;\r\ncase RDMA_CM_EVENT_DEVICE_REMOVAL:\r\ndprintk("svcrdma: Device removal cma_id=%p, xprt = %p, "\r\n"event = %s (%d)\n", cma_id, xprt,\r\nrdma_event_msg(event->event), event->event);\r\nif (xprt) {\r\nset_bit(XPT_CLOSE, &xprt->xpt_flags);\r\nsvc_xprt_enqueue(xprt);\r\nsvc_xprt_put(xprt);\r\n}\r\nbreak;\r\ndefault:\r\ndprintk("svcrdma: Unexpected event on DTO endpoint %p, "\r\n"event = %s (%d)\n", cma_id,\r\nrdma_event_msg(event->event), event->event);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct svc_xprt *svc_rdma_create(struct svc_serv *serv,\r\nstruct net *net,\r\nstruct sockaddr *sa, int salen,\r\nint flags)\r\n{\r\nstruct rdma_cm_id *listen_id;\r\nstruct svcxprt_rdma *cma_xprt;\r\nint ret;\r\ndprintk("svcrdma: Creating RDMA socket\n");\r\nif ((sa->sa_family != AF_INET) && (sa->sa_family != AF_INET6)) {\r\ndprintk("svcrdma: Address family %d is not supported.\n", sa->sa_family);\r\nreturn ERR_PTR(-EAFNOSUPPORT);\r\n}\r\ncma_xprt = rdma_create_xprt(serv, 1);\r\nif (!cma_xprt)\r\nreturn ERR_PTR(-ENOMEM);\r\nlisten_id = rdma_create_id(&init_net, rdma_listen_handler, cma_xprt,\r\nRDMA_PS_TCP, IB_QPT_RC);\r\nif (IS_ERR(listen_id)) {\r\nret = PTR_ERR(listen_id);\r\ndprintk("svcrdma: rdma_create_id failed = %d\n", ret);\r\ngoto err0;\r\n}\r\n#if IS_ENABLED(CONFIG_IPV6)\r\nret = rdma_set_afonly(listen_id, 1);\r\nif (ret) {\r\ndprintk("svcrdma: rdma_set_afonly failed = %d\n", ret);\r\ngoto err1;\r\n}\r\n#endif\r\nret = rdma_bind_addr(listen_id, sa);\r\nif (ret) {\r\ndprintk("svcrdma: rdma_bind_addr failed = %d\n", ret);\r\ngoto err1;\r\n}\r\ncma_xprt->sc_cm_id = listen_id;\r\nret = rdma_listen(listen_id, RPCRDMA_LISTEN_BACKLOG);\r\nif (ret) {\r\ndprintk("svcrdma: rdma_listen failed = %d\n", ret);\r\ngoto err1;\r\n}\r\nsa = (struct sockaddr *)&cma_xprt->sc_cm_id->route.addr.src_addr;\r\nsvc_xprt_set_local(&cma_xprt->sc_xprt, sa, salen);\r\nreturn &cma_xprt->sc_xprt;\r\nerr1:\r\nrdma_destroy_id(listen_id);\r\nerr0:\r\nkfree(cma_xprt);\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic struct svc_rdma_fastreg_mr *rdma_alloc_frmr(struct svcxprt_rdma *xprt)\r\n{\r\nstruct ib_mr *mr;\r\nstruct scatterlist *sg;\r\nstruct svc_rdma_fastreg_mr *frmr;\r\nu32 num_sg;\r\nfrmr = kmalloc(sizeof(*frmr), GFP_KERNEL);\r\nif (!frmr)\r\ngoto err;\r\nnum_sg = min_t(u32, RPCSVC_MAXPAGES, xprt->sc_frmr_pg_list_len);\r\nmr = ib_alloc_mr(xprt->sc_pd, IB_MR_TYPE_MEM_REG, num_sg);\r\nif (IS_ERR(mr))\r\ngoto err_free_frmr;\r\nsg = kcalloc(RPCSVC_MAXPAGES, sizeof(*sg), GFP_KERNEL);\r\nif (!sg)\r\ngoto err_free_mr;\r\nsg_init_table(sg, RPCSVC_MAXPAGES);\r\nfrmr->mr = mr;\r\nfrmr->sg = sg;\r\nINIT_LIST_HEAD(&frmr->frmr_list);\r\nreturn frmr;\r\nerr_free_mr:\r\nib_dereg_mr(mr);\r\nerr_free_frmr:\r\nkfree(frmr);\r\nerr:\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nstatic void rdma_dealloc_frmr_q(struct svcxprt_rdma *xprt)\r\n{\r\nstruct svc_rdma_fastreg_mr *frmr;\r\nwhile (!list_empty(&xprt->sc_frmr_q)) {\r\nfrmr = list_entry(xprt->sc_frmr_q.next,\r\nstruct svc_rdma_fastreg_mr, frmr_list);\r\nlist_del_init(&frmr->frmr_list);\r\nkfree(frmr->sg);\r\nib_dereg_mr(frmr->mr);\r\nkfree(frmr);\r\n}\r\n}\r\nstruct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *rdma)\r\n{\r\nstruct svc_rdma_fastreg_mr *frmr = NULL;\r\nspin_lock_bh(&rdma->sc_frmr_q_lock);\r\nif (!list_empty(&rdma->sc_frmr_q)) {\r\nfrmr = list_entry(rdma->sc_frmr_q.next,\r\nstruct svc_rdma_fastreg_mr, frmr_list);\r\nlist_del_init(&frmr->frmr_list);\r\nfrmr->sg_nents = 0;\r\n}\r\nspin_unlock_bh(&rdma->sc_frmr_q_lock);\r\nif (frmr)\r\nreturn frmr;\r\nreturn rdma_alloc_frmr(rdma);\r\n}\r\nvoid svc_rdma_put_frmr(struct svcxprt_rdma *rdma,\r\nstruct svc_rdma_fastreg_mr *frmr)\r\n{\r\nif (frmr) {\r\nib_dma_unmap_sg(rdma->sc_cm_id->device,\r\nfrmr->sg, frmr->sg_nents, frmr->direction);\r\natomic_dec(&rdma->sc_dma_used);\r\nspin_lock_bh(&rdma->sc_frmr_q_lock);\r\nWARN_ON_ONCE(!list_empty(&frmr->frmr_list));\r\nlist_add(&frmr->frmr_list, &rdma->sc_frmr_q);\r\nspin_unlock_bh(&rdma->sc_frmr_q_lock);\r\n}\r\n}\r\nstatic struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *listen_rdma;\r\nstruct svcxprt_rdma *newxprt = NULL;\r\nstruct rdma_conn_param conn_param;\r\nstruct ib_qp_init_attr qp_attr;\r\nstruct ib_device *dev;\r\nunsigned int i;\r\nint ret = 0;\r\nlisten_rdma = container_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nclear_bit(XPT_CONN, &xprt->xpt_flags);\r\nspin_lock_bh(&listen_rdma->sc_lock);\r\nif (!list_empty(&listen_rdma->sc_accept_q)) {\r\nnewxprt = list_entry(listen_rdma->sc_accept_q.next,\r\nstruct svcxprt_rdma, sc_accept_q);\r\nlist_del_init(&newxprt->sc_accept_q);\r\n}\r\nif (!list_empty(&listen_rdma->sc_accept_q))\r\nset_bit(XPT_CONN, &listen_rdma->sc_xprt.xpt_flags);\r\nspin_unlock_bh(&listen_rdma->sc_lock);\r\nif (!newxprt)\r\nreturn NULL;\r\ndprintk("svcrdma: newxprt from accept queue = %p, cm_id=%p\n",\r\nnewxprt, newxprt->sc_cm_id);\r\ndev = newxprt->sc_cm_id->device;\r\nnewxprt->sc_max_sge = min((size_t)dev->attrs.max_sge,\r\n(size_t)RPCSVC_MAXPAGES);\r\nnewxprt->sc_max_sge_rd = min_t(size_t, dev->attrs.max_sge_rd,\r\nRPCSVC_MAXPAGES);\r\nnewxprt->sc_max_req_size = svcrdma_max_req_size;\r\nnewxprt->sc_max_requests = min_t(u32, dev->attrs.max_qp_wr,\r\nsvcrdma_max_requests);\r\nnewxprt->sc_max_bc_requests = min_t(u32, dev->attrs.max_qp_wr,\r\nsvcrdma_max_bc_requests);\r\nnewxprt->sc_rq_depth = newxprt->sc_max_requests +\r\nnewxprt->sc_max_bc_requests;\r\nnewxprt->sc_sq_depth = RPCRDMA_SQ_DEPTH_MULT * newxprt->sc_rq_depth;\r\nif (!svc_rdma_prealloc_ctxts(newxprt))\r\ngoto errout;\r\nif (!svc_rdma_prealloc_maps(newxprt))\r\ngoto errout;\r\nnewxprt->sc_ord = min_t(size_t, dev->attrs.max_qp_rd_atom, newxprt->sc_ord);\r\nnewxprt->sc_ord = min_t(size_t, svcrdma_ord, newxprt->sc_ord);\r\nnewxprt->sc_pd = ib_alloc_pd(dev);\r\nif (IS_ERR(newxprt->sc_pd)) {\r\ndprintk("svcrdma: error creating PD for connect request\n");\r\ngoto errout;\r\n}\r\nnewxprt->sc_sq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_sq_depth,\r\n0, IB_POLL_SOFTIRQ);\r\nif (IS_ERR(newxprt->sc_sq_cq)) {\r\ndprintk("svcrdma: error creating SQ CQ for connect request\n");\r\ngoto errout;\r\n}\r\nnewxprt->sc_rq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_rq_depth,\r\n0, IB_POLL_SOFTIRQ);\r\nif (IS_ERR(newxprt->sc_rq_cq)) {\r\ndprintk("svcrdma: error creating RQ CQ for connect request\n");\r\ngoto errout;\r\n}\r\nmemset(&qp_attr, 0, sizeof qp_attr);\r\nqp_attr.event_handler = qp_event_handler;\r\nqp_attr.qp_context = &newxprt->sc_xprt;\r\nqp_attr.cap.max_send_wr = newxprt->sc_sq_depth;\r\nqp_attr.cap.max_recv_wr = newxprt->sc_rq_depth;\r\nqp_attr.cap.max_send_sge = newxprt->sc_max_sge;\r\nqp_attr.cap.max_recv_sge = newxprt->sc_max_sge;\r\nqp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\r\nqp_attr.qp_type = IB_QPT_RC;\r\nqp_attr.send_cq = newxprt->sc_sq_cq;\r\nqp_attr.recv_cq = newxprt->sc_rq_cq;\r\ndprintk("svcrdma: newxprt->sc_cm_id=%p, newxprt->sc_pd=%p\n"\r\n" cm_id->device=%p, sc_pd->device=%p\n"\r\n" cap.max_send_wr = %d\n"\r\n" cap.max_recv_wr = %d\n"\r\n" cap.max_send_sge = %d\n"\r\n" cap.max_recv_sge = %d\n",\r\nnewxprt->sc_cm_id, newxprt->sc_pd,\r\ndev, newxprt->sc_pd->device,\r\nqp_attr.cap.max_send_wr,\r\nqp_attr.cap.max_recv_wr,\r\nqp_attr.cap.max_send_sge,\r\nqp_attr.cap.max_recv_sge);\r\nret = rdma_create_qp(newxprt->sc_cm_id, newxprt->sc_pd, &qp_attr);\r\nif (ret) {\r\ndprintk("svcrdma: failed to create QP, ret=%d\n", ret);\r\ngoto errout;\r\n}\r\nnewxprt->sc_qp = newxprt->sc_cm_id->qp;\r\nnewxprt->sc_reader = rdma_read_chunk_lcl;\r\nif (dev->attrs.device_cap_flags & IB_DEVICE_MEM_MGT_EXTENSIONS) {\r\nnewxprt->sc_frmr_pg_list_len =\r\ndev->attrs.max_fast_reg_page_list_len;\r\nnewxprt->sc_dev_caps |= SVCRDMA_DEVCAP_FAST_REG;\r\nnewxprt->sc_reader = rdma_read_chunk_frmr;\r\n}\r\nif (!rdma_protocol_iwarp(dev, newxprt->sc_cm_id->port_num) &&\r\n!rdma_ib_or_roce(dev, newxprt->sc_cm_id->port_num))\r\ngoto errout;\r\nif (rdma_protocol_iwarp(dev, newxprt->sc_cm_id->port_num))\r\nnewxprt->sc_dev_caps |= SVCRDMA_DEVCAP_READ_W_INV;\r\nfor (i = 0; i < newxprt->sc_max_requests; i++) {\r\nret = svc_rdma_post_recv(newxprt, GFP_KERNEL);\r\nif (ret) {\r\ndprintk("svcrdma: failure posting receive buffers\n");\r\ngoto errout;\r\n}\r\n}\r\nnewxprt->sc_cm_id->event_handler = rdma_cma_handler;\r\nset_bit(RDMAXPRT_CONN_PENDING, &newxprt->sc_flags);\r\nmemset(&conn_param, 0, sizeof conn_param);\r\nconn_param.responder_resources = 0;\r\nconn_param.initiator_depth = newxprt->sc_ord;\r\nret = rdma_accept(newxprt->sc_cm_id, &conn_param);\r\nif (ret) {\r\ndprintk("svcrdma: failed to accept new connection, ret=%d\n",\r\nret);\r\ngoto errout;\r\n}\r\ndprintk("svcrdma: new connection %p accepted with the following "\r\n"attributes:\n"\r\n" local_ip : %pI4\n"\r\n" local_port : %d\n"\r\n" remote_ip : %pI4\n"\r\n" remote_port : %d\n"\r\n" max_sge : %d\n"\r\n" max_sge_rd : %d\n"\r\n" sq_depth : %d\n"\r\n" max_requests : %d\n"\r\n" ord : %d\n",\r\nnewxprt,\r\n&((struct sockaddr_in *)&newxprt->sc_cm_id->\r\nroute.addr.src_addr)->sin_addr.s_addr,\r\nntohs(((struct sockaddr_in *)&newxprt->sc_cm_id->\r\nroute.addr.src_addr)->sin_port),\r\n&((struct sockaddr_in *)&newxprt->sc_cm_id->\r\nroute.addr.dst_addr)->sin_addr.s_addr,\r\nntohs(((struct sockaddr_in *)&newxprt->sc_cm_id->\r\nroute.addr.dst_addr)->sin_port),\r\nnewxprt->sc_max_sge,\r\nnewxprt->sc_max_sge_rd,\r\nnewxprt->sc_sq_depth,\r\nnewxprt->sc_max_requests,\r\nnewxprt->sc_ord);\r\nreturn &newxprt->sc_xprt;\r\nerrout:\r\ndprintk("svcrdma: failure accepting new connection rc=%d.\n", ret);\r\nsvc_xprt_get(&newxprt->sc_xprt);\r\nif (newxprt->sc_qp && !IS_ERR(newxprt->sc_qp))\r\nib_destroy_qp(newxprt->sc_qp);\r\nrdma_destroy_id(newxprt->sc_cm_id);\r\nsvc_xprt_put(&newxprt->sc_xprt);\r\nreturn NULL;\r\n}\r\nstatic void svc_rdma_release_rqst(struct svc_rqst *rqstp)\r\n{\r\n}\r\nstatic void svc_rdma_detach(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\ndprintk("svc: svc_rdma_detach(%p)\n", xprt);\r\nrdma_disconnect(rdma->sc_cm_id);\r\n}\r\nstatic void __svc_rdma_free(struct work_struct *work)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(work, struct svcxprt_rdma, sc_work);\r\nstruct svc_xprt *xprt = &rdma->sc_xprt;\r\ndprintk("svcrdma: %s(%p)\n", __func__, rdma);\r\nif (rdma->sc_qp && !IS_ERR(rdma->sc_qp))\r\nib_drain_qp(rdma->sc_qp);\r\nif (atomic_read(&xprt->xpt_ref.refcount) != 0)\r\npr_err("svcrdma: sc_xprt still in use? (%d)\n",\r\natomic_read(&xprt->xpt_ref.refcount));\r\nwhile (!list_empty(&rdma->sc_read_complete_q)) {\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nctxt = list_entry(rdma->sc_read_complete_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\nsvc_rdma_put_context(ctxt, 1);\r\n}\r\nwhile (!list_empty(&rdma->sc_rq_dto_q)) {\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nctxt = list_entry(rdma->sc_rq_dto_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\nsvc_rdma_put_context(ctxt, 1);\r\n}\r\nif (rdma->sc_ctxt_used != 0)\r\npr_err("svcrdma: ctxt still in use? (%d)\n",\r\nrdma->sc_ctxt_used);\r\nif (atomic_read(&rdma->sc_dma_used) != 0)\r\npr_err("svcrdma: dma still in use? (%d)\n",\r\natomic_read(&rdma->sc_dma_used));\r\nif (xprt->xpt_bc_xprt) {\r\nxprt_put(xprt->xpt_bc_xprt);\r\nxprt->xpt_bc_xprt = NULL;\r\n}\r\nrdma_dealloc_frmr_q(rdma);\r\nsvc_rdma_destroy_ctxts(rdma);\r\nsvc_rdma_destroy_maps(rdma);\r\nif (rdma->sc_qp && !IS_ERR(rdma->sc_qp))\r\nib_destroy_qp(rdma->sc_qp);\r\nif (rdma->sc_sq_cq && !IS_ERR(rdma->sc_sq_cq))\r\nib_free_cq(rdma->sc_sq_cq);\r\nif (rdma->sc_rq_cq && !IS_ERR(rdma->sc_rq_cq))\r\nib_free_cq(rdma->sc_rq_cq);\r\nif (rdma->sc_pd && !IS_ERR(rdma->sc_pd))\r\nib_dealloc_pd(rdma->sc_pd);\r\nrdma_destroy_id(rdma->sc_cm_id);\r\nkfree(rdma);\r\n}\r\nstatic void svc_rdma_free(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nINIT_WORK(&rdma->sc_work, __svc_rdma_free);\r\nqueue_work(svc_rdma_wq, &rdma->sc_work);\r\n}\r\nstatic int svc_rdma_has_wspace(struct svc_xprt *xprt)\r\n{\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nif (waitqueue_active(&rdma->sc_send_wait))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int svc_rdma_secure_port(struct svc_rqst *rqstp)\r\n{\r\nreturn 1;\r\n}\r\nint svc_rdma_send(struct svcxprt_rdma *xprt, struct ib_send_wr *wr)\r\n{\r\nstruct ib_send_wr *bad_wr, *n_wr;\r\nint wr_count;\r\nint i;\r\nint ret;\r\nif (test_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags))\r\nreturn -ENOTCONN;\r\nwr_count = 1;\r\nfor (n_wr = wr->next; n_wr; n_wr = n_wr->next)\r\nwr_count++;\r\nwhile (1) {\r\nspin_lock_bh(&xprt->sc_lock);\r\nif (xprt->sc_sq_depth < atomic_read(&xprt->sc_sq_count) + wr_count) {\r\nspin_unlock_bh(&xprt->sc_lock);\r\natomic_inc(&rdma_stat_sq_starve);\r\nwait_event(xprt->sc_send_wait,\r\natomic_read(&xprt->sc_sq_count) <\r\nxprt->sc_sq_depth);\r\nif (test_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags))\r\nreturn -ENOTCONN;\r\ncontinue;\r\n}\r\nfor (i = 0; i < wr_count; i++)\r\nsvc_xprt_get(&xprt->sc_xprt);\r\natomic_add(wr_count, &xprt->sc_sq_count);\r\nret = ib_post_send(xprt->sc_qp, wr, &bad_wr);\r\nif (ret) {\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\natomic_sub(wr_count, &xprt->sc_sq_count);\r\nfor (i = 0; i < wr_count; i ++)\r\nsvc_xprt_put(&xprt->sc_xprt);\r\ndprintk("svcrdma: failed to post SQ WR rc=%d, "\r\n"sc_sq_count=%d, sc_sq_depth=%d\n",\r\nret, atomic_read(&xprt->sc_sq_count),\r\nxprt->sc_sq_depth);\r\n}\r\nspin_unlock_bh(&xprt->sc_lock);\r\nif (ret)\r\nwake_up(&xprt->sc_send_wait);\r\nbreak;\r\n}\r\nreturn ret;\r\n}
