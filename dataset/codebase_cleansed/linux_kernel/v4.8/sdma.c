static const char *sdma_state_name(enum sdma_states state)\r\n{\r\nreturn sdma_state_names[state];\r\n}\r\nstatic void sdma_get(struct sdma_state *ss)\r\n{\r\nkref_get(&ss->kref);\r\n}\r\nstatic void sdma_complete(struct kref *kref)\r\n{\r\nstruct sdma_state *ss =\r\ncontainer_of(kref, struct sdma_state, kref);\r\ncomplete(&ss->comp);\r\n}\r\nstatic void sdma_put(struct sdma_state *ss)\r\n{\r\nkref_put(&ss->kref, sdma_complete);\r\n}\r\nstatic void sdma_finalput(struct sdma_state *ss)\r\n{\r\nsdma_put(ss);\r\nwait_for_completion(&ss->comp);\r\n}\r\nstatic inline void write_sde_csr(\r\nstruct sdma_engine *sde,\r\nu32 offset0,\r\nu64 value)\r\n{\r\nwrite_kctxt_csr(sde->dd, sde->this_idx, offset0, value);\r\n}\r\nstatic inline u64 read_sde_csr(\r\nstruct sdma_engine *sde,\r\nu32 offset0)\r\n{\r\nreturn read_kctxt_csr(sde->dd, sde->this_idx, offset0);\r\n}\r\nstatic void sdma_wait_for_packet_egress(struct sdma_engine *sde,\r\nint pause)\r\n{\r\nu64 off = 8 * sde->this_idx;\r\nstruct hfi1_devdata *dd = sde->dd;\r\nint lcnt = 0;\r\nu64 reg_prev;\r\nu64 reg = 0;\r\nwhile (1) {\r\nreg_prev = reg;\r\nreg = read_csr(dd, off + SEND_EGRESS_SEND_DMA_STATUS);\r\nreg &= SDMA_EGRESS_PACKET_OCCUPANCY_SMASK;\r\nreg >>= SDMA_EGRESS_PACKET_OCCUPANCY_SHIFT;\r\nif (reg == 0)\r\nbreak;\r\nif (reg != reg_prev)\r\nlcnt = 0;\r\nif (lcnt++ > 500) {\r\ndd_dev_err(dd, "%s: engine %u timeout waiting for packets to egress, remaining count %u, bouncing link\n",\r\n__func__, sde->this_idx, (u32)reg);\r\nqueue_work(dd->pport->hfi1_wq,\r\n&dd->pport->link_bounce_work);\r\nbreak;\r\n}\r\nudelay(1);\r\n}\r\n}\r\nvoid sdma_wait(struct hfi1_devdata *dd)\r\n{\r\nint i;\r\nfor (i = 0; i < dd->num_sdma; i++) {\r\nstruct sdma_engine *sde = &dd->per_sdma[i];\r\nsdma_wait_for_packet_egress(sde, 0);\r\n}\r\n}\r\nstatic inline void sdma_set_desc_cnt(struct sdma_engine *sde, unsigned cnt)\r\n{\r\nu64 reg;\r\nif (!(sde->dd->flags & HFI1_HAS_SDMA_TIMEOUT))\r\nreturn;\r\nreg = cnt;\r\nreg &= SD(DESC_CNT_CNT_MASK);\r\nreg <<= SD(DESC_CNT_CNT_SHIFT);\r\nwrite_sde_csr(sde, SD(DESC_CNT), reg);\r\n}\r\nstatic inline void complete_tx(struct sdma_engine *sde,\r\nstruct sdma_txreq *tx,\r\nint res)\r\n{\r\nstruct iowait *wait = tx->wait;\r\ncallback_t complete = tx->complete;\r\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\r\ntrace_hfi1_sdma_out_sn(sde, tx->sn);\r\nif (WARN_ON_ONCE(sde->head_sn != tx->sn))\r\ndd_dev_err(sde->dd, "expected %llu got %llu\n",\r\nsde->head_sn, tx->sn);\r\nsde->head_sn++;\r\n#endif\r\nsdma_txclean(sde->dd, tx);\r\nif (complete)\r\n(*complete)(tx, res);\r\nif (wait && iowait_sdma_dec(wait))\r\niowait_drain_wakeup(wait);\r\n}\r\nstatic void sdma_flush(struct sdma_engine *sde)\r\n{\r\nstruct sdma_txreq *txp, *txp_next;\r\nLIST_HEAD(flushlist);\r\nunsigned long flags;\r\nsdma_flush_descq(sde);\r\nspin_lock_irqsave(&sde->flushlist_lock, flags);\r\nlist_for_each_entry_safe(txp, txp_next, &sde->flushlist, list) {\r\nlist_del_init(&txp->list);\r\nlist_add_tail(&txp->list, &flushlist);\r\n}\r\nspin_unlock_irqrestore(&sde->flushlist_lock, flags);\r\nlist_for_each_entry_safe(txp, txp_next, &flushlist, list)\r\ncomplete_tx(sde, txp, SDMA_TXREQ_S_ABORTED);\r\n}\r\nstatic void sdma_field_flush(struct work_struct *work)\r\n{\r\nunsigned long flags;\r\nstruct sdma_engine *sde =\r\ncontainer_of(work, struct sdma_engine, flush_worker);\r\nwrite_seqlock_irqsave(&sde->head_lock, flags);\r\nif (!__sdma_running(sde))\r\nsdma_flush(sde);\r\nwrite_sequnlock_irqrestore(&sde->head_lock, flags);\r\n}\r\nstatic void sdma_err_halt_wait(struct work_struct *work)\r\n{\r\nstruct sdma_engine *sde = container_of(work, struct sdma_engine,\r\nerr_halt_worker);\r\nu64 statuscsr;\r\nunsigned long timeout;\r\ntimeout = jiffies + msecs_to_jiffies(SDMA_ERR_HALT_TIMEOUT);\r\nwhile (1) {\r\nstatuscsr = read_sde_csr(sde, SD(STATUS));\r\nstatuscsr &= SD(STATUS_ENG_HALTED_SMASK);\r\nif (statuscsr)\r\nbreak;\r\nif (time_after(jiffies, timeout)) {\r\ndd_dev_err(sde->dd,\r\n"SDMA engine %d - timeout waiting for engine to halt\n",\r\nsde->this_idx);\r\nbreak;\r\n}\r\nusleep_range(80, 120);\r\n}\r\nsdma_process_event(sde, sdma_event_e15_hw_halt_done);\r\n}\r\nstatic void sdma_err_progress_check_schedule(struct sdma_engine *sde)\r\n{\r\nif (!is_bx(sde->dd) && HFI1_CAP_IS_KSET(SDMA_AHG)) {\r\nunsigned index;\r\nstruct hfi1_devdata *dd = sde->dd;\r\nfor (index = 0; index < dd->num_sdma; index++) {\r\nstruct sdma_engine *curr_sdma = &dd->per_sdma[index];\r\nif (curr_sdma != sde)\r\ncurr_sdma->progress_check_head =\r\ncurr_sdma->descq_head;\r\n}\r\ndd_dev_err(sde->dd,\r\n"SDMA engine %d - check scheduled\n",\r\nsde->this_idx);\r\nmod_timer(&sde->err_progress_check_timer, jiffies + 10);\r\n}\r\n}\r\nstatic void sdma_err_progress_check(unsigned long data)\r\n{\r\nunsigned index;\r\nstruct sdma_engine *sde = (struct sdma_engine *)data;\r\ndd_dev_err(sde->dd, "SDE progress check event\n");\r\nfor (index = 0; index < sde->dd->num_sdma; index++) {\r\nstruct sdma_engine *curr_sde = &sde->dd->per_sdma[index];\r\nunsigned long flags;\r\nif (curr_sde == sde)\r\ncontinue;\r\nspin_lock_irqsave(&curr_sde->tail_lock, flags);\r\nwrite_seqlock(&curr_sde->head_lock);\r\nif (curr_sde->state.current_state != sdma_state_s99_running) {\r\nwrite_sequnlock(&curr_sde->head_lock);\r\nspin_unlock_irqrestore(&curr_sde->tail_lock, flags);\r\ncontinue;\r\n}\r\nif ((curr_sde->descq_head != curr_sde->descq_tail) &&\r\n(curr_sde->descq_head ==\r\ncurr_sde->progress_check_head))\r\n__sdma_process_event(curr_sde,\r\nsdma_event_e90_sw_halted);\r\nwrite_sequnlock(&curr_sde->head_lock);\r\nspin_unlock_irqrestore(&curr_sde->tail_lock, flags);\r\n}\r\nschedule_work(&sde->err_halt_worker);\r\n}\r\nstatic void sdma_hw_clean_up_task(unsigned long opaque)\r\n{\r\nstruct sdma_engine *sde = (struct sdma_engine *)opaque;\r\nu64 statuscsr;\r\nwhile (1) {\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\ndd_dev_err(sde->dd, "CONFIG SDMA(%u) %s:%d %s()\n",\r\nsde->this_idx, slashstrip(__FILE__), __LINE__,\r\n__func__);\r\n#endif\r\nstatuscsr = read_sde_csr(sde, SD(STATUS));\r\nstatuscsr &= SD(STATUS_ENG_CLEANED_UP_SMASK);\r\nif (statuscsr)\r\nbreak;\r\nudelay(10);\r\n}\r\nsdma_process_event(sde, sdma_event_e25_hw_clean_up_done);\r\n}\r\nstatic inline struct sdma_txreq *get_txhead(struct sdma_engine *sde)\r\n{\r\nsmp_read_barrier_depends();\r\nreturn sde->tx_ring[sde->tx_head & sde->sdma_mask];\r\n}\r\nstatic void sdma_flush_descq(struct sdma_engine *sde)\r\n{\r\nu16 head, tail;\r\nint progress = 0;\r\nstruct sdma_txreq *txp = get_txhead(sde);\r\nhead = sde->descq_head & sde->sdma_mask;\r\ntail = sde->descq_tail & sde->sdma_mask;\r\nwhile (head != tail) {\r\nhead = ++sde->descq_head & sde->sdma_mask;\r\nif (txp && txp->next_descq_idx == head) {\r\nsde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;\r\ncomplete_tx(sde, txp, SDMA_TXREQ_S_ABORTED);\r\ntrace_hfi1_sdma_progress(sde, head, tail, txp);\r\ntxp = get_txhead(sde);\r\n}\r\nprogress++;\r\n}\r\nif (progress)\r\nsdma_desc_avail(sde, sdma_descq_freecnt(sde));\r\n}\r\nstatic void sdma_sw_clean_up_task(unsigned long opaque)\r\n{\r\nstruct sdma_engine *sde = (struct sdma_engine *)opaque;\r\nunsigned long flags;\r\nspin_lock_irqsave(&sde->tail_lock, flags);\r\nwrite_seqlock(&sde->head_lock);\r\nsdma_make_progress(sde, 0);\r\nsdma_flush(sde);\r\nsde->descq_tail = 0;\r\nsde->descq_head = 0;\r\nsde->desc_avail = sdma_descq_freecnt(sde);\r\n*sde->head_dma = 0;\r\n__sdma_process_event(sde, sdma_event_e40_sw_cleaned);\r\nwrite_sequnlock(&sde->head_lock);\r\nspin_unlock_irqrestore(&sde->tail_lock, flags);\r\n}\r\nstatic void sdma_sw_tear_down(struct sdma_engine *sde)\r\n{\r\nstruct sdma_state *ss = &sde->state;\r\nsdma_put(ss);\r\natomic_set(&sde->dd->sdma_unfreeze_count, -1);\r\nwake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\r\n}\r\nstatic void sdma_start_hw_clean_up(struct sdma_engine *sde)\r\n{\r\ntasklet_hi_schedule(&sde->sdma_hw_clean_up_task);\r\n}\r\nstatic void sdma_set_state(struct sdma_engine *sde,\r\nenum sdma_states next_state)\r\n{\r\nstruct sdma_state *ss = &sde->state;\r\nconst struct sdma_set_state_action *action = sdma_action_table;\r\nunsigned op = 0;\r\ntrace_hfi1_sdma_state(\r\nsde,\r\nsdma_state_names[ss->current_state],\r\nsdma_state_names[next_state]);\r\nss->previous_state = ss->current_state;\r\nss->previous_op = ss->current_op;\r\nss->current_state = next_state;\r\nif (ss->previous_state != sdma_state_s99_running &&\r\nnext_state == sdma_state_s99_running)\r\nsdma_flush(sde);\r\nif (action[next_state].op_enable)\r\nop |= SDMA_SENDCTRL_OP_ENABLE;\r\nif (action[next_state].op_intenable)\r\nop |= SDMA_SENDCTRL_OP_INTENABLE;\r\nif (action[next_state].op_halt)\r\nop |= SDMA_SENDCTRL_OP_HALT;\r\nif (action[next_state].op_cleanup)\r\nop |= SDMA_SENDCTRL_OP_CLEANUP;\r\nif (action[next_state].go_s99_running_tofalse)\r\nss->go_s99_running = 0;\r\nif (action[next_state].go_s99_running_totrue)\r\nss->go_s99_running = 1;\r\nss->current_op = op;\r\nsdma_sendctrl(sde, ss->current_op);\r\n}\r\nu16 sdma_get_descq_cnt(void)\r\n{\r\nu16 count = sdma_descq_cnt;\r\nif (!count)\r\nreturn SDMA_DESCQ_CNT;\r\nif (!is_power_of_2(count))\r\nreturn SDMA_DESCQ_CNT;\r\nif (count < 64 || count > 32768)\r\nreturn SDMA_DESCQ_CNT;\r\nreturn count;\r\n}\r\nstruct sdma_engine *sdma_select_engine_vl(\r\nstruct hfi1_devdata *dd,\r\nu32 selector,\r\nu8 vl)\r\n{\r\nstruct sdma_vl_map *m;\r\nstruct sdma_map_elem *e;\r\nstruct sdma_engine *rval;\r\nif (vl >= num_vls) {\r\nrval = NULL;\r\ngoto done;\r\n}\r\nrcu_read_lock();\r\nm = rcu_dereference(dd->sdma_map);\r\nif (unlikely(!m)) {\r\nrcu_read_unlock();\r\nreturn &dd->per_sdma[0];\r\n}\r\ne = m->map[vl & m->mask];\r\nrval = e->sde[selector & e->mask];\r\nrcu_read_unlock();\r\ndone:\r\nrval = !rval ? &dd->per_sdma[0] : rval;\r\ntrace_hfi1_sdma_engine_select(dd, selector, vl, rval->this_idx);\r\nreturn rval;\r\n}\r\nstruct sdma_engine *sdma_select_engine_sc(\r\nstruct hfi1_devdata *dd,\r\nu32 selector,\r\nu8 sc5)\r\n{\r\nu8 vl = sc_to_vlt(dd, sc5);\r\nreturn sdma_select_engine_vl(dd, selector, vl);\r\n}\r\nstatic void sdma_map_free(struct sdma_vl_map *m)\r\n{\r\nint i;\r\nfor (i = 0; m && i < m->actual_vls; i++)\r\nkfree(m->map[i]);\r\nkfree(m);\r\n}\r\nstatic void sdma_map_rcu_callback(struct rcu_head *list)\r\n{\r\nstruct sdma_vl_map *m = container_of(list, struct sdma_vl_map, list);\r\nsdma_map_free(m);\r\n}\r\nint sdma_map_init(struct hfi1_devdata *dd, u8 port, u8 num_vls, u8 *vl_engines)\r\n{\r\nint i, j;\r\nint extra, sde_per_vl;\r\nint engine = 0;\r\nu8 lvl_engines[OPA_MAX_VLS];\r\nstruct sdma_vl_map *oldmap, *newmap;\r\nif (!(dd->flags & HFI1_HAS_SEND_DMA))\r\nreturn 0;\r\nif (!vl_engines) {\r\nsde_per_vl = dd->num_sdma / num_vls;\r\nextra = dd->num_sdma % num_vls;\r\nvl_engines = lvl_engines;\r\nfor (i = num_vls - 1; i >= 0; i--, extra--)\r\nvl_engines[i] = sde_per_vl + (extra > 0 ? 1 : 0);\r\n}\r\nnewmap = kzalloc(\r\nsizeof(struct sdma_vl_map) +\r\nroundup_pow_of_two(num_vls) *\r\nsizeof(struct sdma_map_elem *),\r\nGFP_KERNEL);\r\nif (!newmap)\r\ngoto bail;\r\nnewmap->actual_vls = num_vls;\r\nnewmap->vls = roundup_pow_of_two(num_vls);\r\nnewmap->mask = (1 << ilog2(newmap->vls)) - 1;\r\nfor (i = 0; i < TXE_NUM_SDMA_ENGINES; i++)\r\nnewmap->engine_to_vl[i] = -1;\r\nfor (i = 0; i < newmap->vls; i++) {\r\nint first_engine = engine;\r\nif (i < newmap->actual_vls) {\r\nint sz = roundup_pow_of_two(vl_engines[i]);\r\nnewmap->map[i] = kzalloc(\r\nsizeof(struct sdma_map_elem) +\r\nsz * sizeof(struct sdma_engine *),\r\nGFP_KERNEL);\r\nif (!newmap->map[i])\r\ngoto bail;\r\nnewmap->map[i]->mask = (1 << ilog2(sz)) - 1;\r\nfor (j = 0; j < sz; j++) {\r\nnewmap->map[i]->sde[j] =\r\n&dd->per_sdma[engine];\r\nif (++engine >= first_engine + vl_engines[i])\r\nengine = first_engine;\r\n}\r\nfor (j = 0; j < vl_engines[i]; j++)\r\nnewmap->engine_to_vl[first_engine + j] = i;\r\n} else {\r\nnewmap->map[i] = newmap->map[i % num_vls];\r\n}\r\nengine = first_engine + vl_engines[i];\r\n}\r\nspin_lock_irq(&dd->sde_map_lock);\r\noldmap = rcu_dereference_protected(dd->sdma_map,\r\nlockdep_is_held(&dd->sde_map_lock));\r\nrcu_assign_pointer(dd->sdma_map, newmap);\r\nspin_unlock_irq(&dd->sde_map_lock);\r\nif (oldmap)\r\ncall_rcu(&oldmap->list, sdma_map_rcu_callback);\r\nreturn 0;\r\nbail:\r\nsdma_map_free(newmap);\r\nreturn -ENOMEM;\r\n}\r\nstatic void sdma_clean(struct hfi1_devdata *dd, size_t num_engines)\r\n{\r\nsize_t i;\r\nstruct sdma_engine *sde;\r\nif (dd->sdma_pad_dma) {\r\ndma_free_coherent(&dd->pcidev->dev, 4,\r\n(void *)dd->sdma_pad_dma,\r\ndd->sdma_pad_phys);\r\ndd->sdma_pad_dma = NULL;\r\ndd->sdma_pad_phys = 0;\r\n}\r\nif (dd->sdma_heads_dma) {\r\ndma_free_coherent(&dd->pcidev->dev, dd->sdma_heads_size,\r\n(void *)dd->sdma_heads_dma,\r\ndd->sdma_heads_phys);\r\ndd->sdma_heads_dma = NULL;\r\ndd->sdma_heads_phys = 0;\r\n}\r\nfor (i = 0; dd->per_sdma && i < num_engines; ++i) {\r\nsde = &dd->per_sdma[i];\r\nsde->head_dma = NULL;\r\nsde->head_phys = 0;\r\nif (sde->descq) {\r\ndma_free_coherent(\r\n&dd->pcidev->dev,\r\nsde->descq_cnt * sizeof(u64[2]),\r\nsde->descq,\r\nsde->descq_phys\r\n);\r\nsde->descq = NULL;\r\nsde->descq_phys = 0;\r\n}\r\nkvfree(sde->tx_ring);\r\nsde->tx_ring = NULL;\r\n}\r\nspin_lock_irq(&dd->sde_map_lock);\r\nsdma_map_free(rcu_access_pointer(dd->sdma_map));\r\nRCU_INIT_POINTER(dd->sdma_map, NULL);\r\nspin_unlock_irq(&dd->sde_map_lock);\r\nsynchronize_rcu();\r\nkfree(dd->per_sdma);\r\ndd->per_sdma = NULL;\r\n}\r\nint sdma_init(struct hfi1_devdata *dd, u8 port)\r\n{\r\nunsigned this_idx;\r\nstruct sdma_engine *sde;\r\nu16 descq_cnt;\r\nvoid *curr_head;\r\nstruct hfi1_pportdata *ppd = dd->pport + port;\r\nu32 per_sdma_credits;\r\nuint idle_cnt = sdma_idle_cnt;\r\nsize_t num_engines = dd->chip_sdma_engines;\r\nif (!HFI1_CAP_IS_KSET(SDMA)) {\r\nHFI1_CAP_CLEAR(SDMA_AHG);\r\nreturn 0;\r\n}\r\nif (mod_num_sdma &&\r\nmod_num_sdma <= dd->chip_sdma_engines &&\r\nmod_num_sdma >= num_vls)\r\nnum_engines = mod_num_sdma;\r\ndd_dev_info(dd, "SDMA mod_num_sdma: %u\n", mod_num_sdma);\r\ndd_dev_info(dd, "SDMA chip_sdma_engines: %u\n", dd->chip_sdma_engines);\r\ndd_dev_info(dd, "SDMA chip_sdma_mem_size: %u\n",\r\ndd->chip_sdma_mem_size);\r\nper_sdma_credits =\r\ndd->chip_sdma_mem_size / (num_engines * SDMA_BLOCK_SIZE);\r\ninit_waitqueue_head(&dd->sdma_unfreeze_wq);\r\natomic_set(&dd->sdma_unfreeze_count, 0);\r\ndescq_cnt = sdma_get_descq_cnt();\r\ndd_dev_info(dd, "SDMA engines %zu descq_cnt %u\n",\r\nnum_engines, descq_cnt);\r\ndd->per_sdma = kcalloc(num_engines, sizeof(*dd->per_sdma), GFP_KERNEL);\r\nif (!dd->per_sdma)\r\nreturn -ENOMEM;\r\nidle_cnt = ns_to_cclock(dd, idle_cnt);\r\nif (!sdma_desct_intr)\r\nsdma_desct_intr = SDMA_DESC_INTR;\r\nfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\r\nsde = &dd->per_sdma[this_idx];\r\nsde->dd = dd;\r\nsde->ppd = ppd;\r\nsde->this_idx = this_idx;\r\nsde->descq_cnt = descq_cnt;\r\nsde->desc_avail = sdma_descq_freecnt(sde);\r\nsde->sdma_shift = ilog2(descq_cnt);\r\nsde->sdma_mask = (1 << sde->sdma_shift) - 1;\r\nsde->int_mask = (u64)1 << (0 * TXE_NUM_SDMA_ENGINES +\r\nthis_idx);\r\nsde->progress_mask = (u64)1 << (1 * TXE_NUM_SDMA_ENGINES +\r\nthis_idx);\r\nsde->idle_mask = (u64)1 << (2 * TXE_NUM_SDMA_ENGINES +\r\nthis_idx);\r\nsde->imask = sde->int_mask | sde->progress_mask |\r\nsde->idle_mask;\r\nspin_lock_init(&sde->tail_lock);\r\nseqlock_init(&sde->head_lock);\r\nspin_lock_init(&sde->senddmactrl_lock);\r\nspin_lock_init(&sde->flushlist_lock);\r\nsde->ahg_bits = 0xfffffffe00000000ULL;\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\nkref_init(&sde->state.kref);\r\ninit_completion(&sde->state.comp);\r\nINIT_LIST_HEAD(&sde->flushlist);\r\nINIT_LIST_HEAD(&sde->dmawait);\r\nsde->tail_csr =\r\nget_kctxt_csr_addr(dd, this_idx, SD(TAIL));\r\nif (idle_cnt)\r\ndd->default_desc1 =\r\nSDMA_DESC1_HEAD_TO_HOST_FLAG;\r\nelse\r\ndd->default_desc1 =\r\nSDMA_DESC1_INT_REQ_FLAG;\r\ntasklet_init(&sde->sdma_hw_clean_up_task, sdma_hw_clean_up_task,\r\n(unsigned long)sde);\r\ntasklet_init(&sde->sdma_sw_clean_up_task, sdma_sw_clean_up_task,\r\n(unsigned long)sde);\r\nINIT_WORK(&sde->err_halt_worker, sdma_err_halt_wait);\r\nINIT_WORK(&sde->flush_worker, sdma_field_flush);\r\nsde->progress_check_head = 0;\r\nsetup_timer(&sde->err_progress_check_timer,\r\nsdma_err_progress_check, (unsigned long)sde);\r\nsde->descq = dma_zalloc_coherent(\r\n&dd->pcidev->dev,\r\ndescq_cnt * sizeof(u64[2]),\r\n&sde->descq_phys,\r\nGFP_KERNEL\r\n);\r\nif (!sde->descq)\r\ngoto bail;\r\nsde->tx_ring =\r\nkcalloc(descq_cnt, sizeof(struct sdma_txreq *),\r\nGFP_KERNEL);\r\nif (!sde->tx_ring)\r\nsde->tx_ring =\r\nvzalloc(\r\nsizeof(struct sdma_txreq *) *\r\ndescq_cnt);\r\nif (!sde->tx_ring)\r\ngoto bail;\r\n}\r\ndd->sdma_heads_size = L1_CACHE_BYTES * num_engines;\r\ndd->sdma_heads_dma = dma_zalloc_coherent(\r\n&dd->pcidev->dev,\r\ndd->sdma_heads_size,\r\n&dd->sdma_heads_phys,\r\nGFP_KERNEL\r\n);\r\nif (!dd->sdma_heads_dma) {\r\ndd_dev_err(dd, "failed to allocate SendDMA head memory\n");\r\ngoto bail;\r\n}\r\ndd->sdma_pad_dma = dma_zalloc_coherent(\r\n&dd->pcidev->dev,\r\nsizeof(u32),\r\n&dd->sdma_pad_phys,\r\nGFP_KERNEL\r\n);\r\nif (!dd->sdma_pad_dma) {\r\ndd_dev_err(dd, "failed to allocate SendDMA pad memory\n");\r\ngoto bail;\r\n}\r\ncurr_head = (void *)dd->sdma_heads_dma;\r\nfor (this_idx = 0; this_idx < num_engines; ++this_idx) {\r\nunsigned long phys_offset;\r\nsde = &dd->per_sdma[this_idx];\r\nsde->head_dma = curr_head;\r\ncurr_head += L1_CACHE_BYTES;\r\nphys_offset = (unsigned long)sde->head_dma -\r\n(unsigned long)dd->sdma_heads_dma;\r\nsde->head_phys = dd->sdma_heads_phys + phys_offset;\r\ninit_sdma_regs(sde, per_sdma_credits, idle_cnt);\r\n}\r\ndd->flags |= HFI1_HAS_SEND_DMA;\r\ndd->flags |= idle_cnt ? HFI1_HAS_SDMA_TIMEOUT : 0;\r\ndd->num_sdma = num_engines;\r\nif (sdma_map_init(dd, port, ppd->vls_operational, NULL))\r\ngoto bail;\r\ndd_dev_info(dd, "SDMA num_sdma: %u\n", dd->num_sdma);\r\nreturn 0;\r\nbail:\r\nsdma_clean(dd, num_engines);\r\nreturn -ENOMEM;\r\n}\r\nvoid sdma_all_running(struct hfi1_devdata *dd)\r\n{\r\nstruct sdma_engine *sde;\r\nunsigned int i;\r\nfor (i = 0; i < dd->num_sdma; ++i) {\r\nsde = &dd->per_sdma[i];\r\nsdma_process_event(sde, sdma_event_e30_go_running);\r\n}\r\n}\r\nvoid sdma_all_idle(struct hfi1_devdata *dd)\r\n{\r\nstruct sdma_engine *sde;\r\nunsigned int i;\r\nfor (i = 0; i < dd->num_sdma; ++i) {\r\nsde = &dd->per_sdma[i];\r\nsdma_process_event(sde, sdma_event_e70_go_idle);\r\n}\r\n}\r\nvoid sdma_start(struct hfi1_devdata *dd)\r\n{\r\nunsigned i;\r\nstruct sdma_engine *sde;\r\nfor (i = 0; i < dd->num_sdma; ++i) {\r\nsde = &dd->per_sdma[i];\r\nsdma_process_event(sde, sdma_event_e10_go_hw_start);\r\n}\r\n}\r\nvoid sdma_exit(struct hfi1_devdata *dd)\r\n{\r\nunsigned this_idx;\r\nstruct sdma_engine *sde;\r\nfor (this_idx = 0; dd->per_sdma && this_idx < dd->num_sdma;\r\n++this_idx) {\r\nsde = &dd->per_sdma[this_idx];\r\nif (!list_empty(&sde->dmawait))\r\ndd_dev_err(dd, "sde %u: dmawait list not empty!\n",\r\nsde->this_idx);\r\nsdma_process_event(sde, sdma_event_e00_go_hw_down);\r\ndel_timer_sync(&sde->err_progress_check_timer);\r\nsdma_finalput(&sde->state);\r\n}\r\nsdma_clean(dd, dd->num_sdma);\r\n}\r\nstatic inline void sdma_unmap_desc(\r\nstruct hfi1_devdata *dd,\r\nstruct sdma_desc *descp)\r\n{\r\nswitch (sdma_mapping_type(descp)) {\r\ncase SDMA_MAP_SINGLE:\r\ndma_unmap_single(\r\n&dd->pcidev->dev,\r\nsdma_mapping_addr(descp),\r\nsdma_mapping_len(descp),\r\nDMA_TO_DEVICE);\r\nbreak;\r\ncase SDMA_MAP_PAGE:\r\ndma_unmap_page(\r\n&dd->pcidev->dev,\r\nsdma_mapping_addr(descp),\r\nsdma_mapping_len(descp),\r\nDMA_TO_DEVICE);\r\nbreak;\r\n}\r\n}\r\nstatic inline u8 ahg_mode(struct sdma_txreq *tx)\r\n{\r\nreturn (tx->descp[0].qw[1] & SDMA_DESC1_HEADER_MODE_SMASK)\r\n>> SDMA_DESC1_HEADER_MODE_SHIFT;\r\n}\r\nvoid sdma_txclean(\r\nstruct hfi1_devdata *dd,\r\nstruct sdma_txreq *tx)\r\n{\r\nu16 i;\r\nif (tx->num_desc) {\r\nu8 skip = 0, mode = ahg_mode(tx);\r\nsdma_unmap_desc(dd, &tx->descp[0]);\r\nif (mode > SDMA_AHG_APPLY_UPDATE1)\r\nskip = mode >> 1;\r\nfor (i = 1 + skip; i < tx->num_desc; i++)\r\nsdma_unmap_desc(dd, &tx->descp[i]);\r\ntx->num_desc = 0;\r\n}\r\nkfree(tx->coalesce_buf);\r\ntx->coalesce_buf = NULL;\r\nif (unlikely(tx->desc_limit > ARRAY_SIZE(tx->descs))) {\r\ntx->desc_limit = ARRAY_SIZE(tx->descs);\r\nkfree(tx->descp);\r\n}\r\n}\r\nstatic inline u16 sdma_gethead(struct sdma_engine *sde)\r\n{\r\nstruct hfi1_devdata *dd = sde->dd;\r\nint use_dmahead;\r\nu16 hwhead;\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\ndd_dev_err(sde->dd, "CONFIG SDMA(%u) %s:%d %s()\n",\r\nsde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\r\n#endif\r\nretry:\r\nuse_dmahead = HFI1_CAP_IS_KSET(USE_SDMA_HEAD) && __sdma_running(sde) &&\r\n(dd->flags & HFI1_HAS_SDMA_TIMEOUT);\r\nhwhead = use_dmahead ?\r\n(u16)le64_to_cpu(*sde->head_dma) :\r\n(u16)read_sde_csr(sde, SD(HEAD));\r\nif (unlikely(HFI1_CAP_IS_KSET(SDMA_HEAD_CHECK))) {\r\nu16 cnt;\r\nu16 swtail;\r\nu16 swhead;\r\nint sane;\r\nswhead = sde->descq_head & sde->sdma_mask;\r\nswtail = ACCESS_ONCE(sde->descq_tail) & sde->sdma_mask;\r\ncnt = sde->descq_cnt;\r\nif (swhead < swtail)\r\nsane = (hwhead >= swhead) & (hwhead <= swtail);\r\nelse if (swhead > swtail)\r\nsane = ((hwhead >= swhead) && (hwhead < cnt)) ||\r\n(hwhead <= swtail);\r\nelse\r\nsane = (hwhead == swhead);\r\nif (unlikely(!sane)) {\r\ndd_dev_err(dd, "SDMA(%u) bad head (%s) hwhd=%hu swhd=%hu swtl=%hu cnt=%hu\n",\r\nsde->this_idx,\r\nuse_dmahead ? "dma" : "kreg",\r\nhwhead, swhead, swtail, cnt);\r\nif (use_dmahead) {\r\nuse_dmahead = 0;\r\ngoto retry;\r\n}\r\nhwhead = swhead;\r\n}\r\n}\r\nreturn hwhead;\r\n}\r\nstatic void sdma_desc_avail(struct sdma_engine *sde, unsigned avail)\r\n{\r\nstruct iowait *wait, *nw;\r\nstruct iowait *waits[SDMA_WAIT_BATCH_SIZE];\r\nunsigned i, n = 0, seq;\r\nstruct sdma_txreq *stx;\r\nstruct hfi1_ibdev *dev = &sde->dd->verbs_dev;\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\ndd_dev_err(sde->dd, "CONFIG SDMA(%u) %s:%d %s()\n", sde->this_idx,\r\nslashstrip(__FILE__), __LINE__, __func__);\r\ndd_dev_err(sde->dd, "avail: %u\n", avail);\r\n#endif\r\ndo {\r\nseq = read_seqbegin(&dev->iowait_lock);\r\nif (!list_empty(&sde->dmawait)) {\r\nwrite_seqlock(&dev->iowait_lock);\r\nlist_for_each_entry_safe(\r\nwait,\r\nnw,\r\n&sde->dmawait,\r\nlist) {\r\nu16 num_desc = 0;\r\nif (!wait->wakeup)\r\ncontinue;\r\nif (n == ARRAY_SIZE(waits))\r\nbreak;\r\nif (!list_empty(&wait->tx_head)) {\r\nstx = list_first_entry(\r\n&wait->tx_head,\r\nstruct sdma_txreq,\r\nlist);\r\nnum_desc = stx->num_desc;\r\n}\r\nif (num_desc > avail)\r\nbreak;\r\navail -= num_desc;\r\nlist_del_init(&wait->list);\r\nwaits[n++] = wait;\r\n}\r\nwrite_sequnlock(&dev->iowait_lock);\r\nbreak;\r\n}\r\n} while (read_seqretry(&dev->iowait_lock, seq));\r\nfor (i = 0; i < n; i++)\r\nwaits[i]->wakeup(waits[i], SDMA_AVAIL_REASON);\r\n}\r\nstatic void sdma_make_progress(struct sdma_engine *sde, u64 status)\r\n{\r\nstruct sdma_txreq *txp = NULL;\r\nint progress = 0;\r\nu16 hwhead, swhead;\r\nint idle_check_done = 0;\r\nhwhead = sdma_gethead(sde);\r\nretry:\r\ntxp = get_txhead(sde);\r\nswhead = sde->descq_head & sde->sdma_mask;\r\ntrace_hfi1_sdma_progress(sde, hwhead, swhead, txp);\r\nwhile (swhead != hwhead) {\r\nswhead = ++sde->descq_head & sde->sdma_mask;\r\nif (txp && txp->next_descq_idx == swhead) {\r\nsde->tx_ring[sde->tx_head++ & sde->sdma_mask] = NULL;\r\ncomplete_tx(sde, txp, SDMA_TXREQ_S_OK);\r\ntxp = get_txhead(sde);\r\n}\r\ntrace_hfi1_sdma_progress(sde, hwhead, swhead, txp);\r\nprogress++;\r\n}\r\nif ((status & sde->idle_mask) && !idle_check_done) {\r\nu16 swtail;\r\nswtail = ACCESS_ONCE(sde->descq_tail) & sde->sdma_mask;\r\nif (swtail != hwhead) {\r\nhwhead = (u16)read_sde_csr(sde, SD(HEAD));\r\nidle_check_done = 1;\r\ngoto retry;\r\n}\r\n}\r\nsde->last_status = status;\r\nif (progress)\r\nsdma_desc_avail(sde, sdma_descq_freecnt(sde));\r\n}\r\nvoid sdma_engine_interrupt(struct sdma_engine *sde, u64 status)\r\n{\r\ntrace_hfi1_sdma_engine_interrupt(sde, status);\r\nwrite_seqlock(&sde->head_lock);\r\nsdma_set_desc_cnt(sde, sdma_desct_intr);\r\nif (status & sde->idle_mask)\r\nsde->idle_int_cnt++;\r\nelse if (status & sde->progress_mask)\r\nsde->progress_int_cnt++;\r\nelse if (status & sde->int_mask)\r\nsde->sdma_int_cnt++;\r\nsdma_make_progress(sde, status);\r\nwrite_sequnlock(&sde->head_lock);\r\n}\r\nvoid sdma_engine_error(struct sdma_engine *sde, u64 status)\r\n{\r\nunsigned long flags;\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\ndd_dev_err(sde->dd, "CONFIG SDMA(%u) error status 0x%llx state %s\n",\r\nsde->this_idx,\r\n(unsigned long long)status,\r\nsdma_state_names[sde->state.current_state]);\r\n#endif\r\nspin_lock_irqsave(&sde->tail_lock, flags);\r\nwrite_seqlock(&sde->head_lock);\r\nif (status & ALL_SDMA_ENG_HALT_ERRS)\r\n__sdma_process_event(sde, sdma_event_e60_hw_halted);\r\nif (status & ~SD(ENG_ERR_STATUS_SDMA_HALT_ERR_SMASK)) {\r\ndd_dev_err(sde->dd,\r\n"SDMA (%u) engine error: 0x%llx state %s\n",\r\nsde->this_idx,\r\n(unsigned long long)status,\r\nsdma_state_names[sde->state.current_state]);\r\ndump_sdma_state(sde);\r\n}\r\nwrite_sequnlock(&sde->head_lock);\r\nspin_unlock_irqrestore(&sde->tail_lock, flags);\r\n}\r\nstatic void sdma_sendctrl(struct sdma_engine *sde, unsigned op)\r\n{\r\nu64 set_senddmactrl = 0;\r\nu64 clr_senddmactrl = 0;\r\nunsigned long flags;\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\ndd_dev_err(sde->dd, "CONFIG SDMA(%u) senddmactrl E=%d I=%d H=%d C=%d\n",\r\nsde->this_idx,\r\n(op & SDMA_SENDCTRL_OP_ENABLE) ? 1 : 0,\r\n(op & SDMA_SENDCTRL_OP_INTENABLE) ? 1 : 0,\r\n(op & SDMA_SENDCTRL_OP_HALT) ? 1 : 0,\r\n(op & SDMA_SENDCTRL_OP_CLEANUP) ? 1 : 0);\r\n#endif\r\nif (op & SDMA_SENDCTRL_OP_ENABLE)\r\nset_senddmactrl |= SD(CTRL_SDMA_ENABLE_SMASK);\r\nelse\r\nclr_senddmactrl |= SD(CTRL_SDMA_ENABLE_SMASK);\r\nif (op & SDMA_SENDCTRL_OP_INTENABLE)\r\nset_senddmactrl |= SD(CTRL_SDMA_INT_ENABLE_SMASK);\r\nelse\r\nclr_senddmactrl |= SD(CTRL_SDMA_INT_ENABLE_SMASK);\r\nif (op & SDMA_SENDCTRL_OP_HALT)\r\nset_senddmactrl |= SD(CTRL_SDMA_HALT_SMASK);\r\nelse\r\nclr_senddmactrl |= SD(CTRL_SDMA_HALT_SMASK);\r\nspin_lock_irqsave(&sde->senddmactrl_lock, flags);\r\nsde->p_senddmactrl |= set_senddmactrl;\r\nsde->p_senddmactrl &= ~clr_senddmactrl;\r\nif (op & SDMA_SENDCTRL_OP_CLEANUP)\r\nwrite_sde_csr(sde, SD(CTRL),\r\nsde->p_senddmactrl |\r\nSD(CTRL_SDMA_CLEANUP_SMASK));\r\nelse\r\nwrite_sde_csr(sde, SD(CTRL), sde->p_senddmactrl);\r\nspin_unlock_irqrestore(&sde->senddmactrl_lock, flags);\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\nsdma_dumpstate(sde);\r\n#endif\r\n}\r\nstatic void sdma_setlengen(struct sdma_engine *sde)\r\n{\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\ndd_dev_err(sde->dd, "CONFIG SDMA(%u) %s:%d %s()\n",\r\nsde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\r\n#endif\r\nwrite_sde_csr(sde, SD(LEN_GEN),\r\n(sde->descq_cnt / 64) << SD(LEN_GEN_LENGTH_SHIFT));\r\nwrite_sde_csr(sde, SD(LEN_GEN),\r\n((sde->descq_cnt / 64) << SD(LEN_GEN_LENGTH_SHIFT)) |\r\n(4ULL << SD(LEN_GEN_GENERATION_SHIFT)));\r\n}\r\nstatic inline void sdma_update_tail(struct sdma_engine *sde, u16 tail)\r\n{\r\nsmp_wmb();\r\nwriteq(tail, sde->tail_csr);\r\n}\r\nstatic void sdma_hw_start_up(struct sdma_engine *sde)\r\n{\r\nu64 reg;\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\ndd_dev_err(sde->dd, "CONFIG SDMA(%u) %s:%d %s()\n",\r\nsde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\r\n#endif\r\nsdma_setlengen(sde);\r\nsdma_update_tail(sde, 0);\r\n*sde->head_dma = 0;\r\nreg = SD(ENG_ERR_CLEAR_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_MASK) <<\r\nSD(ENG_ERR_CLEAR_SDMA_HEADER_REQUEST_FIFO_UNC_ERR_SHIFT);\r\nwrite_sde_csr(sde, SD(ENG_ERR_CLEAR), reg);\r\n}\r\nstatic void set_sdma_integrity(struct sdma_engine *sde)\r\n{\r\nstruct hfi1_devdata *dd = sde->dd;\r\nu64 reg;\r\nif (unlikely(HFI1_CAP_IS_KSET(NO_INTEGRITY)))\r\nreturn;\r\nreg = hfi1_pkt_base_sdma_integrity(dd);\r\nif (HFI1_CAP_IS_KSET(STATIC_RATE_CTRL))\r\nCLEAR_STATIC_RATE_CONTROL_SMASK(reg);\r\nelse\r\nSET_STATIC_RATE_CONTROL_SMASK(reg);\r\nwrite_sde_csr(sde, SD(CHECK_ENABLE), reg);\r\n}\r\nstatic void init_sdma_regs(\r\nstruct sdma_engine *sde,\r\nu32 credits,\r\nuint idle_cnt)\r\n{\r\nu8 opval, opmask;\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\nstruct hfi1_devdata *dd = sde->dd;\r\ndd_dev_err(dd, "CONFIG SDMA(%u) %s:%d %s()\n",\r\nsde->this_idx, slashstrip(__FILE__), __LINE__, __func__);\r\n#endif\r\nwrite_sde_csr(sde, SD(BASE_ADDR), sde->descq_phys);\r\nsdma_setlengen(sde);\r\nsdma_update_tail(sde, 0);\r\nwrite_sde_csr(sde, SD(RELOAD_CNT), idle_cnt);\r\nwrite_sde_csr(sde, SD(DESC_CNT), 0);\r\nwrite_sde_csr(sde, SD(HEAD_ADDR), sde->head_phys);\r\nwrite_sde_csr(sde, SD(MEMORY),\r\n((u64)credits << SD(MEMORY_SDMA_MEMORY_CNT_SHIFT)) |\r\n((u64)(credits * sde->this_idx) <<\r\nSD(MEMORY_SDMA_MEMORY_INDEX_SHIFT)));\r\nwrite_sde_csr(sde, SD(ENG_ERR_MASK), ~0ull);\r\nset_sdma_integrity(sde);\r\nopmask = OPCODE_CHECK_MASK_DISABLED;\r\nopval = OPCODE_CHECK_VAL_DISABLED;\r\nwrite_sde_csr(sde, SD(CHECK_OPCODE),\r\n(opmask << SEND_CTXT_CHECK_OPCODE_MASK_SHIFT) |\r\n(opval << SEND_CTXT_CHECK_OPCODE_VALUE_SHIFT));\r\n}\r\nvoid sdma_dumpstate(struct sdma_engine *sde)\r\n{\r\nu64 csr;\r\nunsigned i;\r\nsdma_dumpstate_helper(SD(CTRL));\r\nsdma_dumpstate_helper(SD(STATUS));\r\nsdma_dumpstate_helper0(SD(ERR_STATUS));\r\nsdma_dumpstate_helper0(SD(ERR_MASK));\r\nsdma_dumpstate_helper(SD(ENG_ERR_STATUS));\r\nsdma_dumpstate_helper(SD(ENG_ERR_MASK));\r\nfor (i = 0; i < CCE_NUM_INT_CSRS; ++i) {\r\nsdma_dumpstate_helper2(CCE_INT_STATUS);\r\nsdma_dumpstate_helper2(CCE_INT_MASK);\r\nsdma_dumpstate_helper2(CCE_INT_BLOCKED);\r\n}\r\nsdma_dumpstate_helper(SD(TAIL));\r\nsdma_dumpstate_helper(SD(HEAD));\r\nsdma_dumpstate_helper(SD(PRIORITY_THLD));\r\nsdma_dumpstate_helper(SD(IDLE_CNT));\r\nsdma_dumpstate_helper(SD(RELOAD_CNT));\r\nsdma_dumpstate_helper(SD(DESC_CNT));\r\nsdma_dumpstate_helper(SD(DESC_FETCHED_CNT));\r\nsdma_dumpstate_helper(SD(MEMORY));\r\nsdma_dumpstate_helper0(SD(ENGINES));\r\nsdma_dumpstate_helper0(SD(MEM_SIZE));\r\nsdma_dumpstate_helper(SD(BASE_ADDR));\r\nsdma_dumpstate_helper(SD(LEN_GEN));\r\nsdma_dumpstate_helper(SD(HEAD_ADDR));\r\nsdma_dumpstate_helper(SD(CHECK_ENABLE));\r\nsdma_dumpstate_helper(SD(CHECK_VL));\r\nsdma_dumpstate_helper(SD(CHECK_JOB_KEY));\r\nsdma_dumpstate_helper(SD(CHECK_PARTITION_KEY));\r\nsdma_dumpstate_helper(SD(CHECK_SLID));\r\nsdma_dumpstate_helper(SD(CHECK_OPCODE));\r\n}\r\nstatic void dump_sdma_state(struct sdma_engine *sde)\r\n{\r\nstruct hw_sdma_desc *descq;\r\nstruct hw_sdma_desc *descqp;\r\nu64 desc[2];\r\nu64 addr;\r\nu8 gen;\r\nu16 len;\r\nu16 head, tail, cnt;\r\nhead = sde->descq_head & sde->sdma_mask;\r\ntail = sde->descq_tail & sde->sdma_mask;\r\ncnt = sdma_descq_freecnt(sde);\r\ndescq = sde->descq;\r\ndd_dev_err(sde->dd,\r\n"SDMA (%u) descq_head: %u descq_tail: %u freecnt: %u FLE %d\n",\r\nsde->this_idx, head, tail, cnt,\r\n!list_empty(&sde->flushlist));\r\nwhile (head != tail) {\r\nchar flags[6] = { 'x', 'x', 'x', 'x', 0 };\r\ndescqp = &sde->descq[head];\r\ndesc[0] = le64_to_cpu(descqp->qw[0]);\r\ndesc[1] = le64_to_cpu(descqp->qw[1]);\r\nflags[0] = (desc[1] & SDMA_DESC1_INT_REQ_FLAG) ? 'I' : '-';\r\nflags[1] = (desc[1] & SDMA_DESC1_HEAD_TO_HOST_FLAG) ?\r\n'H' : '-';\r\nflags[2] = (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG) ? 'F' : '-';\r\nflags[3] = (desc[0] & SDMA_DESC0_LAST_DESC_FLAG) ? 'L' : '-';\r\naddr = (desc[0] >> SDMA_DESC0_PHY_ADDR_SHIFT)\r\n& SDMA_DESC0_PHY_ADDR_MASK;\r\ngen = (desc[1] >> SDMA_DESC1_GENERATION_SHIFT)\r\n& SDMA_DESC1_GENERATION_MASK;\r\nlen = (desc[0] >> SDMA_DESC0_BYTE_COUNT_SHIFT)\r\n& SDMA_DESC0_BYTE_COUNT_MASK;\r\ndd_dev_err(sde->dd,\r\n"SDMA sdmadesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes\n",\r\nhead, flags, addr, gen, len);\r\ndd_dev_err(sde->dd,\r\n"\tdesc0:0x%016llx desc1 0x%016llx\n",\r\ndesc[0], desc[1]);\r\nif (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG)\r\ndd_dev_err(sde->dd,\r\n"\taidx: %u amode: %u alen: %u\n",\r\n(u8)((desc[1] &\r\nSDMA_DESC1_HEADER_INDEX_SMASK) >>\r\nSDMA_DESC1_HEADER_INDEX_SHIFT),\r\n(u8)((desc[1] &\r\nSDMA_DESC1_HEADER_MODE_SMASK) >>\r\nSDMA_DESC1_HEADER_MODE_SHIFT),\r\n(u8)((desc[1] &\r\nSDMA_DESC1_HEADER_DWS_SMASK) >>\r\nSDMA_DESC1_HEADER_DWS_SHIFT));\r\nhead++;\r\nhead &= sde->sdma_mask;\r\n}\r\n}\r\nvoid sdma_seqfile_dump_sde(struct seq_file *s, struct sdma_engine *sde)\r\n{\r\nu16 head, tail;\r\nstruct hw_sdma_desc *descqp;\r\nu64 desc[2];\r\nu64 addr;\r\nu8 gen;\r\nu16 len;\r\nhead = sde->descq_head & sde->sdma_mask;\r\ntail = ACCESS_ONCE(sde->descq_tail) & sde->sdma_mask;\r\nseq_printf(s, SDE_FMT, sde->this_idx,\r\nsde->cpu,\r\nsdma_state_name(sde->state.current_state),\r\n(unsigned long long)read_sde_csr(sde, SD(CTRL)),\r\n(unsigned long long)read_sde_csr(sde, SD(STATUS)),\r\n(unsigned long long)read_sde_csr(sde, SD(ENG_ERR_STATUS)),\r\n(unsigned long long)read_sde_csr(sde, SD(TAIL)), tail,\r\n(unsigned long long)read_sde_csr(sde, SD(HEAD)), head,\r\n(unsigned long long)le64_to_cpu(*sde->head_dma),\r\n(unsigned long long)read_sde_csr(sde, SD(MEMORY)),\r\n(unsigned long long)read_sde_csr(sde, SD(LEN_GEN)),\r\n(unsigned long long)read_sde_csr(sde, SD(RELOAD_CNT)),\r\n(unsigned long long)sde->last_status,\r\n(unsigned long long)sde->ahg_bits,\r\nsde->tx_tail,\r\nsde->tx_head,\r\nsde->descq_tail,\r\nsde->descq_head,\r\n!list_empty(&sde->flushlist),\r\nsde->descq_full_count,\r\n(unsigned long long)read_sde_csr(sde, SEND_DMA_CHECK_SLID));\r\nwhile (head != tail) {\r\nchar flags[6] = { 'x', 'x', 'x', 'x', 0 };\r\ndescqp = &sde->descq[head];\r\ndesc[0] = le64_to_cpu(descqp->qw[0]);\r\ndesc[1] = le64_to_cpu(descqp->qw[1]);\r\nflags[0] = (desc[1] & SDMA_DESC1_INT_REQ_FLAG) ? 'I' : '-';\r\nflags[1] = (desc[1] & SDMA_DESC1_HEAD_TO_HOST_FLAG) ?\r\n'H' : '-';\r\nflags[2] = (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG) ? 'F' : '-';\r\nflags[3] = (desc[0] & SDMA_DESC0_LAST_DESC_FLAG) ? 'L' : '-';\r\naddr = (desc[0] >> SDMA_DESC0_PHY_ADDR_SHIFT)\r\n& SDMA_DESC0_PHY_ADDR_MASK;\r\ngen = (desc[1] >> SDMA_DESC1_GENERATION_SHIFT)\r\n& SDMA_DESC1_GENERATION_MASK;\r\nlen = (desc[0] >> SDMA_DESC0_BYTE_COUNT_SHIFT)\r\n& SDMA_DESC0_BYTE_COUNT_MASK;\r\nseq_printf(s,\r\n"\tdesc[%u]: flags:%s addr:0x%016llx gen:%u len:%u bytes\n",\r\nhead, flags, addr, gen, len);\r\nif (desc[0] & SDMA_DESC0_FIRST_DESC_FLAG)\r\nseq_printf(s, "\t\tahgidx: %u ahgmode: %u\n",\r\n(u8)((desc[1] &\r\nSDMA_DESC1_HEADER_INDEX_SMASK) >>\r\nSDMA_DESC1_HEADER_INDEX_SHIFT),\r\n(u8)((desc[1] &\r\nSDMA_DESC1_HEADER_MODE_SMASK) >>\r\nSDMA_DESC1_HEADER_MODE_SHIFT));\r\nhead = (head + 1) & sde->sdma_mask;\r\n}\r\n}\r\nstatic inline u64 add_gen(struct sdma_engine *sde, u64 qw1)\r\n{\r\nu8 generation = (sde->descq_tail >> sde->sdma_shift) & 3;\r\nqw1 &= ~SDMA_DESC1_GENERATION_SMASK;\r\nqw1 |= ((u64)generation & SDMA_DESC1_GENERATION_MASK)\r\n<< SDMA_DESC1_GENERATION_SHIFT;\r\nreturn qw1;\r\n}\r\nstatic inline u16 submit_tx(struct sdma_engine *sde, struct sdma_txreq *tx)\r\n{\r\nint i;\r\nu16 tail;\r\nstruct sdma_desc *descp = tx->descp;\r\nu8 skip = 0, mode = ahg_mode(tx);\r\ntail = sde->descq_tail & sde->sdma_mask;\r\nsde->descq[tail].qw[0] = cpu_to_le64(descp->qw[0]);\r\nsde->descq[tail].qw[1] = cpu_to_le64(add_gen(sde, descp->qw[1]));\r\ntrace_hfi1_sdma_descriptor(sde, descp->qw[0], descp->qw[1],\r\ntail, &sde->descq[tail]);\r\ntail = ++sde->descq_tail & sde->sdma_mask;\r\ndescp++;\r\nif (mode > SDMA_AHG_APPLY_UPDATE1)\r\nskip = mode >> 1;\r\nfor (i = 1; i < tx->num_desc; i++, descp++) {\r\nu64 qw1;\r\nsde->descq[tail].qw[0] = cpu_to_le64(descp->qw[0]);\r\nif (skip) {\r\nqw1 = descp->qw[1];\r\nskip--;\r\n} else {\r\nqw1 = add_gen(sde, descp->qw[1]);\r\n}\r\nsde->descq[tail].qw[1] = cpu_to_le64(qw1);\r\ntrace_hfi1_sdma_descriptor(sde, descp->qw[0], qw1,\r\ntail, &sde->descq[tail]);\r\ntail = ++sde->descq_tail & sde->sdma_mask;\r\n}\r\ntx->next_descq_idx = tail;\r\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\r\ntx->sn = sde->tail_sn++;\r\ntrace_hfi1_sdma_in_sn(sde, tx->sn);\r\nWARN_ON_ONCE(sde->tx_ring[sde->tx_tail & sde->sdma_mask]);\r\n#endif\r\nsde->tx_ring[sde->tx_tail++ & sde->sdma_mask] = tx;\r\nsde->desc_avail -= tx->num_desc;\r\nreturn tail;\r\n}\r\nstatic int sdma_check_progress(\r\nstruct sdma_engine *sde,\r\nstruct iowait *wait,\r\nstruct sdma_txreq *tx)\r\n{\r\nint ret;\r\nsde->desc_avail = sdma_descq_freecnt(sde);\r\nif (tx->num_desc <= sde->desc_avail)\r\nreturn -EAGAIN;\r\nif (wait && wait->sleep) {\r\nunsigned seq;\r\nseq = raw_seqcount_begin(\r\n(const seqcount_t *)&sde->head_lock.seqcount);\r\nret = wait->sleep(sde, wait, tx, seq);\r\nif (ret == -EAGAIN)\r\nsde->desc_avail = sdma_descq_freecnt(sde);\r\n} else {\r\nret = -EBUSY;\r\n}\r\nreturn ret;\r\n}\r\nint sdma_send_txreq(struct sdma_engine *sde,\r\nstruct iowait *wait,\r\nstruct sdma_txreq *tx)\r\n{\r\nint ret = 0;\r\nu16 tail;\r\nunsigned long flags;\r\nif (unlikely(tx->tlen))\r\nreturn -EINVAL;\r\ntx->wait = wait;\r\nspin_lock_irqsave(&sde->tail_lock, flags);\r\nretry:\r\nif (unlikely(!__sdma_running(sde)))\r\ngoto unlock_noconn;\r\nif (unlikely(tx->num_desc > sde->desc_avail))\r\ngoto nodesc;\r\ntail = submit_tx(sde, tx);\r\nif (wait)\r\niowait_sdma_inc(wait);\r\nsdma_update_tail(sde, tail);\r\nunlock:\r\nspin_unlock_irqrestore(&sde->tail_lock, flags);\r\nreturn ret;\r\nunlock_noconn:\r\nif (wait)\r\niowait_sdma_inc(wait);\r\ntx->next_descq_idx = 0;\r\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\r\ntx->sn = sde->tail_sn++;\r\ntrace_hfi1_sdma_in_sn(sde, tx->sn);\r\n#endif\r\nspin_lock(&sde->flushlist_lock);\r\nlist_add_tail(&tx->list, &sde->flushlist);\r\nspin_unlock(&sde->flushlist_lock);\r\nif (wait) {\r\nwait->tx_count++;\r\nwait->count += tx->num_desc;\r\n}\r\nschedule_work(&sde->flush_worker);\r\nret = -ECOMM;\r\ngoto unlock;\r\nnodesc:\r\nret = sdma_check_progress(sde, wait, tx);\r\nif (ret == -EAGAIN) {\r\nret = 0;\r\ngoto retry;\r\n}\r\nsde->descq_full_count++;\r\ngoto unlock;\r\n}\r\nint sdma_send_txlist(struct sdma_engine *sde, struct iowait *wait,\r\nstruct list_head *tx_list)\r\n{\r\nstruct sdma_txreq *tx, *tx_next;\r\nint ret = 0;\r\nunsigned long flags;\r\nu16 tail = INVALID_TAIL;\r\nint count = 0;\r\nspin_lock_irqsave(&sde->tail_lock, flags);\r\nretry:\r\nlist_for_each_entry_safe(tx, tx_next, tx_list, list) {\r\ntx->wait = wait;\r\nif (unlikely(!__sdma_running(sde)))\r\ngoto unlock_noconn;\r\nif (unlikely(tx->num_desc > sde->desc_avail))\r\ngoto nodesc;\r\nif (unlikely(tx->tlen)) {\r\nret = -EINVAL;\r\ngoto update_tail;\r\n}\r\nlist_del_init(&tx->list);\r\ntail = submit_tx(sde, tx);\r\ncount++;\r\nif (tail != INVALID_TAIL &&\r\n(count & SDMA_TAIL_UPDATE_THRESH) == 0) {\r\nsdma_update_tail(sde, tail);\r\ntail = INVALID_TAIL;\r\n}\r\n}\r\nupdate_tail:\r\nif (wait)\r\niowait_sdma_add(wait, count);\r\nif (tail != INVALID_TAIL)\r\nsdma_update_tail(sde, tail);\r\nspin_unlock_irqrestore(&sde->tail_lock, flags);\r\nreturn ret == 0 ? count : ret;\r\nunlock_noconn:\r\nspin_lock(&sde->flushlist_lock);\r\nlist_for_each_entry_safe(tx, tx_next, tx_list, list) {\r\ntx->wait = wait;\r\nlist_del_init(&tx->list);\r\nif (wait)\r\niowait_sdma_inc(wait);\r\ntx->next_descq_idx = 0;\r\n#ifdef CONFIG_HFI1_DEBUG_SDMA_ORDER\r\ntx->sn = sde->tail_sn++;\r\ntrace_hfi1_sdma_in_sn(sde, tx->sn);\r\n#endif\r\nlist_add_tail(&tx->list, &sde->flushlist);\r\nif (wait) {\r\nwait->tx_count++;\r\nwait->count += tx->num_desc;\r\n}\r\n}\r\nspin_unlock(&sde->flushlist_lock);\r\nschedule_work(&sde->flush_worker);\r\nret = -ECOMM;\r\ngoto update_tail;\r\nnodesc:\r\nret = sdma_check_progress(sde, wait, tx);\r\nif (ret == -EAGAIN) {\r\nret = 0;\r\ngoto retry;\r\n}\r\nsde->descq_full_count++;\r\ngoto update_tail;\r\n}\r\nstatic void sdma_process_event(struct sdma_engine *sde, enum sdma_events event)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&sde->tail_lock, flags);\r\nwrite_seqlock(&sde->head_lock);\r\n__sdma_process_event(sde, event);\r\nif (sde->state.current_state == sdma_state_s99_running)\r\nsdma_desc_avail(sde, sdma_descq_freecnt(sde));\r\nwrite_sequnlock(&sde->head_lock);\r\nspin_unlock_irqrestore(&sde->tail_lock, flags);\r\n}\r\nstatic void __sdma_process_event(struct sdma_engine *sde,\r\nenum sdma_events event)\r\n{\r\nstruct sdma_state *ss = &sde->state;\r\nint need_progress = 0;\r\n#ifdef CONFIG_SDMA_VERBOSITY\r\ndd_dev_err(sde->dd, "CONFIG SDMA(%u) [%s] %s\n", sde->this_idx,\r\nsdma_state_names[ss->current_state],\r\nsdma_event_names[event]);\r\n#endif\r\nswitch (ss->current_state) {\r\ncase sdma_state_s00_hw_down:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\ncase sdma_event_e10_go_hw_start:\r\nsdma_get(&sde->state);\r\nsdma_set_state(sde,\r\nsdma_state_s10_hw_start_up_halt_wait);\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nsdma_sw_tear_down(sde);\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s10_hw_start_up_halt_wait:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\nsdma_sw_tear_down(sde);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nsdma_set_state(sde,\r\nsdma_state_s15_hw_start_up_clean_wait);\r\nsdma_start_hw_clean_up(sde);\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nschedule_work(&sde->err_halt_worker);\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s15_hw_start_up_clean_wait:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\nsdma_sw_tear_down(sde);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nsdma_hw_start_up(sde);\r\nsdma_set_state(sde, ss->go_s99_running ?\r\nsdma_state_s99_running :\r\nsdma_state_s20_idle);\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s20_idle:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\nsdma_sw_tear_down(sde);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nsdma_set_state(sde, sdma_state_s99_running);\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nsdma_set_state(sde, sdma_state_s50_hw_halt_wait);\r\nschedule_work(&sde->err_halt_worker);\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\ncase sdma_event_e80_hw_freeze:\r\nsdma_set_state(sde, sdma_state_s80_hw_freeze);\r\natomic_dec(&sde->dd->sdma_unfreeze_count);\r\nwake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s30_sw_clean_up_wait:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nsdma_set_state(sde, sdma_state_s40_hw_clean_up_wait);\r\nsdma_start_hw_clean_up(sde);\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s40_hw_clean_up_wait:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nsdma_hw_start_up(sde);\r\nsdma_set_state(sde, ss->go_s99_running ?\r\nsdma_state_s99_running :\r\nsdma_state_s20_idle);\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s50_hw_halt_wait:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nsdma_set_state(sde, sdma_state_s30_sw_clean_up_wait);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nschedule_work(&sde->err_halt_worker);\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s60_idle_halt_wait:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nsdma_set_state(sde, sdma_state_s30_sw_clean_up_wait);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nschedule_work(&sde->err_halt_worker);\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s80_hw_freeze:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nsdma_set_state(sde, sdma_state_s82_freeze_sw_clean);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s82_freeze_sw_clean:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nss->go_s99_running = 1;\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\natomic_dec(&sde->dd->sdma_unfreeze_count);\r\nwake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nss->go_s99_running = 0;\r\nbreak;\r\ncase sdma_event_e80_hw_freeze:\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nsdma_hw_start_up(sde);\r\nsdma_set_state(sde, ss->go_s99_running ?\r\nsdma_state_s99_running :\r\nsdma_state_s20_idle);\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nbreak;\r\ncase sdma_event_e90_sw_halted:\r\nbreak;\r\n}\r\nbreak;\r\ncase sdma_state_s99_running:\r\nswitch (event) {\r\ncase sdma_event_e00_go_hw_down:\r\nsdma_set_state(sde, sdma_state_s00_hw_down);\r\ntasklet_hi_schedule(&sde->sdma_sw_clean_up_task);\r\nbreak;\r\ncase sdma_event_e10_go_hw_start:\r\nbreak;\r\ncase sdma_event_e15_hw_halt_done:\r\nbreak;\r\ncase sdma_event_e25_hw_clean_up_done:\r\nbreak;\r\ncase sdma_event_e30_go_running:\r\nbreak;\r\ncase sdma_event_e40_sw_cleaned:\r\nbreak;\r\ncase sdma_event_e50_hw_cleaned:\r\nbreak;\r\ncase sdma_event_e60_hw_halted:\r\nneed_progress = 1;\r\nsdma_err_progress_check_schedule(sde);\r\ncase sdma_event_e90_sw_halted:\r\nsdma_set_state(sde, sdma_state_s50_hw_halt_wait);\r\nschedule_work(&sde->err_halt_worker);\r\nbreak;\r\ncase sdma_event_e70_go_idle:\r\nsdma_set_state(sde, sdma_state_s60_idle_halt_wait);\r\nbreak;\r\ncase sdma_event_e85_link_down:\r\nss->go_s99_running = 0;\r\ncase sdma_event_e80_hw_freeze:\r\nsdma_set_state(sde, sdma_state_s80_hw_freeze);\r\natomic_dec(&sde->dd->sdma_unfreeze_count);\r\nwake_up_interruptible(&sde->dd->sdma_unfreeze_wq);\r\nbreak;\r\ncase sdma_event_e81_hw_frozen:\r\nbreak;\r\ncase sdma_event_e82_hw_unfreeze:\r\nbreak;\r\n}\r\nbreak;\r\n}\r\nss->last_event = event;\r\nif (need_progress)\r\nsdma_make_progress(sde, 0);\r\n}\r\nstatic int _extend_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx)\r\n{\r\nint i;\r\nif (unlikely((tx->num_desc == (MAX_DESC - 1)))) {\r\nif (!tx->tlen) {\r\ntx->desc_limit = MAX_DESC;\r\n} else if (!tx->coalesce_buf) {\r\ntx->coalesce_buf = kmalloc(tx->tlen + sizeof(u32),\r\nGFP_ATOMIC);\r\nif (!tx->coalesce_buf)\r\ngoto enomem;\r\ntx->coalesce_idx = 0;\r\n}\r\nreturn 0;\r\n}\r\nif (unlikely(tx->num_desc == MAX_DESC))\r\ngoto enomem;\r\ntx->descp = kmalloc_array(\r\nMAX_DESC,\r\nsizeof(struct sdma_desc),\r\nGFP_ATOMIC);\r\nif (!tx->descp)\r\ngoto enomem;\r\ntx->desc_limit = MAX_DESC - 1;\r\nfor (i = 0; i < tx->num_desc; i++)\r\ntx->descp[i] = tx->descs[i];\r\nreturn 0;\r\nenomem:\r\nsdma_txclean(dd, tx);\r\nreturn -ENOMEM;\r\n}\r\nint ext_coal_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx,\r\nint type, void *kvaddr, struct page *page,\r\nunsigned long offset, u16 len)\r\n{\r\nint pad_len, rval;\r\ndma_addr_t addr;\r\nrval = _extend_sdma_tx_descs(dd, tx);\r\nif (rval) {\r\nsdma_txclean(dd, tx);\r\nreturn rval;\r\n}\r\nif (tx->coalesce_buf) {\r\nif (type == SDMA_MAP_NONE) {\r\nsdma_txclean(dd, tx);\r\nreturn -EINVAL;\r\n}\r\nif (type == SDMA_MAP_PAGE) {\r\nkvaddr = kmap(page);\r\nkvaddr += offset;\r\n} else if (WARN_ON(!kvaddr)) {\r\nsdma_txclean(dd, tx);\r\nreturn -EINVAL;\r\n}\r\nmemcpy(tx->coalesce_buf + tx->coalesce_idx, kvaddr, len);\r\ntx->coalesce_idx += len;\r\nif (type == SDMA_MAP_PAGE)\r\nkunmap(page);\r\nif (tx->tlen - tx->coalesce_idx)\r\nreturn 0;\r\npad_len = tx->packet_len & (sizeof(u32) - 1);\r\nif (pad_len) {\r\npad_len = sizeof(u32) - pad_len;\r\nmemset(tx->coalesce_buf + tx->coalesce_idx, 0, pad_len);\r\ntx->packet_len += pad_len;\r\ntx->tlen += pad_len;\r\n}\r\naddr = dma_map_single(&dd->pcidev->dev,\r\ntx->coalesce_buf,\r\ntx->tlen,\r\nDMA_TO_DEVICE);\r\nif (unlikely(dma_mapping_error(&dd->pcidev->dev, addr))) {\r\nsdma_txclean(dd, tx);\r\nreturn -ENOSPC;\r\n}\r\ntx->desc_limit = MAX_DESC;\r\nreturn _sdma_txadd_daddr(dd, SDMA_MAP_SINGLE, tx,\r\naddr, tx->tlen);\r\n}\r\nreturn 1;\r\n}\r\nvoid sdma_update_lmc(struct hfi1_devdata *dd, u64 mask, u32 lid)\r\n{\r\nstruct sdma_engine *sde;\r\nint i;\r\nu64 sreg;\r\nsreg = ((mask & SD(CHECK_SLID_MASK_MASK)) <<\r\nSD(CHECK_SLID_MASK_SHIFT)) |\r\n(((lid & mask) & SD(CHECK_SLID_VALUE_MASK)) <<\r\nSD(CHECK_SLID_VALUE_SHIFT));\r\nfor (i = 0; i < dd->num_sdma; i++) {\r\nhfi1_cdbg(LINKVERB, "SendDmaEngine[%d].SLID_CHECK = 0x%x",\r\ni, (u32)sreg);\r\nsde = &dd->per_sdma[i];\r\nwrite_sde_csr(sde, SD(CHECK_SLID), sreg);\r\n}\r\n}\r\nint _pad_sdma_tx_descs(struct hfi1_devdata *dd, struct sdma_txreq *tx)\r\n{\r\nint rval = 0;\r\ntx->num_desc++;\r\nif ((unlikely(tx->num_desc == tx->desc_limit))) {\r\nrval = _extend_sdma_tx_descs(dd, tx);\r\nif (rval) {\r\nsdma_txclean(dd, tx);\r\nreturn rval;\r\n}\r\n}\r\nmake_tx_sdma_desc(\r\ntx,\r\nSDMA_MAP_NONE,\r\ndd->sdma_pad_phys,\r\nsizeof(u32) - (tx->packet_len & (sizeof(u32) - 1)));\r\n_sdma_close_tx(dd, tx);\r\nreturn rval;\r\n}\r\nvoid _sdma_txreq_ahgadd(\r\nstruct sdma_txreq *tx,\r\nu8 num_ahg,\r\nu8 ahg_entry,\r\nu32 *ahg,\r\nu8 ahg_hlen)\r\n{\r\nu32 i, shift = 0, desc = 0;\r\nu8 mode;\r\nWARN_ON_ONCE(num_ahg > 9 || (ahg_hlen & 3) || ahg_hlen == 4);\r\nif (num_ahg == 1)\r\nmode = SDMA_AHG_APPLY_UPDATE1;\r\nelse if (num_ahg <= 5)\r\nmode = SDMA_AHG_APPLY_UPDATE2;\r\nelse\r\nmode = SDMA_AHG_APPLY_UPDATE3;\r\ntx->num_desc++;\r\nswitch (mode) {\r\ncase SDMA_AHG_APPLY_UPDATE3:\r\ntx->num_desc++;\r\ntx->descs[2].qw[0] = 0;\r\ntx->descs[2].qw[1] = 0;\r\ncase SDMA_AHG_APPLY_UPDATE2:\r\ntx->num_desc++;\r\ntx->descs[1].qw[0] = 0;\r\ntx->descs[1].qw[1] = 0;\r\nbreak;\r\n}\r\nahg_hlen >>= 2;\r\ntx->descs[0].qw[1] |=\r\n(((u64)ahg_entry & SDMA_DESC1_HEADER_INDEX_MASK)\r\n<< SDMA_DESC1_HEADER_INDEX_SHIFT) |\r\n(((u64)ahg_hlen & SDMA_DESC1_HEADER_DWS_MASK)\r\n<< SDMA_DESC1_HEADER_DWS_SHIFT) |\r\n(((u64)mode & SDMA_DESC1_HEADER_MODE_MASK)\r\n<< SDMA_DESC1_HEADER_MODE_SHIFT) |\r\n(((u64)ahg[0] & SDMA_DESC1_HEADER_UPDATE1_MASK)\r\n<< SDMA_DESC1_HEADER_UPDATE1_SHIFT);\r\nfor (i = 0; i < (num_ahg - 1); i++) {\r\nif (!shift && !(i & 2))\r\ndesc++;\r\ntx->descs[desc].qw[!!(i & 2)] |=\r\n(((u64)ahg[i + 1])\r\n<< shift);\r\nshift = (shift + 32) & 63;\r\n}\r\n}\r\nint sdma_ahg_alloc(struct sdma_engine *sde)\r\n{\r\nint nr;\r\nint oldbit;\r\nif (!sde) {\r\ntrace_hfi1_ahg_allocate(sde, -EINVAL);\r\nreturn -EINVAL;\r\n}\r\nwhile (1) {\r\nnr = ffz(ACCESS_ONCE(sde->ahg_bits));\r\nif (nr > 31) {\r\ntrace_hfi1_ahg_allocate(sde, -ENOSPC);\r\nreturn -ENOSPC;\r\n}\r\noldbit = test_and_set_bit(nr, &sde->ahg_bits);\r\nif (!oldbit)\r\nbreak;\r\ncpu_relax();\r\n}\r\ntrace_hfi1_ahg_allocate(sde, nr);\r\nreturn nr;\r\n}\r\nvoid sdma_ahg_free(struct sdma_engine *sde, int ahg_index)\r\n{\r\nif (!sde)\r\nreturn;\r\ntrace_hfi1_ahg_deallocate(sde, ahg_index);\r\nif (ahg_index < 0 || ahg_index > 31)\r\nreturn;\r\nclear_bit(ahg_index, &sde->ahg_bits);\r\n}\r\nvoid sdma_freeze_notify(struct hfi1_devdata *dd, int link_down)\r\n{\r\nint i;\r\nenum sdma_events event = link_down ? sdma_event_e85_link_down :\r\nsdma_event_e80_hw_freeze;\r\natomic_set(&dd->sdma_unfreeze_count, dd->num_sdma);\r\nfor (i = 0; i < dd->num_sdma; i++)\r\nsdma_process_event(&dd->per_sdma[i], event);\r\n}\r\nvoid sdma_freeze(struct hfi1_devdata *dd)\r\n{\r\nint i;\r\nint ret;\r\nret = wait_event_interruptible(dd->sdma_unfreeze_wq,\r\natomic_read(&dd->sdma_unfreeze_count) <=\r\n0);\r\nif (ret || atomic_read(&dd->sdma_unfreeze_count) < 0)\r\nreturn;\r\natomic_set(&dd->sdma_unfreeze_count, dd->num_sdma);\r\nfor (i = 0; i < dd->num_sdma; i++)\r\nsdma_process_event(&dd->per_sdma[i], sdma_event_e81_hw_frozen);\r\n(void)wait_event_interruptible(dd->sdma_unfreeze_wq,\r\natomic_read(&dd->sdma_unfreeze_count) <= 0);\r\n}\r\nvoid sdma_unfreeze(struct hfi1_devdata *dd)\r\n{\r\nint i;\r\nfor (i = 0; i < dd->num_sdma; i++)\r\nsdma_process_event(&dd->per_sdma[i],\r\nsdma_event_e82_hw_unfreeze);\r\n}\r\nvoid _sdma_engine_progress_schedule(\r\nstruct sdma_engine *sde)\r\n{\r\ntrace_hfi1_sdma_engine_progress(sde, sde->progress_mask);\r\nwrite_csr(sde->dd,\r\nCCE_INT_FORCE + (8 * (IS_SDMA_START / 64)),\r\nsde->progress_mask);\r\n}
