int __meminit hash__vmemmap_create_mapping(unsigned long start,\r\nunsigned long page_size,\r\nunsigned long phys)\r\n{\r\nint rc = htab_bolt_mapping(start, start + page_size, phys,\r\npgprot_val(PAGE_KERNEL),\r\nmmu_vmemmap_psize, mmu_kernel_ssize);\r\nif (rc < 0) {\r\nint rc2 = htab_remove_mapping(start, start + page_size,\r\nmmu_vmemmap_psize,\r\nmmu_kernel_ssize);\r\nBUG_ON(rc2 && (rc2 != -ENOENT));\r\n}\r\nreturn rc;\r\n}\r\nvoid hash__vmemmap_remove_mapping(unsigned long start,\r\nunsigned long page_size)\r\n{\r\nint rc = htab_remove_mapping(start, start + page_size,\r\nmmu_vmemmap_psize,\r\nmmu_kernel_ssize);\r\nBUG_ON((rc < 0) && (rc != -ENOENT));\r\nWARN_ON(rc == -ENOENT);\r\n}\r\nint hash__map_kernel_page(unsigned long ea, unsigned long pa, unsigned long flags)\r\n{\r\npgd_t *pgdp;\r\npud_t *pudp;\r\npmd_t *pmdp;\r\npte_t *ptep;\r\nBUILD_BUG_ON(TASK_SIZE_USER64 > H_PGTABLE_RANGE);\r\nif (slab_is_available()) {\r\npgdp = pgd_offset_k(ea);\r\npudp = pud_alloc(&init_mm, pgdp, ea);\r\nif (!pudp)\r\nreturn -ENOMEM;\r\npmdp = pmd_alloc(&init_mm, pudp, ea);\r\nif (!pmdp)\r\nreturn -ENOMEM;\r\nptep = pte_alloc_kernel(pmdp, ea);\r\nif (!ptep)\r\nreturn -ENOMEM;\r\nset_pte_at(&init_mm, ea, ptep, pfn_pte(pa >> PAGE_SHIFT,\r\n__pgprot(flags)));\r\n} else {\r\nif (htab_bolt_mapping(ea, ea + PAGE_SIZE, pa, flags,\r\nmmu_io_psize, mmu_kernel_ssize)) {\r\nprintk(KERN_ERR "Failed to do bolted mapping IO "\r\n"memory at %016lx !\n", pa);\r\nreturn -ENOMEM;\r\n}\r\n}\r\nsmp_wmb();\r\nreturn 0;\r\n}\r\nunsigned long hash__pmd_hugepage_update(struct mm_struct *mm, unsigned long addr,\r\npmd_t *pmdp, unsigned long clr,\r\nunsigned long set)\r\n{\r\n__be64 old_be, tmp;\r\nunsigned long old;\r\n#ifdef CONFIG_DEBUG_VM\r\nWARN_ON(!pmd_trans_huge(*pmdp));\r\nassert_spin_locked(&mm->page_table_lock);\r\n#endif\r\n__asm__ __volatile__(\r\n"1: ldarx %0,0,%3\n\\r\nand. %1,%0,%6\n\\r\nbne- 1b \n\\r\nandc %1,%0,%4 \n\\r\nor %1,%1,%7\n\\r\nstdcx. %1,0,%3 \n\\r\nbne- 1b"\r\n: "=&r" (old_be), "=&r" (tmp), "=m" (*pmdp)\r\n: "r" (pmdp), "r" (cpu_to_be64(clr)), "m" (*pmdp),\r\n"r" (cpu_to_be64(H_PAGE_BUSY)), "r" (cpu_to_be64(set))\r\n: "cc" );\r\nold = be64_to_cpu(old_be);\r\ntrace_hugepage_update(addr, old, clr, set);\r\nif (old & H_PAGE_HASHPTE)\r\nhpte_do_hugepage_flush(mm, addr, pmdp, old);\r\nreturn old;\r\n}\r\npmd_t hash__pmdp_collapse_flush(struct vm_area_struct *vma, unsigned long address,\r\npmd_t *pmdp)\r\n{\r\npmd_t pmd;\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\nVM_BUG_ON(pmd_trans_huge(*pmdp));\r\npmd = *pmdp;\r\npmd_clear(pmdp);\r\nkick_all_cpus_sync();\r\nflush_tlb_pmd_range(vma->vm_mm, &pmd, address);\r\nreturn pmd;\r\n}\r\nvoid hash__pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,\r\npgtable_t pgtable)\r\n{\r\npgtable_t *pgtable_slot;\r\nassert_spin_locked(&mm->page_table_lock);\r\npgtable_slot = (pgtable_t *)pmdp + PTRS_PER_PMD;\r\n*pgtable_slot = pgtable;\r\nsmp_wmb();\r\n}\r\npgtable_t hash__pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)\r\n{\r\npgtable_t pgtable;\r\npgtable_t *pgtable_slot;\r\nassert_spin_locked(&mm->page_table_lock);\r\npgtable_slot = (pgtable_t *)pmdp + PTRS_PER_PMD;\r\npgtable = *pgtable_slot;\r\n*pgtable_slot = NULL;\r\nmemset(pgtable, 0, PTE_FRAG_SIZE);\r\nreturn pgtable;\r\n}\r\nvoid hash__pmdp_huge_split_prepare(struct vm_area_struct *vma,\r\nunsigned long address, pmd_t *pmdp)\r\n{\r\nVM_BUG_ON(address & ~HPAGE_PMD_MASK);\r\nVM_BUG_ON(REGION_ID(address) != USER_REGION_ID);\r\npmd_hugepage_update(vma->vm_mm, address, pmdp, 0, _PAGE_PRIVILEGED);\r\n}\r\nvoid hpte_do_hugepage_flush(struct mm_struct *mm, unsigned long addr,\r\npmd_t *pmdp, unsigned long old_pmd)\r\n{\r\nint ssize;\r\nunsigned int psize;\r\nunsigned long vsid;\r\nunsigned long flags = 0;\r\nconst struct cpumask *tmp;\r\n#ifdef CONFIG_DEBUG_VM\r\npsize = get_slice_psize(mm, addr);\r\nBUG_ON(psize == MMU_PAGE_16M);\r\n#endif\r\nif (old_pmd & H_PAGE_COMBO)\r\npsize = MMU_PAGE_4K;\r\nelse\r\npsize = MMU_PAGE_64K;\r\nif (!is_kernel_addr(addr)) {\r\nssize = user_segment_size(addr);\r\nvsid = get_vsid(mm->context.id, addr, ssize);\r\nWARN_ON(vsid == 0);\r\n} else {\r\nvsid = get_kernel_vsid(addr, mmu_kernel_ssize);\r\nssize = mmu_kernel_ssize;\r\n}\r\ntmp = cpumask_of(smp_processor_id());\r\nif (cpumask_equal(mm_cpumask(mm), tmp))\r\nflags |= HPTE_LOCAL_UPDATE;\r\nreturn flush_hash_hugepage(vsid, addr, pmdp, psize, ssize, flags);\r\n}\r\npmd_t hash__pmdp_huge_get_and_clear(struct mm_struct *mm,\r\nunsigned long addr, pmd_t *pmdp)\r\n{\r\npmd_t old_pmd;\r\npgtable_t pgtable;\r\nunsigned long old;\r\npgtable_t *pgtable_slot;\r\nold = pmd_hugepage_update(mm, addr, pmdp, ~0UL, 0);\r\nold_pmd = __pmd(old);\r\npgtable_slot = (pgtable_t *)pmdp + PTRS_PER_PMD;\r\npgtable = *pgtable_slot;\r\nmemset(pgtable, 0, PTE_FRAG_SIZE);\r\nkick_all_cpus_sync();\r\nreturn old_pmd;\r\n}\r\nint hash__has_transparent_hugepage(void)\r\n{\r\nif (!mmu_has_feature(MMU_FTR_16M_PAGE))\r\nreturn 0;\r\nif (mmu_psize_defs[MMU_PAGE_16M].shift != PMD_SHIFT)\r\nreturn 0;\r\nif (mmu_psize_defs[MMU_PAGE_64K].shift &&\r\n(mmu_psize_defs[MMU_PAGE_64K].penc[MMU_PAGE_16M] == -1))\r\nreturn 0;\r\nif (mmu_psize_defs[MMU_PAGE_4K].penc[MMU_PAGE_16M] == -1)\r\nreturn 0;\r\nreturn 1;\r\n}
