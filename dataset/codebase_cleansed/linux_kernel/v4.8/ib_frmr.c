static struct rds_ib_mr *rds_ib_alloc_frmr(struct rds_ib_device *rds_ibdev,\r\nint npages)\r\n{\r\nstruct rds_ib_mr_pool *pool;\r\nstruct rds_ib_mr *ibmr = NULL;\r\nstruct rds_ib_frmr *frmr;\r\nint err = 0;\r\nif (npages <= RDS_MR_8K_MSG_SIZE)\r\npool = rds_ibdev->mr_8k_pool;\r\nelse\r\npool = rds_ibdev->mr_1m_pool;\r\nibmr = rds_ib_try_reuse_ibmr(pool);\r\nif (ibmr)\r\nreturn ibmr;\r\nibmr = kzalloc_node(sizeof(*ibmr), GFP_KERNEL,\r\nrdsibdev_to_node(rds_ibdev));\r\nif (!ibmr) {\r\nerr = -ENOMEM;\r\ngoto out_no_cigar;\r\n}\r\nfrmr = &ibmr->u.frmr;\r\nfrmr->mr = ib_alloc_mr(rds_ibdev->pd, IB_MR_TYPE_MEM_REG,\r\npool->fmr_attr.max_pages);\r\nif (IS_ERR(frmr->mr)) {\r\npr_warn("RDS/IB: %s failed to allocate MR", __func__);\r\ngoto out_no_cigar;\r\n}\r\nibmr->pool = pool;\r\nif (pool->pool_type == RDS_IB_MR_8K_POOL)\r\nrds_ib_stats_inc(s_ib_rdma_mr_8k_alloc);\r\nelse\r\nrds_ib_stats_inc(s_ib_rdma_mr_1m_alloc);\r\nif (atomic_read(&pool->item_count) > pool->max_items_soft)\r\npool->max_items_soft = pool->max_items;\r\nfrmr->fr_state = FRMR_IS_FREE;\r\nreturn ibmr;\r\nout_no_cigar:\r\nkfree(ibmr);\r\natomic_dec(&pool->item_count);\r\nreturn ERR_PTR(err);\r\n}\r\nstatic void rds_ib_free_frmr(struct rds_ib_mr *ibmr, bool drop)\r\n{\r\nstruct rds_ib_mr_pool *pool = ibmr->pool;\r\nif (drop)\r\nllist_add(&ibmr->llnode, &pool->drop_list);\r\nelse\r\nllist_add(&ibmr->llnode, &pool->free_list);\r\natomic_add(ibmr->sg_len, &pool->free_pinned);\r\natomic_inc(&pool->dirty_count);\r\nif (atomic_read(&pool->free_pinned) >= pool->max_free_pinned ||\r\natomic_read(&pool->dirty_count) >= pool->max_items / 5)\r\nqueue_delayed_work(rds_ib_mr_wq, &pool->flush_worker, 10);\r\n}\r\nstatic int rds_ib_post_reg_frmr(struct rds_ib_mr *ibmr)\r\n{\r\nstruct rds_ib_frmr *frmr = &ibmr->u.frmr;\r\nstruct ib_send_wr *failed_wr;\r\nstruct ib_reg_wr reg_wr;\r\nint ret;\r\nwhile (atomic_dec_return(&ibmr->ic->i_fastreg_wrs) <= 0) {\r\natomic_inc(&ibmr->ic->i_fastreg_wrs);\r\ncpu_relax();\r\n}\r\nret = ib_map_mr_sg_zbva(frmr->mr, ibmr->sg, ibmr->sg_len, 0, PAGE_SIZE);\r\nif (unlikely(ret != ibmr->sg_len))\r\nreturn ret < 0 ? ret : -EINVAL;\r\nib_update_fast_reg_key(frmr->mr, ibmr->remap_count++);\r\nfrmr->fr_state = FRMR_IS_INUSE;\r\nmemset(&reg_wr, 0, sizeof(reg_wr));\r\nreg_wr.wr.wr_id = (unsigned long)(void *)ibmr;\r\nreg_wr.wr.opcode = IB_WR_REG_MR;\r\nreg_wr.wr.num_sge = 0;\r\nreg_wr.mr = frmr->mr;\r\nreg_wr.key = frmr->mr->rkey;\r\nreg_wr.access = IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE;\r\nreg_wr.wr.send_flags = IB_SEND_SIGNALED;\r\nfailed_wr = &reg_wr.wr;\r\nret = ib_post_send(ibmr->ic->i_cm_id->qp, &reg_wr.wr, &failed_wr);\r\nWARN_ON(failed_wr != &reg_wr.wr);\r\nif (unlikely(ret)) {\r\nfrmr->fr_state = FRMR_IS_STALE;\r\natomic_inc(&ibmr->ic->i_fastreg_wrs);\r\nif (printk_ratelimit())\r\npr_warn("RDS/IB: %s returned error(%d)\n",\r\n__func__, ret);\r\n}\r\nreturn ret;\r\n}\r\nstatic int rds_ib_map_frmr(struct rds_ib_device *rds_ibdev,\r\nstruct rds_ib_mr_pool *pool,\r\nstruct rds_ib_mr *ibmr,\r\nstruct scatterlist *sg, unsigned int sg_len)\r\n{\r\nstruct ib_device *dev = rds_ibdev->dev;\r\nstruct rds_ib_frmr *frmr = &ibmr->u.frmr;\r\nint i;\r\nu32 len;\r\nint ret = 0;\r\nrds_ib_teardown_mr(ibmr);\r\nibmr->sg = sg;\r\nibmr->sg_len = sg_len;\r\nibmr->sg_dma_len = 0;\r\nfrmr->sg_byte_len = 0;\r\nWARN_ON(ibmr->sg_dma_len);\r\nibmr->sg_dma_len = ib_dma_map_sg(dev, ibmr->sg, ibmr->sg_len,\r\nDMA_BIDIRECTIONAL);\r\nif (unlikely(!ibmr->sg_dma_len)) {\r\npr_warn("RDS/IB: %s failed!\n", __func__);\r\nreturn -EBUSY;\r\n}\r\nfrmr->sg_byte_len = 0;\r\nfrmr->dma_npages = 0;\r\nlen = 0;\r\nret = -EINVAL;\r\nfor (i = 0; i < ibmr->sg_dma_len; ++i) {\r\nunsigned int dma_len = ib_sg_dma_len(dev, &ibmr->sg[i]);\r\nu64 dma_addr = ib_sg_dma_address(dev, &ibmr->sg[i]);\r\nfrmr->sg_byte_len += dma_len;\r\nif (dma_addr & ~PAGE_MASK) {\r\nif (i > 0)\r\ngoto out_unmap;\r\nelse\r\n++frmr->dma_npages;\r\n}\r\nif ((dma_addr + dma_len) & ~PAGE_MASK) {\r\nif (i < ibmr->sg_dma_len - 1)\r\ngoto out_unmap;\r\nelse\r\n++frmr->dma_npages;\r\n}\r\nlen += dma_len;\r\n}\r\nfrmr->dma_npages += len >> PAGE_SHIFT;\r\nif (frmr->dma_npages > ibmr->pool->fmr_attr.max_pages) {\r\nret = -EMSGSIZE;\r\ngoto out_unmap;\r\n}\r\nret = rds_ib_post_reg_frmr(ibmr);\r\nif (ret)\r\ngoto out_unmap;\r\nif (ibmr->pool->pool_type == RDS_IB_MR_8K_POOL)\r\nrds_ib_stats_inc(s_ib_rdma_mr_8k_used);\r\nelse\r\nrds_ib_stats_inc(s_ib_rdma_mr_1m_used);\r\nreturn ret;\r\nout_unmap:\r\nib_dma_unmap_sg(rds_ibdev->dev, ibmr->sg, ibmr->sg_len,\r\nDMA_BIDIRECTIONAL);\r\nibmr->sg_dma_len = 0;\r\nreturn ret;\r\n}\r\nstatic int rds_ib_post_inv(struct rds_ib_mr *ibmr)\r\n{\r\nstruct ib_send_wr *s_wr, *failed_wr;\r\nstruct rds_ib_frmr *frmr = &ibmr->u.frmr;\r\nstruct rdma_cm_id *i_cm_id = ibmr->ic->i_cm_id;\r\nint ret = -EINVAL;\r\nif (!i_cm_id || !i_cm_id->qp || !frmr->mr)\r\ngoto out;\r\nif (frmr->fr_state != FRMR_IS_INUSE)\r\ngoto out;\r\nwhile (atomic_dec_return(&ibmr->ic->i_fastreg_wrs) <= 0) {\r\natomic_inc(&ibmr->ic->i_fastreg_wrs);\r\ncpu_relax();\r\n}\r\nfrmr->fr_inv = true;\r\ns_wr = &frmr->fr_wr;\r\nmemset(s_wr, 0, sizeof(*s_wr));\r\ns_wr->wr_id = (unsigned long)(void *)ibmr;\r\ns_wr->opcode = IB_WR_LOCAL_INV;\r\ns_wr->ex.invalidate_rkey = frmr->mr->rkey;\r\ns_wr->send_flags = IB_SEND_SIGNALED;\r\nfailed_wr = s_wr;\r\nret = ib_post_send(i_cm_id->qp, s_wr, &failed_wr);\r\nWARN_ON(failed_wr != s_wr);\r\nif (unlikely(ret)) {\r\nfrmr->fr_state = FRMR_IS_STALE;\r\nfrmr->fr_inv = false;\r\natomic_inc(&ibmr->ic->i_fastreg_wrs);\r\npr_err("RDS/IB: %s returned error(%d)\n", __func__, ret);\r\ngoto out;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nvoid rds_ib_mr_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc)\r\n{\r\nstruct rds_ib_mr *ibmr = (void *)(unsigned long)wc->wr_id;\r\nstruct rds_ib_frmr *frmr = &ibmr->u.frmr;\r\nif (wc->status != IB_WC_SUCCESS) {\r\nfrmr->fr_state = FRMR_IS_STALE;\r\nif (rds_conn_up(ic->conn))\r\nrds_ib_conn_error(ic->conn,\r\n"frmr completion <%pI4,%pI4> status %u(%s), vendor_err 0x%x, disconnecting and reconnecting\n",\r\n&ic->conn->c_laddr,\r\n&ic->conn->c_faddr,\r\nwc->status,\r\nib_wc_status_msg(wc->status),\r\nwc->vendor_err);\r\n}\r\nif (frmr->fr_inv) {\r\nfrmr->fr_state = FRMR_IS_FREE;\r\nfrmr->fr_inv = false;\r\n}\r\natomic_inc(&ic->i_fastreg_wrs);\r\n}\r\nvoid rds_ib_unreg_frmr(struct list_head *list, unsigned int *nfreed,\r\nunsigned long *unpinned, unsigned int goal)\r\n{\r\nstruct rds_ib_mr *ibmr, *next;\r\nstruct rds_ib_frmr *frmr;\r\nint ret = 0;\r\nunsigned int freed = *nfreed;\r\nlist_for_each_entry(ibmr, list, unmap_list) {\r\nif (ibmr->sg_dma_len)\r\nret |= rds_ib_post_inv(ibmr);\r\n}\r\nif (ret)\r\npr_warn("RDS/IB: %s failed (err=%d)\n", __func__, ret);\r\nlist_for_each_entry_safe(ibmr, next, list, unmap_list) {\r\n*unpinned += ibmr->sg_len;\r\nfrmr = &ibmr->u.frmr;\r\n__rds_ib_teardown_mr(ibmr);\r\nif (freed < goal || frmr->fr_state == FRMR_IS_STALE) {\r\nif (frmr->fr_state == FRMR_IS_INUSE)\r\ncontinue;\r\nif (ibmr->pool->pool_type == RDS_IB_MR_8K_POOL)\r\nrds_ib_stats_inc(s_ib_rdma_mr_8k_free);\r\nelse\r\nrds_ib_stats_inc(s_ib_rdma_mr_1m_free);\r\nlist_del(&ibmr->unmap_list);\r\nif (frmr->mr)\r\nib_dereg_mr(frmr->mr);\r\nkfree(ibmr);\r\nfreed++;\r\n}\r\n}\r\n*nfreed = freed;\r\n}\r\nstruct rds_ib_mr *rds_ib_reg_frmr(struct rds_ib_device *rds_ibdev,\r\nstruct rds_ib_connection *ic,\r\nstruct scatterlist *sg,\r\nunsigned long nents, u32 *key)\r\n{\r\nstruct rds_ib_mr *ibmr = NULL;\r\nstruct rds_ib_frmr *frmr;\r\nint ret;\r\ndo {\r\nif (ibmr)\r\nrds_ib_free_frmr(ibmr, true);\r\nibmr = rds_ib_alloc_frmr(rds_ibdev, nents);\r\nif (IS_ERR(ibmr))\r\nreturn ibmr;\r\nfrmr = &ibmr->u.frmr;\r\n} while (frmr->fr_state != FRMR_IS_FREE);\r\nibmr->ic = ic;\r\nibmr->device = rds_ibdev;\r\nret = rds_ib_map_frmr(rds_ibdev, ibmr->pool, ibmr, sg, nents);\r\nif (ret == 0) {\r\n*key = frmr->mr->rkey;\r\n} else {\r\nrds_ib_free_frmr(ibmr, false);\r\nibmr = ERR_PTR(ret);\r\n}\r\nreturn ibmr;\r\n}\r\nvoid rds_ib_free_frmr_list(struct rds_ib_mr *ibmr)\r\n{\r\nstruct rds_ib_mr_pool *pool = ibmr->pool;\r\nstruct rds_ib_frmr *frmr = &ibmr->u.frmr;\r\nif (frmr->fr_state == FRMR_IS_STALE)\r\nllist_add(&ibmr->llnode, &pool->drop_list);\r\nelse\r\nllist_add(&ibmr->llnode, &pool->free_list);\r\n}
