static struct drm_i915_gem_object *dma_buf_to_obj(struct dma_buf *buf)\r\n{\r\nreturn to_intel_bo(buf->priv);\r\n}\r\nstatic struct sg_table *i915_gem_map_dma_buf(struct dma_buf_attachment *attachment,\r\nenum dma_data_direction dir)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(attachment->dmabuf);\r\nstruct sg_table *st;\r\nstruct scatterlist *src, *dst;\r\nint ret, i;\r\nret = i915_mutex_lock_interruptible(obj->base.dev);\r\nif (ret)\r\ngoto err;\r\nret = i915_gem_object_get_pages(obj);\r\nif (ret)\r\ngoto err_unlock;\r\ni915_gem_object_pin_pages(obj);\r\nst = kmalloc(sizeof(struct sg_table), GFP_KERNEL);\r\nif (st == NULL) {\r\nret = -ENOMEM;\r\ngoto err_unpin;\r\n}\r\nret = sg_alloc_table(st, obj->pages->nents, GFP_KERNEL);\r\nif (ret)\r\ngoto err_free;\r\nsrc = obj->pages->sgl;\r\ndst = st->sgl;\r\nfor (i = 0; i < obj->pages->nents; i++) {\r\nsg_set_page(dst, sg_page(src), src->length, 0);\r\ndst = sg_next(dst);\r\nsrc = sg_next(src);\r\n}\r\nif (!dma_map_sg(attachment->dev, st->sgl, st->nents, dir)) {\r\nret =-ENOMEM;\r\ngoto err_free_sg;\r\n}\r\nmutex_unlock(&obj->base.dev->struct_mutex);\r\nreturn st;\r\nerr_free_sg:\r\nsg_free_table(st);\r\nerr_free:\r\nkfree(st);\r\nerr_unpin:\r\ni915_gem_object_unpin_pages(obj);\r\nerr_unlock:\r\nmutex_unlock(&obj->base.dev->struct_mutex);\r\nerr:\r\nreturn ERR_PTR(ret);\r\n}\r\nstatic void i915_gem_unmap_dma_buf(struct dma_buf_attachment *attachment,\r\nstruct sg_table *sg,\r\nenum dma_data_direction dir)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(attachment->dmabuf);\r\ndma_unmap_sg(attachment->dev, sg->sgl, sg->nents, dir);\r\nsg_free_table(sg);\r\nkfree(sg);\r\nmutex_lock(&obj->base.dev->struct_mutex);\r\ni915_gem_object_unpin_pages(obj);\r\nmutex_unlock(&obj->base.dev->struct_mutex);\r\n}\r\nstatic void *i915_gem_dmabuf_vmap(struct dma_buf *dma_buf)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(dma_buf);\r\nstruct drm_device *dev = obj->base.dev;\r\nvoid *addr;\r\nint ret;\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\naddr = i915_gem_object_pin_map(obj);\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn addr;\r\n}\r\nstatic void i915_gem_dmabuf_vunmap(struct dma_buf *dma_buf, void *vaddr)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(dma_buf);\r\nstruct drm_device *dev = obj->base.dev;\r\nmutex_lock(&dev->struct_mutex);\r\ni915_gem_object_unpin_map(obj);\r\nmutex_unlock(&dev->struct_mutex);\r\n}\r\nstatic void *i915_gem_dmabuf_kmap_atomic(struct dma_buf *dma_buf, unsigned long page_num)\r\n{\r\nreturn NULL;\r\n}\r\nstatic void i915_gem_dmabuf_kunmap_atomic(struct dma_buf *dma_buf, unsigned long page_num, void *addr)\r\n{\r\n}\r\nstatic void *i915_gem_dmabuf_kmap(struct dma_buf *dma_buf, unsigned long page_num)\r\n{\r\nreturn NULL;\r\n}\r\nstatic void i915_gem_dmabuf_kunmap(struct dma_buf *dma_buf, unsigned long page_num, void *addr)\r\n{\r\n}\r\nstatic int i915_gem_dmabuf_mmap(struct dma_buf *dma_buf, struct vm_area_struct *vma)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(dma_buf);\r\nint ret;\r\nif (obj->base.size < vma->vm_end - vma->vm_start)\r\nreturn -EINVAL;\r\nif (!obj->base.filp)\r\nreturn -ENODEV;\r\nret = obj->base.filp->f_op->mmap(obj->base.filp, vma);\r\nif (ret)\r\nreturn ret;\r\nfput(vma->vm_file);\r\nvma->vm_file = get_file(obj->base.filp);\r\nreturn 0;\r\n}\r\nstatic int i915_gem_begin_cpu_access(struct dma_buf *dma_buf, enum dma_data_direction direction)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(dma_buf);\r\nstruct drm_device *dev = obj->base.dev;\r\nint ret;\r\nbool write = (direction == DMA_BIDIRECTIONAL || direction == DMA_TO_DEVICE);\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret)\r\nreturn ret;\r\nret = i915_gem_object_set_to_cpu_domain(obj, write);\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn ret;\r\n}\r\nstatic int i915_gem_end_cpu_access(struct dma_buf *dma_buf, enum dma_data_direction direction)\r\n{\r\nstruct drm_i915_gem_object *obj = dma_buf_to_obj(dma_buf);\r\nstruct drm_device *dev = obj->base.dev;\r\nint ret;\r\nret = i915_mutex_lock_interruptible(dev);\r\nif (ret)\r\nreturn ret;\r\nret = i915_gem_object_set_to_gtt_domain(obj, false);\r\nmutex_unlock(&dev->struct_mutex);\r\nreturn ret;\r\n}\r\nstruct dma_buf *i915_gem_prime_export(struct drm_device *dev,\r\nstruct drm_gem_object *gem_obj, int flags)\r\n{\r\nstruct drm_i915_gem_object *obj = to_intel_bo(gem_obj);\r\nDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\r\nexp_info.ops = &i915_dmabuf_ops;\r\nexp_info.size = gem_obj->size;\r\nexp_info.flags = flags;\r\nexp_info.priv = gem_obj;\r\nif (obj->ops->dmabuf_export) {\r\nint ret = obj->ops->dmabuf_export(obj);\r\nif (ret)\r\nreturn ERR_PTR(ret);\r\n}\r\nreturn dma_buf_export(&exp_info);\r\n}\r\nstatic int i915_gem_object_get_pages_dmabuf(struct drm_i915_gem_object *obj)\r\n{\r\nstruct sg_table *sg;\r\nsg = dma_buf_map_attachment(obj->base.import_attach, DMA_BIDIRECTIONAL);\r\nif (IS_ERR(sg))\r\nreturn PTR_ERR(sg);\r\nobj->pages = sg;\r\nreturn 0;\r\n}\r\nstatic void i915_gem_object_put_pages_dmabuf(struct drm_i915_gem_object *obj)\r\n{\r\ndma_buf_unmap_attachment(obj->base.import_attach,\r\nobj->pages, DMA_BIDIRECTIONAL);\r\n}\r\nstruct drm_gem_object *i915_gem_prime_import(struct drm_device *dev,\r\nstruct dma_buf *dma_buf)\r\n{\r\nstruct dma_buf_attachment *attach;\r\nstruct drm_i915_gem_object *obj;\r\nint ret;\r\nif (dma_buf->ops == &i915_dmabuf_ops) {\r\nobj = dma_buf_to_obj(dma_buf);\r\nif (obj->base.dev == dev) {\r\ndrm_gem_object_reference(&obj->base);\r\nreturn &obj->base;\r\n}\r\n}\r\nattach = dma_buf_attach(dma_buf, dev->dev);\r\nif (IS_ERR(attach))\r\nreturn ERR_CAST(attach);\r\nget_dma_buf(dma_buf);\r\nobj = i915_gem_object_alloc(dev);\r\nif (obj == NULL) {\r\nret = -ENOMEM;\r\ngoto fail_detach;\r\n}\r\ndrm_gem_private_object_init(dev, &obj->base, dma_buf->size);\r\ni915_gem_object_init(obj, &i915_gem_object_dmabuf_ops);\r\nobj->base.import_attach = attach;\r\nreturn &obj->base;\r\nfail_detach:\r\ndma_buf_detach(dma_buf, attach);\r\ndma_buf_put(dma_buf);\r\nreturn ERR_PTR(ret);\r\n}
