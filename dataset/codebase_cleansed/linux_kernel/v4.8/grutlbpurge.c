static inline int get_off_blade_tgh(struct gru_state *gru)\r\n{\r\nint n;\r\nn = GRU_NUM_TGH - gru->gs_tgh_first_remote;\r\nn = gru_random() % n;\r\nn += gru->gs_tgh_first_remote;\r\nreturn n;\r\n}\r\nstatic inline int get_on_blade_tgh(struct gru_state *gru)\r\n{\r\nreturn uv_blade_processor_id() >> gru->gs_tgh_local_shift;\r\n}\r\nstatic struct gru_tlb_global_handle *get_lock_tgh_handle(struct gru_state\r\n*gru)\r\n{\r\nstruct gru_tlb_global_handle *tgh;\r\nint n;\r\npreempt_disable();\r\nif (uv_numa_blade_id() == gru->gs_blade_id)\r\nn = get_on_blade_tgh(gru);\r\nelse\r\nn = get_off_blade_tgh(gru);\r\ntgh = get_tgh_by_index(gru, n);\r\nlock_tgh_handle(tgh);\r\nreturn tgh;\r\n}\r\nstatic void get_unlock_tgh_handle(struct gru_tlb_global_handle *tgh)\r\n{\r\nunlock_tgh_handle(tgh);\r\npreempt_enable();\r\n}\r\nvoid gru_flush_tlb_range(struct gru_mm_struct *gms, unsigned long start,\r\nunsigned long len)\r\n{\r\nstruct gru_state *gru;\r\nstruct gru_mm_tracker *asids;\r\nstruct gru_tlb_global_handle *tgh;\r\nunsigned long num;\r\nint grupagesize, pagesize, pageshift, gid, asid;\r\npageshift = PAGE_SHIFT;\r\npagesize = (1UL << pageshift);\r\ngrupagesize = GRU_PAGESIZE(pageshift);\r\nnum = min(((len + pagesize - 1) >> pageshift), GRUMAXINVAL);\r\nSTAT(flush_tlb);\r\ngru_dbg(grudev, "gms %p, start 0x%lx, len 0x%lx, asidmap 0x%lx\n", gms,\r\nstart, len, gms->ms_asidmap[0]);\r\nspin_lock(&gms->ms_asid_lock);\r\nfor_each_gru_in_bitmap(gid, gms->ms_asidmap) {\r\nSTAT(flush_tlb_gru);\r\ngru = GID_TO_GRU(gid);\r\nasids = gms->ms_asids + gid;\r\nasid = asids->mt_asid;\r\nif (asids->mt_ctxbitmap && asid) {\r\nSTAT(flush_tlb_gru_tgh);\r\nasid = GRUASID(asid, start);\r\ngru_dbg(grudev,\r\n" FLUSH gruid %d, asid 0x%x, vaddr 0x%lx, vamask 0x%x, num %ld, cbmap 0x%x\n",\r\ngid, asid, start, grupagesize, num, asids->mt_ctxbitmap);\r\ntgh = get_lock_tgh_handle(gru);\r\ntgh_invalidate(tgh, start, ~0, asid, grupagesize, 0,\r\nnum - 1, asids->mt_ctxbitmap);\r\nget_unlock_tgh_handle(tgh);\r\n} else {\r\nSTAT(flush_tlb_gru_zero_asid);\r\nasids->mt_asid = 0;\r\n__clear_bit(gru->gs_gid, gms->ms_asidmap);\r\ngru_dbg(grudev,\r\n" CLEARASID gruid %d, asid 0x%x, cbtmap 0x%x, asidmap 0x%lx\n",\r\ngid, asid, asids->mt_ctxbitmap,\r\ngms->ms_asidmap[0]);\r\n}\r\n}\r\nspin_unlock(&gms->ms_asid_lock);\r\n}\r\nvoid gru_flush_all_tlb(struct gru_state *gru)\r\n{\r\nstruct gru_tlb_global_handle *tgh;\r\ngru_dbg(grudev, "gid %d\n", gru->gs_gid);\r\ntgh = get_lock_tgh_handle(gru);\r\ntgh_invalidate(tgh, 0, ~0, 0, 1, 1, GRUMAXINVAL - 1, 0xffff);\r\nget_unlock_tgh_handle(tgh);\r\n}\r\nstatic void gru_invalidate_range_start(struct mmu_notifier *mn,\r\nstruct mm_struct *mm,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,\r\nms_notifier);\r\nSTAT(mmu_invalidate_range);\r\natomic_inc(&gms->ms_range_active);\r\ngru_dbg(grudev, "gms %p, start 0x%lx, end 0x%lx, act %d\n", gms,\r\nstart, end, atomic_read(&gms->ms_range_active));\r\ngru_flush_tlb_range(gms, start, end - start);\r\n}\r\nstatic void gru_invalidate_range_end(struct mmu_notifier *mn,\r\nstruct mm_struct *mm, unsigned long start,\r\nunsigned long end)\r\n{\r\nstruct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,\r\nms_notifier);\r\n(void)atomic_dec_and_test(&gms->ms_range_active);\r\nwake_up_all(&gms->ms_wait_queue);\r\ngru_dbg(grudev, "gms %p, start 0x%lx, end 0x%lx\n", gms, start, end);\r\n}\r\nstatic void gru_invalidate_page(struct mmu_notifier *mn, struct mm_struct *mm,\r\nunsigned long address)\r\n{\r\nstruct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,\r\nms_notifier);\r\nSTAT(mmu_invalidate_page);\r\ngru_flush_tlb_range(gms, address, PAGE_SIZE);\r\ngru_dbg(grudev, "gms %p, address 0x%lx\n", gms, address);\r\n}\r\nstatic void gru_release(struct mmu_notifier *mn, struct mm_struct *mm)\r\n{\r\nstruct gru_mm_struct *gms = container_of(mn, struct gru_mm_struct,\r\nms_notifier);\r\ngms->ms_released = 1;\r\ngru_dbg(grudev, "gms %p\n", gms);\r\n}\r\nstatic struct mmu_notifier *mmu_find_ops(struct mm_struct *mm,\r\nconst struct mmu_notifier_ops *ops)\r\n{\r\nstruct mmu_notifier *mn, *gru_mn = NULL;\r\nif (mm->mmu_notifier_mm) {\r\nrcu_read_lock();\r\nhlist_for_each_entry_rcu(mn, &mm->mmu_notifier_mm->list,\r\nhlist)\r\nif (mn->ops == ops) {\r\ngru_mn = mn;\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\n}\r\nreturn gru_mn;\r\n}\r\nstruct gru_mm_struct *gru_register_mmu_notifier(void)\r\n{\r\nstruct gru_mm_struct *gms;\r\nstruct mmu_notifier *mn;\r\nint err;\r\nmn = mmu_find_ops(current->mm, &gru_mmuops);\r\nif (mn) {\r\ngms = container_of(mn, struct gru_mm_struct, ms_notifier);\r\natomic_inc(&gms->ms_refcnt);\r\n} else {\r\ngms = kzalloc(sizeof(*gms), GFP_KERNEL);\r\nif (!gms)\r\nreturn ERR_PTR(-ENOMEM);\r\nSTAT(gms_alloc);\r\nspin_lock_init(&gms->ms_asid_lock);\r\ngms->ms_notifier.ops = &gru_mmuops;\r\natomic_set(&gms->ms_refcnt, 1);\r\ninit_waitqueue_head(&gms->ms_wait_queue);\r\nerr = __mmu_notifier_register(&gms->ms_notifier, current->mm);\r\nif (err)\r\ngoto error;\r\n}\r\nif (gms)\r\ngru_dbg(grudev, "gms %p, refcnt %d\n", gms,\r\natomic_read(&gms->ms_refcnt));\r\nreturn gms;\r\nerror:\r\nkfree(gms);\r\nreturn ERR_PTR(err);\r\n}\r\nvoid gru_drop_mmu_notifier(struct gru_mm_struct *gms)\r\n{\r\ngru_dbg(grudev, "gms %p, refcnt %d, released %d\n", gms,\r\natomic_read(&gms->ms_refcnt), gms->ms_released);\r\nif (atomic_dec_return(&gms->ms_refcnt) == 0) {\r\nif (!gms->ms_released)\r\nmmu_notifier_unregister(&gms->ms_notifier, current->mm);\r\nkfree(gms);\r\nSTAT(gms_free);\r\n}\r\n}\r\nvoid gru_tgh_flush_init(struct gru_state *gru)\r\n{\r\nint cpus, shift = 0, n;\r\ncpus = uv_blade_nr_possible_cpus(gru->gs_blade_id);\r\nif (cpus) {\r\nn = 1 << fls(cpus - 1);\r\nshift = max(0, fls(n - 1) - fls(MAX_LOCAL_TGH - 1));\r\n}\r\ngru->gs_tgh_local_shift = shift;\r\ngru->gs_tgh_first_remote = (cpus + (1 << shift) - 1) >> shift;\r\n}
