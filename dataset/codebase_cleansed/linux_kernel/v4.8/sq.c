void sq_flush_range(unsigned long start, unsigned int len)\r\n{\r\nunsigned long *sq = (unsigned long *)start;\r\nfor (len >>= 5; len--; sq += 8)\r\nprefetchw(sq);\r\nstore_queue_barrier();\r\n}\r\nstatic inline void sq_mapping_list_add(struct sq_mapping *map)\r\n{\r\nstruct sq_mapping **p, *tmp;\r\nspin_lock_irq(&sq_mapping_lock);\r\np = &sq_mapping_list;\r\nwhile ((tmp = *p) != NULL)\r\np = &tmp->next;\r\nmap->next = tmp;\r\n*p = map;\r\nspin_unlock_irq(&sq_mapping_lock);\r\n}\r\nstatic inline void sq_mapping_list_del(struct sq_mapping *map)\r\n{\r\nstruct sq_mapping **p, *tmp;\r\nspin_lock_irq(&sq_mapping_lock);\r\nfor (p = &sq_mapping_list; (tmp = *p); p = &tmp->next)\r\nif (tmp == map) {\r\n*p = tmp->next;\r\nbreak;\r\n}\r\nspin_unlock_irq(&sq_mapping_lock);\r\n}\r\nstatic int __sq_remap(struct sq_mapping *map, pgprot_t prot)\r\n{\r\n#if defined(CONFIG_MMU)\r\nstruct vm_struct *vma;\r\nvma = __get_vm_area(map->size, VM_ALLOC, map->sq_addr, SQ_ADDRMAX);\r\nif (!vma)\r\nreturn -ENOMEM;\r\nvma->phys_addr = map->addr;\r\nif (ioremap_page_range((unsigned long)vma->addr,\r\n(unsigned long)vma->addr + map->size,\r\nvma->phys_addr, prot)) {\r\nvunmap(vma->addr);\r\nreturn -EAGAIN;\r\n}\r\n#else\r\n__raw_writel(((map->addr >> 26) << 2) & 0x1c, SQ_QACR0);\r\n__raw_writel(((map->addr >> 26) << 2) & 0x1c, SQ_QACR1);\r\n#endif\r\nreturn 0;\r\n}\r\nunsigned long sq_remap(unsigned long phys, unsigned int size,\r\nconst char *name, pgprot_t prot)\r\n{\r\nstruct sq_mapping *map;\r\nunsigned long end;\r\nunsigned int psz;\r\nint ret, page;\r\nend = phys + size - 1;\r\nif (unlikely(!size || end < phys))\r\nreturn -EINVAL;\r\nif (unlikely(phys < virt_to_phys(high_memory)))\r\nreturn -EINVAL;\r\nphys &= PAGE_MASK;\r\nsize = PAGE_ALIGN(end + 1) - phys;\r\nmap = kmem_cache_alloc(sq_cache, GFP_KERNEL);\r\nif (unlikely(!map))\r\nreturn -ENOMEM;\r\nmap->addr = phys;\r\nmap->size = size;\r\nmap->name = name;\r\npage = bitmap_find_free_region(sq_bitmap, 0x04000000 >> PAGE_SHIFT,\r\nget_order(map->size));\r\nif (unlikely(page < 0)) {\r\nret = -ENOSPC;\r\ngoto out;\r\n}\r\nmap->sq_addr = P4SEG_STORE_QUE + (page << PAGE_SHIFT);\r\nret = __sq_remap(map, prot);\r\nif (unlikely(ret != 0))\r\ngoto out;\r\npsz = (size + (PAGE_SIZE - 1)) >> PAGE_SHIFT;\r\npr_info("sqremap: %15s [%4d page%s] va 0x%08lx pa 0x%08lx\n",\r\nlikely(map->name) ? map->name : "???",\r\npsz, psz == 1 ? " " : "s",\r\nmap->sq_addr, map->addr);\r\nsq_mapping_list_add(map);\r\nreturn map->sq_addr;\r\nout:\r\nkmem_cache_free(sq_cache, map);\r\nreturn ret;\r\n}\r\nvoid sq_unmap(unsigned long vaddr)\r\n{\r\nstruct sq_mapping **p, *map;\r\nint page;\r\nfor (p = &sq_mapping_list; (map = *p); p = &map->next)\r\nif (map->sq_addr == vaddr)\r\nbreak;\r\nif (unlikely(!map)) {\r\nprintk("%s: bad store queue address 0x%08lx\n",\r\n__func__, vaddr);\r\nreturn;\r\n}\r\npage = (map->sq_addr - P4SEG_STORE_QUE) >> PAGE_SHIFT;\r\nbitmap_release_region(sq_bitmap, page, get_order(map->size));\r\n#ifdef CONFIG_MMU\r\n{\r\nstruct vm_struct *vma;\r\nvma = remove_vm_area((void *)(map->sq_addr & PAGE_MASK));\r\nif (!vma) {\r\nprintk(KERN_ERR "%s: bad address 0x%08lx\n",\r\n__func__, map->sq_addr);\r\nreturn;\r\n}\r\n}\r\n#endif\r\nsq_mapping_list_del(map);\r\nkmem_cache_free(sq_cache, map);\r\n}\r\nstatic ssize_t sq_sysfs_show(struct kobject *kobj, struct attribute *attr,\r\nchar *buf)\r\n{\r\nstruct sq_sysfs_attr *sattr = to_sq_sysfs_attr(attr);\r\nif (likely(sattr->show))\r\nreturn sattr->show(buf);\r\nreturn -EIO;\r\n}\r\nstatic ssize_t sq_sysfs_store(struct kobject *kobj, struct attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct sq_sysfs_attr *sattr = to_sq_sysfs_attr(attr);\r\nif (likely(sattr->store))\r\nreturn sattr->store(buf, count);\r\nreturn -EIO;\r\n}\r\nstatic ssize_t mapping_show(char *buf)\r\n{\r\nstruct sq_mapping **list, *entry;\r\nchar *p = buf;\r\nfor (list = &sq_mapping_list; (entry = *list); list = &entry->next)\r\np += sprintf(p, "%08lx-%08lx [%08lx]: %s\n",\r\nentry->sq_addr, entry->sq_addr + entry->size,\r\nentry->addr, entry->name);\r\nreturn p - buf;\r\n}\r\nstatic ssize_t mapping_store(const char *buf, size_t count)\r\n{\r\nunsigned long base = 0, len = 0;\r\nsscanf(buf, "%lx %lx", &base, &len);\r\nif (!base)\r\nreturn -EIO;\r\nif (likely(len)) {\r\nint ret = sq_remap(base, len, "Userspace", PAGE_SHARED);\r\nif (ret < 0)\r\nreturn ret;\r\n} else\r\nsq_unmap(base);\r\nreturn count;\r\n}\r\nstatic int sq_dev_add(struct device *dev, struct subsys_interface *sif)\r\n{\r\nunsigned int cpu = dev->id;\r\nstruct kobject *kobj;\r\nint error;\r\nsq_kobject[cpu] = kzalloc(sizeof(struct kobject), GFP_KERNEL);\r\nif (unlikely(!sq_kobject[cpu]))\r\nreturn -ENOMEM;\r\nkobj = sq_kobject[cpu];\r\nerror = kobject_init_and_add(kobj, &ktype_percpu_entry, &dev->kobj,\r\n"%s", "sq");\r\nif (!error)\r\nkobject_uevent(kobj, KOBJ_ADD);\r\nreturn error;\r\n}\r\nstatic void sq_dev_remove(struct device *dev, struct subsys_interface *sif)\r\n{\r\nunsigned int cpu = dev->id;\r\nstruct kobject *kobj = sq_kobject[cpu];\r\nkobject_put(kobj);\r\n}\r\nstatic int __init sq_api_init(void)\r\n{\r\nunsigned int nr_pages = 0x04000000 >> PAGE_SHIFT;\r\nunsigned int size = (nr_pages + (BITS_PER_LONG - 1)) / BITS_PER_LONG;\r\nint ret = -ENOMEM;\r\nprintk(KERN_NOTICE "sq: Registering store queue API.\n");\r\nsq_cache = kmem_cache_create("store_queue_cache",\r\nsizeof(struct sq_mapping), 0, 0, NULL);\r\nif (unlikely(!sq_cache))\r\nreturn ret;\r\nsq_bitmap = kzalloc(size, GFP_KERNEL);\r\nif (unlikely(!sq_bitmap))\r\ngoto out;\r\nret = subsys_interface_register(&sq_interface);\r\nif (unlikely(ret != 0))\r\ngoto out;\r\nreturn 0;\r\nout:\r\nkfree(sq_bitmap);\r\nkmem_cache_destroy(sq_cache);\r\nreturn ret;\r\n}\r\nstatic void __exit sq_api_exit(void)\r\n{\r\nsubsys_interface_unregister(&sq_interface);\r\nkfree(sq_bitmap);\r\nkmem_cache_destroy(sq_cache);\r\n}
