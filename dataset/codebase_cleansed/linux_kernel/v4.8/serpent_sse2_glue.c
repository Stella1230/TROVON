static void serpent_decrypt_cbc_xway(void *ctx, u128 *dst, const u128 *src)\r\n{\r\nu128 ivs[SERPENT_PARALLEL_BLOCKS - 1];\r\nunsigned int j;\r\nfor (j = 0; j < SERPENT_PARALLEL_BLOCKS - 1; j++)\r\nivs[j] = src[j];\r\nserpent_dec_blk_xway(ctx, (u8 *)dst, (u8 *)src);\r\nfor (j = 0; j < SERPENT_PARALLEL_BLOCKS - 1; j++)\r\nu128_xor(dst + (j + 1), dst + (j + 1), ivs + j);\r\n}\r\nstatic void serpent_crypt_ctr(void *ctx, u128 *dst, const u128 *src, le128 *iv)\r\n{\r\nbe128 ctrblk;\r\nle128_to_be128(&ctrblk, iv);\r\nle128_inc(iv);\r\n__serpent_encrypt(ctx, (u8 *)&ctrblk, (u8 *)&ctrblk);\r\nu128_xor(dst, src, (u128 *)&ctrblk);\r\n}\r\nstatic void serpent_crypt_ctr_xway(void *ctx, u128 *dst, const u128 *src,\r\nle128 *iv)\r\n{\r\nbe128 ctrblks[SERPENT_PARALLEL_BLOCKS];\r\nunsigned int i;\r\nfor (i = 0; i < SERPENT_PARALLEL_BLOCKS; i++) {\r\nif (dst != src)\r\ndst[i] = src[i];\r\nle128_to_be128(&ctrblks[i], iv);\r\nle128_inc(iv);\r\n}\r\nserpent_enc_blk_xway_xor(ctx, (u8 *)dst, (u8 *)ctrblks);\r\n}\r\nstatic int ecb_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ecb_crypt_128bit(&serpent_enc, desc, dst, src, nbytes);\r\n}\r\nstatic int ecb_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ecb_crypt_128bit(&serpent_dec, desc, dst, src, nbytes);\r\n}\r\nstatic int cbc_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_cbc_encrypt_128bit(GLUE_FUNC_CAST(__serpent_encrypt), desc,\r\ndst, src, nbytes);\r\n}\r\nstatic int cbc_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_cbc_decrypt_128bit(&serpent_dec_cbc, desc, dst, src,\r\nnbytes);\r\n}\r\nstatic int ctr_crypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nreturn glue_ctr_crypt_128bit(&serpent_ctr, desc, dst, src, nbytes);\r\n}\r\nstatic inline bool serpent_fpu_begin(bool fpu_enabled, unsigned int nbytes)\r\n{\r\nreturn glue_fpu_begin(SERPENT_BLOCK_SIZE, SERPENT_PARALLEL_BLOCKS,\r\nNULL, fpu_enabled, nbytes);\r\n}\r\nstatic inline void serpent_fpu_end(bool fpu_enabled)\r\n{\r\nglue_fpu_end(fpu_enabled);\r\n}\r\nstatic void encrypt_callback(void *priv, u8 *srcdst, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nstruct crypt_priv *ctx = priv;\r\nint i;\r\nctx->fpu_enabled = serpent_fpu_begin(ctx->fpu_enabled, nbytes);\r\nif (nbytes == bsize * SERPENT_PARALLEL_BLOCKS) {\r\nserpent_enc_blk_xway(ctx->ctx, srcdst, srcdst);\r\nreturn;\r\n}\r\nfor (i = 0; i < nbytes / bsize; i++, srcdst += bsize)\r\n__serpent_encrypt(ctx->ctx, srcdst, srcdst);\r\n}\r\nstatic void decrypt_callback(void *priv, u8 *srcdst, unsigned int nbytes)\r\n{\r\nconst unsigned int bsize = SERPENT_BLOCK_SIZE;\r\nstruct crypt_priv *ctx = priv;\r\nint i;\r\nctx->fpu_enabled = serpent_fpu_begin(ctx->fpu_enabled, nbytes);\r\nif (nbytes == bsize * SERPENT_PARALLEL_BLOCKS) {\r\nserpent_dec_blk_xway(ctx->ctx, srcdst, srcdst);\r\nreturn;\r\n}\r\nfor (i = 0; i < nbytes / bsize; i++, srcdst += bsize)\r\n__serpent_decrypt(ctx->ctx, srcdst, srcdst);\r\n}\r\nstatic int lrw_serpent_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint err;\r\nerr = __serpent_setkey(&ctx->serpent_ctx, key, keylen -\r\nSERPENT_BLOCK_SIZE);\r\nif (err)\r\nreturn err;\r\nreturn lrw_init_table(&ctx->lrw_table, key + keylen -\r\nSERPENT_BLOCK_SIZE);\r\n}\r\nstatic int lrw_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->serpent_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int lrw_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->serpent_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct lrw_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.table_ctx = &ctx->lrw_table,\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = lrw_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic void lrw_exit_tfm(struct crypto_tfm *tfm)\r\n{\r\nstruct serpent_lrw_ctx *ctx = crypto_tfm_ctx(tfm);\r\nlrw_free_table(&ctx->lrw_table);\r\n}\r\nstatic int xts_serpent_setkey(struct crypto_tfm *tfm, const u8 *key,\r\nunsigned int keylen)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_tfm_ctx(tfm);\r\nint err;\r\nerr = xts_check_key(tfm, key, keylen);\r\nif (err)\r\nreturn err;\r\nerr = __serpent_setkey(&ctx->crypt_ctx, key, keylen / 2);\r\nif (err)\r\nreturn err;\r\nreturn __serpent_setkey(&ctx->tweak_ctx, key + keylen / 2, keylen / 2);\r\n}\r\nstatic int xts_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->crypt_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = &ctx->tweak_ctx,\r\n.tweak_fn = XTS_TWEAK_CAST(__serpent_encrypt),\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = encrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int xts_decrypt(struct blkcipher_desc *desc, struct scatterlist *dst,\r\nstruct scatterlist *src, unsigned int nbytes)\r\n{\r\nstruct serpent_xts_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);\r\nbe128 buf[SERPENT_PARALLEL_BLOCKS];\r\nstruct crypt_priv crypt_ctx = {\r\n.ctx = &ctx->crypt_ctx,\r\n.fpu_enabled = false,\r\n};\r\nstruct xts_crypt_req req = {\r\n.tbuf = buf,\r\n.tbuflen = sizeof(buf),\r\n.tweak_ctx = &ctx->tweak_ctx,\r\n.tweak_fn = XTS_TWEAK_CAST(__serpent_encrypt),\r\n.crypt_ctx = &crypt_ctx,\r\n.crypt_fn = decrypt_callback,\r\n};\r\nint ret;\r\ndesc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;\r\nret = xts_crypt(desc, dst, src, nbytes, &req);\r\nserpent_fpu_end(crypt_ctx.fpu_enabled);\r\nreturn ret;\r\n}\r\nstatic int __init serpent_sse2_init(void)\r\n{\r\nif (!boot_cpu_has(X86_FEATURE_XMM2)) {\r\nprintk(KERN_INFO "SSE2 instructions are not detected.\n");\r\nreturn -ENODEV;\r\n}\r\nreturn crypto_register_algs(serpent_algs, ARRAY_SIZE(serpent_algs));\r\n}\r\nstatic void __exit serpent_sse2_exit(void)\r\n{\r\ncrypto_unregister_algs(serpent_algs, ARRAY_SIZE(serpent_algs));\r\n}
