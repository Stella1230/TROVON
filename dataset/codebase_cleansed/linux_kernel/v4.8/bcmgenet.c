static inline void dmadesc_set_length_status(struct bcmgenet_priv *priv,\r\nvoid __iomem *d, u32 value)\r\n{\r\n__raw_writel(value, d + DMA_DESC_LENGTH_STATUS);\r\n}\r\nstatic inline u32 dmadesc_get_length_status(struct bcmgenet_priv *priv,\r\nvoid __iomem *d)\r\n{\r\nreturn __raw_readl(d + DMA_DESC_LENGTH_STATUS);\r\n}\r\nstatic inline void dmadesc_set_addr(struct bcmgenet_priv *priv,\r\nvoid __iomem *d,\r\ndma_addr_t addr)\r\n{\r\n__raw_writel(lower_32_bits(addr), d + DMA_DESC_ADDRESS_LO);\r\n#ifdef CONFIG_PHYS_ADDR_T_64BIT\r\nif (priv->hw_params->flags & GENET_HAS_40BITS)\r\n__raw_writel(upper_32_bits(addr), d + DMA_DESC_ADDRESS_HI);\r\n#endif\r\n}\r\nstatic inline void dmadesc_set(struct bcmgenet_priv *priv,\r\nvoid __iomem *d, dma_addr_t addr, u32 val)\r\n{\r\ndmadesc_set_addr(priv, d, addr);\r\ndmadesc_set_length_status(priv, d, val);\r\n}\r\nstatic inline dma_addr_t dmadesc_get_addr(struct bcmgenet_priv *priv,\r\nvoid __iomem *d)\r\n{\r\ndma_addr_t addr;\r\naddr = __raw_readl(d + DMA_DESC_ADDRESS_LO);\r\n#ifdef CONFIG_PHYS_ADDR_T_64BIT\r\nif (priv->hw_params->flags & GENET_HAS_40BITS)\r\naddr |= (u64)__raw_readl(d + DMA_DESC_ADDRESS_HI) << 32;\r\n#endif\r\nreturn addr;\r\n}\r\nstatic inline u32 bcmgenet_rbuf_ctrl_get(struct bcmgenet_priv *priv)\r\n{\r\nif (GENET_IS_V1(priv))\r\nreturn bcmgenet_rbuf_readl(priv, RBUF_FLUSH_CTRL_V1);\r\nelse\r\nreturn bcmgenet_sys_readl(priv, SYS_RBUF_FLUSH_CTRL);\r\n}\r\nstatic inline void bcmgenet_rbuf_ctrl_set(struct bcmgenet_priv *priv, u32 val)\r\n{\r\nif (GENET_IS_V1(priv))\r\nbcmgenet_rbuf_writel(priv, val, RBUF_FLUSH_CTRL_V1);\r\nelse\r\nbcmgenet_sys_writel(priv, val, SYS_RBUF_FLUSH_CTRL);\r\n}\r\nstatic inline u32 bcmgenet_tbuf_ctrl_get(struct bcmgenet_priv *priv)\r\n{\r\nif (GENET_IS_V1(priv))\r\nreturn bcmgenet_rbuf_readl(priv, TBUF_CTRL_V1);\r\nelse\r\nreturn __raw_readl(priv->base +\r\npriv->hw_params->tbuf_offset + TBUF_CTRL);\r\n}\r\nstatic inline void bcmgenet_tbuf_ctrl_set(struct bcmgenet_priv *priv, u32 val)\r\n{\r\nif (GENET_IS_V1(priv))\r\nbcmgenet_rbuf_writel(priv, val, TBUF_CTRL_V1);\r\nelse\r\n__raw_writel(val, priv->base +\r\npriv->hw_params->tbuf_offset + TBUF_CTRL);\r\n}\r\nstatic inline u32 bcmgenet_bp_mc_get(struct bcmgenet_priv *priv)\r\n{\r\nif (GENET_IS_V1(priv))\r\nreturn bcmgenet_rbuf_readl(priv, TBUF_BP_MC_V1);\r\nelse\r\nreturn __raw_readl(priv->base +\r\npriv->hw_params->tbuf_offset + TBUF_BP_MC);\r\n}\r\nstatic inline void bcmgenet_bp_mc_set(struct bcmgenet_priv *priv, u32 val)\r\n{\r\nif (GENET_IS_V1(priv))\r\nbcmgenet_rbuf_writel(priv, val, TBUF_BP_MC_V1);\r\nelse\r\n__raw_writel(val, priv->base +\r\npriv->hw_params->tbuf_offset + TBUF_BP_MC);\r\n}\r\nstatic inline struct bcmgenet_priv *dev_to_priv(struct device *dev)\r\n{\r\nreturn netdev_priv(dev_get_drvdata(dev));\r\n}\r\nstatic inline u32 bcmgenet_tdma_readl(struct bcmgenet_priv *priv,\r\nenum dma_reg r)\r\n{\r\nreturn __raw_readl(priv->base + GENET_TDMA_REG_OFF +\r\nDMA_RINGS_SIZE + bcmgenet_dma_regs[r]);\r\n}\r\nstatic inline void bcmgenet_tdma_writel(struct bcmgenet_priv *priv,\r\nu32 val, enum dma_reg r)\r\n{\r\n__raw_writel(val, priv->base + GENET_TDMA_REG_OFF +\r\nDMA_RINGS_SIZE + bcmgenet_dma_regs[r]);\r\n}\r\nstatic inline u32 bcmgenet_rdma_readl(struct bcmgenet_priv *priv,\r\nenum dma_reg r)\r\n{\r\nreturn __raw_readl(priv->base + GENET_RDMA_REG_OFF +\r\nDMA_RINGS_SIZE + bcmgenet_dma_regs[r]);\r\n}\r\nstatic inline void bcmgenet_rdma_writel(struct bcmgenet_priv *priv,\r\nu32 val, enum dma_reg r)\r\n{\r\n__raw_writel(val, priv->base + GENET_RDMA_REG_OFF +\r\nDMA_RINGS_SIZE + bcmgenet_dma_regs[r]);\r\n}\r\nstatic inline u32 bcmgenet_tdma_ring_readl(struct bcmgenet_priv *priv,\r\nunsigned int ring,\r\nenum dma_ring_reg r)\r\n{\r\nreturn __raw_readl(priv->base + GENET_TDMA_REG_OFF +\r\n(DMA_RING_SIZE * ring) +\r\ngenet_dma_ring_regs[r]);\r\n}\r\nstatic inline void bcmgenet_tdma_ring_writel(struct bcmgenet_priv *priv,\r\nunsigned int ring, u32 val,\r\nenum dma_ring_reg r)\r\n{\r\n__raw_writel(val, priv->base + GENET_TDMA_REG_OFF +\r\n(DMA_RING_SIZE * ring) +\r\ngenet_dma_ring_regs[r]);\r\n}\r\nstatic inline u32 bcmgenet_rdma_ring_readl(struct bcmgenet_priv *priv,\r\nunsigned int ring,\r\nenum dma_ring_reg r)\r\n{\r\nreturn __raw_readl(priv->base + GENET_RDMA_REG_OFF +\r\n(DMA_RING_SIZE * ring) +\r\ngenet_dma_ring_regs[r]);\r\n}\r\nstatic inline void bcmgenet_rdma_ring_writel(struct bcmgenet_priv *priv,\r\nunsigned int ring, u32 val,\r\nenum dma_ring_reg r)\r\n{\r\n__raw_writel(val, priv->base + GENET_RDMA_REG_OFF +\r\n(DMA_RING_SIZE * ring) +\r\ngenet_dma_ring_regs[r]);\r\n}\r\nstatic int bcmgenet_get_settings(struct net_device *dev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nif (!priv->phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_gset(priv->phydev, cmd);\r\n}\r\nstatic int bcmgenet_set_settings(struct net_device *dev,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nif (!priv->phydev)\r\nreturn -ENODEV;\r\nreturn phy_ethtool_sset(priv->phydev, cmd);\r\n}\r\nstatic int bcmgenet_set_rx_csum(struct net_device *dev,\r\nnetdev_features_t wanted)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nu32 rbuf_chk_ctrl;\r\nbool rx_csum_en;\r\nrx_csum_en = !!(wanted & NETIF_F_RXCSUM);\r\nrbuf_chk_ctrl = bcmgenet_rbuf_readl(priv, RBUF_CHK_CTRL);\r\nif (rx_csum_en)\r\nrbuf_chk_ctrl |= RBUF_RXCHK_EN;\r\nelse\r\nrbuf_chk_ctrl &= ~RBUF_RXCHK_EN;\r\npriv->desc_rxchk_en = rx_csum_en;\r\nif (rx_csum_en && priv->crc_fwd_en)\r\nrbuf_chk_ctrl |= RBUF_SKIP_FCS;\r\nelse\r\nrbuf_chk_ctrl &= ~RBUF_SKIP_FCS;\r\nbcmgenet_rbuf_writel(priv, rbuf_chk_ctrl, RBUF_CHK_CTRL);\r\nreturn 0;\r\n}\r\nstatic int bcmgenet_set_tx_csum(struct net_device *dev,\r\nnetdev_features_t wanted)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nbool desc_64b_en;\r\nu32 tbuf_ctrl, rbuf_ctrl;\r\ntbuf_ctrl = bcmgenet_tbuf_ctrl_get(priv);\r\nrbuf_ctrl = bcmgenet_rbuf_readl(priv, RBUF_CTRL);\r\ndesc_64b_en = !!(wanted & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM));\r\nif (desc_64b_en) {\r\ntbuf_ctrl |= RBUF_64B_EN;\r\nrbuf_ctrl |= RBUF_64B_EN;\r\n} else {\r\ntbuf_ctrl &= ~RBUF_64B_EN;\r\nrbuf_ctrl &= ~RBUF_64B_EN;\r\n}\r\npriv->desc_64b_en = desc_64b_en;\r\nbcmgenet_tbuf_ctrl_set(priv, tbuf_ctrl);\r\nbcmgenet_rbuf_writel(priv, rbuf_ctrl, RBUF_CTRL);\r\nreturn 0;\r\n}\r\nstatic int bcmgenet_set_features(struct net_device *dev,\r\nnetdev_features_t features)\r\n{\r\nnetdev_features_t changed = features ^ dev->features;\r\nnetdev_features_t wanted = dev->wanted_features;\r\nint ret = 0;\r\nif (changed & (NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM))\r\nret = bcmgenet_set_tx_csum(dev, wanted);\r\nif (changed & (NETIF_F_RXCSUM))\r\nret = bcmgenet_set_rx_csum(dev, wanted);\r\nreturn ret;\r\n}\r\nstatic u32 bcmgenet_get_msglevel(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nreturn priv->msg_enable;\r\n}\r\nstatic void bcmgenet_set_msglevel(struct net_device *dev, u32 level)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\npriv->msg_enable = level;\r\n}\r\nstatic int bcmgenet_get_coalesce(struct net_device *dev,\r\nstruct ethtool_coalesce *ec)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nec->tx_max_coalesced_frames =\r\nbcmgenet_tdma_ring_readl(priv, DESC_INDEX,\r\nDMA_MBUF_DONE_THRESH);\r\nec->rx_max_coalesced_frames =\r\nbcmgenet_rdma_ring_readl(priv, DESC_INDEX,\r\nDMA_MBUF_DONE_THRESH);\r\nec->rx_coalesce_usecs =\r\nbcmgenet_rdma_readl(priv, DMA_RING16_TIMEOUT) * 8192 / 1000;\r\nreturn 0;\r\n}\r\nstatic int bcmgenet_set_coalesce(struct net_device *dev,\r\nstruct ethtool_coalesce *ec)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nunsigned int i;\r\nu32 reg;\r\nif (ec->tx_max_coalesced_frames > DMA_INTR_THRESHOLD_MASK ||\r\nec->tx_max_coalesced_frames == 0 ||\r\nec->rx_max_coalesced_frames > DMA_INTR_THRESHOLD_MASK ||\r\nec->rx_coalesce_usecs > (DMA_TIMEOUT_MASK * 8) + 1)\r\nreturn -EINVAL;\r\nif (ec->rx_coalesce_usecs == 0 && ec->rx_max_coalesced_frames == 0)\r\nreturn -EINVAL;\r\nif (ec->tx_coalesce_usecs || ec->tx_coalesce_usecs_high ||\r\nec->tx_coalesce_usecs_irq || ec->tx_coalesce_usecs_low)\r\nreturn -EOPNOTSUPP;\r\nfor (i = 0; i < priv->hw_params->tx_queues; i++)\r\nbcmgenet_tdma_ring_writel(priv, i,\r\nec->tx_max_coalesced_frames,\r\nDMA_MBUF_DONE_THRESH);\r\nbcmgenet_tdma_ring_writel(priv, DESC_INDEX,\r\nec->tx_max_coalesced_frames,\r\nDMA_MBUF_DONE_THRESH);\r\nfor (i = 0; i < priv->hw_params->rx_queues; i++) {\r\nbcmgenet_rdma_ring_writel(priv, i,\r\nec->rx_max_coalesced_frames,\r\nDMA_MBUF_DONE_THRESH);\r\nreg = bcmgenet_rdma_readl(priv, DMA_RING0_TIMEOUT + i);\r\nreg &= ~DMA_TIMEOUT_MASK;\r\nreg |= DIV_ROUND_UP(ec->rx_coalesce_usecs * 1000, 8192);\r\nbcmgenet_rdma_writel(priv, reg, DMA_RING0_TIMEOUT + i);\r\n}\r\nbcmgenet_rdma_ring_writel(priv, DESC_INDEX,\r\nec->rx_max_coalesced_frames,\r\nDMA_MBUF_DONE_THRESH);\r\nreg = bcmgenet_rdma_readl(priv, DMA_RING16_TIMEOUT);\r\nreg &= ~DMA_TIMEOUT_MASK;\r\nreg |= DIV_ROUND_UP(ec->rx_coalesce_usecs * 1000, 8192);\r\nbcmgenet_rdma_writel(priv, reg, DMA_RING16_TIMEOUT);\r\nreturn 0;\r\n}\r\nstatic void bcmgenet_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *info)\r\n{\r\nstrlcpy(info->driver, "bcmgenet", sizeof(info->driver));\r\nstrlcpy(info->version, "v2.0", sizeof(info->version));\r\n}\r\nstatic int bcmgenet_get_sset_count(struct net_device *dev, int string_set)\r\n{\r\nswitch (string_set) {\r\ncase ETH_SS_STATS:\r\nreturn BCMGENET_STATS_LEN;\r\ndefault:\r\nreturn -EOPNOTSUPP;\r\n}\r\n}\r\nstatic void bcmgenet_get_strings(struct net_device *dev, u32 stringset,\r\nu8 *data)\r\n{\r\nint i;\r\nswitch (stringset) {\r\ncase ETH_SS_STATS:\r\nfor (i = 0; i < BCMGENET_STATS_LEN; i++) {\r\nmemcpy(data + i * ETH_GSTRING_LEN,\r\nbcmgenet_gstrings_stats[i].stat_string,\r\nETH_GSTRING_LEN);\r\n}\r\nbreak;\r\n}\r\n}\r\nstatic void bcmgenet_update_mib_counters(struct bcmgenet_priv *priv)\r\n{\r\nint i, j = 0;\r\nfor (i = 0; i < BCMGENET_STATS_LEN; i++) {\r\nconst struct bcmgenet_stats *s;\r\nu8 offset = 0;\r\nu32 val = 0;\r\nchar *p;\r\ns = &bcmgenet_gstrings_stats[i];\r\nswitch (s->type) {\r\ncase BCMGENET_STAT_NETDEV:\r\ncase BCMGENET_STAT_SOFT:\r\ncontinue;\r\ncase BCMGENET_STAT_MIB_RX:\r\ncase BCMGENET_STAT_MIB_TX:\r\ncase BCMGENET_STAT_RUNT:\r\nif (s->type != BCMGENET_STAT_MIB_RX)\r\noffset = BCMGENET_STAT_OFFSET;\r\nval = bcmgenet_umac_readl(priv,\r\nUMAC_MIB_START + j + offset);\r\nbreak;\r\ncase BCMGENET_STAT_MISC:\r\nval = bcmgenet_umac_readl(priv, s->reg_offset);\r\nif (val == ~0)\r\nbcmgenet_umac_writel(priv, 0, s->reg_offset);\r\nbreak;\r\n}\r\nj += s->stat_sizeof;\r\np = (char *)priv + s->stat_offset;\r\n*(u32 *)p = val;\r\n}\r\n}\r\nstatic void bcmgenet_get_ethtool_stats(struct net_device *dev,\r\nstruct ethtool_stats *stats,\r\nu64 *data)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nint i;\r\nif (netif_running(dev))\r\nbcmgenet_update_mib_counters(priv);\r\nfor (i = 0; i < BCMGENET_STATS_LEN; i++) {\r\nconst struct bcmgenet_stats *s;\r\nchar *p;\r\ns = &bcmgenet_gstrings_stats[i];\r\nif (s->type == BCMGENET_STAT_NETDEV)\r\np = (char *)&dev->stats;\r\nelse\r\np = (char *)priv;\r\np += s->stat_offset;\r\nif (sizeof(unsigned long) != sizeof(u32) &&\r\ns->stat_sizeof == sizeof(unsigned long))\r\ndata[i] = *(unsigned long *)p;\r\nelse\r\ndata[i] = *(u32 *)p;\r\n}\r\n}\r\nstatic void bcmgenet_eee_enable_set(struct net_device *dev, bool enable)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nu32 off = priv->hw_params->tbuf_offset + TBUF_ENERGY_CTRL;\r\nu32 reg;\r\nif (enable && !priv->clk_eee_enabled) {\r\nclk_prepare_enable(priv->clk_eee);\r\npriv->clk_eee_enabled = true;\r\n}\r\nreg = bcmgenet_umac_readl(priv, UMAC_EEE_CTRL);\r\nif (enable)\r\nreg |= EEE_EN;\r\nelse\r\nreg &= ~EEE_EN;\r\nbcmgenet_umac_writel(priv, reg, UMAC_EEE_CTRL);\r\nreg = __raw_readl(priv->base + off);\r\nif (enable)\r\nreg |= TBUF_EEE_EN | TBUF_PM_EN;\r\nelse\r\nreg &= ~(TBUF_EEE_EN | TBUF_PM_EN);\r\n__raw_writel(reg, priv->base + off);\r\nreg = bcmgenet_rbuf_readl(priv, RBUF_ENERGY_CTRL);\r\nif (enable)\r\nreg |= RBUF_EEE_EN | RBUF_PM_EN;\r\nelse\r\nreg &= ~(RBUF_EEE_EN | RBUF_PM_EN);\r\nbcmgenet_rbuf_writel(priv, reg, RBUF_ENERGY_CTRL);\r\nif (!enable && priv->clk_eee_enabled) {\r\nclk_disable_unprepare(priv->clk_eee);\r\npriv->clk_eee_enabled = false;\r\n}\r\npriv->eee.eee_enabled = enable;\r\npriv->eee.eee_active = enable;\r\n}\r\nstatic int bcmgenet_get_eee(struct net_device *dev, struct ethtool_eee *e)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nstruct ethtool_eee *p = &priv->eee;\r\nif (GENET_IS_V1(priv))\r\nreturn -EOPNOTSUPP;\r\ne->eee_enabled = p->eee_enabled;\r\ne->eee_active = p->eee_active;\r\ne->tx_lpi_timer = bcmgenet_umac_readl(priv, UMAC_EEE_LPI_TIMER);\r\nreturn phy_ethtool_get_eee(priv->phydev, e);\r\n}\r\nstatic int bcmgenet_set_eee(struct net_device *dev, struct ethtool_eee *e)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nstruct ethtool_eee *p = &priv->eee;\r\nint ret = 0;\r\nif (GENET_IS_V1(priv))\r\nreturn -EOPNOTSUPP;\r\np->eee_enabled = e->eee_enabled;\r\nif (!p->eee_enabled) {\r\nbcmgenet_eee_enable_set(dev, false);\r\n} else {\r\nret = phy_init_eee(priv->phydev, 0);\r\nif (ret) {\r\nnetif_err(priv, hw, dev, "EEE initialization failed\n");\r\nreturn ret;\r\n}\r\nbcmgenet_umac_writel(priv, e->tx_lpi_timer, UMAC_EEE_LPI_TIMER);\r\nbcmgenet_eee_enable_set(dev, true);\r\n}\r\nreturn phy_ethtool_set_eee(priv->phydev, e);\r\n}\r\nstatic int bcmgenet_nway_reset(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nreturn genphy_restart_aneg(priv->phydev);\r\n}\r\nstatic int bcmgenet_power_down(struct bcmgenet_priv *priv,\r\nenum bcmgenet_power_mode mode)\r\n{\r\nint ret = 0;\r\nu32 reg;\r\nswitch (mode) {\r\ncase GENET_POWER_CABLE_SENSE:\r\nphy_detach(priv->phydev);\r\nbreak;\r\ncase GENET_POWER_WOL_MAGIC:\r\nret = bcmgenet_wol_power_down_cfg(priv, mode);\r\nbreak;\r\ncase GENET_POWER_PASSIVE:\r\nif (priv->hw_params->flags & GENET_HAS_EXT) {\r\nreg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);\r\nreg |= (EXT_PWR_DOWN_PHY |\r\nEXT_PWR_DOWN_DLL | EXT_PWR_DOWN_BIAS);\r\nbcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);\r\nbcmgenet_phy_power_set(priv->dev, false);\r\n}\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic void bcmgenet_power_up(struct bcmgenet_priv *priv,\r\nenum bcmgenet_power_mode mode)\r\n{\r\nu32 reg;\r\nif (!(priv->hw_params->flags & GENET_HAS_EXT))\r\nreturn;\r\nreg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);\r\nswitch (mode) {\r\ncase GENET_POWER_PASSIVE:\r\nreg &= ~(EXT_PWR_DOWN_DLL | EXT_PWR_DOWN_PHY |\r\nEXT_PWR_DOWN_BIAS);\r\ncase GENET_POWER_CABLE_SENSE:\r\nreg |= EXT_PWR_DN_EN_LD;\r\nbreak;\r\ncase GENET_POWER_WOL_MAGIC:\r\nbcmgenet_wol_power_up_cfg(priv, mode);\r\nreturn;\r\ndefault:\r\nbreak;\r\n}\r\nbcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);\r\nif (mode == GENET_POWER_PASSIVE) {\r\nbcmgenet_phy_power_set(priv->dev, true);\r\nbcmgenet_mii_reset(priv->dev);\r\n}\r\n}\r\nstatic int bcmgenet_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nint val = 0;\r\nif (!netif_running(dev))\r\nreturn -EINVAL;\r\nswitch (cmd) {\r\ncase SIOCGMIIPHY:\r\ncase SIOCGMIIREG:\r\ncase SIOCSMIIREG:\r\nif (!priv->phydev)\r\nval = -ENODEV;\r\nelse\r\nval = phy_mii_ioctl(priv->phydev, rq, cmd);\r\nbreak;\r\ndefault:\r\nval = -EINVAL;\r\nbreak;\r\n}\r\nreturn val;\r\n}\r\nstatic struct enet_cb *bcmgenet_get_txcb(struct bcmgenet_priv *priv,\r\nstruct bcmgenet_tx_ring *ring)\r\n{\r\nstruct enet_cb *tx_cb_ptr;\r\ntx_cb_ptr = ring->cbs;\r\ntx_cb_ptr += ring->write_ptr - ring->cb_ptr;\r\nif (ring->write_ptr == ring->end_ptr)\r\nring->write_ptr = ring->cb_ptr;\r\nelse\r\nring->write_ptr++;\r\nreturn tx_cb_ptr;\r\n}\r\nstatic void bcmgenet_free_cb(struct enet_cb *cb)\r\n{\r\ndev_kfree_skb_any(cb->skb);\r\ncb->skb = NULL;\r\ndma_unmap_addr_set(cb, dma_addr, 0);\r\n}\r\nstatic inline void bcmgenet_rx_ring16_int_disable(struct bcmgenet_rx_ring *ring)\r\n{\r\nbcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_RXDMA_DONE,\r\nINTRL2_CPU_MASK_SET);\r\n}\r\nstatic inline void bcmgenet_rx_ring16_int_enable(struct bcmgenet_rx_ring *ring)\r\n{\r\nbcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_RXDMA_DONE,\r\nINTRL2_CPU_MASK_CLEAR);\r\n}\r\nstatic inline void bcmgenet_rx_ring_int_disable(struct bcmgenet_rx_ring *ring)\r\n{\r\nbcmgenet_intrl2_1_writel(ring->priv,\r\n1 << (UMAC_IRQ1_RX_INTR_SHIFT + ring->index),\r\nINTRL2_CPU_MASK_SET);\r\n}\r\nstatic inline void bcmgenet_rx_ring_int_enable(struct bcmgenet_rx_ring *ring)\r\n{\r\nbcmgenet_intrl2_1_writel(ring->priv,\r\n1 << (UMAC_IRQ1_RX_INTR_SHIFT + ring->index),\r\nINTRL2_CPU_MASK_CLEAR);\r\n}\r\nstatic inline void bcmgenet_tx_ring16_int_disable(struct bcmgenet_tx_ring *ring)\r\n{\r\nbcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_TXDMA_DONE,\r\nINTRL2_CPU_MASK_SET);\r\n}\r\nstatic inline void bcmgenet_tx_ring16_int_enable(struct bcmgenet_tx_ring *ring)\r\n{\r\nbcmgenet_intrl2_0_writel(ring->priv, UMAC_IRQ_TXDMA_DONE,\r\nINTRL2_CPU_MASK_CLEAR);\r\n}\r\nstatic inline void bcmgenet_tx_ring_int_enable(struct bcmgenet_tx_ring *ring)\r\n{\r\nbcmgenet_intrl2_1_writel(ring->priv, 1 << ring->index,\r\nINTRL2_CPU_MASK_CLEAR);\r\n}\r\nstatic inline void bcmgenet_tx_ring_int_disable(struct bcmgenet_tx_ring *ring)\r\n{\r\nbcmgenet_intrl2_1_writel(ring->priv, 1 << ring->index,\r\nINTRL2_CPU_MASK_SET);\r\n}\r\nstatic unsigned int __bcmgenet_tx_reclaim(struct net_device *dev,\r\nstruct bcmgenet_tx_ring *ring)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nstruct enet_cb *tx_cb_ptr;\r\nstruct netdev_queue *txq;\r\nunsigned int pkts_compl = 0;\r\nunsigned int bytes_compl = 0;\r\nunsigned int c_index;\r\nunsigned int txbds_ready;\r\nunsigned int txbds_processed = 0;\r\nc_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_CONS_INDEX);\r\nc_index &= DMA_C_INDEX_MASK;\r\nif (likely(c_index >= ring->c_index))\r\ntxbds_ready = c_index - ring->c_index;\r\nelse\r\ntxbds_ready = (DMA_C_INDEX_MASK + 1) - ring->c_index + c_index;\r\nnetif_dbg(priv, tx_done, dev,\r\n"%s ring=%d old_c_index=%u c_index=%u txbds_ready=%u\n",\r\n__func__, ring->index, ring->c_index, c_index, txbds_ready);\r\nwhile (txbds_processed < txbds_ready) {\r\ntx_cb_ptr = &priv->tx_cbs[ring->clean_ptr];\r\nif (tx_cb_ptr->skb) {\r\npkts_compl++;\r\nbytes_compl += GENET_CB(tx_cb_ptr->skb)->bytes_sent;\r\ndma_unmap_single(&dev->dev,\r\ndma_unmap_addr(tx_cb_ptr, dma_addr),\r\ndma_unmap_len(tx_cb_ptr, dma_len),\r\nDMA_TO_DEVICE);\r\nbcmgenet_free_cb(tx_cb_ptr);\r\n} else if (dma_unmap_addr(tx_cb_ptr, dma_addr)) {\r\ndma_unmap_page(&dev->dev,\r\ndma_unmap_addr(tx_cb_ptr, dma_addr),\r\ndma_unmap_len(tx_cb_ptr, dma_len),\r\nDMA_TO_DEVICE);\r\ndma_unmap_addr_set(tx_cb_ptr, dma_addr, 0);\r\n}\r\ntxbds_processed++;\r\nif (likely(ring->clean_ptr < ring->end_ptr))\r\nring->clean_ptr++;\r\nelse\r\nring->clean_ptr = ring->cb_ptr;\r\n}\r\nring->free_bds += txbds_processed;\r\nring->c_index = (ring->c_index + txbds_processed) & DMA_C_INDEX_MASK;\r\ndev->stats.tx_packets += pkts_compl;\r\ndev->stats.tx_bytes += bytes_compl;\r\ntxq = netdev_get_tx_queue(dev, ring->queue);\r\nnetdev_tx_completed_queue(txq, pkts_compl, bytes_compl);\r\nif (ring->free_bds > (MAX_SKB_FRAGS + 1)) {\r\nif (netif_tx_queue_stopped(txq))\r\nnetif_tx_wake_queue(txq);\r\n}\r\nreturn pkts_compl;\r\n}\r\nstatic unsigned int bcmgenet_tx_reclaim(struct net_device *dev,\r\nstruct bcmgenet_tx_ring *ring)\r\n{\r\nunsigned int released;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ring->lock, flags);\r\nreleased = __bcmgenet_tx_reclaim(dev, ring);\r\nspin_unlock_irqrestore(&ring->lock, flags);\r\nreturn released;\r\n}\r\nstatic int bcmgenet_tx_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct bcmgenet_tx_ring *ring =\r\ncontainer_of(napi, struct bcmgenet_tx_ring, napi);\r\nunsigned int work_done = 0;\r\nwork_done = bcmgenet_tx_reclaim(ring->priv->dev, ring);\r\nif (work_done == 0) {\r\nnapi_complete(napi);\r\nring->int_enable(ring);\r\nreturn 0;\r\n}\r\nreturn budget;\r\n}\r\nstatic void bcmgenet_tx_reclaim_all(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nint i;\r\nif (netif_is_multiqueue(dev)) {\r\nfor (i = 0; i < priv->hw_params->tx_queues; i++)\r\nbcmgenet_tx_reclaim(dev, &priv->tx_rings[i]);\r\n}\r\nbcmgenet_tx_reclaim(dev, &priv->tx_rings[DESC_INDEX]);\r\n}\r\nstatic int bcmgenet_xmit_single(struct net_device *dev,\r\nstruct sk_buff *skb,\r\nu16 dma_desc_flags,\r\nstruct bcmgenet_tx_ring *ring)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nstruct device *kdev = &priv->pdev->dev;\r\nstruct enet_cb *tx_cb_ptr;\r\nunsigned int skb_len;\r\ndma_addr_t mapping;\r\nu32 length_status;\r\nint ret;\r\ntx_cb_ptr = bcmgenet_get_txcb(priv, ring);\r\nif (unlikely(!tx_cb_ptr))\r\nBUG();\r\ntx_cb_ptr->skb = skb;\r\nskb_len = skb_headlen(skb);\r\nmapping = dma_map_single(kdev, skb->data, skb_len, DMA_TO_DEVICE);\r\nret = dma_mapping_error(kdev, mapping);\r\nif (ret) {\r\npriv->mib.tx_dma_failed++;\r\nnetif_err(priv, tx_err, dev, "Tx DMA map failed\n");\r\ndev_kfree_skb(skb);\r\nreturn ret;\r\n}\r\ndma_unmap_addr_set(tx_cb_ptr, dma_addr, mapping);\r\ndma_unmap_len_set(tx_cb_ptr, dma_len, skb_len);\r\nlength_status = (skb_len << DMA_BUFLENGTH_SHIFT) | dma_desc_flags |\r\n(priv->hw_params->qtag_mask << DMA_TX_QTAG_SHIFT) |\r\nDMA_TX_APPEND_CRC;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL)\r\nlength_status |= DMA_TX_DO_CSUM;\r\ndmadesc_set(priv, tx_cb_ptr->bd_addr, mapping, length_status);\r\nreturn 0;\r\n}\r\nstatic int bcmgenet_xmit_frag(struct net_device *dev,\r\nskb_frag_t *frag,\r\nu16 dma_desc_flags,\r\nstruct bcmgenet_tx_ring *ring)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nstruct device *kdev = &priv->pdev->dev;\r\nstruct enet_cb *tx_cb_ptr;\r\nunsigned int frag_size;\r\ndma_addr_t mapping;\r\nint ret;\r\ntx_cb_ptr = bcmgenet_get_txcb(priv, ring);\r\nif (unlikely(!tx_cb_ptr))\r\nBUG();\r\ntx_cb_ptr->skb = NULL;\r\nfrag_size = skb_frag_size(frag);\r\nmapping = skb_frag_dma_map(kdev, frag, 0, frag_size, DMA_TO_DEVICE);\r\nret = dma_mapping_error(kdev, mapping);\r\nif (ret) {\r\npriv->mib.tx_dma_failed++;\r\nnetif_err(priv, tx_err, dev, "%s: Tx DMA map failed\n",\r\n__func__);\r\nreturn ret;\r\n}\r\ndma_unmap_addr_set(tx_cb_ptr, dma_addr, mapping);\r\ndma_unmap_len_set(tx_cb_ptr, dma_len, frag_size);\r\ndmadesc_set(priv, tx_cb_ptr->bd_addr, mapping,\r\n(frag_size << DMA_BUFLENGTH_SHIFT) | dma_desc_flags |\r\n(priv->hw_params->qtag_mask << DMA_TX_QTAG_SHIFT));\r\nreturn 0;\r\n}\r\nstatic struct sk_buff *bcmgenet_put_tx_csum(struct net_device *dev,\r\nstruct sk_buff *skb)\r\n{\r\nstruct status_64 *status = NULL;\r\nstruct sk_buff *new_skb;\r\nu16 offset;\r\nu8 ip_proto;\r\nu16 ip_ver;\r\nu32 tx_csum_info;\r\nif (unlikely(skb_headroom(skb) < sizeof(*status))) {\r\nnew_skb = skb_realloc_headroom(skb, sizeof(*status));\r\ndev_kfree_skb(skb);\r\nif (!new_skb) {\r\ndev->stats.tx_dropped++;\r\nreturn NULL;\r\n}\r\nskb = new_skb;\r\n}\r\nskb_push(skb, sizeof(*status));\r\nstatus = (struct status_64 *)skb->data;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nip_ver = htons(skb->protocol);\r\nswitch (ip_ver) {\r\ncase ETH_P_IP:\r\nip_proto = ip_hdr(skb)->protocol;\r\nbreak;\r\ncase ETH_P_IPV6:\r\nip_proto = ipv6_hdr(skb)->nexthdr;\r\nbreak;\r\ndefault:\r\nreturn skb;\r\n}\r\noffset = skb_checksum_start_offset(skb) - sizeof(*status);\r\ntx_csum_info = (offset << STATUS_TX_CSUM_START_SHIFT) |\r\n(offset + skb->csum_offset);\r\nif (ip_proto == IPPROTO_TCP || ip_proto == IPPROTO_UDP) {\r\ntx_csum_info |= STATUS_TX_CSUM_LV;\r\nif (ip_proto == IPPROTO_UDP && ip_ver == ETH_P_IP)\r\ntx_csum_info |= STATUS_TX_CSUM_PROTO_UDP;\r\n} else {\r\ntx_csum_info = 0;\r\n}\r\nstatus->tx_csum_info = tx_csum_info;\r\n}\r\nreturn skb;\r\n}\r\nstatic netdev_tx_t bcmgenet_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nstruct bcmgenet_tx_ring *ring = NULL;\r\nstruct netdev_queue *txq;\r\nunsigned long flags = 0;\r\nint nr_frags, index;\r\nu16 dma_desc_flags;\r\nint ret;\r\nint i;\r\nindex = skb_get_queue_mapping(skb);\r\nif (index == 0)\r\nindex = DESC_INDEX;\r\nelse\r\nindex -= 1;\r\nring = &priv->tx_rings[index];\r\ntxq = netdev_get_tx_queue(dev, ring->queue);\r\nnr_frags = skb_shinfo(skb)->nr_frags;\r\nspin_lock_irqsave(&ring->lock, flags);\r\nif (ring->free_bds <= (nr_frags + 1)) {\r\nif (!netif_tx_queue_stopped(txq)) {\r\nnetif_tx_stop_queue(txq);\r\nnetdev_err(dev,\r\n"%s: tx ring %d full when queue %d awake\n",\r\n__func__, index, ring->queue);\r\n}\r\nret = NETDEV_TX_BUSY;\r\ngoto out;\r\n}\r\nif (skb_padto(skb, ETH_ZLEN)) {\r\nret = NETDEV_TX_OK;\r\ngoto out;\r\n}\r\nGENET_CB(skb)->bytes_sent = skb->len;\r\nif (priv->desc_64b_en) {\r\nskb = bcmgenet_put_tx_csum(dev, skb);\r\nif (!skb) {\r\nret = NETDEV_TX_OK;\r\ngoto out;\r\n}\r\n}\r\ndma_desc_flags = DMA_SOP;\r\nif (nr_frags == 0)\r\ndma_desc_flags |= DMA_EOP;\r\nret = bcmgenet_xmit_single(dev, skb, dma_desc_flags, ring);\r\nif (ret) {\r\nret = NETDEV_TX_OK;\r\ngoto out;\r\n}\r\nfor (i = 0; i < nr_frags; i++) {\r\nret = bcmgenet_xmit_frag(dev,\r\n&skb_shinfo(skb)->frags[i],\r\n(i == nr_frags - 1) ? DMA_EOP : 0,\r\nring);\r\nif (ret) {\r\nret = NETDEV_TX_OK;\r\ngoto out;\r\n}\r\n}\r\nskb_tx_timestamp(skb);\r\nring->free_bds -= nr_frags + 1;\r\nring->prod_index += nr_frags + 1;\r\nring->prod_index &= DMA_P_INDEX_MASK;\r\nnetdev_tx_sent_queue(txq, GENET_CB(skb)->bytes_sent);\r\nif (ring->free_bds <= (MAX_SKB_FRAGS + 1))\r\nnetif_tx_stop_queue(txq);\r\nif (!skb->xmit_more || netif_xmit_stopped(txq))\r\nbcmgenet_tdma_ring_writel(priv, ring->index,\r\nring->prod_index, TDMA_PROD_INDEX);\r\nout:\r\nspin_unlock_irqrestore(&ring->lock, flags);\r\nreturn ret;\r\n}\r\nstatic struct sk_buff *bcmgenet_rx_refill(struct bcmgenet_priv *priv,\r\nstruct enet_cb *cb)\r\n{\r\nstruct device *kdev = &priv->pdev->dev;\r\nstruct sk_buff *skb;\r\nstruct sk_buff *rx_skb;\r\ndma_addr_t mapping;\r\nskb = netdev_alloc_skb(priv->dev, priv->rx_buf_len + SKB_ALIGNMENT);\r\nif (!skb) {\r\npriv->mib.alloc_rx_buff_failed++;\r\nnetif_err(priv, rx_err, priv->dev,\r\n"%s: Rx skb allocation failed\n", __func__);\r\nreturn NULL;\r\n}\r\nmapping = dma_map_single(kdev, skb->data, priv->rx_buf_len,\r\nDMA_FROM_DEVICE);\r\nif (dma_mapping_error(kdev, mapping)) {\r\npriv->mib.rx_dma_failed++;\r\ndev_kfree_skb_any(skb);\r\nnetif_err(priv, rx_err, priv->dev,\r\n"%s: Rx skb DMA mapping failed\n", __func__);\r\nreturn NULL;\r\n}\r\nrx_skb = cb->skb;\r\nif (likely(rx_skb))\r\ndma_unmap_single(kdev, dma_unmap_addr(cb, dma_addr),\r\npriv->rx_buf_len, DMA_FROM_DEVICE);\r\ncb->skb = skb;\r\ndma_unmap_addr_set(cb, dma_addr, mapping);\r\ndmadesc_set_addr(priv, cb->bd_addr, mapping);\r\nreturn rx_skb;\r\n}\r\nstatic unsigned int bcmgenet_desc_rx(struct bcmgenet_rx_ring *ring,\r\nunsigned int budget)\r\n{\r\nstruct bcmgenet_priv *priv = ring->priv;\r\nstruct net_device *dev = priv->dev;\r\nstruct enet_cb *cb;\r\nstruct sk_buff *skb;\r\nu32 dma_length_status;\r\nunsigned long dma_flag;\r\nint len;\r\nunsigned int rxpktprocessed = 0, rxpkttoprocess;\r\nunsigned int p_index;\r\nunsigned int discards;\r\nunsigned int chksum_ok = 0;\r\np_index = bcmgenet_rdma_ring_readl(priv, ring->index, RDMA_PROD_INDEX);\r\ndiscards = (p_index >> DMA_P_INDEX_DISCARD_CNT_SHIFT) &\r\nDMA_P_INDEX_DISCARD_CNT_MASK;\r\nif (discards > ring->old_discards) {\r\ndiscards = discards - ring->old_discards;\r\ndev->stats.rx_missed_errors += discards;\r\ndev->stats.rx_errors += discards;\r\nring->old_discards += discards;\r\nif (ring->old_discards >= 0xC000) {\r\nring->old_discards = 0;\r\nbcmgenet_rdma_ring_writel(priv, ring->index, 0,\r\nRDMA_PROD_INDEX);\r\n}\r\n}\r\np_index &= DMA_P_INDEX_MASK;\r\nif (likely(p_index >= ring->c_index))\r\nrxpkttoprocess = p_index - ring->c_index;\r\nelse\r\nrxpkttoprocess = (DMA_C_INDEX_MASK + 1) - ring->c_index +\r\np_index;\r\nnetif_dbg(priv, rx_status, dev,\r\n"RDMA: rxpkttoprocess=%d\n", rxpkttoprocess);\r\nwhile ((rxpktprocessed < rxpkttoprocess) &&\r\n(rxpktprocessed < budget)) {\r\ncb = &priv->rx_cbs[ring->read_ptr];\r\nskb = bcmgenet_rx_refill(priv, cb);\r\nif (unlikely(!skb)) {\r\ndev->stats.rx_dropped++;\r\ngoto next;\r\n}\r\nif (!priv->desc_64b_en) {\r\ndma_length_status =\r\ndmadesc_get_length_status(priv, cb->bd_addr);\r\n} else {\r\nstruct status_64 *status;\r\nstatus = (struct status_64 *)skb->data;\r\ndma_length_status = status->length_status;\r\n}\r\ndma_flag = dma_length_status & 0xffff;\r\nlen = dma_length_status >> DMA_BUFLENGTH_SHIFT;\r\nnetif_dbg(priv, rx_status, dev,\r\n"%s:p_ind=%d c_ind=%d read_ptr=%d len_stat=0x%08x\n",\r\n__func__, p_index, ring->c_index,\r\nring->read_ptr, dma_length_status);\r\nif (unlikely(!(dma_flag & DMA_EOP) || !(dma_flag & DMA_SOP))) {\r\nnetif_err(priv, rx_status, dev,\r\n"dropping fragmented packet!\n");\r\ndev->stats.rx_errors++;\r\ndev_kfree_skb_any(skb);\r\ngoto next;\r\n}\r\nif (unlikely(dma_flag & (DMA_RX_CRC_ERROR |\r\nDMA_RX_OV |\r\nDMA_RX_NO |\r\nDMA_RX_LG |\r\nDMA_RX_RXER))) {\r\nnetif_err(priv, rx_status, dev, "dma_flag=0x%x\n",\r\n(unsigned int)dma_flag);\r\nif (dma_flag & DMA_RX_CRC_ERROR)\r\ndev->stats.rx_crc_errors++;\r\nif (dma_flag & DMA_RX_OV)\r\ndev->stats.rx_over_errors++;\r\nif (dma_flag & DMA_RX_NO)\r\ndev->stats.rx_frame_errors++;\r\nif (dma_flag & DMA_RX_LG)\r\ndev->stats.rx_length_errors++;\r\ndev->stats.rx_errors++;\r\ndev_kfree_skb_any(skb);\r\ngoto next;\r\n}\r\nchksum_ok = (dma_flag & priv->dma_rx_chk_bit) &&\r\npriv->desc_rxchk_en;\r\nskb_put(skb, len);\r\nif (priv->desc_64b_en) {\r\nskb_pull(skb, 64);\r\nlen -= 64;\r\n}\r\nif (likely(chksum_ok))\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb_pull(skb, 2);\r\nlen -= 2;\r\nif (priv->crc_fwd_en) {\r\nskb_trim(skb, len - ETH_FCS_LEN);\r\nlen -= ETH_FCS_LEN;\r\n}\r\nskb->protocol = eth_type_trans(skb, priv->dev);\r\ndev->stats.rx_packets++;\r\ndev->stats.rx_bytes += len;\r\nif (dma_flag & DMA_RX_MULT)\r\ndev->stats.multicast++;\r\nnapi_gro_receive(&ring->napi, skb);\r\nnetif_dbg(priv, rx_status, dev, "pushed up to kernel\n");\r\nnext:\r\nrxpktprocessed++;\r\nif (likely(ring->read_ptr < ring->end_ptr))\r\nring->read_ptr++;\r\nelse\r\nring->read_ptr = ring->cb_ptr;\r\nring->c_index = (ring->c_index + 1) & DMA_C_INDEX_MASK;\r\nbcmgenet_rdma_ring_writel(priv, ring->index, ring->c_index, RDMA_CONS_INDEX);\r\n}\r\nreturn rxpktprocessed;\r\n}\r\nstatic int bcmgenet_rx_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct bcmgenet_rx_ring *ring = container_of(napi,\r\nstruct bcmgenet_rx_ring, napi);\r\nunsigned int work_done;\r\nwork_done = bcmgenet_desc_rx(ring, budget);\r\nif (work_done < budget) {\r\nnapi_complete_done(napi, work_done);\r\nring->int_enable(ring);\r\n}\r\nreturn work_done;\r\n}\r\nstatic int bcmgenet_alloc_rx_buffers(struct bcmgenet_priv *priv,\r\nstruct bcmgenet_rx_ring *ring)\r\n{\r\nstruct enet_cb *cb;\r\nstruct sk_buff *skb;\r\nint i;\r\nnetif_dbg(priv, hw, priv->dev, "%s\n", __func__);\r\nfor (i = 0; i < ring->size; i++) {\r\ncb = ring->cbs + i;\r\nskb = bcmgenet_rx_refill(priv, cb);\r\nif (skb)\r\ndev_kfree_skb_any(skb);\r\nif (!cb->skb)\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic void bcmgenet_free_rx_buffers(struct bcmgenet_priv *priv)\r\n{\r\nstruct enet_cb *cb;\r\nint i;\r\nfor (i = 0; i < priv->num_rx_bds; i++) {\r\ncb = &priv->rx_cbs[i];\r\nif (dma_unmap_addr(cb, dma_addr)) {\r\ndma_unmap_single(&priv->dev->dev,\r\ndma_unmap_addr(cb, dma_addr),\r\npriv->rx_buf_len, DMA_FROM_DEVICE);\r\ndma_unmap_addr_set(cb, dma_addr, 0);\r\n}\r\nif (cb->skb)\r\nbcmgenet_free_cb(cb);\r\n}\r\n}\r\nstatic void umac_enable_set(struct bcmgenet_priv *priv, u32 mask, bool enable)\r\n{\r\nu32 reg;\r\nreg = bcmgenet_umac_readl(priv, UMAC_CMD);\r\nif (enable)\r\nreg |= mask;\r\nelse\r\nreg &= ~mask;\r\nbcmgenet_umac_writel(priv, reg, UMAC_CMD);\r\nif (enable == 0)\r\nusleep_range(1000, 2000);\r\n}\r\nstatic int reset_umac(struct bcmgenet_priv *priv)\r\n{\r\nstruct device *kdev = &priv->pdev->dev;\r\nunsigned int timeout = 0;\r\nu32 reg;\r\nbcmgenet_rbuf_ctrl_set(priv, 0);\r\nudelay(10);\r\nbcmgenet_umac_writel(priv, 0, UMAC_CMD);\r\nbcmgenet_umac_writel(priv, CMD_SW_RESET, UMAC_CMD);\r\nwhile (timeout++ < 1000) {\r\nreg = bcmgenet_umac_readl(priv, UMAC_CMD);\r\nif (!(reg & CMD_SW_RESET))\r\nreturn 0;\r\nudelay(1);\r\n}\r\nif (timeout == 1000) {\r\ndev_err(kdev,\r\n"timeout waiting for MAC to come out of reset\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nreturn 0;\r\n}\r\nstatic void bcmgenet_intr_disable(struct bcmgenet_priv *priv)\r\n{\r\nbcmgenet_intrl2_0_writel(priv, 0xFFFFFFFF, INTRL2_CPU_MASK_SET);\r\nbcmgenet_intrl2_0_writel(priv, 0xFFFFFFFF, INTRL2_CPU_CLEAR);\r\nbcmgenet_intrl2_0_writel(priv, 0, INTRL2_CPU_MASK_CLEAR);\r\nbcmgenet_intrl2_1_writel(priv, 0xFFFFFFFF, INTRL2_CPU_MASK_SET);\r\nbcmgenet_intrl2_1_writel(priv, 0xFFFFFFFF, INTRL2_CPU_CLEAR);\r\nbcmgenet_intrl2_1_writel(priv, 0, INTRL2_CPU_MASK_CLEAR);\r\n}\r\nstatic void bcmgenet_link_intr_enable(struct bcmgenet_priv *priv)\r\n{\r\nu32 int0_enable = 0;\r\nif (priv->internal_phy) {\r\nint0_enable |= UMAC_IRQ_LINK_EVENT;\r\n} else if (priv->ext_phy) {\r\nint0_enable |= UMAC_IRQ_LINK_EVENT;\r\n} else if (priv->phy_interface == PHY_INTERFACE_MODE_MOCA) {\r\nif (priv->hw_params->flags & GENET_HAS_MOCA_LINK_DET)\r\nint0_enable |= UMAC_IRQ_LINK_EVENT;\r\n}\r\nbcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);\r\n}\r\nstatic int init_umac(struct bcmgenet_priv *priv)\r\n{\r\nstruct device *kdev = &priv->pdev->dev;\r\nint ret;\r\nu32 reg;\r\nu32 int0_enable = 0;\r\nu32 int1_enable = 0;\r\nint i;\r\ndev_dbg(&priv->pdev->dev, "bcmgenet: init_umac\n");\r\nret = reset_umac(priv);\r\nif (ret)\r\nreturn ret;\r\nbcmgenet_umac_writel(priv, 0, UMAC_CMD);\r\nbcmgenet_umac_writel(priv,\r\nMIB_RESET_RX | MIB_RESET_TX | MIB_RESET_RUNT,\r\nUMAC_MIB_CTRL);\r\nbcmgenet_umac_writel(priv, 0, UMAC_MIB_CTRL);\r\nbcmgenet_umac_writel(priv, ENET_MAX_MTU_SIZE, UMAC_MAX_FRAME_LEN);\r\nreg = bcmgenet_rbuf_readl(priv, RBUF_CTRL);\r\nreg |= RBUF_ALIGN_2B;\r\nbcmgenet_rbuf_writel(priv, reg, RBUF_CTRL);\r\nif (!GENET_IS_V1(priv) && !GENET_IS_V2(priv))\r\nbcmgenet_rbuf_writel(priv, 1, RBUF_TBUF_SIZE_CTRL);\r\nbcmgenet_intr_disable(priv);\r\nint0_enable |= UMAC_IRQ_RXDMA_DONE;\r\nint0_enable |= UMAC_IRQ_TXDMA_DONE;\r\nif (priv->phy_interface == PHY_INTERFACE_MODE_MOCA) {\r\nreg = bcmgenet_bp_mc_get(priv);\r\nreg |= BIT(priv->hw_params->bp_in_en_shift);\r\nif (netif_is_multiqueue(priv->dev))\r\nreg |= priv->hw_params->bp_in_mask;\r\nelse\r\nreg &= ~priv->hw_params->bp_in_mask;\r\nbcmgenet_bp_mc_set(priv, reg);\r\n}\r\nif (priv->hw_params->flags & GENET_HAS_MDIO_INTR)\r\nint0_enable |= (UMAC_IRQ_MDIO_DONE | UMAC_IRQ_MDIO_ERROR);\r\nfor (i = 0; i < priv->hw_params->rx_queues; ++i)\r\nint1_enable |= (1 << (UMAC_IRQ1_RX_INTR_SHIFT + i));\r\nfor (i = 0; i < priv->hw_params->tx_queues; ++i)\r\nint1_enable |= (1 << i);\r\nbcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);\r\nbcmgenet_intrl2_1_writel(priv, int1_enable, INTRL2_CPU_MASK_CLEAR);\r\ndev_dbg(kdev, "done init umac\n");\r\nreturn 0;\r\n}\r\nstatic void bcmgenet_init_tx_ring(struct bcmgenet_priv *priv,\r\nunsigned int index, unsigned int size,\r\nunsigned int start_ptr, unsigned int end_ptr)\r\n{\r\nstruct bcmgenet_tx_ring *ring = &priv->tx_rings[index];\r\nu32 words_per_bd = WORDS_PER_BD(priv);\r\nu32 flow_period_val = 0;\r\nspin_lock_init(&ring->lock);\r\nring->priv = priv;\r\nring->index = index;\r\nif (index == DESC_INDEX) {\r\nring->queue = 0;\r\nring->int_enable = bcmgenet_tx_ring16_int_enable;\r\nring->int_disable = bcmgenet_tx_ring16_int_disable;\r\n} else {\r\nring->queue = index + 1;\r\nring->int_enable = bcmgenet_tx_ring_int_enable;\r\nring->int_disable = bcmgenet_tx_ring_int_disable;\r\n}\r\nring->cbs = priv->tx_cbs + start_ptr;\r\nring->size = size;\r\nring->clean_ptr = start_ptr;\r\nring->c_index = 0;\r\nring->free_bds = size;\r\nring->write_ptr = start_ptr;\r\nring->cb_ptr = start_ptr;\r\nring->end_ptr = end_ptr - 1;\r\nring->prod_index = 0;\r\nif (index != DESC_INDEX)\r\nflow_period_val = ENET_MAX_MTU_SIZE << 16;\r\nbcmgenet_tdma_ring_writel(priv, index, 0, TDMA_PROD_INDEX);\r\nbcmgenet_tdma_ring_writel(priv, index, 0, TDMA_CONS_INDEX);\r\nbcmgenet_tdma_ring_writel(priv, index, 1, DMA_MBUF_DONE_THRESH);\r\nbcmgenet_tdma_ring_writel(priv, index, flow_period_val,\r\nTDMA_FLOW_PERIOD);\r\nbcmgenet_tdma_ring_writel(priv, index,\r\n((size << DMA_RING_SIZE_SHIFT) |\r\nRX_BUF_LENGTH), DMA_RING_BUF_SIZE);\r\nbcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,\r\nDMA_START_ADDR);\r\nbcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,\r\nTDMA_READ_PTR);\r\nbcmgenet_tdma_ring_writel(priv, index, start_ptr * words_per_bd,\r\nTDMA_WRITE_PTR);\r\nbcmgenet_tdma_ring_writel(priv, index, end_ptr * words_per_bd - 1,\r\nDMA_END_ADDR);\r\n}\r\nstatic int bcmgenet_init_rx_ring(struct bcmgenet_priv *priv,\r\nunsigned int index, unsigned int size,\r\nunsigned int start_ptr, unsigned int end_ptr)\r\n{\r\nstruct bcmgenet_rx_ring *ring = &priv->rx_rings[index];\r\nu32 words_per_bd = WORDS_PER_BD(priv);\r\nint ret;\r\nring->priv = priv;\r\nring->index = index;\r\nif (index == DESC_INDEX) {\r\nring->int_enable = bcmgenet_rx_ring16_int_enable;\r\nring->int_disable = bcmgenet_rx_ring16_int_disable;\r\n} else {\r\nring->int_enable = bcmgenet_rx_ring_int_enable;\r\nring->int_disable = bcmgenet_rx_ring_int_disable;\r\n}\r\nring->cbs = priv->rx_cbs + start_ptr;\r\nring->size = size;\r\nring->c_index = 0;\r\nring->read_ptr = start_ptr;\r\nring->cb_ptr = start_ptr;\r\nring->end_ptr = end_ptr - 1;\r\nret = bcmgenet_alloc_rx_buffers(priv, ring);\r\nif (ret)\r\nreturn ret;\r\nbcmgenet_rdma_ring_writel(priv, index, 0, RDMA_PROD_INDEX);\r\nbcmgenet_rdma_ring_writel(priv, index, 0, RDMA_CONS_INDEX);\r\nbcmgenet_rdma_ring_writel(priv, index, 1, DMA_MBUF_DONE_THRESH);\r\nbcmgenet_rdma_ring_writel(priv, index,\r\n((size << DMA_RING_SIZE_SHIFT) |\r\nRX_BUF_LENGTH), DMA_RING_BUF_SIZE);\r\nbcmgenet_rdma_ring_writel(priv, index,\r\n(DMA_FC_THRESH_LO <<\r\nDMA_XOFF_THRESHOLD_SHIFT) |\r\nDMA_FC_THRESH_HI, RDMA_XON_XOFF_THRESH);\r\nbcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,\r\nDMA_START_ADDR);\r\nbcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,\r\nRDMA_READ_PTR);\r\nbcmgenet_rdma_ring_writel(priv, index, start_ptr * words_per_bd,\r\nRDMA_WRITE_PTR);\r\nbcmgenet_rdma_ring_writel(priv, index, end_ptr * words_per_bd - 1,\r\nDMA_END_ADDR);\r\nreturn ret;\r\n}\r\nstatic void bcmgenet_init_tx_napi(struct bcmgenet_priv *priv)\r\n{\r\nunsigned int i;\r\nstruct bcmgenet_tx_ring *ring;\r\nfor (i = 0; i < priv->hw_params->tx_queues; ++i) {\r\nring = &priv->tx_rings[i];\r\nnetif_tx_napi_add(priv->dev, &ring->napi, bcmgenet_tx_poll, 64);\r\n}\r\nring = &priv->tx_rings[DESC_INDEX];\r\nnetif_tx_napi_add(priv->dev, &ring->napi, bcmgenet_tx_poll, 64);\r\n}\r\nstatic void bcmgenet_enable_tx_napi(struct bcmgenet_priv *priv)\r\n{\r\nunsigned int i;\r\nstruct bcmgenet_tx_ring *ring;\r\nfor (i = 0; i < priv->hw_params->tx_queues; ++i) {\r\nring = &priv->tx_rings[i];\r\nnapi_enable(&ring->napi);\r\n}\r\nring = &priv->tx_rings[DESC_INDEX];\r\nnapi_enable(&ring->napi);\r\n}\r\nstatic void bcmgenet_disable_tx_napi(struct bcmgenet_priv *priv)\r\n{\r\nunsigned int i;\r\nstruct bcmgenet_tx_ring *ring;\r\nfor (i = 0; i < priv->hw_params->tx_queues; ++i) {\r\nring = &priv->tx_rings[i];\r\nnapi_disable(&ring->napi);\r\n}\r\nring = &priv->tx_rings[DESC_INDEX];\r\nnapi_disable(&ring->napi);\r\n}\r\nstatic void bcmgenet_fini_tx_napi(struct bcmgenet_priv *priv)\r\n{\r\nunsigned int i;\r\nstruct bcmgenet_tx_ring *ring;\r\nfor (i = 0; i < priv->hw_params->tx_queues; ++i) {\r\nring = &priv->tx_rings[i];\r\nnetif_napi_del(&ring->napi);\r\n}\r\nring = &priv->tx_rings[DESC_INDEX];\r\nnetif_napi_del(&ring->napi);\r\n}\r\nstatic void bcmgenet_init_tx_queues(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nu32 i, dma_enable;\r\nu32 dma_ctrl, ring_cfg;\r\nu32 dma_priority[3] = {0, 0, 0};\r\ndma_ctrl = bcmgenet_tdma_readl(priv, DMA_CTRL);\r\ndma_enable = dma_ctrl & DMA_EN;\r\ndma_ctrl &= ~DMA_EN;\r\nbcmgenet_tdma_writel(priv, dma_ctrl, DMA_CTRL);\r\ndma_ctrl = 0;\r\nring_cfg = 0;\r\nbcmgenet_tdma_writel(priv, DMA_ARBITER_SP, DMA_ARB_CTRL);\r\nfor (i = 0; i < priv->hw_params->tx_queues; i++) {\r\nbcmgenet_init_tx_ring(priv, i, priv->hw_params->tx_bds_per_q,\r\ni * priv->hw_params->tx_bds_per_q,\r\n(i + 1) * priv->hw_params->tx_bds_per_q);\r\nring_cfg |= (1 << i);\r\ndma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\r\ndma_priority[DMA_PRIO_REG_INDEX(i)] |=\r\n((GENET_Q0_PRIORITY + i) << DMA_PRIO_REG_SHIFT(i));\r\n}\r\nbcmgenet_init_tx_ring(priv, DESC_INDEX, GENET_Q16_TX_BD_CNT,\r\npriv->hw_params->tx_queues *\r\npriv->hw_params->tx_bds_per_q,\r\nTOTAL_DESC);\r\nring_cfg |= (1 << DESC_INDEX);\r\ndma_ctrl |= (1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT));\r\ndma_priority[DMA_PRIO_REG_INDEX(DESC_INDEX)] |=\r\n((GENET_Q0_PRIORITY + priv->hw_params->tx_queues) <<\r\nDMA_PRIO_REG_SHIFT(DESC_INDEX));\r\nbcmgenet_tdma_writel(priv, dma_priority[0], DMA_PRIORITY_0);\r\nbcmgenet_tdma_writel(priv, dma_priority[1], DMA_PRIORITY_1);\r\nbcmgenet_tdma_writel(priv, dma_priority[2], DMA_PRIORITY_2);\r\nbcmgenet_init_tx_napi(priv);\r\nbcmgenet_tdma_writel(priv, ring_cfg, DMA_RING_CFG);\r\nif (dma_enable)\r\ndma_ctrl |= DMA_EN;\r\nbcmgenet_tdma_writel(priv, dma_ctrl, DMA_CTRL);\r\n}\r\nstatic void bcmgenet_init_rx_napi(struct bcmgenet_priv *priv)\r\n{\r\nunsigned int i;\r\nstruct bcmgenet_rx_ring *ring;\r\nfor (i = 0; i < priv->hw_params->rx_queues; ++i) {\r\nring = &priv->rx_rings[i];\r\nnetif_napi_add(priv->dev, &ring->napi, bcmgenet_rx_poll, 64);\r\n}\r\nring = &priv->rx_rings[DESC_INDEX];\r\nnetif_napi_add(priv->dev, &ring->napi, bcmgenet_rx_poll, 64);\r\n}\r\nstatic void bcmgenet_enable_rx_napi(struct bcmgenet_priv *priv)\r\n{\r\nunsigned int i;\r\nstruct bcmgenet_rx_ring *ring;\r\nfor (i = 0; i < priv->hw_params->rx_queues; ++i) {\r\nring = &priv->rx_rings[i];\r\nnapi_enable(&ring->napi);\r\n}\r\nring = &priv->rx_rings[DESC_INDEX];\r\nnapi_enable(&ring->napi);\r\n}\r\nstatic void bcmgenet_disable_rx_napi(struct bcmgenet_priv *priv)\r\n{\r\nunsigned int i;\r\nstruct bcmgenet_rx_ring *ring;\r\nfor (i = 0; i < priv->hw_params->rx_queues; ++i) {\r\nring = &priv->rx_rings[i];\r\nnapi_disable(&ring->napi);\r\n}\r\nring = &priv->rx_rings[DESC_INDEX];\r\nnapi_disable(&ring->napi);\r\n}\r\nstatic void bcmgenet_fini_rx_napi(struct bcmgenet_priv *priv)\r\n{\r\nunsigned int i;\r\nstruct bcmgenet_rx_ring *ring;\r\nfor (i = 0; i < priv->hw_params->rx_queues; ++i) {\r\nring = &priv->rx_rings[i];\r\nnetif_napi_del(&ring->napi);\r\n}\r\nring = &priv->rx_rings[DESC_INDEX];\r\nnetif_napi_del(&ring->napi);\r\n}\r\nstatic int bcmgenet_init_rx_queues(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nu32 i;\r\nu32 dma_enable;\r\nu32 dma_ctrl;\r\nu32 ring_cfg;\r\nint ret;\r\ndma_ctrl = bcmgenet_rdma_readl(priv, DMA_CTRL);\r\ndma_enable = dma_ctrl & DMA_EN;\r\ndma_ctrl &= ~DMA_EN;\r\nbcmgenet_rdma_writel(priv, dma_ctrl, DMA_CTRL);\r\ndma_ctrl = 0;\r\nring_cfg = 0;\r\nfor (i = 0; i < priv->hw_params->rx_queues; i++) {\r\nret = bcmgenet_init_rx_ring(priv, i,\r\npriv->hw_params->rx_bds_per_q,\r\ni * priv->hw_params->rx_bds_per_q,\r\n(i + 1) *\r\npriv->hw_params->rx_bds_per_q);\r\nif (ret)\r\nreturn ret;\r\nring_cfg |= (1 << i);\r\ndma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\r\n}\r\nret = bcmgenet_init_rx_ring(priv, DESC_INDEX, GENET_Q16_RX_BD_CNT,\r\npriv->hw_params->rx_queues *\r\npriv->hw_params->rx_bds_per_q,\r\nTOTAL_DESC);\r\nif (ret)\r\nreturn ret;\r\nring_cfg |= (1 << DESC_INDEX);\r\ndma_ctrl |= (1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT));\r\nbcmgenet_init_rx_napi(priv);\r\nbcmgenet_rdma_writel(priv, ring_cfg, DMA_RING_CFG);\r\nif (dma_enable)\r\ndma_ctrl |= DMA_EN;\r\nbcmgenet_rdma_writel(priv, dma_ctrl, DMA_CTRL);\r\nreturn 0;\r\n}\r\nstatic int bcmgenet_dma_teardown(struct bcmgenet_priv *priv)\r\n{\r\nint ret = 0;\r\nint timeout = 0;\r\nu32 reg;\r\nu32 dma_ctrl;\r\nint i;\r\nreg = bcmgenet_tdma_readl(priv, DMA_CTRL);\r\nreg &= ~DMA_EN;\r\nbcmgenet_tdma_writel(priv, reg, DMA_CTRL);\r\nwhile (timeout++ < DMA_TIMEOUT_VAL) {\r\nreg = bcmgenet_tdma_readl(priv, DMA_STATUS);\r\nif (reg & DMA_DISABLED)\r\nbreak;\r\nudelay(1);\r\n}\r\nif (timeout == DMA_TIMEOUT_VAL) {\r\nnetdev_warn(priv->dev, "Timed out while disabling TX DMA\n");\r\nret = -ETIMEDOUT;\r\n}\r\nusleep_range(10000, 20000);\r\nreg = bcmgenet_rdma_readl(priv, DMA_CTRL);\r\nreg &= ~DMA_EN;\r\nbcmgenet_rdma_writel(priv, reg, DMA_CTRL);\r\ntimeout = 0;\r\nwhile (timeout++ < DMA_TIMEOUT_VAL) {\r\nreg = bcmgenet_rdma_readl(priv, DMA_STATUS);\r\nif (reg & DMA_DISABLED)\r\nbreak;\r\nudelay(1);\r\n}\r\nif (timeout == DMA_TIMEOUT_VAL) {\r\nnetdev_warn(priv->dev, "Timed out while disabling RX DMA\n");\r\nret = -ETIMEDOUT;\r\n}\r\ndma_ctrl = 0;\r\nfor (i = 0; i < priv->hw_params->rx_queues; i++)\r\ndma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\r\nreg = bcmgenet_rdma_readl(priv, DMA_CTRL);\r\nreg &= ~dma_ctrl;\r\nbcmgenet_rdma_writel(priv, reg, DMA_CTRL);\r\ndma_ctrl = 0;\r\nfor (i = 0; i < priv->hw_params->tx_queues; i++)\r\ndma_ctrl |= (1 << (i + DMA_RING_BUF_EN_SHIFT));\r\nreg = bcmgenet_tdma_readl(priv, DMA_CTRL);\r\nreg &= ~dma_ctrl;\r\nbcmgenet_tdma_writel(priv, reg, DMA_CTRL);\r\nreturn ret;\r\n}\r\nstatic void bcmgenet_fini_dma(struct bcmgenet_priv *priv)\r\n{\r\nint i;\r\nstruct netdev_queue *txq;\r\nbcmgenet_fini_rx_napi(priv);\r\nbcmgenet_fini_tx_napi(priv);\r\nbcmgenet_dma_teardown(priv);\r\nfor (i = 0; i < priv->num_tx_bds; i++) {\r\nif (priv->tx_cbs[i].skb != NULL) {\r\ndev_kfree_skb(priv->tx_cbs[i].skb);\r\npriv->tx_cbs[i].skb = NULL;\r\n}\r\n}\r\nfor (i = 0; i < priv->hw_params->tx_queues; i++) {\r\ntxq = netdev_get_tx_queue(priv->dev, priv->tx_rings[i].queue);\r\nnetdev_tx_reset_queue(txq);\r\n}\r\ntxq = netdev_get_tx_queue(priv->dev, priv->tx_rings[DESC_INDEX].queue);\r\nnetdev_tx_reset_queue(txq);\r\nbcmgenet_free_rx_buffers(priv);\r\nkfree(priv->rx_cbs);\r\nkfree(priv->tx_cbs);\r\n}\r\nstatic int bcmgenet_init_dma(struct bcmgenet_priv *priv)\r\n{\r\nint ret;\r\nunsigned int i;\r\nstruct enet_cb *cb;\r\nnetif_dbg(priv, hw, priv->dev, "%s\n", __func__);\r\npriv->rx_bds = priv->base + priv->hw_params->rdma_offset;\r\npriv->num_rx_bds = TOTAL_DESC;\r\npriv->rx_cbs = kcalloc(priv->num_rx_bds, sizeof(struct enet_cb),\r\nGFP_KERNEL);\r\nif (!priv->rx_cbs)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < priv->num_rx_bds; i++) {\r\ncb = priv->rx_cbs + i;\r\ncb->bd_addr = priv->rx_bds + i * DMA_DESC_SIZE;\r\n}\r\npriv->tx_bds = priv->base + priv->hw_params->tdma_offset;\r\npriv->num_tx_bds = TOTAL_DESC;\r\npriv->tx_cbs = kcalloc(priv->num_tx_bds, sizeof(struct enet_cb),\r\nGFP_KERNEL);\r\nif (!priv->tx_cbs) {\r\nkfree(priv->rx_cbs);\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < priv->num_tx_bds; i++) {\r\ncb = priv->tx_cbs + i;\r\ncb->bd_addr = priv->tx_bds + i * DMA_DESC_SIZE;\r\n}\r\nbcmgenet_rdma_writel(priv, DMA_MAX_BURST_LENGTH, DMA_SCB_BURST_SIZE);\r\nret = bcmgenet_init_rx_queues(priv->dev);\r\nif (ret) {\r\nnetdev_err(priv->dev, "failed to initialize Rx queues\n");\r\nbcmgenet_free_rx_buffers(priv);\r\nkfree(priv->rx_cbs);\r\nkfree(priv->tx_cbs);\r\nreturn ret;\r\n}\r\nbcmgenet_tdma_writel(priv, DMA_MAX_BURST_LENGTH, DMA_SCB_BURST_SIZE);\r\nbcmgenet_init_tx_queues(priv->dev);\r\nreturn 0;\r\n}\r\nstatic void bcmgenet_irq_task(struct work_struct *work)\r\n{\r\nstruct bcmgenet_priv *priv = container_of(\r\nwork, struct bcmgenet_priv, bcmgenet_irq_work);\r\nnetif_dbg(priv, intr, priv->dev, "%s\n", __func__);\r\nif (priv->irq0_stat & UMAC_IRQ_MPD_R) {\r\npriv->irq0_stat &= ~UMAC_IRQ_MPD_R;\r\nnetif_dbg(priv, wol, priv->dev,\r\n"magic packet detected, waking up\n");\r\nbcmgenet_power_up(priv, GENET_POWER_WOL_MAGIC);\r\n}\r\nif (priv->irq0_stat & UMAC_IRQ_LINK_EVENT) {\r\nphy_mac_interrupt(priv->phydev,\r\n!!(priv->irq0_stat & UMAC_IRQ_LINK_UP));\r\npriv->irq0_stat &= ~UMAC_IRQ_LINK_EVENT;\r\n}\r\n}\r\nstatic irqreturn_t bcmgenet_isr1(int irq, void *dev_id)\r\n{\r\nstruct bcmgenet_priv *priv = dev_id;\r\nstruct bcmgenet_rx_ring *rx_ring;\r\nstruct bcmgenet_tx_ring *tx_ring;\r\nunsigned int index;\r\npriv->irq1_stat =\r\nbcmgenet_intrl2_1_readl(priv, INTRL2_CPU_STAT) &\r\n~bcmgenet_intrl2_1_readl(priv, INTRL2_CPU_MASK_STATUS);\r\nbcmgenet_intrl2_1_writel(priv, priv->irq1_stat, INTRL2_CPU_CLEAR);\r\nnetif_dbg(priv, intr, priv->dev,\r\n"%s: IRQ=0x%x\n", __func__, priv->irq1_stat);\r\nfor (index = 0; index < priv->hw_params->rx_queues; index++) {\r\nif (!(priv->irq1_stat & BIT(UMAC_IRQ1_RX_INTR_SHIFT + index)))\r\ncontinue;\r\nrx_ring = &priv->rx_rings[index];\r\nif (likely(napi_schedule_prep(&rx_ring->napi))) {\r\nrx_ring->int_disable(rx_ring);\r\n__napi_schedule_irqoff(&rx_ring->napi);\r\n}\r\n}\r\nfor (index = 0; index < priv->hw_params->tx_queues; index++) {\r\nif (!(priv->irq1_stat & BIT(index)))\r\ncontinue;\r\ntx_ring = &priv->tx_rings[index];\r\nif (likely(napi_schedule_prep(&tx_ring->napi))) {\r\ntx_ring->int_disable(tx_ring);\r\n__napi_schedule_irqoff(&tx_ring->napi);\r\n}\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t bcmgenet_isr0(int irq, void *dev_id)\r\n{\r\nstruct bcmgenet_priv *priv = dev_id;\r\nstruct bcmgenet_rx_ring *rx_ring;\r\nstruct bcmgenet_tx_ring *tx_ring;\r\npriv->irq0_stat =\r\nbcmgenet_intrl2_0_readl(priv, INTRL2_CPU_STAT) &\r\n~bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_MASK_STATUS);\r\nbcmgenet_intrl2_0_writel(priv, priv->irq0_stat, INTRL2_CPU_CLEAR);\r\nnetif_dbg(priv, intr, priv->dev,\r\n"IRQ=0x%x\n", priv->irq0_stat);\r\nif (priv->irq0_stat & UMAC_IRQ_RXDMA_DONE) {\r\nrx_ring = &priv->rx_rings[DESC_INDEX];\r\nif (likely(napi_schedule_prep(&rx_ring->napi))) {\r\nrx_ring->int_disable(rx_ring);\r\n__napi_schedule_irqoff(&rx_ring->napi);\r\n}\r\n}\r\nif (priv->irq0_stat & UMAC_IRQ_TXDMA_DONE) {\r\ntx_ring = &priv->tx_rings[DESC_INDEX];\r\nif (likely(napi_schedule_prep(&tx_ring->napi))) {\r\ntx_ring->int_disable(tx_ring);\r\n__napi_schedule_irqoff(&tx_ring->napi);\r\n}\r\n}\r\nif (priv->irq0_stat & (UMAC_IRQ_PHY_DET_R |\r\nUMAC_IRQ_PHY_DET_F |\r\nUMAC_IRQ_LINK_EVENT |\r\nUMAC_IRQ_HFB_SM |\r\nUMAC_IRQ_HFB_MM |\r\nUMAC_IRQ_MPD_R)) {\r\nschedule_work(&priv->bcmgenet_irq_work);\r\n}\r\nif ((priv->hw_params->flags & GENET_HAS_MDIO_INTR) &&\r\npriv->irq0_stat & (UMAC_IRQ_MDIO_DONE | UMAC_IRQ_MDIO_ERROR)) {\r\npriv->irq0_stat &= ~(UMAC_IRQ_MDIO_DONE | UMAC_IRQ_MDIO_ERROR);\r\nwake_up(&priv->wq);\r\n}\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t bcmgenet_wol_isr(int irq, void *dev_id)\r\n{\r\nstruct bcmgenet_priv *priv = dev_id;\r\npm_wakeup_event(&priv->pdev->dev, 0);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void bcmgenet_poll_controller(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\ndisable_irq(priv->irq0);\r\nbcmgenet_isr0(priv->irq0, priv);\r\nenable_irq(priv->irq0);\r\ndisable_irq(priv->irq1);\r\nbcmgenet_isr1(priv->irq1, priv);\r\nenable_irq(priv->irq1);\r\n}\r\nstatic void bcmgenet_umac_reset(struct bcmgenet_priv *priv)\r\n{\r\nu32 reg;\r\nreg = bcmgenet_rbuf_ctrl_get(priv);\r\nreg |= BIT(1);\r\nbcmgenet_rbuf_ctrl_set(priv, reg);\r\nudelay(10);\r\nreg &= ~BIT(1);\r\nbcmgenet_rbuf_ctrl_set(priv, reg);\r\nudelay(10);\r\n}\r\nstatic void bcmgenet_set_hw_addr(struct bcmgenet_priv *priv,\r\nunsigned char *addr)\r\n{\r\nbcmgenet_umac_writel(priv, (addr[0] << 24) | (addr[1] << 16) |\r\n(addr[2] << 8) | addr[3], UMAC_MAC0);\r\nbcmgenet_umac_writel(priv, (addr[4] << 8) | addr[5], UMAC_MAC1);\r\n}\r\nstatic u32 bcmgenet_dma_disable(struct bcmgenet_priv *priv)\r\n{\r\nu32 reg;\r\nu32 dma_ctrl;\r\ndma_ctrl = 1 << (DESC_INDEX + DMA_RING_BUF_EN_SHIFT) | DMA_EN;\r\nreg = bcmgenet_tdma_readl(priv, DMA_CTRL);\r\nreg &= ~dma_ctrl;\r\nbcmgenet_tdma_writel(priv, reg, DMA_CTRL);\r\nreg = bcmgenet_rdma_readl(priv, DMA_CTRL);\r\nreg &= ~dma_ctrl;\r\nbcmgenet_rdma_writel(priv, reg, DMA_CTRL);\r\nbcmgenet_umac_writel(priv, 1, UMAC_TX_FLUSH);\r\nudelay(10);\r\nbcmgenet_umac_writel(priv, 0, UMAC_TX_FLUSH);\r\nreturn dma_ctrl;\r\n}\r\nstatic void bcmgenet_enable_dma(struct bcmgenet_priv *priv, u32 dma_ctrl)\r\n{\r\nu32 reg;\r\nreg = bcmgenet_rdma_readl(priv, DMA_CTRL);\r\nreg |= dma_ctrl;\r\nbcmgenet_rdma_writel(priv, reg, DMA_CTRL);\r\nreg = bcmgenet_tdma_readl(priv, DMA_CTRL);\r\nreg |= dma_ctrl;\r\nbcmgenet_tdma_writel(priv, reg, DMA_CTRL);\r\n}\r\nstatic bool bcmgenet_hfb_is_filter_enabled(struct bcmgenet_priv *priv,\r\nu32 f_index)\r\n{\r\nu32 offset;\r\nu32 reg;\r\noffset = HFB_FLT_ENABLE_V3PLUS + (f_index < 32) * sizeof(u32);\r\nreg = bcmgenet_hfb_reg_readl(priv, offset);\r\nreturn !!(reg & (1 << (f_index % 32)));\r\n}\r\nstatic void bcmgenet_hfb_enable_filter(struct bcmgenet_priv *priv, u32 f_index)\r\n{\r\nu32 offset;\r\nu32 reg;\r\noffset = HFB_FLT_ENABLE_V3PLUS + (f_index < 32) * sizeof(u32);\r\nreg = bcmgenet_hfb_reg_readl(priv, offset);\r\nreg |= (1 << (f_index % 32));\r\nbcmgenet_hfb_reg_writel(priv, reg, offset);\r\n}\r\nstatic void bcmgenet_hfb_set_filter_rx_queue_mapping(struct bcmgenet_priv *priv,\r\nu32 f_index, u32 rx_queue)\r\n{\r\nu32 offset;\r\nu32 reg;\r\noffset = f_index / 8;\r\nreg = bcmgenet_rdma_readl(priv, DMA_INDEX2RING_0 + offset);\r\nreg &= ~(0xF << (4 * (f_index % 8)));\r\nreg |= ((rx_queue & 0xF) << (4 * (f_index % 8)));\r\nbcmgenet_rdma_writel(priv, reg, DMA_INDEX2RING_0 + offset);\r\n}\r\nstatic void bcmgenet_hfb_set_filter_length(struct bcmgenet_priv *priv,\r\nu32 f_index, u32 f_length)\r\n{\r\nu32 offset;\r\nu32 reg;\r\noffset = HFB_FLT_LEN_V3PLUS +\r\n((priv->hw_params->hfb_filter_cnt - 1 - f_index) / 4) *\r\nsizeof(u32);\r\nreg = bcmgenet_hfb_reg_readl(priv, offset);\r\nreg &= ~(0xFF << (8 * (f_index % 4)));\r\nreg |= ((f_length & 0xFF) << (8 * (f_index % 4)));\r\nbcmgenet_hfb_reg_writel(priv, reg, offset);\r\n}\r\nstatic int bcmgenet_hfb_find_unused_filter(struct bcmgenet_priv *priv)\r\n{\r\nu32 f_index;\r\nfor (f_index = 0; f_index < priv->hw_params->hfb_filter_cnt; f_index++)\r\nif (!bcmgenet_hfb_is_filter_enabled(priv, f_index))\r\nreturn f_index;\r\nreturn -ENOMEM;\r\n}\r\nint bcmgenet_hfb_add_filter(struct bcmgenet_priv *priv, u32 *f_data,\r\nu32 f_length, u32 rx_queue)\r\n{\r\nint f_index;\r\nu32 i;\r\nf_index = bcmgenet_hfb_find_unused_filter(priv);\r\nif (f_index < 0)\r\nreturn -ENOMEM;\r\nif (f_length > priv->hw_params->hfb_filter_size)\r\nreturn -EINVAL;\r\nfor (i = 0; i < f_length; i++)\r\nbcmgenet_hfb_writel(priv, f_data[i],\r\n(f_index * priv->hw_params->hfb_filter_size + i) *\r\nsizeof(u32));\r\nbcmgenet_hfb_set_filter_length(priv, f_index, 2 * f_length);\r\nbcmgenet_hfb_set_filter_rx_queue_mapping(priv, f_index, rx_queue);\r\nbcmgenet_hfb_enable_filter(priv, f_index);\r\nbcmgenet_hfb_reg_writel(priv, 0x1, HFB_CTRL);\r\nreturn 0;\r\n}\r\nstatic void bcmgenet_hfb_clear(struct bcmgenet_priv *priv)\r\n{\r\nu32 i;\r\nbcmgenet_hfb_reg_writel(priv, 0x0, HFB_CTRL);\r\nbcmgenet_hfb_reg_writel(priv, 0x0, HFB_FLT_ENABLE_V3PLUS);\r\nbcmgenet_hfb_reg_writel(priv, 0x0, HFB_FLT_ENABLE_V3PLUS + 4);\r\nfor (i = DMA_INDEX2RING_0; i <= DMA_INDEX2RING_7; i++)\r\nbcmgenet_rdma_writel(priv, 0x0, i);\r\nfor (i = 0; i < (priv->hw_params->hfb_filter_cnt / 4); i++)\r\nbcmgenet_hfb_reg_writel(priv, 0x0,\r\nHFB_FLT_LEN_V3PLUS + i * sizeof(u32));\r\nfor (i = 0; i < priv->hw_params->hfb_filter_cnt *\r\npriv->hw_params->hfb_filter_size; i++)\r\nbcmgenet_hfb_writel(priv, 0x0, i * sizeof(u32));\r\n}\r\nstatic void bcmgenet_hfb_init(struct bcmgenet_priv *priv)\r\n{\r\nif (GENET_IS_V1(priv) || GENET_IS_V2(priv))\r\nreturn;\r\nbcmgenet_hfb_clear(priv);\r\n}\r\nstatic void bcmgenet_netif_start(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nbcmgenet_enable_rx_napi(priv);\r\nbcmgenet_enable_tx_napi(priv);\r\numac_enable_set(priv, CMD_TX_EN | CMD_RX_EN, true);\r\nnetif_tx_start_all_queues(dev);\r\nbcmgenet_link_intr_enable(priv);\r\nphy_start(priv->phydev);\r\n}\r\nstatic int bcmgenet_open(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nunsigned long dma_ctrl;\r\nu32 reg;\r\nint ret;\r\nnetif_dbg(priv, ifup, dev, "bcmgenet_open\n");\r\nclk_prepare_enable(priv->clk);\r\nif (priv->internal_phy)\r\nbcmgenet_power_up(priv, GENET_POWER_PASSIVE);\r\nbcmgenet_umac_reset(priv);\r\nret = init_umac(priv);\r\nif (ret)\r\ngoto err_clk_disable;\r\numac_enable_set(priv, CMD_TX_EN | CMD_RX_EN, false);\r\nreg = bcmgenet_umac_readl(priv, UMAC_CMD);\r\npriv->crc_fwd_en = !!(reg & CMD_CRC_FWD);\r\nbcmgenet_set_hw_addr(priv, dev->dev_addr);\r\nif (priv->internal_phy) {\r\nreg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);\r\nreg |= EXT_ENERGY_DET_MASK;\r\nbcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);\r\n}\r\ndma_ctrl = bcmgenet_dma_disable(priv);\r\nret = bcmgenet_init_dma(priv);\r\nif (ret) {\r\nnetdev_err(dev, "failed to initialize DMA\n");\r\ngoto err_clk_disable;\r\n}\r\nbcmgenet_enable_dma(priv, dma_ctrl);\r\nbcmgenet_hfb_init(priv);\r\nret = request_irq(priv->irq0, bcmgenet_isr0, IRQF_SHARED,\r\ndev->name, priv);\r\nif (ret < 0) {\r\nnetdev_err(dev, "can't request IRQ %d\n", priv->irq0);\r\ngoto err_fini_dma;\r\n}\r\nret = request_irq(priv->irq1, bcmgenet_isr1, IRQF_SHARED,\r\ndev->name, priv);\r\nif (ret < 0) {\r\nnetdev_err(dev, "can't request IRQ %d\n", priv->irq1);\r\ngoto err_irq0;\r\n}\r\nret = bcmgenet_mii_probe(dev);\r\nif (ret) {\r\nnetdev_err(dev, "failed to connect to PHY\n");\r\ngoto err_irq1;\r\n}\r\nbcmgenet_netif_start(dev);\r\nreturn 0;\r\nerr_irq1:\r\nfree_irq(priv->irq1, priv);\r\nerr_irq0:\r\nfree_irq(priv->irq0, priv);\r\nerr_fini_dma:\r\nbcmgenet_fini_dma(priv);\r\nerr_clk_disable:\r\nclk_disable_unprepare(priv->clk);\r\nreturn ret;\r\n}\r\nstatic void bcmgenet_netif_stop(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nnetif_tx_stop_all_queues(dev);\r\nphy_stop(priv->phydev);\r\nbcmgenet_intr_disable(priv);\r\nbcmgenet_disable_rx_napi(priv);\r\nbcmgenet_disable_tx_napi(priv);\r\ncancel_work_sync(&priv->bcmgenet_irq_work);\r\npriv->old_link = -1;\r\npriv->old_speed = -1;\r\npriv->old_duplex = -1;\r\npriv->old_pause = -1;\r\n}\r\nstatic int bcmgenet_close(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nint ret;\r\nnetif_dbg(priv, ifdown, dev, "bcmgenet_close\n");\r\nbcmgenet_netif_stop(dev);\r\nphy_disconnect(priv->phydev);\r\numac_enable_set(priv, CMD_RX_EN, false);\r\nret = bcmgenet_dma_teardown(priv);\r\nif (ret)\r\nreturn ret;\r\numac_enable_set(priv, CMD_TX_EN, false);\r\nbcmgenet_tx_reclaim_all(dev);\r\nbcmgenet_fini_dma(priv);\r\nfree_irq(priv->irq0, priv);\r\nfree_irq(priv->irq1, priv);\r\nif (priv->internal_phy)\r\nret = bcmgenet_power_down(priv, GENET_POWER_PASSIVE);\r\nclk_disable_unprepare(priv->clk);\r\nreturn ret;\r\n}\r\nstatic void bcmgenet_dump_tx_queue(struct bcmgenet_tx_ring *ring)\r\n{\r\nstruct bcmgenet_priv *priv = ring->priv;\r\nu32 p_index, c_index, intsts, intmsk;\r\nstruct netdev_queue *txq;\r\nunsigned int free_bds;\r\nunsigned long flags;\r\nbool txq_stopped;\r\nif (!netif_msg_tx_err(priv))\r\nreturn;\r\ntxq = netdev_get_tx_queue(priv->dev, ring->queue);\r\nspin_lock_irqsave(&ring->lock, flags);\r\nif (ring->index == DESC_INDEX) {\r\nintsts = ~bcmgenet_intrl2_0_readl(priv, INTRL2_CPU_MASK_STATUS);\r\nintmsk = UMAC_IRQ_TXDMA_DONE | UMAC_IRQ_TXDMA_MBDONE;\r\n} else {\r\nintsts = ~bcmgenet_intrl2_1_readl(priv, INTRL2_CPU_MASK_STATUS);\r\nintmsk = 1 << ring->index;\r\n}\r\nc_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_CONS_INDEX);\r\np_index = bcmgenet_tdma_ring_readl(priv, ring->index, TDMA_PROD_INDEX);\r\ntxq_stopped = netif_tx_queue_stopped(txq);\r\nfree_bds = ring->free_bds;\r\nspin_unlock_irqrestore(&ring->lock, flags);\r\nnetif_err(priv, tx_err, priv->dev, "Ring %d queue %d status summary\n"\r\n"TX queue status: %s, interrupts: %s\n"\r\n"(sw)free_bds: %d (sw)size: %d\n"\r\n"(sw)p_index: %d (hw)p_index: %d\n"\r\n"(sw)c_index: %d (hw)c_index: %d\n"\r\n"(sw)clean_p: %d (sw)write_p: %d\n"\r\n"(sw)cb_ptr: %d (sw)end_ptr: %d\n",\r\nring->index, ring->queue,\r\ntxq_stopped ? "stopped" : "active",\r\nintsts & intmsk ? "enabled" : "disabled",\r\nfree_bds, ring->size,\r\nring->prod_index, p_index & DMA_P_INDEX_MASK,\r\nring->c_index, c_index & DMA_C_INDEX_MASK,\r\nring->clean_ptr, ring->write_ptr,\r\nring->cb_ptr, ring->end_ptr);\r\n}\r\nstatic void bcmgenet_timeout(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nu32 int0_enable = 0;\r\nu32 int1_enable = 0;\r\nunsigned int q;\r\nnetif_dbg(priv, tx_err, dev, "bcmgenet_timeout\n");\r\nfor (q = 0; q < priv->hw_params->tx_queues; q++)\r\nbcmgenet_dump_tx_queue(&priv->tx_rings[q]);\r\nbcmgenet_dump_tx_queue(&priv->tx_rings[DESC_INDEX]);\r\nbcmgenet_tx_reclaim_all(dev);\r\nfor (q = 0; q < priv->hw_params->tx_queues; q++)\r\nint1_enable |= (1 << q);\r\nint0_enable = UMAC_IRQ_TXDMA_DONE;\r\nbcmgenet_intrl2_0_writel(priv, int0_enable, INTRL2_CPU_MASK_CLEAR);\r\nbcmgenet_intrl2_1_writel(priv, int1_enable, INTRL2_CPU_MASK_CLEAR);\r\nnetif_trans_update(dev);\r\ndev->stats.tx_errors++;\r\nnetif_tx_wake_all_queues(dev);\r\n}\r\nstatic inline void bcmgenet_set_mdf_addr(struct bcmgenet_priv *priv,\r\nunsigned char *addr,\r\nint *i,\r\nint *mc)\r\n{\r\nu32 reg;\r\nbcmgenet_umac_writel(priv, addr[0] << 8 | addr[1],\r\nUMAC_MDF_ADDR + (*i * 4));\r\nbcmgenet_umac_writel(priv, addr[2] << 24 | addr[3] << 16 |\r\naddr[4] << 8 | addr[5],\r\nUMAC_MDF_ADDR + ((*i + 1) * 4));\r\nreg = bcmgenet_umac_readl(priv, UMAC_MDF_CTRL);\r\nreg |= (1 << (MAX_MC_COUNT - *mc));\r\nbcmgenet_umac_writel(priv, reg, UMAC_MDF_CTRL);\r\n*i += 2;\r\n(*mc)++;\r\n}\r\nstatic void bcmgenet_set_rx_mode(struct net_device *dev)\r\n{\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nstruct netdev_hw_addr *ha;\r\nint i, mc;\r\nu32 reg;\r\nnetif_dbg(priv, hw, dev, "%s: %08X\n", __func__, dev->flags);\r\nreg = bcmgenet_umac_readl(priv, UMAC_CMD);\r\nif (dev->flags & IFF_PROMISC) {\r\nreg |= CMD_PROMISC;\r\nbcmgenet_umac_writel(priv, reg, UMAC_CMD);\r\nbcmgenet_umac_writel(priv, 0, UMAC_MDF_CTRL);\r\nreturn;\r\n} else {\r\nreg &= ~CMD_PROMISC;\r\nbcmgenet_umac_writel(priv, reg, UMAC_CMD);\r\n}\r\nif (dev->flags & IFF_ALLMULTI) {\r\nnetdev_warn(dev, "ALLMULTI is not supported\n");\r\nreturn;\r\n}\r\ni = 0;\r\nmc = 0;\r\nbcmgenet_set_mdf_addr(priv, dev->broadcast, &i, &mc);\r\nbcmgenet_set_mdf_addr(priv, dev->dev_addr, &i, &mc);\r\nif (netdev_uc_count(dev) > (MAX_MC_COUNT - mc))\r\nreturn;\r\nif (!netdev_uc_empty(dev))\r\nnetdev_for_each_uc_addr(ha, dev)\r\nbcmgenet_set_mdf_addr(priv, ha->addr, &i, &mc);\r\nif (netdev_mc_empty(dev) || netdev_mc_count(dev) >= (MAX_MC_COUNT - mc))\r\nreturn;\r\nnetdev_for_each_mc_addr(ha, dev)\r\nbcmgenet_set_mdf_addr(priv, ha->addr, &i, &mc);\r\n}\r\nstatic int bcmgenet_set_mac_addr(struct net_device *dev, void *p)\r\n{\r\nstruct sockaddr *addr = p;\r\nif (netif_running(dev))\r\nreturn -EBUSY;\r\nether_addr_copy(dev->dev_addr, addr->sa_data);\r\nreturn 0;\r\n}\r\nstatic void bcmgenet_set_hw_params(struct bcmgenet_priv *priv)\r\n{\r\nstruct bcmgenet_hw_params *params;\r\nu32 reg;\r\nu8 major;\r\nu16 gphy_rev;\r\nif (GENET_IS_V4(priv)) {\r\nbcmgenet_dma_regs = bcmgenet_dma_regs_v3plus;\r\ngenet_dma_ring_regs = genet_dma_ring_regs_v4;\r\npriv->dma_rx_chk_bit = DMA_RX_CHK_V3PLUS;\r\npriv->version = GENET_V4;\r\n} else if (GENET_IS_V3(priv)) {\r\nbcmgenet_dma_regs = bcmgenet_dma_regs_v3plus;\r\ngenet_dma_ring_regs = genet_dma_ring_regs_v123;\r\npriv->dma_rx_chk_bit = DMA_RX_CHK_V3PLUS;\r\npriv->version = GENET_V3;\r\n} else if (GENET_IS_V2(priv)) {\r\nbcmgenet_dma_regs = bcmgenet_dma_regs_v2;\r\ngenet_dma_ring_regs = genet_dma_ring_regs_v123;\r\npriv->dma_rx_chk_bit = DMA_RX_CHK_V12;\r\npriv->version = GENET_V2;\r\n} else if (GENET_IS_V1(priv)) {\r\nbcmgenet_dma_regs = bcmgenet_dma_regs_v1;\r\ngenet_dma_ring_regs = genet_dma_ring_regs_v123;\r\npriv->dma_rx_chk_bit = DMA_RX_CHK_V12;\r\npriv->version = GENET_V1;\r\n}\r\npriv->hw_params = &bcmgenet_hw_params[priv->version];\r\nparams = priv->hw_params;\r\nreg = bcmgenet_sys_readl(priv, SYS_REV_CTRL);\r\nmajor = (reg >> 24 & 0x0f);\r\nif (major == 5)\r\nmajor = 4;\r\nelse if (major == 0)\r\nmajor = 1;\r\nif (major != priv->version) {\r\ndev_err(&priv->pdev->dev,\r\n"GENET version mismatch, got: %d, configured for: %d\n",\r\nmajor, priv->version);\r\n}\r\ndev_info(&priv->pdev->dev, "GENET " GENET_VER_FMT,\r\nmajor, (reg >> 16) & 0x0f, reg & 0xffff);\r\ngphy_rev = reg & 0xffff;\r\nif ((gphy_rev & 0xf0) != 0)\r\npriv->gphy_rev = gphy_rev << 8;\r\nelse if ((gphy_rev & 0xff00) != 0)\r\npriv->gphy_rev = gphy_rev;\r\nelse if (gphy_rev == 0 || gphy_rev == 0x01ff) {\r\npr_warn("Invalid GPHY revision detected: 0x%04x\n", gphy_rev);\r\nreturn;\r\n}\r\n#ifdef CONFIG_PHYS_ADDR_T_64BIT\r\nif (!(params->flags & GENET_HAS_40BITS))\r\npr_warn("GENET does not support 40-bits PA\n");\r\n#endif\r\npr_debug("Configuration for version: %d\n"\r\n"TXq: %1d, TXqBDs: %1d, RXq: %1d, RXqBDs: %1d\n"\r\n"BP << en: %2d, BP msk: 0x%05x\n"\r\n"HFB count: %2d, QTAQ msk: 0x%05x\n"\r\n"TBUF: 0x%04x, HFB: 0x%04x, HFBreg: 0x%04x\n"\r\n"RDMA: 0x%05x, TDMA: 0x%05x\n"\r\n"Words/BD: %d\n",\r\npriv->version,\r\nparams->tx_queues, params->tx_bds_per_q,\r\nparams->rx_queues, params->rx_bds_per_q,\r\nparams->bp_in_en_shift, params->bp_in_mask,\r\nparams->hfb_filter_cnt, params->qtag_mask,\r\nparams->tbuf_offset, params->hfb_offset,\r\nparams->hfb_reg_offset,\r\nparams->rdma_offset, params->tdma_offset,\r\nparams->words_per_bd);\r\n}\r\nstatic int bcmgenet_probe(struct platform_device *pdev)\r\n{\r\nstruct bcmgenet_platform_data *pd = pdev->dev.platform_data;\r\nstruct device_node *dn = pdev->dev.of_node;\r\nconst struct of_device_id *of_id = NULL;\r\nstruct bcmgenet_priv *priv;\r\nstruct net_device *dev;\r\nconst void *macaddr;\r\nstruct resource *r;\r\nint err = -EIO;\r\ndev = alloc_etherdev_mqs(sizeof(*priv), GENET_MAX_MQ_CNT + 1,\r\nGENET_MAX_MQ_CNT + 1);\r\nif (!dev) {\r\ndev_err(&pdev->dev, "can't allocate net device\n");\r\nreturn -ENOMEM;\r\n}\r\nif (dn) {\r\nof_id = of_match_node(bcmgenet_match, dn);\r\nif (!of_id)\r\nreturn -EINVAL;\r\n}\r\npriv = netdev_priv(dev);\r\npriv->irq0 = platform_get_irq(pdev, 0);\r\npriv->irq1 = platform_get_irq(pdev, 1);\r\npriv->wol_irq = platform_get_irq(pdev, 2);\r\nif (!priv->irq0 || !priv->irq1) {\r\ndev_err(&pdev->dev, "can't find IRQs\n");\r\nerr = -EINVAL;\r\ngoto err;\r\n}\r\nif (dn) {\r\nmacaddr = of_get_mac_address(dn);\r\nif (!macaddr) {\r\ndev_err(&pdev->dev, "can't find MAC address\n");\r\nerr = -EINVAL;\r\ngoto err;\r\n}\r\n} else {\r\nmacaddr = pd->mac_address;\r\n}\r\nr = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\npriv->base = devm_ioremap_resource(&pdev->dev, r);\r\nif (IS_ERR(priv->base)) {\r\nerr = PTR_ERR(priv->base);\r\ngoto err;\r\n}\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\ndev_set_drvdata(&pdev->dev, dev);\r\nether_addr_copy(dev->dev_addr, macaddr);\r\ndev->watchdog_timeo = 2 * HZ;\r\ndev->ethtool_ops = &bcmgenet_ethtool_ops;\r\ndev->netdev_ops = &bcmgenet_netdev_ops;\r\npriv->msg_enable = netif_msg_init(-1, GENET_MSG_DEFAULT);\r\ndev->hw_features |= NETIF_F_SG | NETIF_F_IP_CSUM |\r\nNETIF_F_IPV6_CSUM | NETIF_F_RXCSUM;\r\npriv->wol_irq_disabled = true;\r\nerr = devm_request_irq(&pdev->dev, priv->wol_irq, bcmgenet_wol_isr, 0,\r\ndev->name, priv);\r\nif (!err)\r\ndevice_set_wakeup_capable(&pdev->dev, 1);\r\ndev->needed_headroom += 64;\r\nnetdev_boot_setup_check(dev);\r\npriv->dev = dev;\r\npriv->pdev = pdev;\r\nif (of_id)\r\npriv->version = (enum bcmgenet_version)of_id->data;\r\nelse\r\npriv->version = pd->genet_version;\r\npriv->clk = devm_clk_get(&priv->pdev->dev, "enet");\r\nif (IS_ERR(priv->clk)) {\r\ndev_warn(&priv->pdev->dev, "failed to get enet clock\n");\r\npriv->clk = NULL;\r\n}\r\nclk_prepare_enable(priv->clk);\r\nbcmgenet_set_hw_params(priv);\r\ninit_waitqueue_head(&priv->wq);\r\npriv->rx_buf_len = RX_BUF_LENGTH;\r\nINIT_WORK(&priv->bcmgenet_irq_work, bcmgenet_irq_task);\r\npriv->clk_wol = devm_clk_get(&priv->pdev->dev, "enet-wol");\r\nif (IS_ERR(priv->clk_wol)) {\r\ndev_warn(&priv->pdev->dev, "failed to get enet-wol clock\n");\r\npriv->clk_wol = NULL;\r\n}\r\npriv->clk_eee = devm_clk_get(&priv->pdev->dev, "enet-eee");\r\nif (IS_ERR(priv->clk_eee)) {\r\ndev_warn(&priv->pdev->dev, "failed to get enet-eee clock\n");\r\npriv->clk_eee = NULL;\r\n}\r\nerr = reset_umac(priv);\r\nif (err)\r\ngoto err_clk_disable;\r\nerr = bcmgenet_mii_init(dev);\r\nif (err)\r\ngoto err_clk_disable;\r\nnetif_set_real_num_tx_queues(priv->dev, priv->hw_params->tx_queues + 1);\r\nnetif_set_real_num_rx_queues(priv->dev, priv->hw_params->rx_queues + 1);\r\nnetif_carrier_off(dev);\r\nclk_disable_unprepare(priv->clk);\r\nerr = register_netdev(dev);\r\nif (err)\r\ngoto err;\r\nreturn err;\r\nerr_clk_disable:\r\nclk_disable_unprepare(priv->clk);\r\nerr:\r\nfree_netdev(dev);\r\nreturn err;\r\n}\r\nstatic int bcmgenet_remove(struct platform_device *pdev)\r\n{\r\nstruct bcmgenet_priv *priv = dev_to_priv(&pdev->dev);\r\ndev_set_drvdata(&pdev->dev, NULL);\r\nunregister_netdev(priv->dev);\r\nbcmgenet_mii_exit(priv->dev);\r\nfree_netdev(priv->dev);\r\nreturn 0;\r\n}\r\nstatic int bcmgenet_suspend(struct device *d)\r\n{\r\nstruct net_device *dev = dev_get_drvdata(d);\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nint ret;\r\nif (!netif_running(dev))\r\nreturn 0;\r\nbcmgenet_netif_stop(dev);\r\nphy_suspend(priv->phydev);\r\nnetif_device_detach(dev);\r\numac_enable_set(priv, CMD_RX_EN, false);\r\nret = bcmgenet_dma_teardown(priv);\r\nif (ret)\r\nreturn ret;\r\numac_enable_set(priv, CMD_TX_EN, false);\r\nbcmgenet_tx_reclaim_all(dev);\r\nbcmgenet_fini_dma(priv);\r\nif (device_may_wakeup(d) && priv->wolopts) {\r\nret = bcmgenet_power_down(priv, GENET_POWER_WOL_MAGIC);\r\nclk_prepare_enable(priv->clk_wol);\r\n} else if (priv->internal_phy) {\r\nret = bcmgenet_power_down(priv, GENET_POWER_PASSIVE);\r\n}\r\nclk_disable_unprepare(priv->clk);\r\nreturn ret;\r\n}\r\nstatic int bcmgenet_resume(struct device *d)\r\n{\r\nstruct net_device *dev = dev_get_drvdata(d);\r\nstruct bcmgenet_priv *priv = netdev_priv(dev);\r\nunsigned long dma_ctrl;\r\nint ret;\r\nu32 reg;\r\nif (!netif_running(dev))\r\nreturn 0;\r\nret = clk_prepare_enable(priv->clk);\r\nif (ret)\r\nreturn ret;\r\nif (priv->internal_phy)\r\nbcmgenet_power_up(priv, GENET_POWER_PASSIVE);\r\nbcmgenet_umac_reset(priv);\r\nret = init_umac(priv);\r\nif (ret)\r\ngoto out_clk_disable;\r\nif (priv->wolopts)\r\nclk_disable_unprepare(priv->clk_wol);\r\nphy_init_hw(priv->phydev);\r\nbcmgenet_mii_config(priv->dev);\r\numac_enable_set(priv, CMD_TX_EN | CMD_RX_EN, false);\r\nbcmgenet_set_hw_addr(priv, dev->dev_addr);\r\nif (priv->internal_phy) {\r\nreg = bcmgenet_ext_readl(priv, EXT_EXT_PWR_MGMT);\r\nreg |= EXT_ENERGY_DET_MASK;\r\nbcmgenet_ext_writel(priv, reg, EXT_EXT_PWR_MGMT);\r\n}\r\nif (priv->wolopts)\r\nbcmgenet_power_up(priv, GENET_POWER_WOL_MAGIC);\r\ndma_ctrl = bcmgenet_dma_disable(priv);\r\nret = bcmgenet_init_dma(priv);\r\nif (ret) {\r\nnetdev_err(dev, "failed to initialize DMA\n");\r\ngoto out_clk_disable;\r\n}\r\nbcmgenet_enable_dma(priv, dma_ctrl);\r\nnetif_device_attach(dev);\r\nphy_resume(priv->phydev);\r\nif (priv->eee.eee_enabled)\r\nbcmgenet_eee_enable_set(dev, true);\r\nbcmgenet_netif_start(dev);\r\nreturn 0;\r\nout_clk_disable:\r\nclk_disable_unprepare(priv->clk);\r\nreturn ret;\r\n}
