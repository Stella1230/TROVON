static void rdma_build_arg_xdr(struct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *ctxt,\r\nu32 byte_count)\r\n{\r\nstruct rpcrdma_msg *rmsgp;\r\nstruct page *page;\r\nu32 bc;\r\nint sge_no;\r\npage = ctxt->pages[0];\r\nput_page(rqstp->rq_pages[0]);\r\nrqstp->rq_pages[0] = page;\r\nrqstp->rq_arg.head[0].iov_base = page_address(page);\r\nrqstp->rq_arg.head[0].iov_len =\r\nmin_t(size_t, byte_count, ctxt->sge[0].length);\r\nrqstp->rq_arg.len = byte_count;\r\nrqstp->rq_arg.buflen = byte_count;\r\nbc = byte_count - rqstp->rq_arg.head[0].iov_len;\r\nrqstp->rq_arg.page_len = bc;\r\nrqstp->rq_arg.page_base = 0;\r\nrmsgp = (struct rpcrdma_msg *)rqstp->rq_arg.head[0].iov_base;\r\nif (rmsgp->rm_type == rdma_nomsg)\r\nrqstp->rq_arg.pages = &rqstp->rq_pages[0];\r\nelse\r\nrqstp->rq_arg.pages = &rqstp->rq_pages[1];\r\nsge_no = 1;\r\nwhile (bc && sge_no < ctxt->count) {\r\npage = ctxt->pages[sge_no];\r\nput_page(rqstp->rq_pages[sge_no]);\r\nrqstp->rq_pages[sge_no] = page;\r\nbc -= min_t(u32, bc, ctxt->sge[sge_no].length);\r\nrqstp->rq_arg.buflen += ctxt->sge[sge_no].length;\r\nsge_no++;\r\n}\r\nrqstp->rq_respages = &rqstp->rq_pages[sge_no];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nbc = sge_no;\r\nwhile (sge_no < ctxt->count) {\r\npage = ctxt->pages[sge_no++];\r\nput_page(page);\r\n}\r\nctxt->count = bc;\r\nrqstp->rq_arg.tail[0].iov_base = NULL;\r\nrqstp->rq_arg.tail[0].iov_len = 0;\r\n}\r\nint rdma_read_chunk_lcl(struct svcxprt_rdma *xprt,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head,\r\nint *page_no,\r\nu32 *page_offset,\r\nu32 rs_handle,\r\nu32 rs_length,\r\nu64 rs_offset,\r\nbool last)\r\n{\r\nstruct ib_rdma_wr read_wr;\r\nint pages_needed = PAGE_ALIGN(*page_offset + rs_length) >> PAGE_SHIFT;\r\nstruct svc_rdma_op_ctxt *ctxt = svc_rdma_get_context(xprt);\r\nint ret, read, pno;\r\nu32 pg_off = *page_offset;\r\nu32 pg_no = *page_no;\r\nctxt->direction = DMA_FROM_DEVICE;\r\nctxt->read_hdr = head;\r\npages_needed = min_t(int, pages_needed, xprt->sc_max_sge_rd);\r\nread = min_t(int, (pages_needed << PAGE_SHIFT) - *page_offset,\r\nrs_length);\r\nfor (pno = 0; pno < pages_needed; pno++) {\r\nint len = min_t(int, rs_length, PAGE_SIZE - pg_off);\r\nhead->arg.pages[pg_no] = rqstp->rq_arg.pages[pg_no];\r\nhead->arg.page_len += len;\r\nhead->arg.len += len;\r\nif (!pg_off)\r\nhead->count++;\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[pg_no+1];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nctxt->sge[pno].addr =\r\nib_dma_map_page(xprt->sc_cm_id->device,\r\nhead->arg.pages[pg_no], pg_off,\r\nPAGE_SIZE - pg_off,\r\nDMA_FROM_DEVICE);\r\nret = ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nctxt->sge[pno].addr);\r\nif (ret)\r\ngoto err;\r\natomic_inc(&xprt->sc_dma_used);\r\nctxt->sge[pno].lkey = xprt->sc_pd->local_dma_lkey;\r\nctxt->sge[pno].length = len;\r\nctxt->count++;\r\npg_off += len;\r\nif (pg_off == PAGE_SIZE) {\r\npg_off = 0;\r\npg_no++;\r\n}\r\nrs_length -= len;\r\n}\r\nif (last && rs_length == 0)\r\nset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nelse\r\nclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nmemset(&read_wr, 0, sizeof(read_wr));\r\nctxt->cqe.done = svc_rdma_wc_read;\r\nread_wr.wr.wr_cqe = &ctxt->cqe;\r\nread_wr.wr.opcode = IB_WR_RDMA_READ;\r\nread_wr.wr.send_flags = IB_SEND_SIGNALED;\r\nread_wr.rkey = rs_handle;\r\nread_wr.remote_addr = rs_offset;\r\nread_wr.wr.sg_list = ctxt->sge;\r\nread_wr.wr.num_sge = pages_needed;\r\nret = svc_rdma_send(xprt, &read_wr.wr);\r\nif (ret) {\r\npr_err("svcrdma: Error %d posting RDMA_READ\n", ret);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\ngoto err;\r\n}\r\n*page_no = pg_no;\r\n*page_offset = pg_off;\r\nret = read;\r\natomic_inc(&rdma_stat_read);\r\nreturn ret;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn ret;\r\n}\r\nint rdma_read_chunk_frmr(struct svcxprt_rdma *xprt,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head,\r\nint *page_no,\r\nu32 *page_offset,\r\nu32 rs_handle,\r\nu32 rs_length,\r\nu64 rs_offset,\r\nbool last)\r\n{\r\nstruct ib_rdma_wr read_wr;\r\nstruct ib_send_wr inv_wr;\r\nstruct ib_reg_wr reg_wr;\r\nu8 key;\r\nint nents = PAGE_ALIGN(*page_offset + rs_length) >> PAGE_SHIFT;\r\nstruct svc_rdma_op_ctxt *ctxt = svc_rdma_get_context(xprt);\r\nstruct svc_rdma_fastreg_mr *frmr = svc_rdma_get_frmr(xprt);\r\nint ret, read, pno, dma_nents, n;\r\nu32 pg_off = *page_offset;\r\nu32 pg_no = *page_no;\r\nif (IS_ERR(frmr))\r\nreturn -ENOMEM;\r\nctxt->direction = DMA_FROM_DEVICE;\r\nctxt->frmr = frmr;\r\nnents = min_t(unsigned int, nents, xprt->sc_frmr_pg_list_len);\r\nread = min_t(int, (nents << PAGE_SHIFT) - *page_offset, rs_length);\r\nfrmr->direction = DMA_FROM_DEVICE;\r\nfrmr->access_flags = (IB_ACCESS_LOCAL_WRITE|IB_ACCESS_REMOTE_WRITE);\r\nfrmr->sg_nents = nents;\r\nfor (pno = 0; pno < nents; pno++) {\r\nint len = min_t(int, rs_length, PAGE_SIZE - pg_off);\r\nhead->arg.pages[pg_no] = rqstp->rq_arg.pages[pg_no];\r\nhead->arg.page_len += len;\r\nhead->arg.len += len;\r\nif (!pg_off)\r\nhead->count++;\r\nsg_set_page(&frmr->sg[pno], rqstp->rq_arg.pages[pg_no],\r\nlen, pg_off);\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[pg_no+1];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\npg_off += len;\r\nif (pg_off == PAGE_SIZE) {\r\npg_off = 0;\r\npg_no++;\r\n}\r\nrs_length -= len;\r\n}\r\nif (last && rs_length == 0)\r\nset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\nelse\r\nclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\r\ndma_nents = ib_dma_map_sg(xprt->sc_cm_id->device,\r\nfrmr->sg, frmr->sg_nents,\r\nfrmr->direction);\r\nif (!dma_nents) {\r\npr_err("svcrdma: failed to dma map sg %p\n",\r\nfrmr->sg);\r\nreturn -ENOMEM;\r\n}\r\natomic_inc(&xprt->sc_dma_used);\r\nn = ib_map_mr_sg(frmr->mr, frmr->sg, frmr->sg_nents, NULL, PAGE_SIZE);\r\nif (unlikely(n != frmr->sg_nents)) {\r\npr_err("svcrdma: failed to map mr %p (%d/%d elements)\n",\r\nfrmr->mr, n, frmr->sg_nents);\r\nreturn n < 0 ? n : -EINVAL;\r\n}\r\nkey = (u8)(frmr->mr->lkey & 0x000000FF);\r\nib_update_fast_reg_key(frmr->mr, ++key);\r\nctxt->sge[0].addr = frmr->mr->iova;\r\nctxt->sge[0].lkey = frmr->mr->lkey;\r\nctxt->sge[0].length = frmr->mr->length;\r\nctxt->count = 1;\r\nctxt->read_hdr = head;\r\nctxt->reg_cqe.done = svc_rdma_wc_reg;\r\nreg_wr.wr.wr_cqe = &ctxt->reg_cqe;\r\nreg_wr.wr.opcode = IB_WR_REG_MR;\r\nreg_wr.wr.send_flags = IB_SEND_SIGNALED;\r\nreg_wr.wr.num_sge = 0;\r\nreg_wr.mr = frmr->mr;\r\nreg_wr.key = frmr->mr->lkey;\r\nreg_wr.access = frmr->access_flags;\r\nreg_wr.wr.next = &read_wr.wr;\r\nmemset(&read_wr, 0, sizeof(read_wr));\r\nctxt->cqe.done = svc_rdma_wc_read;\r\nread_wr.wr.wr_cqe = &ctxt->cqe;\r\nread_wr.wr.send_flags = IB_SEND_SIGNALED;\r\nread_wr.rkey = rs_handle;\r\nread_wr.remote_addr = rs_offset;\r\nread_wr.wr.sg_list = ctxt->sge;\r\nread_wr.wr.num_sge = 1;\r\nif (xprt->sc_dev_caps & SVCRDMA_DEVCAP_READ_W_INV) {\r\nread_wr.wr.opcode = IB_WR_RDMA_READ_WITH_INV;\r\nread_wr.wr.ex.invalidate_rkey = ctxt->frmr->mr->lkey;\r\n} else {\r\nread_wr.wr.opcode = IB_WR_RDMA_READ;\r\nread_wr.wr.next = &inv_wr;\r\nmemset(&inv_wr, 0, sizeof(inv_wr));\r\nctxt->inv_cqe.done = svc_rdma_wc_inv;\r\ninv_wr.wr_cqe = &ctxt->inv_cqe;\r\ninv_wr.opcode = IB_WR_LOCAL_INV;\r\ninv_wr.send_flags = IB_SEND_SIGNALED | IB_SEND_FENCE;\r\ninv_wr.ex.invalidate_rkey = frmr->mr->lkey;\r\n}\r\nret = svc_rdma_send(xprt, &reg_wr.wr);\r\nif (ret) {\r\npr_err("svcrdma: Error %d posting RDMA_READ\n", ret);\r\nset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\r\ngoto err;\r\n}\r\n*page_no = pg_no;\r\n*page_offset = pg_off;\r\nret = read;\r\natomic_inc(&rdma_stat_read);\r\nreturn ret;\r\nerr:\r\nib_dma_unmap_sg(xprt->sc_cm_id->device,\r\nfrmr->sg, frmr->sg_nents, frmr->direction);\r\nsvc_rdma_put_context(ctxt, 0);\r\nsvc_rdma_put_frmr(xprt, frmr);\r\nreturn ret;\r\n}\r\nstatic unsigned int\r\nrdma_rcl_chunk_count(struct rpcrdma_read_chunk *ch)\r\n{\r\nunsigned int count;\r\nfor (count = 0; ch->rc_discrim != xdr_zero; ch++)\r\ncount++;\r\nreturn count;\r\n}\r\nstatic int\r\nrdma_copy_tail(struct svc_rqst *rqstp, struct svc_rdma_op_ctxt *head,\r\nu32 position, u32 byte_count, u32 page_offset, int page_no)\r\n{\r\nchar *srcp, *destp;\r\nint ret;\r\nret = 0;\r\nsrcp = head->arg.head[0].iov_base + position;\r\nbyte_count = head->arg.head[0].iov_len - position;\r\nif (byte_count > PAGE_SIZE) {\r\ndprintk("svcrdma: large tail unsupported\n");\r\nreturn 0;\r\n}\r\nif (page_offset != PAGE_SIZE) {\r\ndestp = page_address(rqstp->rq_arg.pages[page_no]);\r\ndestp += page_offset;\r\nwhile (byte_count--) {\r\n*destp++ = *srcp++;\r\npage_offset++;\r\nif (page_offset == PAGE_SIZE && byte_count)\r\ngoto more;\r\n}\r\ngoto done;\r\n}\r\nmore:\r\npage_no++;\r\ndestp = page_address(rqstp->rq_arg.pages[page_no]);\r\nwhile (byte_count--)\r\n*destp++ = *srcp++;\r\nrqstp->rq_respages = &rqstp->rq_arg.pages[page_no+1];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\ndone:\r\nbyte_count = head->arg.head[0].iov_len - position;\r\nhead->arg.page_len += byte_count;\r\nhead->arg.len += byte_count;\r\nhead->arg.buflen += byte_count;\r\nreturn 1;\r\n}\r\nstatic int rdma_read_chunks(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_msg *rmsgp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head)\r\n{\r\nint page_no, ret;\r\nstruct rpcrdma_read_chunk *ch;\r\nu32 handle, page_offset, byte_count;\r\nu32 position;\r\nu64 rs_offset;\r\nbool last;\r\nch = svc_rdma_get_read_chunk(rmsgp);\r\nif (!ch)\r\nreturn 0;\r\nif (rdma_rcl_chunk_count(ch) > RPCSVC_MAXPAGES)\r\nreturn -EINVAL;\r\nhead->arg.head[0] = rqstp->rq_arg.head[0];\r\nhead->arg.tail[0] = rqstp->rq_arg.tail[0];\r\nhead->hdr_count = head->count;\r\nhead->arg.page_base = 0;\r\nhead->arg.page_len = 0;\r\nhead->arg.len = rqstp->rq_arg.len;\r\nhead->arg.buflen = rqstp->rq_arg.buflen;\r\nposition = be32_to_cpu(ch->rc_position);\r\nif (position == 0) {\r\nhead->arg.pages = &head->pages[0];\r\npage_offset = head->byte_len;\r\n} else {\r\nhead->arg.pages = &head->pages[head->count];\r\npage_offset = 0;\r\n}\r\nret = 0;\r\npage_no = 0;\r\nfor (; ch->rc_discrim != xdr_zero; ch++) {\r\nif (be32_to_cpu(ch->rc_position) != position)\r\ngoto err;\r\nhandle = be32_to_cpu(ch->rc_target.rs_handle),\r\nbyte_count = be32_to_cpu(ch->rc_target.rs_length);\r\nxdr_decode_hyper((__be32 *)&ch->rc_target.rs_offset,\r\n&rs_offset);\r\nwhile (byte_count > 0) {\r\nlast = (ch + 1)->rc_discrim == xdr_zero;\r\nret = xprt->sc_reader(xprt, rqstp, head,\r\n&page_no, &page_offset,\r\nhandle, byte_count,\r\nrs_offset, last);\r\nif (ret < 0)\r\ngoto err;\r\nbyte_count -= ret;\r\nrs_offset += ret;\r\nhead->arg.buflen += ret;\r\n}\r\n}\r\nif (page_offset & 3) {\r\nu32 pad = 4 - (page_offset & 3);\r\nhead->arg.tail[0].iov_len += pad;\r\nhead->arg.len += pad;\r\nhead->arg.buflen += pad;\r\npage_offset += pad;\r\n}\r\nret = 1;\r\nif (position && position < head->arg.head[0].iov_len)\r\nret = rdma_copy_tail(rqstp, head, position,\r\nbyte_count, page_offset, page_no);\r\nhead->arg.head[0].iov_len = position;\r\nhead->position = position;\r\nerr:\r\nfor (page_no = 0;\r\n&rqstp->rq_pages[page_no] < rqstp->rq_respages; page_no++)\r\nrqstp->rq_pages[page_no] = NULL;\r\nreturn ret;\r\n}\r\nstatic void rdma_read_complete(struct svc_rqst *rqstp,\r\nstruct svc_rdma_op_ctxt *head)\r\n{\r\nint page_no;\r\nfor (page_no = 0; page_no < head->count; page_no++) {\r\nput_page(rqstp->rq_pages[page_no]);\r\nrqstp->rq_pages[page_no] = head->pages[page_no];\r\n}\r\nif (head->position == 0) {\r\nif (head->arg.len <= head->sge[0].length) {\r\nhead->arg.head[0].iov_len = head->arg.len -\r\nhead->byte_len;\r\nhead->arg.page_len = 0;\r\n} else {\r\nhead->arg.head[0].iov_len = head->sge[0].length -\r\nhead->byte_len;\r\nhead->arg.page_len = head->arg.len -\r\nhead->sge[0].length;\r\n}\r\n}\r\nrqstp->rq_arg.pages = &rqstp->rq_pages[head->hdr_count];\r\nrqstp->rq_arg.page_len = head->arg.page_len;\r\nrqstp->rq_arg.page_base = head->arg.page_base;\r\nrqstp->rq_respages = &rqstp->rq_pages[page_no];\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nrqstp->rq_arg.head[0] = head->arg.head[0];\r\nrqstp->rq_arg.tail[0] = head->arg.tail[0];\r\nrqstp->rq_arg.len = head->arg.len;\r\nrqstp->rq_arg.buflen = head->arg.buflen;\r\n}\r\nstatic bool\r\nsvc_rdma_is_backchannel_reply(struct svc_xprt *xprt, struct rpcrdma_msg *rmsgp)\r\n{\r\n__be32 *p = (__be32 *)rmsgp;\r\nif (!xprt->xpt_bc_xprt)\r\nreturn false;\r\nif (rmsgp->rm_type != rdma_msg)\r\nreturn false;\r\nif (rmsgp->rm_body.rm_chunks[0] != xdr_zero)\r\nreturn false;\r\nif (rmsgp->rm_body.rm_chunks[1] != xdr_zero)\r\nreturn false;\r\nif (rmsgp->rm_body.rm_chunks[2] != xdr_zero)\r\nreturn false;\r\nif (p[7] != rmsgp->rm_xid)\r\nreturn false;\r\nif (p[8] == cpu_to_be32(RPC_CALL))\r\nreturn false;\r\nreturn true;\r\n}\r\nint svc_rdma_recvfrom(struct svc_rqst *rqstp)\r\n{\r\nstruct svc_xprt *xprt = rqstp->rq_xprt;\r\nstruct svcxprt_rdma *rdma_xprt =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nstruct svc_rdma_op_ctxt *ctxt = NULL;\r\nstruct rpcrdma_msg *rmsgp;\r\nint ret = 0;\r\ndprintk("svcrdma: rqstp=%p\n", rqstp);\r\nspin_lock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nif (!list_empty(&rdma_xprt->sc_read_complete_q)) {\r\nctxt = list_entry(rdma_xprt->sc_read_complete_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\nspin_unlock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nrdma_read_complete(rqstp, ctxt);\r\ngoto complete;\r\n} else if (!list_empty(&rdma_xprt->sc_rq_dto_q)) {\r\nctxt = list_entry(rdma_xprt->sc_rq_dto_q.next,\r\nstruct svc_rdma_op_ctxt,\r\ndto_q);\r\nlist_del_init(&ctxt->dto_q);\r\n} else {\r\natomic_inc(&rdma_stat_rq_starve);\r\nclear_bit(XPT_DATA, &xprt->xpt_flags);\r\nctxt = NULL;\r\n}\r\nspin_unlock_bh(&rdma_xprt->sc_rq_dto_lock);\r\nif (!ctxt) {\r\nif (test_bit(XPT_CLOSE, &xprt->xpt_flags))\r\ngoto defer;\r\ngoto out;\r\n}\r\ndprintk("svcrdma: processing ctxt=%p on xprt=%p, rqstp=%p, status=%d\n",\r\nctxt, rdma_xprt, rqstp, ctxt->wc_status);\r\natomic_inc(&rdma_stat_recv);\r\nrdma_build_arg_xdr(rqstp, ctxt, ctxt->byte_len);\r\nrmsgp = (struct rpcrdma_msg *)rqstp->rq_arg.head[0].iov_base;\r\nret = svc_rdma_xdr_decode_req(&rqstp->rq_arg);\r\nif (ret < 0)\r\ngoto out_err;\r\nif (ret == 0)\r\ngoto out_drop;\r\nrqstp->rq_xprt_hlen = ret;\r\nif (svc_rdma_is_backchannel_reply(xprt, rmsgp)) {\r\nret = svc_rdma_handle_bc_reply(xprt->xpt_bc_xprt, rmsgp,\r\n&rqstp->rq_arg);\r\nsvc_rdma_put_context(ctxt, 0);\r\nif (ret)\r\ngoto repost;\r\nreturn ret;\r\n}\r\nret = rdma_read_chunks(rdma_xprt, rmsgp, rqstp, ctxt);\r\nif (ret > 0) {\r\ngoto defer;\r\n} else if (ret < 0) {\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn 0;\r\n}\r\ncomplete:\r\nret = rqstp->rq_arg.head[0].iov_len\r\n+ rqstp->rq_arg.page_len\r\n+ rqstp->rq_arg.tail[0].iov_len;\r\nsvc_rdma_put_context(ctxt, 0);\r\nout:\r\ndprintk("svcrdma: ret=%d, rq_arg.len=%u, "\r\n"rq_arg.head[0].iov_base=%p, rq_arg.head[0].iov_len=%zd\n",\r\nret, rqstp->rq_arg.len,\r\nrqstp->rq_arg.head[0].iov_base,\r\nrqstp->rq_arg.head[0].iov_len);\r\nrqstp->rq_prot = IPPROTO_MAX;\r\nsvc_xprt_copy_addrs(rqstp, xprt);\r\nreturn ret;\r\nout_err:\r\nsvc_rdma_send_error(rdma_xprt, rmsgp, ret);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn 0;\r\ndefer:\r\nreturn 0;\r\nout_drop:\r\nsvc_rdma_put_context(ctxt, 1);\r\nrepost:\r\nreturn svc_rdma_repost_recv(rdma_xprt, GFP_KERNEL);\r\n}
