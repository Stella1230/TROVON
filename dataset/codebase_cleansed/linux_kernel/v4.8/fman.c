static irqreturn_t fman_exceptions(struct fman *fman,\r\nenum fman_exceptions exception)\r\n{\r\ndev_dbg(fman->dev, "%s: FMan[%d] exception %d\n",\r\n__func__, fman->state->fm_id, exception);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic irqreturn_t fman_bus_error(struct fman *fman, u8 __maybe_unused port_id,\r\nu64 __maybe_unused addr,\r\nu8 __maybe_unused tnum,\r\nu16 __maybe_unused liodn)\r\n{\r\ndev_dbg(fman->dev, "%s: FMan[%d] bus error: port_id[%d]\n",\r\n__func__, fman->state->fm_id, port_id);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic inline irqreturn_t call_mac_isr(struct fman *fman, u8 id)\r\n{\r\nif (fman->intr_mng[id].isr_cb) {\r\nfman->intr_mng[id].isr_cb(fman->intr_mng[id].src_handle);\r\nreturn IRQ_HANDLED;\r\n}\r\nreturn IRQ_NONE;\r\n}\r\nstatic inline u8 hw_port_id_to_sw_port_id(u8 major, u8 hw_port_id)\r\n{\r\nu8 sw_port_id = 0;\r\nif (hw_port_id >= BASE_TX_PORTID)\r\nsw_port_id = hw_port_id - BASE_TX_PORTID;\r\nelse if (hw_port_id >= BASE_RX_PORTID)\r\nsw_port_id = hw_port_id - BASE_RX_PORTID;\r\nelse\r\nsw_port_id = 0;\r\nreturn sw_port_id;\r\n}\r\nstatic void set_port_order_restoration(struct fman_fpm_regs __iomem *fpm_rg,\r\nu8 port_id)\r\n{\r\nu32 tmp = 0;\r\ntmp = port_id << FPM_PORT_FM_CTL_PORTID_SHIFT;\r\ntmp |= FPM_PRT_FM_CTL2 | FPM_PRT_FM_CTL1;\r\nif (port_id % 2)\r\ntmp |= FPM_PRT_FM_CTL1 << FPM_PRC_ORA_FM_CTL_SEL_SHIFT;\r\nelse\r\ntmp |= FPM_PRT_FM_CTL2 << FPM_PRC_ORA_FM_CTL_SEL_SHIFT;\r\niowrite32be(tmp, &fpm_rg->fmfp_prc);\r\n}\r\nstatic void set_port_liodn(struct fman *fman, u8 port_id,\r\nu32 liodn_base, u32 liodn_ofst)\r\n{\r\nu32 tmp;\r\ntmp = ioread32be(&fman->dma_regs->fmdmplr[port_id / 2]);\r\nif (port_id % 2) {\r\ntmp &= ~DMA_LIODN_BASE_MASK;\r\ntmp |= liodn_base;\r\n} else {\r\ntmp &= ~(DMA_LIODN_BASE_MASK << DMA_LIODN_SHIFT);\r\ntmp |= liodn_base << DMA_LIODN_SHIFT;\r\n}\r\niowrite32be(tmp, &fman->dma_regs->fmdmplr[port_id / 2]);\r\niowrite32be(liodn_ofst, &fman->bmi_regs->fmbm_spliodn[port_id - 1]);\r\n}\r\nstatic void enable_rams_ecc(struct fman_fpm_regs __iomem *fpm_rg)\r\n{\r\nu32 tmp;\r\ntmp = ioread32be(&fpm_rg->fm_rcr);\r\nif (tmp & FPM_RAM_RAMS_ECC_EN_SRC_SEL)\r\niowrite32be(tmp | FPM_RAM_IRAM_ECC_EN, &fpm_rg->fm_rcr);\r\nelse\r\niowrite32be(tmp | FPM_RAM_RAMS_ECC_EN |\r\nFPM_RAM_IRAM_ECC_EN, &fpm_rg->fm_rcr);\r\n}\r\nstatic void disable_rams_ecc(struct fman_fpm_regs __iomem *fpm_rg)\r\n{\r\nu32 tmp;\r\ntmp = ioread32be(&fpm_rg->fm_rcr);\r\nif (tmp & FPM_RAM_RAMS_ECC_EN_SRC_SEL)\r\niowrite32be(tmp & ~FPM_RAM_IRAM_ECC_EN, &fpm_rg->fm_rcr);\r\nelse\r\niowrite32be(tmp & ~(FPM_RAM_RAMS_ECC_EN | FPM_RAM_IRAM_ECC_EN),\r\n&fpm_rg->fm_rcr);\r\n}\r\nstatic void fman_defconfig(struct fman_cfg *cfg)\r\n{\r\nmemset(cfg, 0, sizeof(struct fman_cfg));\r\ncfg->catastrophic_err = DEFAULT_CATASTROPHIC_ERR;\r\ncfg->dma_err = DEFAULT_DMA_ERR;\r\ncfg->dma_aid_mode = DEFAULT_AID_MODE;\r\ncfg->dma_comm_qtsh_clr_emer = DEFAULT_DMA_COMM_Q_LOW;\r\ncfg->dma_comm_qtsh_asrt_emer = DEFAULT_DMA_COMM_Q_HIGH;\r\ncfg->dma_cache_override = DEFAULT_CACHE_OVERRIDE;\r\ncfg->dma_cam_num_of_entries = DEFAULT_DMA_CAM_NUM_OF_ENTRIES;\r\ncfg->dma_dbg_cnt_mode = DEFAULT_DMA_DBG_CNT_MODE;\r\ncfg->dma_sos_emergency = DEFAULT_DMA_SOS_EMERGENCY;\r\ncfg->dma_watchdog = DEFAULT_DMA_WATCHDOG;\r\ncfg->disp_limit_tsh = DEFAULT_DISP_LIMIT;\r\ncfg->prs_disp_tsh = DEFAULT_PRS_DISP_TH;\r\ncfg->plcr_disp_tsh = DEFAULT_PLCR_DISP_TH;\r\ncfg->kg_disp_tsh = DEFAULT_KG_DISP_TH;\r\ncfg->bmi_disp_tsh = DEFAULT_BMI_DISP_TH;\r\ncfg->qmi_enq_disp_tsh = DEFAULT_QMI_ENQ_DISP_TH;\r\ncfg->qmi_deq_disp_tsh = DEFAULT_QMI_DEQ_DISP_TH;\r\ncfg->fm_ctl1_disp_tsh = DEFAULT_FM_CTL1_DISP_TH;\r\ncfg->fm_ctl2_disp_tsh = DEFAULT_FM_CTL2_DISP_TH;\r\n}\r\nstatic int dma_init(struct fman *fman)\r\n{\r\nstruct fman_dma_regs __iomem *dma_rg = fman->dma_regs;\r\nstruct fman_cfg *cfg = fman->cfg;\r\nu32 tmp_reg;\r\ntmp_reg = (DMA_STATUS_BUS_ERR | DMA_STATUS_READ_ECC |\r\nDMA_STATUS_SYSTEM_WRITE_ECC | DMA_STATUS_FM_WRITE_ECC);\r\niowrite32be(ioread32be(&dma_rg->fmdmsr) | tmp_reg, &dma_rg->fmdmsr);\r\ntmp_reg = 0;\r\ntmp_reg |= cfg->dma_cache_override << DMA_MODE_CACHE_OR_SHIFT;\r\nif (cfg->exceptions & EX_DMA_BUS_ERROR)\r\ntmp_reg |= DMA_MODE_BER;\r\nif ((cfg->exceptions & EX_DMA_SYSTEM_WRITE_ECC) |\r\n(cfg->exceptions & EX_DMA_READ_ECC) |\r\n(cfg->exceptions & EX_DMA_FM_WRITE_ECC))\r\ntmp_reg |= DMA_MODE_ECC;\r\nif (cfg->dma_axi_dbg_num_of_beats)\r\ntmp_reg |= (DMA_MODE_AXI_DBG_MASK &\r\n((cfg->dma_axi_dbg_num_of_beats - 1)\r\n<< DMA_MODE_AXI_DBG_SHIFT));\r\ntmp_reg |= (((cfg->dma_cam_num_of_entries / DMA_CAM_UNITS) - 1) &\r\nDMA_MODE_CEN_MASK) << DMA_MODE_CEN_SHIFT;\r\ntmp_reg |= DMA_MODE_SECURE_PROT;\r\ntmp_reg |= cfg->dma_dbg_cnt_mode << DMA_MODE_DBG_SHIFT;\r\ntmp_reg |= cfg->dma_aid_mode << DMA_MODE_AID_MODE_SHIFT;\r\niowrite32be(tmp_reg, &dma_rg->fmdmmr);\r\ntmp_reg = ((u32)cfg->dma_comm_qtsh_asrt_emer <<\r\nDMA_THRESH_COMMQ_SHIFT);\r\ntmp_reg |= (cfg->dma_read_buf_tsh_asrt_emer &\r\nDMA_THRESH_READ_INT_BUF_MASK) << DMA_THRESH_READ_INT_BUF_SHIFT;\r\ntmp_reg |= cfg->dma_write_buf_tsh_asrt_emer &\r\nDMA_THRESH_WRITE_INT_BUF_MASK;\r\niowrite32be(tmp_reg, &dma_rg->fmdmtr);\r\ntmp_reg = ((u32)cfg->dma_comm_qtsh_clr_emer <<\r\nDMA_THRESH_COMMQ_SHIFT);\r\ntmp_reg |= (cfg->dma_read_buf_tsh_clr_emer &\r\nDMA_THRESH_READ_INT_BUF_MASK) << DMA_THRESH_READ_INT_BUF_SHIFT;\r\ntmp_reg |= cfg->dma_write_buf_tsh_clr_emer &\r\nDMA_THRESH_WRITE_INT_BUF_MASK;\r\niowrite32be(tmp_reg, &dma_rg->fmdmhy);\r\niowrite32be(cfg->dma_sos_emergency, &dma_rg->fmdmsetr);\r\niowrite32be((cfg->dma_watchdog * cfg->clk_freq), &dma_rg->fmdmwcr);\r\niowrite32be(cfg->cam_base_addr, &dma_rg->fmdmebcr);\r\nfman->cam_size =\r\n(u32)(fman->cfg->dma_cam_num_of_entries * DMA_CAM_SIZEOF_ENTRY);\r\nfman->cam_offset = fman_muram_alloc(fman->muram, fman->cam_size);\r\nif (IS_ERR_VALUE(fman->cam_offset)) {\r\ndev_err(fman->dev, "%s: MURAM alloc for DMA CAM failed\n",\r\n__func__);\r\nreturn -ENOMEM;\r\n}\r\nif (fman->state->rev_info.major == 2) {\r\nu32 __iomem *cam_base_addr;\r\nfman_muram_free_mem(fman->muram, fman->cam_offset,\r\nfman->cam_size);\r\nfman->cam_size = fman->cfg->dma_cam_num_of_entries * 72 + 128;\r\nfman->cam_offset = fman_muram_alloc(fman->muram,\r\nfman->cam_size);\r\nif (IS_ERR_VALUE(fman->cam_offset)) {\r\ndev_err(fman->dev, "%s: MURAM alloc for DMA CAM failed\n",\r\n__func__);\r\nreturn -ENOMEM;\r\n}\r\nif (fman->cfg->dma_cam_num_of_entries % 8 ||\r\nfman->cfg->dma_cam_num_of_entries > 32) {\r\ndev_err(fman->dev, "%s: wrong dma_cam_num_of_entries\n",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\ncam_base_addr = (u32 __iomem *)\r\nfman_muram_offset_to_vbase(fman->muram,\r\nfman->cam_offset);\r\niowrite32be(~((1 <<\r\n(32 - fman->cfg->dma_cam_num_of_entries)) - 1),\r\ncam_base_addr);\r\n}\r\nfman->cfg->cam_base_addr = fman->cam_offset;\r\nreturn 0;\r\n}\r\nstatic void fpm_init(struct fman_fpm_regs __iomem *fpm_rg, struct fman_cfg *cfg)\r\n{\r\nu32 tmp_reg;\r\nint i;\r\ntmp_reg = (u32)(cfg->disp_limit_tsh << FPM_DISP_LIMIT_SHIFT);\r\niowrite32be(tmp_reg, &fpm_rg->fmfp_mxd);\r\ntmp_reg = (((u32)cfg->prs_disp_tsh << FPM_THR1_PRS_SHIFT) |\r\n((u32)cfg->kg_disp_tsh << FPM_THR1_KG_SHIFT) |\r\n((u32)cfg->plcr_disp_tsh << FPM_THR1_PLCR_SHIFT) |\r\n((u32)cfg->bmi_disp_tsh << FPM_THR1_BMI_SHIFT));\r\niowrite32be(tmp_reg, &fpm_rg->fmfp_dist1);\r\ntmp_reg =\r\n(((u32)cfg->qmi_enq_disp_tsh << FPM_THR2_QMI_ENQ_SHIFT) |\r\n((u32)cfg->qmi_deq_disp_tsh << FPM_THR2_QMI_DEQ_SHIFT) |\r\n((u32)cfg->fm_ctl1_disp_tsh << FPM_THR2_FM_CTL1_SHIFT) |\r\n((u32)cfg->fm_ctl2_disp_tsh << FPM_THR2_FM_CTL2_SHIFT));\r\niowrite32be(tmp_reg, &fpm_rg->fmfp_dist2);\r\ntmp_reg = 0;\r\ntmp_reg |= (FPM_EV_MASK_STALL | FPM_EV_MASK_DOUBLE_ECC |\r\nFPM_EV_MASK_SINGLE_ECC);\r\nif (cfg->exceptions & EX_FPM_STALL_ON_TASKS)\r\ntmp_reg |= FPM_EV_MASK_STALL_EN;\r\nif (cfg->exceptions & EX_FPM_SINGLE_ECC)\r\ntmp_reg |= FPM_EV_MASK_SINGLE_ECC_EN;\r\nif (cfg->exceptions & EX_FPM_DOUBLE_ECC)\r\ntmp_reg |= FPM_EV_MASK_DOUBLE_ECC_EN;\r\ntmp_reg |= (cfg->catastrophic_err << FPM_EV_MASK_CAT_ERR_SHIFT);\r\ntmp_reg |= (cfg->dma_err << FPM_EV_MASK_DMA_ERR_SHIFT);\r\ntmp_reg |= FPM_EV_MASK_EXTERNAL_HALT;\r\ntmp_reg |= FPM_EV_MASK_ECC_ERR_HALT;\r\niowrite32be(tmp_reg, &fpm_rg->fmfp_ee);\r\nfor (i = 0; i < FM_NUM_OF_FMAN_CTRL_EVENT_REGS; i++)\r\niowrite32be(0xFFFFFFFF, &fpm_rg->fmfp_cev[i]);\r\ntmp_reg = (FPM_RAM_MURAM_ECC | FPM_RAM_IRAM_ECC);\r\niowrite32be(tmp_reg, &fpm_rg->fm_rcr);\r\ntmp_reg = 0;\r\nif (cfg->exceptions & EX_IRAM_ECC) {\r\ntmp_reg |= FPM_IRAM_ECC_ERR_EX_EN;\r\nenable_rams_ecc(fpm_rg);\r\n}\r\nif (cfg->exceptions & EX_MURAM_ECC) {\r\ntmp_reg |= FPM_MURAM_ECC_ERR_EX_EN;\r\nenable_rams_ecc(fpm_rg);\r\n}\r\niowrite32be(tmp_reg, &fpm_rg->fm_rie);\r\n}\r\nstatic void bmi_init(struct fman_bmi_regs __iomem *bmi_rg,\r\nstruct fman_cfg *cfg)\r\n{\r\nu32 tmp_reg;\r\ntmp_reg = cfg->fifo_base_addr;\r\ntmp_reg = tmp_reg / BMI_FIFO_ALIGN;\r\ntmp_reg |= ((cfg->total_fifo_size / FMAN_BMI_FIFO_UNITS - 1) <<\r\nBMI_CFG1_FIFO_SIZE_SHIFT);\r\niowrite32be(tmp_reg, &bmi_rg->fmbm_cfg1);\r\ntmp_reg = ((cfg->total_num_of_tasks - 1) & BMI_CFG2_TASKS_MASK) <<\r\nBMI_CFG2_TASKS_SHIFT;\r\niowrite32be(tmp_reg, &bmi_rg->fmbm_cfg2);\r\ntmp_reg = 0;\r\niowrite32be(BMI_ERR_INTR_EN_LIST_RAM_ECC |\r\nBMI_ERR_INTR_EN_STORAGE_PROFILE_ECC |\r\nBMI_ERR_INTR_EN_STATISTICS_RAM_ECC |\r\nBMI_ERR_INTR_EN_DISPATCH_RAM_ECC, &bmi_rg->fmbm_ievr);\r\nif (cfg->exceptions & EX_BMI_LIST_RAM_ECC)\r\ntmp_reg |= BMI_ERR_INTR_EN_LIST_RAM_ECC;\r\nif (cfg->exceptions & EX_BMI_STORAGE_PROFILE_ECC)\r\ntmp_reg |= BMI_ERR_INTR_EN_STORAGE_PROFILE_ECC;\r\nif (cfg->exceptions & EX_BMI_STATISTICS_RAM_ECC)\r\ntmp_reg |= BMI_ERR_INTR_EN_STATISTICS_RAM_ECC;\r\nif (cfg->exceptions & EX_BMI_DISPATCH_RAM_ECC)\r\ntmp_reg |= BMI_ERR_INTR_EN_DISPATCH_RAM_ECC;\r\niowrite32be(tmp_reg, &bmi_rg->fmbm_ier);\r\n}\r\nstatic void qmi_init(struct fman_qmi_regs __iomem *qmi_rg,\r\nstruct fman_cfg *cfg)\r\n{\r\nu32 tmp_reg;\r\niowrite32be(QMI_ERR_INTR_EN_DOUBLE_ECC | QMI_ERR_INTR_EN_DEQ_FROM_DEF,\r\n&qmi_rg->fmqm_eie);\r\ntmp_reg = 0;\r\nif (cfg->exceptions & EX_QMI_DEQ_FROM_UNKNOWN_PORTID)\r\ntmp_reg |= QMI_ERR_INTR_EN_DEQ_FROM_DEF;\r\nif (cfg->exceptions & EX_QMI_DOUBLE_ECC)\r\ntmp_reg |= QMI_ERR_INTR_EN_DOUBLE_ECC;\r\niowrite32be(tmp_reg, &qmi_rg->fmqm_eien);\r\ntmp_reg = 0;\r\niowrite32be(QMI_INTR_EN_SINGLE_ECC, &qmi_rg->fmqm_ie);\r\nif (cfg->exceptions & EX_QMI_SINGLE_ECC)\r\ntmp_reg |= QMI_INTR_EN_SINGLE_ECC;\r\niowrite32be(tmp_reg, &qmi_rg->fmqm_ien);\r\n}\r\nstatic int enable(struct fman *fman, struct fman_cfg *cfg)\r\n{\r\nu32 cfg_reg = 0;\r\ncfg_reg = QMI_CFG_EN_COUNTERS;\r\ncfg_reg |= (cfg->qmi_def_tnums_thresh << 8) | cfg->qmi_def_tnums_thresh;\r\niowrite32be(BMI_INIT_START, &fman->bmi_regs->fmbm_init);\r\niowrite32be(cfg_reg | QMI_CFG_ENQ_EN | QMI_CFG_DEQ_EN,\r\n&fman->qmi_regs->fmqm_gc);\r\nreturn 0;\r\n}\r\nstatic int set_exception(struct fman *fman,\r\nenum fman_exceptions exception, bool enable)\r\n{\r\nu32 tmp;\r\nswitch (exception) {\r\ncase FMAN_EX_DMA_BUS_ERROR:\r\ntmp = ioread32be(&fman->dma_regs->fmdmmr);\r\nif (enable)\r\ntmp |= DMA_MODE_BER;\r\nelse\r\ntmp &= ~DMA_MODE_BER;\r\niowrite32be(tmp, &fman->dma_regs->fmdmmr);\r\nbreak;\r\ncase FMAN_EX_DMA_READ_ECC:\r\ncase FMAN_EX_DMA_SYSTEM_WRITE_ECC:\r\ncase FMAN_EX_DMA_FM_WRITE_ECC:\r\ntmp = ioread32be(&fman->dma_regs->fmdmmr);\r\nif (enable)\r\ntmp |= DMA_MODE_ECC;\r\nelse\r\ntmp &= ~DMA_MODE_ECC;\r\niowrite32be(tmp, &fman->dma_regs->fmdmmr);\r\nbreak;\r\ncase FMAN_EX_FPM_STALL_ON_TASKS:\r\ntmp = ioread32be(&fman->fpm_regs->fmfp_ee);\r\nif (enable)\r\ntmp |= FPM_EV_MASK_STALL_EN;\r\nelse\r\ntmp &= ~FPM_EV_MASK_STALL_EN;\r\niowrite32be(tmp, &fman->fpm_regs->fmfp_ee);\r\nbreak;\r\ncase FMAN_EX_FPM_SINGLE_ECC:\r\ntmp = ioread32be(&fman->fpm_regs->fmfp_ee);\r\nif (enable)\r\ntmp |= FPM_EV_MASK_SINGLE_ECC_EN;\r\nelse\r\ntmp &= ~FPM_EV_MASK_SINGLE_ECC_EN;\r\niowrite32be(tmp, &fman->fpm_regs->fmfp_ee);\r\nbreak;\r\ncase FMAN_EX_FPM_DOUBLE_ECC:\r\ntmp = ioread32be(&fman->fpm_regs->fmfp_ee);\r\nif (enable)\r\ntmp |= FPM_EV_MASK_DOUBLE_ECC_EN;\r\nelse\r\ntmp &= ~FPM_EV_MASK_DOUBLE_ECC_EN;\r\niowrite32be(tmp, &fman->fpm_regs->fmfp_ee);\r\nbreak;\r\ncase FMAN_EX_QMI_SINGLE_ECC:\r\ntmp = ioread32be(&fman->qmi_regs->fmqm_ien);\r\nif (enable)\r\ntmp |= QMI_INTR_EN_SINGLE_ECC;\r\nelse\r\ntmp &= ~QMI_INTR_EN_SINGLE_ECC;\r\niowrite32be(tmp, &fman->qmi_regs->fmqm_ien);\r\nbreak;\r\ncase FMAN_EX_QMI_DOUBLE_ECC:\r\ntmp = ioread32be(&fman->qmi_regs->fmqm_eien);\r\nif (enable)\r\ntmp |= QMI_ERR_INTR_EN_DOUBLE_ECC;\r\nelse\r\ntmp &= ~QMI_ERR_INTR_EN_DOUBLE_ECC;\r\niowrite32be(tmp, &fman->qmi_regs->fmqm_eien);\r\nbreak;\r\ncase FMAN_EX_QMI_DEQ_FROM_UNKNOWN_PORTID:\r\ntmp = ioread32be(&fman->qmi_regs->fmqm_eien);\r\nif (enable)\r\ntmp |= QMI_ERR_INTR_EN_DEQ_FROM_DEF;\r\nelse\r\ntmp &= ~QMI_ERR_INTR_EN_DEQ_FROM_DEF;\r\niowrite32be(tmp, &fman->qmi_regs->fmqm_eien);\r\nbreak;\r\ncase FMAN_EX_BMI_LIST_RAM_ECC:\r\ntmp = ioread32be(&fman->bmi_regs->fmbm_ier);\r\nif (enable)\r\ntmp |= BMI_ERR_INTR_EN_LIST_RAM_ECC;\r\nelse\r\ntmp &= ~BMI_ERR_INTR_EN_LIST_RAM_ECC;\r\niowrite32be(tmp, &fman->bmi_regs->fmbm_ier);\r\nbreak;\r\ncase FMAN_EX_BMI_STORAGE_PROFILE_ECC:\r\ntmp = ioread32be(&fman->bmi_regs->fmbm_ier);\r\nif (enable)\r\ntmp |= BMI_ERR_INTR_EN_STORAGE_PROFILE_ECC;\r\nelse\r\ntmp &= ~BMI_ERR_INTR_EN_STORAGE_PROFILE_ECC;\r\niowrite32be(tmp, &fman->bmi_regs->fmbm_ier);\r\nbreak;\r\ncase FMAN_EX_BMI_STATISTICS_RAM_ECC:\r\ntmp = ioread32be(&fman->bmi_regs->fmbm_ier);\r\nif (enable)\r\ntmp |= BMI_ERR_INTR_EN_STATISTICS_RAM_ECC;\r\nelse\r\ntmp &= ~BMI_ERR_INTR_EN_STATISTICS_RAM_ECC;\r\niowrite32be(tmp, &fman->bmi_regs->fmbm_ier);\r\nbreak;\r\ncase FMAN_EX_BMI_DISPATCH_RAM_ECC:\r\ntmp = ioread32be(&fman->bmi_regs->fmbm_ier);\r\nif (enable)\r\ntmp |= BMI_ERR_INTR_EN_DISPATCH_RAM_ECC;\r\nelse\r\ntmp &= ~BMI_ERR_INTR_EN_DISPATCH_RAM_ECC;\r\niowrite32be(tmp, &fman->bmi_regs->fmbm_ier);\r\nbreak;\r\ncase FMAN_EX_IRAM_ECC:\r\ntmp = ioread32be(&fman->fpm_regs->fm_rie);\r\nif (enable) {\r\nenable_rams_ecc(fman->fpm_regs);\r\ntmp |= FPM_IRAM_ECC_ERR_EX_EN;\r\n} else {\r\ndisable_rams_ecc(fman->fpm_regs);\r\ntmp &= ~FPM_IRAM_ECC_ERR_EX_EN;\r\n}\r\niowrite32be(tmp, &fman->fpm_regs->fm_rie);\r\nbreak;\r\ncase FMAN_EX_MURAM_ECC:\r\ntmp = ioread32be(&fman->fpm_regs->fm_rie);\r\nif (enable) {\r\nenable_rams_ecc(fman->fpm_regs);\r\ntmp |= FPM_MURAM_ECC_ERR_EX_EN;\r\n} else {\r\ndisable_rams_ecc(fman->fpm_regs);\r\ntmp &= ~FPM_MURAM_ECC_ERR_EX_EN;\r\n}\r\niowrite32be(tmp, &fman->fpm_regs->fm_rie);\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void resume(struct fman_fpm_regs __iomem *fpm_rg)\r\n{\r\nu32 tmp;\r\ntmp = ioread32be(&fpm_rg->fmfp_ee);\r\ntmp &= ~(FPM_EV_MASK_DOUBLE_ECC |\r\nFPM_EV_MASK_STALL | FPM_EV_MASK_SINGLE_ECC);\r\ntmp |= FPM_EV_MASK_RELEASE_FM;\r\niowrite32be(tmp, &fpm_rg->fmfp_ee);\r\n}\r\nstatic int fill_soc_specific_params(struct fman_state_struct *state)\r\n{\r\nu8 minor = state->rev_info.minor;\r\nswitch (state->rev_info.major) {\r\ncase 3:\r\nstate->bmi_max_fifo_size = 160 * 1024;\r\nstate->fm_iram_size = 64 * 1024;\r\nstate->dma_thresh_max_commq = 31;\r\nstate->dma_thresh_max_buf = 127;\r\nstate->qmi_max_num_of_tnums = 64;\r\nstate->qmi_def_tnums_thresh = 48;\r\nstate->bmi_max_num_of_tasks = 128;\r\nstate->max_num_of_open_dmas = 32;\r\nstate->fm_port_num_of_cg = 256;\r\nstate->num_of_rx_ports = 6;\r\nstate->total_fifo_size = 122 * 1024;\r\nbreak;\r\ncase 2:\r\nstate->bmi_max_fifo_size = 160 * 1024;\r\nstate->fm_iram_size = 64 * 1024;\r\nstate->dma_thresh_max_commq = 31;\r\nstate->dma_thresh_max_buf = 127;\r\nstate->qmi_max_num_of_tnums = 64;\r\nstate->qmi_def_tnums_thresh = 48;\r\nstate->bmi_max_num_of_tasks = 128;\r\nstate->max_num_of_open_dmas = 32;\r\nstate->fm_port_num_of_cg = 256;\r\nstate->num_of_rx_ports = 5;\r\nstate->total_fifo_size = 100 * 1024;\r\nbreak;\r\ncase 6:\r\nstate->dma_thresh_max_commq = 83;\r\nstate->dma_thresh_max_buf = 127;\r\nstate->qmi_max_num_of_tnums = 64;\r\nstate->qmi_def_tnums_thresh = 32;\r\nstate->fm_port_num_of_cg = 256;\r\nif (minor == 1 || minor == 4) {\r\nstate->bmi_max_fifo_size = 192 * 1024;\r\nstate->bmi_max_num_of_tasks = 64;\r\nstate->max_num_of_open_dmas = 32;\r\nstate->num_of_rx_ports = 5;\r\nif (minor == 1)\r\nstate->fm_iram_size = 32 * 1024;\r\nelse\r\nstate->fm_iram_size = 64 * 1024;\r\nstate->total_fifo_size = 156 * 1024;\r\n}\r\nelse if (minor == 0 || minor == 2 || minor == 3) {\r\nstate->bmi_max_fifo_size = 384 * 1024;\r\nstate->fm_iram_size = 64 * 1024;\r\nstate->bmi_max_num_of_tasks = 128;\r\nstate->max_num_of_open_dmas = 84;\r\nstate->num_of_rx_ports = 8;\r\nstate->total_fifo_size = 295 * 1024;\r\n} else {\r\npr_err("Unsupported FManv3 version\n");\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ndefault:\r\npr_err("Unsupported FMan version\n");\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic bool is_init_done(struct fman_cfg *cfg)\r\n{\r\nif (!cfg)\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic void free_init_resources(struct fman *fman)\r\n{\r\nif (fman->cam_offset)\r\nfman_muram_free_mem(fman->muram, fman->cam_offset,\r\nfman->cam_size);\r\nif (fman->fifo_offset)\r\nfman_muram_free_mem(fman->muram, fman->fifo_offset,\r\nfman->fifo_size);\r\n}\r\nstatic irqreturn_t bmi_err_event(struct fman *fman)\r\n{\r\nu32 event, mask, force;\r\nstruct fman_bmi_regs __iomem *bmi_rg = fman->bmi_regs;\r\nirqreturn_t ret = IRQ_NONE;\r\nevent = ioread32be(&bmi_rg->fmbm_ievr);\r\nmask = ioread32be(&bmi_rg->fmbm_ier);\r\nevent &= mask;\r\nforce = ioread32be(&bmi_rg->fmbm_ifr);\r\nif (force & event)\r\niowrite32be(force & ~event, &bmi_rg->fmbm_ifr);\r\niowrite32be(event, &bmi_rg->fmbm_ievr);\r\nif (event & BMI_ERR_INTR_EN_STORAGE_PROFILE_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_BMI_STORAGE_PROFILE_ECC);\r\nif (event & BMI_ERR_INTR_EN_LIST_RAM_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_BMI_LIST_RAM_ECC);\r\nif (event & BMI_ERR_INTR_EN_STATISTICS_RAM_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_BMI_STATISTICS_RAM_ECC);\r\nif (event & BMI_ERR_INTR_EN_DISPATCH_RAM_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_BMI_DISPATCH_RAM_ECC);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t qmi_err_event(struct fman *fman)\r\n{\r\nu32 event, mask, force;\r\nstruct fman_qmi_regs __iomem *qmi_rg = fman->qmi_regs;\r\nirqreturn_t ret = IRQ_NONE;\r\nevent = ioread32be(&qmi_rg->fmqm_eie);\r\nmask = ioread32be(&qmi_rg->fmqm_eien);\r\nevent &= mask;\r\nforce = ioread32be(&qmi_rg->fmqm_eif);\r\nif (force & event)\r\niowrite32be(force & ~event, &qmi_rg->fmqm_eif);\r\niowrite32be(event, &qmi_rg->fmqm_eie);\r\nif (event & QMI_ERR_INTR_EN_DOUBLE_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_QMI_DOUBLE_ECC);\r\nif (event & QMI_ERR_INTR_EN_DEQ_FROM_DEF)\r\nret = fman->exception_cb(fman,\r\nFMAN_EX_QMI_DEQ_FROM_UNKNOWN_PORTID);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t dma_err_event(struct fman *fman)\r\n{\r\nu32 status, mask, com_id;\r\nu8 tnum, port_id, relative_port_id;\r\nu16 liodn;\r\nstruct fman_dma_regs __iomem *dma_rg = fman->dma_regs;\r\nirqreturn_t ret = IRQ_NONE;\r\nstatus = ioread32be(&dma_rg->fmdmsr);\r\nmask = ioread32be(&dma_rg->fmdmmr);\r\nif ((mask & DMA_MODE_BER) != DMA_MODE_BER)\r\nstatus &= ~DMA_STATUS_BUS_ERR;\r\nif ((mask & DMA_MODE_ECC) != DMA_MODE_ECC)\r\nstatus &= ~(DMA_STATUS_FM_SPDAT_ECC |\r\nDMA_STATUS_READ_ECC |\r\nDMA_STATUS_SYSTEM_WRITE_ECC |\r\nDMA_STATUS_FM_WRITE_ECC);\r\niowrite32be(status, &dma_rg->fmdmsr);\r\nif (status & DMA_STATUS_BUS_ERR) {\r\nu64 addr;\r\naddr = (u64)ioread32be(&dma_rg->fmdmtal);\r\naddr |= ((u64)(ioread32be(&dma_rg->fmdmtah)) << 32);\r\ncom_id = ioread32be(&dma_rg->fmdmtcid);\r\nport_id = (u8)(((com_id & DMA_TRANSFER_PORTID_MASK) >>\r\nDMA_TRANSFER_PORTID_SHIFT));\r\nrelative_port_id =\r\nhw_port_id_to_sw_port_id(fman->state->rev_info.major, port_id);\r\ntnum = (u8)((com_id & DMA_TRANSFER_TNUM_MASK) >>\r\nDMA_TRANSFER_TNUM_SHIFT);\r\nliodn = (u16)(com_id & DMA_TRANSFER_LIODN_MASK);\r\nret = fman->bus_error_cb(fman, relative_port_id, addr, tnum,\r\nliodn);\r\n}\r\nif (status & DMA_STATUS_FM_SPDAT_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_DMA_SINGLE_PORT_ECC);\r\nif (status & DMA_STATUS_READ_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_DMA_READ_ECC);\r\nif (status & DMA_STATUS_SYSTEM_WRITE_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_DMA_SYSTEM_WRITE_ECC);\r\nif (status & DMA_STATUS_FM_WRITE_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_DMA_FM_WRITE_ECC);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t fpm_err_event(struct fman *fman)\r\n{\r\nu32 event;\r\nstruct fman_fpm_regs __iomem *fpm_rg = fman->fpm_regs;\r\nirqreturn_t ret = IRQ_NONE;\r\nevent = ioread32be(&fpm_rg->fmfp_ee);\r\niowrite32be(event, &fpm_rg->fmfp_ee);\r\nif ((event & FPM_EV_MASK_DOUBLE_ECC) &&\r\n(event & FPM_EV_MASK_DOUBLE_ECC_EN))\r\nret = fman->exception_cb(fman, FMAN_EX_FPM_DOUBLE_ECC);\r\nif ((event & FPM_EV_MASK_STALL) && (event & FPM_EV_MASK_STALL_EN))\r\nret = fman->exception_cb(fman, FMAN_EX_FPM_STALL_ON_TASKS);\r\nif ((event & FPM_EV_MASK_SINGLE_ECC) &&\r\n(event & FPM_EV_MASK_SINGLE_ECC_EN))\r\nret = fman->exception_cb(fman, FMAN_EX_FPM_SINGLE_ECC);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t muram_err_intr(struct fman *fman)\r\n{\r\nu32 event, mask;\r\nstruct fman_fpm_regs __iomem *fpm_rg = fman->fpm_regs;\r\nirqreturn_t ret = IRQ_NONE;\r\nevent = ioread32be(&fpm_rg->fm_rcr);\r\nmask = ioread32be(&fpm_rg->fm_rie);\r\niowrite32be(event & ~FPM_RAM_IRAM_ECC, &fpm_rg->fm_rcr);\r\nif ((mask & FPM_MURAM_ECC_ERR_EX_EN) && (event & FPM_RAM_MURAM_ECC))\r\nret = fman->exception_cb(fman, FMAN_EX_MURAM_ECC);\r\nreturn ret;\r\n}\r\nstatic irqreturn_t qmi_event(struct fman *fman)\r\n{\r\nu32 event, mask, force;\r\nstruct fman_qmi_regs __iomem *qmi_rg = fman->qmi_regs;\r\nirqreturn_t ret = IRQ_NONE;\r\nevent = ioread32be(&qmi_rg->fmqm_ie);\r\nmask = ioread32be(&qmi_rg->fmqm_ien);\r\nevent &= mask;\r\nforce = ioread32be(&qmi_rg->fmqm_if);\r\nif (force & event)\r\niowrite32be(force & ~event, &qmi_rg->fmqm_if);\r\niowrite32be(event, &qmi_rg->fmqm_ie);\r\nif (event & QMI_INTR_EN_SINGLE_ECC)\r\nret = fman->exception_cb(fman, FMAN_EX_QMI_SINGLE_ECC);\r\nreturn ret;\r\n}\r\nstatic void enable_time_stamp(struct fman *fman)\r\n{\r\nstruct fman_fpm_regs __iomem *fpm_rg = fman->fpm_regs;\r\nu16 fm_clk_freq = fman->state->fm_clk_freq;\r\nu32 tmp, intgr, ts_freq;\r\nu64 frac;\r\nts_freq = (u32)(1 << fman->state->count1_micro_bit);\r\nintgr = ts_freq / fm_clk_freq;\r\nfrac = ((ts_freq << 16) - (intgr << 16) * fm_clk_freq) / fm_clk_freq;\r\nif (((ts_freq << 16) - (intgr << 16) * fm_clk_freq) % fm_clk_freq)\r\nfrac++;\r\ntmp = (intgr << FPM_TS_INT_SHIFT) | (u16)frac;\r\niowrite32be(tmp, &fpm_rg->fmfp_tsc2);\r\niowrite32be(FPM_TS_CTL_EN, &fpm_rg->fmfp_tsc1);\r\nfman->state->enabled_time_stamp = true;\r\n}\r\nstatic int clear_iram(struct fman *fman)\r\n{\r\nstruct fman_iram_regs __iomem *iram;\r\nint i, count;\r\niram = fman->base_addr + IMEM_OFFSET;\r\niowrite32be(IRAM_IADD_AIE, &iram->iadd);\r\ncount = 100;\r\ndo {\r\nudelay(1);\r\n} while ((ioread32be(&iram->iadd) != IRAM_IADD_AIE) && --count);\r\nif (count == 0)\r\nreturn -EBUSY;\r\nfor (i = 0; i < (fman->state->fm_iram_size / 4); i++)\r\niowrite32be(0xffffffff, &iram->idata);\r\niowrite32be(fman->state->fm_iram_size - 4, &iram->iadd);\r\ncount = 100;\r\ndo {\r\nudelay(1);\r\n} while ((ioread32be(&iram->idata) != 0xffffffff) && --count);\r\nif (count == 0)\r\nreturn -EBUSY;\r\nreturn 0;\r\n}\r\nstatic u32 get_exception_flag(enum fman_exceptions exception)\r\n{\r\nu32 bit_mask;\r\nswitch (exception) {\r\ncase FMAN_EX_DMA_BUS_ERROR:\r\nbit_mask = EX_DMA_BUS_ERROR;\r\nbreak;\r\ncase FMAN_EX_DMA_SINGLE_PORT_ECC:\r\nbit_mask = EX_DMA_SINGLE_PORT_ECC;\r\nbreak;\r\ncase FMAN_EX_DMA_READ_ECC:\r\nbit_mask = EX_DMA_READ_ECC;\r\nbreak;\r\ncase FMAN_EX_DMA_SYSTEM_WRITE_ECC:\r\nbit_mask = EX_DMA_SYSTEM_WRITE_ECC;\r\nbreak;\r\ncase FMAN_EX_DMA_FM_WRITE_ECC:\r\nbit_mask = EX_DMA_FM_WRITE_ECC;\r\nbreak;\r\ncase FMAN_EX_FPM_STALL_ON_TASKS:\r\nbit_mask = EX_FPM_STALL_ON_TASKS;\r\nbreak;\r\ncase FMAN_EX_FPM_SINGLE_ECC:\r\nbit_mask = EX_FPM_SINGLE_ECC;\r\nbreak;\r\ncase FMAN_EX_FPM_DOUBLE_ECC:\r\nbit_mask = EX_FPM_DOUBLE_ECC;\r\nbreak;\r\ncase FMAN_EX_QMI_SINGLE_ECC:\r\nbit_mask = EX_QMI_SINGLE_ECC;\r\nbreak;\r\ncase FMAN_EX_QMI_DOUBLE_ECC:\r\nbit_mask = EX_QMI_DOUBLE_ECC;\r\nbreak;\r\ncase FMAN_EX_QMI_DEQ_FROM_UNKNOWN_PORTID:\r\nbit_mask = EX_QMI_DEQ_FROM_UNKNOWN_PORTID;\r\nbreak;\r\ncase FMAN_EX_BMI_LIST_RAM_ECC:\r\nbit_mask = EX_BMI_LIST_RAM_ECC;\r\nbreak;\r\ncase FMAN_EX_BMI_STORAGE_PROFILE_ECC:\r\nbit_mask = EX_BMI_STORAGE_PROFILE_ECC;\r\nbreak;\r\ncase FMAN_EX_BMI_STATISTICS_RAM_ECC:\r\nbit_mask = EX_BMI_STATISTICS_RAM_ECC;\r\nbreak;\r\ncase FMAN_EX_BMI_DISPATCH_RAM_ECC:\r\nbit_mask = EX_BMI_DISPATCH_RAM_ECC;\r\nbreak;\r\ncase FMAN_EX_MURAM_ECC:\r\nbit_mask = EX_MURAM_ECC;\r\nbreak;\r\ndefault:\r\nbit_mask = 0;\r\nbreak;\r\n}\r\nreturn bit_mask;\r\n}\r\nstatic int get_module_event(enum fman_event_modules module, u8 mod_id,\r\nenum fman_intr_type intr_type)\r\n{\r\nint event;\r\nswitch (module) {\r\ncase FMAN_MOD_MAC:\r\nif (intr_type == FMAN_INTR_TYPE_ERR)\r\nevent = FMAN_EV_ERR_MAC0 + mod_id;\r\nelse\r\nevent = FMAN_EV_MAC0 + mod_id;\r\nbreak;\r\ncase FMAN_MOD_FMAN_CTRL:\r\nif (intr_type == FMAN_INTR_TYPE_ERR)\r\nevent = FMAN_EV_CNT;\r\nelse\r\nevent = (FMAN_EV_FMAN_CTRL_0 + mod_id);\r\nbreak;\r\ncase FMAN_MOD_DUMMY_LAST:\r\nevent = FMAN_EV_CNT;\r\nbreak;\r\ndefault:\r\nevent = FMAN_EV_CNT;\r\nbreak;\r\n}\r\nreturn event;\r\n}\r\nstatic int set_size_of_fifo(struct fman *fman, u8 port_id, u32 *size_of_fifo,\r\nu32 *extra_size_of_fifo)\r\n{\r\nstruct fman_bmi_regs __iomem *bmi_rg = fman->bmi_regs;\r\nu32 fifo = *size_of_fifo;\r\nu32 extra_fifo = *extra_size_of_fifo;\r\nu32 tmp;\r\nif (extra_fifo && !fman->state->extra_fifo_pool_size)\r\nfman->state->extra_fifo_pool_size =\r\nfman->state->num_of_rx_ports * FMAN_BMI_FIFO_UNITS;\r\nfman->state->extra_fifo_pool_size =\r\nmax(fman->state->extra_fifo_pool_size, extra_fifo);\r\nif ((fman->state->accumulated_fifo_size + fifo) >\r\n(fman->state->total_fifo_size -\r\nfman->state->extra_fifo_pool_size)) {\r\ndev_err(fman->dev, "%s: Requested fifo size and extra size exceed total FIFO size.\n",\r\n__func__);\r\nreturn -EAGAIN;\r\n}\r\ntmp = (fifo / FMAN_BMI_FIFO_UNITS - 1) |\r\n((extra_fifo / FMAN_BMI_FIFO_UNITS) <<\r\nBMI_EXTRA_FIFO_SIZE_SHIFT);\r\niowrite32be(tmp, &bmi_rg->fmbm_pfs[port_id - 1]);\r\nfman->state->accumulated_fifo_size += fifo;\r\nreturn 0;\r\n}\r\nstatic int set_num_of_tasks(struct fman *fman, u8 port_id, u8 *num_of_tasks,\r\nu8 *num_of_extra_tasks)\r\n{\r\nstruct fman_bmi_regs __iomem *bmi_rg = fman->bmi_regs;\r\nu8 tasks = *num_of_tasks;\r\nu8 extra_tasks = *num_of_extra_tasks;\r\nu32 tmp;\r\nif (extra_tasks)\r\nfman->state->extra_tasks_pool_size =\r\nmax(fman->state->extra_tasks_pool_size, extra_tasks);\r\nif ((fman->state->accumulated_num_of_tasks + tasks) >\r\n(fman->state->total_num_of_tasks -\r\nfman->state->extra_tasks_pool_size)) {\r\ndev_err(fman->dev, "%s: Requested num_of_tasks and extra tasks pool for fm%d exceed total num_of_tasks.\n",\r\n__func__, fman->state->fm_id);\r\nreturn -EAGAIN;\r\n}\r\nfman->state->accumulated_num_of_tasks += tasks;\r\ntmp = ioread32be(&bmi_rg->fmbm_pp[port_id - 1]) &\r\n~(BMI_NUM_OF_TASKS_MASK | BMI_NUM_OF_EXTRA_TASKS_MASK);\r\ntmp |= ((u32)((tasks - 1) << BMI_NUM_OF_TASKS_SHIFT) |\r\n(u32)(extra_tasks << BMI_EXTRA_NUM_OF_TASKS_SHIFT));\r\niowrite32be(tmp, &bmi_rg->fmbm_pp[port_id - 1]);\r\nreturn 0;\r\n}\r\nstatic int set_num_of_open_dmas(struct fman *fman, u8 port_id,\r\nu8 *num_of_open_dmas,\r\nu8 *num_of_extra_open_dmas)\r\n{\r\nstruct fman_bmi_regs __iomem *bmi_rg = fman->bmi_regs;\r\nu8 open_dmas = *num_of_open_dmas;\r\nu8 extra_open_dmas = *num_of_extra_open_dmas;\r\nu8 total_num_dmas = 0, current_val = 0, current_extra_val = 0;\r\nu32 tmp;\r\nif (!open_dmas) {\r\ntmp = ioread32be(&bmi_rg->fmbm_pp[port_id - 1]);\r\ncurrent_extra_val = (u8)((tmp & BMI_NUM_OF_EXTRA_DMAS_MASK) >>\r\nBMI_EXTRA_NUM_OF_DMAS_SHIFT);\r\ntmp = ioread32be(&bmi_rg->fmbm_pp[port_id - 1]);\r\ncurrent_val = (u8)(((tmp & BMI_NUM_OF_DMAS_MASK) >>\r\nBMI_NUM_OF_DMAS_SHIFT) + 1);\r\nfman->state->extra_open_dmas_pool_size =\r\n(u8)max(fman->state->extra_open_dmas_pool_size,\r\ncurrent_extra_val);\r\nfman->state->accumulated_num_of_open_dmas += current_val;\r\n*num_of_open_dmas = current_val;\r\n*num_of_extra_open_dmas = current_extra_val;\r\nreturn 0;\r\n}\r\nif (extra_open_dmas > current_extra_val)\r\nfman->state->extra_open_dmas_pool_size =\r\n(u8)max(fman->state->extra_open_dmas_pool_size,\r\nextra_open_dmas);\r\nif ((fman->state->rev_info.major < 6) &&\r\n(fman->state->accumulated_num_of_open_dmas - current_val +\r\nopen_dmas > fman->state->max_num_of_open_dmas)) {\r\ndev_err(fman->dev, "%s: Requested num_of_open_dmas for fm%d exceeds total num_of_open_dmas.\n",\r\n__func__, fman->state->fm_id);\r\nreturn -EAGAIN;\r\n} else if ((fman->state->rev_info.major >= 6) &&\r\n!((fman->state->rev_info.major == 6) &&\r\n(fman->state->rev_info.minor == 0)) &&\r\n(fman->state->accumulated_num_of_open_dmas -\r\ncurrent_val + open_dmas >\r\nfman->state->dma_thresh_max_commq + 1)) {\r\ndev_err(fman->dev, "%s: Requested num_of_open_dmas for fm%d exceeds DMA Command queue (%d)\n",\r\n__func__, fman->state->fm_id,\r\nfman->state->dma_thresh_max_commq + 1);\r\nreturn -EAGAIN;\r\n}\r\nWARN_ON(fman->state->accumulated_num_of_open_dmas < current_val);\r\nfman->state->accumulated_num_of_open_dmas -= current_val;\r\nfman->state->accumulated_num_of_open_dmas += open_dmas;\r\nif (fman->state->rev_info.major < 6)\r\ntotal_num_dmas =\r\n(u8)(fman->state->accumulated_num_of_open_dmas +\r\nfman->state->extra_open_dmas_pool_size);\r\ntmp = ioread32be(&bmi_rg->fmbm_pp[port_id - 1]) &\r\n~(BMI_NUM_OF_DMAS_MASK | BMI_NUM_OF_EXTRA_DMAS_MASK);\r\ntmp |= (u32)(((open_dmas - 1) << BMI_NUM_OF_DMAS_SHIFT) |\r\n(extra_open_dmas << BMI_EXTRA_NUM_OF_DMAS_SHIFT));\r\niowrite32be(tmp, &bmi_rg->fmbm_pp[port_id - 1]);\r\nif (total_num_dmas) {\r\ntmp = ioread32be(&bmi_rg->fmbm_cfg2) & ~BMI_CFG2_DMAS_MASK;\r\ntmp |= (u32)(total_num_dmas - 1) << BMI_CFG2_DMAS_SHIFT;\r\niowrite32be(tmp, &bmi_rg->fmbm_cfg2);\r\n}\r\nreturn 0;\r\n}\r\nstatic int fman_config(struct fman *fman)\r\n{\r\nvoid __iomem *base_addr;\r\nint err;\r\nbase_addr = fman->dts_params.base_addr;\r\nfman->state = kzalloc(sizeof(*fman->state), GFP_KERNEL);\r\nif (!fman->state)\r\ngoto err_fm_state;\r\nfman->cfg = kzalloc(sizeof(*fman->cfg), GFP_KERNEL);\r\nif (!fman->cfg)\r\ngoto err_fm_drv;\r\nfman->muram =\r\nfman_muram_init(fman->dts_params.muram_res.start,\r\nresource_size(&fman->dts_params.muram_res));\r\nif (!fman->muram)\r\ngoto err_fm_soc_specific;\r\nfman->state->fm_id = fman->dts_params.id;\r\nfman->state->fm_clk_freq = fman->dts_params.clk_freq;\r\nfman->state->qman_channel_base = fman->dts_params.qman_channel_base;\r\nfman->state->num_of_qman_channels =\r\nfman->dts_params.num_of_qman_channels;\r\nfman->state->res = fman->dts_params.res;\r\nfman->exception_cb = fman_exceptions;\r\nfman->bus_error_cb = fman_bus_error;\r\nfman->fpm_regs = base_addr + FPM_OFFSET;\r\nfman->bmi_regs = base_addr + BMI_OFFSET;\r\nfman->qmi_regs = base_addr + QMI_OFFSET;\r\nfman->dma_regs = base_addr + DMA_OFFSET;\r\nfman->base_addr = base_addr;\r\nspin_lock_init(&fman->spinlock);\r\nfman_defconfig(fman->cfg);\r\nfman->state->extra_fifo_pool_size = 0;\r\nfman->state->exceptions = (EX_DMA_BUS_ERROR |\r\nEX_DMA_READ_ECC |\r\nEX_DMA_SYSTEM_WRITE_ECC |\r\nEX_DMA_FM_WRITE_ECC |\r\nEX_FPM_STALL_ON_TASKS |\r\nEX_FPM_SINGLE_ECC |\r\nEX_FPM_DOUBLE_ECC |\r\nEX_QMI_DEQ_FROM_UNKNOWN_PORTID |\r\nEX_BMI_LIST_RAM_ECC |\r\nEX_BMI_STORAGE_PROFILE_ECC |\r\nEX_BMI_STATISTICS_RAM_ECC |\r\nEX_MURAM_ECC |\r\nEX_BMI_DISPATCH_RAM_ECC |\r\nEX_QMI_DOUBLE_ECC |\r\nEX_QMI_SINGLE_ECC);\r\nfman_get_revision(fman, &fman->state->rev_info);\r\nerr = fill_soc_specific_params(fman->state);\r\nif (err)\r\ngoto err_fm_soc_specific;\r\nif (fman->state->rev_info.major >= 6)\r\nfman->cfg->dma_aid_mode = FMAN_DMA_AID_OUT_PORT_ID;\r\nfman->cfg->qmi_def_tnums_thresh = fman->state->qmi_def_tnums_thresh;\r\nfman->state->total_num_of_tasks =\r\n(u8)DFLT_TOTAL_NUM_OF_TASKS(fman->state->rev_info.major,\r\nfman->state->rev_info.minor,\r\nfman->state->bmi_max_num_of_tasks);\r\nif (fman->state->rev_info.major < 6) {\r\nfman->cfg->dma_comm_qtsh_clr_emer =\r\n(u8)DFLT_DMA_COMM_Q_LOW(fman->state->rev_info.major,\r\nfman->state->dma_thresh_max_commq);\r\nfman->cfg->dma_comm_qtsh_asrt_emer =\r\n(u8)DFLT_DMA_COMM_Q_HIGH(fman->state->rev_info.major,\r\nfman->state->dma_thresh_max_commq);\r\nfman->cfg->dma_cam_num_of_entries =\r\nDFLT_DMA_CAM_NUM_OF_ENTRIES(fman->state->rev_info.major);\r\nfman->cfg->dma_read_buf_tsh_clr_emer =\r\nDFLT_DMA_READ_INT_BUF_LOW(fman->state->dma_thresh_max_buf);\r\nfman->cfg->dma_read_buf_tsh_asrt_emer =\r\nDFLT_DMA_READ_INT_BUF_HIGH(fman->state->dma_thresh_max_buf);\r\nfman->cfg->dma_write_buf_tsh_clr_emer =\r\nDFLT_DMA_WRITE_INT_BUF_LOW(fman->state->dma_thresh_max_buf);\r\nfman->cfg->dma_write_buf_tsh_asrt_emer =\r\nDFLT_DMA_WRITE_INT_BUF_HIGH(fman->state->dma_thresh_max_buf);\r\nfman->cfg->dma_axi_dbg_num_of_beats =\r\nDFLT_AXI_DBG_NUM_OF_BEATS;\r\n}\r\nreturn 0;\r\nerr_fm_soc_specific:\r\nkfree(fman->cfg);\r\nerr_fm_drv:\r\nkfree(fman->state);\r\nerr_fm_state:\r\nkfree(fman);\r\nreturn -EINVAL;\r\n}\r\nstatic int fman_reset(struct fman *fman)\r\n{\r\nu32 count;\r\nint err = 0;\r\nif (fman->state->rev_info.major < 6) {\r\niowrite32be(FPM_RSTC_FM_RESET, &fman->fpm_regs->fm_rstc);\r\ncount = 100;\r\ndo {\r\nudelay(1);\r\n} while (((ioread32be(&fman->fpm_regs->fm_rstc)) &\r\nFPM_RSTC_FM_RESET) && --count);\r\nif (count == 0)\r\nerr = -EBUSY;\r\ngoto _return;\r\n} else {\r\nstruct device_node *guts_node;\r\nstruct ccsr_guts __iomem *guts_regs;\r\nu32 devdisr2, reg;\r\nguts_node =\r\nof_find_compatible_node(NULL, NULL,\r\n"fsl,qoriq-device-config-2.0");\r\nif (!guts_node) {\r\ndev_err(fman->dev, "%s: Couldn't find guts node\n",\r\n__func__);\r\ngoto guts_node;\r\n}\r\nguts_regs = of_iomap(guts_node, 0);\r\nif (!guts_regs) {\r\ndev_err(fman->dev, "%s: Couldn't map %s regs\n",\r\n__func__, guts_node->full_name);\r\ngoto guts_regs;\r\n}\r\n#define FMAN1_ALL_MACS_MASK 0xFCC00000\r\n#define FMAN2_ALL_MACS_MASK 0x000FCC00\r\ndevdisr2 = ioread32be(&guts_regs->devdisr2);\r\nif (fman->dts_params.id == 0)\r\nreg = devdisr2 & ~FMAN1_ALL_MACS_MASK;\r\nelse\r\nreg = devdisr2 & ~FMAN2_ALL_MACS_MASK;\r\niowrite32be(reg, &guts_regs->devdisr2);\r\niowrite32be(FPM_RSTC_FM_RESET, &fman->fpm_regs->fm_rstc);\r\ncount = 100;\r\ndo {\r\nudelay(1);\r\n} while (((ioread32be(&fman->fpm_regs->fm_rstc)) &\r\nFPM_RSTC_FM_RESET) && --count);\r\nif (count == 0) {\r\niounmap(guts_regs);\r\nof_node_put(guts_node);\r\nerr = -EBUSY;\r\ngoto _return;\r\n}\r\niowrite32be(devdisr2, &guts_regs->devdisr2);\r\niounmap(guts_regs);\r\nof_node_put(guts_node);\r\ngoto _return;\r\nguts_regs:\r\nof_node_put(guts_node);\r\nguts_node:\r\ndev_dbg(fman->dev, "%s: Didn't perform FManV3 reset due to Errata A007273!\n",\r\n__func__);\r\n}\r\n_return:\r\nreturn err;\r\n}\r\nstatic int fman_init(struct fman *fman)\r\n{\r\nstruct fman_cfg *cfg = NULL;\r\nint err = 0, i, count;\r\nif (is_init_done(fman->cfg))\r\nreturn -EINVAL;\r\nfman->state->count1_micro_bit = FM_TIMESTAMP_1_USEC_BIT;\r\ncfg = fman->cfg;\r\nif (fman->state->rev_info.major < 6)\r\nfman->state->exceptions &= ~FMAN_EX_BMI_DISPATCH_RAM_ECC;\r\nif (fman->state->rev_info.major >= 6)\r\nfman->state->exceptions &= ~FMAN_EX_QMI_SINGLE_ECC;\r\nmemset_io((void __iomem *)(fman->base_addr + CGP_OFFSET), 0,\r\nfman->state->fm_port_num_of_cg);\r\nfor (i = 1; i < FMAN_LIODN_TBL; i++) {\r\nu32 liodn_base;\r\nfman->liodn_offset[i] =\r\nioread32be(&fman->bmi_regs->fmbm_spliodn[i - 1]);\r\nliodn_base = ioread32be(&fman->dma_regs->fmdmplr[i / 2]);\r\nif (i % 2) {\r\nliodn_base &= DMA_LIODN_BASE_MASK;\r\n} else {\r\nliodn_base >>= DMA_LIODN_SHIFT;\r\nliodn_base &= DMA_LIODN_BASE_MASK;\r\n}\r\nfman->liodn_base[i] = liodn_base;\r\n}\r\nerr = fman_reset(fman);\r\nif (err)\r\nreturn err;\r\nif (ioread32be(&fman->qmi_regs->fmqm_gs) & QMI_GS_HALT_NOT_BUSY) {\r\nresume(fman->fpm_regs);\r\ncount = 100;\r\ndo {\r\nudelay(1);\r\n} while (((ioread32be(&fman->qmi_regs->fmqm_gs)) &\r\nQMI_GS_HALT_NOT_BUSY) && --count);\r\nif (count == 0)\r\ndev_warn(fman->dev, "%s: QMI is in halt not busy state\n",\r\n__func__);\r\n}\r\nif (clear_iram(fman) != 0)\r\nreturn -EINVAL;\r\ncfg->exceptions = fman->state->exceptions;\r\nerr = dma_init(fman);\r\nif (err != 0) {\r\nfree_init_resources(fman);\r\nreturn err;\r\n}\r\nfpm_init(fman->fpm_regs, fman->cfg);\r\nfman->fifo_offset = fman_muram_alloc(fman->muram,\r\nfman->state->total_fifo_size);\r\nif (IS_ERR_VALUE(fman->cam_offset)) {\r\nfree_init_resources(fman);\r\ndev_err(fman->dev, "%s: MURAM alloc for BMI FIFO failed\n",\r\n__func__);\r\nreturn -ENOMEM;\r\n}\r\ncfg->fifo_base_addr = fman->fifo_offset;\r\ncfg->total_fifo_size = fman->state->total_fifo_size;\r\ncfg->total_num_of_tasks = fman->state->total_num_of_tasks;\r\ncfg->clk_freq = fman->state->fm_clk_freq;\r\nbmi_init(fman->bmi_regs, fman->cfg);\r\nqmi_init(fman->qmi_regs, fman->cfg);\r\nerr = enable(fman, cfg);\r\nif (err != 0)\r\nreturn err;\r\nenable_time_stamp(fman);\r\nkfree(fman->cfg);\r\nfman->cfg = NULL;\r\nreturn 0;\r\n}\r\nstatic int fman_set_exception(struct fman *fman,\r\nenum fman_exceptions exception, bool enable)\r\n{\r\nu32 bit_mask = 0;\r\nif (!is_init_done(fman->cfg))\r\nreturn -EINVAL;\r\nbit_mask = get_exception_flag(exception);\r\nif (bit_mask) {\r\nif (enable)\r\nfman->state->exceptions |= bit_mask;\r\nelse\r\nfman->state->exceptions &= ~bit_mask;\r\n} else {\r\ndev_err(fman->dev, "%s: Undefined exception (%d)\n",\r\n__func__, exception);\r\nreturn -EINVAL;\r\n}\r\nreturn set_exception(fman, exception, enable);\r\n}\r\nvoid fman_register_intr(struct fman *fman, enum fman_event_modules module,\r\nu8 mod_id, enum fman_intr_type intr_type,\r\nvoid (*isr_cb)(void *src_arg), void *src_arg)\r\n{\r\nint event = 0;\r\nevent = get_module_event(module, mod_id, intr_type);\r\nWARN_ON(event >= FMAN_EV_CNT);\r\nfman->intr_mng[event].isr_cb = isr_cb;\r\nfman->intr_mng[event].src_handle = src_arg;\r\n}\r\nvoid fman_unregister_intr(struct fman *fman, enum fman_event_modules module,\r\nu8 mod_id, enum fman_intr_type intr_type)\r\n{\r\nint event = 0;\r\nevent = get_module_event(module, mod_id, intr_type);\r\nWARN_ON(event >= FMAN_EV_CNT);\r\nfman->intr_mng[event].isr_cb = NULL;\r\nfman->intr_mng[event].src_handle = NULL;\r\n}\r\nint fman_set_port_params(struct fman *fman,\r\nstruct fman_port_init_params *port_params)\r\n{\r\nint err;\r\nunsigned long flags;\r\nu8 port_id = port_params->port_id, mac_id;\r\nspin_lock_irqsave(&fman->spinlock, flags);\r\nerr = set_num_of_tasks(fman, port_params->port_id,\r\n&port_params->num_of_tasks,\r\n&port_params->num_of_extra_tasks);\r\nif (err)\r\ngoto return_err;\r\nif (port_params->port_type != FMAN_PORT_TYPE_RX) {\r\nu32 enq_th, deq_th, reg;\r\nfman->state->accumulated_num_of_deq_tnums +=\r\nport_params->deq_pipeline_depth;\r\nenq_th = (ioread32be(&fman->qmi_regs->fmqm_gc) &\r\nQMI_CFG_ENQ_MASK) >> QMI_CFG_ENQ_SHIFT;\r\nif (enq_th >= (fman->state->qmi_max_num_of_tnums -\r\nfman->state->accumulated_num_of_deq_tnums)) {\r\nenq_th =\r\nfman->state->qmi_max_num_of_tnums -\r\nfman->state->accumulated_num_of_deq_tnums - 1;\r\nreg = ioread32be(&fman->qmi_regs->fmqm_gc);\r\nreg &= ~QMI_CFG_ENQ_MASK;\r\nreg |= (enq_th << QMI_CFG_ENQ_SHIFT);\r\niowrite32be(reg, &fman->qmi_regs->fmqm_gc);\r\n}\r\ndeq_th = ioread32be(&fman->qmi_regs->fmqm_gc) &\r\nQMI_CFG_DEQ_MASK;\r\nif ((deq_th <= fman->state->accumulated_num_of_deq_tnums) &&\r\n(deq_th < fman->state->qmi_max_num_of_tnums - 1)) {\r\ndeq_th = fman->state->accumulated_num_of_deq_tnums + 1;\r\nreg = ioread32be(&fman->qmi_regs->fmqm_gc);\r\nreg &= ~QMI_CFG_DEQ_MASK;\r\nreg |= deq_th;\r\niowrite32be(reg, &fman->qmi_regs->fmqm_gc);\r\n}\r\n}\r\nerr = set_size_of_fifo(fman, port_params->port_id,\r\n&port_params->size_of_fifo,\r\n&port_params->extra_size_of_fifo);\r\nif (err)\r\ngoto return_err;\r\nerr = set_num_of_open_dmas(fman, port_params->port_id,\r\n&port_params->num_of_open_dmas,\r\n&port_params->num_of_extra_open_dmas);\r\nif (err)\r\ngoto return_err;\r\nset_port_liodn(fman, port_id, fman->liodn_base[port_id],\r\nfman->liodn_offset[port_id]);\r\nif (fman->state->rev_info.major < 6)\r\nset_port_order_restoration(fman->fpm_regs, port_id);\r\nmac_id = hw_port_id_to_sw_port_id(fman->state->rev_info.major, port_id);\r\nif (port_params->max_frame_length >= fman->state->mac_mfl[mac_id]) {\r\nfman->state->port_mfl[mac_id] = port_params->max_frame_length;\r\n} else {\r\ndev_warn(fman->dev, "%s: Port (%d) max_frame_length is smaller than MAC (%d) current MTU\n",\r\n__func__, port_id, mac_id);\r\nerr = -EINVAL;\r\ngoto return_err;\r\n}\r\nspin_unlock_irqrestore(&fman->spinlock, flags);\r\nreturn 0;\r\nreturn_err:\r\nspin_unlock_irqrestore(&fman->spinlock, flags);\r\nreturn err;\r\n}\r\nint fman_reset_mac(struct fman *fman, u8 mac_id)\r\n{\r\nstruct fman_fpm_regs __iomem *fpm_rg = fman->fpm_regs;\r\nu32 msk, timeout = 100;\r\nif (fman->state->rev_info.major >= 6) {\r\ndev_err(fman->dev, "%s: FMan MAC reset no available for FMan V3!\n",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nswitch (mac_id) {\r\ncase 0:\r\nmsk = FPM_RSTC_MAC0_RESET;\r\nbreak;\r\ncase 1:\r\nmsk = FPM_RSTC_MAC1_RESET;\r\nbreak;\r\ncase 2:\r\nmsk = FPM_RSTC_MAC2_RESET;\r\nbreak;\r\ncase 3:\r\nmsk = FPM_RSTC_MAC3_RESET;\r\nbreak;\r\ncase 4:\r\nmsk = FPM_RSTC_MAC4_RESET;\r\nbreak;\r\ncase 5:\r\nmsk = FPM_RSTC_MAC5_RESET;\r\nbreak;\r\ncase 6:\r\nmsk = FPM_RSTC_MAC6_RESET;\r\nbreak;\r\ncase 7:\r\nmsk = FPM_RSTC_MAC7_RESET;\r\nbreak;\r\ncase 8:\r\nmsk = FPM_RSTC_MAC8_RESET;\r\nbreak;\r\ncase 9:\r\nmsk = FPM_RSTC_MAC9_RESET;\r\nbreak;\r\ndefault:\r\ndev_warn(fman->dev, "%s: Illegal MAC Id [%d]\n",\r\n__func__, mac_id);\r\nreturn -EINVAL;\r\n}\r\niowrite32be(msk, &fpm_rg->fm_rstc);\r\nwhile ((ioread32be(&fpm_rg->fm_rstc) & msk) && --timeout)\r\nudelay(10);\r\nif (!timeout)\r\nreturn -EIO;\r\nreturn 0;\r\n}\r\nint fman_set_mac_max_frame(struct fman *fman, u8 mac_id, u16 mfl)\r\n{\r\nif ((!fman->state->port_mfl[mac_id]) ||\r\n(fman->state->port_mfl[mac_id] &&\r\n(mfl <= fman->state->port_mfl[mac_id]))) {\r\nfman->state->mac_mfl[mac_id] = mfl;\r\n} else {\r\ndev_warn(fman->dev, "%s: MAC max_frame_length is larger than Port max_frame_length\n",\r\n__func__);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nu16 fman_get_clock_freq(struct fman *fman)\r\n{\r\nreturn fman->state->fm_clk_freq;\r\n}\r\nu32 fman_get_bmi_max_fifo_size(struct fman *fman)\r\n{\r\nreturn fman->state->bmi_max_fifo_size;\r\n}\r\nvoid fman_get_revision(struct fman *fman, struct fman_rev_info *rev_info)\r\n{\r\nu32 tmp;\r\ntmp = ioread32be(&fman->fpm_regs->fm_ip_rev_1);\r\nrev_info->major = (u8)((tmp & FPM_REV1_MAJOR_MASK) >>\r\nFPM_REV1_MAJOR_SHIFT);\r\nrev_info->minor = tmp & FPM_REV1_MINOR_MASK;\r\n}\r\nu32 fman_get_qman_channel_id(struct fman *fman, u32 port_id)\r\n{\r\nint i;\r\nif (fman->state->rev_info.major >= 6) {\r\nu32 port_ids[] = {0x30, 0x31, 0x28, 0x29, 0x2a, 0x2b,\r\n0x2c, 0x2d, 0x2, 0x3, 0x4, 0x5, 0x7, 0x7};\r\nfor (i = 0; i < fman->state->num_of_qman_channels; i++) {\r\nif (port_ids[i] == port_id)\r\nbreak;\r\n}\r\n} else {\r\nu32 port_ids[] = {0x30, 0x28, 0x29, 0x2a, 0x2b, 0x2c, 0x1,\r\n0x2, 0x3, 0x4, 0x5, 0x7, 0x7};\r\nfor (i = 0; i < fman->state->num_of_qman_channels; i++) {\r\nif (port_ids[i] == port_id)\r\nbreak;\r\n}\r\n}\r\nif (i == fman->state->num_of_qman_channels)\r\nreturn 0;\r\nreturn fman->state->qman_channel_base + i;\r\n}\r\nstruct resource *fman_get_mem_region(struct fman *fman)\r\n{\r\nreturn fman->state->res;\r\n}\r\nu16 fman_get_max_frm(void)\r\n{\r\nstatic bool fm_check_mfl;\r\nif (!fm_check_mfl) {\r\nif (fsl_fm_max_frm > FSL_FM_MAX_POSSIBLE_FRAME_SIZE ||\r\nfsl_fm_max_frm < FSL_FM_MIN_POSSIBLE_FRAME_SIZE) {\r\npr_warn("Invalid fsl_fm_max_frm value (%d) in bootargs, valid range is %d-%d. Falling back to the default (%d)\n",\r\nfsl_fm_max_frm,\r\nFSL_FM_MIN_POSSIBLE_FRAME_SIZE,\r\nFSL_FM_MAX_POSSIBLE_FRAME_SIZE,\r\nFSL_FM_MAX_FRAME_SIZE);\r\nfsl_fm_max_frm = FSL_FM_MAX_FRAME_SIZE;\r\n}\r\nfm_check_mfl = true;\r\n}\r\nreturn fsl_fm_max_frm;\r\n}\r\nint fman_get_rx_extra_headroom(void)\r\n{\r\nstatic bool fm_check_rx_extra_headroom;\r\nif (!fm_check_rx_extra_headroom) {\r\nif (fsl_fm_rx_extra_headroom > FSL_FM_RX_EXTRA_HEADROOM_MAX ||\r\nfsl_fm_rx_extra_headroom < FSL_FM_RX_EXTRA_HEADROOM_MIN) {\r\npr_warn("Invalid fsl_fm_rx_extra_headroom value (%d) in bootargs, valid range is %d-%d. Falling back to the default (%d)\n",\r\nfsl_fm_rx_extra_headroom,\r\nFSL_FM_RX_EXTRA_HEADROOM_MIN,\r\nFSL_FM_RX_EXTRA_HEADROOM_MAX,\r\nFSL_FM_RX_EXTRA_HEADROOM);\r\nfsl_fm_rx_extra_headroom = FSL_FM_RX_EXTRA_HEADROOM;\r\n}\r\nfm_check_rx_extra_headroom = true;\r\nfsl_fm_rx_extra_headroom = ALIGN(fsl_fm_rx_extra_headroom, 16);\r\n}\r\nreturn fsl_fm_rx_extra_headroom;\r\n}\r\nstruct fman *fman_bind(struct device *fm_dev)\r\n{\r\nreturn (struct fman *)(dev_get_drvdata(get_device(fm_dev)));\r\n}\r\nstatic irqreturn_t fman_err_irq(int irq, void *handle)\r\n{\r\nstruct fman *fman = (struct fman *)handle;\r\nu32 pending;\r\nstruct fman_fpm_regs __iomem *fpm_rg;\r\nirqreturn_t single_ret, ret = IRQ_NONE;\r\nif (!is_init_done(fman->cfg))\r\nreturn IRQ_NONE;\r\nfpm_rg = fman->fpm_regs;\r\npending = ioread32be(&fpm_rg->fm_epi);\r\nif (!pending)\r\nreturn IRQ_NONE;\r\nif (pending & ERR_INTR_EN_BMI) {\r\nsingle_ret = bmi_err_event(fman);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_QMI) {\r\nsingle_ret = qmi_err_event(fman);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_FPM) {\r\nsingle_ret = fpm_err_event(fman);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_DMA) {\r\nsingle_ret = dma_err_event(fman);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MURAM) {\r\nsingle_ret = muram_err_intr(fman);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC0) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 0);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC1) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 1);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC2) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 2);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC3) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 3);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC4) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 4);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC5) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 5);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC6) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 6);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC7) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 7);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC8) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 8);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & ERR_INTR_EN_MAC9) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_ERR_MAC0 + 9);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nreturn ret;\r\n}\r\nstatic irqreturn_t fman_irq(int irq, void *handle)\r\n{\r\nstruct fman *fman = (struct fman *)handle;\r\nu32 pending;\r\nstruct fman_fpm_regs __iomem *fpm_rg;\r\nirqreturn_t single_ret, ret = IRQ_NONE;\r\nif (!is_init_done(fman->cfg))\r\nreturn IRQ_NONE;\r\nfpm_rg = fman->fpm_regs;\r\npending = ioread32be(&fpm_rg->fm_npi);\r\nif (!pending)\r\nreturn IRQ_NONE;\r\nif (pending & INTR_EN_QMI) {\r\nsingle_ret = qmi_event(fman);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC0) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 0);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC1) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 1);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC2) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 2);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC3) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 3);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC4) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 4);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC5) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 5);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC6) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 6);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC7) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 7);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC8) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 8);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nif (pending & INTR_EN_MAC9) {\r\nsingle_ret = call_mac_isr(fman, FMAN_EV_MAC0 + 9);\r\nif (single_ret == IRQ_HANDLED)\r\nret = IRQ_HANDLED;\r\n}\r\nreturn ret;\r\n}\r\nstatic struct fman *read_dts_node(struct platform_device *of_dev)\r\n{\r\nstruct fman *fman;\r\nstruct device_node *fm_node, *muram_node;\r\nstruct resource *res;\r\nconst u32 *u32_prop;\r\nint lenp, err, irq;\r\nstruct clk *clk;\r\nu32 clk_rate;\r\nphys_addr_t phys_base_addr;\r\nresource_size_t mem_size;\r\nfman = kzalloc(sizeof(*fman), GFP_KERNEL);\r\nif (!fman)\r\nreturn NULL;\r\nfm_node = of_node_get(of_dev->dev.of_node);\r\nu32_prop = (const u32 *)of_get_property(fm_node, "cell-index", &lenp);\r\nif (!u32_prop) {\r\ndev_err(&of_dev->dev, "%s: of_get_property(%s, cell-index) failed\n",\r\n__func__, fm_node->full_name);\r\ngoto fman_node_put;\r\n}\r\nif (WARN_ON(lenp != sizeof(u32)))\r\ngoto fman_node_put;\r\nfman->dts_params.id = (u8)fdt32_to_cpu(u32_prop[0]);\r\nres = platform_get_resource(of_dev, IORESOURCE_IRQ, 0);\r\nif (!res) {\r\ndev_err(&of_dev->dev, "%s: Can't get FMan IRQ resource\n",\r\n__func__);\r\ngoto fman_node_put;\r\n}\r\nirq = res->start;\r\nres = platform_get_resource(of_dev, IORESOURCE_IRQ, 1);\r\nif (!res) {\r\ndev_err(&of_dev->dev, "%s: Can't get FMan Error IRQ resource\n",\r\n__func__);\r\ngoto fman_node_put;\r\n}\r\nfman->dts_params.err_irq = res->start;\r\nres = platform_get_resource(of_dev, IORESOURCE_MEM, 0);\r\nif (!res) {\r\ndev_err(&of_dev->dev, "%s: Can't get FMan memory resource\n",\r\n__func__);\r\ngoto fman_node_put;\r\n}\r\nphys_base_addr = res->start;\r\nmem_size = resource_size(res);\r\nclk = of_clk_get(fm_node, 0);\r\nif (IS_ERR(clk)) {\r\ndev_err(&of_dev->dev, "%s: Failed to get FM%d clock structure\n",\r\n__func__, fman->dts_params.id);\r\ngoto fman_node_put;\r\n}\r\nclk_rate = clk_get_rate(clk);\r\nif (!clk_rate) {\r\ndev_err(&of_dev->dev, "%s: Failed to determine FM%d clock rate\n",\r\n__func__, fman->dts_params.id);\r\ngoto fman_node_put;\r\n}\r\nfman->dts_params.clk_freq = DIV_ROUND_UP(clk_rate, 1000000);\r\nu32_prop = (const u32 *)of_get_property(fm_node,\r\n"fsl,qman-channel-range",\r\n&lenp);\r\nif (!u32_prop) {\r\ndev_err(&of_dev->dev, "%s: of_get_property(%s, fsl,qman-channel-range) failed\n",\r\n__func__, fm_node->full_name);\r\ngoto fman_node_put;\r\n}\r\nif (WARN_ON(lenp != sizeof(u32) * 2))\r\ngoto fman_node_put;\r\nfman->dts_params.qman_channel_base = fdt32_to_cpu(u32_prop[0]);\r\nfman->dts_params.num_of_qman_channels = fdt32_to_cpu(u32_prop[1]);\r\nmuram_node = of_find_matching_node(fm_node, fman_muram_match);\r\nif (!muram_node) {\r\ndev_err(&of_dev->dev, "%s: could not find MURAM node\n",\r\n__func__);\r\ngoto fman_node_put;\r\n}\r\nerr = of_address_to_resource(muram_node, 0,\r\n&fman->dts_params.muram_res);\r\nif (err) {\r\nof_node_put(muram_node);\r\ndev_err(&of_dev->dev, "%s: of_address_to_resource() = %d\n",\r\n__func__, err);\r\ngoto fman_node_put;\r\n}\r\nof_node_put(muram_node);\r\nof_node_put(fm_node);\r\nerr = devm_request_irq(&of_dev->dev, irq, fman_irq, 0, "fman", fman);\r\nif (err < 0) {\r\ndev_err(&of_dev->dev, "%s: irq %d allocation failed (error = %d)\n",\r\n__func__, irq, err);\r\ngoto fman_free;\r\n}\r\nif (fman->dts_params.err_irq != 0) {\r\nerr = devm_request_irq(&of_dev->dev, fman->dts_params.err_irq,\r\nfman_err_irq, IRQF_SHARED,\r\n"fman-err", fman);\r\nif (err < 0) {\r\ndev_err(&of_dev->dev, "%s: irq %d allocation failed (error = %d)\n",\r\n__func__, fman->dts_params.err_irq, err);\r\ngoto fman_free;\r\n}\r\n}\r\nfman->dts_params.res =\r\ndevm_request_mem_region(&of_dev->dev, phys_base_addr,\r\nmem_size, "fman");\r\nif (!fman->dts_params.res) {\r\ndev_err(&of_dev->dev, "%s: request_mem_region() failed\n",\r\n__func__);\r\ngoto fman_free;\r\n}\r\nfman->dts_params.base_addr =\r\ndevm_ioremap(&of_dev->dev, phys_base_addr, mem_size);\r\nif (fman->dts_params.base_addr == 0) {\r\ndev_err(&of_dev->dev, "%s: devm_ioremap() failed\n", __func__);\r\ngoto fman_free;\r\n}\r\nfman->dev = &of_dev->dev;\r\nreturn fman;\r\nfman_node_put:\r\nof_node_put(fm_node);\r\nfman_free:\r\nkfree(fman);\r\nreturn NULL;\r\n}\r\nstatic int fman_probe(struct platform_device *of_dev)\r\n{\r\nstruct fman *fman;\r\nstruct device *dev;\r\nint err;\r\ndev = &of_dev->dev;\r\nfman = read_dts_node(of_dev);\r\nif (!fman)\r\nreturn -EIO;\r\nerr = fman_config(fman);\r\nif (err) {\r\ndev_err(dev, "%s: FMan config failed\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nif (fman_init(fman) != 0) {\r\ndev_err(dev, "%s: FMan init failed\n", __func__);\r\nreturn -EINVAL;\r\n}\r\nif (fman->dts_params.err_irq == 0) {\r\nfman_set_exception(fman, FMAN_EX_DMA_BUS_ERROR, false);\r\nfman_set_exception(fman, FMAN_EX_DMA_READ_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_DMA_SYSTEM_WRITE_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_DMA_FM_WRITE_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_DMA_SINGLE_PORT_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_FPM_STALL_ON_TASKS, false);\r\nfman_set_exception(fman, FMAN_EX_FPM_SINGLE_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_FPM_DOUBLE_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_QMI_SINGLE_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_QMI_DOUBLE_ECC, false);\r\nfman_set_exception(fman,\r\nFMAN_EX_QMI_DEQ_FROM_UNKNOWN_PORTID, false);\r\nfman_set_exception(fman, FMAN_EX_BMI_LIST_RAM_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_BMI_STORAGE_PROFILE_ECC,\r\nfalse);\r\nfman_set_exception(fman, FMAN_EX_BMI_STATISTICS_RAM_ECC, false);\r\nfman_set_exception(fman, FMAN_EX_BMI_DISPATCH_RAM_ECC, false);\r\n}\r\ndev_set_drvdata(dev, fman);\r\ndev_dbg(dev, "FMan%d probed\n", fman->dts_params.id);\r\nreturn 0;\r\n}
