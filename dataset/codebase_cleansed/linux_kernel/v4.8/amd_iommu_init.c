static inline void update_last_devid(u16 devid)\r\n{\r\nif (devid > amd_iommu_last_bdf)\r\namd_iommu_last_bdf = devid;\r\n}\r\nstatic inline unsigned long tbl_size(int entry_size)\r\n{\r\nunsigned shift = PAGE_SHIFT +\r\nget_order(((int)amd_iommu_last_bdf + 1) * entry_size);\r\nreturn 1UL << shift;\r\n}\r\nstatic u32 iommu_read_l1(struct amd_iommu *iommu, u16 l1, u8 address)\r\n{\r\nu32 val;\r\npci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16));\r\npci_read_config_dword(iommu->dev, 0xfc, &val);\r\nreturn val;\r\n}\r\nstatic void iommu_write_l1(struct amd_iommu *iommu, u16 l1, u8 address, u32 val)\r\n{\r\npci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16 | 1 << 31));\r\npci_write_config_dword(iommu->dev, 0xfc, val);\r\npci_write_config_dword(iommu->dev, 0xf8, (address | l1 << 16));\r\n}\r\nstatic u32 iommu_read_l2(struct amd_iommu *iommu, u8 address)\r\n{\r\nu32 val;\r\npci_write_config_dword(iommu->dev, 0xf0, address);\r\npci_read_config_dword(iommu->dev, 0xf4, &val);\r\nreturn val;\r\n}\r\nstatic void iommu_write_l2(struct amd_iommu *iommu, u8 address, u32 val)\r\n{\r\npci_write_config_dword(iommu->dev, 0xf0, (address | 1 << 8));\r\npci_write_config_dword(iommu->dev, 0xf4, val);\r\n}\r\nstatic void iommu_set_exclusion_range(struct amd_iommu *iommu)\r\n{\r\nu64 start = iommu->exclusion_start & PAGE_MASK;\r\nu64 limit = (start + iommu->exclusion_length) & PAGE_MASK;\r\nu64 entry;\r\nif (!iommu->exclusion_start)\r\nreturn;\r\nentry = start | MMIO_EXCL_ENABLE_MASK;\r\nmemcpy_toio(iommu->mmio_base + MMIO_EXCL_BASE_OFFSET,\r\n&entry, sizeof(entry));\r\nentry = limit;\r\nmemcpy_toio(iommu->mmio_base + MMIO_EXCL_LIMIT_OFFSET,\r\n&entry, sizeof(entry));\r\n}\r\nstatic void iommu_set_device_table(struct amd_iommu *iommu)\r\n{\r\nu64 entry;\r\nBUG_ON(iommu->mmio_base == NULL);\r\nentry = virt_to_phys(amd_iommu_dev_table);\r\nentry |= (dev_table_size >> 12) - 1;\r\nmemcpy_toio(iommu->mmio_base + MMIO_DEV_TABLE_OFFSET,\r\n&entry, sizeof(entry));\r\n}\r\nstatic void iommu_feature_enable(struct amd_iommu *iommu, u8 bit)\r\n{\r\nu32 ctrl;\r\nctrl = readl(iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\nctrl |= (1 << bit);\r\nwritel(ctrl, iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\n}\r\nstatic void iommu_feature_disable(struct amd_iommu *iommu, u8 bit)\r\n{\r\nu32 ctrl;\r\nctrl = readl(iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\nctrl &= ~(1 << bit);\r\nwritel(ctrl, iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\n}\r\nstatic void iommu_set_inv_tlb_timeout(struct amd_iommu *iommu, int timeout)\r\n{\r\nu32 ctrl;\r\nctrl = readl(iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\nctrl &= ~CTRL_INV_TO_MASK;\r\nctrl |= (timeout << CONTROL_INV_TIMEOUT) & CTRL_INV_TO_MASK;\r\nwritel(ctrl, iommu->mmio_base + MMIO_CONTROL_OFFSET);\r\n}\r\nstatic void iommu_enable(struct amd_iommu *iommu)\r\n{\r\niommu_feature_enable(iommu, CONTROL_IOMMU_EN);\r\n}\r\nstatic void iommu_disable(struct amd_iommu *iommu)\r\n{\r\niommu_feature_disable(iommu, CONTROL_CMDBUF_EN);\r\niommu_feature_disable(iommu, CONTROL_EVT_INT_EN);\r\niommu_feature_disable(iommu, CONTROL_EVT_LOG_EN);\r\niommu_feature_disable(iommu, CONTROL_IOMMU_EN);\r\n}\r\nstatic u8 __iomem * __init iommu_map_mmio_space(u64 address, u64 end)\r\n{\r\nif (!request_mem_region(address, end, "amd_iommu")) {\r\npr_err("AMD-Vi: Can not reserve memory region %llx-%llx for mmio\n",\r\naddress, end);\r\npr_err("AMD-Vi: This is a BIOS bug. Please contact your hardware vendor\n");\r\nreturn NULL;\r\n}\r\nreturn (u8 __iomem *)ioremap_nocache(address, end);\r\n}\r\nstatic void __init iommu_unmap_mmio_space(struct amd_iommu *iommu)\r\n{\r\nif (iommu->mmio_base)\r\niounmap(iommu->mmio_base);\r\nrelease_mem_region(iommu->mmio_phys, iommu->mmio_phys_end);\r\n}\r\nstatic inline u32 get_ivhd_header_size(struct ivhd_header *h)\r\n{\r\nu32 size = 0;\r\nswitch (h->type) {\r\ncase 0x10:\r\nsize = 24;\r\nbreak;\r\ncase 0x11:\r\ncase 0x40:\r\nsize = 40;\r\nbreak;\r\n}\r\nreturn size;\r\n}\r\nstatic inline int ivhd_entry_length(u8 *ivhd)\r\n{\r\nu32 type = ((struct ivhd_entry *)ivhd)->type;\r\nif (type < 0x80) {\r\nreturn 0x04 << (*ivhd >> 6);\r\n} else if (type == IVHD_DEV_ACPI_HID) {\r\nreturn *((u8 *)ivhd + 21) + 22;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init find_last_devid_from_ivhd(struct ivhd_header *h)\r\n{\r\nu8 *p = (void *)h, *end = (void *)h;\r\nstruct ivhd_entry *dev;\r\nu32 ivhd_size = get_ivhd_header_size(h);\r\nif (!ivhd_size) {\r\npr_err("AMD-Vi: Unsupported IVHD type %#x\n", h->type);\r\nreturn -EINVAL;\r\n}\r\np += ivhd_size;\r\nend += h->length;\r\nwhile (p < end) {\r\ndev = (struct ivhd_entry *)p;\r\nswitch (dev->type) {\r\ncase IVHD_DEV_ALL:\r\nupdate_last_devid(0xffff);\r\nbreak;\r\ncase IVHD_DEV_SELECT:\r\ncase IVHD_DEV_RANGE_END:\r\ncase IVHD_DEV_ALIAS:\r\ncase IVHD_DEV_EXT_SELECT:\r\nupdate_last_devid(dev->devid);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\np += ivhd_entry_length(p);\r\n}\r\nWARN_ON(p != end);\r\nreturn 0;\r\n}\r\nstatic int __init check_ivrs_checksum(struct acpi_table_header *table)\r\n{\r\nint i;\r\nu8 checksum = 0, *p = (u8 *)table;\r\nfor (i = 0; i < table->length; ++i)\r\nchecksum += p[i];\r\nif (checksum != 0) {\r\npr_err(FW_BUG "AMD-Vi: IVRS invalid checksum\n");\r\nreturn -ENODEV;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init find_last_devid_acpi(struct acpi_table_header *table)\r\n{\r\nu8 *p = (u8 *)table, *end = (u8 *)table;\r\nstruct ivhd_header *h;\r\np += IVRS_HEADER_LENGTH;\r\nend += table->length;\r\nwhile (p < end) {\r\nh = (struct ivhd_header *)p;\r\nif (h->type == amd_iommu_target_ivhd_type) {\r\nint ret = find_last_devid_from_ivhd(h);\r\nif (ret)\r\nreturn ret;\r\n}\r\np += h->length;\r\n}\r\nWARN_ON(p != end);\r\nreturn 0;\r\n}\r\nstatic int __init alloc_command_buffer(struct amd_iommu *iommu)\r\n{\r\niommu->cmd_buf = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(CMD_BUFFER_SIZE));\r\nreturn iommu->cmd_buf ? 0 : -ENOMEM;\r\n}\r\nvoid amd_iommu_reset_cmd_buffer(struct amd_iommu *iommu)\r\n{\r\niommu_feature_disable(iommu, CONTROL_CMDBUF_EN);\r\nwritel(0x00, iommu->mmio_base + MMIO_CMD_HEAD_OFFSET);\r\nwritel(0x00, iommu->mmio_base + MMIO_CMD_TAIL_OFFSET);\r\niommu_feature_enable(iommu, CONTROL_CMDBUF_EN);\r\n}\r\nstatic void iommu_enable_command_buffer(struct amd_iommu *iommu)\r\n{\r\nu64 entry;\r\nBUG_ON(iommu->cmd_buf == NULL);\r\nentry = (u64)virt_to_phys(iommu->cmd_buf);\r\nentry |= MMIO_CMD_SIZE_512;\r\nmemcpy_toio(iommu->mmio_base + MMIO_CMD_BUF_OFFSET,\r\n&entry, sizeof(entry));\r\namd_iommu_reset_cmd_buffer(iommu);\r\n}\r\nstatic void __init free_command_buffer(struct amd_iommu *iommu)\r\n{\r\nfree_pages((unsigned long)iommu->cmd_buf, get_order(CMD_BUFFER_SIZE));\r\n}\r\nstatic int __init alloc_event_buffer(struct amd_iommu *iommu)\r\n{\r\niommu->evt_buf = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(EVT_BUFFER_SIZE));\r\nreturn iommu->evt_buf ? 0 : -ENOMEM;\r\n}\r\nstatic void iommu_enable_event_buffer(struct amd_iommu *iommu)\r\n{\r\nu64 entry;\r\nBUG_ON(iommu->evt_buf == NULL);\r\nentry = (u64)virt_to_phys(iommu->evt_buf) | EVT_LEN_MASK;\r\nmemcpy_toio(iommu->mmio_base + MMIO_EVT_BUF_OFFSET,\r\n&entry, sizeof(entry));\r\nwritel(0x00, iommu->mmio_base + MMIO_EVT_HEAD_OFFSET);\r\nwritel(0x00, iommu->mmio_base + MMIO_EVT_TAIL_OFFSET);\r\niommu_feature_enable(iommu, CONTROL_EVT_LOG_EN);\r\n}\r\nstatic void __init free_event_buffer(struct amd_iommu *iommu)\r\n{\r\nfree_pages((unsigned long)iommu->evt_buf, get_order(EVT_BUFFER_SIZE));\r\n}\r\nstatic int __init alloc_ppr_log(struct amd_iommu *iommu)\r\n{\r\niommu->ppr_log = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(PPR_LOG_SIZE));\r\nreturn iommu->ppr_log ? 0 : -ENOMEM;\r\n}\r\nstatic void iommu_enable_ppr_log(struct amd_iommu *iommu)\r\n{\r\nu64 entry;\r\nif (iommu->ppr_log == NULL)\r\nreturn;\r\nentry = (u64)virt_to_phys(iommu->ppr_log) | PPR_LOG_SIZE_512;\r\nmemcpy_toio(iommu->mmio_base + MMIO_PPR_LOG_OFFSET,\r\n&entry, sizeof(entry));\r\nwritel(0x00, iommu->mmio_base + MMIO_PPR_HEAD_OFFSET);\r\nwritel(0x00, iommu->mmio_base + MMIO_PPR_TAIL_OFFSET);\r\niommu_feature_enable(iommu, CONTROL_PPFLOG_EN);\r\niommu_feature_enable(iommu, CONTROL_PPR_EN);\r\n}\r\nstatic void __init free_ppr_log(struct amd_iommu *iommu)\r\n{\r\nif (iommu->ppr_log == NULL)\r\nreturn;\r\nfree_pages((unsigned long)iommu->ppr_log, get_order(PPR_LOG_SIZE));\r\n}\r\nstatic void iommu_enable_gt(struct amd_iommu *iommu)\r\n{\r\nif (!iommu_feature(iommu, FEATURE_GT))\r\nreturn;\r\niommu_feature_enable(iommu, CONTROL_GT_EN);\r\n}\r\nstatic void set_dev_entry_bit(u16 devid, u8 bit)\r\n{\r\nint i = (bit >> 6) & 0x03;\r\nint _bit = bit & 0x3f;\r\namd_iommu_dev_table[devid].data[i] |= (1UL << _bit);\r\n}\r\nstatic int get_dev_entry_bit(u16 devid, u8 bit)\r\n{\r\nint i = (bit >> 6) & 0x03;\r\nint _bit = bit & 0x3f;\r\nreturn (amd_iommu_dev_table[devid].data[i] & (1UL << _bit)) >> _bit;\r\n}\r\nvoid amd_iommu_apply_erratum_63(u16 devid)\r\n{\r\nint sysmgt;\r\nsysmgt = get_dev_entry_bit(devid, DEV_ENTRY_SYSMGT1) |\r\n(get_dev_entry_bit(devid, DEV_ENTRY_SYSMGT2) << 1);\r\nif (sysmgt == 0x01)\r\nset_dev_entry_bit(devid, DEV_ENTRY_IW);\r\n}\r\nstatic void __init set_iommu_for_device(struct amd_iommu *iommu, u16 devid)\r\n{\r\namd_iommu_rlookup_table[devid] = iommu;\r\n}\r\nstatic void __init set_dev_entry_from_acpi(struct amd_iommu *iommu,\r\nu16 devid, u32 flags, u32 ext_flags)\r\n{\r\nif (flags & ACPI_DEVFLAG_INITPASS)\r\nset_dev_entry_bit(devid, DEV_ENTRY_INIT_PASS);\r\nif (flags & ACPI_DEVFLAG_EXTINT)\r\nset_dev_entry_bit(devid, DEV_ENTRY_EINT_PASS);\r\nif (flags & ACPI_DEVFLAG_NMI)\r\nset_dev_entry_bit(devid, DEV_ENTRY_NMI_PASS);\r\nif (flags & ACPI_DEVFLAG_SYSMGT1)\r\nset_dev_entry_bit(devid, DEV_ENTRY_SYSMGT1);\r\nif (flags & ACPI_DEVFLAG_SYSMGT2)\r\nset_dev_entry_bit(devid, DEV_ENTRY_SYSMGT2);\r\nif (flags & ACPI_DEVFLAG_LINT0)\r\nset_dev_entry_bit(devid, DEV_ENTRY_LINT0_PASS);\r\nif (flags & ACPI_DEVFLAG_LINT1)\r\nset_dev_entry_bit(devid, DEV_ENTRY_LINT1_PASS);\r\namd_iommu_apply_erratum_63(devid);\r\nset_iommu_for_device(iommu, devid);\r\n}\r\nstatic int __init add_special_device(u8 type, u8 id, u16 *devid, bool cmd_line)\r\n{\r\nstruct devid_map *entry;\r\nstruct list_head *list;\r\nif (type == IVHD_SPECIAL_IOAPIC)\r\nlist = &ioapic_map;\r\nelse if (type == IVHD_SPECIAL_HPET)\r\nlist = &hpet_map;\r\nelse\r\nreturn -EINVAL;\r\nlist_for_each_entry(entry, list, list) {\r\nif (!(entry->id == id && entry->cmd_line))\r\ncontinue;\r\npr_info("AMD-Vi: Command-line override present for %s id %d - ignoring\n",\r\ntype == IVHD_SPECIAL_IOAPIC ? "IOAPIC" : "HPET", id);\r\n*devid = entry->devid;\r\nreturn 0;\r\n}\r\nentry = kzalloc(sizeof(*entry), GFP_KERNEL);\r\nif (!entry)\r\nreturn -ENOMEM;\r\nentry->id = id;\r\nentry->devid = *devid;\r\nentry->cmd_line = cmd_line;\r\nlist_add_tail(&entry->list, list);\r\nreturn 0;\r\n}\r\nstatic int __init add_acpi_hid_device(u8 *hid, u8 *uid, u16 *devid,\r\nbool cmd_line)\r\n{\r\nstruct acpihid_map_entry *entry;\r\nstruct list_head *list = &acpihid_map;\r\nlist_for_each_entry(entry, list, list) {\r\nif (strcmp(entry->hid, hid) ||\r\n(*uid && *entry->uid && strcmp(entry->uid, uid)) ||\r\n!entry->cmd_line)\r\ncontinue;\r\npr_info("AMD-Vi: Command-line override for hid:%s uid:%s\n",\r\nhid, uid);\r\n*devid = entry->devid;\r\nreturn 0;\r\n}\r\nentry = kzalloc(sizeof(*entry), GFP_KERNEL);\r\nif (!entry)\r\nreturn -ENOMEM;\r\nmemcpy(entry->uid, uid, strlen(uid));\r\nmemcpy(entry->hid, hid, strlen(hid));\r\nentry->devid = *devid;\r\nentry->cmd_line = cmd_line;\r\nentry->root_devid = (entry->devid & (~0x7));\r\npr_info("AMD-Vi:%s, add hid:%s, uid:%s, rdevid:%d\n",\r\nentry->cmd_line ? "cmd" : "ivrs",\r\nentry->hid, entry->uid, entry->root_devid);\r\nlist_add_tail(&entry->list, list);\r\nreturn 0;\r\n}\r\nstatic int __init add_early_maps(void)\r\n{\r\nint i, ret;\r\nfor (i = 0; i < early_ioapic_map_size; ++i) {\r\nret = add_special_device(IVHD_SPECIAL_IOAPIC,\r\nearly_ioapic_map[i].id,\r\n&early_ioapic_map[i].devid,\r\nearly_ioapic_map[i].cmd_line);\r\nif (ret)\r\nreturn ret;\r\n}\r\nfor (i = 0; i < early_hpet_map_size; ++i) {\r\nret = add_special_device(IVHD_SPECIAL_HPET,\r\nearly_hpet_map[i].id,\r\n&early_hpet_map[i].devid,\r\nearly_hpet_map[i].cmd_line);\r\nif (ret)\r\nreturn ret;\r\n}\r\nfor (i = 0; i < early_acpihid_map_size; ++i) {\r\nret = add_acpi_hid_device(early_acpihid_map[i].hid,\r\nearly_acpihid_map[i].uid,\r\n&early_acpihid_map[i].devid,\r\nearly_acpihid_map[i].cmd_line);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __init set_device_exclusion_range(u16 devid, struct ivmd_header *m)\r\n{\r\nstruct amd_iommu *iommu = amd_iommu_rlookup_table[devid];\r\nif (!(m->flags & IVMD_FLAG_EXCL_RANGE))\r\nreturn;\r\nif (iommu) {\r\nset_dev_entry_bit(devid, DEV_ENTRY_EX);\r\niommu->exclusion_start = m->range_start;\r\niommu->exclusion_length = m->range_length;\r\n}\r\n}\r\nstatic int __init init_iommu_from_acpi(struct amd_iommu *iommu,\r\nstruct ivhd_header *h)\r\n{\r\nu8 *p = (u8 *)h;\r\nu8 *end = p, flags = 0;\r\nu16 devid = 0, devid_start = 0, devid_to = 0;\r\nu32 dev_i, ext_flags = 0;\r\nbool alias = false;\r\nstruct ivhd_entry *e;\r\nu32 ivhd_size;\r\nint ret;\r\nret = add_early_maps();\r\nif (ret)\r\nreturn ret;\r\niommu->acpi_flags = h->flags;\r\nivhd_size = get_ivhd_header_size(h);\r\nif (!ivhd_size) {\r\npr_err("AMD-Vi: Unsupported IVHD type %#x\n", h->type);\r\nreturn -EINVAL;\r\n}\r\np += ivhd_size;\r\nend += h->length;\r\nwhile (p < end) {\r\ne = (struct ivhd_entry *)p;\r\nswitch (e->type) {\r\ncase IVHD_DEV_ALL:\r\nDUMP_printk(" DEV_ALL\t\t\tflags: %02x\n", e->flags);\r\nfor (dev_i = 0; dev_i <= amd_iommu_last_bdf; ++dev_i)\r\nset_dev_entry_from_acpi(iommu, dev_i, e->flags, 0);\r\nbreak;\r\ncase IVHD_DEV_SELECT:\r\nDUMP_printk(" DEV_SELECT\t\t\t devid: %02x:%02x.%x "\r\n"flags: %02x\n",\r\nPCI_BUS_NUM(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags);\r\ndevid = e->devid;\r\nset_dev_entry_from_acpi(iommu, devid, e->flags, 0);\r\nbreak;\r\ncase IVHD_DEV_SELECT_RANGE_START:\r\nDUMP_printk(" DEV_SELECT_RANGE_START\t "\r\n"devid: %02x:%02x.%x flags: %02x\n",\r\nPCI_BUS_NUM(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags);\r\ndevid_start = e->devid;\r\nflags = e->flags;\r\next_flags = 0;\r\nalias = false;\r\nbreak;\r\ncase IVHD_DEV_ALIAS:\r\nDUMP_printk(" DEV_ALIAS\t\t\t devid: %02x:%02x.%x "\r\n"flags: %02x devid_to: %02x:%02x.%x\n",\r\nPCI_BUS_NUM(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags,\r\nPCI_BUS_NUM(e->ext >> 8),\r\nPCI_SLOT(e->ext >> 8),\r\nPCI_FUNC(e->ext >> 8));\r\ndevid = e->devid;\r\ndevid_to = e->ext >> 8;\r\nset_dev_entry_from_acpi(iommu, devid , e->flags, 0);\r\nset_dev_entry_from_acpi(iommu, devid_to, e->flags, 0);\r\namd_iommu_alias_table[devid] = devid_to;\r\nbreak;\r\ncase IVHD_DEV_ALIAS_RANGE:\r\nDUMP_printk(" DEV_ALIAS_RANGE\t\t "\r\n"devid: %02x:%02x.%x flags: %02x "\r\n"devid_to: %02x:%02x.%x\n",\r\nPCI_BUS_NUM(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags,\r\nPCI_BUS_NUM(e->ext >> 8),\r\nPCI_SLOT(e->ext >> 8),\r\nPCI_FUNC(e->ext >> 8));\r\ndevid_start = e->devid;\r\nflags = e->flags;\r\ndevid_to = e->ext >> 8;\r\next_flags = 0;\r\nalias = true;\r\nbreak;\r\ncase IVHD_DEV_EXT_SELECT:\r\nDUMP_printk(" DEV_EXT_SELECT\t\t devid: %02x:%02x.%x "\r\n"flags: %02x ext: %08x\n",\r\nPCI_BUS_NUM(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags, e->ext);\r\ndevid = e->devid;\r\nset_dev_entry_from_acpi(iommu, devid, e->flags,\r\ne->ext);\r\nbreak;\r\ncase IVHD_DEV_EXT_SELECT_RANGE:\r\nDUMP_printk(" DEV_EXT_SELECT_RANGE\t devid: "\r\n"%02x:%02x.%x flags: %02x ext: %08x\n",\r\nPCI_BUS_NUM(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid),\r\ne->flags, e->ext);\r\ndevid_start = e->devid;\r\nflags = e->flags;\r\next_flags = e->ext;\r\nalias = false;\r\nbreak;\r\ncase IVHD_DEV_RANGE_END:\r\nDUMP_printk(" DEV_RANGE_END\t\t devid: %02x:%02x.%x\n",\r\nPCI_BUS_NUM(e->devid),\r\nPCI_SLOT(e->devid),\r\nPCI_FUNC(e->devid));\r\ndevid = e->devid;\r\nfor (dev_i = devid_start; dev_i <= devid; ++dev_i) {\r\nif (alias) {\r\namd_iommu_alias_table[dev_i] = devid_to;\r\nset_dev_entry_from_acpi(iommu,\r\ndevid_to, flags, ext_flags);\r\n}\r\nset_dev_entry_from_acpi(iommu, dev_i,\r\nflags, ext_flags);\r\n}\r\nbreak;\r\ncase IVHD_DEV_SPECIAL: {\r\nu8 handle, type;\r\nconst char *var;\r\nu16 devid;\r\nint ret;\r\nhandle = e->ext & 0xff;\r\ndevid = (e->ext >> 8) & 0xffff;\r\ntype = (e->ext >> 24) & 0xff;\r\nif (type == IVHD_SPECIAL_IOAPIC)\r\nvar = "IOAPIC";\r\nelse if (type == IVHD_SPECIAL_HPET)\r\nvar = "HPET";\r\nelse\r\nvar = "UNKNOWN";\r\nDUMP_printk(" DEV_SPECIAL(%s[%d])\t\tdevid: %02x:%02x.%x\n",\r\nvar, (int)handle,\r\nPCI_BUS_NUM(devid),\r\nPCI_SLOT(devid),\r\nPCI_FUNC(devid));\r\nret = add_special_device(type, handle, &devid, false);\r\nif (ret)\r\nreturn ret;\r\nset_dev_entry_from_acpi(iommu, devid, e->flags, 0);\r\nbreak;\r\n}\r\ncase IVHD_DEV_ACPI_HID: {\r\nu16 devid;\r\nu8 hid[ACPIHID_HID_LEN] = {0};\r\nu8 uid[ACPIHID_UID_LEN] = {0};\r\nint ret;\r\nif (h->type != 0x40) {\r\npr_err(FW_BUG "Invalid IVHD device type %#x\n",\r\ne->type);\r\nbreak;\r\n}\r\nmemcpy(hid, (u8 *)(&e->ext), ACPIHID_HID_LEN - 1);\r\nhid[ACPIHID_HID_LEN - 1] = '\0';\r\nif (!(*hid)) {\r\npr_err(FW_BUG "Invalid HID.\n");\r\nbreak;\r\n}\r\nswitch (e->uidf) {\r\ncase UID_NOT_PRESENT:\r\nif (e->uidl != 0)\r\npr_warn(FW_BUG "Invalid UID length.\n");\r\nbreak;\r\ncase UID_IS_INTEGER:\r\nsprintf(uid, "%d", e->uid);\r\nbreak;\r\ncase UID_IS_CHARACTER:\r\nmemcpy(uid, (u8 *)(&e->uid), ACPIHID_UID_LEN - 1);\r\nuid[ACPIHID_UID_LEN - 1] = '\0';\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\ndevid = e->devid;\r\nDUMP_printk(" DEV_ACPI_HID(%s[%s])\t\tdevid: %02x:%02x.%x\n",\r\nhid, uid,\r\nPCI_BUS_NUM(devid),\r\nPCI_SLOT(devid),\r\nPCI_FUNC(devid));\r\nflags = e->flags;\r\nret = add_acpi_hid_device(hid, uid, &devid, false);\r\nif (ret)\r\nreturn ret;\r\nset_dev_entry_from_acpi(iommu, devid, e->flags, 0);\r\nbreak;\r\n}\r\ndefault:\r\nbreak;\r\n}\r\np += ivhd_entry_length(p);\r\n}\r\nreturn 0;\r\n}\r\nstatic void __init free_iommu_one(struct amd_iommu *iommu)\r\n{\r\nfree_command_buffer(iommu);\r\nfree_event_buffer(iommu);\r\nfree_ppr_log(iommu);\r\niommu_unmap_mmio_space(iommu);\r\n}\r\nstatic void __init free_iommu_all(void)\r\n{\r\nstruct amd_iommu *iommu, *next;\r\nfor_each_iommu_safe(iommu, next) {\r\nlist_del(&iommu->list);\r\nfree_iommu_one(iommu);\r\nkfree(iommu);\r\n}\r\n}\r\nstatic void amd_iommu_erratum_746_workaround(struct amd_iommu *iommu)\r\n{\r\nu32 value;\r\nif ((boot_cpu_data.x86 != 0x15) ||\r\n(boot_cpu_data.x86_model < 0x10) ||\r\n(boot_cpu_data.x86_model > 0x1f))\r\nreturn;\r\npci_write_config_dword(iommu->dev, 0xf0, 0x90);\r\npci_read_config_dword(iommu->dev, 0xf4, &value);\r\nif (value & BIT(2))\r\nreturn;\r\npci_write_config_dword(iommu->dev, 0xf0, 0x90 | (1 << 8));\r\npci_write_config_dword(iommu->dev, 0xf4, value | 0x4);\r\npr_info("AMD-Vi: Applying erratum 746 workaround for IOMMU at %s\n",\r\ndev_name(&iommu->dev->dev));\r\npci_write_config_dword(iommu->dev, 0xf0, 0x90);\r\n}\r\nstatic void amd_iommu_ats_write_check_workaround(struct amd_iommu *iommu)\r\n{\r\nu32 value;\r\nif ((boot_cpu_data.x86 != 0x15) ||\r\n(boot_cpu_data.x86_model < 0x30) ||\r\n(boot_cpu_data.x86_model > 0x3f))\r\nreturn;\r\nvalue = iommu_read_l2(iommu, 0x47);\r\nif (value & BIT(0))\r\nreturn;\r\niommu_write_l2(iommu, 0x47, value | BIT(0));\r\npr_info("AMD-Vi: Applying ATS write check workaround for IOMMU at %s\n",\r\ndev_name(&iommu->dev->dev));\r\n}\r\nstatic int __init init_iommu_one(struct amd_iommu *iommu, struct ivhd_header *h)\r\n{\r\nint ret;\r\nspin_lock_init(&iommu->lock);\r\nlist_add_tail(&iommu->list, &amd_iommu_list);\r\niommu->index = amd_iommus_present++;\r\nif (unlikely(iommu->index >= MAX_IOMMUS)) {\r\nWARN(1, "AMD-Vi: System has more IOMMUs than supported by this driver\n");\r\nreturn -ENOSYS;\r\n}\r\namd_iommus[iommu->index] = iommu;\r\niommu->devid = h->devid;\r\niommu->cap_ptr = h->cap_ptr;\r\niommu->pci_seg = h->pci_seg;\r\niommu->mmio_phys = h->mmio_phys;\r\nswitch (h->type) {\r\ncase 0x10:\r\nif ((h->efr_attr != 0) &&\r\n((h->efr_attr & (0xF << 13)) != 0) &&\r\n((h->efr_attr & (0x3F << 17)) != 0))\r\niommu->mmio_phys_end = MMIO_REG_END_OFFSET;\r\nelse\r\niommu->mmio_phys_end = MMIO_CNTR_CONF_OFFSET;\r\nbreak;\r\ncase 0x11:\r\ncase 0x40:\r\nif (h->efr_reg & (1 << 9))\r\niommu->mmio_phys_end = MMIO_REG_END_OFFSET;\r\nelse\r\niommu->mmio_phys_end = MMIO_CNTR_CONF_OFFSET;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\niommu->mmio_base = iommu_map_mmio_space(iommu->mmio_phys,\r\niommu->mmio_phys_end);\r\nif (!iommu->mmio_base)\r\nreturn -ENOMEM;\r\nif (alloc_command_buffer(iommu))\r\nreturn -ENOMEM;\r\nif (alloc_event_buffer(iommu))\r\nreturn -ENOMEM;\r\niommu->int_enabled = false;\r\nret = init_iommu_from_acpi(iommu, h);\r\nif (ret)\r\nreturn ret;\r\nret = amd_iommu_create_irq_domain(iommu);\r\nif (ret)\r\nreturn ret;\r\namd_iommu_rlookup_table[iommu->devid] = NULL;\r\nreturn 0;\r\n}\r\nstatic u8 get_highest_supported_ivhd_type(struct acpi_table_header *ivrs)\r\n{\r\nu8 *base = (u8 *)ivrs;\r\nstruct ivhd_header *ivhd = (struct ivhd_header *)\r\n(base + IVRS_HEADER_LENGTH);\r\nu8 last_type = ivhd->type;\r\nu16 devid = ivhd->devid;\r\nwhile (((u8 *)ivhd - base < ivrs->length) &&\r\n(ivhd->type <= ACPI_IVHD_TYPE_MAX_SUPPORTED)) {\r\nu8 *p = (u8 *) ivhd;\r\nif (ivhd->devid == devid)\r\nlast_type = ivhd->type;\r\nivhd = (struct ivhd_header *)(p + ivhd->length);\r\n}\r\nreturn last_type;\r\n}\r\nstatic int __init init_iommu_all(struct acpi_table_header *table)\r\n{\r\nu8 *p = (u8 *)table, *end = (u8 *)table;\r\nstruct ivhd_header *h;\r\nstruct amd_iommu *iommu;\r\nint ret;\r\nend += table->length;\r\np += IVRS_HEADER_LENGTH;\r\nwhile (p < end) {\r\nh = (struct ivhd_header *)p;\r\nif (*p == amd_iommu_target_ivhd_type) {\r\nDUMP_printk("device: %02x:%02x.%01x cap: %04x "\r\n"seg: %d flags: %01x info %04x\n",\r\nPCI_BUS_NUM(h->devid), PCI_SLOT(h->devid),\r\nPCI_FUNC(h->devid), h->cap_ptr,\r\nh->pci_seg, h->flags, h->info);\r\nDUMP_printk(" mmio-addr: %016llx\n",\r\nh->mmio_phys);\r\niommu = kzalloc(sizeof(struct amd_iommu), GFP_KERNEL);\r\nif (iommu == NULL)\r\nreturn -ENOMEM;\r\nret = init_iommu_one(iommu, h);\r\nif (ret)\r\nreturn ret;\r\n}\r\np += h->length;\r\n}\r\nWARN_ON(p != end);\r\nreturn 0;\r\n}\r\nstatic void init_iommu_perf_ctr(struct amd_iommu *iommu)\r\n{\r\nu64 val = 0xabcd, val2 = 0;\r\nif (!iommu_feature(iommu, FEATURE_PC))\r\nreturn;\r\namd_iommu_pc_present = true;\r\nif ((0 != iommu_pc_get_set_reg_val(iommu, 0, 0, 0, &val, true)) ||\r\n(0 != iommu_pc_get_set_reg_val(iommu, 0, 0, 0, &val2, false)) ||\r\n(val != val2)) {\r\npr_err("AMD-Vi: Unable to write to IOMMU perf counter.\n");\r\namd_iommu_pc_present = false;\r\nreturn;\r\n}\r\npr_info("AMD-Vi: IOMMU performance counters supported\n");\r\nval = readl(iommu->mmio_base + MMIO_CNTR_CONF_OFFSET);\r\niommu->max_banks = (u8) ((val >> 12) & 0x3f);\r\niommu->max_counters = (u8) ((val >> 7) & 0xf);\r\n}\r\nstatic ssize_t amd_iommu_show_cap(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct amd_iommu *iommu = dev_get_drvdata(dev);\r\nreturn sprintf(buf, "%x\n", iommu->cap);\r\n}\r\nstatic ssize_t amd_iommu_show_features(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct amd_iommu *iommu = dev_get_drvdata(dev);\r\nreturn sprintf(buf, "%llx\n", iommu->features);\r\n}\r\nstatic int iommu_init_pci(struct amd_iommu *iommu)\r\n{\r\nint cap_ptr = iommu->cap_ptr;\r\nu32 range, misc, low, high;\r\niommu->dev = pci_get_bus_and_slot(PCI_BUS_NUM(iommu->devid),\r\niommu->devid & 0xff);\r\nif (!iommu->dev)\r\nreturn -ENODEV;\r\niommu->dev->match_driver = false;\r\npci_read_config_dword(iommu->dev, cap_ptr + MMIO_CAP_HDR_OFFSET,\r\n&iommu->cap);\r\npci_read_config_dword(iommu->dev, cap_ptr + MMIO_RANGE_OFFSET,\r\n&range);\r\npci_read_config_dword(iommu->dev, cap_ptr + MMIO_MISC_OFFSET,\r\n&misc);\r\nif (!(iommu->cap & (1 << IOMMU_CAP_IOTLB)))\r\namd_iommu_iotlb_sup = false;\r\nlow = readl(iommu->mmio_base + MMIO_EXT_FEATURES);\r\nhigh = readl(iommu->mmio_base + MMIO_EXT_FEATURES + 4);\r\niommu->features = ((u64)high << 32) | low;\r\nif (iommu_feature(iommu, FEATURE_GT)) {\r\nint glxval;\r\nu32 max_pasid;\r\nu64 pasmax;\r\npasmax = iommu->features & FEATURE_PASID_MASK;\r\npasmax >>= FEATURE_PASID_SHIFT;\r\nmax_pasid = (1 << (pasmax + 1)) - 1;\r\namd_iommu_max_pasid = min(amd_iommu_max_pasid, max_pasid);\r\nBUG_ON(amd_iommu_max_pasid & ~PASID_MASK);\r\nglxval = iommu->features & FEATURE_GLXVAL_MASK;\r\nglxval >>= FEATURE_GLXVAL_SHIFT;\r\nif (amd_iommu_max_glx_val == -1)\r\namd_iommu_max_glx_val = glxval;\r\nelse\r\namd_iommu_max_glx_val = min(amd_iommu_max_glx_val, glxval);\r\n}\r\nif (iommu_feature(iommu, FEATURE_GT) &&\r\niommu_feature(iommu, FEATURE_PPR)) {\r\niommu->is_iommu_v2 = true;\r\namd_iommu_v2_present = true;\r\n}\r\nif (iommu_feature(iommu, FEATURE_PPR) && alloc_ppr_log(iommu))\r\nreturn -ENOMEM;\r\nif (iommu->cap & (1UL << IOMMU_CAP_NPCACHE))\r\namd_iommu_np_cache = true;\r\ninit_iommu_perf_ctr(iommu);\r\nif (is_rd890_iommu(iommu->dev)) {\r\nint i, j;\r\niommu->root_pdev = pci_get_bus_and_slot(iommu->dev->bus->number,\r\nPCI_DEVFN(0, 0));\r\npci_read_config_dword(iommu->dev, iommu->cap_ptr + 4,\r\n&iommu->stored_addr_lo);\r\npci_read_config_dword(iommu->dev, iommu->cap_ptr + 8,\r\n&iommu->stored_addr_hi);\r\niommu->stored_addr_lo &= ~1;\r\nfor (i = 0; i < 6; i++)\r\nfor (j = 0; j < 0x12; j++)\r\niommu->stored_l1[i][j] = iommu_read_l1(iommu, i, j);\r\nfor (i = 0; i < 0x83; i++)\r\niommu->stored_l2[i] = iommu_read_l2(iommu, i);\r\n}\r\namd_iommu_erratum_746_workaround(iommu);\r\namd_iommu_ats_write_check_workaround(iommu);\r\niommu->iommu_dev = iommu_device_create(&iommu->dev->dev, iommu,\r\namd_iommu_groups, "ivhd%d",\r\niommu->index);\r\nreturn pci_enable_device(iommu->dev);\r\n}\r\nstatic void print_iommu_info(void)\r\n{\r\nstatic const char * const feat_str[] = {\r\n"PreF", "PPR", "X2APIC", "NX", "GT", "[5]",\r\n"IA", "GA", "HE", "PC"\r\n};\r\nstruct amd_iommu *iommu;\r\nfor_each_iommu(iommu) {\r\nint i;\r\npr_info("AMD-Vi: Found IOMMU at %s cap 0x%hx\n",\r\ndev_name(&iommu->dev->dev), iommu->cap_ptr);\r\nif (iommu->cap & (1 << IOMMU_CAP_EFR)) {\r\npr_info("AMD-Vi: Extended features: ");\r\nfor (i = 0; i < ARRAY_SIZE(feat_str); ++i) {\r\nif (iommu_feature(iommu, (1ULL << i)))\r\npr_cont(" %s", feat_str[i]);\r\n}\r\npr_cont("\n");\r\n}\r\n}\r\nif (irq_remapping_enabled)\r\npr_info("AMD-Vi: Interrupt remapping enabled\n");\r\n}\r\nstatic int __init amd_iommu_init_pci(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nint ret = 0;\r\nfor_each_iommu(iommu) {\r\nret = iommu_init_pci(iommu);\r\nif (ret)\r\nbreak;\r\n}\r\nret = amd_iommu_init_api();\r\ninit_device_table_dma();\r\nfor_each_iommu(iommu)\r\niommu_flush_all_caches(iommu);\r\nif (!ret)\r\nprint_iommu_info();\r\nreturn ret;\r\n}\r\nstatic int iommu_setup_msi(struct amd_iommu *iommu)\r\n{\r\nint r;\r\nr = pci_enable_msi(iommu->dev);\r\nif (r)\r\nreturn r;\r\nr = request_threaded_irq(iommu->dev->irq,\r\namd_iommu_int_handler,\r\namd_iommu_int_thread,\r\n0, "AMD-Vi",\r\niommu);\r\nif (r) {\r\npci_disable_msi(iommu->dev);\r\nreturn r;\r\n}\r\niommu->int_enabled = true;\r\nreturn 0;\r\n}\r\nstatic int iommu_init_msi(struct amd_iommu *iommu)\r\n{\r\nint ret;\r\nif (iommu->int_enabled)\r\ngoto enable_faults;\r\nif (iommu->dev->msi_cap)\r\nret = iommu_setup_msi(iommu);\r\nelse\r\nret = -ENODEV;\r\nif (ret)\r\nreturn ret;\r\nenable_faults:\r\niommu_feature_enable(iommu, CONTROL_EVT_INT_EN);\r\nif (iommu->ppr_log != NULL)\r\niommu_feature_enable(iommu, CONTROL_PPFINT_EN);\r\nreturn 0;\r\n}\r\nstatic void __init free_unity_maps(void)\r\n{\r\nstruct unity_map_entry *entry, *next;\r\nlist_for_each_entry_safe(entry, next, &amd_iommu_unity_map, list) {\r\nlist_del(&entry->list);\r\nkfree(entry);\r\n}\r\n}\r\nstatic int __init init_exclusion_range(struct ivmd_header *m)\r\n{\r\nint i;\r\nswitch (m->type) {\r\ncase ACPI_IVMD_TYPE:\r\nset_device_exclusion_range(m->devid, m);\r\nbreak;\r\ncase ACPI_IVMD_TYPE_ALL:\r\nfor (i = 0; i <= amd_iommu_last_bdf; ++i)\r\nset_device_exclusion_range(i, m);\r\nbreak;\r\ncase ACPI_IVMD_TYPE_RANGE:\r\nfor (i = m->devid; i <= m->aux; ++i)\r\nset_device_exclusion_range(i, m);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int __init init_unity_map_range(struct ivmd_header *m)\r\n{\r\nstruct unity_map_entry *e = NULL;\r\nchar *s;\r\ne = kzalloc(sizeof(*e), GFP_KERNEL);\r\nif (e == NULL)\r\nreturn -ENOMEM;\r\nswitch (m->type) {\r\ndefault:\r\nkfree(e);\r\nreturn 0;\r\ncase ACPI_IVMD_TYPE:\r\ns = "IVMD_TYPEi\t\t\t";\r\ne->devid_start = e->devid_end = m->devid;\r\nbreak;\r\ncase ACPI_IVMD_TYPE_ALL:\r\ns = "IVMD_TYPE_ALL\t\t";\r\ne->devid_start = 0;\r\ne->devid_end = amd_iommu_last_bdf;\r\nbreak;\r\ncase ACPI_IVMD_TYPE_RANGE:\r\ns = "IVMD_TYPE_RANGE\t\t";\r\ne->devid_start = m->devid;\r\ne->devid_end = m->aux;\r\nbreak;\r\n}\r\ne->address_start = PAGE_ALIGN(m->range_start);\r\ne->address_end = e->address_start + PAGE_ALIGN(m->range_length);\r\ne->prot = m->flags >> 1;\r\nDUMP_printk("%s devid_start: %02x:%02x.%x devid_end: %02x:%02x.%x"\r\n" range_start: %016llx range_end: %016llx flags: %x\n", s,\r\nPCI_BUS_NUM(e->devid_start), PCI_SLOT(e->devid_start),\r\nPCI_FUNC(e->devid_start), PCI_BUS_NUM(e->devid_end),\r\nPCI_SLOT(e->devid_end), PCI_FUNC(e->devid_end),\r\ne->address_start, e->address_end, m->flags);\r\nlist_add_tail(&e->list, &amd_iommu_unity_map);\r\nreturn 0;\r\n}\r\nstatic int __init init_memory_definitions(struct acpi_table_header *table)\r\n{\r\nu8 *p = (u8 *)table, *end = (u8 *)table;\r\nstruct ivmd_header *m;\r\nend += table->length;\r\np += IVRS_HEADER_LENGTH;\r\nwhile (p < end) {\r\nm = (struct ivmd_header *)p;\r\nif (m->flags & IVMD_FLAG_EXCL_RANGE)\r\ninit_exclusion_range(m);\r\nelse if (m->flags & IVMD_FLAG_UNITY_MAP)\r\ninit_unity_map_range(m);\r\np += m->length;\r\n}\r\nreturn 0;\r\n}\r\nstatic void init_device_table_dma(void)\r\n{\r\nu32 devid;\r\nfor (devid = 0; devid <= amd_iommu_last_bdf; ++devid) {\r\nset_dev_entry_bit(devid, DEV_ENTRY_VALID);\r\nset_dev_entry_bit(devid, DEV_ENTRY_TRANSLATION);\r\n}\r\n}\r\nstatic void __init uninit_device_table_dma(void)\r\n{\r\nu32 devid;\r\nfor (devid = 0; devid <= amd_iommu_last_bdf; ++devid) {\r\namd_iommu_dev_table[devid].data[0] = 0ULL;\r\namd_iommu_dev_table[devid].data[1] = 0ULL;\r\n}\r\n}\r\nstatic void init_device_table(void)\r\n{\r\nu32 devid;\r\nif (!amd_iommu_irq_remap)\r\nreturn;\r\nfor (devid = 0; devid <= amd_iommu_last_bdf; ++devid)\r\nset_dev_entry_bit(devid, DEV_ENTRY_IRQ_TBL_EN);\r\n}\r\nstatic void iommu_init_flags(struct amd_iommu *iommu)\r\n{\r\niommu->acpi_flags & IVHD_FLAG_HT_TUN_EN_MASK ?\r\niommu_feature_enable(iommu, CONTROL_HT_TUN_EN) :\r\niommu_feature_disable(iommu, CONTROL_HT_TUN_EN);\r\niommu->acpi_flags & IVHD_FLAG_PASSPW_EN_MASK ?\r\niommu_feature_enable(iommu, CONTROL_PASSPW_EN) :\r\niommu_feature_disable(iommu, CONTROL_PASSPW_EN);\r\niommu->acpi_flags & IVHD_FLAG_RESPASSPW_EN_MASK ?\r\niommu_feature_enable(iommu, CONTROL_RESPASSPW_EN) :\r\niommu_feature_disable(iommu, CONTROL_RESPASSPW_EN);\r\niommu->acpi_flags & IVHD_FLAG_ISOC_EN_MASK ?\r\niommu_feature_enable(iommu, CONTROL_ISOC_EN) :\r\niommu_feature_disable(iommu, CONTROL_ISOC_EN);\r\niommu_feature_enable(iommu, CONTROL_COHERENT_EN);\r\niommu_set_inv_tlb_timeout(iommu, CTRL_INV_TO_1S);\r\n}\r\nstatic void iommu_apply_resume_quirks(struct amd_iommu *iommu)\r\n{\r\nint i, j;\r\nu32 ioc_feature_control;\r\nstruct pci_dev *pdev = iommu->root_pdev;\r\nif (!is_rd890_iommu(iommu->dev) || !pdev)\r\nreturn;\r\npci_write_config_dword(pdev, 0x60, 0x75 | (1 << 7));\r\npci_read_config_dword(pdev, 0x64, &ioc_feature_control);\r\nif (!(ioc_feature_control & 0x1))\r\npci_write_config_dword(pdev, 0x64, ioc_feature_control | 1);\r\npci_write_config_dword(iommu->dev, iommu->cap_ptr + 4,\r\niommu->stored_addr_lo);\r\npci_write_config_dword(iommu->dev, iommu->cap_ptr + 8,\r\niommu->stored_addr_hi);\r\nfor (i = 0; i < 6; i++)\r\nfor (j = 0; j < 0x12; j++)\r\niommu_write_l1(iommu, i, j, iommu->stored_l1[i][j]);\r\nfor (i = 0; i < 0x83; i++)\r\niommu_write_l2(iommu, i, iommu->stored_l2[i]);\r\npci_write_config_dword(iommu->dev, iommu->cap_ptr + 4,\r\niommu->stored_addr_lo | 1);\r\n}\r\nstatic void early_enable_iommus(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nfor_each_iommu(iommu) {\r\niommu_disable(iommu);\r\niommu_init_flags(iommu);\r\niommu_set_device_table(iommu);\r\niommu_enable_command_buffer(iommu);\r\niommu_enable_event_buffer(iommu);\r\niommu_set_exclusion_range(iommu);\r\niommu_enable(iommu);\r\niommu_flush_all_caches(iommu);\r\n}\r\n}\r\nstatic void enable_iommus_v2(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nfor_each_iommu(iommu) {\r\niommu_enable_ppr_log(iommu);\r\niommu_enable_gt(iommu);\r\n}\r\n}\r\nstatic void enable_iommus(void)\r\n{\r\nearly_enable_iommus();\r\nenable_iommus_v2();\r\n}\r\nstatic void disable_iommus(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nfor_each_iommu(iommu)\r\niommu_disable(iommu);\r\n}\r\nstatic void amd_iommu_resume(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nfor_each_iommu(iommu)\r\niommu_apply_resume_quirks(iommu);\r\nenable_iommus();\r\namd_iommu_enable_interrupts();\r\n}\r\nstatic int amd_iommu_suspend(void)\r\n{\r\ndisable_iommus();\r\nreturn 0;\r\n}\r\nstatic void __init free_on_init_error(void)\r\n{\r\nfree_pages((unsigned long)irq_lookup_table,\r\nget_order(rlookup_table_size));\r\nkmem_cache_destroy(amd_iommu_irq_cache);\r\namd_iommu_irq_cache = NULL;\r\nfree_pages((unsigned long)amd_iommu_rlookup_table,\r\nget_order(rlookup_table_size));\r\nfree_pages((unsigned long)amd_iommu_alias_table,\r\nget_order(alias_table_size));\r\nfree_pages((unsigned long)amd_iommu_dev_table,\r\nget_order(dev_table_size));\r\nfree_iommu_all();\r\n#ifdef CONFIG_GART_IOMMU\r\ngart_iommu_init();\r\n#endif\r\n}\r\nstatic bool __init check_ioapic_information(void)\r\n{\r\nconst char *fw_bug = FW_BUG;\r\nbool ret, has_sb_ioapic;\r\nint idx;\r\nhas_sb_ioapic = false;\r\nret = false;\r\nif (cmdline_maps)\r\nfw_bug = "";\r\nfor (idx = 0; idx < nr_ioapics; idx++) {\r\nint devid, id = mpc_ioapic_id(idx);\r\ndevid = get_ioapic_devid(id);\r\nif (devid < 0) {\r\npr_err("%sAMD-Vi: IOAPIC[%d] not in IVRS table\n",\r\nfw_bug, id);\r\nret = false;\r\n} else if (devid == IOAPIC_SB_DEVID) {\r\nhas_sb_ioapic = true;\r\nret = true;\r\n}\r\n}\r\nif (!has_sb_ioapic) {\r\npr_err("%sAMD-Vi: No southbridge IOAPIC found\n", fw_bug);\r\n}\r\nif (!ret)\r\npr_err("AMD-Vi: Disabling interrupt remapping\n");\r\nreturn ret;\r\n}\r\nstatic void __init free_dma_resources(void)\r\n{\r\nfree_pages((unsigned long)amd_iommu_pd_alloc_bitmap,\r\nget_order(MAX_DOMAIN_ID/8));\r\nfree_unity_maps();\r\n}\r\nstatic int __init early_amd_iommu_init(void)\r\n{\r\nstruct acpi_table_header *ivrs_base;\r\nacpi_size ivrs_size;\r\nacpi_status status;\r\nint i, ret = 0;\r\nif (!amd_iommu_detected)\r\nreturn -ENODEV;\r\nstatus = acpi_get_table_with_size("IVRS", 0, &ivrs_base, &ivrs_size);\r\nif (status == AE_NOT_FOUND)\r\nreturn -ENODEV;\r\nelse if (ACPI_FAILURE(status)) {\r\nconst char *err = acpi_format_exception(status);\r\npr_err("AMD-Vi: IVRS table error: %s\n", err);\r\nreturn -EINVAL;\r\n}\r\nret = check_ivrs_checksum(ivrs_base);\r\nif (ret)\r\nreturn ret;\r\namd_iommu_target_ivhd_type = get_highest_supported_ivhd_type(ivrs_base);\r\nDUMP_printk("Using IVHD type %#x\n", amd_iommu_target_ivhd_type);\r\nret = find_last_devid_acpi(ivrs_base);\r\nif (ret)\r\ngoto out;\r\ndev_table_size = tbl_size(DEV_TABLE_ENTRY_SIZE);\r\nalias_table_size = tbl_size(ALIAS_TABLE_ENTRY_SIZE);\r\nrlookup_table_size = tbl_size(RLOOKUP_TABLE_ENTRY_SIZE);\r\nret = -ENOMEM;\r\namd_iommu_dev_table = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\r\nget_order(dev_table_size));\r\nif (amd_iommu_dev_table == NULL)\r\ngoto out;\r\namd_iommu_alias_table = (void *)__get_free_pages(GFP_KERNEL,\r\nget_order(alias_table_size));\r\nif (amd_iommu_alias_table == NULL)\r\ngoto out;\r\namd_iommu_rlookup_table = (void *)__get_free_pages(\r\nGFP_KERNEL | __GFP_ZERO,\r\nget_order(rlookup_table_size));\r\nif (amd_iommu_rlookup_table == NULL)\r\ngoto out;\r\namd_iommu_pd_alloc_bitmap = (void *)__get_free_pages(\r\nGFP_KERNEL | __GFP_ZERO,\r\nget_order(MAX_DOMAIN_ID/8));\r\nif (amd_iommu_pd_alloc_bitmap == NULL)\r\ngoto out;\r\nfor (i = 0; i <= amd_iommu_last_bdf; ++i)\r\namd_iommu_alias_table[i] = i;\r\namd_iommu_pd_alloc_bitmap[0] = 1;\r\nspin_lock_init(&amd_iommu_pd_lock);\r\nret = init_iommu_all(ivrs_base);\r\nif (ret)\r\ngoto out;\r\nif (amd_iommu_irq_remap)\r\namd_iommu_irq_remap = check_ioapic_information();\r\nif (amd_iommu_irq_remap) {\r\nret = -ENOMEM;\r\namd_iommu_irq_cache = kmem_cache_create("irq_remap_cache",\r\nMAX_IRQS_PER_TABLE * sizeof(u32),\r\nIRQ_TABLE_ALIGNMENT,\r\n0, NULL);\r\nif (!amd_iommu_irq_cache)\r\ngoto out;\r\nirq_lookup_table = (void *)__get_free_pages(\r\nGFP_KERNEL | __GFP_ZERO,\r\nget_order(rlookup_table_size));\r\nif (!irq_lookup_table)\r\ngoto out;\r\n}\r\nret = init_memory_definitions(ivrs_base);\r\nif (ret)\r\ngoto out;\r\ninit_device_table();\r\nout:\r\nearly_acpi_os_unmap_memory((char __iomem *)ivrs_base, ivrs_size);\r\nivrs_base = NULL;\r\nreturn ret;\r\n}\r\nstatic int amd_iommu_enable_interrupts(void)\r\n{\r\nstruct amd_iommu *iommu;\r\nint ret = 0;\r\nfor_each_iommu(iommu) {\r\nret = iommu_init_msi(iommu);\r\nif (ret)\r\ngoto out;\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic bool detect_ivrs(void)\r\n{\r\nstruct acpi_table_header *ivrs_base;\r\nacpi_size ivrs_size;\r\nacpi_status status;\r\nstatus = acpi_get_table_with_size("IVRS", 0, &ivrs_base, &ivrs_size);\r\nif (status == AE_NOT_FOUND)\r\nreturn false;\r\nelse if (ACPI_FAILURE(status)) {\r\nconst char *err = acpi_format_exception(status);\r\npr_err("AMD-Vi: IVRS table error: %s\n", err);\r\nreturn false;\r\n}\r\nearly_acpi_os_unmap_memory((char __iomem *)ivrs_base, ivrs_size);\r\npci_request_acs();\r\nreturn true;\r\n}\r\nstatic int __init state_next(void)\r\n{\r\nint ret = 0;\r\nswitch (init_state) {\r\ncase IOMMU_START_STATE:\r\nif (!detect_ivrs()) {\r\ninit_state = IOMMU_NOT_FOUND;\r\nret = -ENODEV;\r\n} else {\r\ninit_state = IOMMU_IVRS_DETECTED;\r\n}\r\nbreak;\r\ncase IOMMU_IVRS_DETECTED:\r\nret = early_amd_iommu_init();\r\ninit_state = ret ? IOMMU_INIT_ERROR : IOMMU_ACPI_FINISHED;\r\nbreak;\r\ncase IOMMU_ACPI_FINISHED:\r\nearly_enable_iommus();\r\nregister_syscore_ops(&amd_iommu_syscore_ops);\r\nx86_platform.iommu_shutdown = disable_iommus;\r\ninit_state = IOMMU_ENABLED;\r\nbreak;\r\ncase IOMMU_ENABLED:\r\nret = amd_iommu_init_pci();\r\ninit_state = ret ? IOMMU_INIT_ERROR : IOMMU_PCI_INIT;\r\nenable_iommus_v2();\r\nbreak;\r\ncase IOMMU_PCI_INIT:\r\nret = amd_iommu_enable_interrupts();\r\ninit_state = ret ? IOMMU_INIT_ERROR : IOMMU_INTERRUPTS_EN;\r\nbreak;\r\ncase IOMMU_INTERRUPTS_EN:\r\nret = amd_iommu_init_dma_ops();\r\ninit_state = ret ? IOMMU_INIT_ERROR : IOMMU_DMA_OPS;\r\nbreak;\r\ncase IOMMU_DMA_OPS:\r\ninit_state = IOMMU_INITIALIZED;\r\nbreak;\r\ncase IOMMU_INITIALIZED:\r\nbreak;\r\ncase IOMMU_NOT_FOUND:\r\ncase IOMMU_INIT_ERROR:\r\nret = -EINVAL;\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nreturn ret;\r\n}\r\nstatic int __init iommu_go_to_state(enum iommu_init_state state)\r\n{\r\nint ret = 0;\r\nwhile (init_state != state) {\r\nret = state_next();\r\nif (init_state == IOMMU_NOT_FOUND ||\r\ninit_state == IOMMU_INIT_ERROR)\r\nbreak;\r\n}\r\nreturn ret;\r\n}\r\nint __init amd_iommu_prepare(void)\r\n{\r\nint ret;\r\namd_iommu_irq_remap = true;\r\nret = iommu_go_to_state(IOMMU_ACPI_FINISHED);\r\nif (ret)\r\nreturn ret;\r\nreturn amd_iommu_irq_remap ? 0 : -ENODEV;\r\n}\r\nint __init amd_iommu_enable(void)\r\n{\r\nint ret;\r\nret = iommu_go_to_state(IOMMU_ENABLED);\r\nif (ret)\r\nreturn ret;\r\nirq_remapping_enabled = 1;\r\nreturn 0;\r\n}\r\nvoid amd_iommu_disable(void)\r\n{\r\namd_iommu_suspend();\r\n}\r\nint amd_iommu_reenable(int mode)\r\n{\r\namd_iommu_resume();\r\nreturn 0;\r\n}\r\nint __init amd_iommu_enable_faulting(void)\r\n{\r\nreturn 0;\r\n}\r\nstatic int __init amd_iommu_init(void)\r\n{\r\nint ret;\r\nret = iommu_go_to_state(IOMMU_INITIALIZED);\r\nif (ret) {\r\nfree_dma_resources();\r\nif (!irq_remapping_enabled) {\r\ndisable_iommus();\r\nfree_on_init_error();\r\n} else {\r\nstruct amd_iommu *iommu;\r\nuninit_device_table_dma();\r\nfor_each_iommu(iommu)\r\niommu_flush_all_caches(iommu);\r\n}\r\n}\r\nreturn ret;\r\n}\r\nint __init amd_iommu_detect(void)\r\n{\r\nint ret;\r\nif (no_iommu || (iommu_detected && !gart_iommu_aperture))\r\nreturn -ENODEV;\r\nif (amd_iommu_disabled)\r\nreturn -ENODEV;\r\nret = iommu_go_to_state(IOMMU_IVRS_DETECTED);\r\nif (ret)\r\nreturn ret;\r\namd_iommu_detected = true;\r\niommu_detected = 1;\r\nx86_init.iommu.iommu_init = amd_iommu_init;\r\nreturn 1;\r\n}\r\nstatic int __init parse_amd_iommu_dump(char *str)\r\n{\r\namd_iommu_dump = true;\r\nreturn 1;\r\n}\r\nstatic int __init parse_amd_iommu_options(char *str)\r\n{\r\nfor (; *str; ++str) {\r\nif (strncmp(str, "fullflush", 9) == 0)\r\namd_iommu_unmap_flush = true;\r\nif (strncmp(str, "off", 3) == 0)\r\namd_iommu_disabled = true;\r\nif (strncmp(str, "force_isolation", 15) == 0)\r\namd_iommu_force_isolation = true;\r\n}\r\nreturn 1;\r\n}\r\nstatic int __init parse_ivrs_ioapic(char *str)\r\n{\r\nunsigned int bus, dev, fn;\r\nint ret, id, i;\r\nu16 devid;\r\nret = sscanf(str, "[%d]=%x:%x.%x", &id, &bus, &dev, &fn);\r\nif (ret != 4) {\r\npr_err("AMD-Vi: Invalid command line: ivrs_ioapic%s\n", str);\r\nreturn 1;\r\n}\r\nif (early_ioapic_map_size == EARLY_MAP_SIZE) {\r\npr_err("AMD-Vi: Early IOAPIC map overflow - ignoring ivrs_ioapic%s\n",\r\nstr);\r\nreturn 1;\r\n}\r\ndevid = ((bus & 0xff) << 8) | ((dev & 0x1f) << 3) | (fn & 0x7);\r\ncmdline_maps = true;\r\ni = early_ioapic_map_size++;\r\nearly_ioapic_map[i].id = id;\r\nearly_ioapic_map[i].devid = devid;\r\nearly_ioapic_map[i].cmd_line = true;\r\nreturn 1;\r\n}\r\nstatic int __init parse_ivrs_hpet(char *str)\r\n{\r\nunsigned int bus, dev, fn;\r\nint ret, id, i;\r\nu16 devid;\r\nret = sscanf(str, "[%d]=%x:%x.%x", &id, &bus, &dev, &fn);\r\nif (ret != 4) {\r\npr_err("AMD-Vi: Invalid command line: ivrs_hpet%s\n", str);\r\nreturn 1;\r\n}\r\nif (early_hpet_map_size == EARLY_MAP_SIZE) {\r\npr_err("AMD-Vi: Early HPET map overflow - ignoring ivrs_hpet%s\n",\r\nstr);\r\nreturn 1;\r\n}\r\ndevid = ((bus & 0xff) << 8) | ((dev & 0x1f) << 3) | (fn & 0x7);\r\ncmdline_maps = true;\r\ni = early_hpet_map_size++;\r\nearly_hpet_map[i].id = id;\r\nearly_hpet_map[i].devid = devid;\r\nearly_hpet_map[i].cmd_line = true;\r\nreturn 1;\r\n}\r\nstatic int __init parse_ivrs_acpihid(char *str)\r\n{\r\nu32 bus, dev, fn;\r\nchar *hid, *uid, *p;\r\nchar acpiid[ACPIHID_UID_LEN + ACPIHID_HID_LEN] = {0};\r\nint ret, i;\r\nret = sscanf(str, "[%x:%x.%x]=%s", &bus, &dev, &fn, acpiid);\r\nif (ret != 4) {\r\npr_err("AMD-Vi: Invalid command line: ivrs_acpihid(%s)\n", str);\r\nreturn 1;\r\n}\r\np = acpiid;\r\nhid = strsep(&p, ":");\r\nuid = p;\r\nif (!hid || !(*hid) || !uid) {\r\npr_err("AMD-Vi: Invalid command line: hid or uid\n");\r\nreturn 1;\r\n}\r\ni = early_acpihid_map_size++;\r\nmemcpy(early_acpihid_map[i].hid, hid, strlen(hid));\r\nmemcpy(early_acpihid_map[i].uid, uid, strlen(uid));\r\nearly_acpihid_map[i].devid =\r\n((bus & 0xff) << 8) | ((dev & 0x1f) << 3) | (fn & 0x7);\r\nearly_acpihid_map[i].cmd_line = true;\r\nreturn 1;\r\n}\r\nbool amd_iommu_v2_supported(void)\r\n{\r\nreturn amd_iommu_v2_present;\r\n}\r\nu8 amd_iommu_pc_get_max_banks(u16 devid)\r\n{\r\nstruct amd_iommu *iommu;\r\nu8 ret = 0;\r\niommu = amd_iommu_rlookup_table[devid];\r\nif (iommu)\r\nret = iommu->max_banks;\r\nreturn ret;\r\n}\r\nbool amd_iommu_pc_supported(void)\r\n{\r\nreturn amd_iommu_pc_present;\r\n}\r\nu8 amd_iommu_pc_get_max_counters(u16 devid)\r\n{\r\nstruct amd_iommu *iommu;\r\nu8 ret = 0;\r\niommu = amd_iommu_rlookup_table[devid];\r\nif (iommu)\r\nret = iommu->max_counters;\r\nreturn ret;\r\n}\r\nstatic int iommu_pc_get_set_reg_val(struct amd_iommu *iommu,\r\nu8 bank, u8 cntr, u8 fxn,\r\nu64 *value, bool is_write)\r\n{\r\nu32 offset;\r\nu32 max_offset_lim;\r\nif (WARN_ON((fxn > 0x28) || (fxn & 7)))\r\nreturn -ENODEV;\r\noffset = (u32)(((0x40|bank) << 12) | (cntr << 8) | fxn);\r\nmax_offset_lim = (u32)(((0x40|iommu->max_banks) << 12) |\r\n(iommu->max_counters << 8) | 0x28);\r\nif ((offset < MMIO_CNTR_REG_OFFSET) ||\r\n(offset > max_offset_lim))\r\nreturn -EINVAL;\r\nif (is_write) {\r\nwritel((u32)*value, iommu->mmio_base + offset);\r\nwritel((*value >> 32), iommu->mmio_base + offset + 4);\r\n} else {\r\n*value = readl(iommu->mmio_base + offset + 4);\r\n*value <<= 32;\r\n*value = readl(iommu->mmio_base + offset);\r\n}\r\nreturn 0;\r\n}\r\nint amd_iommu_pc_get_set_reg_val(u16 devid, u8 bank, u8 cntr, u8 fxn,\r\nu64 *value, bool is_write)\r\n{\r\nstruct amd_iommu *iommu = amd_iommu_rlookup_table[devid];\r\nif (!amd_iommu_pc_present || iommu == NULL)\r\nreturn -ENODEV;\r\nreturn iommu_pc_get_set_reg_val(iommu, bank, cntr, fxn,\r\nvalue, is_write);\r\n}
