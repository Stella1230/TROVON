static int madvise_need_mmap_write(int behavior)\r\n{\r\nswitch (behavior) {\r\ncase MADV_REMOVE:\r\ncase MADV_WILLNEED:\r\ncase MADV_DONTNEED:\r\ncase MADV_FREE:\r\nreturn 0;\r\ndefault:\r\nreturn 1;\r\n}\r\n}\r\nstatic long madvise_behavior(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end, int behavior)\r\n{\r\nstruct mm_struct *mm = vma->vm_mm;\r\nint error = 0;\r\npgoff_t pgoff;\r\nunsigned long new_flags = vma->vm_flags;\r\nswitch (behavior) {\r\ncase MADV_NORMAL:\r\nnew_flags = new_flags & ~VM_RAND_READ & ~VM_SEQ_READ;\r\nbreak;\r\ncase MADV_SEQUENTIAL:\r\nnew_flags = (new_flags & ~VM_RAND_READ) | VM_SEQ_READ;\r\nbreak;\r\ncase MADV_RANDOM:\r\nnew_flags = (new_flags & ~VM_SEQ_READ) | VM_RAND_READ;\r\nbreak;\r\ncase MADV_DONTFORK:\r\nnew_flags |= VM_DONTCOPY;\r\nbreak;\r\ncase MADV_DOFORK:\r\nif (vma->vm_flags & VM_IO) {\r\nerror = -EINVAL;\r\ngoto out;\r\n}\r\nnew_flags &= ~VM_DONTCOPY;\r\nbreak;\r\ncase MADV_DONTDUMP:\r\nnew_flags |= VM_DONTDUMP;\r\nbreak;\r\ncase MADV_DODUMP:\r\nif (new_flags & VM_SPECIAL) {\r\nerror = -EINVAL;\r\ngoto out;\r\n}\r\nnew_flags &= ~VM_DONTDUMP;\r\nbreak;\r\ncase MADV_MERGEABLE:\r\ncase MADV_UNMERGEABLE:\r\nerror = ksm_madvise(vma, start, end, behavior, &new_flags);\r\nif (error)\r\ngoto out;\r\nbreak;\r\ncase MADV_HUGEPAGE:\r\ncase MADV_NOHUGEPAGE:\r\nerror = hugepage_madvise(vma, &new_flags, behavior);\r\nif (error)\r\ngoto out;\r\nbreak;\r\n}\r\nif (new_flags == vma->vm_flags) {\r\n*prev = vma;\r\ngoto out;\r\n}\r\npgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\r\n*prev = vma_merge(mm, *prev, start, end, new_flags, vma->anon_vma,\r\nvma->vm_file, pgoff, vma_policy(vma),\r\nvma->vm_userfaultfd_ctx);\r\nif (*prev) {\r\nvma = *prev;\r\ngoto success;\r\n}\r\n*prev = vma;\r\nif (start != vma->vm_start) {\r\nerror = split_vma(mm, vma, start, 1);\r\nif (error)\r\ngoto out;\r\n}\r\nif (end != vma->vm_end) {\r\nerror = split_vma(mm, vma, end, 0);\r\nif (error)\r\ngoto out;\r\n}\r\nsuccess:\r\nvma->vm_flags = new_flags;\r\nout:\r\nif (error == -ENOMEM)\r\nerror = -EAGAIN;\r\nreturn error;\r\n}\r\nstatic int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,\r\nunsigned long end, struct mm_walk *walk)\r\n{\r\npte_t *orig_pte;\r\nstruct vm_area_struct *vma = walk->private;\r\nunsigned long index;\r\nif (pmd_none_or_trans_huge_or_clear_bad(pmd))\r\nreturn 0;\r\nfor (index = start; index != end; index += PAGE_SIZE) {\r\npte_t pte;\r\nswp_entry_t entry;\r\nstruct page *page;\r\nspinlock_t *ptl;\r\norig_pte = pte_offset_map_lock(vma->vm_mm, pmd, start, &ptl);\r\npte = *(orig_pte + ((index - start) / PAGE_SIZE));\r\npte_unmap_unlock(orig_pte, ptl);\r\nif (pte_present(pte) || pte_none(pte))\r\ncontinue;\r\nentry = pte_to_swp_entry(pte);\r\nif (unlikely(non_swap_entry(entry)))\r\ncontinue;\r\npage = read_swap_cache_async(entry, GFP_HIGHUSER_MOVABLE,\r\nvma, index);\r\nif (page)\r\nput_page(page);\r\n}\r\nreturn 0;\r\n}\r\nstatic void force_swapin_readahead(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct mm_walk walk = {\r\n.mm = vma->vm_mm,\r\n.pmd_entry = swapin_walk_pmd_entry,\r\n.private = vma,\r\n};\r\nwalk_page_range(start, end, &walk);\r\nlru_add_drain();\r\n}\r\nstatic void force_shm_swapin_readahead(struct vm_area_struct *vma,\r\nunsigned long start, unsigned long end,\r\nstruct address_space *mapping)\r\n{\r\npgoff_t index;\r\nstruct page *page;\r\nswp_entry_t swap;\r\nfor (; start < end; start += PAGE_SIZE) {\r\nindex = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\npage = find_get_entry(mapping, index);\r\nif (!radix_tree_exceptional_entry(page)) {\r\nif (page)\r\nput_page(page);\r\ncontinue;\r\n}\r\nswap = radix_to_swp_entry(page);\r\npage = read_swap_cache_async(swap, GFP_HIGHUSER_MOVABLE,\r\nNULL, 0);\r\nif (page)\r\nput_page(page);\r\n}\r\nlru_add_drain();\r\n}\r\nstatic long madvise_willneed(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end)\r\n{\r\nstruct file *file = vma->vm_file;\r\n#ifdef CONFIG_SWAP\r\nif (!file) {\r\n*prev = vma;\r\nforce_swapin_readahead(vma, start, end);\r\nreturn 0;\r\n}\r\nif (shmem_mapping(file->f_mapping)) {\r\n*prev = vma;\r\nforce_shm_swapin_readahead(vma, start, end,\r\nfile->f_mapping);\r\nreturn 0;\r\n}\r\n#else\r\nif (!file)\r\nreturn -EBADF;\r\n#endif\r\nif (IS_DAX(file_inode(file))) {\r\nreturn 0;\r\n}\r\n*prev = vma;\r\nstart = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\nif (end > vma->vm_end)\r\nend = vma->vm_end;\r\nend = ((end - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;\r\nforce_page_cache_readahead(file->f_mapping, file, start, end - start);\r\nreturn 0;\r\n}\r\nstatic int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,\r\nunsigned long end, struct mm_walk *walk)\r\n{\r\nstruct mmu_gather *tlb = walk->private;\r\nstruct mm_struct *mm = tlb->mm;\r\nstruct vm_area_struct *vma = walk->vma;\r\nspinlock_t *ptl;\r\npte_t *orig_pte, *pte, ptent;\r\nstruct page *page;\r\nint nr_swap = 0;\r\nunsigned long next;\r\nnext = pmd_addr_end(addr, end);\r\nif (pmd_trans_huge(*pmd))\r\nif (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))\r\ngoto next;\r\nif (pmd_trans_unstable(pmd))\r\nreturn 0;\r\norig_pte = pte = pte_offset_map_lock(mm, pmd, addr, &ptl);\r\narch_enter_lazy_mmu_mode();\r\nfor (; addr != end; pte++, addr += PAGE_SIZE) {\r\nptent = *pte;\r\nif (pte_none(ptent))\r\ncontinue;\r\nif (!pte_present(ptent)) {\r\nswp_entry_t entry;\r\nentry = pte_to_swp_entry(ptent);\r\nif (non_swap_entry(entry))\r\ncontinue;\r\nnr_swap--;\r\nfree_swap_and_cache(entry);\r\npte_clear_not_present_full(mm, addr, pte, tlb->fullmm);\r\ncontinue;\r\n}\r\npage = vm_normal_page(vma, addr, ptent);\r\nif (!page)\r\ncontinue;\r\nif (PageTransCompound(page)) {\r\nif (page_mapcount(page) != 1)\r\ngoto out;\r\nget_page(page);\r\nif (!trylock_page(page)) {\r\nput_page(page);\r\ngoto out;\r\n}\r\npte_unmap_unlock(orig_pte, ptl);\r\nif (split_huge_page(page)) {\r\nunlock_page(page);\r\nput_page(page);\r\npte_offset_map_lock(mm, pmd, addr, &ptl);\r\ngoto out;\r\n}\r\nput_page(page);\r\nunlock_page(page);\r\npte = pte_offset_map_lock(mm, pmd, addr, &ptl);\r\npte--;\r\naddr -= PAGE_SIZE;\r\ncontinue;\r\n}\r\nVM_BUG_ON_PAGE(PageTransCompound(page), page);\r\nif (PageSwapCache(page) || PageDirty(page)) {\r\nif (!trylock_page(page))\r\ncontinue;\r\nif (page_mapcount(page) != 1) {\r\nunlock_page(page);\r\ncontinue;\r\n}\r\nif (PageSwapCache(page) && !try_to_free_swap(page)) {\r\nunlock_page(page);\r\ncontinue;\r\n}\r\nClearPageDirty(page);\r\nunlock_page(page);\r\n}\r\nif (pte_young(ptent) || pte_dirty(ptent)) {\r\nptent = ptep_get_and_clear_full(mm, addr, pte,\r\ntlb->fullmm);\r\nptent = pte_mkold(ptent);\r\nptent = pte_mkclean(ptent);\r\nset_pte_at(mm, addr, pte, ptent);\r\nif (PageActive(page))\r\ndeactivate_page(page);\r\ntlb_remove_tlb_entry(tlb, pte, addr);\r\n}\r\n}\r\nout:\r\nif (nr_swap) {\r\nif (current->mm == mm)\r\nsync_mm_rss(mm);\r\nadd_mm_counter(mm, MM_SWAPENTS, nr_swap);\r\n}\r\narch_leave_lazy_mmu_mode();\r\npte_unmap_unlock(orig_pte, ptl);\r\ncond_resched();\r\nnext:\r\nreturn 0;\r\n}\r\nstatic void madvise_free_page_range(struct mmu_gather *tlb,\r\nstruct vm_area_struct *vma,\r\nunsigned long addr, unsigned long end)\r\n{\r\nstruct mm_walk free_walk = {\r\n.pmd_entry = madvise_free_pte_range,\r\n.mm = vma->vm_mm,\r\n.private = tlb,\r\n};\r\ntlb_start_vma(tlb, vma);\r\nwalk_page_range(addr, end, &free_walk);\r\ntlb_end_vma(tlb, vma);\r\n}\r\nstatic int madvise_free_single_vma(struct vm_area_struct *vma,\r\nunsigned long start_addr, unsigned long end_addr)\r\n{\r\nunsigned long start, end;\r\nstruct mm_struct *mm = vma->vm_mm;\r\nstruct mmu_gather tlb;\r\nif (vma->vm_flags & (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))\r\nreturn -EINVAL;\r\nif (!vma_is_anonymous(vma))\r\nreturn -EINVAL;\r\nstart = max(vma->vm_start, start_addr);\r\nif (start >= vma->vm_end)\r\nreturn -EINVAL;\r\nend = min(vma->vm_end, end_addr);\r\nif (end <= vma->vm_start)\r\nreturn -EINVAL;\r\nlru_add_drain();\r\ntlb_gather_mmu(&tlb, mm, start, end);\r\nupdate_hiwater_rss(mm);\r\nmmu_notifier_invalidate_range_start(mm, start, end);\r\nmadvise_free_page_range(&tlb, vma, start, end);\r\nmmu_notifier_invalidate_range_end(mm, start, end);\r\ntlb_finish_mmu(&tlb, start, end);\r\nreturn 0;\r\n}\r\nstatic long madvise_free(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end)\r\n{\r\n*prev = vma;\r\nreturn madvise_free_single_vma(vma, start, end);\r\n}\r\nstatic long madvise_dontneed(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end)\r\n{\r\n*prev = vma;\r\nif (vma->vm_flags & (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))\r\nreturn -EINVAL;\r\nzap_page_range(vma, start, end - start, NULL);\r\nreturn 0;\r\n}\r\nstatic long madvise_remove(struct vm_area_struct *vma,\r\nstruct vm_area_struct **prev,\r\nunsigned long start, unsigned long end)\r\n{\r\nloff_t offset;\r\nint error;\r\nstruct file *f;\r\n*prev = NULL;\r\nif (vma->vm_flags & VM_LOCKED)\r\nreturn -EINVAL;\r\nf = vma->vm_file;\r\nif (!f || !f->f_mapping || !f->f_mapping->host) {\r\nreturn -EINVAL;\r\n}\r\nif ((vma->vm_flags & (VM_SHARED|VM_WRITE)) != (VM_SHARED|VM_WRITE))\r\nreturn -EACCES;\r\noffset = (loff_t)(start - vma->vm_start)\r\n+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);\r\nget_file(f);\r\nup_read(&current->mm->mmap_sem);\r\nerror = vfs_fallocate(f,\r\nFALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,\r\noffset, end - start);\r\nfput(f);\r\ndown_read(&current->mm->mmap_sem);\r\nreturn error;\r\n}\r\nstatic int madvise_hwpoison(int bhv, unsigned long start, unsigned long end)\r\n{\r\nstruct page *p;\r\nif (!capable(CAP_SYS_ADMIN))\r\nreturn -EPERM;\r\nfor (; start < end; start += PAGE_SIZE <<\r\ncompound_order(compound_head(p))) {\r\nint ret;\r\nret = get_user_pages_fast(start, 1, 0, &p);\r\nif (ret != 1)\r\nreturn ret;\r\nif (PageHWPoison(p)) {\r\nput_page(p);\r\ncontinue;\r\n}\r\nif (bhv == MADV_SOFT_OFFLINE) {\r\npr_info("Soft offlining page %#lx at %#lx\n",\r\npage_to_pfn(p), start);\r\nret = soft_offline_page(p, MF_COUNT_INCREASED);\r\nif (ret)\r\nreturn ret;\r\ncontinue;\r\n}\r\npr_info("Injecting memory failure for page %#lx at %#lx\n",\r\npage_to_pfn(p), start);\r\nret = memory_failure(page_to_pfn(p), 0, MF_COUNT_INCREASED);\r\nif (ret)\r\nreturn ret;\r\n}\r\nreturn 0;\r\n}\r\nstatic long\r\nmadvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,\r\nunsigned long start, unsigned long end, int behavior)\r\n{\r\nswitch (behavior) {\r\ncase MADV_REMOVE:\r\nreturn madvise_remove(vma, prev, start, end);\r\ncase MADV_WILLNEED:\r\nreturn madvise_willneed(vma, prev, start, end);\r\ncase MADV_FREE:\r\nif (get_nr_swap_pages() > 0)\r\nreturn madvise_free(vma, prev, start, end);\r\ncase MADV_DONTNEED:\r\nreturn madvise_dontneed(vma, prev, start, end);\r\ndefault:\r\nreturn madvise_behavior(vma, prev, start, end, behavior);\r\n}\r\n}\r\nstatic bool\r\nmadvise_behavior_valid(int behavior)\r\n{\r\nswitch (behavior) {\r\ncase MADV_DOFORK:\r\ncase MADV_DONTFORK:\r\ncase MADV_NORMAL:\r\ncase MADV_SEQUENTIAL:\r\ncase MADV_RANDOM:\r\ncase MADV_REMOVE:\r\ncase MADV_WILLNEED:\r\ncase MADV_DONTNEED:\r\ncase MADV_FREE:\r\n#ifdef CONFIG_KSM\r\ncase MADV_MERGEABLE:\r\ncase MADV_UNMERGEABLE:\r\n#endif\r\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\r\ncase MADV_HUGEPAGE:\r\ncase MADV_NOHUGEPAGE:\r\n#endif\r\ncase MADV_DONTDUMP:\r\ncase MADV_DODUMP:\r\nreturn true;\r\ndefault:\r\nreturn false;\r\n}\r\n}
