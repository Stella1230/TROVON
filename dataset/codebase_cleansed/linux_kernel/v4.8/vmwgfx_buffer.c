static bool __vmw_piter_non_sg_next(struct vmw_piter *viter)\r\n{\r\nreturn ++(viter->i) < viter->num_pages;\r\n}\r\nstatic bool __vmw_piter_sg_next(struct vmw_piter *viter)\r\n{\r\nreturn __sg_page_iter_next(&viter->iter);\r\n}\r\nstatic struct page *__vmw_piter_non_sg_page(struct vmw_piter *viter)\r\n{\r\nreturn viter->pages[viter->i];\r\n}\r\nstatic struct page *__vmw_piter_sg_page(struct vmw_piter *viter)\r\n{\r\nreturn sg_page_iter_page(&viter->iter);\r\n}\r\nstatic dma_addr_t __vmw_piter_phys_addr(struct vmw_piter *viter)\r\n{\r\nreturn page_to_phys(viter->pages[viter->i]);\r\n}\r\nstatic dma_addr_t __vmw_piter_dma_addr(struct vmw_piter *viter)\r\n{\r\nreturn viter->addrs[viter->i];\r\n}\r\nstatic dma_addr_t __vmw_piter_sg_addr(struct vmw_piter *viter)\r\n{\r\nreturn sg_page_iter_dma_address(&viter->iter);\r\n}\r\nvoid vmw_piter_start(struct vmw_piter *viter, const struct vmw_sg_table *vsgt,\r\nunsigned long p_offset)\r\n{\r\nviter->i = p_offset - 1;\r\nviter->num_pages = vsgt->num_pages;\r\nswitch (vsgt->mode) {\r\ncase vmw_dma_phys:\r\nviter->next = &__vmw_piter_non_sg_next;\r\nviter->dma_address = &__vmw_piter_phys_addr;\r\nviter->page = &__vmw_piter_non_sg_page;\r\nviter->pages = vsgt->pages;\r\nbreak;\r\ncase vmw_dma_alloc_coherent:\r\nviter->next = &__vmw_piter_non_sg_next;\r\nviter->dma_address = &__vmw_piter_dma_addr;\r\nviter->page = &__vmw_piter_non_sg_page;\r\nviter->addrs = vsgt->addrs;\r\nviter->pages = vsgt->pages;\r\nbreak;\r\ncase vmw_dma_map_populate:\r\ncase vmw_dma_map_bind:\r\nviter->next = &__vmw_piter_sg_next;\r\nviter->dma_address = &__vmw_piter_sg_addr;\r\nviter->page = &__vmw_piter_sg_page;\r\n__sg_page_iter_start(&viter->iter, vsgt->sgt->sgl,\r\nvsgt->sgt->orig_nents, p_offset);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n}\r\nstatic void vmw_ttm_unmap_from_dma(struct vmw_ttm_tt *vmw_tt)\r\n{\r\nstruct device *dev = vmw_tt->dev_priv->dev->dev;\r\ndma_unmap_sg(dev, vmw_tt->sgt.sgl, vmw_tt->sgt.nents,\r\nDMA_BIDIRECTIONAL);\r\nvmw_tt->sgt.nents = vmw_tt->sgt.orig_nents;\r\n}\r\nstatic int vmw_ttm_map_for_dma(struct vmw_ttm_tt *vmw_tt)\r\n{\r\nstruct device *dev = vmw_tt->dev_priv->dev->dev;\r\nint ret;\r\nret = dma_map_sg(dev, vmw_tt->sgt.sgl, vmw_tt->sgt.orig_nents,\r\nDMA_BIDIRECTIONAL);\r\nif (unlikely(ret == 0))\r\nreturn -ENOMEM;\r\nvmw_tt->sgt.nents = ret;\r\nreturn 0;\r\n}\r\nstatic int vmw_ttm_map_dma(struct vmw_ttm_tt *vmw_tt)\r\n{\r\nstruct vmw_private *dev_priv = vmw_tt->dev_priv;\r\nstruct ttm_mem_global *glob = vmw_mem_glob(dev_priv);\r\nstruct vmw_sg_table *vsgt = &vmw_tt->vsgt;\r\nstruct vmw_piter iter;\r\ndma_addr_t old;\r\nint ret = 0;\r\nstatic size_t sgl_size;\r\nstatic size_t sgt_size;\r\nif (vmw_tt->mapped)\r\nreturn 0;\r\nvsgt->mode = dev_priv->map_mode;\r\nvsgt->pages = vmw_tt->dma_ttm.ttm.pages;\r\nvsgt->num_pages = vmw_tt->dma_ttm.ttm.num_pages;\r\nvsgt->addrs = vmw_tt->dma_ttm.dma_address;\r\nvsgt->sgt = &vmw_tt->sgt;\r\nswitch (dev_priv->map_mode) {\r\ncase vmw_dma_map_bind:\r\ncase vmw_dma_map_populate:\r\nif (unlikely(!sgl_size)) {\r\nsgl_size = ttm_round_pot(sizeof(struct scatterlist));\r\nsgt_size = ttm_round_pot(sizeof(struct sg_table));\r\n}\r\nvmw_tt->sg_alloc_size = sgt_size + sgl_size * vsgt->num_pages;\r\nret = ttm_mem_global_alloc(glob, vmw_tt->sg_alloc_size, false,\r\ntrue);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = sg_alloc_table_from_pages(&vmw_tt->sgt, vsgt->pages,\r\nvsgt->num_pages, 0,\r\n(unsigned long)\r\nvsgt->num_pages << PAGE_SHIFT,\r\nGFP_KERNEL);\r\nif (unlikely(ret != 0))\r\ngoto out_sg_alloc_fail;\r\nif (vsgt->num_pages > vmw_tt->sgt.nents) {\r\nuint64_t over_alloc =\r\nsgl_size * (vsgt->num_pages -\r\nvmw_tt->sgt.nents);\r\nttm_mem_global_free(glob, over_alloc);\r\nvmw_tt->sg_alloc_size -= over_alloc;\r\n}\r\nret = vmw_ttm_map_for_dma(vmw_tt);\r\nif (unlikely(ret != 0))\r\ngoto out_map_fail;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nold = ~((dma_addr_t) 0);\r\nvmw_tt->vsgt.num_regions = 0;\r\nfor (vmw_piter_start(&iter, vsgt, 0); vmw_piter_next(&iter);) {\r\ndma_addr_t cur = vmw_piter_dma_addr(&iter);\r\nif (cur != old + PAGE_SIZE)\r\nvmw_tt->vsgt.num_regions++;\r\nold = cur;\r\n}\r\nvmw_tt->mapped = true;\r\nreturn 0;\r\nout_map_fail:\r\nsg_free_table(vmw_tt->vsgt.sgt);\r\nvmw_tt->vsgt.sgt = NULL;\r\nout_sg_alloc_fail:\r\nttm_mem_global_free(glob, vmw_tt->sg_alloc_size);\r\nreturn ret;\r\n}\r\nstatic void vmw_ttm_unmap_dma(struct vmw_ttm_tt *vmw_tt)\r\n{\r\nstruct vmw_private *dev_priv = vmw_tt->dev_priv;\r\nif (!vmw_tt->vsgt.sgt)\r\nreturn;\r\nswitch (dev_priv->map_mode) {\r\ncase vmw_dma_map_bind:\r\ncase vmw_dma_map_populate:\r\nvmw_ttm_unmap_from_dma(vmw_tt);\r\nsg_free_table(vmw_tt->vsgt.sgt);\r\nvmw_tt->vsgt.sgt = NULL;\r\nttm_mem_global_free(vmw_mem_glob(dev_priv),\r\nvmw_tt->sg_alloc_size);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nvmw_tt->mapped = false;\r\n}\r\nint vmw_bo_map_dma(struct ttm_buffer_object *bo)\r\n{\r\nstruct vmw_ttm_tt *vmw_tt =\r\ncontainer_of(bo->ttm, struct vmw_ttm_tt, dma_ttm.ttm);\r\nreturn vmw_ttm_map_dma(vmw_tt);\r\n}\r\nvoid vmw_bo_unmap_dma(struct ttm_buffer_object *bo)\r\n{\r\nstruct vmw_ttm_tt *vmw_tt =\r\ncontainer_of(bo->ttm, struct vmw_ttm_tt, dma_ttm.ttm);\r\nvmw_ttm_unmap_dma(vmw_tt);\r\n}\r\nconst struct vmw_sg_table *vmw_bo_sg_table(struct ttm_buffer_object *bo)\r\n{\r\nstruct vmw_ttm_tt *vmw_tt =\r\ncontainer_of(bo->ttm, struct vmw_ttm_tt, dma_ttm.ttm);\r\nreturn &vmw_tt->vsgt;\r\n}\r\nstatic int vmw_ttm_bind(struct ttm_tt *ttm, struct ttm_mem_reg *bo_mem)\r\n{\r\nstruct vmw_ttm_tt *vmw_be =\r\ncontainer_of(ttm, struct vmw_ttm_tt, dma_ttm.ttm);\r\nint ret;\r\nret = vmw_ttm_map_dma(vmw_be);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nvmw_be->gmr_id = bo_mem->start;\r\nvmw_be->mem_type = bo_mem->mem_type;\r\nswitch (bo_mem->mem_type) {\r\ncase VMW_PL_GMR:\r\nreturn vmw_gmr_bind(vmw_be->dev_priv, &vmw_be->vsgt,\r\nttm->num_pages, vmw_be->gmr_id);\r\ncase VMW_PL_MOB:\r\nif (unlikely(vmw_be->mob == NULL)) {\r\nvmw_be->mob =\r\nvmw_mob_create(ttm->num_pages);\r\nif (unlikely(vmw_be->mob == NULL))\r\nreturn -ENOMEM;\r\n}\r\nreturn vmw_mob_bind(vmw_be->dev_priv, vmw_be->mob,\r\n&vmw_be->vsgt, ttm->num_pages,\r\nvmw_be->gmr_id);\r\ndefault:\r\nBUG();\r\n}\r\nreturn 0;\r\n}\r\nstatic int vmw_ttm_unbind(struct ttm_tt *ttm)\r\n{\r\nstruct vmw_ttm_tt *vmw_be =\r\ncontainer_of(ttm, struct vmw_ttm_tt, dma_ttm.ttm);\r\nswitch (vmw_be->mem_type) {\r\ncase VMW_PL_GMR:\r\nvmw_gmr_unbind(vmw_be->dev_priv, vmw_be->gmr_id);\r\nbreak;\r\ncase VMW_PL_MOB:\r\nvmw_mob_unbind(vmw_be->dev_priv, vmw_be->mob);\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\nif (vmw_be->dev_priv->map_mode == vmw_dma_map_bind)\r\nvmw_ttm_unmap_dma(vmw_be);\r\nreturn 0;\r\n}\r\nstatic void vmw_ttm_destroy(struct ttm_tt *ttm)\r\n{\r\nstruct vmw_ttm_tt *vmw_be =\r\ncontainer_of(ttm, struct vmw_ttm_tt, dma_ttm.ttm);\r\nvmw_ttm_unmap_dma(vmw_be);\r\nif (vmw_be->dev_priv->map_mode == vmw_dma_alloc_coherent)\r\nttm_dma_tt_fini(&vmw_be->dma_ttm);\r\nelse\r\nttm_tt_fini(ttm);\r\nif (vmw_be->mob)\r\nvmw_mob_destroy(vmw_be->mob);\r\nkfree(vmw_be);\r\n}\r\nstatic int vmw_ttm_populate(struct ttm_tt *ttm)\r\n{\r\nstruct vmw_ttm_tt *vmw_tt =\r\ncontainer_of(ttm, struct vmw_ttm_tt, dma_ttm.ttm);\r\nstruct vmw_private *dev_priv = vmw_tt->dev_priv;\r\nstruct ttm_mem_global *glob = vmw_mem_glob(dev_priv);\r\nint ret;\r\nif (ttm->state != tt_unpopulated)\r\nreturn 0;\r\nif (dev_priv->map_mode == vmw_dma_alloc_coherent) {\r\nsize_t size =\r\nttm_round_pot(ttm->num_pages * sizeof(dma_addr_t));\r\nret = ttm_mem_global_alloc(glob, size, false, true);\r\nif (unlikely(ret != 0))\r\nreturn ret;\r\nret = ttm_dma_populate(&vmw_tt->dma_ttm, dev_priv->dev->dev);\r\nif (unlikely(ret != 0))\r\nttm_mem_global_free(glob, size);\r\n} else\r\nret = ttm_pool_populate(ttm);\r\nreturn ret;\r\n}\r\nstatic void vmw_ttm_unpopulate(struct ttm_tt *ttm)\r\n{\r\nstruct vmw_ttm_tt *vmw_tt = container_of(ttm, struct vmw_ttm_tt,\r\ndma_ttm.ttm);\r\nstruct vmw_private *dev_priv = vmw_tt->dev_priv;\r\nstruct ttm_mem_global *glob = vmw_mem_glob(dev_priv);\r\nif (vmw_tt->mob) {\r\nvmw_mob_destroy(vmw_tt->mob);\r\nvmw_tt->mob = NULL;\r\n}\r\nvmw_ttm_unmap_dma(vmw_tt);\r\nif (dev_priv->map_mode == vmw_dma_alloc_coherent) {\r\nsize_t size =\r\nttm_round_pot(ttm->num_pages * sizeof(dma_addr_t));\r\nttm_dma_unpopulate(&vmw_tt->dma_ttm, dev_priv->dev->dev);\r\nttm_mem_global_free(glob, size);\r\n} else\r\nttm_pool_unpopulate(ttm);\r\n}\r\nstatic struct ttm_tt *vmw_ttm_tt_create(struct ttm_bo_device *bdev,\r\nunsigned long size, uint32_t page_flags,\r\nstruct page *dummy_read_page)\r\n{\r\nstruct vmw_ttm_tt *vmw_be;\r\nint ret;\r\nvmw_be = kzalloc(sizeof(*vmw_be), GFP_KERNEL);\r\nif (!vmw_be)\r\nreturn NULL;\r\nvmw_be->dma_ttm.ttm.func = &vmw_ttm_func;\r\nvmw_be->dev_priv = container_of(bdev, struct vmw_private, bdev);\r\nvmw_be->mob = NULL;\r\nif (vmw_be->dev_priv->map_mode == vmw_dma_alloc_coherent)\r\nret = ttm_dma_tt_init(&vmw_be->dma_ttm, bdev, size, page_flags,\r\ndummy_read_page);\r\nelse\r\nret = ttm_tt_init(&vmw_be->dma_ttm.ttm, bdev, size, page_flags,\r\ndummy_read_page);\r\nif (unlikely(ret != 0))\r\ngoto out_no_init;\r\nreturn &vmw_be->dma_ttm.ttm;\r\nout_no_init:\r\nkfree(vmw_be);\r\nreturn NULL;\r\n}\r\nstatic int vmw_invalidate_caches(struct ttm_bo_device *bdev, uint32_t flags)\r\n{\r\nreturn 0;\r\n}\r\nstatic int vmw_init_mem_type(struct ttm_bo_device *bdev, uint32_t type,\r\nstruct ttm_mem_type_manager *man)\r\n{\r\nswitch (type) {\r\ncase TTM_PL_SYSTEM:\r\nman->flags = TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_CACHED;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nman->func = &ttm_bo_manager_func;\r\nman->gpu_offset = 0;\r\nman->flags = TTM_MEMTYPE_FLAG_FIXED | TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_CACHED;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ncase VMW_PL_GMR:\r\ncase VMW_PL_MOB:\r\nman->func = &vmw_gmrid_manager_func;\r\nman->gpu_offset = 0;\r\nman->flags = TTM_MEMTYPE_FLAG_CMA | TTM_MEMTYPE_FLAG_MAPPABLE;\r\nman->available_caching = TTM_PL_FLAG_CACHED;\r\nman->default_caching = TTM_PL_FLAG_CACHED;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Unsupported memory type %u\n", (unsigned)type);\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void vmw_evict_flags(struct ttm_buffer_object *bo,\r\nstruct ttm_placement *placement)\r\n{\r\n*placement = vmw_sys_placement;\r\n}\r\nstatic int vmw_verify_access(struct ttm_buffer_object *bo, struct file *filp)\r\n{\r\nstruct ttm_object_file *tfile =\r\nvmw_fpriv((struct drm_file *)filp->private_data)->tfile;\r\nreturn vmw_user_dmabuf_verify_access(bo, tfile);\r\n}\r\nstatic int vmw_ttm_io_mem_reserve(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\nstruct ttm_mem_type_manager *man = &bdev->man[mem->mem_type];\r\nstruct vmw_private *dev_priv = container_of(bdev, struct vmw_private, bdev);\r\nmem->bus.addr = NULL;\r\nmem->bus.is_iomem = false;\r\nmem->bus.offset = 0;\r\nmem->bus.size = mem->num_pages << PAGE_SHIFT;\r\nmem->bus.base = 0;\r\nif (!(man->flags & TTM_MEMTYPE_FLAG_MAPPABLE))\r\nreturn -EINVAL;\r\nswitch (mem->mem_type) {\r\ncase TTM_PL_SYSTEM:\r\ncase VMW_PL_GMR:\r\ncase VMW_PL_MOB:\r\nreturn 0;\r\ncase TTM_PL_VRAM:\r\nmem->bus.offset = mem->start << PAGE_SHIFT;\r\nmem->bus.base = dev_priv->vram_start;\r\nmem->bus.is_iomem = true;\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nreturn 0;\r\n}\r\nstatic void vmw_ttm_io_mem_free(struct ttm_bo_device *bdev, struct ttm_mem_reg *mem)\r\n{\r\n}\r\nstatic int vmw_ttm_fault_reserve_notify(struct ttm_buffer_object *bo)\r\n{\r\nreturn 0;\r\n}\r\nstatic void vmw_move_notify(struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *mem)\r\n{\r\nvmw_resource_move_notify(bo, mem);\r\nvmw_query_move_notify(bo, mem);\r\n}\r\nstatic void vmw_swap_notify(struct ttm_buffer_object *bo)\r\n{\r\nttm_bo_wait(bo, false, false);\r\n}
