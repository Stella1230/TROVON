static void radeon_cs_buckets_init(struct radeon_cs_buckets *b)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < RADEON_CS_NUM_BUCKETS; i++)\r\nINIT_LIST_HEAD(&b->bucket[i]);\r\n}\r\nstatic void radeon_cs_buckets_add(struct radeon_cs_buckets *b,\r\nstruct list_head *item, unsigned priority)\r\n{\r\nlist_add_tail(item, &b->bucket[min(priority, RADEON_CS_MAX_PRIORITY)]);\r\n}\r\nstatic void radeon_cs_buckets_get_list(struct radeon_cs_buckets *b,\r\nstruct list_head *out_list)\r\n{\r\nunsigned i;\r\nfor (i = 0; i < RADEON_CS_NUM_BUCKETS; i++) {\r\nlist_splice(&b->bucket[i], out_list);\r\n}\r\n}\r\nstatic int radeon_cs_parser_relocs(struct radeon_cs_parser *p)\r\n{\r\nstruct radeon_cs_chunk *chunk;\r\nstruct radeon_cs_buckets buckets;\r\nunsigned i;\r\nbool need_mmap_lock = false;\r\nint r;\r\nif (p->chunk_relocs == NULL) {\r\nreturn 0;\r\n}\r\nchunk = p->chunk_relocs;\r\np->dma_reloc_idx = 0;\r\np->nrelocs = chunk->length_dw / 4;\r\np->relocs = drm_calloc_large(p->nrelocs, sizeof(struct radeon_bo_list));\r\nif (p->relocs == NULL) {\r\nreturn -ENOMEM;\r\n}\r\nradeon_cs_buckets_init(&buckets);\r\nfor (i = 0; i < p->nrelocs; i++) {\r\nstruct drm_radeon_cs_reloc *r;\r\nstruct drm_gem_object *gobj;\r\nunsigned priority;\r\nr = (struct drm_radeon_cs_reloc *)&chunk->kdata[i*4];\r\ngobj = drm_gem_object_lookup(p->filp, r->handle);\r\nif (gobj == NULL) {\r\nDRM_ERROR("gem object lookup failed 0x%x\n",\r\nr->handle);\r\nreturn -ENOENT;\r\n}\r\np->relocs[i].robj = gem_to_radeon_bo(gobj);\r\npriority = (r->flags & RADEON_RELOC_PRIO_MASK) * 2\r\n+ !!r->write_domain;\r\nif (p->ring == R600_RING_TYPE_UVD_INDEX &&\r\n(i == 0 || drm_pci_device_is_agp(p->rdev->ddev) ||\r\np->rdev->family == CHIP_RS780 ||\r\np->rdev->family == CHIP_RS880)) {\r\np->relocs[i].prefered_domains =\r\nRADEON_GEM_DOMAIN_VRAM;\r\np->relocs[i].allowed_domains =\r\nRADEON_GEM_DOMAIN_VRAM;\r\npriority = RADEON_CS_MAX_PRIORITY;\r\n} else {\r\nuint32_t domain = r->write_domain ?\r\nr->write_domain : r->read_domains;\r\nif (domain & RADEON_GEM_DOMAIN_CPU) {\r\nDRM_ERROR("RADEON_GEM_DOMAIN_CPU is not valid "\r\n"for command submission\n");\r\nreturn -EINVAL;\r\n}\r\np->relocs[i].prefered_domains = domain;\r\nif (domain == RADEON_GEM_DOMAIN_VRAM)\r\ndomain |= RADEON_GEM_DOMAIN_GTT;\r\np->relocs[i].allowed_domains = domain;\r\n}\r\nif (radeon_ttm_tt_has_userptr(p->relocs[i].robj->tbo.ttm)) {\r\nuint32_t domain = p->relocs[i].prefered_domains;\r\nif (!(domain & RADEON_GEM_DOMAIN_GTT)) {\r\nDRM_ERROR("Only RADEON_GEM_DOMAIN_GTT is "\r\n"allowed for userptr BOs\n");\r\nreturn -EINVAL;\r\n}\r\nneed_mmap_lock = true;\r\ndomain = RADEON_GEM_DOMAIN_GTT;\r\np->relocs[i].prefered_domains = domain;\r\np->relocs[i].allowed_domains = domain;\r\n}\r\np->relocs[i].tv.bo = &p->relocs[i].robj->tbo;\r\np->relocs[i].tv.shared = !r->write_domain;\r\nradeon_cs_buckets_add(&buckets, &p->relocs[i].tv.head,\r\npriority);\r\n}\r\nradeon_cs_buckets_get_list(&buckets, &p->validated);\r\nif (p->cs_flags & RADEON_CS_USE_VM)\r\np->vm_bos = radeon_vm_get_bos(p->rdev, p->ib.vm,\r\n&p->validated);\r\nif (need_mmap_lock)\r\ndown_read(&current->mm->mmap_sem);\r\nr = radeon_bo_list_validate(p->rdev, &p->ticket, &p->validated, p->ring);\r\nif (need_mmap_lock)\r\nup_read(&current->mm->mmap_sem);\r\nreturn r;\r\n}\r\nstatic int radeon_cs_get_ring(struct radeon_cs_parser *p, u32 ring, s32 priority)\r\n{\r\np->priority = priority;\r\nswitch (ring) {\r\ndefault:\r\nDRM_ERROR("unknown ring id: %d\n", ring);\r\nreturn -EINVAL;\r\ncase RADEON_CS_RING_GFX:\r\np->ring = RADEON_RING_TYPE_GFX_INDEX;\r\nbreak;\r\ncase RADEON_CS_RING_COMPUTE:\r\nif (p->rdev->family >= CHIP_TAHITI) {\r\nif (p->priority > 0)\r\np->ring = CAYMAN_RING_TYPE_CP1_INDEX;\r\nelse\r\np->ring = CAYMAN_RING_TYPE_CP2_INDEX;\r\n} else\r\np->ring = RADEON_RING_TYPE_GFX_INDEX;\r\nbreak;\r\ncase RADEON_CS_RING_DMA:\r\nif (p->rdev->family >= CHIP_CAYMAN) {\r\nif (p->priority > 0)\r\np->ring = R600_RING_TYPE_DMA_INDEX;\r\nelse\r\np->ring = CAYMAN_RING_TYPE_DMA1_INDEX;\r\n} else if (p->rdev->family >= CHIP_RV770) {\r\np->ring = R600_RING_TYPE_DMA_INDEX;\r\n} else {\r\nreturn -EINVAL;\r\n}\r\nbreak;\r\ncase RADEON_CS_RING_UVD:\r\np->ring = R600_RING_TYPE_UVD_INDEX;\r\nbreak;\r\ncase RADEON_CS_RING_VCE:\r\np->ring = TN_RING_TYPE_VCE1_INDEX;\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nstatic int radeon_cs_sync_rings(struct radeon_cs_parser *p)\r\n{\r\nstruct radeon_bo_list *reloc;\r\nint r;\r\nlist_for_each_entry(reloc, &p->validated, tv.head) {\r\nstruct reservation_object *resv;\r\nresv = reloc->robj->tbo.resv;\r\nr = radeon_sync_resv(p->rdev, &p->ib.sync, resv,\r\nreloc->tv.shared);\r\nif (r)\r\nreturn r;\r\n}\r\nreturn 0;\r\n}\r\nint radeon_cs_parser_init(struct radeon_cs_parser *p, void *data)\r\n{\r\nstruct drm_radeon_cs *cs = data;\r\nuint64_t *chunk_array_ptr;\r\nunsigned size, i;\r\nu32 ring = RADEON_CS_RING_GFX;\r\ns32 priority = 0;\r\nINIT_LIST_HEAD(&p->validated);\r\nif (!cs->num_chunks) {\r\nreturn 0;\r\n}\r\np->idx = 0;\r\np->ib.sa_bo = NULL;\r\np->const_ib.sa_bo = NULL;\r\np->chunk_ib = NULL;\r\np->chunk_relocs = NULL;\r\np->chunk_flags = NULL;\r\np->chunk_const_ib = NULL;\r\np->chunks_array = kcalloc(cs->num_chunks, sizeof(uint64_t), GFP_KERNEL);\r\nif (p->chunks_array == NULL) {\r\nreturn -ENOMEM;\r\n}\r\nchunk_array_ptr = (uint64_t *)(unsigned long)(cs->chunks);\r\nif (copy_from_user(p->chunks_array, chunk_array_ptr,\r\nsizeof(uint64_t)*cs->num_chunks)) {\r\nreturn -EFAULT;\r\n}\r\np->cs_flags = 0;\r\np->nchunks = cs->num_chunks;\r\np->chunks = kcalloc(p->nchunks, sizeof(struct radeon_cs_chunk), GFP_KERNEL);\r\nif (p->chunks == NULL) {\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < p->nchunks; i++) {\r\nstruct drm_radeon_cs_chunk __user **chunk_ptr = NULL;\r\nstruct drm_radeon_cs_chunk user_chunk;\r\nuint32_t __user *cdata;\r\nchunk_ptr = (void __user*)(unsigned long)p->chunks_array[i];\r\nif (copy_from_user(&user_chunk, chunk_ptr,\r\nsizeof(struct drm_radeon_cs_chunk))) {\r\nreturn -EFAULT;\r\n}\r\np->chunks[i].length_dw = user_chunk.length_dw;\r\nif (user_chunk.chunk_id == RADEON_CHUNK_ID_RELOCS) {\r\np->chunk_relocs = &p->chunks[i];\r\n}\r\nif (user_chunk.chunk_id == RADEON_CHUNK_ID_IB) {\r\np->chunk_ib = &p->chunks[i];\r\nif (p->chunks[i].length_dw == 0)\r\nreturn -EINVAL;\r\n}\r\nif (user_chunk.chunk_id == RADEON_CHUNK_ID_CONST_IB) {\r\np->chunk_const_ib = &p->chunks[i];\r\nif (p->chunks[i].length_dw == 0)\r\nreturn -EINVAL;\r\n}\r\nif (user_chunk.chunk_id == RADEON_CHUNK_ID_FLAGS) {\r\np->chunk_flags = &p->chunks[i];\r\nif (p->chunks[i].length_dw == 0)\r\nreturn -EINVAL;\r\n}\r\nsize = p->chunks[i].length_dw;\r\ncdata = (void __user *)(unsigned long)user_chunk.chunk_data;\r\np->chunks[i].user_ptr = cdata;\r\nif (user_chunk.chunk_id == RADEON_CHUNK_ID_CONST_IB)\r\ncontinue;\r\nif (user_chunk.chunk_id == RADEON_CHUNK_ID_IB) {\r\nif (!p->rdev || !(p->rdev->flags & RADEON_IS_AGP))\r\ncontinue;\r\n}\r\np->chunks[i].kdata = drm_malloc_ab(size, sizeof(uint32_t));\r\nsize *= sizeof(uint32_t);\r\nif (p->chunks[i].kdata == NULL) {\r\nreturn -ENOMEM;\r\n}\r\nif (copy_from_user(p->chunks[i].kdata, cdata, size)) {\r\nreturn -EFAULT;\r\n}\r\nif (user_chunk.chunk_id == RADEON_CHUNK_ID_FLAGS) {\r\np->cs_flags = p->chunks[i].kdata[0];\r\nif (p->chunks[i].length_dw > 1)\r\nring = p->chunks[i].kdata[1];\r\nif (p->chunks[i].length_dw > 2)\r\npriority = (s32)p->chunks[i].kdata[2];\r\n}\r\n}\r\nif (p->rdev) {\r\nif ((p->cs_flags & RADEON_CS_USE_VM) &&\r\n!p->rdev->vm_manager.enabled) {\r\nDRM_ERROR("VM not active on asic!\n");\r\nreturn -EINVAL;\r\n}\r\nif (radeon_cs_get_ring(p, ring, priority))\r\nreturn -EINVAL;\r\nif ((p->cs_flags & RADEON_CS_USE_VM) == 0) {\r\nif (p->rdev->asic->ring[p->ring]->cs_parse == NULL) {\r\nDRM_ERROR("Ring %d requires VM!\n", p->ring);\r\nreturn -EINVAL;\r\n}\r\n} else {\r\nif (p->rdev->asic->ring[p->ring]->ib_parse == NULL) {\r\nDRM_ERROR("VM not supported on ring %d!\n",\r\np->ring);\r\nreturn -EINVAL;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int cmp_size_smaller_first(void *priv, struct list_head *a,\r\nstruct list_head *b)\r\n{\r\nstruct radeon_bo_list *la = list_entry(a, struct radeon_bo_list, tv.head);\r\nstruct radeon_bo_list *lb = list_entry(b, struct radeon_bo_list, tv.head);\r\nreturn (int)la->robj->tbo.num_pages - (int)lb->robj->tbo.num_pages;\r\n}\r\nstatic void radeon_cs_parser_fini(struct radeon_cs_parser *parser, int error, bool backoff)\r\n{\r\nunsigned i;\r\nif (!error) {\r\nlist_sort(NULL, &parser->validated, cmp_size_smaller_first);\r\nttm_eu_fence_buffer_objects(&parser->ticket,\r\n&parser->validated,\r\n&parser->ib.fence->base);\r\n} else if (backoff) {\r\nttm_eu_backoff_reservation(&parser->ticket,\r\n&parser->validated);\r\n}\r\nif (parser->relocs != NULL) {\r\nfor (i = 0; i < parser->nrelocs; i++) {\r\nstruct radeon_bo *bo = parser->relocs[i].robj;\r\nif (bo == NULL)\r\ncontinue;\r\ndrm_gem_object_unreference_unlocked(&bo->gem_base);\r\n}\r\n}\r\nkfree(parser->track);\r\ndrm_free_large(parser->relocs);\r\ndrm_free_large(parser->vm_bos);\r\nfor (i = 0; i < parser->nchunks; i++)\r\ndrm_free_large(parser->chunks[i].kdata);\r\nkfree(parser->chunks);\r\nkfree(parser->chunks_array);\r\nradeon_ib_free(parser->rdev, &parser->ib);\r\nradeon_ib_free(parser->rdev, &parser->const_ib);\r\n}\r\nstatic int radeon_cs_ib_chunk(struct radeon_device *rdev,\r\nstruct radeon_cs_parser *parser)\r\n{\r\nint r;\r\nif (parser->chunk_ib == NULL)\r\nreturn 0;\r\nif (parser->cs_flags & RADEON_CS_USE_VM)\r\nreturn 0;\r\nr = radeon_cs_parse(rdev, parser->ring, parser);\r\nif (r || parser->parser_error) {\r\nDRM_ERROR("Invalid command stream !\n");\r\nreturn r;\r\n}\r\nr = radeon_cs_sync_rings(parser);\r\nif (r) {\r\nif (r != -ERESTARTSYS)\r\nDRM_ERROR("Failed to sync rings: %i\n", r);\r\nreturn r;\r\n}\r\nif (parser->ring == R600_RING_TYPE_UVD_INDEX)\r\nradeon_uvd_note_usage(rdev);\r\nelse if ((parser->ring == TN_RING_TYPE_VCE1_INDEX) ||\r\n(parser->ring == TN_RING_TYPE_VCE2_INDEX))\r\nradeon_vce_note_usage(rdev);\r\nr = radeon_ib_schedule(rdev, &parser->ib, NULL, true);\r\nif (r) {\r\nDRM_ERROR("Failed to schedule IB !\n");\r\n}\r\nreturn r;\r\n}\r\nstatic int radeon_bo_vm_update_pte(struct radeon_cs_parser *p,\r\nstruct radeon_vm *vm)\r\n{\r\nstruct radeon_device *rdev = p->rdev;\r\nstruct radeon_bo_va *bo_va;\r\nint i, r;\r\nr = radeon_vm_update_page_directory(rdev, vm);\r\nif (r)\r\nreturn r;\r\nr = radeon_vm_clear_freed(rdev, vm);\r\nif (r)\r\nreturn r;\r\nif (vm->ib_bo_va == NULL) {\r\nDRM_ERROR("Tmp BO not in VM!\n");\r\nreturn -EINVAL;\r\n}\r\nr = radeon_vm_bo_update(rdev, vm->ib_bo_va,\r\n&rdev->ring_tmp_bo.bo->tbo.mem);\r\nif (r)\r\nreturn r;\r\nfor (i = 0; i < p->nrelocs; i++) {\r\nstruct radeon_bo *bo;\r\nbo = p->relocs[i].robj;\r\nbo_va = radeon_vm_bo_find(vm, bo);\r\nif (bo_va == NULL) {\r\ndev_err(rdev->dev, "bo %p not in vm %p\n", bo, vm);\r\nreturn -EINVAL;\r\n}\r\nr = radeon_vm_bo_update(rdev, bo_va, &bo->tbo.mem);\r\nif (r)\r\nreturn r;\r\nradeon_sync_fence(&p->ib.sync, bo_va->last_pt_update);\r\n}\r\nreturn radeon_vm_clear_invalids(rdev, vm);\r\n}\r\nstatic int radeon_cs_ib_vm_chunk(struct radeon_device *rdev,\r\nstruct radeon_cs_parser *parser)\r\n{\r\nstruct radeon_fpriv *fpriv = parser->filp->driver_priv;\r\nstruct radeon_vm *vm = &fpriv->vm;\r\nint r;\r\nif (parser->chunk_ib == NULL)\r\nreturn 0;\r\nif ((parser->cs_flags & RADEON_CS_USE_VM) == 0)\r\nreturn 0;\r\nif (parser->const_ib.length_dw) {\r\nr = radeon_ring_ib_parse(rdev, parser->ring, &parser->const_ib);\r\nif (r) {\r\nreturn r;\r\n}\r\n}\r\nr = radeon_ring_ib_parse(rdev, parser->ring, &parser->ib);\r\nif (r) {\r\nreturn r;\r\n}\r\nif (parser->ring == R600_RING_TYPE_UVD_INDEX)\r\nradeon_uvd_note_usage(rdev);\r\nmutex_lock(&vm->mutex);\r\nr = radeon_bo_vm_update_pte(parser, vm);\r\nif (r) {\r\ngoto out;\r\n}\r\nr = radeon_cs_sync_rings(parser);\r\nif (r) {\r\nif (r != -ERESTARTSYS)\r\nDRM_ERROR("Failed to sync rings: %i\n", r);\r\ngoto out;\r\n}\r\nif ((rdev->family >= CHIP_TAHITI) &&\r\n(parser->chunk_const_ib != NULL)) {\r\nr = radeon_ib_schedule(rdev, &parser->ib, &parser->const_ib, true);\r\n} else {\r\nr = radeon_ib_schedule(rdev, &parser->ib, NULL, true);\r\n}\r\nout:\r\nmutex_unlock(&vm->mutex);\r\nreturn r;\r\n}\r\nstatic int radeon_cs_handle_lockup(struct radeon_device *rdev, int r)\r\n{\r\nif (r == -EDEADLK) {\r\nr = radeon_gpu_reset(rdev);\r\nif (!r)\r\nr = -EAGAIN;\r\n}\r\nreturn r;\r\n}\r\nstatic int radeon_cs_ib_fill(struct radeon_device *rdev, struct radeon_cs_parser *parser)\r\n{\r\nstruct radeon_cs_chunk *ib_chunk;\r\nstruct radeon_vm *vm = NULL;\r\nint r;\r\nif (parser->chunk_ib == NULL)\r\nreturn 0;\r\nif (parser->cs_flags & RADEON_CS_USE_VM) {\r\nstruct radeon_fpriv *fpriv = parser->filp->driver_priv;\r\nvm = &fpriv->vm;\r\nif ((rdev->family >= CHIP_TAHITI) &&\r\n(parser->chunk_const_ib != NULL)) {\r\nib_chunk = parser->chunk_const_ib;\r\nif (ib_chunk->length_dw > RADEON_IB_VM_MAX_SIZE) {\r\nDRM_ERROR("cs IB CONST too big: %d\n", ib_chunk->length_dw);\r\nreturn -EINVAL;\r\n}\r\nr = radeon_ib_get(rdev, parser->ring, &parser->const_ib,\r\nvm, ib_chunk->length_dw * 4);\r\nif (r) {\r\nDRM_ERROR("Failed to get const ib !\n");\r\nreturn r;\r\n}\r\nparser->const_ib.is_const_ib = true;\r\nparser->const_ib.length_dw = ib_chunk->length_dw;\r\nif (copy_from_user(parser->const_ib.ptr,\r\nib_chunk->user_ptr,\r\nib_chunk->length_dw * 4))\r\nreturn -EFAULT;\r\n}\r\nib_chunk = parser->chunk_ib;\r\nif (ib_chunk->length_dw > RADEON_IB_VM_MAX_SIZE) {\r\nDRM_ERROR("cs IB too big: %d\n", ib_chunk->length_dw);\r\nreturn -EINVAL;\r\n}\r\n}\r\nib_chunk = parser->chunk_ib;\r\nr = radeon_ib_get(rdev, parser->ring, &parser->ib,\r\nvm, ib_chunk->length_dw * 4);\r\nif (r) {\r\nDRM_ERROR("Failed to get ib !\n");\r\nreturn r;\r\n}\r\nparser->ib.length_dw = ib_chunk->length_dw;\r\nif (ib_chunk->kdata)\r\nmemcpy(parser->ib.ptr, ib_chunk->kdata, ib_chunk->length_dw * 4);\r\nelse if (copy_from_user(parser->ib.ptr, ib_chunk->user_ptr, ib_chunk->length_dw * 4))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nint radeon_cs_ioctl(struct drm_device *dev, void *data, struct drm_file *filp)\r\n{\r\nstruct radeon_device *rdev = dev->dev_private;\r\nstruct radeon_cs_parser parser;\r\nint r;\r\ndown_read(&rdev->exclusive_lock);\r\nif (!rdev->accel_working) {\r\nup_read(&rdev->exclusive_lock);\r\nreturn -EBUSY;\r\n}\r\nif (rdev->in_reset) {\r\nup_read(&rdev->exclusive_lock);\r\nr = radeon_gpu_reset(rdev);\r\nif (!r)\r\nr = -EAGAIN;\r\nreturn r;\r\n}\r\nmemset(&parser, 0, sizeof(struct radeon_cs_parser));\r\nparser.filp = filp;\r\nparser.rdev = rdev;\r\nparser.dev = rdev->dev;\r\nparser.family = rdev->family;\r\nr = radeon_cs_parser_init(&parser, data);\r\nif (r) {\r\nDRM_ERROR("Failed to initialize parser !\n");\r\nradeon_cs_parser_fini(&parser, r, false);\r\nup_read(&rdev->exclusive_lock);\r\nr = radeon_cs_handle_lockup(rdev, r);\r\nreturn r;\r\n}\r\nr = radeon_cs_ib_fill(rdev, &parser);\r\nif (!r) {\r\nr = radeon_cs_parser_relocs(&parser);\r\nif (r && r != -ERESTARTSYS)\r\nDRM_ERROR("Failed to parse relocation %d!\n", r);\r\n}\r\nif (r) {\r\nradeon_cs_parser_fini(&parser, r, false);\r\nup_read(&rdev->exclusive_lock);\r\nr = radeon_cs_handle_lockup(rdev, r);\r\nreturn r;\r\n}\r\ntrace_radeon_cs(&parser);\r\nr = radeon_cs_ib_chunk(rdev, &parser);\r\nif (r) {\r\ngoto out;\r\n}\r\nr = radeon_cs_ib_vm_chunk(rdev, &parser);\r\nif (r) {\r\ngoto out;\r\n}\r\nout:\r\nradeon_cs_parser_fini(&parser, r, true);\r\nup_read(&rdev->exclusive_lock);\r\nr = radeon_cs_handle_lockup(rdev, r);\r\nreturn r;\r\n}\r\nint radeon_cs_packet_parse(struct radeon_cs_parser *p,\r\nstruct radeon_cs_packet *pkt,\r\nunsigned idx)\r\n{\r\nstruct radeon_cs_chunk *ib_chunk = p->chunk_ib;\r\nstruct radeon_device *rdev = p->rdev;\r\nuint32_t header;\r\nint ret = 0, i;\r\nif (idx >= ib_chunk->length_dw) {\r\nDRM_ERROR("Can not parse packet at %d after CS end %d !\n",\r\nidx, ib_chunk->length_dw);\r\nreturn -EINVAL;\r\n}\r\nheader = radeon_get_ib_value(p, idx);\r\npkt->idx = idx;\r\npkt->type = RADEON_CP_PACKET_GET_TYPE(header);\r\npkt->count = RADEON_CP_PACKET_GET_COUNT(header);\r\npkt->one_reg_wr = 0;\r\nswitch (pkt->type) {\r\ncase RADEON_PACKET_TYPE0:\r\nif (rdev->family < CHIP_R600) {\r\npkt->reg = R100_CP_PACKET0_GET_REG(header);\r\npkt->one_reg_wr =\r\nRADEON_CP_PACKET0_GET_ONE_REG_WR(header);\r\n} else\r\npkt->reg = R600_CP_PACKET0_GET_REG(header);\r\nbreak;\r\ncase RADEON_PACKET_TYPE3:\r\npkt->opcode = RADEON_CP_PACKET3_GET_OPCODE(header);\r\nbreak;\r\ncase RADEON_PACKET_TYPE2:\r\npkt->count = -1;\r\nbreak;\r\ndefault:\r\nDRM_ERROR("Unknown packet type %d at %d !\n", pkt->type, idx);\r\nret = -EINVAL;\r\ngoto dump_ib;\r\n}\r\nif ((pkt->count + 1 + pkt->idx) >= ib_chunk->length_dw) {\r\nDRM_ERROR("Packet (%d:%d:%d) end after CS buffer (%d) !\n",\r\npkt->idx, pkt->type, pkt->count, ib_chunk->length_dw);\r\nret = -EINVAL;\r\ngoto dump_ib;\r\n}\r\nreturn 0;\r\ndump_ib:\r\nfor (i = 0; i < ib_chunk->length_dw; i++) {\r\nif (i == idx)\r\nprintk("\t0x%08x <---\n", radeon_get_ib_value(p, i));\r\nelse\r\nprintk("\t0x%08x\n", radeon_get_ib_value(p, i));\r\n}\r\nreturn ret;\r\n}\r\nbool radeon_cs_packet_next_is_pkt3_nop(struct radeon_cs_parser *p)\r\n{\r\nstruct radeon_cs_packet p3reloc;\r\nint r;\r\nr = radeon_cs_packet_parse(p, &p3reloc, p->idx);\r\nif (r)\r\nreturn false;\r\nif (p3reloc.type != RADEON_PACKET_TYPE3)\r\nreturn false;\r\nif (p3reloc.opcode != RADEON_PACKET3_NOP)\r\nreturn false;\r\nreturn true;\r\n}\r\nvoid radeon_cs_dump_packet(struct radeon_cs_parser *p,\r\nstruct radeon_cs_packet *pkt)\r\n{\r\nvolatile uint32_t *ib;\r\nunsigned i;\r\nunsigned idx;\r\nib = p->ib.ptr;\r\nidx = pkt->idx;\r\nfor (i = 0; i <= (pkt->count + 1); i++, idx++)\r\nDRM_INFO("ib[%d]=0x%08X\n", idx, ib[idx]);\r\n}\r\nint radeon_cs_packet_next_reloc(struct radeon_cs_parser *p,\r\nstruct radeon_bo_list **cs_reloc,\r\nint nomm)\r\n{\r\nstruct radeon_cs_chunk *relocs_chunk;\r\nstruct radeon_cs_packet p3reloc;\r\nunsigned idx;\r\nint r;\r\nif (p->chunk_relocs == NULL) {\r\nDRM_ERROR("No relocation chunk !\n");\r\nreturn -EINVAL;\r\n}\r\n*cs_reloc = NULL;\r\nrelocs_chunk = p->chunk_relocs;\r\nr = radeon_cs_packet_parse(p, &p3reloc, p->idx);\r\nif (r)\r\nreturn r;\r\np->idx += p3reloc.count + 2;\r\nif (p3reloc.type != RADEON_PACKET_TYPE3 ||\r\np3reloc.opcode != RADEON_PACKET3_NOP) {\r\nDRM_ERROR("No packet3 for relocation for packet at %d.\n",\r\np3reloc.idx);\r\nradeon_cs_dump_packet(p, &p3reloc);\r\nreturn -EINVAL;\r\n}\r\nidx = radeon_get_ib_value(p, p3reloc.idx + 1);\r\nif (idx >= relocs_chunk->length_dw) {\r\nDRM_ERROR("Relocs at %d after relocations chunk end %d !\n",\r\nidx, relocs_chunk->length_dw);\r\nradeon_cs_dump_packet(p, &p3reloc);\r\nreturn -EINVAL;\r\n}\r\nif (nomm) {\r\n*cs_reloc = p->relocs;\r\n(*cs_reloc)->gpu_offset =\r\n(u64)relocs_chunk->kdata[idx + 3] << 32;\r\n(*cs_reloc)->gpu_offset |= relocs_chunk->kdata[idx + 0];\r\n} else\r\n*cs_reloc = &p->relocs[(idx / 4)];\r\nreturn 0;\r\n}
