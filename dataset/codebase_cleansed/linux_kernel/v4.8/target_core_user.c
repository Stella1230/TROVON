static struct tcmu_cmd *tcmu_alloc_cmd(struct se_cmd *se_cmd)\r\n{\r\nstruct se_device *se_dev = se_cmd->se_dev;\r\nstruct tcmu_dev *udev = TCMU_DEV(se_dev);\r\nstruct tcmu_cmd *tcmu_cmd;\r\nint cmd_id;\r\ntcmu_cmd = kmem_cache_zalloc(tcmu_cmd_cache, GFP_KERNEL);\r\nif (!tcmu_cmd)\r\nreturn NULL;\r\ntcmu_cmd->se_cmd = se_cmd;\r\ntcmu_cmd->tcmu_dev = udev;\r\ntcmu_cmd->deadline = jiffies + msecs_to_jiffies(TCMU_TIME_OUT);\r\nidr_preload(GFP_KERNEL);\r\nspin_lock_irq(&udev->commands_lock);\r\ncmd_id = idr_alloc(&udev->commands, tcmu_cmd, 0,\r\nUSHRT_MAX, GFP_NOWAIT);\r\nspin_unlock_irq(&udev->commands_lock);\r\nidr_preload_end();\r\nif (cmd_id < 0) {\r\nkmem_cache_free(tcmu_cmd_cache, tcmu_cmd);\r\nreturn NULL;\r\n}\r\ntcmu_cmd->cmd_id = cmd_id;\r\nreturn tcmu_cmd;\r\n}\r\nstatic inline void tcmu_flush_dcache_range(void *vaddr, size_t size)\r\n{\r\nunsigned long offset = offset_in_page(vaddr);\r\nsize = round_up(size+offset, PAGE_SIZE);\r\nvaddr -= offset;\r\nwhile (size) {\r\nflush_dcache_page(virt_to_page(vaddr));\r\nsize -= PAGE_SIZE;\r\n}\r\n}\r\nstatic inline size_t spc_used(size_t head, size_t tail, size_t size)\r\n{\r\nint diff = head - tail;\r\nif (diff >= 0)\r\nreturn diff;\r\nelse\r\nreturn size + diff;\r\n}\r\nstatic inline size_t spc_free(size_t head, size_t tail, size_t size)\r\n{\r\nreturn (size - spc_used(head, tail, size) - 1);\r\n}\r\nstatic inline size_t head_to_end(size_t head, size_t size)\r\n{\r\nreturn size - head;\r\n}\r\nstatic inline void new_iov(struct iovec **iov, int *iov_cnt,\r\nstruct tcmu_dev *udev)\r\n{\r\nstruct iovec *iovec;\r\nif (*iov_cnt != 0)\r\n(*iov)++;\r\n(*iov_cnt)++;\r\niovec = *iov;\r\nmemset(iovec, 0, sizeof(struct iovec));\r\n}\r\nstatic inline size_t get_block_offset(struct tcmu_dev *dev,\r\nint block, int remaining)\r\n{\r\nreturn dev->data_off + block * DATA_BLOCK_SIZE +\r\nDATA_BLOCK_SIZE - remaining;\r\n}\r\nstatic inline size_t iov_tail(struct tcmu_dev *udev, struct iovec *iov)\r\n{\r\nreturn (size_t)iov->iov_base + iov->iov_len;\r\n}\r\nstatic void alloc_and_scatter_data_area(struct tcmu_dev *udev,\r\nstruct scatterlist *data_sg, unsigned int data_nents,\r\nstruct iovec **iov, int *iov_cnt, bool copy_data)\r\n{\r\nint i, block;\r\nint block_remaining = 0;\r\nvoid *from, *to;\r\nsize_t copy_bytes, to_offset;\r\nstruct scatterlist *sg;\r\nfor_each_sg(data_sg, sg, data_nents, i) {\r\nint sg_remaining = sg->length;\r\nfrom = kmap_atomic(sg_page(sg)) + sg->offset;\r\nwhile (sg_remaining > 0) {\r\nif (block_remaining == 0) {\r\nblock = find_first_zero_bit(udev->data_bitmap,\r\nDATA_BLOCK_BITS);\r\nblock_remaining = DATA_BLOCK_SIZE;\r\nset_bit(block, udev->data_bitmap);\r\n}\r\ncopy_bytes = min_t(size_t, sg_remaining,\r\nblock_remaining);\r\nto_offset = get_block_offset(udev, block,\r\nblock_remaining);\r\nto = (void *)udev->mb_addr + to_offset;\r\nif (*iov_cnt != 0 &&\r\nto_offset == iov_tail(udev, *iov)) {\r\n(*iov)->iov_len += copy_bytes;\r\n} else {\r\nnew_iov(iov, iov_cnt, udev);\r\n(*iov)->iov_base = (void __user *) to_offset;\r\n(*iov)->iov_len = copy_bytes;\r\n}\r\nif (copy_data) {\r\nmemcpy(to, from + sg->length - sg_remaining,\r\ncopy_bytes);\r\ntcmu_flush_dcache_range(to, copy_bytes);\r\n}\r\nsg_remaining -= copy_bytes;\r\nblock_remaining -= copy_bytes;\r\n}\r\nkunmap_atomic(from - sg->offset);\r\n}\r\n}\r\nstatic void free_data_area(struct tcmu_dev *udev, struct tcmu_cmd *cmd)\r\n{\r\nbitmap_xor(udev->data_bitmap, udev->data_bitmap, cmd->data_bitmap,\r\nDATA_BLOCK_BITS);\r\n}\r\nstatic void gather_data_area(struct tcmu_dev *udev, unsigned long *cmd_bitmap,\r\nstruct scatterlist *data_sg, unsigned int data_nents)\r\n{\r\nint i, block;\r\nint block_remaining = 0;\r\nvoid *from, *to;\r\nsize_t copy_bytes, from_offset;\r\nstruct scatterlist *sg;\r\nfor_each_sg(data_sg, sg, data_nents, i) {\r\nint sg_remaining = sg->length;\r\nto = kmap_atomic(sg_page(sg)) + sg->offset;\r\nwhile (sg_remaining > 0) {\r\nif (block_remaining == 0) {\r\nblock = find_first_bit(cmd_bitmap,\r\nDATA_BLOCK_BITS);\r\nblock_remaining = DATA_BLOCK_SIZE;\r\nclear_bit(block, cmd_bitmap);\r\n}\r\ncopy_bytes = min_t(size_t, sg_remaining,\r\nblock_remaining);\r\nfrom_offset = get_block_offset(udev, block,\r\nblock_remaining);\r\nfrom = (void *) udev->mb_addr + from_offset;\r\ntcmu_flush_dcache_range(from, copy_bytes);\r\nmemcpy(to + sg->length - sg_remaining, from,\r\ncopy_bytes);\r\nsg_remaining -= copy_bytes;\r\nblock_remaining -= copy_bytes;\r\n}\r\nkunmap_atomic(to - sg->offset);\r\n}\r\n}\r\nstatic inline size_t spc_bitmap_free(unsigned long *bitmap)\r\n{\r\nreturn DATA_BLOCK_SIZE * (DATA_BLOCK_BITS -\r\nbitmap_weight(bitmap, DATA_BLOCK_BITS));\r\n}\r\nstatic bool is_ring_space_avail(struct tcmu_dev *udev, size_t cmd_size, size_t data_needed)\r\n{\r\nstruct tcmu_mailbox *mb = udev->mb_addr;\r\nsize_t space, cmd_needed;\r\nu32 cmd_head;\r\ntcmu_flush_dcache_range(mb, sizeof(*mb));\r\ncmd_head = mb->cmd_head % udev->cmdr_size;\r\nif (head_to_end(cmd_head, udev->cmdr_size) >= cmd_size)\r\ncmd_needed = cmd_size;\r\nelse\r\ncmd_needed = cmd_size + head_to_end(cmd_head, udev->cmdr_size);\r\nspace = spc_free(cmd_head, udev->cmdr_last_cleaned, udev->cmdr_size);\r\nif (space < cmd_needed) {\r\npr_debug("no cmd space: %u %u %u\n", cmd_head,\r\nudev->cmdr_last_cleaned, udev->cmdr_size);\r\nreturn false;\r\n}\r\nspace = spc_bitmap_free(udev->data_bitmap);\r\nif (space < data_needed) {\r\npr_debug("no data space: only %zu available, but ask for %zu\n",\r\nspace, data_needed);\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int tcmu_queue_cmd_ring(struct tcmu_cmd *tcmu_cmd)\r\n{\r\nstruct tcmu_dev *udev = tcmu_cmd->tcmu_dev;\r\nstruct se_cmd *se_cmd = tcmu_cmd->se_cmd;\r\nsize_t base_command_size, command_size;\r\nstruct tcmu_mailbox *mb;\r\nstruct tcmu_cmd_entry *entry;\r\nstruct iovec *iov;\r\nint iov_cnt;\r\nuint32_t cmd_head;\r\nuint64_t cdb_off;\r\nbool copy_to_data_area;\r\nsize_t data_length;\r\nDECLARE_BITMAP(old_bitmap, DATA_BLOCK_BITS);\r\nif (test_bit(TCMU_DEV_BIT_BROKEN, &udev->flags))\r\nreturn -EINVAL;\r\nbase_command_size = max(offsetof(struct tcmu_cmd_entry,\r\nreq.iov[se_cmd->t_bidi_data_nents +\r\nse_cmd->t_data_nents]),\r\nsizeof(struct tcmu_cmd_entry));\r\ncommand_size = base_command_size\r\n+ round_up(scsi_command_size(se_cmd->t_task_cdb), TCMU_OP_ALIGN_SIZE);\r\nWARN_ON(command_size & (TCMU_OP_ALIGN_SIZE-1));\r\nspin_lock_irq(&udev->cmdr_lock);\r\nmb = udev->mb_addr;\r\ncmd_head = mb->cmd_head % udev->cmdr_size;\r\ndata_length = se_cmd->data_length;\r\nif (se_cmd->se_cmd_flags & SCF_BIDI) {\r\nBUG_ON(!(se_cmd->t_bidi_data_sg && se_cmd->t_bidi_data_nents));\r\ndata_length += se_cmd->t_bidi_data_sg->length;\r\n}\r\nif ((command_size > (udev->cmdr_size / 2))\r\n|| data_length > udev->data_size)\r\npr_warn("TCMU: Request of size %zu/%zu may be too big for %u/%zu "\r\n"cmd/data ring buffers\n", command_size, data_length,\r\nudev->cmdr_size, udev->data_size);\r\nwhile (!is_ring_space_avail(udev, command_size, data_length)) {\r\nint ret;\r\nDEFINE_WAIT(__wait);\r\nprepare_to_wait(&udev->wait_cmdr, &__wait, TASK_INTERRUPTIBLE);\r\npr_debug("sleeping for ring space\n");\r\nspin_unlock_irq(&udev->cmdr_lock);\r\nret = schedule_timeout(msecs_to_jiffies(TCMU_TIME_OUT));\r\nfinish_wait(&udev->wait_cmdr, &__wait);\r\nif (!ret) {\r\npr_warn("tcmu: command timed out\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nspin_lock_irq(&udev->cmdr_lock);\r\ncmd_head = mb->cmd_head % udev->cmdr_size;\r\n}\r\nif (head_to_end(cmd_head, udev->cmdr_size) < command_size) {\r\nsize_t pad_size = head_to_end(cmd_head, udev->cmdr_size);\r\nentry = (void *) mb + CMDR_OFF + cmd_head;\r\ntcmu_flush_dcache_range(entry, sizeof(*entry));\r\ntcmu_hdr_set_op(&entry->hdr.len_op, TCMU_OP_PAD);\r\ntcmu_hdr_set_len(&entry->hdr.len_op, pad_size);\r\nentry->hdr.cmd_id = 0;\r\nentry->hdr.kflags = 0;\r\nentry->hdr.uflags = 0;\r\nUPDATE_HEAD(mb->cmd_head, pad_size, udev->cmdr_size);\r\ncmd_head = mb->cmd_head % udev->cmdr_size;\r\nWARN_ON(cmd_head != 0);\r\n}\r\nentry = (void *) mb + CMDR_OFF + cmd_head;\r\ntcmu_flush_dcache_range(entry, sizeof(*entry));\r\ntcmu_hdr_set_op(&entry->hdr.len_op, TCMU_OP_CMD);\r\ntcmu_hdr_set_len(&entry->hdr.len_op, command_size);\r\nentry->hdr.cmd_id = tcmu_cmd->cmd_id;\r\nentry->hdr.kflags = 0;\r\nentry->hdr.uflags = 0;\r\nbitmap_copy(old_bitmap, udev->data_bitmap, DATA_BLOCK_BITS);\r\niov = &entry->req.iov[0];\r\niov_cnt = 0;\r\ncopy_to_data_area = (se_cmd->data_direction == DMA_TO_DEVICE\r\n|| se_cmd->se_cmd_flags & SCF_BIDI);\r\nalloc_and_scatter_data_area(udev, se_cmd->t_data_sg,\r\nse_cmd->t_data_nents, &iov, &iov_cnt, copy_to_data_area);\r\nentry->req.iov_cnt = iov_cnt;\r\nentry->req.iov_dif_cnt = 0;\r\niov_cnt = 0;\r\nalloc_and_scatter_data_area(udev, se_cmd->t_bidi_data_sg,\r\nse_cmd->t_bidi_data_nents, &iov, &iov_cnt, false);\r\nentry->req.iov_bidi_cnt = iov_cnt;\r\nbitmap_xor(tcmu_cmd->data_bitmap, old_bitmap, udev->data_bitmap,\r\nDATA_BLOCK_BITS);\r\ncdb_off = CMDR_OFF + cmd_head + base_command_size;\r\nmemcpy((void *) mb + cdb_off, se_cmd->t_task_cdb, scsi_command_size(se_cmd->t_task_cdb));\r\nentry->req.cdb_off = cdb_off;\r\ntcmu_flush_dcache_range(entry, sizeof(*entry));\r\nUPDATE_HEAD(mb->cmd_head, command_size, udev->cmdr_size);\r\ntcmu_flush_dcache_range(mb, sizeof(*mb));\r\nspin_unlock_irq(&udev->cmdr_lock);\r\nuio_event_notify(&udev->uio_info);\r\nmod_timer(&udev->timeout,\r\nround_jiffies_up(jiffies + msecs_to_jiffies(TCMU_TIME_OUT)));\r\nreturn 0;\r\n}\r\nstatic int tcmu_queue_cmd(struct se_cmd *se_cmd)\r\n{\r\nstruct se_device *se_dev = se_cmd->se_dev;\r\nstruct tcmu_dev *udev = TCMU_DEV(se_dev);\r\nstruct tcmu_cmd *tcmu_cmd;\r\nint ret;\r\ntcmu_cmd = tcmu_alloc_cmd(se_cmd);\r\nif (!tcmu_cmd)\r\nreturn -ENOMEM;\r\nret = tcmu_queue_cmd_ring(tcmu_cmd);\r\nif (ret < 0) {\r\npr_err("TCMU: Could not queue command\n");\r\nspin_lock_irq(&udev->commands_lock);\r\nidr_remove(&udev->commands, tcmu_cmd->cmd_id);\r\nspin_unlock_irq(&udev->commands_lock);\r\nkmem_cache_free(tcmu_cmd_cache, tcmu_cmd);\r\n}\r\nreturn ret;\r\n}\r\nstatic void tcmu_handle_completion(struct tcmu_cmd *cmd, struct tcmu_cmd_entry *entry)\r\n{\r\nstruct se_cmd *se_cmd = cmd->se_cmd;\r\nstruct tcmu_dev *udev = cmd->tcmu_dev;\r\nif (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags)) {\r\nfree_data_area(udev, cmd);\r\nkmem_cache_free(tcmu_cmd_cache, cmd);\r\nreturn;\r\n}\r\nif (entry->hdr.uflags & TCMU_UFLAG_UNKNOWN_OP) {\r\nfree_data_area(udev, cmd);\r\npr_warn("TCMU: Userspace set UNKNOWN_OP flag on se_cmd %p\n",\r\ncmd->se_cmd);\r\nentry->rsp.scsi_status = SAM_STAT_CHECK_CONDITION;\r\n} else if (entry->rsp.scsi_status == SAM_STAT_CHECK_CONDITION) {\r\nmemcpy(se_cmd->sense_buffer, entry->rsp.sense_buffer,\r\nse_cmd->scsi_sense_length);\r\nfree_data_area(udev, cmd);\r\n} else if (se_cmd->se_cmd_flags & SCF_BIDI) {\r\nDECLARE_BITMAP(bitmap, DATA_BLOCK_BITS);\r\nbitmap_copy(bitmap, cmd->data_bitmap, DATA_BLOCK_BITS);\r\ngather_data_area(udev, bitmap,\r\nse_cmd->t_bidi_data_sg, se_cmd->t_bidi_data_nents);\r\nfree_data_area(udev, cmd);\r\n} else if (se_cmd->data_direction == DMA_FROM_DEVICE) {\r\nDECLARE_BITMAP(bitmap, DATA_BLOCK_BITS);\r\nbitmap_copy(bitmap, cmd->data_bitmap, DATA_BLOCK_BITS);\r\ngather_data_area(udev, bitmap,\r\nse_cmd->t_data_sg, se_cmd->t_data_nents);\r\nfree_data_area(udev, cmd);\r\n} else if (se_cmd->data_direction == DMA_TO_DEVICE) {\r\nfree_data_area(udev, cmd);\r\n} else if (se_cmd->data_direction != DMA_NONE) {\r\npr_warn("TCMU: data direction was %d!\n",\r\nse_cmd->data_direction);\r\n}\r\ntarget_complete_cmd(cmd->se_cmd, entry->rsp.scsi_status);\r\ncmd->se_cmd = NULL;\r\nkmem_cache_free(tcmu_cmd_cache, cmd);\r\n}\r\nstatic unsigned int tcmu_handle_completions(struct tcmu_dev *udev)\r\n{\r\nstruct tcmu_mailbox *mb;\r\nunsigned long flags;\r\nint handled = 0;\r\nif (test_bit(TCMU_DEV_BIT_BROKEN, &udev->flags)) {\r\npr_err("ring broken, not handling completions\n");\r\nreturn 0;\r\n}\r\nspin_lock_irqsave(&udev->cmdr_lock, flags);\r\nmb = udev->mb_addr;\r\ntcmu_flush_dcache_range(mb, sizeof(*mb));\r\nwhile (udev->cmdr_last_cleaned != ACCESS_ONCE(mb->cmd_tail)) {\r\nstruct tcmu_cmd_entry *entry = (void *) mb + CMDR_OFF + udev->cmdr_last_cleaned;\r\nstruct tcmu_cmd *cmd;\r\ntcmu_flush_dcache_range(entry, sizeof(*entry));\r\nif (tcmu_hdr_get_op(entry->hdr.len_op) == TCMU_OP_PAD) {\r\nUPDATE_HEAD(udev->cmdr_last_cleaned,\r\ntcmu_hdr_get_len(entry->hdr.len_op),\r\nudev->cmdr_size);\r\ncontinue;\r\n}\r\nWARN_ON(tcmu_hdr_get_op(entry->hdr.len_op) != TCMU_OP_CMD);\r\nspin_lock(&udev->commands_lock);\r\ncmd = idr_find(&udev->commands, entry->hdr.cmd_id);\r\nif (cmd)\r\nidr_remove(&udev->commands, cmd->cmd_id);\r\nspin_unlock(&udev->commands_lock);\r\nif (!cmd) {\r\npr_err("cmd_id not found, ring is broken\n");\r\nset_bit(TCMU_DEV_BIT_BROKEN, &udev->flags);\r\nbreak;\r\n}\r\ntcmu_handle_completion(cmd, entry);\r\nUPDATE_HEAD(udev->cmdr_last_cleaned,\r\ntcmu_hdr_get_len(entry->hdr.len_op),\r\nudev->cmdr_size);\r\nhandled++;\r\n}\r\nif (mb->cmd_tail == mb->cmd_head)\r\ndel_timer(&udev->timeout);\r\nspin_unlock_irqrestore(&udev->cmdr_lock, flags);\r\nwake_up(&udev->wait_cmdr);\r\nreturn handled;\r\n}\r\nstatic int tcmu_check_expired_cmd(int id, void *p, void *data)\r\n{\r\nstruct tcmu_cmd *cmd = p;\r\nif (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags))\r\nreturn 0;\r\nif (!time_after(jiffies, cmd->deadline))\r\nreturn 0;\r\nset_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags);\r\ntarget_complete_cmd(cmd->se_cmd, SAM_STAT_CHECK_CONDITION);\r\ncmd->se_cmd = NULL;\r\nkmem_cache_free(tcmu_cmd_cache, cmd);\r\nreturn 0;\r\n}\r\nstatic void tcmu_device_timedout(unsigned long data)\r\n{\r\nstruct tcmu_dev *udev = (struct tcmu_dev *)data;\r\nunsigned long flags;\r\nint handled;\r\nhandled = tcmu_handle_completions(udev);\r\npr_warn("%d completions handled from timeout\n", handled);\r\nspin_lock_irqsave(&udev->commands_lock, flags);\r\nidr_for_each(&udev->commands, tcmu_check_expired_cmd, NULL);\r\nspin_unlock_irqrestore(&udev->commands_lock, flags);\r\n}\r\nstatic int tcmu_attach_hba(struct se_hba *hba, u32 host_id)\r\n{\r\nstruct tcmu_hba *tcmu_hba;\r\ntcmu_hba = kzalloc(sizeof(struct tcmu_hba), GFP_KERNEL);\r\nif (!tcmu_hba)\r\nreturn -ENOMEM;\r\ntcmu_hba->host_id = host_id;\r\nhba->hba_ptr = tcmu_hba;\r\nreturn 0;\r\n}\r\nstatic void tcmu_detach_hba(struct se_hba *hba)\r\n{\r\nkfree(hba->hba_ptr);\r\nhba->hba_ptr = NULL;\r\n}\r\nstatic struct se_device *tcmu_alloc_device(struct se_hba *hba, const char *name)\r\n{\r\nstruct tcmu_dev *udev;\r\nudev = kzalloc(sizeof(struct tcmu_dev), GFP_KERNEL);\r\nif (!udev)\r\nreturn NULL;\r\nudev->name = kstrdup(name, GFP_KERNEL);\r\nif (!udev->name) {\r\nkfree(udev);\r\nreturn NULL;\r\n}\r\nudev->hba = hba;\r\ninit_waitqueue_head(&udev->wait_cmdr);\r\nspin_lock_init(&udev->cmdr_lock);\r\nidr_init(&udev->commands);\r\nspin_lock_init(&udev->commands_lock);\r\nsetup_timer(&udev->timeout, tcmu_device_timedout,\r\n(unsigned long)udev);\r\nreturn &udev->se_dev;\r\n}\r\nstatic int tcmu_irqcontrol(struct uio_info *info, s32 irq_on)\r\n{\r\nstruct tcmu_dev *tcmu_dev = container_of(info, struct tcmu_dev, uio_info);\r\ntcmu_handle_completions(tcmu_dev);\r\nreturn 0;\r\n}\r\nstatic int tcmu_find_mem_index(struct vm_area_struct *vma)\r\n{\r\nstruct tcmu_dev *udev = vma->vm_private_data;\r\nstruct uio_info *info = &udev->uio_info;\r\nif (vma->vm_pgoff < MAX_UIO_MAPS) {\r\nif (info->mem[vma->vm_pgoff].size == 0)\r\nreturn -1;\r\nreturn (int)vma->vm_pgoff;\r\n}\r\nreturn -1;\r\n}\r\nstatic int tcmu_vma_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct tcmu_dev *udev = vma->vm_private_data;\r\nstruct uio_info *info = &udev->uio_info;\r\nstruct page *page;\r\nunsigned long offset;\r\nvoid *addr;\r\nint mi = tcmu_find_mem_index(vma);\r\nif (mi < 0)\r\nreturn VM_FAULT_SIGBUS;\r\noffset = (vmf->pgoff - mi) << PAGE_SHIFT;\r\naddr = (void *)(unsigned long)info->mem[mi].addr + offset;\r\nif (info->mem[mi].memtype == UIO_MEM_LOGICAL)\r\npage = virt_to_page(addr);\r\nelse\r\npage = vmalloc_to_page(addr);\r\nget_page(page);\r\nvmf->page = page;\r\nreturn 0;\r\n}\r\nstatic int tcmu_mmap(struct uio_info *info, struct vm_area_struct *vma)\r\n{\r\nstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\r\nvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\r\nvma->vm_ops = &tcmu_vm_ops;\r\nvma->vm_private_data = udev;\r\nif (vma_pages(vma) != (TCMU_RING_SIZE >> PAGE_SHIFT))\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nstatic int tcmu_open(struct uio_info *info, struct inode *inode)\r\n{\r\nstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\r\nif (test_and_set_bit(TCMU_DEV_BIT_OPEN, &udev->flags))\r\nreturn -EBUSY;\r\npr_debug("open\n");\r\nreturn 0;\r\n}\r\nstatic int tcmu_release(struct uio_info *info, struct inode *inode)\r\n{\r\nstruct tcmu_dev *udev = container_of(info, struct tcmu_dev, uio_info);\r\nclear_bit(TCMU_DEV_BIT_OPEN, &udev->flags);\r\npr_debug("close\n");\r\nreturn 0;\r\n}\r\nstatic int tcmu_netlink_event(enum tcmu_genl_cmd cmd, const char *name, int minor)\r\n{\r\nstruct sk_buff *skb;\r\nvoid *msg_header;\r\nint ret = -ENOMEM;\r\nskb = genlmsg_new(NLMSG_GOODSIZE, GFP_KERNEL);\r\nif (!skb)\r\nreturn ret;\r\nmsg_header = genlmsg_put(skb, 0, 0, &tcmu_genl_family, 0, cmd);\r\nif (!msg_header)\r\ngoto free_skb;\r\nret = nla_put_string(skb, TCMU_ATTR_DEVICE, name);\r\nif (ret < 0)\r\ngoto free_skb;\r\nret = nla_put_u32(skb, TCMU_ATTR_MINOR, minor);\r\nif (ret < 0)\r\ngoto free_skb;\r\ngenlmsg_end(skb, msg_header);\r\nret = genlmsg_multicast_allns(&tcmu_genl_family, skb, 0,\r\nTCMU_MCGRP_CONFIG, GFP_KERNEL);\r\nif (ret == -ESRCH)\r\nret = 0;\r\nreturn ret;\r\nfree_skb:\r\nnlmsg_free(skb);\r\nreturn ret;\r\n}\r\nstatic int tcmu_configure_device(struct se_device *dev)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nstruct tcmu_hba *hba = udev->hba->hba_ptr;\r\nstruct uio_info *info;\r\nstruct tcmu_mailbox *mb;\r\nsize_t size;\r\nsize_t used;\r\nint ret = 0;\r\nchar *str;\r\ninfo = &udev->uio_info;\r\nsize = snprintf(NULL, 0, "tcm-user/%u/%s/%s", hba->host_id, udev->name,\r\nudev->dev_config);\r\nsize += 1;\r\nstr = kmalloc(size, GFP_KERNEL);\r\nif (!str)\r\nreturn -ENOMEM;\r\nused = snprintf(str, size, "tcm-user/%u/%s", hba->host_id, udev->name);\r\nif (udev->dev_config[0])\r\nsnprintf(str + used, size - used, "/%s", udev->dev_config);\r\ninfo->name = str;\r\nudev->mb_addr = vzalloc(TCMU_RING_SIZE);\r\nif (!udev->mb_addr) {\r\nret = -ENOMEM;\r\ngoto err_vzalloc;\r\n}\r\nudev->cmdr_size = CMDR_SIZE - CMDR_OFF;\r\nudev->data_off = CMDR_SIZE;\r\nudev->data_size = TCMU_RING_SIZE - CMDR_SIZE;\r\nmb = udev->mb_addr;\r\nmb->version = TCMU_MAILBOX_VERSION;\r\nmb->flags = TCMU_MAILBOX_FLAG_CAP_OOOC;\r\nmb->cmdr_off = CMDR_OFF;\r\nmb->cmdr_size = udev->cmdr_size;\r\nWARN_ON(!PAGE_ALIGNED(udev->data_off));\r\nWARN_ON(udev->data_size % PAGE_SIZE);\r\nWARN_ON(udev->data_size % DATA_BLOCK_SIZE);\r\ninfo->version = __stringify(TCMU_MAILBOX_VERSION);\r\ninfo->mem[0].name = "tcm-user command & data buffer";\r\ninfo->mem[0].addr = (phys_addr_t)(uintptr_t)udev->mb_addr;\r\ninfo->mem[0].size = TCMU_RING_SIZE;\r\ninfo->mem[0].memtype = UIO_MEM_VIRTUAL;\r\ninfo->irqcontrol = tcmu_irqcontrol;\r\ninfo->irq = UIO_IRQ_CUSTOM;\r\ninfo->mmap = tcmu_mmap;\r\ninfo->open = tcmu_open;\r\ninfo->release = tcmu_release;\r\nret = uio_register_device(tcmu_root_device, info);\r\nif (ret)\r\ngoto err_register;\r\nif (dev->dev_attrib.hw_block_size == 0)\r\ndev->dev_attrib.hw_block_size = 512;\r\ndev->dev_attrib.hw_max_sectors = 128;\r\ndev->dev_attrib.hw_queue_depth = 128;\r\nret = tcmu_netlink_event(TCMU_CMD_ADDED_DEVICE, udev->uio_info.name,\r\nudev->uio_info.uio_dev->minor);\r\nif (ret)\r\ngoto err_netlink;\r\nreturn 0;\r\nerr_netlink:\r\nuio_unregister_device(&udev->uio_info);\r\nerr_register:\r\nvfree(udev->mb_addr);\r\nerr_vzalloc:\r\nkfree(info->name);\r\nreturn ret;\r\n}\r\nstatic int tcmu_check_and_free_pending_cmd(struct tcmu_cmd *cmd)\r\n{\r\nif (test_bit(TCMU_CMD_BIT_EXPIRED, &cmd->flags)) {\r\nkmem_cache_free(tcmu_cmd_cache, cmd);\r\nreturn 0;\r\n}\r\nreturn -EINVAL;\r\n}\r\nstatic void tcmu_dev_call_rcu(struct rcu_head *p)\r\n{\r\nstruct se_device *dev = container_of(p, struct se_device, rcu_head);\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nkfree(udev);\r\n}\r\nstatic void tcmu_free_device(struct se_device *dev)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nstruct tcmu_cmd *cmd;\r\nbool all_expired = true;\r\nint i;\r\ndel_timer_sync(&udev->timeout);\r\nvfree(udev->mb_addr);\r\nspin_lock_irq(&udev->commands_lock);\r\nidr_for_each_entry(&udev->commands, cmd, i) {\r\nif (tcmu_check_and_free_pending_cmd(cmd) != 0)\r\nall_expired = false;\r\n}\r\nidr_destroy(&udev->commands);\r\nspin_unlock_irq(&udev->commands_lock);\r\nWARN_ON(!all_expired);\r\nif (udev->uio_info.uio_dev) {\r\ntcmu_netlink_event(TCMU_CMD_REMOVED_DEVICE, udev->uio_info.name,\r\nudev->uio_info.uio_dev->minor);\r\nuio_unregister_device(&udev->uio_info);\r\nkfree(udev->uio_info.name);\r\nkfree(udev->name);\r\n}\r\ncall_rcu(&dev->rcu_head, tcmu_dev_call_rcu);\r\n}\r\nstatic ssize_t tcmu_set_configfs_dev_params(struct se_device *dev,\r\nconst char *page, ssize_t count)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nchar *orig, *ptr, *opts, *arg_p;\r\nsubstring_t args[MAX_OPT_ARGS];\r\nint ret = 0, token;\r\nunsigned long tmp_ul;\r\nopts = kstrdup(page, GFP_KERNEL);\r\nif (!opts)\r\nreturn -ENOMEM;\r\norig = opts;\r\nwhile ((ptr = strsep(&opts, ",\n")) != NULL) {\r\nif (!*ptr)\r\ncontinue;\r\ntoken = match_token(ptr, tokens, args);\r\nswitch (token) {\r\ncase Opt_dev_config:\r\nif (match_strlcpy(udev->dev_config, &args[0],\r\nTCMU_CONFIG_LEN) == 0) {\r\nret = -EINVAL;\r\nbreak;\r\n}\r\npr_debug("TCMU: Referencing Path: %s\n", udev->dev_config);\r\nbreak;\r\ncase Opt_dev_size:\r\narg_p = match_strdup(&args[0]);\r\nif (!arg_p) {\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nret = kstrtoul(arg_p, 0, (unsigned long *) &udev->dev_size);\r\nkfree(arg_p);\r\nif (ret < 0)\r\npr_err("kstrtoul() failed for dev_size=\n");\r\nbreak;\r\ncase Opt_hw_block_size:\r\narg_p = match_strdup(&args[0]);\r\nif (!arg_p) {\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\nret = kstrtoul(arg_p, 0, &tmp_ul);\r\nkfree(arg_p);\r\nif (ret < 0) {\r\npr_err("kstrtoul() failed for hw_block_size=\n");\r\nbreak;\r\n}\r\nif (!tmp_ul) {\r\npr_err("hw_block_size must be nonzero\n");\r\nbreak;\r\n}\r\ndev->dev_attrib.hw_block_size = tmp_ul;\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\n}\r\nkfree(orig);\r\nreturn (!ret) ? count : ret;\r\n}\r\nstatic ssize_t tcmu_show_configfs_dev_params(struct se_device *dev, char *b)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nssize_t bl = 0;\r\nbl = sprintf(b + bl, "Config: %s ",\r\nudev->dev_config[0] ? udev->dev_config : "NULL");\r\nbl += sprintf(b + bl, "Size: %zu\n", udev->dev_size);\r\nreturn bl;\r\n}\r\nstatic sector_t tcmu_get_blocks(struct se_device *dev)\r\n{\r\nstruct tcmu_dev *udev = TCMU_DEV(dev);\r\nreturn div_u64(udev->dev_size - dev->dev_attrib.block_size,\r\ndev->dev_attrib.block_size);\r\n}\r\nstatic sense_reason_t\r\ntcmu_pass_op(struct se_cmd *se_cmd)\r\n{\r\nint ret = tcmu_queue_cmd(se_cmd);\r\nif (ret != 0)\r\nreturn TCM_LOGICAL_UNIT_COMMUNICATION_FAILURE;\r\nelse\r\nreturn TCM_NO_SENSE;\r\n}\r\nstatic sense_reason_t\r\ntcmu_parse_cdb(struct se_cmd *cmd)\r\n{\r\nreturn passthrough_parse_cdb(cmd, tcmu_pass_op);\r\n}\r\nstatic int __init tcmu_module_init(void)\r\n{\r\nint ret;\r\nBUILD_BUG_ON((sizeof(struct tcmu_cmd_entry) % TCMU_OP_ALIGN_SIZE) != 0);\r\ntcmu_cmd_cache = kmem_cache_create("tcmu_cmd_cache",\r\nsizeof(struct tcmu_cmd),\r\n__alignof__(struct tcmu_cmd),\r\n0, NULL);\r\nif (!tcmu_cmd_cache)\r\nreturn -ENOMEM;\r\ntcmu_root_device = root_device_register("tcm_user");\r\nif (IS_ERR(tcmu_root_device)) {\r\nret = PTR_ERR(tcmu_root_device);\r\ngoto out_free_cache;\r\n}\r\nret = genl_register_family(&tcmu_genl_family);\r\nif (ret < 0) {\r\ngoto out_unreg_device;\r\n}\r\nret = transport_backend_register(&tcmu_ops);\r\nif (ret)\r\ngoto out_unreg_genl;\r\nreturn 0;\r\nout_unreg_genl:\r\ngenl_unregister_family(&tcmu_genl_family);\r\nout_unreg_device:\r\nroot_device_unregister(tcmu_root_device);\r\nout_free_cache:\r\nkmem_cache_destroy(tcmu_cmd_cache);\r\nreturn ret;\r\n}\r\nstatic void __exit tcmu_module_exit(void)\r\n{\r\ntarget_backend_unregister(&tcmu_ops);\r\ngenl_unregister_family(&tcmu_genl_family);\r\nroot_device_unregister(tcmu_root_device);\r\nkmem_cache_destroy(tcmu_cmd_cache);\r\n}
