static u32 xdr_padsize(u32 len)\r\n{\r\nreturn (len & 3) ? (4 - (len & 3)) : 0;\r\n}\r\nint svc_rdma_map_xdr(struct svcxprt_rdma *xprt,\r\nstruct xdr_buf *xdr,\r\nstruct svc_rdma_req_map *vec,\r\nbool write_chunk_present)\r\n{\r\nint sge_no;\r\nu32 sge_bytes;\r\nu32 page_bytes;\r\nu32 page_off;\r\nint page_no;\r\nif (xdr->len !=\r\n(xdr->head[0].iov_len + xdr->page_len + xdr->tail[0].iov_len)) {\r\npr_err("svcrdma: %s: XDR buffer length error\n", __func__);\r\nreturn -EIO;\r\n}\r\nsge_no = 1;\r\nvec->sge[sge_no].iov_base = xdr->head[0].iov_base;\r\nvec->sge[sge_no].iov_len = xdr->head[0].iov_len;\r\nsge_no++;\r\npage_no = 0;\r\npage_bytes = xdr->page_len;\r\npage_off = xdr->page_base;\r\nwhile (page_bytes) {\r\nvec->sge[sge_no].iov_base =\r\npage_address(xdr->pages[page_no]) + page_off;\r\nsge_bytes = min_t(u32, page_bytes, (PAGE_SIZE - page_off));\r\npage_bytes -= sge_bytes;\r\nvec->sge[sge_no].iov_len = sge_bytes;\r\nsge_no++;\r\npage_no++;\r\npage_off = 0;\r\n}\r\nif (xdr->tail[0].iov_len) {\r\nunsigned char *base = xdr->tail[0].iov_base;\r\nsize_t len = xdr->tail[0].iov_len;\r\nu32 xdr_pad = xdr_padsize(xdr->page_len);\r\nif (write_chunk_present && xdr_pad) {\r\nbase += xdr_pad;\r\nlen -= xdr_pad;\r\n}\r\nif (len) {\r\nvec->sge[sge_no].iov_base = base;\r\nvec->sge[sge_no].iov_len = len;\r\nsge_no++;\r\n}\r\n}\r\ndprintk("svcrdma: %s: sge_no %d page_no %d "\r\n"page_base %u page_len %u head_len %zu tail_len %zu\n",\r\n__func__, sge_no, page_no, xdr->page_base, xdr->page_len,\r\nxdr->head[0].iov_len, xdr->tail[0].iov_len);\r\nvec->count = sge_no;\r\nreturn 0;\r\n}\r\nstatic dma_addr_t dma_map_xdr(struct svcxprt_rdma *xprt,\r\nstruct xdr_buf *xdr,\r\nu32 xdr_off, size_t len, int dir)\r\n{\r\nstruct page *page;\r\ndma_addr_t dma_addr;\r\nif (xdr_off < xdr->head[0].iov_len) {\r\nxdr_off += (unsigned long)xdr->head[0].iov_base & ~PAGE_MASK;\r\npage = virt_to_page(xdr->head[0].iov_base);\r\n} else {\r\nxdr_off -= xdr->head[0].iov_len;\r\nif (xdr_off < xdr->page_len) {\r\nxdr_off += xdr->page_base;\r\npage = xdr->pages[xdr_off >> PAGE_SHIFT];\r\nxdr_off &= ~PAGE_MASK;\r\n} else {\r\nxdr_off -= xdr->page_len;\r\nxdr_off += (unsigned long)\r\nxdr->tail[0].iov_base & ~PAGE_MASK;\r\npage = virt_to_page(xdr->tail[0].iov_base);\r\n}\r\n}\r\ndma_addr = ib_dma_map_page(xprt->sc_cm_id->device, page, xdr_off,\r\nmin_t(size_t, PAGE_SIZE, len), dir);\r\nreturn dma_addr;\r\n}\r\nstruct rpcrdma_read_chunk *\r\nsvc_rdma_get_read_chunk(struct rpcrdma_msg *rmsgp)\r\n{\r\nstruct rpcrdma_read_chunk *ch =\r\n(struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];\r\nif (ch->rc_discrim == xdr_zero)\r\nreturn NULL;\r\nreturn ch;\r\n}\r\nstatic struct rpcrdma_write_array *\r\nsvc_rdma_get_write_array(struct rpcrdma_msg *rmsgp)\r\n{\r\nif (rmsgp->rm_body.rm_chunks[0] != xdr_zero ||\r\nrmsgp->rm_body.rm_chunks[1] == xdr_zero)\r\nreturn NULL;\r\nreturn (struct rpcrdma_write_array *)&rmsgp->rm_body.rm_chunks[1];\r\n}\r\nstatic struct rpcrdma_write_array *\r\nsvc_rdma_get_reply_array(struct rpcrdma_msg *rmsgp,\r\nstruct rpcrdma_write_array *wr_ary)\r\n{\r\nstruct rpcrdma_read_chunk *rch;\r\nstruct rpcrdma_write_array *rp_ary;\r\nif (rmsgp->rm_body.rm_chunks[0] != xdr_zero ||\r\nrmsgp->rm_body.rm_chunks[1] != xdr_zero)\r\nreturn NULL;\r\nrch = svc_rdma_get_read_chunk(rmsgp);\r\nif (rch) {\r\nwhile (rch->rc_discrim != xdr_zero)\r\nrch++;\r\nrp_ary = (struct rpcrdma_write_array *)&rch->rc_target;\r\ngoto found_it;\r\n}\r\nif (wr_ary) {\r\nint chunk = be32_to_cpu(wr_ary->wc_nchunks);\r\nrp_ary = (struct rpcrdma_write_array *)\r\n&wr_ary->wc_array[chunk].wc_target.rs_length;\r\ngoto found_it;\r\n}\r\nrp_ary = (struct rpcrdma_write_array *)&rmsgp->rm_body.rm_chunks[2];\r\nfound_it:\r\nif (rp_ary->wc_discrim == xdr_zero)\r\nreturn NULL;\r\nreturn rp_ary;\r\n}\r\nstatic int send_write(struct svcxprt_rdma *xprt, struct svc_rqst *rqstp,\r\nu32 rmr, u64 to,\r\nu32 xdr_off, int write_len,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nstruct ib_rdma_wr write_wr;\r\nstruct ib_sge *sge;\r\nint xdr_sge_no;\r\nint sge_no;\r\nint sge_bytes;\r\nint sge_off;\r\nint bc;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nif (vec->count > RPCSVC_MAXPAGES) {\r\npr_err("svcrdma: Too many pages (%lu)\n", vec->count);\r\nreturn -EIO;\r\n}\r\ndprintk("svcrdma: RDMA_WRITE rmr=%x, to=%llx, xdr_off=%d, "\r\n"write_len=%d, vec->sge=%p, vec->count=%lu\n",\r\nrmr, (unsigned long long)to, xdr_off,\r\nwrite_len, vec->sge, vec->count);\r\nctxt = svc_rdma_get_context(xprt);\r\nctxt->direction = DMA_TO_DEVICE;\r\nsge = ctxt->sge;\r\nfor (bc = xdr_off, xdr_sge_no = 1; bc && xdr_sge_no < vec->count;\r\nxdr_sge_no++) {\r\nif (vec->sge[xdr_sge_no].iov_len > bc)\r\nbreak;\r\nbc -= vec->sge[xdr_sge_no].iov_len;\r\n}\r\nsge_off = bc;\r\nbc = write_len;\r\nsge_no = 0;\r\nwhile (bc != 0) {\r\nsge_bytes = min_t(size_t,\r\nbc, vec->sge[xdr_sge_no].iov_len-sge_off);\r\nsge[sge_no].length = sge_bytes;\r\nsge[sge_no].addr =\r\ndma_map_xdr(xprt, &rqstp->rq_res, xdr_off,\r\nsge_bytes, DMA_TO_DEVICE);\r\nxdr_off += sge_bytes;\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device,\r\nsge[sge_no].addr))\r\ngoto err;\r\natomic_inc(&xprt->sc_dma_used);\r\nsge[sge_no].lkey = xprt->sc_pd->local_dma_lkey;\r\nctxt->count++;\r\nsge_off = 0;\r\nsge_no++;\r\nxdr_sge_no++;\r\nif (xdr_sge_no > vec->count) {\r\npr_err("svcrdma: Too many sges (%d)\n", xdr_sge_no);\r\ngoto err;\r\n}\r\nbc -= sge_bytes;\r\nif (sge_no == xprt->sc_max_sge)\r\nbreak;\r\n}\r\nmemset(&write_wr, 0, sizeof write_wr);\r\nctxt->cqe.done = svc_rdma_wc_write;\r\nwrite_wr.wr.wr_cqe = &ctxt->cqe;\r\nwrite_wr.wr.sg_list = &sge[0];\r\nwrite_wr.wr.num_sge = sge_no;\r\nwrite_wr.wr.opcode = IB_WR_RDMA_WRITE;\r\nwrite_wr.wr.send_flags = IB_SEND_SIGNALED;\r\nwrite_wr.rkey = rmr;\r\nwrite_wr.remote_addr = to;\r\natomic_inc(&rdma_stat_write);\r\nif (svc_rdma_send(xprt, &write_wr.wr))\r\ngoto err;\r\nreturn write_len - bc;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 0);\r\nreturn -EIO;\r\n}\r\nnoinline\r\nstatic int send_write_chunks(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_write_array *wr_ary,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nu32 xfer_len = rqstp->rq_res.page_len;\r\nint write_len;\r\nu32 xdr_off;\r\nint chunk_off;\r\nint chunk_no;\r\nint nchunks;\r\nstruct rpcrdma_write_array *res_ary;\r\nint ret;\r\nres_ary = (struct rpcrdma_write_array *)\r\n&rdma_resp->rm_body.rm_chunks[1];\r\nnchunks = be32_to_cpu(wr_ary->wc_nchunks);\r\nfor (xdr_off = rqstp->rq_res.head[0].iov_len, chunk_no = 0;\r\nxfer_len && chunk_no < nchunks;\r\nchunk_no++) {\r\nstruct rpcrdma_segment *arg_ch;\r\nu64 rs_offset;\r\narg_ch = &wr_ary->wc_array[chunk_no].wc_target;\r\nwrite_len = min(xfer_len, be32_to_cpu(arg_ch->rs_length));\r\nxdr_decode_hyper((__be32 *)&arg_ch->rs_offset, &rs_offset);\r\nsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\r\narg_ch->rs_handle,\r\narg_ch->rs_offset,\r\nwrite_len);\r\nchunk_off = 0;\r\nwhile (write_len) {\r\nret = send_write(xprt, rqstp,\r\nbe32_to_cpu(arg_ch->rs_handle),\r\nrs_offset + chunk_off,\r\nxdr_off,\r\nwrite_len,\r\nvec);\r\nif (ret <= 0)\r\ngoto out_err;\r\nchunk_off += ret;\r\nxdr_off += ret;\r\nxfer_len -= ret;\r\nwrite_len -= ret;\r\n}\r\n}\r\nsvc_rdma_xdr_encode_write_list(rdma_resp, chunk_no);\r\nreturn rqstp->rq_res.page_len;\r\nout_err:\r\npr_err("svcrdma: failed to send write chunks, rc=%d\n", ret);\r\nreturn -EIO;\r\n}\r\nnoinline\r\nstatic int send_reply_chunks(struct svcxprt_rdma *xprt,\r\nstruct rpcrdma_write_array *rp_ary,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rqst *rqstp,\r\nstruct svc_rdma_req_map *vec)\r\n{\r\nu32 xfer_len = rqstp->rq_res.len;\r\nint write_len;\r\nu32 xdr_off;\r\nint chunk_no;\r\nint chunk_off;\r\nint nchunks;\r\nstruct rpcrdma_segment *ch;\r\nstruct rpcrdma_write_array *res_ary;\r\nint ret;\r\nres_ary = (struct rpcrdma_write_array *)\r\n&rdma_resp->rm_body.rm_chunks[2];\r\nnchunks = be32_to_cpu(rp_ary->wc_nchunks);\r\nfor (xdr_off = 0, chunk_no = 0;\r\nxfer_len && chunk_no < nchunks;\r\nchunk_no++) {\r\nu64 rs_offset;\r\nch = &rp_ary->wc_array[chunk_no].wc_target;\r\nwrite_len = min(xfer_len, be32_to_cpu(ch->rs_length));\r\nxdr_decode_hyper((__be32 *)&ch->rs_offset, &rs_offset);\r\nsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\r\nch->rs_handle, ch->rs_offset,\r\nwrite_len);\r\nchunk_off = 0;\r\nwhile (write_len) {\r\nret = send_write(xprt, rqstp,\r\nbe32_to_cpu(ch->rs_handle),\r\nrs_offset + chunk_off,\r\nxdr_off,\r\nwrite_len,\r\nvec);\r\nif (ret <= 0)\r\ngoto out_err;\r\nchunk_off += ret;\r\nxdr_off += ret;\r\nxfer_len -= ret;\r\nwrite_len -= ret;\r\n}\r\n}\r\nsvc_rdma_xdr_encode_reply_array(res_ary, chunk_no);\r\nreturn rqstp->rq_res.len;\r\nout_err:\r\npr_err("svcrdma: failed to send reply chunks, rc=%d\n", ret);\r\nreturn -EIO;\r\n}\r\nstatic int send_reply(struct svcxprt_rdma *rdma,\r\nstruct svc_rqst *rqstp,\r\nstruct page *page,\r\nstruct rpcrdma_msg *rdma_resp,\r\nstruct svc_rdma_req_map *vec,\r\nint byte_count)\r\n{\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nstruct ib_send_wr send_wr;\r\nu32 xdr_off;\r\nint sge_no;\r\nint sge_bytes;\r\nint page_no;\r\nint pages;\r\nint ret = -EIO;\r\nctxt = svc_rdma_get_context(rdma);\r\nctxt->direction = DMA_TO_DEVICE;\r\nctxt->pages[0] = page;\r\nctxt->count = 1;\r\nctxt->sge[0].lkey = rdma->sc_pd->local_dma_lkey;\r\nctxt->sge[0].length = svc_rdma_xdr_get_reply_hdr_len(rdma_resp);\r\nctxt->sge[0].addr =\r\nib_dma_map_page(rdma->sc_cm_id->device, page, 0,\r\nctxt->sge[0].length, DMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr))\r\ngoto err;\r\natomic_inc(&rdma->sc_dma_used);\r\nctxt->direction = DMA_TO_DEVICE;\r\nxdr_off = 0;\r\nfor (sge_no = 1; byte_count && sge_no < vec->count; sge_no++) {\r\nsge_bytes = min_t(size_t, vec->sge[sge_no].iov_len, byte_count);\r\nbyte_count -= sge_bytes;\r\nctxt->sge[sge_no].addr =\r\ndma_map_xdr(rdma, &rqstp->rq_res, xdr_off,\r\nsge_bytes, DMA_TO_DEVICE);\r\nxdr_off += sge_bytes;\r\nif (ib_dma_mapping_error(rdma->sc_cm_id->device,\r\nctxt->sge[sge_no].addr))\r\ngoto err;\r\natomic_inc(&rdma->sc_dma_used);\r\nctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;\r\nctxt->sge[sge_no].length = sge_bytes;\r\n}\r\nif (byte_count != 0) {\r\npr_err("svcrdma: Could not map %d bytes\n", byte_count);\r\ngoto err;\r\n}\r\npages = rqstp->rq_next_page - rqstp->rq_respages;\r\nfor (page_no = 0; page_no < pages; page_no++) {\r\nctxt->pages[page_no+1] = rqstp->rq_respages[page_no];\r\nctxt->count++;\r\nrqstp->rq_respages[page_no] = NULL;\r\nif (page_no+1 >= sge_no)\r\nctxt->sge[page_no+1].length = 0;\r\n}\r\nrqstp->rq_next_page = rqstp->rq_respages + 1;\r\nif (sge_no > ctxt->count)\r\natomic_dec(&rdma->sc_dma_used);\r\nif (sge_no > rdma->sc_max_sge) {\r\npr_err("svcrdma: Too many sges (%d)\n", sge_no);\r\ngoto err;\r\n}\r\nmemset(&send_wr, 0, sizeof send_wr);\r\nctxt->cqe.done = svc_rdma_wc_send;\r\nsend_wr.wr_cqe = &ctxt->cqe;\r\nsend_wr.sg_list = ctxt->sge;\r\nsend_wr.num_sge = sge_no;\r\nsend_wr.opcode = IB_WR_SEND;\r\nsend_wr.send_flags = IB_SEND_SIGNALED;\r\nret = svc_rdma_send(rdma, &send_wr);\r\nif (ret)\r\ngoto err;\r\nreturn 0;\r\nerr:\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn ret;\r\n}\r\nvoid svc_rdma_prep_reply_hdr(struct svc_rqst *rqstp)\r\n{\r\n}\r\nint svc_rdma_sendto(struct svc_rqst *rqstp)\r\n{\r\nstruct svc_xprt *xprt = rqstp->rq_xprt;\r\nstruct svcxprt_rdma *rdma =\r\ncontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\r\nstruct rpcrdma_msg *rdma_argp;\r\nstruct rpcrdma_msg *rdma_resp;\r\nstruct rpcrdma_write_array *wr_ary, *rp_ary;\r\nenum rpcrdma_proc reply_type;\r\nint ret;\r\nint inline_bytes;\r\nstruct page *res_page;\r\nstruct svc_rdma_req_map *vec;\r\ndprintk("svcrdma: sending response for rqstp=%p\n", rqstp);\r\nrdma_argp = page_address(rqstp->rq_pages[0]);\r\nwr_ary = svc_rdma_get_write_array(rdma_argp);\r\nrp_ary = svc_rdma_get_reply_array(rdma_argp, wr_ary);\r\nvec = svc_rdma_get_req_map(rdma);\r\nret = svc_rdma_map_xdr(rdma, &rqstp->rq_res, vec, wr_ary != NULL);\r\nif (ret)\r\ngoto err0;\r\ninline_bytes = rqstp->rq_res.len;\r\nret = -ENOMEM;\r\nres_page = alloc_page(GFP_KERNEL);\r\nif (!res_page)\r\ngoto err0;\r\nrdma_resp = page_address(res_page);\r\nif (rp_ary)\r\nreply_type = RDMA_NOMSG;\r\nelse\r\nreply_type = RDMA_MSG;\r\nsvc_rdma_xdr_encode_reply_header(rdma, rdma_argp,\r\nrdma_resp, reply_type);\r\nif (wr_ary) {\r\nret = send_write_chunks(rdma, wr_ary, rdma_resp, rqstp, vec);\r\nif (ret < 0)\r\ngoto err1;\r\ninline_bytes -= ret + xdr_padsize(ret);\r\n}\r\nif (rp_ary) {\r\nret = send_reply_chunks(rdma, rp_ary, rdma_resp, rqstp, vec);\r\nif (ret < 0)\r\ngoto err1;\r\ninline_bytes -= ret;\r\n}\r\nret = svc_rdma_post_recv(rdma, GFP_KERNEL);\r\nif (ret)\r\ngoto err1;\r\nret = send_reply(rdma, rqstp, res_page, rdma_resp, vec,\r\ninline_bytes);\r\nif (ret < 0)\r\ngoto err1;\r\nsvc_rdma_put_req_map(rdma, vec);\r\ndprintk("svcrdma: send_reply returns %d\n", ret);\r\nreturn ret;\r\nerr1:\r\nput_page(res_page);\r\nerr0:\r\nsvc_rdma_put_req_map(rdma, vec);\r\npr_err("svcrdma: Could not send reply, err=%d. Closing transport.\n",\r\nret);\r\nset_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);\r\nreturn -ENOTCONN;\r\n}\r\nvoid svc_rdma_send_error(struct svcxprt_rdma *xprt, struct rpcrdma_msg *rmsgp,\r\nint status)\r\n{\r\nstruct ib_send_wr err_wr;\r\nstruct page *p;\r\nstruct svc_rdma_op_ctxt *ctxt;\r\nenum rpcrdma_errcode err;\r\n__be32 *va;\r\nint length;\r\nint ret;\r\nret = svc_rdma_repost_recv(xprt, GFP_KERNEL);\r\nif (ret)\r\nreturn;\r\np = alloc_page(GFP_KERNEL);\r\nif (!p)\r\nreturn;\r\nva = page_address(p);\r\nerr = ERR_CHUNK;\r\nif (status == -EPROTONOSUPPORT)\r\nerr = ERR_VERS;\r\nlength = svc_rdma_xdr_encode_error(xprt, rmsgp, err, va);\r\nctxt = svc_rdma_get_context(xprt);\r\nctxt->direction = DMA_TO_DEVICE;\r\nctxt->count = 1;\r\nctxt->pages[0] = p;\r\nctxt->sge[0].lkey = xprt->sc_pd->local_dma_lkey;\r\nctxt->sge[0].length = length;\r\nctxt->sge[0].addr = ib_dma_map_page(xprt->sc_cm_id->device,\r\np, 0, length, DMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(xprt->sc_cm_id->device, ctxt->sge[0].addr)) {\r\ndprintk("svcrdma: Error mapping buffer for protocol error\n");\r\nsvc_rdma_put_context(ctxt, 1);\r\nreturn;\r\n}\r\natomic_inc(&xprt->sc_dma_used);\r\nmemset(&err_wr, 0, sizeof(err_wr));\r\nctxt->cqe.done = svc_rdma_wc_send;\r\nerr_wr.wr_cqe = &ctxt->cqe;\r\nerr_wr.sg_list = ctxt->sge;\r\nerr_wr.num_sge = 1;\r\nerr_wr.opcode = IB_WR_SEND;\r\nerr_wr.send_flags = IB_SEND_SIGNALED;\r\nret = svc_rdma_send(xprt, &err_wr);\r\nif (ret) {\r\ndprintk("svcrdma: Error %d posting send for protocol error\n",\r\nret);\r\nsvc_rdma_unmap_dma(ctxt);\r\nsvc_rdma_put_context(ctxt, 1);\r\n}\r\n}
