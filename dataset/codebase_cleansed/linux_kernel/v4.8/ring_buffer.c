void hv_begin_read(struct hv_ring_buffer_info *rbi)\r\n{\r\nrbi->ring_buffer->interrupt_mask = 1;\r\nvirt_mb();\r\n}\r\nu32 hv_end_read(struct hv_ring_buffer_info *rbi)\r\n{\r\nrbi->ring_buffer->interrupt_mask = 0;\r\nvirt_mb();\r\nreturn hv_get_bytes_to_read(rbi);\r\n}\r\nstatic bool hv_need_to_signal(u32 old_write, struct hv_ring_buffer_info *rbi)\r\n{\r\nvirt_mb();\r\nif (READ_ONCE(rbi->ring_buffer->interrupt_mask))\r\nreturn false;\r\nvirt_rmb();\r\nif (old_write == READ_ONCE(rbi->ring_buffer->read_index))\r\nreturn true;\r\nreturn false;\r\n}\r\nstatic inline u32\r\nhv_get_next_write_location(struct hv_ring_buffer_info *ring_info)\r\n{\r\nu32 next = ring_info->ring_buffer->write_index;\r\nreturn next;\r\n}\r\nstatic inline void\r\nhv_set_next_write_location(struct hv_ring_buffer_info *ring_info,\r\nu32 next_write_location)\r\n{\r\nring_info->ring_buffer->write_index = next_write_location;\r\n}\r\nstatic inline u32\r\nhv_get_next_read_location(struct hv_ring_buffer_info *ring_info)\r\n{\r\nu32 next = ring_info->ring_buffer->read_index;\r\nreturn next;\r\n}\r\nstatic inline u32\r\nhv_get_next_readlocation_withoffset(struct hv_ring_buffer_info *ring_info,\r\nu32 offset)\r\n{\r\nu32 next = ring_info->ring_buffer->read_index;\r\nnext += offset;\r\nnext %= ring_info->ring_datasize;\r\nreturn next;\r\n}\r\nstatic inline void\r\nhv_set_next_read_location(struct hv_ring_buffer_info *ring_info,\r\nu32 next_read_location)\r\n{\r\nring_info->ring_buffer->read_index = next_read_location;\r\nring_info->priv_read_index = next_read_location;\r\n}\r\nstatic inline u32\r\nhv_get_ring_buffersize(struct hv_ring_buffer_info *ring_info)\r\n{\r\nreturn ring_info->ring_datasize;\r\n}\r\nstatic inline u64\r\nhv_get_ring_bufferindices(struct hv_ring_buffer_info *ring_info)\r\n{\r\nreturn (u64)ring_info->ring_buffer->write_index << 32;\r\n}\r\nstatic u32 hv_copyfrom_ringbuffer(\r\nstruct hv_ring_buffer_info *ring_info,\r\nvoid *dest,\r\nu32 destlen,\r\nu32 start_read_offset)\r\n{\r\nvoid *ring_buffer = hv_get_ring_buffer(ring_info);\r\nu32 ring_buffer_size = hv_get_ring_buffersize(ring_info);\r\nu32 frag_len;\r\nif (destlen > ring_buffer_size - start_read_offset) {\r\nfrag_len = ring_buffer_size - start_read_offset;\r\nmemcpy(dest, ring_buffer + start_read_offset, frag_len);\r\nmemcpy(dest + frag_len, ring_buffer, destlen - frag_len);\r\n} else\r\nmemcpy(dest, ring_buffer + start_read_offset, destlen);\r\nstart_read_offset += destlen;\r\nstart_read_offset %= ring_buffer_size;\r\nreturn start_read_offset;\r\n}\r\nstatic u32 hv_copyto_ringbuffer(\r\nstruct hv_ring_buffer_info *ring_info,\r\nu32 start_write_offset,\r\nvoid *src,\r\nu32 srclen)\r\n{\r\nvoid *ring_buffer = hv_get_ring_buffer(ring_info);\r\nu32 ring_buffer_size = hv_get_ring_buffersize(ring_info);\r\nu32 frag_len;\r\nif (srclen > ring_buffer_size - start_write_offset) {\r\nfrag_len = ring_buffer_size - start_write_offset;\r\nmemcpy(ring_buffer + start_write_offset, src, frag_len);\r\nmemcpy(ring_buffer, src + frag_len, srclen - frag_len);\r\n} else\r\nmemcpy(ring_buffer + start_write_offset, src, srclen);\r\nstart_write_offset += srclen;\r\nstart_write_offset %= ring_buffer_size;\r\nreturn start_write_offset;\r\n}\r\nvoid hv_ringbuffer_get_debuginfo(struct hv_ring_buffer_info *ring_info,\r\nstruct hv_ring_buffer_debug_info *debug_info)\r\n{\r\nu32 bytes_avail_towrite;\r\nu32 bytes_avail_toread;\r\nif (ring_info->ring_buffer) {\r\nhv_get_ringbuffer_availbytes(ring_info,\r\n&bytes_avail_toread,\r\n&bytes_avail_towrite);\r\ndebug_info->bytes_avail_toread = bytes_avail_toread;\r\ndebug_info->bytes_avail_towrite = bytes_avail_towrite;\r\ndebug_info->current_read_index =\r\nring_info->ring_buffer->read_index;\r\ndebug_info->current_write_index =\r\nring_info->ring_buffer->write_index;\r\ndebug_info->current_interrupt_mask =\r\nring_info->ring_buffer->interrupt_mask;\r\n}\r\n}\r\nint hv_ringbuffer_init(struct hv_ring_buffer_info *ring_info,\r\nvoid *buffer, u32 buflen)\r\n{\r\nif (sizeof(struct hv_ring_buffer) != PAGE_SIZE)\r\nreturn -EINVAL;\r\nmemset(ring_info, 0, sizeof(struct hv_ring_buffer_info));\r\nring_info->ring_buffer = (struct hv_ring_buffer *)buffer;\r\nring_info->ring_buffer->read_index =\r\nring_info->ring_buffer->write_index = 0;\r\nring_info->ring_buffer->feature_bits.value = 1;\r\nring_info->ring_size = buflen;\r\nring_info->ring_datasize = buflen - sizeof(struct hv_ring_buffer);\r\nspin_lock_init(&ring_info->ring_lock);\r\nreturn 0;\r\n}\r\nvoid hv_ringbuffer_cleanup(struct hv_ring_buffer_info *ring_info)\r\n{\r\n}\r\nint hv_ringbuffer_write(struct hv_ring_buffer_info *outring_info,\r\nstruct kvec *kv_list, u32 kv_count, bool *signal, bool lock)\r\n{\r\nint i = 0;\r\nu32 bytes_avail_towrite;\r\nu32 totalbytes_towrite = 0;\r\nu32 next_write_location;\r\nu32 old_write;\r\nu64 prev_indices = 0;\r\nunsigned long flags = 0;\r\nfor (i = 0; i < kv_count; i++)\r\ntotalbytes_towrite += kv_list[i].iov_len;\r\ntotalbytes_towrite += sizeof(u64);\r\nif (lock)\r\nspin_lock_irqsave(&outring_info->ring_lock, flags);\r\nbytes_avail_towrite = hv_get_bytes_to_write(outring_info);\r\nif (bytes_avail_towrite <= totalbytes_towrite) {\r\nif (lock)\r\nspin_unlock_irqrestore(&outring_info->ring_lock, flags);\r\nreturn -EAGAIN;\r\n}\r\nnext_write_location = hv_get_next_write_location(outring_info);\r\nold_write = next_write_location;\r\nfor (i = 0; i < kv_count; i++) {\r\nnext_write_location = hv_copyto_ringbuffer(outring_info,\r\nnext_write_location,\r\nkv_list[i].iov_base,\r\nkv_list[i].iov_len);\r\n}\r\nprev_indices = hv_get_ring_bufferindices(outring_info);\r\nnext_write_location = hv_copyto_ringbuffer(outring_info,\r\nnext_write_location,\r\n&prev_indices,\r\nsizeof(u64));\r\nvirt_mb();\r\nhv_set_next_write_location(outring_info, next_write_location);\r\nif (lock)\r\nspin_unlock_irqrestore(&outring_info->ring_lock, flags);\r\n*signal = hv_need_to_signal(old_write, outring_info);\r\nreturn 0;\r\n}\r\nint hv_ringbuffer_read(struct hv_ring_buffer_info *inring_info,\r\nvoid *buffer, u32 buflen, u32 *buffer_actual_len,\r\nu64 *requestid, bool *signal, bool raw)\r\n{\r\nu32 bytes_avail_toread;\r\nu32 next_read_location = 0;\r\nu64 prev_indices = 0;\r\nstruct vmpacket_descriptor desc;\r\nu32 offset;\r\nu32 packetlen;\r\nint ret = 0;\r\nif (buflen <= 0)\r\nreturn -EINVAL;\r\n*buffer_actual_len = 0;\r\n*requestid = 0;\r\nbytes_avail_toread = hv_get_bytes_to_read(inring_info);\r\nif (bytes_avail_toread < sizeof(desc)) {\r\nreturn ret;\r\n}\r\nnext_read_location = hv_get_next_read_location(inring_info);\r\nnext_read_location = hv_copyfrom_ringbuffer(inring_info, &desc,\r\nsizeof(desc),\r\nnext_read_location);\r\noffset = raw ? 0 : (desc.offset8 << 3);\r\npacketlen = (desc.len8 << 3) - offset;\r\n*buffer_actual_len = packetlen;\r\n*requestid = desc.trans_id;\r\nif (bytes_avail_toread < packetlen + offset)\r\nreturn -EAGAIN;\r\nif (packetlen > buflen)\r\nreturn -ENOBUFS;\r\nnext_read_location =\r\nhv_get_next_readlocation_withoffset(inring_info, offset);\r\nnext_read_location = hv_copyfrom_ringbuffer(inring_info,\r\nbuffer,\r\npacketlen,\r\nnext_read_location);\r\nnext_read_location = hv_copyfrom_ringbuffer(inring_info,\r\n&prev_indices,\r\nsizeof(u64),\r\nnext_read_location);\r\nvirt_mb();\r\nhv_set_next_read_location(inring_info, next_read_location);\r\n*signal = hv_need_to_signal_on_read(inring_info);\r\nreturn ret;\r\n}
