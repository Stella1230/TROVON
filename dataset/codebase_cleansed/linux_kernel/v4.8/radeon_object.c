static void radeon_update_memory_usage(struct radeon_bo *bo,\r\nunsigned mem_type, int sign)\r\n{\r\nstruct radeon_device *rdev = bo->rdev;\r\nu64 size = (u64)bo->tbo.num_pages << PAGE_SHIFT;\r\nswitch (mem_type) {\r\ncase TTM_PL_TT:\r\nif (sign > 0)\r\natomic64_add(size, &rdev->gtt_usage);\r\nelse\r\natomic64_sub(size, &rdev->gtt_usage);\r\nbreak;\r\ncase TTM_PL_VRAM:\r\nif (sign > 0)\r\natomic64_add(size, &rdev->vram_usage);\r\nelse\r\natomic64_sub(size, &rdev->vram_usage);\r\nbreak;\r\n}\r\n}\r\nstatic void radeon_ttm_bo_destroy(struct ttm_buffer_object *tbo)\r\n{\r\nstruct radeon_bo *bo;\r\nbo = container_of(tbo, struct radeon_bo, tbo);\r\nradeon_update_memory_usage(bo, bo->tbo.mem.mem_type, -1);\r\nmutex_lock(&bo->rdev->gem.mutex);\r\nlist_del_init(&bo->list);\r\nmutex_unlock(&bo->rdev->gem.mutex);\r\nradeon_bo_clear_surface_reg(bo);\r\nWARN_ON(!list_empty(&bo->va));\r\ndrm_gem_object_release(&bo->gem_base);\r\nkfree(bo);\r\n}\r\nbool radeon_ttm_bo_is_radeon_bo(struct ttm_buffer_object *bo)\r\n{\r\nif (bo->destroy == &radeon_ttm_bo_destroy)\r\nreturn true;\r\nreturn false;\r\n}\r\nvoid radeon_ttm_placement_from_domain(struct radeon_bo *rbo, u32 domain)\r\n{\r\nu32 c = 0, i;\r\nrbo->placement.placement = rbo->placements;\r\nrbo->placement.busy_placement = rbo->placements;\r\nif (domain & RADEON_GEM_DOMAIN_VRAM) {\r\nif ((rbo->flags & RADEON_GEM_NO_CPU_ACCESS) &&\r\nrbo->rdev->mc.visible_vram_size < rbo->rdev->mc.real_vram_size) {\r\nrbo->placements[c].fpfn =\r\nrbo->rdev->mc.visible_vram_size >> PAGE_SHIFT;\r\nrbo->placements[c++].flags = TTM_PL_FLAG_WC |\r\nTTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_VRAM;\r\n}\r\nrbo->placements[c].fpfn = 0;\r\nrbo->placements[c++].flags = TTM_PL_FLAG_WC |\r\nTTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_VRAM;\r\n}\r\nif (domain & RADEON_GEM_DOMAIN_GTT) {\r\nif (rbo->flags & RADEON_GEM_GTT_UC) {\r\nrbo->placements[c].fpfn = 0;\r\nrbo->placements[c++].flags = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_TT;\r\n} else if ((rbo->flags & RADEON_GEM_GTT_WC) ||\r\n(rbo->rdev->flags & RADEON_IS_AGP)) {\r\nrbo->placements[c].fpfn = 0;\r\nrbo->placements[c++].flags = TTM_PL_FLAG_WC |\r\nTTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_TT;\r\n} else {\r\nrbo->placements[c].fpfn = 0;\r\nrbo->placements[c++].flags = TTM_PL_FLAG_CACHED |\r\nTTM_PL_FLAG_TT;\r\n}\r\n}\r\nif (domain & RADEON_GEM_DOMAIN_CPU) {\r\nif (rbo->flags & RADEON_GEM_GTT_UC) {\r\nrbo->placements[c].fpfn = 0;\r\nrbo->placements[c++].flags = TTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_SYSTEM;\r\n} else if ((rbo->flags & RADEON_GEM_GTT_WC) ||\r\nrbo->rdev->flags & RADEON_IS_AGP) {\r\nrbo->placements[c].fpfn = 0;\r\nrbo->placements[c++].flags = TTM_PL_FLAG_WC |\r\nTTM_PL_FLAG_UNCACHED |\r\nTTM_PL_FLAG_SYSTEM;\r\n} else {\r\nrbo->placements[c].fpfn = 0;\r\nrbo->placements[c++].flags = TTM_PL_FLAG_CACHED |\r\nTTM_PL_FLAG_SYSTEM;\r\n}\r\n}\r\nif (!c) {\r\nrbo->placements[c].fpfn = 0;\r\nrbo->placements[c++].flags = TTM_PL_MASK_CACHING |\r\nTTM_PL_FLAG_SYSTEM;\r\n}\r\nrbo->placement.num_placement = c;\r\nrbo->placement.num_busy_placement = c;\r\nfor (i = 0; i < c; ++i) {\r\nif ((rbo->flags & RADEON_GEM_CPU_ACCESS) &&\r\n(rbo->placements[i].flags & TTM_PL_FLAG_VRAM) &&\r\n!rbo->placements[i].fpfn)\r\nrbo->placements[i].lpfn =\r\nrbo->rdev->mc.visible_vram_size >> PAGE_SHIFT;\r\nelse\r\nrbo->placements[i].lpfn = 0;\r\n}\r\n}\r\nint radeon_bo_create(struct radeon_device *rdev,\r\nunsigned long size, int byte_align, bool kernel,\r\nu32 domain, u32 flags, struct sg_table *sg,\r\nstruct reservation_object *resv,\r\nstruct radeon_bo **bo_ptr)\r\n{\r\nstruct radeon_bo *bo;\r\nenum ttm_bo_type type;\r\nunsigned long page_align = roundup(byte_align, PAGE_SIZE) >> PAGE_SHIFT;\r\nsize_t acc_size;\r\nint r;\r\nsize = ALIGN(size, PAGE_SIZE);\r\nif (kernel) {\r\ntype = ttm_bo_type_kernel;\r\n} else if (sg) {\r\ntype = ttm_bo_type_sg;\r\n} else {\r\ntype = ttm_bo_type_device;\r\n}\r\n*bo_ptr = NULL;\r\nacc_size = ttm_bo_dma_acc_size(&rdev->mman.bdev, size,\r\nsizeof(struct radeon_bo));\r\nbo = kzalloc(sizeof(struct radeon_bo), GFP_KERNEL);\r\nif (bo == NULL)\r\nreturn -ENOMEM;\r\nr = drm_gem_object_init(rdev->ddev, &bo->gem_base, size);\r\nif (unlikely(r)) {\r\nkfree(bo);\r\nreturn r;\r\n}\r\nbo->rdev = rdev;\r\nbo->surface_reg = -1;\r\nINIT_LIST_HEAD(&bo->list);\r\nINIT_LIST_HEAD(&bo->va);\r\nbo->initial_domain = domain & (RADEON_GEM_DOMAIN_VRAM |\r\nRADEON_GEM_DOMAIN_GTT |\r\nRADEON_GEM_DOMAIN_CPU);\r\nbo->flags = flags;\r\nif (!(rdev->flags & RADEON_IS_PCIE))\r\nbo->flags &= ~(RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC);\r\nif (rdev->family >= CHIP_RV610 && rdev->family <= CHIP_RV635)\r\nbo->flags &= ~(RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC);\r\n#ifdef CONFIG_X86_32\r\nbo->flags &= ~(RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC);\r\n#elif defined(CONFIG_X86) && !defined(CONFIG_X86_PAT)\r\n#warning Please enable CONFIG_MTRR and CONFIG_X86_PAT for better performance \\r\nthanks to write-combining\r\nif (bo->flags & RADEON_GEM_GTT_WC)\r\nDRM_INFO_ONCE("Please enable CONFIG_MTRR and CONFIG_X86_PAT for "\r\n"better performance thanks to write-combining\n");\r\nbo->flags &= ~(RADEON_GEM_GTT_WC | RADEON_GEM_GTT_UC);\r\n#else\r\nif (!drm_arch_can_wc_memory())\r\nbo->flags &= ~RADEON_GEM_GTT_WC;\r\n#endif\r\nradeon_ttm_placement_from_domain(bo, domain);\r\ndown_read(&rdev->pm.mclk_lock);\r\nr = ttm_bo_init(&rdev->mman.bdev, &bo->tbo, size, type,\r\n&bo->placement, page_align, !kernel, NULL,\r\nacc_size, sg, resv, &radeon_ttm_bo_destroy);\r\nup_read(&rdev->pm.mclk_lock);\r\nif (unlikely(r != 0)) {\r\nreturn r;\r\n}\r\n*bo_ptr = bo;\r\ntrace_radeon_bo_create(bo);\r\nreturn 0;\r\n}\r\nint radeon_bo_kmap(struct radeon_bo *bo, void **ptr)\r\n{\r\nbool is_iomem;\r\nint r;\r\nif (bo->kptr) {\r\nif (ptr) {\r\n*ptr = bo->kptr;\r\n}\r\nreturn 0;\r\n}\r\nr = ttm_bo_kmap(&bo->tbo, 0, bo->tbo.num_pages, &bo->kmap);\r\nif (r) {\r\nreturn r;\r\n}\r\nbo->kptr = ttm_kmap_obj_virtual(&bo->kmap, &is_iomem);\r\nif (ptr) {\r\n*ptr = bo->kptr;\r\n}\r\nradeon_bo_check_tiling(bo, 0, 0);\r\nreturn 0;\r\n}\r\nvoid radeon_bo_kunmap(struct radeon_bo *bo)\r\n{\r\nif (bo->kptr == NULL)\r\nreturn;\r\nbo->kptr = NULL;\r\nradeon_bo_check_tiling(bo, 0, 0);\r\nttm_bo_kunmap(&bo->kmap);\r\n}\r\nstruct radeon_bo *radeon_bo_ref(struct radeon_bo *bo)\r\n{\r\nif (bo == NULL)\r\nreturn NULL;\r\nttm_bo_reference(&bo->tbo);\r\nreturn bo;\r\n}\r\nvoid radeon_bo_unref(struct radeon_bo **bo)\r\n{\r\nstruct ttm_buffer_object *tbo;\r\nstruct radeon_device *rdev;\r\nif ((*bo) == NULL)\r\nreturn;\r\nrdev = (*bo)->rdev;\r\ntbo = &((*bo)->tbo);\r\nttm_bo_unref(&tbo);\r\nif (tbo == NULL)\r\n*bo = NULL;\r\n}\r\nint radeon_bo_pin_restricted(struct radeon_bo *bo, u32 domain, u64 max_offset,\r\nu64 *gpu_addr)\r\n{\r\nint r, i;\r\nif (radeon_ttm_tt_has_userptr(bo->tbo.ttm))\r\nreturn -EPERM;\r\nif (bo->pin_count) {\r\nbo->pin_count++;\r\nif (gpu_addr)\r\n*gpu_addr = radeon_bo_gpu_offset(bo);\r\nif (max_offset != 0) {\r\nu64 domain_start;\r\nif (domain == RADEON_GEM_DOMAIN_VRAM)\r\ndomain_start = bo->rdev->mc.vram_start;\r\nelse\r\ndomain_start = bo->rdev->mc.gtt_start;\r\nWARN_ON_ONCE(max_offset <\r\n(radeon_bo_gpu_offset(bo) - domain_start));\r\n}\r\nreturn 0;\r\n}\r\nradeon_ttm_placement_from_domain(bo, domain);\r\nfor (i = 0; i < bo->placement.num_placement; i++) {\r\nif ((bo->placements[i].flags & TTM_PL_FLAG_VRAM) &&\r\n!(bo->flags & RADEON_GEM_NO_CPU_ACCESS) &&\r\n(!max_offset || max_offset > bo->rdev->mc.visible_vram_size))\r\nbo->placements[i].lpfn =\r\nbo->rdev->mc.visible_vram_size >> PAGE_SHIFT;\r\nelse\r\nbo->placements[i].lpfn = max_offset >> PAGE_SHIFT;\r\nbo->placements[i].flags |= TTM_PL_FLAG_NO_EVICT;\r\n}\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (likely(r == 0)) {\r\nbo->pin_count = 1;\r\nif (gpu_addr != NULL)\r\n*gpu_addr = radeon_bo_gpu_offset(bo);\r\nif (domain == RADEON_GEM_DOMAIN_VRAM)\r\nbo->rdev->vram_pin_size += radeon_bo_size(bo);\r\nelse\r\nbo->rdev->gart_pin_size += radeon_bo_size(bo);\r\n} else {\r\ndev_err(bo->rdev->dev, "%p pin failed\n", bo);\r\n}\r\nreturn r;\r\n}\r\nint radeon_bo_pin(struct radeon_bo *bo, u32 domain, u64 *gpu_addr)\r\n{\r\nreturn radeon_bo_pin_restricted(bo, domain, 0, gpu_addr);\r\n}\r\nint radeon_bo_unpin(struct radeon_bo *bo)\r\n{\r\nint r, i;\r\nif (!bo->pin_count) {\r\ndev_warn(bo->rdev->dev, "%p unpin not necessary\n", bo);\r\nreturn 0;\r\n}\r\nbo->pin_count--;\r\nif (bo->pin_count)\r\nreturn 0;\r\nfor (i = 0; i < bo->placement.num_placement; i++) {\r\nbo->placements[i].lpfn = 0;\r\nbo->placements[i].flags &= ~TTM_PL_FLAG_NO_EVICT;\r\n}\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, false, false);\r\nif (likely(r == 0)) {\r\nif (bo->tbo.mem.mem_type == TTM_PL_VRAM)\r\nbo->rdev->vram_pin_size -= radeon_bo_size(bo);\r\nelse\r\nbo->rdev->gart_pin_size -= radeon_bo_size(bo);\r\n} else {\r\ndev_err(bo->rdev->dev, "%p validate failed for unpin\n", bo);\r\n}\r\nreturn r;\r\n}\r\nint radeon_bo_evict_vram(struct radeon_device *rdev)\r\n{\r\nif (0 && (rdev->flags & RADEON_IS_IGP)) {\r\nif (rdev->mc.igp_sideport_enabled == false)\r\nreturn 0;\r\n}\r\nreturn ttm_bo_evict_mm(&rdev->mman.bdev, TTM_PL_VRAM);\r\n}\r\nvoid radeon_bo_force_delete(struct radeon_device *rdev)\r\n{\r\nstruct radeon_bo *bo, *n;\r\nif (list_empty(&rdev->gem.objects)) {\r\nreturn;\r\n}\r\ndev_err(rdev->dev, "Userspace still has active objects !\n");\r\nlist_for_each_entry_safe(bo, n, &rdev->gem.objects, list) {\r\ndev_err(rdev->dev, "%p %p %lu %lu force free\n",\r\n&bo->gem_base, bo, (unsigned long)bo->gem_base.size,\r\n*((unsigned long *)&bo->gem_base.refcount));\r\nmutex_lock(&bo->rdev->gem.mutex);\r\nlist_del_init(&bo->list);\r\nmutex_unlock(&bo->rdev->gem.mutex);\r\ndrm_gem_object_unreference_unlocked(&bo->gem_base);\r\n}\r\n}\r\nint radeon_bo_init(struct radeon_device *rdev)\r\n{\r\nif (!rdev->fastfb_working) {\r\nrdev->mc.vram_mtrr = arch_phys_wc_add(rdev->mc.aper_base,\r\nrdev->mc.aper_size);\r\n}\r\nDRM_INFO("Detected VRAM RAM=%lluM, BAR=%lluM\n",\r\nrdev->mc.mc_vram_size >> 20,\r\n(unsigned long long)rdev->mc.aper_size >> 20);\r\nDRM_INFO("RAM width %dbits %cDR\n",\r\nrdev->mc.vram_width, rdev->mc.vram_is_ddr ? 'D' : 'S');\r\nreturn radeon_ttm_init(rdev);\r\n}\r\nvoid radeon_bo_fini(struct radeon_device *rdev)\r\n{\r\nradeon_ttm_fini(rdev);\r\narch_phys_wc_del(rdev->mc.vram_mtrr);\r\n}\r\nstatic u64 radeon_bo_get_threshold_for_moves(struct radeon_device *rdev)\r\n{\r\nu64 real_vram_size = rdev->mc.real_vram_size;\r\nu64 vram_usage = atomic64_read(&rdev->vram_usage);\r\nu64 half_vram = real_vram_size >> 1;\r\nu64 half_free_vram = vram_usage >= half_vram ? 0 : half_vram - vram_usage;\r\nu64 bytes_moved_threshold = half_free_vram >> 1;\r\nreturn max(bytes_moved_threshold, 1024*1024ull);\r\n}\r\nint radeon_bo_list_validate(struct radeon_device *rdev,\r\nstruct ww_acquire_ctx *ticket,\r\nstruct list_head *head, int ring)\r\n{\r\nstruct radeon_bo_list *lobj;\r\nstruct list_head duplicates;\r\nint r;\r\nu64 bytes_moved = 0, initial_bytes_moved;\r\nu64 bytes_moved_threshold = radeon_bo_get_threshold_for_moves(rdev);\r\nINIT_LIST_HEAD(&duplicates);\r\nr = ttm_eu_reserve_buffers(ticket, head, true, &duplicates);\r\nif (unlikely(r != 0)) {\r\nreturn r;\r\n}\r\nlist_for_each_entry(lobj, head, tv.head) {\r\nstruct radeon_bo *bo = lobj->robj;\r\nif (!bo->pin_count) {\r\nu32 domain = lobj->prefered_domains;\r\nu32 allowed = lobj->allowed_domains;\r\nu32 current_domain =\r\nradeon_mem_type_to_domain(bo->tbo.mem.mem_type);\r\nif ((allowed & current_domain) != 0 &&\r\n(domain & current_domain) == 0 &&\r\nbytes_moved > bytes_moved_threshold) {\r\ndomain = current_domain;\r\n}\r\nretry:\r\nradeon_ttm_placement_from_domain(bo, domain);\r\nif (ring == R600_RING_TYPE_UVD_INDEX)\r\nradeon_uvd_force_into_uvd_segment(bo, allowed);\r\ninitial_bytes_moved = atomic64_read(&rdev->num_bytes_moved);\r\nr = ttm_bo_validate(&bo->tbo, &bo->placement, true, false);\r\nbytes_moved += atomic64_read(&rdev->num_bytes_moved) -\r\ninitial_bytes_moved;\r\nif (unlikely(r)) {\r\nif (r != -ERESTARTSYS &&\r\ndomain != lobj->allowed_domains) {\r\ndomain = lobj->allowed_domains;\r\ngoto retry;\r\n}\r\nttm_eu_backoff_reservation(ticket, head);\r\nreturn r;\r\n}\r\n}\r\nlobj->gpu_offset = radeon_bo_gpu_offset(bo);\r\nlobj->tiling_flags = bo->tiling_flags;\r\n}\r\nlist_for_each_entry(lobj, &duplicates, tv.head) {\r\nlobj->gpu_offset = radeon_bo_gpu_offset(lobj->robj);\r\nlobj->tiling_flags = lobj->robj->tiling_flags;\r\n}\r\nreturn 0;\r\n}\r\nint radeon_bo_get_surface_reg(struct radeon_bo *bo)\r\n{\r\nstruct radeon_device *rdev = bo->rdev;\r\nstruct radeon_surface_reg *reg;\r\nstruct radeon_bo *old_object;\r\nint steal;\r\nint i;\r\nlockdep_assert_held(&bo->tbo.resv->lock.base);\r\nif (!bo->tiling_flags)\r\nreturn 0;\r\nif (bo->surface_reg >= 0) {\r\nreg = &rdev->surface_regs[bo->surface_reg];\r\ni = bo->surface_reg;\r\ngoto out;\r\n}\r\nsteal = -1;\r\nfor (i = 0; i < RADEON_GEM_MAX_SURFACES; i++) {\r\nreg = &rdev->surface_regs[i];\r\nif (!reg->bo)\r\nbreak;\r\nold_object = reg->bo;\r\nif (old_object->pin_count == 0)\r\nsteal = i;\r\n}\r\nif (i == RADEON_GEM_MAX_SURFACES) {\r\nif (steal == -1)\r\nreturn -ENOMEM;\r\nreg = &rdev->surface_regs[steal];\r\nold_object = reg->bo;\r\nDRM_DEBUG("stealing surface reg %d from %p\n", steal, old_object);\r\nttm_bo_unmap_virtual(&old_object->tbo);\r\nold_object->surface_reg = -1;\r\ni = steal;\r\n}\r\nbo->surface_reg = i;\r\nreg->bo = bo;\r\nout:\r\nradeon_set_surface_reg(rdev, i, bo->tiling_flags, bo->pitch,\r\nbo->tbo.mem.start << PAGE_SHIFT,\r\nbo->tbo.num_pages << PAGE_SHIFT);\r\nreturn 0;\r\n}\r\nstatic void radeon_bo_clear_surface_reg(struct radeon_bo *bo)\r\n{\r\nstruct radeon_device *rdev = bo->rdev;\r\nstruct radeon_surface_reg *reg;\r\nif (bo->surface_reg == -1)\r\nreturn;\r\nreg = &rdev->surface_regs[bo->surface_reg];\r\nradeon_clear_surface_reg(rdev, bo->surface_reg);\r\nreg->bo = NULL;\r\nbo->surface_reg = -1;\r\n}\r\nint radeon_bo_set_tiling_flags(struct radeon_bo *bo,\r\nuint32_t tiling_flags, uint32_t pitch)\r\n{\r\nstruct radeon_device *rdev = bo->rdev;\r\nint r;\r\nif (rdev->family >= CHIP_CEDAR) {\r\nunsigned bankw, bankh, mtaspect, tilesplit, stilesplit;\r\nbankw = (tiling_flags >> RADEON_TILING_EG_BANKW_SHIFT) & RADEON_TILING_EG_BANKW_MASK;\r\nbankh = (tiling_flags >> RADEON_TILING_EG_BANKH_SHIFT) & RADEON_TILING_EG_BANKH_MASK;\r\nmtaspect = (tiling_flags >> RADEON_TILING_EG_MACRO_TILE_ASPECT_SHIFT) & RADEON_TILING_EG_MACRO_TILE_ASPECT_MASK;\r\ntilesplit = (tiling_flags >> RADEON_TILING_EG_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_TILE_SPLIT_MASK;\r\nstilesplit = (tiling_flags >> RADEON_TILING_EG_STENCIL_TILE_SPLIT_SHIFT) & RADEON_TILING_EG_STENCIL_TILE_SPLIT_MASK;\r\nswitch (bankw) {\r\ncase 0:\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nswitch (bankh) {\r\ncase 0:\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nswitch (mtaspect) {\r\ncase 0:\r\ncase 1:\r\ncase 2:\r\ncase 4:\r\ncase 8:\r\nbreak;\r\ndefault:\r\nreturn -EINVAL;\r\n}\r\nif (tilesplit > 6) {\r\nreturn -EINVAL;\r\n}\r\nif (stilesplit > 6) {\r\nreturn -EINVAL;\r\n}\r\n}\r\nr = radeon_bo_reserve(bo, false);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nbo->tiling_flags = tiling_flags;\r\nbo->pitch = pitch;\r\nradeon_bo_unreserve(bo);\r\nreturn 0;\r\n}\r\nvoid radeon_bo_get_tiling_flags(struct radeon_bo *bo,\r\nuint32_t *tiling_flags,\r\nuint32_t *pitch)\r\n{\r\nlockdep_assert_held(&bo->tbo.resv->lock.base);\r\nif (tiling_flags)\r\n*tiling_flags = bo->tiling_flags;\r\nif (pitch)\r\n*pitch = bo->pitch;\r\n}\r\nint radeon_bo_check_tiling(struct radeon_bo *bo, bool has_moved,\r\nbool force_drop)\r\n{\r\nif (!force_drop)\r\nlockdep_assert_held(&bo->tbo.resv->lock.base);\r\nif (!(bo->tiling_flags & RADEON_TILING_SURFACE))\r\nreturn 0;\r\nif (force_drop) {\r\nradeon_bo_clear_surface_reg(bo);\r\nreturn 0;\r\n}\r\nif (bo->tbo.mem.mem_type != TTM_PL_VRAM) {\r\nif (!has_moved)\r\nreturn 0;\r\nif (bo->surface_reg >= 0)\r\nradeon_bo_clear_surface_reg(bo);\r\nreturn 0;\r\n}\r\nif ((bo->surface_reg >= 0) && !has_moved)\r\nreturn 0;\r\nreturn radeon_bo_get_surface_reg(bo);\r\n}\r\nvoid radeon_bo_move_notify(struct ttm_buffer_object *bo,\r\nstruct ttm_mem_reg *new_mem)\r\n{\r\nstruct radeon_bo *rbo;\r\nif (!radeon_ttm_bo_is_radeon_bo(bo))\r\nreturn;\r\nrbo = container_of(bo, struct radeon_bo, tbo);\r\nradeon_bo_check_tiling(rbo, 0, 1);\r\nradeon_vm_bo_invalidate(rbo->rdev, rbo);\r\nif (!new_mem)\r\nreturn;\r\nradeon_update_memory_usage(rbo, bo->mem.mem_type, -1);\r\nradeon_update_memory_usage(rbo, new_mem->mem_type, 1);\r\n}\r\nint radeon_bo_fault_reserve_notify(struct ttm_buffer_object *bo)\r\n{\r\nstruct radeon_device *rdev;\r\nstruct radeon_bo *rbo;\r\nunsigned long offset, size, lpfn;\r\nint i, r;\r\nif (!radeon_ttm_bo_is_radeon_bo(bo))\r\nreturn 0;\r\nrbo = container_of(bo, struct radeon_bo, tbo);\r\nradeon_bo_check_tiling(rbo, 0, 0);\r\nrdev = rbo->rdev;\r\nif (bo->mem.mem_type != TTM_PL_VRAM)\r\nreturn 0;\r\nsize = bo->mem.num_pages << PAGE_SHIFT;\r\noffset = bo->mem.start << PAGE_SHIFT;\r\nif ((offset + size) <= rdev->mc.visible_vram_size)\r\nreturn 0;\r\nif (rbo->pin_count > 0)\r\nreturn -EINVAL;\r\nradeon_ttm_placement_from_domain(rbo, RADEON_GEM_DOMAIN_VRAM);\r\nlpfn = rdev->mc.visible_vram_size >> PAGE_SHIFT;\r\nfor (i = 0; i < rbo->placement.num_placement; i++) {\r\nif ((rbo->placements[i].flags & TTM_PL_FLAG_VRAM) &&\r\n(!rbo->placements[i].lpfn || rbo->placements[i].lpfn > lpfn))\r\nrbo->placements[i].lpfn = lpfn;\r\n}\r\nr = ttm_bo_validate(bo, &rbo->placement, false, false);\r\nif (unlikely(r == -ENOMEM)) {\r\nradeon_ttm_placement_from_domain(rbo, RADEON_GEM_DOMAIN_GTT);\r\nreturn ttm_bo_validate(bo, &rbo->placement, false, false);\r\n} else if (unlikely(r != 0)) {\r\nreturn r;\r\n}\r\noffset = bo->mem.start << PAGE_SHIFT;\r\nif ((offset + size) > rdev->mc.visible_vram_size)\r\nreturn -EINVAL;\r\nreturn 0;\r\n}\r\nint radeon_bo_wait(struct radeon_bo *bo, u32 *mem_type, bool no_wait)\r\n{\r\nint r;\r\nr = ttm_bo_reserve(&bo->tbo, true, no_wait, NULL);\r\nif (unlikely(r != 0))\r\nreturn r;\r\nif (mem_type)\r\n*mem_type = bo->tbo.mem.mem_type;\r\nr = ttm_bo_wait(&bo->tbo, true, no_wait);\r\nttm_bo_unreserve(&bo->tbo);\r\nreturn r;\r\n}\r\nvoid radeon_bo_fence(struct radeon_bo *bo, struct radeon_fence *fence,\r\nbool shared)\r\n{\r\nstruct reservation_object *resv = bo->tbo.resv;\r\nif (shared)\r\nreservation_object_add_shared_fence(resv, &fence->base);\r\nelse\r\nreservation_object_add_excl_fence(resv, &fence->base);\r\n}
