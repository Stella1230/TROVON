static u32 inet_ehashfn(const struct net *net, const __be32 laddr,\r\nconst __u16 lport, const __be32 faddr,\r\nconst __be16 fport)\r\n{\r\nstatic u32 inet_ehash_secret __read_mostly;\r\nnet_get_random_once(&inet_ehash_secret, sizeof(inet_ehash_secret));\r\nreturn __inet_ehashfn(laddr, lport, faddr, fport,\r\ninet_ehash_secret + net_hash_mix(net));\r\n}\r\nu32 sk_ehashfn(const struct sock *sk)\r\n{\r\n#if IS_ENABLED(CONFIG_IPV6)\r\nif (sk->sk_family == AF_INET6 &&\r\n!ipv6_addr_v4mapped(&sk->sk_v6_daddr))\r\nreturn inet6_ehashfn(sock_net(sk),\r\n&sk->sk_v6_rcv_saddr, sk->sk_num,\r\n&sk->sk_v6_daddr, sk->sk_dport);\r\n#endif\r\nreturn inet_ehashfn(sock_net(sk),\r\nsk->sk_rcv_saddr, sk->sk_num,\r\nsk->sk_daddr, sk->sk_dport);\r\n}\r\nstruct inet_bind_bucket *inet_bind_bucket_create(struct kmem_cache *cachep,\r\nstruct net *net,\r\nstruct inet_bind_hashbucket *head,\r\nconst unsigned short snum)\r\n{\r\nstruct inet_bind_bucket *tb = kmem_cache_alloc(cachep, GFP_ATOMIC);\r\nif (tb) {\r\nwrite_pnet(&tb->ib_net, net);\r\ntb->port = snum;\r\ntb->fastreuse = 0;\r\ntb->fastreuseport = 0;\r\ntb->num_owners = 0;\r\nINIT_HLIST_HEAD(&tb->owners);\r\nhlist_add_head(&tb->node, &head->chain);\r\n}\r\nreturn tb;\r\n}\r\nvoid inet_bind_bucket_destroy(struct kmem_cache *cachep, struct inet_bind_bucket *tb)\r\n{\r\nif (hlist_empty(&tb->owners)) {\r\n__hlist_del(&tb->node);\r\nkmem_cache_free(cachep, tb);\r\n}\r\n}\r\nvoid inet_bind_hash(struct sock *sk, struct inet_bind_bucket *tb,\r\nconst unsigned short snum)\r\n{\r\ninet_sk(sk)->inet_num = snum;\r\nsk_add_bind_node(sk, &tb->owners);\r\ntb->num_owners++;\r\ninet_csk(sk)->icsk_bind_hash = tb;\r\n}\r\nstatic void __inet_put_port(struct sock *sk)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nconst int bhash = inet_bhashfn(sock_net(sk), inet_sk(sk)->inet_num,\r\nhashinfo->bhash_size);\r\nstruct inet_bind_hashbucket *head = &hashinfo->bhash[bhash];\r\nstruct inet_bind_bucket *tb;\r\nspin_lock(&head->lock);\r\ntb = inet_csk(sk)->icsk_bind_hash;\r\n__sk_del_bind_node(sk);\r\ntb->num_owners--;\r\ninet_csk(sk)->icsk_bind_hash = NULL;\r\ninet_sk(sk)->inet_num = 0;\r\ninet_bind_bucket_destroy(hashinfo->bind_bucket_cachep, tb);\r\nspin_unlock(&head->lock);\r\n}\r\nvoid inet_put_port(struct sock *sk)\r\n{\r\nlocal_bh_disable();\r\n__inet_put_port(sk);\r\nlocal_bh_enable();\r\n}\r\nint __inet_inherit_port(const struct sock *sk, struct sock *child)\r\n{\r\nstruct inet_hashinfo *table = sk->sk_prot->h.hashinfo;\r\nunsigned short port = inet_sk(child)->inet_num;\r\nconst int bhash = inet_bhashfn(sock_net(sk), port,\r\ntable->bhash_size);\r\nstruct inet_bind_hashbucket *head = &table->bhash[bhash];\r\nstruct inet_bind_bucket *tb;\r\nspin_lock(&head->lock);\r\ntb = inet_csk(sk)->icsk_bind_hash;\r\nif (unlikely(!tb)) {\r\nspin_unlock(&head->lock);\r\nreturn -ENOENT;\r\n}\r\nif (tb->port != port) {\r\ninet_bind_bucket_for_each(tb, &head->chain) {\r\nif (net_eq(ib_net(tb), sock_net(sk)) &&\r\ntb->port == port)\r\nbreak;\r\n}\r\nif (!tb) {\r\ntb = inet_bind_bucket_create(table->bind_bucket_cachep,\r\nsock_net(sk), head, port);\r\nif (!tb) {\r\nspin_unlock(&head->lock);\r\nreturn -ENOMEM;\r\n}\r\n}\r\n}\r\ninet_bind_hash(child, tb, port);\r\nspin_unlock(&head->lock);\r\nreturn 0;\r\n}\r\nstatic inline int compute_score(struct sock *sk, struct net *net,\r\nconst unsigned short hnum, const __be32 daddr,\r\nconst int dif)\r\n{\r\nint score = -1;\r\nstruct inet_sock *inet = inet_sk(sk);\r\nif (net_eq(sock_net(sk), net) && inet->inet_num == hnum &&\r\n!ipv6_only_sock(sk)) {\r\n__be32 rcv_saddr = inet->inet_rcv_saddr;\r\nscore = sk->sk_family == PF_INET ? 2 : 1;\r\nif (rcv_saddr) {\r\nif (rcv_saddr != daddr)\r\nreturn -1;\r\nscore += 4;\r\n}\r\nif (sk->sk_bound_dev_if) {\r\nif (sk->sk_bound_dev_if != dif)\r\nreturn -1;\r\nscore += 4;\r\n}\r\nif (sk->sk_incoming_cpu == raw_smp_processor_id())\r\nscore++;\r\n}\r\nreturn score;\r\n}\r\nstruct sock *__inet_lookup_listener(struct net *net,\r\nstruct inet_hashinfo *hashinfo,\r\nstruct sk_buff *skb, int doff,\r\nconst __be32 saddr, __be16 sport,\r\nconst __be32 daddr, const unsigned short hnum,\r\nconst int dif)\r\n{\r\nunsigned int hash = inet_lhashfn(net, hnum);\r\nstruct inet_listen_hashbucket *ilb = &hashinfo->listening_hash[hash];\r\nint score, hiscore = 0, matches = 0, reuseport = 0;\r\nstruct sock *sk, *result = NULL;\r\nu32 phash = 0;\r\nsk_for_each_rcu(sk, &ilb->head) {\r\nscore = compute_score(sk, net, hnum, daddr, dif);\r\nif (score > hiscore) {\r\nreuseport = sk->sk_reuseport;\r\nif (reuseport) {\r\nphash = inet_ehashfn(net, daddr, hnum,\r\nsaddr, sport);\r\nresult = reuseport_select_sock(sk, phash,\r\nskb, doff);\r\nif (result)\r\nreturn result;\r\nmatches = 1;\r\n}\r\nresult = sk;\r\nhiscore = score;\r\n} else if (score == hiscore && reuseport) {\r\nmatches++;\r\nif (reciprocal_scale(phash, matches) == 0)\r\nresult = sk;\r\nphash = next_pseudo_random32(phash);\r\n}\r\n}\r\nreturn result;\r\n}\r\nvoid sock_gen_put(struct sock *sk)\r\n{\r\nif (!atomic_dec_and_test(&sk->sk_refcnt))\r\nreturn;\r\nif (sk->sk_state == TCP_TIME_WAIT)\r\ninet_twsk_free(inet_twsk(sk));\r\nelse if (sk->sk_state == TCP_NEW_SYN_RECV)\r\nreqsk_free(inet_reqsk(sk));\r\nelse\r\nsk_free(sk);\r\n}\r\nvoid sock_edemux(struct sk_buff *skb)\r\n{\r\nsock_gen_put(skb->sk);\r\n}\r\nstruct sock *__inet_lookup_established(struct net *net,\r\nstruct inet_hashinfo *hashinfo,\r\nconst __be32 saddr, const __be16 sport,\r\nconst __be32 daddr, const u16 hnum,\r\nconst int dif)\r\n{\r\nINET_ADDR_COOKIE(acookie, saddr, daddr);\r\nconst __portpair ports = INET_COMBINED_PORTS(sport, hnum);\r\nstruct sock *sk;\r\nconst struct hlist_nulls_node *node;\r\nunsigned int hash = inet_ehashfn(net, daddr, hnum, saddr, sport);\r\nunsigned int slot = hash & hashinfo->ehash_mask;\r\nstruct inet_ehash_bucket *head = &hashinfo->ehash[slot];\r\nbegin:\r\nsk_nulls_for_each_rcu(sk, node, &head->chain) {\r\nif (sk->sk_hash != hash)\r\ncontinue;\r\nif (likely(INET_MATCH(sk, net, acookie,\r\nsaddr, daddr, ports, dif))) {\r\nif (unlikely(!atomic_inc_not_zero(&sk->sk_refcnt)))\r\ngoto out;\r\nif (unlikely(!INET_MATCH(sk, net, acookie,\r\nsaddr, daddr, ports, dif))) {\r\nsock_gen_put(sk);\r\ngoto begin;\r\n}\r\ngoto found;\r\n}\r\n}\r\nif (get_nulls_value(node) != slot)\r\ngoto begin;\r\nout:\r\nsk = NULL;\r\nfound:\r\nreturn sk;\r\n}\r\nstatic int __inet_check_established(struct inet_timewait_death_row *death_row,\r\nstruct sock *sk, __u16 lport,\r\nstruct inet_timewait_sock **twp)\r\n{\r\nstruct inet_hashinfo *hinfo = death_row->hashinfo;\r\nstruct inet_sock *inet = inet_sk(sk);\r\n__be32 daddr = inet->inet_rcv_saddr;\r\n__be32 saddr = inet->inet_daddr;\r\nint dif = sk->sk_bound_dev_if;\r\nINET_ADDR_COOKIE(acookie, saddr, daddr);\r\nconst __portpair ports = INET_COMBINED_PORTS(inet->inet_dport, lport);\r\nstruct net *net = sock_net(sk);\r\nunsigned int hash = inet_ehashfn(net, daddr, lport,\r\nsaddr, inet->inet_dport);\r\nstruct inet_ehash_bucket *head = inet_ehash_bucket(hinfo, hash);\r\nspinlock_t *lock = inet_ehash_lockp(hinfo, hash);\r\nstruct sock *sk2;\r\nconst struct hlist_nulls_node *node;\r\nstruct inet_timewait_sock *tw = NULL;\r\nspin_lock(lock);\r\nsk_nulls_for_each(sk2, node, &head->chain) {\r\nif (sk2->sk_hash != hash)\r\ncontinue;\r\nif (likely(INET_MATCH(sk2, net, acookie,\r\nsaddr, daddr, ports, dif))) {\r\nif (sk2->sk_state == TCP_TIME_WAIT) {\r\ntw = inet_twsk(sk2);\r\nif (twsk_unique(sk, sk2, twp))\r\nbreak;\r\n}\r\ngoto not_unique;\r\n}\r\n}\r\ninet->inet_num = lport;\r\ninet->inet_sport = htons(lport);\r\nsk->sk_hash = hash;\r\nWARN_ON(!sk_unhashed(sk));\r\n__sk_nulls_add_node_rcu(sk, &head->chain);\r\nif (tw) {\r\nsk_nulls_del_node_init_rcu((struct sock *)tw);\r\n__NET_INC_STATS(net, LINUX_MIB_TIMEWAITRECYCLED);\r\n}\r\nspin_unlock(lock);\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\r\nif (twp) {\r\n*twp = tw;\r\n} else if (tw) {\r\ninet_twsk_deschedule_put(tw);\r\n}\r\nreturn 0;\r\nnot_unique:\r\nspin_unlock(lock);\r\nreturn -EADDRNOTAVAIL;\r\n}\r\nstatic u32 inet_sk_port_offset(const struct sock *sk)\r\n{\r\nconst struct inet_sock *inet = inet_sk(sk);\r\nreturn secure_ipv4_port_ephemeral(inet->inet_rcv_saddr,\r\ninet->inet_daddr,\r\ninet->inet_dport);\r\n}\r\nbool inet_ehash_insert(struct sock *sk, struct sock *osk)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nstruct hlist_nulls_head *list;\r\nstruct inet_ehash_bucket *head;\r\nspinlock_t *lock;\r\nbool ret = true;\r\nWARN_ON_ONCE(!sk_unhashed(sk));\r\nsk->sk_hash = sk_ehashfn(sk);\r\nhead = inet_ehash_bucket(hashinfo, sk->sk_hash);\r\nlist = &head->chain;\r\nlock = inet_ehash_lockp(hashinfo, sk->sk_hash);\r\nspin_lock(lock);\r\nif (osk) {\r\nWARN_ON_ONCE(sk->sk_hash != osk->sk_hash);\r\nret = sk_nulls_del_node_init_rcu(osk);\r\n}\r\nif (ret)\r\n__sk_nulls_add_node_rcu(sk, list);\r\nspin_unlock(lock);\r\nreturn ret;\r\n}\r\nbool inet_ehash_nolisten(struct sock *sk, struct sock *osk)\r\n{\r\nbool ok = inet_ehash_insert(sk, osk);\r\nif (ok) {\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\r\n} else {\r\npercpu_counter_inc(sk->sk_prot->orphan_count);\r\nsk->sk_state = TCP_CLOSE;\r\nsock_set_flag(sk, SOCK_DEAD);\r\ninet_csk_destroy_sock(sk);\r\n}\r\nreturn ok;\r\n}\r\nstatic int inet_reuseport_add_sock(struct sock *sk,\r\nstruct inet_listen_hashbucket *ilb,\r\nint (*saddr_same)(const struct sock *sk1,\r\nconst struct sock *sk2,\r\nbool match_wildcard))\r\n{\r\nstruct inet_bind_bucket *tb = inet_csk(sk)->icsk_bind_hash;\r\nstruct sock *sk2;\r\nkuid_t uid = sock_i_uid(sk);\r\nsk_for_each_rcu(sk2, &ilb->head) {\r\nif (sk2 != sk &&\r\nsk2->sk_family == sk->sk_family &&\r\nipv6_only_sock(sk2) == ipv6_only_sock(sk) &&\r\nsk2->sk_bound_dev_if == sk->sk_bound_dev_if &&\r\ninet_csk(sk2)->icsk_bind_hash == tb &&\r\nsk2->sk_reuseport && uid_eq(uid, sock_i_uid(sk2)) &&\r\nsaddr_same(sk, sk2, false))\r\nreturn reuseport_add_sock(sk, sk2);\r\n}\r\nif (!rcu_access_pointer(sk->sk_reuseport_cb))\r\nreturn reuseport_alloc(sk);\r\nreturn 0;\r\n}\r\nint __inet_hash(struct sock *sk, struct sock *osk,\r\nint (*saddr_same)(const struct sock *sk1,\r\nconst struct sock *sk2,\r\nbool match_wildcard))\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nstruct inet_listen_hashbucket *ilb;\r\nint err = 0;\r\nif (sk->sk_state != TCP_LISTEN) {\r\ninet_ehash_nolisten(sk, osk);\r\nreturn 0;\r\n}\r\nWARN_ON(!sk_unhashed(sk));\r\nilb = &hashinfo->listening_hash[inet_sk_listen_hashfn(sk)];\r\nspin_lock(&ilb->lock);\r\nif (sk->sk_reuseport) {\r\nerr = inet_reuseport_add_sock(sk, ilb, saddr_same);\r\nif (err)\r\ngoto unlock;\r\n}\r\nif (IS_ENABLED(CONFIG_IPV6) && sk->sk_reuseport &&\r\nsk->sk_family == AF_INET6)\r\nhlist_add_tail_rcu(&sk->sk_node, &ilb->head);\r\nelse\r\nhlist_add_head_rcu(&sk->sk_node, &ilb->head);\r\nsock_set_flag(sk, SOCK_RCU_FREE);\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, 1);\r\nunlock:\r\nspin_unlock(&ilb->lock);\r\nreturn err;\r\n}\r\nint inet_hash(struct sock *sk)\r\n{\r\nint err = 0;\r\nif (sk->sk_state != TCP_CLOSE) {\r\nlocal_bh_disable();\r\nerr = __inet_hash(sk, NULL, ipv4_rcv_saddr_equal);\r\nlocal_bh_enable();\r\n}\r\nreturn err;\r\n}\r\nvoid inet_unhash(struct sock *sk)\r\n{\r\nstruct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;\r\nspinlock_t *lock;\r\nbool listener = false;\r\nint done;\r\nif (sk_unhashed(sk))\r\nreturn;\r\nif (sk->sk_state == TCP_LISTEN) {\r\nlock = &hashinfo->listening_hash[inet_sk_listen_hashfn(sk)].lock;\r\nlistener = true;\r\n} else {\r\nlock = inet_ehash_lockp(hashinfo, sk->sk_hash);\r\n}\r\nspin_lock_bh(lock);\r\nif (rcu_access_pointer(sk->sk_reuseport_cb))\r\nreuseport_detach_sock(sk);\r\nif (listener)\r\ndone = __sk_del_node_init(sk);\r\nelse\r\ndone = __sk_nulls_del_node_init_rcu(sk);\r\nif (done)\r\nsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);\r\nspin_unlock_bh(lock);\r\n}\r\nint __inet_hash_connect(struct inet_timewait_death_row *death_row,\r\nstruct sock *sk, u32 port_offset,\r\nint (*check_established)(struct inet_timewait_death_row *,\r\nstruct sock *, __u16, struct inet_timewait_sock **))\r\n{\r\nstruct inet_hashinfo *hinfo = death_row->hashinfo;\r\nstruct inet_timewait_sock *tw = NULL;\r\nstruct inet_bind_hashbucket *head;\r\nint port = inet_sk(sk)->inet_num;\r\nstruct net *net = sock_net(sk);\r\nstruct inet_bind_bucket *tb;\r\nu32 remaining, offset;\r\nint ret, i, low, high;\r\nstatic u32 hint;\r\nif (port) {\r\nhead = &hinfo->bhash[inet_bhashfn(net, port,\r\nhinfo->bhash_size)];\r\ntb = inet_csk(sk)->icsk_bind_hash;\r\nspin_lock_bh(&head->lock);\r\nif (sk_head(&tb->owners) == sk && !sk->sk_bind_node.next) {\r\ninet_ehash_nolisten(sk, NULL);\r\nspin_unlock_bh(&head->lock);\r\nreturn 0;\r\n}\r\nspin_unlock(&head->lock);\r\nret = check_established(death_row, sk, port, NULL);\r\nlocal_bh_enable();\r\nreturn ret;\r\n}\r\ninet_get_local_port_range(net, &low, &high);\r\nhigh++;\r\nremaining = high - low;\r\nif (likely(remaining > 1))\r\nremaining &= ~1U;\r\noffset = (hint + port_offset) % remaining;\r\noffset &= ~1U;\r\nother_parity_scan:\r\nport = low + offset;\r\nfor (i = 0; i < remaining; i += 2, port += 2) {\r\nif (unlikely(port >= high))\r\nport -= remaining;\r\nif (inet_is_local_reserved_port(net, port))\r\ncontinue;\r\nhead = &hinfo->bhash[inet_bhashfn(net, port,\r\nhinfo->bhash_size)];\r\nspin_lock_bh(&head->lock);\r\ninet_bind_bucket_for_each(tb, &head->chain) {\r\nif (net_eq(ib_net(tb), net) && tb->port == port) {\r\nif (tb->fastreuse >= 0 ||\r\ntb->fastreuseport >= 0)\r\ngoto next_port;\r\nWARN_ON(hlist_empty(&tb->owners));\r\nif (!check_established(death_row, sk,\r\nport, &tw))\r\ngoto ok;\r\ngoto next_port;\r\n}\r\n}\r\ntb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,\r\nnet, head, port);\r\nif (!tb) {\r\nspin_unlock_bh(&head->lock);\r\nreturn -ENOMEM;\r\n}\r\ntb->fastreuse = -1;\r\ntb->fastreuseport = -1;\r\ngoto ok;\r\nnext_port:\r\nspin_unlock_bh(&head->lock);\r\ncond_resched();\r\n}\r\noffset++;\r\nif ((offset & 1) && remaining > 1)\r\ngoto other_parity_scan;\r\nreturn -EADDRNOTAVAIL;\r\nok:\r\nhint += i + 2;\r\ninet_bind_hash(sk, tb, port);\r\nif (sk_unhashed(sk)) {\r\ninet_sk(sk)->inet_sport = htons(port);\r\ninet_ehash_nolisten(sk, (struct sock *)tw);\r\n}\r\nif (tw)\r\ninet_twsk_bind_unhash(tw, hinfo);\r\nspin_unlock(&head->lock);\r\nif (tw)\r\ninet_twsk_deschedule_put(tw);\r\nlocal_bh_enable();\r\nreturn 0;\r\n}\r\nint inet_hash_connect(struct inet_timewait_death_row *death_row,\r\nstruct sock *sk)\r\n{\r\nu32 port_offset = 0;\r\nif (!inet_sk(sk)->inet_num)\r\nport_offset = inet_sk_port_offset(sk);\r\nreturn __inet_hash_connect(death_row, sk, port_offset,\r\n__inet_check_established);\r\n}\r\nvoid inet_hashinfo_init(struct inet_hashinfo *h)\r\n{\r\nint i;\r\nfor (i = 0; i < INET_LHTABLE_SIZE; i++) {\r\nspin_lock_init(&h->listening_hash[i].lock);\r\nINIT_HLIST_HEAD(&h->listening_hash[i].head);\r\n}\r\n}\r\nint inet_ehash_locks_alloc(struct inet_hashinfo *hashinfo)\r\n{\r\nunsigned int locksz = sizeof(spinlock_t);\r\nunsigned int i, nblocks = 1;\r\nif (locksz != 0) {\r\nnblocks = max(2U * L1_CACHE_BYTES / locksz, 1U);\r\nnblocks = roundup_pow_of_two(nblocks * num_possible_cpus());\r\nnblocks = min(nblocks, hashinfo->ehash_mask + 1);\r\nhashinfo->ehash_locks = kmalloc_array(nblocks, locksz,\r\nGFP_KERNEL | __GFP_NOWARN);\r\nif (!hashinfo->ehash_locks)\r\nhashinfo->ehash_locks = vmalloc(nblocks * locksz);\r\nif (!hashinfo->ehash_locks)\r\nreturn -ENOMEM;\r\nfor (i = 0; i < nblocks; i++)\r\nspin_lock_init(&hashinfo->ehash_locks[i]);\r\n}\r\nhashinfo->ehash_locks_mask = nblocks - 1;\r\nreturn 0;\r\n}
