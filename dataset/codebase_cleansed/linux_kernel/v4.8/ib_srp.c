static int srp_tmo_get(char *buffer, const struct kernel_param *kp)\r\n{\r\nint tmo = *(int *)kp->arg;\r\nif (tmo >= 0)\r\nreturn sprintf(buffer, "%d", tmo);\r\nelse\r\nreturn sprintf(buffer, "off");\r\n}\r\nstatic int srp_tmo_set(const char *val, const struct kernel_param *kp)\r\n{\r\nint tmo, res;\r\nres = srp_parse_tmo(&tmo, val);\r\nif (res)\r\ngoto out;\r\nif (kp->arg == &srp_reconnect_delay)\r\nres = srp_tmo_valid(tmo, srp_fast_io_fail_tmo,\r\nsrp_dev_loss_tmo);\r\nelse if (kp->arg == &srp_fast_io_fail_tmo)\r\nres = srp_tmo_valid(srp_reconnect_delay, tmo, srp_dev_loss_tmo);\r\nelse\r\nres = srp_tmo_valid(srp_reconnect_delay, srp_fast_io_fail_tmo,\r\ntmo);\r\nif (res)\r\ngoto out;\r\n*(int *)kp->arg = tmo;\r\nout:\r\nreturn res;\r\n}\r\nstatic inline struct srp_target_port *host_to_target(struct Scsi_Host *host)\r\n{\r\nreturn (struct srp_target_port *) host->hostdata;\r\n}\r\nstatic const char *srp_target_info(struct Scsi_Host *host)\r\n{\r\nreturn host_to_target(host)->target_name;\r\n}\r\nstatic int srp_target_is_topspin(struct srp_target_port *target)\r\n{\r\nstatic const u8 topspin_oui[3] = { 0x00, 0x05, 0xad };\r\nstatic const u8 cisco_oui[3] = { 0x00, 0x1b, 0x0d };\r\nreturn topspin_workarounds &&\r\n(!memcmp(&target->ioc_guid, topspin_oui, sizeof topspin_oui) ||\r\n!memcmp(&target->ioc_guid, cisco_oui, sizeof cisco_oui));\r\n}\r\nstatic struct srp_iu *srp_alloc_iu(struct srp_host *host, size_t size,\r\ngfp_t gfp_mask,\r\nenum dma_data_direction direction)\r\n{\r\nstruct srp_iu *iu;\r\niu = kmalloc(sizeof *iu, gfp_mask);\r\nif (!iu)\r\ngoto out;\r\niu->buf = kzalloc(size, gfp_mask);\r\nif (!iu->buf)\r\ngoto out_free_iu;\r\niu->dma = ib_dma_map_single(host->srp_dev->dev, iu->buf, size,\r\ndirection);\r\nif (ib_dma_mapping_error(host->srp_dev->dev, iu->dma))\r\ngoto out_free_buf;\r\niu->size = size;\r\niu->direction = direction;\r\nreturn iu;\r\nout_free_buf:\r\nkfree(iu->buf);\r\nout_free_iu:\r\nkfree(iu);\r\nout:\r\nreturn NULL;\r\n}\r\nstatic void srp_free_iu(struct srp_host *host, struct srp_iu *iu)\r\n{\r\nif (!iu)\r\nreturn;\r\nib_dma_unmap_single(host->srp_dev->dev, iu->dma, iu->size,\r\niu->direction);\r\nkfree(iu->buf);\r\nkfree(iu);\r\n}\r\nstatic void srp_qp_event(struct ib_event *event, void *context)\r\n{\r\npr_debug("QP event %s (%d)\n",\r\nib_event_msg(event->event), event->event);\r\n}\r\nstatic int srp_init_qp(struct srp_target_port *target,\r\nstruct ib_qp *qp)\r\n{\r\nstruct ib_qp_attr *attr;\r\nint ret;\r\nattr = kmalloc(sizeof *attr, GFP_KERNEL);\r\nif (!attr)\r\nreturn -ENOMEM;\r\nret = ib_find_cached_pkey(target->srp_host->srp_dev->dev,\r\ntarget->srp_host->port,\r\nbe16_to_cpu(target->pkey),\r\n&attr->pkey_index);\r\nif (ret)\r\ngoto out;\r\nattr->qp_state = IB_QPS_INIT;\r\nattr->qp_access_flags = (IB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE);\r\nattr->port_num = target->srp_host->port;\r\nret = ib_modify_qp(qp, attr,\r\nIB_QP_STATE |\r\nIB_QP_PKEY_INDEX |\r\nIB_QP_ACCESS_FLAGS |\r\nIB_QP_PORT);\r\nout:\r\nkfree(attr);\r\nreturn ret;\r\n}\r\nstatic int srp_new_cm_id(struct srp_rdma_ch *ch)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct ib_cm_id *new_cm_id;\r\nnew_cm_id = ib_create_cm_id(target->srp_host->srp_dev->dev,\r\nsrp_cm_handler, ch);\r\nif (IS_ERR(new_cm_id))\r\nreturn PTR_ERR(new_cm_id);\r\nif (ch->cm_id)\r\nib_destroy_cm_id(ch->cm_id);\r\nch->cm_id = new_cm_id;\r\nch->path.sgid = target->sgid;\r\nch->path.dgid = target->orig_dgid;\r\nch->path.pkey = target->pkey;\r\nch->path.service_id = target->service_id;\r\nreturn 0;\r\n}\r\nstatic struct ib_fmr_pool *srp_alloc_fmr_pool(struct srp_target_port *target)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_fmr_pool_param fmr_param;\r\nmemset(&fmr_param, 0, sizeof(fmr_param));\r\nfmr_param.pool_size = target->mr_pool_size;\r\nfmr_param.dirty_watermark = fmr_param.pool_size / 4;\r\nfmr_param.cache = 1;\r\nfmr_param.max_pages_per_fmr = dev->max_pages_per_mr;\r\nfmr_param.page_shift = ilog2(dev->mr_page_size);\r\nfmr_param.access = (IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_WRITE |\r\nIB_ACCESS_REMOTE_READ);\r\nreturn ib_create_fmr_pool(dev->pd, &fmr_param);\r\n}\r\nstatic void srp_destroy_fr_pool(struct srp_fr_pool *pool)\r\n{\r\nint i;\r\nstruct srp_fr_desc *d;\r\nif (!pool)\r\nreturn;\r\nfor (i = 0, d = &pool->desc[0]; i < pool->size; i++, d++) {\r\nif (d->mr)\r\nib_dereg_mr(d->mr);\r\n}\r\nkfree(pool);\r\n}\r\nstatic struct srp_fr_pool *srp_create_fr_pool(struct ib_device *device,\r\nstruct ib_pd *pd, int pool_size,\r\nint max_page_list_len)\r\n{\r\nstruct srp_fr_pool *pool;\r\nstruct srp_fr_desc *d;\r\nstruct ib_mr *mr;\r\nint i, ret = -EINVAL;\r\nif (pool_size <= 0)\r\ngoto err;\r\nret = -ENOMEM;\r\npool = kzalloc(sizeof(struct srp_fr_pool) +\r\npool_size * sizeof(struct srp_fr_desc), GFP_KERNEL);\r\nif (!pool)\r\ngoto err;\r\npool->size = pool_size;\r\npool->max_page_list_len = max_page_list_len;\r\nspin_lock_init(&pool->lock);\r\nINIT_LIST_HEAD(&pool->free_list);\r\nfor (i = 0, d = &pool->desc[0]; i < pool->size; i++, d++) {\r\nmr = ib_alloc_mr(pd, IB_MR_TYPE_MEM_REG,\r\nmax_page_list_len);\r\nif (IS_ERR(mr)) {\r\nret = PTR_ERR(mr);\r\ngoto destroy_pool;\r\n}\r\nd->mr = mr;\r\nlist_add_tail(&d->entry, &pool->free_list);\r\n}\r\nout:\r\nreturn pool;\r\ndestroy_pool:\r\nsrp_destroy_fr_pool(pool);\r\nerr:\r\npool = ERR_PTR(ret);\r\ngoto out;\r\n}\r\nstatic struct srp_fr_desc *srp_fr_pool_get(struct srp_fr_pool *pool)\r\n{\r\nstruct srp_fr_desc *d = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nif (!list_empty(&pool->free_list)) {\r\nd = list_first_entry(&pool->free_list, typeof(*d), entry);\r\nlist_del(&d->entry);\r\n}\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\nreturn d;\r\n}\r\nstatic void srp_fr_pool_put(struct srp_fr_pool *pool, struct srp_fr_desc **desc,\r\nint n)\r\n{\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&pool->lock, flags);\r\nfor (i = 0; i < n; i++)\r\nlist_add(&desc[i]->entry, &pool->free_list);\r\nspin_unlock_irqrestore(&pool->lock, flags);\r\n}\r\nstatic struct srp_fr_pool *srp_alloc_fr_pool(struct srp_target_port *target)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nreturn srp_create_fr_pool(dev->dev, dev->pd, target->mr_pool_size,\r\ndev->max_pages_per_mr);\r\n}\r\nstatic void srp_destroy_qp(struct ib_qp *qp)\r\n{\r\nib_drain_rq(qp);\r\nib_destroy_qp(qp);\r\n}\r\nstatic int srp_create_ch_ib(struct srp_rdma_ch *ch)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_qp_init_attr *init_attr;\r\nstruct ib_cq *recv_cq, *send_cq;\r\nstruct ib_qp *qp;\r\nstruct ib_fmr_pool *fmr_pool = NULL;\r\nstruct srp_fr_pool *fr_pool = NULL;\r\nconst int m = 1 + dev->use_fast_reg * target->mr_per_cmd * 2;\r\nint ret;\r\ninit_attr = kzalloc(sizeof *init_attr, GFP_KERNEL);\r\nif (!init_attr)\r\nreturn -ENOMEM;\r\nrecv_cq = ib_alloc_cq(dev->dev, ch, target->queue_size + 1,\r\nch->comp_vector, IB_POLL_SOFTIRQ);\r\nif (IS_ERR(recv_cq)) {\r\nret = PTR_ERR(recv_cq);\r\ngoto err;\r\n}\r\nsend_cq = ib_alloc_cq(dev->dev, ch, m * target->queue_size,\r\nch->comp_vector, IB_POLL_DIRECT);\r\nif (IS_ERR(send_cq)) {\r\nret = PTR_ERR(send_cq);\r\ngoto err_recv_cq;\r\n}\r\ninit_attr->event_handler = srp_qp_event;\r\ninit_attr->cap.max_send_wr = m * target->queue_size;\r\ninit_attr->cap.max_recv_wr = target->queue_size + 1;\r\ninit_attr->cap.max_recv_sge = 1;\r\ninit_attr->cap.max_send_sge = 1;\r\ninit_attr->sq_sig_type = IB_SIGNAL_REQ_WR;\r\ninit_attr->qp_type = IB_QPT_RC;\r\ninit_attr->send_cq = send_cq;\r\ninit_attr->recv_cq = recv_cq;\r\nqp = ib_create_qp(dev->pd, init_attr);\r\nif (IS_ERR(qp)) {\r\nret = PTR_ERR(qp);\r\ngoto err_send_cq;\r\n}\r\nret = srp_init_qp(target, qp);\r\nif (ret)\r\ngoto err_qp;\r\nif (dev->use_fast_reg) {\r\nfr_pool = srp_alloc_fr_pool(target);\r\nif (IS_ERR(fr_pool)) {\r\nret = PTR_ERR(fr_pool);\r\nshost_printk(KERN_WARNING, target->scsi_host, PFX\r\n"FR pool allocation failed (%d)\n", ret);\r\ngoto err_qp;\r\n}\r\n} else if (dev->use_fmr) {\r\nfmr_pool = srp_alloc_fmr_pool(target);\r\nif (IS_ERR(fmr_pool)) {\r\nret = PTR_ERR(fmr_pool);\r\nshost_printk(KERN_WARNING, target->scsi_host, PFX\r\n"FMR pool allocation failed (%d)\n", ret);\r\ngoto err_qp;\r\n}\r\n}\r\nif (ch->qp)\r\nsrp_destroy_qp(ch->qp);\r\nif (ch->recv_cq)\r\nib_free_cq(ch->recv_cq);\r\nif (ch->send_cq)\r\nib_free_cq(ch->send_cq);\r\nch->qp = qp;\r\nch->recv_cq = recv_cq;\r\nch->send_cq = send_cq;\r\nif (dev->use_fast_reg) {\r\nif (ch->fr_pool)\r\nsrp_destroy_fr_pool(ch->fr_pool);\r\nch->fr_pool = fr_pool;\r\n} else if (dev->use_fmr) {\r\nif (ch->fmr_pool)\r\nib_destroy_fmr_pool(ch->fmr_pool);\r\nch->fmr_pool = fmr_pool;\r\n}\r\nkfree(init_attr);\r\nreturn 0;\r\nerr_qp:\r\nsrp_destroy_qp(qp);\r\nerr_send_cq:\r\nib_free_cq(send_cq);\r\nerr_recv_cq:\r\nib_free_cq(recv_cq);\r\nerr:\r\nkfree(init_attr);\r\nreturn ret;\r\n}\r\nstatic void srp_free_ch_ib(struct srp_target_port *target,\r\nstruct srp_rdma_ch *ch)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nint i;\r\nif (!ch->target)\r\nreturn;\r\nif (ch->cm_id) {\r\nib_destroy_cm_id(ch->cm_id);\r\nch->cm_id = NULL;\r\n}\r\nif (!ch->qp)\r\nreturn;\r\nif (dev->use_fast_reg) {\r\nif (ch->fr_pool)\r\nsrp_destroy_fr_pool(ch->fr_pool);\r\n} else if (dev->use_fmr) {\r\nif (ch->fmr_pool)\r\nib_destroy_fmr_pool(ch->fmr_pool);\r\n}\r\nsrp_destroy_qp(ch->qp);\r\nib_free_cq(ch->send_cq);\r\nib_free_cq(ch->recv_cq);\r\nch->target = NULL;\r\nch->qp = NULL;\r\nch->send_cq = ch->recv_cq = NULL;\r\nif (ch->rx_ring) {\r\nfor (i = 0; i < target->queue_size; ++i)\r\nsrp_free_iu(target->srp_host, ch->rx_ring[i]);\r\nkfree(ch->rx_ring);\r\nch->rx_ring = NULL;\r\n}\r\nif (ch->tx_ring) {\r\nfor (i = 0; i < target->queue_size; ++i)\r\nsrp_free_iu(target->srp_host, ch->tx_ring[i]);\r\nkfree(ch->tx_ring);\r\nch->tx_ring = NULL;\r\n}\r\n}\r\nstatic void srp_path_rec_completion(int status,\r\nstruct ib_sa_path_rec *pathrec,\r\nvoid *ch_ptr)\r\n{\r\nstruct srp_rdma_ch *ch = ch_ptr;\r\nstruct srp_target_port *target = ch->target;\r\nch->status = status;\r\nif (status)\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Got failed path rec status %d\n", status);\r\nelse\r\nch->path = *pathrec;\r\ncomplete(&ch->done);\r\n}\r\nstatic int srp_lookup_path(struct srp_rdma_ch *ch)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nint ret;\r\nch->path.numb_path = 1;\r\ninit_completion(&ch->done);\r\nch->path_query_id = ib_sa_path_rec_get(&srp_sa_client,\r\ntarget->srp_host->srp_dev->dev,\r\ntarget->srp_host->port,\r\n&ch->path,\r\nIB_SA_PATH_REC_SERVICE_ID |\r\nIB_SA_PATH_REC_DGID |\r\nIB_SA_PATH_REC_SGID |\r\nIB_SA_PATH_REC_NUMB_PATH |\r\nIB_SA_PATH_REC_PKEY,\r\nSRP_PATH_REC_TIMEOUT_MS,\r\nGFP_KERNEL,\r\nsrp_path_rec_completion,\r\nch, &ch->path_query);\r\nif (ch->path_query_id < 0)\r\nreturn ch->path_query_id;\r\nret = wait_for_completion_interruptible(&ch->done);\r\nif (ret < 0)\r\nreturn ret;\r\nif (ch->status < 0)\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Path record query failed\n");\r\nreturn ch->status;\r\n}\r\nstatic int srp_send_req(struct srp_rdma_ch *ch, bool multich)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct {\r\nstruct ib_cm_req_param param;\r\nstruct srp_login_req priv;\r\n} *req = NULL;\r\nint status;\r\nreq = kzalloc(sizeof *req, GFP_KERNEL);\r\nif (!req)\r\nreturn -ENOMEM;\r\nreq->param.primary_path = &ch->path;\r\nreq->param.alternate_path = NULL;\r\nreq->param.service_id = target->service_id;\r\nreq->param.qp_num = ch->qp->qp_num;\r\nreq->param.qp_type = ch->qp->qp_type;\r\nreq->param.private_data = &req->priv;\r\nreq->param.private_data_len = sizeof req->priv;\r\nreq->param.flow_control = 1;\r\nget_random_bytes(&req->param.starting_psn, 4);\r\nreq->param.starting_psn &= 0xffffff;\r\nreq->param.responder_resources = 4;\r\nreq->param.remote_cm_response_timeout = 20;\r\nreq->param.local_cm_response_timeout = 20;\r\nreq->param.retry_count = target->tl_retry_count;\r\nreq->param.rnr_retry_count = 7;\r\nreq->param.max_cm_retries = 15;\r\nreq->priv.opcode = SRP_LOGIN_REQ;\r\nreq->priv.tag = 0;\r\nreq->priv.req_it_iu_len = cpu_to_be32(target->max_iu_len);\r\nreq->priv.req_buf_fmt = cpu_to_be16(SRP_BUF_FORMAT_DIRECT |\r\nSRP_BUF_FORMAT_INDIRECT);\r\nreq->priv.req_flags = (multich ? SRP_MULTICHAN_MULTI :\r\nSRP_MULTICHAN_SINGLE);\r\nif (target->io_class == SRP_REV10_IB_IO_CLASS) {\r\nmemcpy(req->priv.initiator_port_id,\r\n&target->sgid.global.interface_id, 8);\r\nmemcpy(req->priv.initiator_port_id + 8,\r\n&target->initiator_ext, 8);\r\nmemcpy(req->priv.target_port_id, &target->ioc_guid, 8);\r\nmemcpy(req->priv.target_port_id + 8, &target->id_ext, 8);\r\n} else {\r\nmemcpy(req->priv.initiator_port_id,\r\n&target->initiator_ext, 8);\r\nmemcpy(req->priv.initiator_port_id + 8,\r\n&target->sgid.global.interface_id, 8);\r\nmemcpy(req->priv.target_port_id, &target->id_ext, 8);\r\nmemcpy(req->priv.target_port_id + 8, &target->ioc_guid, 8);\r\n}\r\nif (srp_target_is_topspin(target)) {\r\nshost_printk(KERN_DEBUG, target->scsi_host,\r\nPFX "Topspin/Cisco initiator port ID workaround "\r\n"activated for target GUID %016llx\n",\r\nbe64_to_cpu(target->ioc_guid));\r\nmemset(req->priv.initiator_port_id, 0, 8);\r\nmemcpy(req->priv.initiator_port_id + 8,\r\n&target->srp_host->srp_dev->dev->node_guid, 8);\r\n}\r\nstatus = ib_send_cm_req(ch->cm_id, &req->param);\r\nkfree(req);\r\nreturn status;\r\n}\r\nstatic bool srp_queue_remove_work(struct srp_target_port *target)\r\n{\r\nbool changed = false;\r\nspin_lock_irq(&target->lock);\r\nif (target->state != SRP_TARGET_REMOVED) {\r\ntarget->state = SRP_TARGET_REMOVED;\r\nchanged = true;\r\n}\r\nspin_unlock_irq(&target->lock);\r\nif (changed)\r\nqueue_work(srp_remove_wq, &target->remove_work);\r\nreturn changed;\r\n}\r\nstatic void srp_disconnect_target(struct srp_target_port *target)\r\n{\r\nstruct srp_rdma_ch *ch;\r\nint i;\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nch->connected = false;\r\nif (ch->cm_id && ib_send_cm_dreq(ch->cm_id, NULL, 0)) {\r\nshost_printk(KERN_DEBUG, target->scsi_host,\r\nPFX "Sending CM DREQ failed\n");\r\n}\r\n}\r\n}\r\nstatic void srp_free_req_data(struct srp_target_port *target,\r\nstruct srp_rdma_ch *ch)\r\n{\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = dev->dev;\r\nstruct srp_request *req;\r\nint i;\r\nif (!ch->req_ring)\r\nreturn;\r\nfor (i = 0; i < target->req_ring_size; ++i) {\r\nreq = &ch->req_ring[i];\r\nif (dev->use_fast_reg) {\r\nkfree(req->fr_list);\r\n} else {\r\nkfree(req->fmr_list);\r\nkfree(req->map_page);\r\n}\r\nif (req->indirect_dma_addr) {\r\nib_dma_unmap_single(ibdev, req->indirect_dma_addr,\r\ntarget->indirect_size,\r\nDMA_TO_DEVICE);\r\n}\r\nkfree(req->indirect_desc);\r\n}\r\nkfree(ch->req_ring);\r\nch->req_ring = NULL;\r\n}\r\nstatic int srp_alloc_req_data(struct srp_rdma_ch *ch)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_device *srp_dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = srp_dev->dev;\r\nstruct srp_request *req;\r\nvoid *mr_list;\r\ndma_addr_t dma_addr;\r\nint i, ret = -ENOMEM;\r\nch->req_ring = kcalloc(target->req_ring_size, sizeof(*ch->req_ring),\r\nGFP_KERNEL);\r\nif (!ch->req_ring)\r\ngoto out;\r\nfor (i = 0; i < target->req_ring_size; ++i) {\r\nreq = &ch->req_ring[i];\r\nmr_list = kmalloc(target->mr_per_cmd * sizeof(void *),\r\nGFP_KERNEL);\r\nif (!mr_list)\r\ngoto out;\r\nif (srp_dev->use_fast_reg) {\r\nreq->fr_list = mr_list;\r\n} else {\r\nreq->fmr_list = mr_list;\r\nreq->map_page = kmalloc(srp_dev->max_pages_per_mr *\r\nsizeof(void *), GFP_KERNEL);\r\nif (!req->map_page)\r\ngoto out;\r\n}\r\nreq->indirect_desc = kmalloc(target->indirect_size, GFP_KERNEL);\r\nif (!req->indirect_desc)\r\ngoto out;\r\ndma_addr = ib_dma_map_single(ibdev, req->indirect_desc,\r\ntarget->indirect_size,\r\nDMA_TO_DEVICE);\r\nif (ib_dma_mapping_error(ibdev, dma_addr))\r\ngoto out;\r\nreq->indirect_dma_addr = dma_addr;\r\n}\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic void srp_del_scsi_host_attr(struct Scsi_Host *shost)\r\n{\r\nstruct device_attribute **attr;\r\nfor (attr = shost->hostt->shost_attrs; attr && *attr; ++attr)\r\ndevice_remove_file(&shost->shost_dev, *attr);\r\n}\r\nstatic void srp_remove_target(struct srp_target_port *target)\r\n{\r\nstruct srp_rdma_ch *ch;\r\nint i;\r\nWARN_ON_ONCE(target->state != SRP_TARGET_REMOVED);\r\nsrp_del_scsi_host_attr(target->scsi_host);\r\nsrp_rport_get(target->rport);\r\nsrp_remove_host(target->scsi_host);\r\nscsi_remove_host(target->scsi_host);\r\nsrp_stop_rport_timers(target->rport);\r\nsrp_disconnect_target(target);\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nsrp_free_ch_ib(target, ch);\r\n}\r\ncancel_work_sync(&target->tl_err_work);\r\nsrp_rport_put(target->rport);\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nsrp_free_req_data(target, ch);\r\n}\r\nkfree(target->ch);\r\ntarget->ch = NULL;\r\nspin_lock(&target->srp_host->target_lock);\r\nlist_del(&target->list);\r\nspin_unlock(&target->srp_host->target_lock);\r\nscsi_host_put(target->scsi_host);\r\n}\r\nstatic void srp_remove_work(struct work_struct *work)\r\n{\r\nstruct srp_target_port *target =\r\ncontainer_of(work, struct srp_target_port, remove_work);\r\nWARN_ON_ONCE(target->state != SRP_TARGET_REMOVED);\r\nsrp_remove_target(target);\r\n}\r\nstatic void srp_rport_delete(struct srp_rport *rport)\r\n{\r\nstruct srp_target_port *target = rport->lld_data;\r\nsrp_queue_remove_work(target);\r\n}\r\nstatic int srp_connected_ch(struct srp_target_port *target)\r\n{\r\nint i, c = 0;\r\nfor (i = 0; i < target->ch_count; i++)\r\nc += target->ch[i].connected;\r\nreturn c;\r\n}\r\nstatic int srp_connect_ch(struct srp_rdma_ch *ch, bool multich)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nint ret;\r\nWARN_ON_ONCE(!multich && srp_connected_ch(target) > 0);\r\nret = srp_lookup_path(ch);\r\nif (ret)\r\ngoto out;\r\nwhile (1) {\r\ninit_completion(&ch->done);\r\nret = srp_send_req(ch, multich);\r\nif (ret)\r\ngoto out;\r\nret = wait_for_completion_interruptible(&ch->done);\r\nif (ret < 0)\r\ngoto out;\r\nret = ch->status;\r\nswitch (ret) {\r\ncase 0:\r\nch->connected = true;\r\ngoto out;\r\ncase SRP_PORT_REDIRECT:\r\nret = srp_lookup_path(ch);\r\nif (ret)\r\ngoto out;\r\nbreak;\r\ncase SRP_DLID_REDIRECT:\r\nbreak;\r\ncase SRP_STALE_CONN:\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"giving up on stale connection\n");\r\nret = -ECONNRESET;\r\ngoto out;\r\ndefault:\r\ngoto out;\r\n}\r\n}\r\nout:\r\nreturn ret <= 0 ? ret : -ENODEV;\r\n}\r\nstatic void srp_inv_rkey_err_done(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nsrp_handle_qp_err(cq, wc, "INV RKEY");\r\n}\r\nstatic int srp_inv_rkey(struct srp_request *req, struct srp_rdma_ch *ch,\r\nu32 rkey)\r\n{\r\nstruct ib_send_wr *bad_wr;\r\nstruct ib_send_wr wr = {\r\n.opcode = IB_WR_LOCAL_INV,\r\n.next = NULL,\r\n.num_sge = 0,\r\n.send_flags = 0,\r\n.ex.invalidate_rkey = rkey,\r\n};\r\nwr.wr_cqe = &req->reg_cqe;\r\nreq->reg_cqe.done = srp_inv_rkey_err_done;\r\nreturn ib_post_send(ch->qp, &wr, &bad_wr);\r\n}\r\nstatic void srp_unmap_data(struct scsi_cmnd *scmnd,\r\nstruct srp_rdma_ch *ch,\r\nstruct srp_request *req)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = dev->dev;\r\nint i, res;\r\nif (!scsi_sglist(scmnd) ||\r\n(scmnd->sc_data_direction != DMA_TO_DEVICE &&\r\nscmnd->sc_data_direction != DMA_FROM_DEVICE))\r\nreturn;\r\nif (dev->use_fast_reg) {\r\nstruct srp_fr_desc **pfr;\r\nfor (i = req->nmdesc, pfr = req->fr_list; i > 0; i--, pfr++) {\r\nres = srp_inv_rkey(req, ch, (*pfr)->mr->rkey);\r\nif (res < 0) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"Queueing INV WR for rkey %#x failed (%d)\n",\r\n(*pfr)->mr->rkey, res);\r\nqueue_work(system_long_wq,\r\n&target->tl_err_work);\r\n}\r\n}\r\nif (req->nmdesc)\r\nsrp_fr_pool_put(ch->fr_pool, req->fr_list,\r\nreq->nmdesc);\r\n} else if (dev->use_fmr) {\r\nstruct ib_pool_fmr **pfmr;\r\nfor (i = req->nmdesc, pfmr = req->fmr_list; i > 0; i--, pfmr++)\r\nib_fmr_pool_unmap(*pfmr);\r\n}\r\nib_dma_unmap_sg(ibdev, scsi_sglist(scmnd), scsi_sg_count(scmnd),\r\nscmnd->sc_data_direction);\r\n}\r\nstatic struct scsi_cmnd *srp_claim_req(struct srp_rdma_ch *ch,\r\nstruct srp_request *req,\r\nstruct scsi_device *sdev,\r\nstruct scsi_cmnd *scmnd)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ch->lock, flags);\r\nif (req->scmnd &&\r\n(!sdev || req->scmnd->device == sdev) &&\r\n(!scmnd || req->scmnd == scmnd)) {\r\nscmnd = req->scmnd;\r\nreq->scmnd = NULL;\r\n} else {\r\nscmnd = NULL;\r\n}\r\nspin_unlock_irqrestore(&ch->lock, flags);\r\nreturn scmnd;\r\n}\r\nstatic void srp_free_req(struct srp_rdma_ch *ch, struct srp_request *req,\r\nstruct scsi_cmnd *scmnd, s32 req_lim_delta)\r\n{\r\nunsigned long flags;\r\nsrp_unmap_data(scmnd, ch, req);\r\nspin_lock_irqsave(&ch->lock, flags);\r\nch->req_lim += req_lim_delta;\r\nspin_unlock_irqrestore(&ch->lock, flags);\r\n}\r\nstatic void srp_finish_req(struct srp_rdma_ch *ch, struct srp_request *req,\r\nstruct scsi_device *sdev, int result)\r\n{\r\nstruct scsi_cmnd *scmnd = srp_claim_req(ch, req, sdev, NULL);\r\nif (scmnd) {\r\nsrp_free_req(ch, req, scmnd, 0);\r\nscmnd->result = result;\r\nscmnd->scsi_done(scmnd);\r\n}\r\n}\r\nstatic void srp_terminate_io(struct srp_rport *rport)\r\n{\r\nstruct srp_target_port *target = rport->lld_data;\r\nstruct srp_rdma_ch *ch;\r\nstruct Scsi_Host *shost = target->scsi_host;\r\nstruct scsi_device *sdev;\r\nint i, j;\r\nshost_for_each_device(sdev, shost)\r\nWARN_ON_ONCE(sdev->request_queue->request_fn_active);\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nfor (j = 0; j < target->req_ring_size; ++j) {\r\nstruct srp_request *req = &ch->req_ring[j];\r\nsrp_finish_req(ch, req, NULL,\r\nDID_TRANSPORT_FAILFAST << 16);\r\n}\r\n}\r\n}\r\nstatic int srp_rport_reconnect(struct srp_rport *rport)\r\n{\r\nstruct srp_target_port *target = rport->lld_data;\r\nstruct srp_rdma_ch *ch;\r\nint i, j, ret = 0;\r\nbool multich = false;\r\nsrp_disconnect_target(target);\r\nif (target->state == SRP_TARGET_SCANNING)\r\nreturn -ENODEV;\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nret += srp_new_cm_id(ch);\r\n}\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nfor (j = 0; j < target->req_ring_size; ++j) {\r\nstruct srp_request *req = &ch->req_ring[j];\r\nsrp_finish_req(ch, req, NULL, DID_RESET << 16);\r\n}\r\n}\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nret += srp_create_ch_ib(ch);\r\nINIT_LIST_HEAD(&ch->free_tx);\r\nfor (j = 0; j < target->queue_size; ++j)\r\nlist_add(&ch->tx_ring[j]->list, &ch->free_tx);\r\n}\r\ntarget->qp_in_error = false;\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nif (ret)\r\nbreak;\r\nret = srp_connect_ch(ch, multich);\r\nmultich = true;\r\n}\r\nif (ret == 0)\r\nshost_printk(KERN_INFO, target->scsi_host,\r\nPFX "reconnect succeeded\n");\r\nreturn ret;\r\n}\r\nstatic void srp_map_desc(struct srp_map_state *state, dma_addr_t dma_addr,\r\nunsigned int dma_len, u32 rkey)\r\n{\r\nstruct srp_direct_buf *desc = state->desc;\r\nWARN_ON_ONCE(!dma_len);\r\ndesc->va = cpu_to_be64(dma_addr);\r\ndesc->key = cpu_to_be32(rkey);\r\ndesc->len = cpu_to_be32(dma_len);\r\nstate->total_len += dma_len;\r\nstate->desc++;\r\nstate->ndesc++;\r\n}\r\nstatic int srp_map_finish_fmr(struct srp_map_state *state,\r\nstruct srp_rdma_ch *ch)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_pool_fmr *fmr;\r\nu64 io_addr = 0;\r\nif (state->fmr.next >= state->fmr.end)\r\nreturn -ENOMEM;\r\nWARN_ON_ONCE(!dev->use_fmr);\r\nif (state->npages == 0)\r\nreturn 0;\r\nif (state->npages == 1 && target->global_mr) {\r\nsrp_map_desc(state, state->base_dma_addr, state->dma_len,\r\ntarget->global_mr->rkey);\r\ngoto reset_state;\r\n}\r\nfmr = ib_fmr_pool_map_phys(ch->fmr_pool, state->pages,\r\nstate->npages, io_addr);\r\nif (IS_ERR(fmr))\r\nreturn PTR_ERR(fmr);\r\n*state->fmr.next++ = fmr;\r\nstate->nmdesc++;\r\nsrp_map_desc(state, state->base_dma_addr & ~dev->mr_page_mask,\r\nstate->dma_len, fmr->fmr->rkey);\r\nreset_state:\r\nstate->npages = 0;\r\nstate->dma_len = 0;\r\nreturn 0;\r\n}\r\nstatic void srp_reg_mr_err_done(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nsrp_handle_qp_err(cq, wc, "FAST REG");\r\n}\r\nstatic int srp_map_finish_fr(struct srp_map_state *state,\r\nstruct srp_request *req,\r\nstruct srp_rdma_ch *ch, int sg_nents,\r\nunsigned int *sg_offset_p)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_send_wr *bad_wr;\r\nstruct ib_reg_wr wr;\r\nstruct srp_fr_desc *desc;\r\nu32 rkey;\r\nint n, err;\r\nif (state->fr.next >= state->fr.end)\r\nreturn -ENOMEM;\r\nWARN_ON_ONCE(!dev->use_fast_reg);\r\nif (sg_nents == 1 && target->global_mr) {\r\nunsigned int sg_offset = sg_offset_p ? *sg_offset_p : 0;\r\nsrp_map_desc(state, sg_dma_address(state->sg) + sg_offset,\r\nsg_dma_len(state->sg) - sg_offset,\r\ntarget->global_mr->rkey);\r\nif (sg_offset_p)\r\n*sg_offset_p = 0;\r\nreturn 1;\r\n}\r\ndesc = srp_fr_pool_get(ch->fr_pool);\r\nif (!desc)\r\nreturn -ENOMEM;\r\nrkey = ib_inc_rkey(desc->mr->rkey);\r\nib_update_fast_reg_key(desc->mr, rkey);\r\nn = ib_map_mr_sg(desc->mr, state->sg, sg_nents, sg_offset_p,\r\ndev->mr_page_size);\r\nif (unlikely(n < 0)) {\r\nsrp_fr_pool_put(ch->fr_pool, &desc, 1);\r\npr_debug("%s: ib_map_mr_sg(%d, %d) returned %d.\n",\r\ndev_name(&req->scmnd->device->sdev_gendev), sg_nents,\r\nsg_offset_p ? *sg_offset_p : -1, n);\r\nreturn n;\r\n}\r\nWARN_ON_ONCE(desc->mr->length == 0);\r\nreq->reg_cqe.done = srp_reg_mr_err_done;\r\nwr.wr.next = NULL;\r\nwr.wr.opcode = IB_WR_REG_MR;\r\nwr.wr.wr_cqe = &req->reg_cqe;\r\nwr.wr.num_sge = 0;\r\nwr.wr.send_flags = 0;\r\nwr.mr = desc->mr;\r\nwr.key = desc->mr->rkey;\r\nwr.access = (IB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE);\r\n*state->fr.next++ = desc;\r\nstate->nmdesc++;\r\nsrp_map_desc(state, desc->mr->iova,\r\ndesc->mr->length, desc->mr->rkey);\r\nerr = ib_post_send(ch->qp, &wr.wr, &bad_wr);\r\nif (unlikely(err)) {\r\nWARN_ON_ONCE(err == -ENOMEM);\r\nreturn err;\r\n}\r\nreturn n;\r\n}\r\nstatic int srp_map_sg_entry(struct srp_map_state *state,\r\nstruct srp_rdma_ch *ch,\r\nstruct scatterlist *sg, int sg_index)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = dev->dev;\r\ndma_addr_t dma_addr = ib_sg_dma_address(ibdev, sg);\r\nunsigned int dma_len = ib_sg_dma_len(ibdev, sg);\r\nunsigned int len = 0;\r\nint ret;\r\nWARN_ON_ONCE(!dma_len);\r\nwhile (dma_len) {\r\nunsigned offset = dma_addr & ~dev->mr_page_mask;\r\nif (state->npages == dev->max_pages_per_mr || offset != 0) {\r\nret = srp_map_finish_fmr(state, ch);\r\nif (ret)\r\nreturn ret;\r\n}\r\nlen = min_t(unsigned int, dma_len, dev->mr_page_size - offset);\r\nif (!state->npages)\r\nstate->base_dma_addr = dma_addr;\r\nstate->pages[state->npages++] = dma_addr & dev->mr_page_mask;\r\nstate->dma_len += len;\r\ndma_addr += len;\r\ndma_len -= len;\r\n}\r\nret = 0;\r\nif (len != dev->mr_page_size)\r\nret = srp_map_finish_fmr(state, ch);\r\nreturn ret;\r\n}\r\nstatic int srp_map_sg_fmr(struct srp_map_state *state, struct srp_rdma_ch *ch,\r\nstruct srp_request *req, struct scatterlist *scat,\r\nint count)\r\n{\r\nstruct scatterlist *sg;\r\nint i, ret;\r\nstate->pages = req->map_page;\r\nstate->fmr.next = req->fmr_list;\r\nstate->fmr.end = req->fmr_list + ch->target->mr_per_cmd;\r\nfor_each_sg(scat, sg, count, i) {\r\nret = srp_map_sg_entry(state, ch, sg, i);\r\nif (ret)\r\nreturn ret;\r\n}\r\nret = srp_map_finish_fmr(state, ch);\r\nif (ret)\r\nreturn ret;\r\nreturn 0;\r\n}\r\nstatic int srp_map_sg_fr(struct srp_map_state *state, struct srp_rdma_ch *ch,\r\nstruct srp_request *req, struct scatterlist *scat,\r\nint count)\r\n{\r\nunsigned int sg_offset = 0;\r\nstate->fr.next = req->fr_list;\r\nstate->fr.end = req->fr_list + ch->target->mr_per_cmd;\r\nstate->sg = scat;\r\nif (count == 0)\r\nreturn 0;\r\nwhile (count) {\r\nint i, n;\r\nn = srp_map_finish_fr(state, req, ch, count, &sg_offset);\r\nif (unlikely(n < 0))\r\nreturn n;\r\ncount -= n;\r\nfor (i = 0; i < n; i++)\r\nstate->sg = sg_next(state->sg);\r\n}\r\nreturn 0;\r\n}\r\nstatic int srp_map_sg_dma(struct srp_map_state *state, struct srp_rdma_ch *ch,\r\nstruct srp_request *req, struct scatterlist *scat,\r\nint count)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct scatterlist *sg;\r\nint i;\r\nfor_each_sg(scat, sg, count, i) {\r\nsrp_map_desc(state, ib_sg_dma_address(dev->dev, sg),\r\nib_sg_dma_len(dev->dev, sg),\r\ntarget->global_mr->rkey);\r\n}\r\nreturn 0;\r\n}\r\nstatic int srp_map_idb(struct srp_rdma_ch *ch, struct srp_request *req,\r\nvoid **next_mr, void **end_mr, u32 idb_len,\r\n__be32 *idb_rkey)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_device *dev = target->srp_host->srp_dev;\r\nstruct srp_map_state state;\r\nstruct srp_direct_buf idb_desc;\r\nu64 idb_pages[1];\r\nstruct scatterlist idb_sg[1];\r\nint ret;\r\nmemset(&state, 0, sizeof(state));\r\nmemset(&idb_desc, 0, sizeof(idb_desc));\r\nstate.gen.next = next_mr;\r\nstate.gen.end = end_mr;\r\nstate.desc = &idb_desc;\r\nstate.base_dma_addr = req->indirect_dma_addr;\r\nstate.dma_len = idb_len;\r\nif (dev->use_fast_reg) {\r\nstate.sg = idb_sg;\r\nsg_init_one(idb_sg, req->indirect_desc, idb_len);\r\nidb_sg->dma_address = req->indirect_dma_addr;\r\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\r\nidb_sg->dma_length = idb_sg->length;\r\n#endif\r\nret = srp_map_finish_fr(&state, req, ch, 1, NULL);\r\nif (ret < 0)\r\nreturn ret;\r\nWARN_ON_ONCE(ret < 1);\r\n} else if (dev->use_fmr) {\r\nstate.pages = idb_pages;\r\nstate.pages[0] = (req->indirect_dma_addr &\r\ndev->mr_page_mask);\r\nstate.npages = 1;\r\nret = srp_map_finish_fmr(&state, ch);\r\nif (ret < 0)\r\nreturn ret;\r\n} else {\r\nreturn -EINVAL;\r\n}\r\n*idb_rkey = idb_desc.key;\r\nreturn 0;\r\n}\r\nstatic void srp_check_mapping(struct srp_map_state *state,\r\nstruct srp_rdma_ch *ch, struct srp_request *req,\r\nstruct scatterlist *scat, int count)\r\n{\r\nstruct srp_device *dev = ch->target->srp_host->srp_dev;\r\nstruct srp_fr_desc **pfr;\r\nu64 desc_len = 0, mr_len = 0;\r\nint i;\r\nfor (i = 0; i < state->ndesc; i++)\r\ndesc_len += be32_to_cpu(req->indirect_desc[i].len);\r\nif (dev->use_fast_reg)\r\nfor (i = 0, pfr = req->fr_list; i < state->nmdesc; i++, pfr++)\r\nmr_len += (*pfr)->mr->length;\r\nelse if (dev->use_fmr)\r\nfor (i = 0; i < state->nmdesc; i++)\r\nmr_len += be32_to_cpu(req->indirect_desc[i].len);\r\nif (desc_len != scsi_bufflen(req->scmnd) ||\r\nmr_len > scsi_bufflen(req->scmnd))\r\npr_err("Inconsistent: scsi len %d <> desc len %lld <> mr len %lld; ndesc %d; nmdesc = %d\n",\r\nscsi_bufflen(req->scmnd), desc_len, mr_len,\r\nstate->ndesc, state->nmdesc);\r\n}\r\nstatic int srp_map_data(struct scsi_cmnd *scmnd, struct srp_rdma_ch *ch,\r\nstruct srp_request *req)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct scatterlist *scat;\r\nstruct srp_cmd *cmd = req->cmd->buf;\r\nint len, nents, count, ret;\r\nstruct srp_device *dev;\r\nstruct ib_device *ibdev;\r\nstruct srp_map_state state;\r\nstruct srp_indirect_buf *indirect_hdr;\r\nu32 idb_len, table_len;\r\n__be32 idb_rkey;\r\nu8 fmt;\r\nif (!scsi_sglist(scmnd) || scmnd->sc_data_direction == DMA_NONE)\r\nreturn sizeof (struct srp_cmd);\r\nif (scmnd->sc_data_direction != DMA_FROM_DEVICE &&\r\nscmnd->sc_data_direction != DMA_TO_DEVICE) {\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Unhandled data direction %d\n",\r\nscmnd->sc_data_direction);\r\nreturn -EINVAL;\r\n}\r\nnents = scsi_sg_count(scmnd);\r\nscat = scsi_sglist(scmnd);\r\ndev = target->srp_host->srp_dev;\r\nibdev = dev->dev;\r\ncount = ib_dma_map_sg(ibdev, scat, nents, scmnd->sc_data_direction);\r\nif (unlikely(count == 0))\r\nreturn -EIO;\r\nfmt = SRP_DATA_DESC_DIRECT;\r\nlen = sizeof (struct srp_cmd) + sizeof (struct srp_direct_buf);\r\nif (count == 1 && target->global_mr) {\r\nstruct srp_direct_buf *buf = (void *) cmd->add_data;\r\nbuf->va = cpu_to_be64(ib_sg_dma_address(ibdev, scat));\r\nbuf->key = cpu_to_be32(target->global_mr->rkey);\r\nbuf->len = cpu_to_be32(ib_sg_dma_len(ibdev, scat));\r\nreq->nmdesc = 0;\r\ngoto map_complete;\r\n}\r\nindirect_hdr = (void *) cmd->add_data;\r\nib_dma_sync_single_for_cpu(ibdev, req->indirect_dma_addr,\r\ntarget->indirect_size, DMA_TO_DEVICE);\r\nmemset(&state, 0, sizeof(state));\r\nstate.desc = req->indirect_desc;\r\nif (dev->use_fast_reg)\r\nret = srp_map_sg_fr(&state, ch, req, scat, count);\r\nelse if (dev->use_fmr)\r\nret = srp_map_sg_fmr(&state, ch, req, scat, count);\r\nelse\r\nret = srp_map_sg_dma(&state, ch, req, scat, count);\r\nreq->nmdesc = state.nmdesc;\r\nif (ret < 0)\r\ngoto unmap;\r\n#if defined(DYNAMIC_DEBUG)\r\n{\r\nDEFINE_DYNAMIC_DEBUG_METADATA(ddm,\r\n"Memory mapping consistency check");\r\nif (unlikely(ddm.flags & _DPRINTK_FLAGS_PRINT))\r\nsrp_check_mapping(&state, ch, req, scat, count);\r\n}\r\n#endif\r\nif (state.ndesc == 1) {\r\nstruct srp_direct_buf *buf = (void *) cmd->add_data;\r\n*buf = req->indirect_desc[0];\r\ngoto map_complete;\r\n}\r\nif (unlikely(target->cmd_sg_cnt < state.ndesc &&\r\n!target->allow_ext_sg)) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\n"Could not fit S/G list into SRP_CMD\n");\r\nret = -EIO;\r\ngoto unmap;\r\n}\r\ncount = min(state.ndesc, target->cmd_sg_cnt);\r\ntable_len = state.ndesc * sizeof (struct srp_direct_buf);\r\nidb_len = sizeof(struct srp_indirect_buf) + table_len;\r\nfmt = SRP_DATA_DESC_INDIRECT;\r\nlen = sizeof(struct srp_cmd) + sizeof (struct srp_indirect_buf);\r\nlen += count * sizeof (struct srp_direct_buf);\r\nmemcpy(indirect_hdr->desc_list, req->indirect_desc,\r\ncount * sizeof (struct srp_direct_buf));\r\nif (!target->global_mr) {\r\nret = srp_map_idb(ch, req, state.gen.next, state.gen.end,\r\nidb_len, &idb_rkey);\r\nif (ret < 0)\r\ngoto unmap;\r\nreq->nmdesc++;\r\n} else {\r\nidb_rkey = cpu_to_be32(target->global_mr->rkey);\r\n}\r\nindirect_hdr->table_desc.va = cpu_to_be64(req->indirect_dma_addr);\r\nindirect_hdr->table_desc.key = idb_rkey;\r\nindirect_hdr->table_desc.len = cpu_to_be32(table_len);\r\nindirect_hdr->len = cpu_to_be32(state.total_len);\r\nif (scmnd->sc_data_direction == DMA_TO_DEVICE)\r\ncmd->data_out_desc_cnt = count;\r\nelse\r\ncmd->data_in_desc_cnt = count;\r\nib_dma_sync_single_for_device(ibdev, req->indirect_dma_addr, table_len,\r\nDMA_TO_DEVICE);\r\nmap_complete:\r\nif (scmnd->sc_data_direction == DMA_TO_DEVICE)\r\ncmd->buf_fmt = fmt << 4;\r\nelse\r\ncmd->buf_fmt = fmt;\r\nreturn len;\r\nunmap:\r\nsrp_unmap_data(scmnd, ch, req);\r\nif (ret == -ENOMEM && req->nmdesc >= target->mr_pool_size)\r\nret = -E2BIG;\r\nreturn ret;\r\n}\r\nstatic void srp_put_tx_iu(struct srp_rdma_ch *ch, struct srp_iu *iu,\r\nenum srp_iu_type iu_type)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&ch->lock, flags);\r\nlist_add(&iu->list, &ch->free_tx);\r\nif (iu_type != SRP_IU_RSP)\r\n++ch->req_lim;\r\nspin_unlock_irqrestore(&ch->lock, flags);\r\n}\r\nstatic struct srp_iu *__srp_get_tx_iu(struct srp_rdma_ch *ch,\r\nenum srp_iu_type iu_type)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\ns32 rsv = (iu_type == SRP_IU_TSK_MGMT) ? 0 : SRP_TSK_MGMT_SQ_SIZE;\r\nstruct srp_iu *iu;\r\nib_process_cq_direct(ch->send_cq, -1);\r\nif (list_empty(&ch->free_tx))\r\nreturn NULL;\r\nif (iu_type != SRP_IU_RSP) {\r\nif (ch->req_lim <= rsv) {\r\n++target->zero_req_lim;\r\nreturn NULL;\r\n}\r\n--ch->req_lim;\r\n}\r\niu = list_first_entry(&ch->free_tx, struct srp_iu, list);\r\nlist_del(&iu->list);\r\nreturn iu;\r\n}\r\nstatic void srp_send_done(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nstruct srp_iu *iu = container_of(wc->wr_cqe, struct srp_iu, cqe);\r\nstruct srp_rdma_ch *ch = cq->cq_context;\r\nif (unlikely(wc->status != IB_WC_SUCCESS)) {\r\nsrp_handle_qp_err(cq, wc, "SEND");\r\nreturn;\r\n}\r\nlist_add(&iu->list, &ch->free_tx);\r\n}\r\nstatic int srp_post_send(struct srp_rdma_ch *ch, struct srp_iu *iu, int len)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct ib_sge list;\r\nstruct ib_send_wr wr, *bad_wr;\r\nlist.addr = iu->dma;\r\nlist.length = len;\r\nlist.lkey = target->lkey;\r\niu->cqe.done = srp_send_done;\r\nwr.next = NULL;\r\nwr.wr_cqe = &iu->cqe;\r\nwr.sg_list = &list;\r\nwr.num_sge = 1;\r\nwr.opcode = IB_WR_SEND;\r\nwr.send_flags = IB_SEND_SIGNALED;\r\nreturn ib_post_send(ch->qp, &wr, &bad_wr);\r\n}\r\nstatic int srp_post_recv(struct srp_rdma_ch *ch, struct srp_iu *iu)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct ib_recv_wr wr, *bad_wr;\r\nstruct ib_sge list;\r\nlist.addr = iu->dma;\r\nlist.length = iu->size;\r\nlist.lkey = target->lkey;\r\niu->cqe.done = srp_recv_done;\r\nwr.next = NULL;\r\nwr.wr_cqe = &iu->cqe;\r\nwr.sg_list = &list;\r\nwr.num_sge = 1;\r\nreturn ib_post_recv(ch->qp, &wr, &bad_wr);\r\n}\r\nstatic void srp_process_rsp(struct srp_rdma_ch *ch, struct srp_rsp *rsp)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_request *req;\r\nstruct scsi_cmnd *scmnd;\r\nunsigned long flags;\r\nif (unlikely(rsp->tag & SRP_TAG_TSK_MGMT)) {\r\nspin_lock_irqsave(&ch->lock, flags);\r\nch->req_lim += be32_to_cpu(rsp->req_lim_delta);\r\nspin_unlock_irqrestore(&ch->lock, flags);\r\nch->tsk_mgmt_status = -1;\r\nif (be32_to_cpu(rsp->resp_data_len) >= 4)\r\nch->tsk_mgmt_status = rsp->data[3];\r\ncomplete(&ch->tsk_mgmt_done);\r\n} else {\r\nscmnd = scsi_host_find_tag(target->scsi_host, rsp->tag);\r\nif (scmnd) {\r\nreq = (void *)scmnd->host_scribble;\r\nscmnd = srp_claim_req(ch, req, NULL, scmnd);\r\n}\r\nif (!scmnd) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\n"Null scmnd for RSP w/tag %#016llx received on ch %td / QP %#x\n",\r\nrsp->tag, ch - target->ch, ch->qp->qp_num);\r\nspin_lock_irqsave(&ch->lock, flags);\r\nch->req_lim += be32_to_cpu(rsp->req_lim_delta);\r\nspin_unlock_irqrestore(&ch->lock, flags);\r\nreturn;\r\n}\r\nscmnd->result = rsp->status;\r\nif (rsp->flags & SRP_RSP_FLAG_SNSVALID) {\r\nmemcpy(scmnd->sense_buffer, rsp->data +\r\nbe32_to_cpu(rsp->resp_data_len),\r\nmin_t(int, be32_to_cpu(rsp->sense_data_len),\r\nSCSI_SENSE_BUFFERSIZE));\r\n}\r\nif (unlikely(rsp->flags & SRP_RSP_FLAG_DIUNDER))\r\nscsi_set_resid(scmnd, be32_to_cpu(rsp->data_in_res_cnt));\r\nelse if (unlikely(rsp->flags & SRP_RSP_FLAG_DIOVER))\r\nscsi_set_resid(scmnd, -be32_to_cpu(rsp->data_in_res_cnt));\r\nelse if (unlikely(rsp->flags & SRP_RSP_FLAG_DOUNDER))\r\nscsi_set_resid(scmnd, be32_to_cpu(rsp->data_out_res_cnt));\r\nelse if (unlikely(rsp->flags & SRP_RSP_FLAG_DOOVER))\r\nscsi_set_resid(scmnd, -be32_to_cpu(rsp->data_out_res_cnt));\r\nsrp_free_req(ch, req, scmnd,\r\nbe32_to_cpu(rsp->req_lim_delta));\r\nscmnd->host_scribble = NULL;\r\nscmnd->scsi_done(scmnd);\r\n}\r\n}\r\nstatic int srp_response_common(struct srp_rdma_ch *ch, s32 req_delta,\r\nvoid *rsp, int len)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct ib_device *dev = target->srp_host->srp_dev->dev;\r\nunsigned long flags;\r\nstruct srp_iu *iu;\r\nint err;\r\nspin_lock_irqsave(&ch->lock, flags);\r\nch->req_lim += req_delta;\r\niu = __srp_get_tx_iu(ch, SRP_IU_RSP);\r\nspin_unlock_irqrestore(&ch->lock, flags);\r\nif (!iu) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"no IU available to send response\n");\r\nreturn 1;\r\n}\r\nib_dma_sync_single_for_cpu(dev, iu->dma, len, DMA_TO_DEVICE);\r\nmemcpy(iu->buf, rsp, len);\r\nib_dma_sync_single_for_device(dev, iu->dma, len, DMA_TO_DEVICE);\r\nerr = srp_post_send(ch, iu, len);\r\nif (err) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"unable to post response: %d\n", err);\r\nsrp_put_tx_iu(ch, iu, SRP_IU_RSP);\r\n}\r\nreturn err;\r\n}\r\nstatic void srp_process_cred_req(struct srp_rdma_ch *ch,\r\nstruct srp_cred_req *req)\r\n{\r\nstruct srp_cred_rsp rsp = {\r\n.opcode = SRP_CRED_RSP,\r\n.tag = req->tag,\r\n};\r\ns32 delta = be32_to_cpu(req->req_lim_delta);\r\nif (srp_response_common(ch, delta, &rsp, sizeof(rsp)))\r\nshost_printk(KERN_ERR, ch->target->scsi_host, PFX\r\n"problems processing SRP_CRED_REQ\n");\r\n}\r\nstatic void srp_process_aer_req(struct srp_rdma_ch *ch,\r\nstruct srp_aer_req *req)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_aer_rsp rsp = {\r\n.opcode = SRP_AER_RSP,\r\n.tag = req->tag,\r\n};\r\ns32 delta = be32_to_cpu(req->req_lim_delta);\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"ignoring AER for LUN %llu\n", scsilun_to_int(&req->lun));\r\nif (srp_response_common(ch, delta, &rsp, sizeof(rsp)))\r\nshost_printk(KERN_ERR, target->scsi_host, PFX\r\n"problems processing SRP_AER_REQ\n");\r\n}\r\nstatic void srp_recv_done(struct ib_cq *cq, struct ib_wc *wc)\r\n{\r\nstruct srp_iu *iu = container_of(wc->wr_cqe, struct srp_iu, cqe);\r\nstruct srp_rdma_ch *ch = cq->cq_context;\r\nstruct srp_target_port *target = ch->target;\r\nstruct ib_device *dev = target->srp_host->srp_dev->dev;\r\nint res;\r\nu8 opcode;\r\nif (unlikely(wc->status != IB_WC_SUCCESS)) {\r\nsrp_handle_qp_err(cq, wc, "RECV");\r\nreturn;\r\n}\r\nib_dma_sync_single_for_cpu(dev, iu->dma, ch->max_ti_iu_len,\r\nDMA_FROM_DEVICE);\r\nopcode = *(u8 *) iu->buf;\r\nif (0) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "recv completion, opcode 0x%02x\n", opcode);\r\nprint_hex_dump(KERN_ERR, "", DUMP_PREFIX_OFFSET, 8, 1,\r\niu->buf, wc->byte_len, true);\r\n}\r\nswitch (opcode) {\r\ncase SRP_RSP:\r\nsrp_process_rsp(ch, iu->buf);\r\nbreak;\r\ncase SRP_CRED_REQ:\r\nsrp_process_cred_req(ch, iu->buf);\r\nbreak;\r\ncase SRP_AER_REQ:\r\nsrp_process_aer_req(ch, iu->buf);\r\nbreak;\r\ncase SRP_T_LOGOUT:\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Got target logout request\n");\r\nbreak;\r\ndefault:\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Unhandled SRP opcode 0x%02x\n", opcode);\r\nbreak;\r\n}\r\nib_dma_sync_single_for_device(dev, iu->dma, ch->max_ti_iu_len,\r\nDMA_FROM_DEVICE);\r\nres = srp_post_recv(ch, iu);\r\nif (res != 0)\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Recv failed with error code %d\n", res);\r\n}\r\nstatic void srp_tl_err_work(struct work_struct *work)\r\n{\r\nstruct srp_target_port *target;\r\ntarget = container_of(work, struct srp_target_port, tl_err_work);\r\nif (target->rport)\r\nsrp_start_tl_fail_timers(target->rport);\r\n}\r\nstatic void srp_handle_qp_err(struct ib_cq *cq, struct ib_wc *wc,\r\nconst char *opname)\r\n{\r\nstruct srp_rdma_ch *ch = cq->cq_context;\r\nstruct srp_target_port *target = ch->target;\r\nif (ch->connected && !target->qp_in_error) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "failed %s status %s (%d) for CQE %p\n",\r\nopname, ib_wc_status_msg(wc->status), wc->status,\r\nwc->wr_cqe);\r\nqueue_work(system_long_wq, &target->tl_err_work);\r\n}\r\ntarget->qp_in_error = true;\r\n}\r\nstatic int srp_queuecommand(struct Scsi_Host *shost, struct scsi_cmnd *scmnd)\r\n{\r\nstruct srp_target_port *target = host_to_target(shost);\r\nstruct srp_rport *rport = target->rport;\r\nstruct srp_rdma_ch *ch;\r\nstruct srp_request *req;\r\nstruct srp_iu *iu;\r\nstruct srp_cmd *cmd;\r\nstruct ib_device *dev;\r\nunsigned long flags;\r\nu32 tag;\r\nu16 idx;\r\nint len, ret;\r\nconst bool in_scsi_eh = !in_interrupt() && current == shost->ehandler;\r\nif (in_scsi_eh)\r\nmutex_lock(&rport->mutex);\r\nscmnd->result = srp_chkready(target->rport);\r\nif (unlikely(scmnd->result))\r\ngoto err;\r\nWARN_ON_ONCE(scmnd->request->tag < 0);\r\ntag = blk_mq_unique_tag(scmnd->request);\r\nch = &target->ch[blk_mq_unique_tag_to_hwq(tag)];\r\nidx = blk_mq_unique_tag_to_tag(tag);\r\nWARN_ONCE(idx >= target->req_ring_size, "%s: tag %#x: idx %d >= %d\n",\r\ndev_name(&shost->shost_gendev), tag, idx,\r\ntarget->req_ring_size);\r\nspin_lock_irqsave(&ch->lock, flags);\r\niu = __srp_get_tx_iu(ch, SRP_IU_CMD);\r\nspin_unlock_irqrestore(&ch->lock, flags);\r\nif (!iu)\r\ngoto err;\r\nreq = &ch->req_ring[idx];\r\ndev = target->srp_host->srp_dev->dev;\r\nib_dma_sync_single_for_cpu(dev, iu->dma, target->max_iu_len,\r\nDMA_TO_DEVICE);\r\nscmnd->host_scribble = (void *) req;\r\ncmd = iu->buf;\r\nmemset(cmd, 0, sizeof *cmd);\r\ncmd->opcode = SRP_CMD;\r\nint_to_scsilun(scmnd->device->lun, &cmd->lun);\r\ncmd->tag = tag;\r\nmemcpy(cmd->cdb, scmnd->cmnd, scmnd->cmd_len);\r\nreq->scmnd = scmnd;\r\nreq->cmd = iu;\r\nlen = srp_map_data(scmnd, ch, req);\r\nif (len < 0) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Failed to map data (%d)\n", len);\r\nscmnd->result = len == -ENOMEM ?\r\nDID_OK << 16 | QUEUE_FULL << 1 : DID_ERROR << 16;\r\ngoto err_iu;\r\n}\r\nib_dma_sync_single_for_device(dev, iu->dma, target->max_iu_len,\r\nDMA_TO_DEVICE);\r\nif (srp_post_send(ch, iu, len)) {\r\nshost_printk(KERN_ERR, target->scsi_host, PFX "Send failed\n");\r\ngoto err_unmap;\r\n}\r\nret = 0;\r\nunlock_rport:\r\nif (in_scsi_eh)\r\nmutex_unlock(&rport->mutex);\r\nreturn ret;\r\nerr_unmap:\r\nsrp_unmap_data(scmnd, ch, req);\r\nerr_iu:\r\nsrp_put_tx_iu(ch, iu, SRP_IU_CMD);\r\nreq->scmnd = NULL;\r\nerr:\r\nif (scmnd->result) {\r\nscmnd->scsi_done(scmnd);\r\nret = 0;\r\n} else {\r\nret = SCSI_MLQUEUE_HOST_BUSY;\r\n}\r\ngoto unlock_rport;\r\n}\r\nstatic int srp_alloc_iu_bufs(struct srp_rdma_ch *ch)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nint i;\r\nch->rx_ring = kcalloc(target->queue_size, sizeof(*ch->rx_ring),\r\nGFP_KERNEL);\r\nif (!ch->rx_ring)\r\ngoto err_no_ring;\r\nch->tx_ring = kcalloc(target->queue_size, sizeof(*ch->tx_ring),\r\nGFP_KERNEL);\r\nif (!ch->tx_ring)\r\ngoto err_no_ring;\r\nfor (i = 0; i < target->queue_size; ++i) {\r\nch->rx_ring[i] = srp_alloc_iu(target->srp_host,\r\nch->max_ti_iu_len,\r\nGFP_KERNEL, DMA_FROM_DEVICE);\r\nif (!ch->rx_ring[i])\r\ngoto err;\r\n}\r\nfor (i = 0; i < target->queue_size; ++i) {\r\nch->tx_ring[i] = srp_alloc_iu(target->srp_host,\r\ntarget->max_iu_len,\r\nGFP_KERNEL, DMA_TO_DEVICE);\r\nif (!ch->tx_ring[i])\r\ngoto err;\r\nlist_add(&ch->tx_ring[i]->list, &ch->free_tx);\r\n}\r\nreturn 0;\r\nerr:\r\nfor (i = 0; i < target->queue_size; ++i) {\r\nsrp_free_iu(target->srp_host, ch->rx_ring[i]);\r\nsrp_free_iu(target->srp_host, ch->tx_ring[i]);\r\n}\r\nerr_no_ring:\r\nkfree(ch->tx_ring);\r\nch->tx_ring = NULL;\r\nkfree(ch->rx_ring);\r\nch->rx_ring = NULL;\r\nreturn -ENOMEM;\r\n}\r\nstatic uint32_t srp_compute_rq_tmo(struct ib_qp_attr *qp_attr, int attr_mask)\r\n{\r\nuint64_t T_tr_ns, max_compl_time_ms;\r\nuint32_t rq_tmo_jiffies;\r\nWARN_ON_ONCE((attr_mask & (IB_QP_TIMEOUT | IB_QP_RETRY_CNT)) !=\r\n(IB_QP_TIMEOUT | IB_QP_RETRY_CNT));\r\nT_tr_ns = 4096 * (1ULL << qp_attr->timeout);\r\nmax_compl_time_ms = qp_attr->retry_cnt * 4 * T_tr_ns;\r\ndo_div(max_compl_time_ms, NSEC_PER_MSEC);\r\nrq_tmo_jiffies = msecs_to_jiffies(max_compl_time_ms + 1000);\r\nreturn rq_tmo_jiffies;\r\n}\r\nstatic void srp_cm_rep_handler(struct ib_cm_id *cm_id,\r\nconst struct srp_login_rsp *lrsp,\r\nstruct srp_rdma_ch *ch)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct ib_qp_attr *qp_attr = NULL;\r\nint attr_mask = 0;\r\nint ret;\r\nint i;\r\nif (lrsp->opcode == SRP_LOGIN_RSP) {\r\nch->max_ti_iu_len = be32_to_cpu(lrsp->max_ti_iu_len);\r\nch->req_lim = be32_to_cpu(lrsp->req_lim_delta);\r\ntarget->scsi_host->can_queue\r\n= min(ch->req_lim - SRP_TSK_MGMT_SQ_SIZE,\r\ntarget->scsi_host->can_queue);\r\ntarget->scsi_host->cmd_per_lun\r\n= min_t(int, target->scsi_host->can_queue,\r\ntarget->scsi_host->cmd_per_lun);\r\n} else {\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Unhandled RSP opcode %#x\n", lrsp->opcode);\r\nret = -ECONNRESET;\r\ngoto error;\r\n}\r\nif (!ch->rx_ring) {\r\nret = srp_alloc_iu_bufs(ch);\r\nif (ret)\r\ngoto error;\r\n}\r\nret = -ENOMEM;\r\nqp_attr = kmalloc(sizeof *qp_attr, GFP_KERNEL);\r\nif (!qp_attr)\r\ngoto error;\r\nqp_attr->qp_state = IB_QPS_RTR;\r\nret = ib_cm_init_qp_attr(cm_id, qp_attr, &attr_mask);\r\nif (ret)\r\ngoto error_free;\r\nret = ib_modify_qp(ch->qp, qp_attr, attr_mask);\r\nif (ret)\r\ngoto error_free;\r\nfor (i = 0; i < target->queue_size; i++) {\r\nstruct srp_iu *iu = ch->rx_ring[i];\r\nret = srp_post_recv(ch, iu);\r\nif (ret)\r\ngoto error_free;\r\n}\r\nqp_attr->qp_state = IB_QPS_RTS;\r\nret = ib_cm_init_qp_attr(cm_id, qp_attr, &attr_mask);\r\nif (ret)\r\ngoto error_free;\r\ntarget->rq_tmo_jiffies = srp_compute_rq_tmo(qp_attr, attr_mask);\r\nret = ib_modify_qp(ch->qp, qp_attr, attr_mask);\r\nif (ret)\r\ngoto error_free;\r\nret = ib_send_cm_rtu(cm_id, NULL, 0);\r\nerror_free:\r\nkfree(qp_attr);\r\nerror:\r\nch->status = ret;\r\n}\r\nstatic void srp_cm_rej_handler(struct ib_cm_id *cm_id,\r\nstruct ib_cm_event *event,\r\nstruct srp_rdma_ch *ch)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct Scsi_Host *shost = target->scsi_host;\r\nstruct ib_class_port_info *cpi;\r\nint opcode;\r\nswitch (event->param.rej_rcvd.reason) {\r\ncase IB_CM_REJ_PORT_CM_REDIRECT:\r\ncpi = event->param.rej_rcvd.ari;\r\nch->path.dlid = cpi->redirect_lid;\r\nch->path.pkey = cpi->redirect_pkey;\r\ncm_id->remote_cm_qpn = be32_to_cpu(cpi->redirect_qp) & 0x00ffffff;\r\nmemcpy(ch->path.dgid.raw, cpi->redirect_gid, 16);\r\nch->status = ch->path.dlid ?\r\nSRP_DLID_REDIRECT : SRP_PORT_REDIRECT;\r\nbreak;\r\ncase IB_CM_REJ_PORT_REDIRECT:\r\nif (srp_target_is_topspin(target)) {\r\nmemcpy(ch->path.dgid.raw,\r\nevent->param.rej_rcvd.ari, 16);\r\nshost_printk(KERN_DEBUG, shost,\r\nPFX "Topspin/Cisco redirect to target port GID %016llx%016llx\n",\r\nbe64_to_cpu(ch->path.dgid.global.subnet_prefix),\r\nbe64_to_cpu(ch->path.dgid.global.interface_id));\r\nch->status = SRP_PORT_REDIRECT;\r\n} else {\r\nshost_printk(KERN_WARNING, shost,\r\n" REJ reason: IB_CM_REJ_PORT_REDIRECT\n");\r\nch->status = -ECONNRESET;\r\n}\r\nbreak;\r\ncase IB_CM_REJ_DUPLICATE_LOCAL_COMM_ID:\r\nshost_printk(KERN_WARNING, shost,\r\n" REJ reason: IB_CM_REJ_DUPLICATE_LOCAL_COMM_ID\n");\r\nch->status = -ECONNRESET;\r\nbreak;\r\ncase IB_CM_REJ_CONSUMER_DEFINED:\r\nopcode = *(u8 *) event->private_data;\r\nif (opcode == SRP_LOGIN_REJ) {\r\nstruct srp_login_rej *rej = event->private_data;\r\nu32 reason = be32_to_cpu(rej->reason);\r\nif (reason == SRP_LOGIN_REJ_REQ_IT_IU_LENGTH_TOO_LARGE)\r\nshost_printk(KERN_WARNING, shost,\r\nPFX "SRP_LOGIN_REJ: requested max_it_iu_len too large\n");\r\nelse\r\nshost_printk(KERN_WARNING, shost, PFX\r\n"SRP LOGIN from %pI6 to %pI6 REJECTED, reason 0x%08x\n",\r\ntarget->sgid.raw,\r\ntarget->orig_dgid.raw, reason);\r\n} else\r\nshost_printk(KERN_WARNING, shost,\r\n" REJ reason: IB_CM_REJ_CONSUMER_DEFINED,"\r\n" opcode 0x%02x\n", opcode);\r\nch->status = -ECONNRESET;\r\nbreak;\r\ncase IB_CM_REJ_STALE_CONN:\r\nshost_printk(KERN_WARNING, shost, " REJ reason: stale connection\n");\r\nch->status = SRP_STALE_CONN;\r\nbreak;\r\ndefault:\r\nshost_printk(KERN_WARNING, shost, " REJ reason 0x%x\n",\r\nevent->param.rej_rcvd.reason);\r\nch->status = -ECONNRESET;\r\n}\r\n}\r\nstatic int srp_cm_handler(struct ib_cm_id *cm_id, struct ib_cm_event *event)\r\n{\r\nstruct srp_rdma_ch *ch = cm_id->context;\r\nstruct srp_target_port *target = ch->target;\r\nint comp = 0;\r\nswitch (event->event) {\r\ncase IB_CM_REQ_ERROR:\r\nshost_printk(KERN_DEBUG, target->scsi_host,\r\nPFX "Sending CM REQ failed\n");\r\ncomp = 1;\r\nch->status = -ECONNRESET;\r\nbreak;\r\ncase IB_CM_REP_RECEIVED:\r\ncomp = 1;\r\nsrp_cm_rep_handler(cm_id, event->private_data, ch);\r\nbreak;\r\ncase IB_CM_REJ_RECEIVED:\r\nshost_printk(KERN_DEBUG, target->scsi_host, PFX "REJ received\n");\r\ncomp = 1;\r\nsrp_cm_rej_handler(cm_id, event, ch);\r\nbreak;\r\ncase IB_CM_DREQ_RECEIVED:\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "DREQ received - connection closed\n");\r\nch->connected = false;\r\nif (ib_send_cm_drep(cm_id, NULL, 0))\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Sending CM DREP failed\n");\r\nqueue_work(system_long_wq, &target->tl_err_work);\r\nbreak;\r\ncase IB_CM_TIMEWAIT_EXIT:\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "connection closed\n");\r\ncomp = 1;\r\nch->status = 0;\r\nbreak;\r\ncase IB_CM_MRA_RECEIVED:\r\ncase IB_CM_DREQ_ERROR:\r\ncase IB_CM_DREP_RECEIVED:\r\nbreak;\r\ndefault:\r\nshost_printk(KERN_WARNING, target->scsi_host,\r\nPFX "Unhandled CM event %d\n", event->event);\r\nbreak;\r\n}\r\nif (comp)\r\ncomplete(&ch->done);\r\nreturn 0;\r\n}\r\nstatic int\r\nsrp_change_queue_depth(struct scsi_device *sdev, int qdepth)\r\n{\r\nif (!sdev->tagged_supported)\r\nqdepth = 1;\r\nreturn scsi_change_queue_depth(sdev, qdepth);\r\n}\r\nstatic int srp_send_tsk_mgmt(struct srp_rdma_ch *ch, u64 req_tag, u64 lun,\r\nu8 func)\r\n{\r\nstruct srp_target_port *target = ch->target;\r\nstruct srp_rport *rport = target->rport;\r\nstruct ib_device *dev = target->srp_host->srp_dev->dev;\r\nstruct srp_iu *iu;\r\nstruct srp_tsk_mgmt *tsk_mgmt;\r\nif (!ch->connected || target->qp_in_error)\r\nreturn -1;\r\ninit_completion(&ch->tsk_mgmt_done);\r\nmutex_lock(&rport->mutex);\r\nspin_lock_irq(&ch->lock);\r\niu = __srp_get_tx_iu(ch, SRP_IU_TSK_MGMT);\r\nspin_unlock_irq(&ch->lock);\r\nif (!iu) {\r\nmutex_unlock(&rport->mutex);\r\nreturn -1;\r\n}\r\nib_dma_sync_single_for_cpu(dev, iu->dma, sizeof *tsk_mgmt,\r\nDMA_TO_DEVICE);\r\ntsk_mgmt = iu->buf;\r\nmemset(tsk_mgmt, 0, sizeof *tsk_mgmt);\r\ntsk_mgmt->opcode = SRP_TSK_MGMT;\r\nint_to_scsilun(lun, &tsk_mgmt->lun);\r\ntsk_mgmt->tag = req_tag | SRP_TAG_TSK_MGMT;\r\ntsk_mgmt->tsk_mgmt_func = func;\r\ntsk_mgmt->task_tag = req_tag;\r\nib_dma_sync_single_for_device(dev, iu->dma, sizeof *tsk_mgmt,\r\nDMA_TO_DEVICE);\r\nif (srp_post_send(ch, iu, sizeof(*tsk_mgmt))) {\r\nsrp_put_tx_iu(ch, iu, SRP_IU_TSK_MGMT);\r\nmutex_unlock(&rport->mutex);\r\nreturn -1;\r\n}\r\nmutex_unlock(&rport->mutex);\r\nif (!wait_for_completion_timeout(&ch->tsk_mgmt_done,\r\nmsecs_to_jiffies(SRP_ABORT_TIMEOUT_MS)))\r\nreturn -1;\r\nreturn 0;\r\n}\r\nstatic int srp_abort(struct scsi_cmnd *scmnd)\r\n{\r\nstruct srp_target_port *target = host_to_target(scmnd->device->host);\r\nstruct srp_request *req = (struct srp_request *) scmnd->host_scribble;\r\nu32 tag;\r\nu16 ch_idx;\r\nstruct srp_rdma_ch *ch;\r\nint ret;\r\nshost_printk(KERN_ERR, target->scsi_host, "SRP abort called\n");\r\nif (!req)\r\nreturn SUCCESS;\r\ntag = blk_mq_unique_tag(scmnd->request);\r\nch_idx = blk_mq_unique_tag_to_hwq(tag);\r\nif (WARN_ON_ONCE(ch_idx >= target->ch_count))\r\nreturn SUCCESS;\r\nch = &target->ch[ch_idx];\r\nif (!srp_claim_req(ch, req, NULL, scmnd))\r\nreturn SUCCESS;\r\nshost_printk(KERN_ERR, target->scsi_host,\r\n"Sending SRP abort for tag %#x\n", tag);\r\nif (srp_send_tsk_mgmt(ch, tag, scmnd->device->lun,\r\nSRP_TSK_ABORT_TASK) == 0)\r\nret = SUCCESS;\r\nelse if (target->rport->state == SRP_RPORT_LOST)\r\nret = FAST_IO_FAIL;\r\nelse\r\nret = FAILED;\r\nsrp_free_req(ch, req, scmnd, 0);\r\nscmnd->result = DID_ABORT << 16;\r\nscmnd->scsi_done(scmnd);\r\nreturn ret;\r\n}\r\nstatic int srp_reset_device(struct scsi_cmnd *scmnd)\r\n{\r\nstruct srp_target_port *target = host_to_target(scmnd->device->host);\r\nstruct srp_rdma_ch *ch;\r\nint i;\r\nshost_printk(KERN_ERR, target->scsi_host, "SRP reset_device called\n");\r\nch = &target->ch[0];\r\nif (srp_send_tsk_mgmt(ch, SRP_TAG_NO_REQ, scmnd->device->lun,\r\nSRP_TSK_LUN_RESET))\r\nreturn FAILED;\r\nif (ch->tsk_mgmt_status)\r\nreturn FAILED;\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nfor (i = 0; i < target->req_ring_size; ++i) {\r\nstruct srp_request *req = &ch->req_ring[i];\r\nsrp_finish_req(ch, req, scmnd->device, DID_RESET << 16);\r\n}\r\n}\r\nreturn SUCCESS;\r\n}\r\nstatic int srp_reset_host(struct scsi_cmnd *scmnd)\r\n{\r\nstruct srp_target_port *target = host_to_target(scmnd->device->host);\r\nshost_printk(KERN_ERR, target->scsi_host, PFX "SRP reset_host called\n");\r\nreturn srp_reconnect_rport(target->rport) == 0 ? SUCCESS : FAILED;\r\n}\r\nstatic int srp_slave_alloc(struct scsi_device *sdev)\r\n{\r\nstruct Scsi_Host *shost = sdev->host;\r\nstruct srp_target_port *target = host_to_target(shost);\r\nstruct srp_device *srp_dev = target->srp_host->srp_dev;\r\nstruct ib_device *ibdev = srp_dev->dev;\r\nif (!(ibdev->attrs.device_cap_flags & IB_DEVICE_SG_GAPS_REG))\r\nblk_queue_virt_boundary(sdev->request_queue,\r\n~srp_dev->mr_page_mask);\r\nreturn 0;\r\n}\r\nstatic int srp_slave_configure(struct scsi_device *sdev)\r\n{\r\nstruct Scsi_Host *shost = sdev->host;\r\nstruct srp_target_port *target = host_to_target(shost);\r\nstruct request_queue *q = sdev->request_queue;\r\nunsigned long timeout;\r\nif (sdev->type == TYPE_DISK) {\r\ntimeout = max_t(unsigned, 30 * HZ, target->rq_tmo_jiffies);\r\nblk_queue_rq_timeout(q, timeout);\r\n}\r\nreturn 0;\r\n}\r\nstatic ssize_t show_id_ext(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "0x%016llx\n", be64_to_cpu(target->id_ext));\r\n}\r\nstatic ssize_t show_ioc_guid(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "0x%016llx\n", be64_to_cpu(target->ioc_guid));\r\n}\r\nstatic ssize_t show_service_id(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "0x%016llx\n", be64_to_cpu(target->service_id));\r\n}\r\nstatic ssize_t show_pkey(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "0x%04x\n", be16_to_cpu(target->pkey));\r\n}\r\nstatic ssize_t show_sgid(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%pI6\n", target->sgid.raw);\r\n}\r\nstatic ssize_t show_dgid(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nstruct srp_rdma_ch *ch = &target->ch[0];\r\nreturn sprintf(buf, "%pI6\n", ch->path.dgid.raw);\r\n}\r\nstatic ssize_t show_orig_dgid(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%pI6\n", target->orig_dgid.raw);\r\n}\r\nstatic ssize_t show_req_lim(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nstruct srp_rdma_ch *ch;\r\nint i, req_lim = INT_MAX;\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nreq_lim = min(req_lim, ch->req_lim);\r\n}\r\nreturn sprintf(buf, "%d\n", req_lim);\r\n}\r\nstatic ssize_t show_zero_req_lim(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->zero_req_lim);\r\n}\r\nstatic ssize_t show_local_ib_port(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->srp_host->port);\r\n}\r\nstatic ssize_t show_local_ib_device(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%s\n", target->srp_host->srp_dev->dev->name);\r\n}\r\nstatic ssize_t show_ch_count(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->ch_count);\r\n}\r\nstatic ssize_t show_comp_vector(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->comp_vector);\r\n}\r\nstatic ssize_t show_tl_retry_count(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%d\n", target->tl_retry_count);\r\n}\r\nstatic ssize_t show_cmd_sg_entries(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%u\n", target->cmd_sg_cnt);\r\n}\r\nstatic ssize_t show_allow_ext_sg(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct srp_target_port *target = host_to_target(class_to_shost(dev));\r\nreturn sprintf(buf, "%s\n", target->allow_ext_sg ? "true" : "false");\r\n}\r\nstatic int srp_sdev_count(struct Scsi_Host *host)\r\n{\r\nstruct scsi_device *sdev;\r\nint c = 0;\r\nshost_for_each_device(sdev, host)\r\nc++;\r\nreturn c;\r\n}\r\nstatic int srp_add_target(struct srp_host *host, struct srp_target_port *target)\r\n{\r\nstruct srp_rport_identifiers ids;\r\nstruct srp_rport *rport;\r\ntarget->state = SRP_TARGET_SCANNING;\r\nsprintf(target->target_name, "SRP.T10:%016llX",\r\nbe64_to_cpu(target->id_ext));\r\nif (scsi_add_host(target->scsi_host, host->srp_dev->dev->dma_device))\r\nreturn -ENODEV;\r\nmemcpy(ids.port_id, &target->id_ext, 8);\r\nmemcpy(ids.port_id + 8, &target->ioc_guid, 8);\r\nids.roles = SRP_RPORT_ROLE_TARGET;\r\nrport = srp_rport_add(target->scsi_host, &ids);\r\nif (IS_ERR(rport)) {\r\nscsi_remove_host(target->scsi_host);\r\nreturn PTR_ERR(rport);\r\n}\r\nrport->lld_data = target;\r\ntarget->rport = rport;\r\nspin_lock(&host->target_lock);\r\nlist_add_tail(&target->list, &host->target_list);\r\nspin_unlock(&host->target_lock);\r\nscsi_scan_target(&target->scsi_host->shost_gendev,\r\n0, target->scsi_id, SCAN_WILD_CARD, SCSI_SCAN_INITIAL);\r\nif (srp_connected_ch(target) < target->ch_count ||\r\ntarget->qp_in_error) {\r\nshost_printk(KERN_INFO, target->scsi_host,\r\nPFX "SCSI scan failed - removing SCSI host\n");\r\nsrp_queue_remove_work(target);\r\ngoto out;\r\n}\r\npr_debug("%s: SCSI scan succeeded - detected %d LUNs\n",\r\ndev_name(&target->scsi_host->shost_gendev),\r\nsrp_sdev_count(target->scsi_host));\r\nspin_lock_irq(&target->lock);\r\nif (target->state == SRP_TARGET_SCANNING)\r\ntarget->state = SRP_TARGET_LIVE;\r\nspin_unlock_irq(&target->lock);\r\nout:\r\nreturn 0;\r\n}\r\nstatic void srp_release_dev(struct device *dev)\r\n{\r\nstruct srp_host *host =\r\ncontainer_of(dev, struct srp_host, dev);\r\ncomplete(&host->released);\r\n}\r\nstatic bool srp_conn_unique(struct srp_host *host,\r\nstruct srp_target_port *target)\r\n{\r\nstruct srp_target_port *t;\r\nbool ret = false;\r\nif (target->state == SRP_TARGET_REMOVED)\r\ngoto out;\r\nret = true;\r\nspin_lock(&host->target_lock);\r\nlist_for_each_entry(t, &host->target_list, list) {\r\nif (t != target &&\r\ntarget->id_ext == t->id_ext &&\r\ntarget->ioc_guid == t->ioc_guid &&\r\ntarget->initiator_ext == t->initiator_ext) {\r\nret = false;\r\nbreak;\r\n}\r\n}\r\nspin_unlock(&host->target_lock);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int srp_parse_options(const char *buf, struct srp_target_port *target)\r\n{\r\nchar *options, *sep_opt;\r\nchar *p;\r\nchar dgid[3];\r\nsubstring_t args[MAX_OPT_ARGS];\r\nint opt_mask = 0;\r\nint token;\r\nint ret = -EINVAL;\r\nint i;\r\noptions = kstrdup(buf, GFP_KERNEL);\r\nif (!options)\r\nreturn -ENOMEM;\r\nsep_opt = options;\r\nwhile ((p = strsep(&sep_opt, ",\n")) != NULL) {\r\nif (!*p)\r\ncontinue;\r\ntoken = match_token(p, srp_opt_tokens, args);\r\nopt_mask |= token;\r\nswitch (token) {\r\ncase SRP_OPT_ID_EXT:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ntarget->id_ext = cpu_to_be64(simple_strtoull(p, NULL, 16));\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_IOC_GUID:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ntarget->ioc_guid = cpu_to_be64(simple_strtoull(p, NULL, 16));\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_DGID:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nif (strlen(p) != 32) {\r\npr_warn("bad dest GID parameter '%s'\n", p);\r\nkfree(p);\r\ngoto out;\r\n}\r\nfor (i = 0; i < 16; ++i) {\r\nstrlcpy(dgid, p + i * 2, sizeof(dgid));\r\nif (sscanf(dgid, "%hhx",\r\n&target->orig_dgid.raw[i]) < 1) {\r\nret = -EINVAL;\r\nkfree(p);\r\ngoto out;\r\n}\r\n}\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_PKEY:\r\nif (match_hex(args, &token)) {\r\npr_warn("bad P_Key parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->pkey = cpu_to_be16(token);\r\nbreak;\r\ncase SRP_OPT_SERVICE_ID:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ntarget->service_id = cpu_to_be64(simple_strtoull(p, NULL, 16));\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_MAX_SECT:\r\nif (match_int(args, &token)) {\r\npr_warn("bad max sect parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->scsi_host->max_sectors = token;\r\nbreak;\r\ncase SRP_OPT_QUEUE_SIZE:\r\nif (match_int(args, &token) || token < 1) {\r\npr_warn("bad queue_size parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->scsi_host->can_queue = token;\r\ntarget->queue_size = token + SRP_RSP_SQ_SIZE +\r\nSRP_TSK_MGMT_SQ_SIZE;\r\nif (!(opt_mask & SRP_OPT_MAX_CMD_PER_LUN))\r\ntarget->scsi_host->cmd_per_lun = token;\r\nbreak;\r\ncase SRP_OPT_MAX_CMD_PER_LUN:\r\nif (match_int(args, &token) || token < 1) {\r\npr_warn("bad max cmd_per_lun parameter '%s'\n",\r\np);\r\ngoto out;\r\n}\r\ntarget->scsi_host->cmd_per_lun = token;\r\nbreak;\r\ncase SRP_OPT_IO_CLASS:\r\nif (match_hex(args, &token)) {\r\npr_warn("bad IO class parameter '%s'\n", p);\r\ngoto out;\r\n}\r\nif (token != SRP_REV10_IB_IO_CLASS &&\r\ntoken != SRP_REV16A_IB_IO_CLASS) {\r\npr_warn("unknown IO class parameter value %x specified (use %x or %x).\n",\r\ntoken, SRP_REV10_IB_IO_CLASS,\r\nSRP_REV16A_IB_IO_CLASS);\r\ngoto out;\r\n}\r\ntarget->io_class = token;\r\nbreak;\r\ncase SRP_OPT_INITIATOR_EXT:\r\np = match_strdup(args);\r\nif (!p) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\ntarget->initiator_ext = cpu_to_be64(simple_strtoull(p, NULL, 16));\r\nkfree(p);\r\nbreak;\r\ncase SRP_OPT_CMD_SG_ENTRIES:\r\nif (match_int(args, &token) || token < 1 || token > 255) {\r\npr_warn("bad max cmd_sg_entries parameter '%s'\n",\r\np);\r\ngoto out;\r\n}\r\ntarget->cmd_sg_cnt = token;\r\nbreak;\r\ncase SRP_OPT_ALLOW_EXT_SG:\r\nif (match_int(args, &token)) {\r\npr_warn("bad allow_ext_sg parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->allow_ext_sg = !!token;\r\nbreak;\r\ncase SRP_OPT_SG_TABLESIZE:\r\nif (match_int(args, &token) || token < 1 ||\r\ntoken > SG_MAX_SEGMENTS) {\r\npr_warn("bad max sg_tablesize parameter '%s'\n",\r\np);\r\ngoto out;\r\n}\r\ntarget->sg_tablesize = token;\r\nbreak;\r\ncase SRP_OPT_COMP_VECTOR:\r\nif (match_int(args, &token) || token < 0) {\r\npr_warn("bad comp_vector parameter '%s'\n", p);\r\ngoto out;\r\n}\r\ntarget->comp_vector = token;\r\nbreak;\r\ncase SRP_OPT_TL_RETRY_COUNT:\r\nif (match_int(args, &token) || token < 2 || token > 7) {\r\npr_warn("bad tl_retry_count parameter '%s' (must be a number between 2 and 7)\n",\r\np);\r\ngoto out;\r\n}\r\ntarget->tl_retry_count = token;\r\nbreak;\r\ndefault:\r\npr_warn("unknown parameter or missing value '%s' in target creation request\n",\r\np);\r\ngoto out;\r\n}\r\n}\r\nif ((opt_mask & SRP_OPT_ALL) == SRP_OPT_ALL)\r\nret = 0;\r\nelse\r\nfor (i = 0; i < ARRAY_SIZE(srp_opt_tokens); ++i)\r\nif ((srp_opt_tokens[i].token & SRP_OPT_ALL) &&\r\n!(srp_opt_tokens[i].token & opt_mask))\r\npr_warn("target creation request is missing parameter '%s'\n",\r\nsrp_opt_tokens[i].pattern);\r\nif (target->scsi_host->cmd_per_lun > target->scsi_host->can_queue\r\n&& (opt_mask & SRP_OPT_MAX_CMD_PER_LUN))\r\npr_warn("cmd_per_lun = %d > queue_size = %d\n",\r\ntarget->scsi_host->cmd_per_lun,\r\ntarget->scsi_host->can_queue);\r\nout:\r\nkfree(options);\r\nreturn ret;\r\n}\r\nstatic ssize_t srp_create_target(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nstruct srp_host *host =\r\ncontainer_of(dev, struct srp_host, dev);\r\nstruct Scsi_Host *target_host;\r\nstruct srp_target_port *target;\r\nstruct srp_rdma_ch *ch;\r\nstruct srp_device *srp_dev = host->srp_dev;\r\nstruct ib_device *ibdev = srp_dev->dev;\r\nint ret, node_idx, node, cpu, i;\r\nunsigned int max_sectors_per_mr, mr_per_cmd = 0;\r\nbool multich = false;\r\ntarget_host = scsi_host_alloc(&srp_template,\r\nsizeof (struct srp_target_port));\r\nif (!target_host)\r\nreturn -ENOMEM;\r\ntarget_host->transportt = ib_srp_transport_template;\r\ntarget_host->max_channel = 0;\r\ntarget_host->max_id = 1;\r\ntarget_host->max_lun = -1LL;\r\ntarget_host->max_cmd_len = sizeof ((struct srp_cmd *) (void *) 0L)->cdb;\r\ntarget = host_to_target(target_host);\r\ntarget->io_class = SRP_REV16A_IB_IO_CLASS;\r\ntarget->scsi_host = target_host;\r\ntarget->srp_host = host;\r\ntarget->lkey = host->srp_dev->pd->local_dma_lkey;\r\ntarget->global_mr = host->srp_dev->global_mr;\r\ntarget->cmd_sg_cnt = cmd_sg_entries;\r\ntarget->sg_tablesize = indirect_sg_entries ? : cmd_sg_entries;\r\ntarget->allow_ext_sg = allow_ext_sg;\r\ntarget->tl_retry_count = 7;\r\ntarget->queue_size = SRP_DEFAULT_QUEUE_SIZE;\r\nscsi_host_get(target->scsi_host);\r\nmutex_lock(&host->add_target_mutex);\r\nret = srp_parse_options(buf, target);\r\nif (ret)\r\ngoto out;\r\ntarget->req_ring_size = target->queue_size - SRP_TSK_MGMT_SQ_SIZE;\r\nif (!srp_conn_unique(target->srp_host, target)) {\r\nshost_printk(KERN_INFO, target->scsi_host,\r\nPFX "Already connected to target port with id_ext=%016llx;ioc_guid=%016llx;initiator_ext=%016llx\n",\r\nbe64_to_cpu(target->id_ext),\r\nbe64_to_cpu(target->ioc_guid),\r\nbe64_to_cpu(target->initiator_ext));\r\nret = -EEXIST;\r\ngoto out;\r\n}\r\nif (!srp_dev->has_fmr && !srp_dev->has_fr && !target->allow_ext_sg &&\r\ntarget->cmd_sg_cnt < target->sg_tablesize) {\r\npr_warn("No MR pool and no external indirect descriptors, limiting sg_tablesize to cmd_sg_cnt\n");\r\ntarget->sg_tablesize = target->cmd_sg_cnt;\r\n}\r\nif (srp_dev->use_fast_reg || srp_dev->use_fmr) {\r\nmax_sectors_per_mr = srp_dev->max_pages_per_mr <<\r\n(ilog2(srp_dev->mr_page_size) - 9);\r\nmr_per_cmd = register_always +\r\n(target->scsi_host->max_sectors + 1 +\r\nmax_sectors_per_mr - 1) / max_sectors_per_mr;\r\npr_debug("max_sectors = %u; max_pages_per_mr = %u; mr_page_size = %u; max_sectors_per_mr = %u; mr_per_cmd = %u\n",\r\ntarget->scsi_host->max_sectors,\r\nsrp_dev->max_pages_per_mr, srp_dev->mr_page_size,\r\nmax_sectors_per_mr, mr_per_cmd);\r\n}\r\ntarget_host->sg_tablesize = target->sg_tablesize;\r\ntarget->mr_pool_size = target->scsi_host->can_queue * mr_per_cmd;\r\ntarget->mr_per_cmd = mr_per_cmd;\r\ntarget->indirect_size = target->sg_tablesize *\r\nsizeof (struct srp_direct_buf);\r\ntarget->max_iu_len = sizeof (struct srp_cmd) +\r\nsizeof (struct srp_indirect_buf) +\r\ntarget->cmd_sg_cnt * sizeof (struct srp_direct_buf);\r\nINIT_WORK(&target->tl_err_work, srp_tl_err_work);\r\nINIT_WORK(&target->remove_work, srp_remove_work);\r\nspin_lock_init(&target->lock);\r\nret = ib_query_gid(ibdev, host->port, 0, &target->sgid, NULL);\r\nif (ret)\r\ngoto out;\r\nret = -ENOMEM;\r\ntarget->ch_count = max_t(unsigned, num_online_nodes(),\r\nmin(ch_count ? :\r\nmin(4 * num_online_nodes(),\r\nibdev->num_comp_vectors),\r\nnum_online_cpus()));\r\ntarget->ch = kcalloc(target->ch_count, sizeof(*target->ch),\r\nGFP_KERNEL);\r\nif (!target->ch)\r\ngoto out;\r\nnode_idx = 0;\r\nfor_each_online_node(node) {\r\nconst int ch_start = (node_idx * target->ch_count /\r\nnum_online_nodes());\r\nconst int ch_end = ((node_idx + 1) * target->ch_count /\r\nnum_online_nodes());\r\nconst int cv_start = (node_idx * ibdev->num_comp_vectors /\r\nnum_online_nodes() + target->comp_vector)\r\n% ibdev->num_comp_vectors;\r\nconst int cv_end = ((node_idx + 1) * ibdev->num_comp_vectors /\r\nnum_online_nodes() + target->comp_vector)\r\n% ibdev->num_comp_vectors;\r\nint cpu_idx = 0;\r\nfor_each_online_cpu(cpu) {\r\nif (cpu_to_node(cpu) != node)\r\ncontinue;\r\nif (ch_start + cpu_idx >= ch_end)\r\ncontinue;\r\nch = &target->ch[ch_start + cpu_idx];\r\nch->target = target;\r\nch->comp_vector = cv_start == cv_end ? cv_start :\r\ncv_start + cpu_idx % (cv_end - cv_start);\r\nspin_lock_init(&ch->lock);\r\nINIT_LIST_HEAD(&ch->free_tx);\r\nret = srp_new_cm_id(ch);\r\nif (ret)\r\ngoto err_disconnect;\r\nret = srp_create_ch_ib(ch);\r\nif (ret)\r\ngoto err_disconnect;\r\nret = srp_alloc_req_data(ch);\r\nif (ret)\r\ngoto err_disconnect;\r\nret = srp_connect_ch(ch, multich);\r\nif (ret) {\r\nshost_printk(KERN_ERR, target->scsi_host,\r\nPFX "Connection %d/%d failed\n",\r\nch_start + cpu_idx,\r\ntarget->ch_count);\r\nif (node_idx == 0 && cpu_idx == 0) {\r\ngoto err_disconnect;\r\n} else {\r\nsrp_free_ch_ib(target, ch);\r\nsrp_free_req_data(target, ch);\r\ntarget->ch_count = ch - target->ch;\r\ngoto connected;\r\n}\r\n}\r\nmultich = true;\r\ncpu_idx++;\r\n}\r\nnode_idx++;\r\n}\r\nconnected:\r\ntarget->scsi_host->nr_hw_queues = target->ch_count;\r\nret = srp_add_target(host, target);\r\nif (ret)\r\ngoto err_disconnect;\r\nif (target->state != SRP_TARGET_REMOVED) {\r\nshost_printk(KERN_DEBUG, target->scsi_host, PFX\r\n"new target: id_ext %016llx ioc_guid %016llx pkey %04x service_id %016llx sgid %pI6 dgid %pI6\n",\r\nbe64_to_cpu(target->id_ext),\r\nbe64_to_cpu(target->ioc_guid),\r\nbe16_to_cpu(target->pkey),\r\nbe64_to_cpu(target->service_id),\r\ntarget->sgid.raw, target->orig_dgid.raw);\r\n}\r\nret = count;\r\nout:\r\nmutex_unlock(&host->add_target_mutex);\r\nscsi_host_put(target->scsi_host);\r\nif (ret < 0)\r\nscsi_host_put(target->scsi_host);\r\nreturn ret;\r\nerr_disconnect:\r\nsrp_disconnect_target(target);\r\nfor (i = 0; i < target->ch_count; i++) {\r\nch = &target->ch[i];\r\nsrp_free_ch_ib(target, ch);\r\nsrp_free_req_data(target, ch);\r\n}\r\nkfree(target->ch);\r\ngoto out;\r\n}\r\nstatic ssize_t show_ibdev(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_host *host = container_of(dev, struct srp_host, dev);\r\nreturn sprintf(buf, "%s\n", host->srp_dev->dev->name);\r\n}\r\nstatic ssize_t show_port(struct device *dev, struct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct srp_host *host = container_of(dev, struct srp_host, dev);\r\nreturn sprintf(buf, "%d\n", host->port);\r\n}\r\nstatic struct srp_host *srp_add_port(struct srp_device *device, u8 port)\r\n{\r\nstruct srp_host *host;\r\nhost = kzalloc(sizeof *host, GFP_KERNEL);\r\nif (!host)\r\nreturn NULL;\r\nINIT_LIST_HEAD(&host->target_list);\r\nspin_lock_init(&host->target_lock);\r\ninit_completion(&host->released);\r\nmutex_init(&host->add_target_mutex);\r\nhost->srp_dev = device;\r\nhost->port = port;\r\nhost->dev.class = &srp_class;\r\nhost->dev.parent = device->dev->dma_device;\r\ndev_set_name(&host->dev, "srp-%s-%d", device->dev->name, port);\r\nif (device_register(&host->dev))\r\ngoto free_host;\r\nif (device_create_file(&host->dev, &dev_attr_add_target))\r\ngoto err_class;\r\nif (device_create_file(&host->dev, &dev_attr_ibdev))\r\ngoto err_class;\r\nif (device_create_file(&host->dev, &dev_attr_port))\r\ngoto err_class;\r\nreturn host;\r\nerr_class:\r\ndevice_unregister(&host->dev);\r\nfree_host:\r\nkfree(host);\r\nreturn NULL;\r\n}\r\nstatic void srp_add_one(struct ib_device *device)\r\n{\r\nstruct srp_device *srp_dev;\r\nstruct srp_host *host;\r\nint mr_page_shift, p;\r\nu64 max_pages_per_mr;\r\nsrp_dev = kzalloc(sizeof(*srp_dev), GFP_KERNEL);\r\nif (!srp_dev)\r\nreturn;\r\nmr_page_shift = max(12, ffs(device->attrs.page_size_cap) - 1);\r\nsrp_dev->mr_page_size = 1 << mr_page_shift;\r\nsrp_dev->mr_page_mask = ~((u64) srp_dev->mr_page_size - 1);\r\nmax_pages_per_mr = device->attrs.max_mr_size;\r\ndo_div(max_pages_per_mr, srp_dev->mr_page_size);\r\npr_debug("%s: %llu / %u = %llu <> %u\n", __func__,\r\ndevice->attrs.max_mr_size, srp_dev->mr_page_size,\r\nmax_pages_per_mr, SRP_MAX_PAGES_PER_MR);\r\nsrp_dev->max_pages_per_mr = min_t(u64, SRP_MAX_PAGES_PER_MR,\r\nmax_pages_per_mr);\r\nsrp_dev->has_fmr = (device->alloc_fmr && device->dealloc_fmr &&\r\ndevice->map_phys_fmr && device->unmap_fmr);\r\nsrp_dev->has_fr = (device->attrs.device_cap_flags &\r\nIB_DEVICE_MEM_MGT_EXTENSIONS);\r\nif (!never_register && !srp_dev->has_fmr && !srp_dev->has_fr) {\r\ndev_warn(&device->dev, "neither FMR nor FR is supported\n");\r\n} else if (!never_register &&\r\ndevice->attrs.max_mr_size >= 2 * srp_dev->mr_page_size) {\r\nsrp_dev->use_fast_reg = (srp_dev->has_fr &&\r\n(!srp_dev->has_fmr || prefer_fr));\r\nsrp_dev->use_fmr = !srp_dev->use_fast_reg && srp_dev->has_fmr;\r\n}\r\nif (srp_dev->use_fast_reg) {\r\nsrp_dev->max_pages_per_mr =\r\nmin_t(u32, srp_dev->max_pages_per_mr,\r\ndevice->attrs.max_fast_reg_page_list_len);\r\n}\r\nsrp_dev->mr_max_size = srp_dev->mr_page_size *\r\nsrp_dev->max_pages_per_mr;\r\npr_debug("%s: mr_page_shift = %d, device->max_mr_size = %#llx, device->max_fast_reg_page_list_len = %u, max_pages_per_mr = %d, mr_max_size = %#x\n",\r\ndevice->name, mr_page_shift, device->attrs.max_mr_size,\r\ndevice->attrs.max_fast_reg_page_list_len,\r\nsrp_dev->max_pages_per_mr, srp_dev->mr_max_size);\r\nINIT_LIST_HEAD(&srp_dev->dev_list);\r\nsrp_dev->dev = device;\r\nsrp_dev->pd = ib_alloc_pd(device);\r\nif (IS_ERR(srp_dev->pd))\r\ngoto free_dev;\r\nif (never_register || !register_always ||\r\n(!srp_dev->has_fmr && !srp_dev->has_fr)) {\r\nsrp_dev->global_mr = ib_get_dma_mr(srp_dev->pd,\r\nIB_ACCESS_LOCAL_WRITE |\r\nIB_ACCESS_REMOTE_READ |\r\nIB_ACCESS_REMOTE_WRITE);\r\nif (IS_ERR(srp_dev->global_mr))\r\ngoto err_pd;\r\n}\r\nfor (p = rdma_start_port(device); p <= rdma_end_port(device); ++p) {\r\nhost = srp_add_port(srp_dev, p);\r\nif (host)\r\nlist_add_tail(&host->list, &srp_dev->dev_list);\r\n}\r\nib_set_client_data(device, &srp_client, srp_dev);\r\nreturn;\r\nerr_pd:\r\nib_dealloc_pd(srp_dev->pd);\r\nfree_dev:\r\nkfree(srp_dev);\r\n}\r\nstatic void srp_remove_one(struct ib_device *device, void *client_data)\r\n{\r\nstruct srp_device *srp_dev;\r\nstruct srp_host *host, *tmp_host;\r\nstruct srp_target_port *target;\r\nsrp_dev = client_data;\r\nif (!srp_dev)\r\nreturn;\r\nlist_for_each_entry_safe(host, tmp_host, &srp_dev->dev_list, list) {\r\ndevice_unregister(&host->dev);\r\nwait_for_completion(&host->released);\r\nspin_lock(&host->target_lock);\r\nlist_for_each_entry(target, &host->target_list, list)\r\nsrp_queue_remove_work(target);\r\nspin_unlock(&host->target_lock);\r\nflush_workqueue(system_long_wq);\r\nflush_workqueue(srp_remove_wq);\r\nkfree(host);\r\n}\r\nif (srp_dev->global_mr)\r\nib_dereg_mr(srp_dev->global_mr);\r\nib_dealloc_pd(srp_dev->pd);\r\nkfree(srp_dev);\r\n}\r\nstatic int __init srp_init_module(void)\r\n{\r\nint ret;\r\nif (srp_sg_tablesize) {\r\npr_warn("srp_sg_tablesize is deprecated, please use cmd_sg_entries\n");\r\nif (!cmd_sg_entries)\r\ncmd_sg_entries = srp_sg_tablesize;\r\n}\r\nif (!cmd_sg_entries)\r\ncmd_sg_entries = SRP_DEF_SG_TABLESIZE;\r\nif (cmd_sg_entries > 255) {\r\npr_warn("Clamping cmd_sg_entries to 255\n");\r\ncmd_sg_entries = 255;\r\n}\r\nif (!indirect_sg_entries)\r\nindirect_sg_entries = cmd_sg_entries;\r\nelse if (indirect_sg_entries < cmd_sg_entries) {\r\npr_warn("Bumping up indirect_sg_entries to match cmd_sg_entries (%u)\n",\r\ncmd_sg_entries);\r\nindirect_sg_entries = cmd_sg_entries;\r\n}\r\nsrp_remove_wq = create_workqueue("srp_remove");\r\nif (!srp_remove_wq) {\r\nret = -ENOMEM;\r\ngoto out;\r\n}\r\nret = -ENOMEM;\r\nib_srp_transport_template =\r\nsrp_attach_transport(&ib_srp_transport_functions);\r\nif (!ib_srp_transport_template)\r\ngoto destroy_wq;\r\nret = class_register(&srp_class);\r\nif (ret) {\r\npr_err("couldn't register class infiniband_srp\n");\r\ngoto release_tr;\r\n}\r\nib_sa_register_client(&srp_sa_client);\r\nret = ib_register_client(&srp_client);\r\nif (ret) {\r\npr_err("couldn't register IB client\n");\r\ngoto unreg_sa;\r\n}\r\nout:\r\nreturn ret;\r\nunreg_sa:\r\nib_sa_unregister_client(&srp_sa_client);\r\nclass_unregister(&srp_class);\r\nrelease_tr:\r\nsrp_release_transport(ib_srp_transport_template);\r\ndestroy_wq:\r\ndestroy_workqueue(srp_remove_wq);\r\ngoto out;\r\n}\r\nstatic void __exit srp_cleanup_module(void)\r\n{\r\nib_unregister_client(&srp_client);\r\nib_sa_unregister_client(&srp_sa_client);\r\nclass_unregister(&srp_class);\r\nsrp_release_transport(ib_srp_transport_template);\r\ndestroy_workqueue(srp_remove_wq);\r\n}
