void ivtv_udma_get_page_info(struct ivtv_dma_page_info *dma_page, unsigned long first, unsigned long size)\r\n{\r\ndma_page->uaddr = first & PAGE_MASK;\r\ndma_page->offset = first & ~PAGE_MASK;\r\ndma_page->tail = 1 + ((first+size-1) & ~PAGE_MASK);\r\ndma_page->first = (first & PAGE_MASK) >> PAGE_SHIFT;\r\ndma_page->last = ((first+size-1) & PAGE_MASK) >> PAGE_SHIFT;\r\ndma_page->page_count = dma_page->last - dma_page->first + 1;\r\nif (dma_page->page_count == 1) dma_page->tail -= dma_page->offset;\r\n}\r\nint ivtv_udma_fill_sg_list (struct ivtv_user_dma *dma, struct ivtv_dma_page_info *dma_page, int map_offset)\r\n{\r\nint i, offset;\r\nunsigned long flags;\r\nif (map_offset < 0)\r\nreturn map_offset;\r\noffset = dma_page->offset;\r\nfor (i = 0; i < dma_page->page_count; i++) {\r\nunsigned int len = (i == dma_page->page_count - 1) ?\r\ndma_page->tail : PAGE_SIZE - offset;\r\nif (PageHighMem(dma->map[map_offset])) {\r\nvoid *src;\r\nif (dma->bouncemap[map_offset] == NULL)\r\ndma->bouncemap[map_offset] = alloc_page(GFP_KERNEL);\r\nif (dma->bouncemap[map_offset] == NULL)\r\nreturn -1;\r\nlocal_irq_save(flags);\r\nsrc = kmap_atomic(dma->map[map_offset]) + offset;\r\nmemcpy(page_address(dma->bouncemap[map_offset]) + offset, src, len);\r\nkunmap_atomic(src);\r\nlocal_irq_restore(flags);\r\nsg_set_page(&dma->SGlist[map_offset], dma->bouncemap[map_offset], len, offset);\r\n}\r\nelse {\r\nsg_set_page(&dma->SGlist[map_offset], dma->map[map_offset], len, offset);\r\n}\r\noffset = 0;\r\nmap_offset++;\r\n}\r\nreturn map_offset;\r\n}\r\nvoid ivtv_udma_fill_sg_array (struct ivtv_user_dma *dma, u32 buffer_offset, u32 buffer_offset_2, u32 split) {\r\nint i;\r\nstruct scatterlist *sg;\r\nfor (i = 0, sg = dma->SGlist; i < dma->SG_length; i++, sg = sg_next(sg)) {\r\ndma->SGarray[i].size = cpu_to_le32(sg_dma_len(sg));\r\ndma->SGarray[i].src = cpu_to_le32(sg_dma_address(sg));\r\ndma->SGarray[i].dst = cpu_to_le32(buffer_offset);\r\nbuffer_offset += sg_dma_len(sg);\r\nsplit -= sg_dma_len(sg);\r\nif (split == 0)\r\nbuffer_offset = buffer_offset_2;\r\n}\r\n}\r\nvoid ivtv_udma_alloc(struct ivtv *itv)\r\n{\r\nif (itv->udma.SG_handle == 0) {\r\nitv->udma.SG_handle = pci_map_single(itv->pdev, itv->udma.SGarray,\r\nsizeof(itv->udma.SGarray), PCI_DMA_TODEVICE);\r\nivtv_udma_sync_for_cpu(itv);\r\n}\r\n}\r\nint ivtv_udma_setup(struct ivtv *itv, unsigned long ivtv_dest_addr,\r\nvoid __user *userbuf, int size_in_bytes)\r\n{\r\nstruct ivtv_dma_page_info user_dma;\r\nstruct ivtv_user_dma *dma = &itv->udma;\r\nint i, err;\r\nIVTV_DEBUG_DMA("ivtv_udma_setup, dst: 0x%08x\n", (unsigned int)ivtv_dest_addr);\r\nif (dma->SG_length || dma->page_count) {\r\nIVTV_DEBUG_WARN("ivtv_udma_setup: SG_length %d page_count %d still full?\n",\r\ndma->SG_length, dma->page_count);\r\nreturn -EBUSY;\r\n}\r\nivtv_udma_get_page_info(&user_dma, (unsigned long)userbuf, size_in_bytes);\r\nif (user_dma.page_count <= 0) {\r\nIVTV_DEBUG_WARN("ivtv_udma_setup: Error %d page_count from %d bytes %d offset\n",\r\nuser_dma.page_count, size_in_bytes, user_dma.offset);\r\nreturn -EINVAL;\r\n}\r\nerr = get_user_pages_unlocked(user_dma.uaddr, user_dma.page_count, 0,\r\n1, dma->map);\r\nif (user_dma.page_count != err) {\r\nIVTV_DEBUG_WARN("failed to map user pages, returned %d instead of %d\n",\r\nerr, user_dma.page_count);\r\nif (err >= 0) {\r\nfor (i = 0; i < err; i++)\r\nput_page(dma->map[i]);\r\nreturn -EINVAL;\r\n}\r\nreturn err;\r\n}\r\ndma->page_count = user_dma.page_count;\r\nif (ivtv_udma_fill_sg_list(dma, &user_dma, 0) < 0) {\r\nfor (i = 0; i < dma->page_count; i++) {\r\nput_page(dma->map[i]);\r\n}\r\ndma->page_count = 0;\r\nreturn -ENOMEM;\r\n}\r\ndma->SG_length = pci_map_sg(itv->pdev, dma->SGlist, dma->page_count, PCI_DMA_TODEVICE);\r\nivtv_udma_fill_sg_array (dma, ivtv_dest_addr, 0, -1);\r\ndma->SGarray[dma->SG_length - 1].size |= cpu_to_le32(0x80000000);\r\nivtv_udma_sync_for_device(itv);\r\nreturn dma->page_count;\r\n}\r\nvoid ivtv_udma_unmap(struct ivtv *itv)\r\n{\r\nstruct ivtv_user_dma *dma = &itv->udma;\r\nint i;\r\nIVTV_DEBUG_INFO("ivtv_unmap_user_dma\n");\r\nif (dma->page_count == 0)\r\nreturn;\r\nif (dma->SG_length) {\r\npci_unmap_sg(itv->pdev, dma->SGlist, dma->page_count, PCI_DMA_TODEVICE);\r\ndma->SG_length = 0;\r\n}\r\nivtv_udma_sync_for_cpu(itv);\r\nfor (i = 0; i < dma->page_count; i++) {\r\nput_page(dma->map[i]);\r\n}\r\ndma->page_count = 0;\r\n}\r\nvoid ivtv_udma_free(struct ivtv *itv)\r\n{\r\nint i;\r\nif (itv->udma.SG_handle) {\r\npci_unmap_single(itv->pdev, itv->udma.SG_handle,\r\nsizeof(itv->udma.SGarray), PCI_DMA_TODEVICE);\r\n}\r\nif (itv->udma.SG_length) {\r\npci_unmap_sg(itv->pdev, itv->udma.SGlist, itv->udma.page_count, PCI_DMA_TODEVICE);\r\n}\r\nfor (i = 0; i < IVTV_DMA_SG_OSD_ENT; i++) {\r\nif (itv->udma.bouncemap[i])\r\n__free_page(itv->udma.bouncemap[i]);\r\n}\r\n}\r\nvoid ivtv_udma_start(struct ivtv *itv)\r\n{\r\nIVTV_DEBUG_DMA("start UDMA\n");\r\nwrite_reg(itv->udma.SG_handle, IVTV_REG_DECDMAADDR);\r\nwrite_reg_sync(read_reg(IVTV_REG_DMAXFER) | 0x01, IVTV_REG_DMAXFER);\r\nset_bit(IVTV_F_I_DMA, &itv->i_flags);\r\nset_bit(IVTV_F_I_UDMA, &itv->i_flags);\r\nclear_bit(IVTV_F_I_UDMA_PENDING, &itv->i_flags);\r\n}\r\nvoid ivtv_udma_prepare(struct ivtv *itv)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&itv->dma_reg_lock, flags);\r\nif (!test_bit(IVTV_F_I_DMA, &itv->i_flags))\r\nivtv_udma_start(itv);\r\nelse\r\nset_bit(IVTV_F_I_UDMA_PENDING, &itv->i_flags);\r\nspin_unlock_irqrestore(&itv->dma_reg_lock, flags);\r\n}
