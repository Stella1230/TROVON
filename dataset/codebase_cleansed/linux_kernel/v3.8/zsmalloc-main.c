static int is_first_page(struct page *page)\r\n{\r\nreturn PagePrivate(page);\r\n}\r\nstatic int is_last_page(struct page *page)\r\n{\r\nreturn PagePrivate2(page);\r\n}\r\nstatic void get_zspage_mapping(struct page *page, unsigned int *class_idx,\r\nenum fullness_group *fullness)\r\n{\r\nunsigned long m;\r\nBUG_ON(!is_first_page(page));\r\nm = (unsigned long)page->mapping;\r\n*fullness = m & FULLNESS_MASK;\r\n*class_idx = (m >> FULLNESS_BITS) & CLASS_IDX_MASK;\r\n}\r\nstatic void set_zspage_mapping(struct page *page, unsigned int class_idx,\r\nenum fullness_group fullness)\r\n{\r\nunsigned long m;\r\nBUG_ON(!is_first_page(page));\r\nm = ((class_idx & CLASS_IDX_MASK) << FULLNESS_BITS) |\r\n(fullness & FULLNESS_MASK);\r\npage->mapping = (struct address_space *)m;\r\n}\r\nstatic int get_size_class_index(int size)\r\n{\r\nint idx = 0;\r\nif (likely(size > ZS_MIN_ALLOC_SIZE))\r\nidx = DIV_ROUND_UP(size - ZS_MIN_ALLOC_SIZE,\r\nZS_SIZE_CLASS_DELTA);\r\nreturn idx;\r\n}\r\nstatic enum fullness_group get_fullness_group(struct page *page)\r\n{\r\nint inuse, max_objects;\r\nenum fullness_group fg;\r\nBUG_ON(!is_first_page(page));\r\ninuse = page->inuse;\r\nmax_objects = page->objects;\r\nif (inuse == 0)\r\nfg = ZS_EMPTY;\r\nelse if (inuse == max_objects)\r\nfg = ZS_FULL;\r\nelse if (inuse <= max_objects / fullness_threshold_frac)\r\nfg = ZS_ALMOST_EMPTY;\r\nelse\r\nfg = ZS_ALMOST_FULL;\r\nreturn fg;\r\n}\r\nstatic void insert_zspage(struct page *page, struct size_class *class,\r\nenum fullness_group fullness)\r\n{\r\nstruct page **head;\r\nBUG_ON(!is_first_page(page));\r\nif (fullness >= _ZS_NR_FULLNESS_GROUPS)\r\nreturn;\r\nhead = &class->fullness_list[fullness];\r\nif (*head)\r\nlist_add_tail(&page->lru, &(*head)->lru);\r\n*head = page;\r\n}\r\nstatic void remove_zspage(struct page *page, struct size_class *class,\r\nenum fullness_group fullness)\r\n{\r\nstruct page **head;\r\nBUG_ON(!is_first_page(page));\r\nif (fullness >= _ZS_NR_FULLNESS_GROUPS)\r\nreturn;\r\nhead = &class->fullness_list[fullness];\r\nBUG_ON(!*head);\r\nif (list_empty(&(*head)->lru))\r\n*head = NULL;\r\nelse if (*head == page)\r\n*head = (struct page *)list_entry((*head)->lru.next,\r\nstruct page, lru);\r\nlist_del_init(&page->lru);\r\n}\r\nstatic enum fullness_group fix_fullness_group(struct zs_pool *pool,\r\nstruct page *page)\r\n{\r\nint class_idx;\r\nstruct size_class *class;\r\nenum fullness_group currfg, newfg;\r\nBUG_ON(!is_first_page(page));\r\nget_zspage_mapping(page, &class_idx, &currfg);\r\nnewfg = get_fullness_group(page);\r\nif (newfg == currfg)\r\ngoto out;\r\nclass = &pool->size_class[class_idx];\r\nremove_zspage(page, class, currfg);\r\ninsert_zspage(page, class, newfg);\r\nset_zspage_mapping(page, class_idx, newfg);\r\nout:\r\nreturn newfg;\r\n}\r\nstatic int get_pages_per_zspage(int class_size)\r\n{\r\nint i, max_usedpc = 0;\r\nint max_usedpc_order = 1;\r\nfor (i = 1; i <= ZS_MAX_PAGES_PER_ZSPAGE; i++) {\r\nint zspage_size;\r\nint waste, usedpc;\r\nzspage_size = i * PAGE_SIZE;\r\nwaste = zspage_size % class_size;\r\nusedpc = (zspage_size - waste) * 100 / zspage_size;\r\nif (usedpc > max_usedpc) {\r\nmax_usedpc = usedpc;\r\nmax_usedpc_order = i;\r\n}\r\n}\r\nreturn max_usedpc_order;\r\n}\r\nstatic struct page *get_first_page(struct page *page)\r\n{\r\nif (is_first_page(page))\r\nreturn page;\r\nelse\r\nreturn page->first_page;\r\n}\r\nstatic struct page *get_next_page(struct page *page)\r\n{\r\nstruct page *next;\r\nif (is_last_page(page))\r\nnext = NULL;\r\nelse if (is_first_page(page))\r\nnext = (struct page *)page->private;\r\nelse\r\nnext = list_entry(page->lru.next, struct page, lru);\r\nreturn next;\r\n}\r\nstatic void *obj_location_to_handle(struct page *page, unsigned long obj_idx)\r\n{\r\nunsigned long handle;\r\nif (!page) {\r\nBUG_ON(obj_idx);\r\nreturn NULL;\r\n}\r\nhandle = page_to_pfn(page) << OBJ_INDEX_BITS;\r\nhandle |= (obj_idx & OBJ_INDEX_MASK);\r\nreturn (void *)handle;\r\n}\r\nstatic void obj_handle_to_location(unsigned long handle, struct page **page,\r\nunsigned long *obj_idx)\r\n{\r\n*page = pfn_to_page(handle >> OBJ_INDEX_BITS);\r\n*obj_idx = handle & OBJ_INDEX_MASK;\r\n}\r\nstatic unsigned long obj_idx_to_offset(struct page *page,\r\nunsigned long obj_idx, int class_size)\r\n{\r\nunsigned long off = 0;\r\nif (!is_first_page(page))\r\noff = page->index;\r\nreturn off + obj_idx * class_size;\r\n}\r\nstatic void reset_page(struct page *page)\r\n{\r\nclear_bit(PG_private, &page->flags);\r\nclear_bit(PG_private_2, &page->flags);\r\nset_page_private(page, 0);\r\npage->mapping = NULL;\r\npage->freelist = NULL;\r\nreset_page_mapcount(page);\r\n}\r\nstatic void free_zspage(struct page *first_page)\r\n{\r\nstruct page *nextp, *tmp, *head_extra;\r\nBUG_ON(!is_first_page(first_page));\r\nBUG_ON(first_page->inuse);\r\nhead_extra = (struct page *)page_private(first_page);\r\nreset_page(first_page);\r\n__free_page(first_page);\r\nif (!head_extra)\r\nreturn;\r\nlist_for_each_entry_safe(nextp, tmp, &head_extra->lru, lru) {\r\nlist_del(&nextp->lru);\r\nreset_page(nextp);\r\n__free_page(nextp);\r\n}\r\nreset_page(head_extra);\r\n__free_page(head_extra);\r\n}\r\nstatic void init_zspage(struct page *first_page, struct size_class *class)\r\n{\r\nunsigned long off = 0;\r\nstruct page *page = first_page;\r\nBUG_ON(!is_first_page(first_page));\r\nwhile (page) {\r\nstruct page *next_page;\r\nstruct link_free *link;\r\nunsigned int i, objs_on_page;\r\nif (page != first_page)\r\npage->index = off;\r\nlink = (struct link_free *)kmap_atomic(page) +\r\noff / sizeof(*link);\r\nobjs_on_page = (PAGE_SIZE - off) / class->size;\r\nfor (i = 1; i <= objs_on_page; i++) {\r\noff += class->size;\r\nif (off < PAGE_SIZE) {\r\nlink->next = obj_location_to_handle(page, i);\r\nlink += class->size / sizeof(*link);\r\n}\r\n}\r\nnext_page = get_next_page(page);\r\nlink->next = obj_location_to_handle(next_page, 0);\r\nkunmap_atomic(link);\r\npage = next_page;\r\noff = (off + class->size) % PAGE_SIZE;\r\n}\r\n}\r\nstatic struct page *alloc_zspage(struct size_class *class, gfp_t flags)\r\n{\r\nint i, error;\r\nstruct page *first_page = NULL, *uninitialized_var(prev_page);\r\nerror = -ENOMEM;\r\nfor (i = 0; i < class->pages_per_zspage; i++) {\r\nstruct page *page;\r\npage = alloc_page(flags);\r\nif (!page)\r\ngoto cleanup;\r\nINIT_LIST_HEAD(&page->lru);\r\nif (i == 0) {\r\nSetPagePrivate(page);\r\nset_page_private(page, 0);\r\nfirst_page = page;\r\nfirst_page->inuse = 0;\r\n}\r\nif (i == 1)\r\nfirst_page->private = (unsigned long)page;\r\nif (i >= 1)\r\npage->first_page = first_page;\r\nif (i >= 2)\r\nlist_add(&page->lru, &prev_page->lru);\r\nif (i == class->pages_per_zspage - 1)\r\nSetPagePrivate2(page);\r\nprev_page = page;\r\n}\r\ninit_zspage(first_page, class);\r\nfirst_page->freelist = obj_location_to_handle(first_page, 0);\r\nfirst_page->objects = class->pages_per_zspage * PAGE_SIZE / class->size;\r\nerror = 0;\r\ncleanup:\r\nif (unlikely(error) && first_page) {\r\nfree_zspage(first_page);\r\nfirst_page = NULL;\r\n}\r\nreturn first_page;\r\n}\r\nstatic struct page *find_get_zspage(struct size_class *class)\r\n{\r\nint i;\r\nstruct page *page;\r\nfor (i = 0; i < _ZS_NR_FULLNESS_GROUPS; i++) {\r\npage = class->fullness_list[i];\r\nif (page)\r\nbreak;\r\n}\r\nreturn page;\r\n}\r\nstatic inline int __zs_cpu_up(struct mapping_area *area)\r\n{\r\nif (area->vm)\r\nreturn 0;\r\narea->vm = alloc_vm_area(PAGE_SIZE * 2, NULL);\r\nif (!area->vm)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic inline void __zs_cpu_down(struct mapping_area *area)\r\n{\r\nif (area->vm)\r\nfree_vm_area(area->vm);\r\narea->vm = NULL;\r\n}\r\nstatic inline void *__zs_map_object(struct mapping_area *area,\r\nstruct page *pages[2], int off, int size)\r\n{\r\nBUG_ON(map_vm_area(area->vm, PAGE_KERNEL, &pages));\r\narea->vm_addr = area->vm->addr;\r\nreturn area->vm_addr + off;\r\n}\r\nstatic inline void __zs_unmap_object(struct mapping_area *area,\r\nstruct page *pages[2], int off, int size)\r\n{\r\nunsigned long addr = (unsigned long)area->vm_addr;\r\nunsigned long end = addr + (PAGE_SIZE * 2);\r\nflush_cache_vunmap(addr, end);\r\nunmap_kernel_range_noflush(addr, PAGE_SIZE * 2);\r\nlocal_flush_tlb_kernel_range(addr, end);\r\n}\r\nstatic inline int __zs_cpu_up(struct mapping_area *area)\r\n{\r\nif (area->vm_buf)\r\nreturn 0;\r\narea->vm_buf = (char *)__get_free_page(GFP_KERNEL);\r\nif (!area->vm_buf)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nstatic inline void __zs_cpu_down(struct mapping_area *area)\r\n{\r\nif (area->vm_buf)\r\nfree_page((unsigned long)area->vm_buf);\r\narea->vm_buf = NULL;\r\n}\r\nstatic void *__zs_map_object(struct mapping_area *area,\r\nstruct page *pages[2], int off, int size)\r\n{\r\nint sizes[2];\r\nvoid *addr;\r\nchar *buf = area->vm_buf;\r\npagefault_disable();\r\nif (area->vm_mm == ZS_MM_WO)\r\ngoto out;\r\nsizes[0] = PAGE_SIZE - off;\r\nsizes[1] = size - sizes[0];\r\naddr = kmap_atomic(pages[0]);\r\nmemcpy(buf, addr + off, sizes[0]);\r\nkunmap_atomic(addr);\r\naddr = kmap_atomic(pages[1]);\r\nmemcpy(buf + sizes[0], addr, sizes[1]);\r\nkunmap_atomic(addr);\r\nout:\r\nreturn area->vm_buf;\r\n}\r\nstatic void __zs_unmap_object(struct mapping_area *area,\r\nstruct page *pages[2], int off, int size)\r\n{\r\nint sizes[2];\r\nvoid *addr;\r\nchar *buf = area->vm_buf;\r\nif (area->vm_mm == ZS_MM_RO)\r\ngoto out;\r\nsizes[0] = PAGE_SIZE - off;\r\nsizes[1] = size - sizes[0];\r\naddr = kmap_atomic(pages[0]);\r\nmemcpy(addr + off, buf, sizes[0]);\r\nkunmap_atomic(addr);\r\naddr = kmap_atomic(pages[1]);\r\nmemcpy(addr, buf + sizes[0], sizes[1]);\r\nkunmap_atomic(addr);\r\nout:\r\npagefault_enable();\r\n}\r\nstatic int zs_cpu_notifier(struct notifier_block *nb, unsigned long action,\r\nvoid *pcpu)\r\n{\r\nint ret, cpu = (long)pcpu;\r\nstruct mapping_area *area;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\narea = &per_cpu(zs_map_area, cpu);\r\nret = __zs_cpu_up(area);\r\nif (ret)\r\nreturn notifier_from_errno(ret);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_UP_CANCELED:\r\narea = &per_cpu(zs_map_area, cpu);\r\n__zs_cpu_down(area);\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void zs_exit(void)\r\n{\r\nint cpu;\r\nfor_each_online_cpu(cpu)\r\nzs_cpu_notifier(NULL, CPU_DEAD, (void *)(long)cpu);\r\nunregister_cpu_notifier(&zs_cpu_nb);\r\n}\r\nstatic int zs_init(void)\r\n{\r\nint cpu, ret;\r\nregister_cpu_notifier(&zs_cpu_nb);\r\nfor_each_online_cpu(cpu) {\r\nret = zs_cpu_notifier(NULL, CPU_UP_PREPARE, (void *)(long)cpu);\r\nif (notifier_to_errno(ret))\r\ngoto fail;\r\n}\r\nreturn 0;\r\nfail:\r\nzs_exit();\r\nreturn notifier_to_errno(ret);\r\n}\r\nstruct zs_pool *zs_create_pool(const char *name, gfp_t flags)\r\n{\r\nint i, ovhd_size;\r\nstruct zs_pool *pool;\r\nif (!name)\r\nreturn NULL;\r\novhd_size = roundup(sizeof(*pool), PAGE_SIZE);\r\npool = kzalloc(ovhd_size, GFP_KERNEL);\r\nif (!pool)\r\nreturn NULL;\r\nfor (i = 0; i < ZS_SIZE_CLASSES; i++) {\r\nint size;\r\nstruct size_class *class;\r\nsize = ZS_MIN_ALLOC_SIZE + i * ZS_SIZE_CLASS_DELTA;\r\nif (size > ZS_MAX_ALLOC_SIZE)\r\nsize = ZS_MAX_ALLOC_SIZE;\r\nclass = &pool->size_class[i];\r\nclass->size = size;\r\nclass->index = i;\r\nspin_lock_init(&class->lock);\r\nclass->pages_per_zspage = get_pages_per_zspage(size);\r\n}\r\npool->flags = flags;\r\npool->name = name;\r\nreturn pool;\r\n}\r\nvoid zs_destroy_pool(struct zs_pool *pool)\r\n{\r\nint i;\r\nfor (i = 0; i < ZS_SIZE_CLASSES; i++) {\r\nint fg;\r\nstruct size_class *class = &pool->size_class[i];\r\nfor (fg = 0; fg < _ZS_NR_FULLNESS_GROUPS; fg++) {\r\nif (class->fullness_list[fg]) {\r\npr_info("Freeing non-empty class with size "\r\n"%db, fullness group %d\n",\r\nclass->size, fg);\r\n}\r\n}\r\n}\r\nkfree(pool);\r\n}\r\nunsigned long zs_malloc(struct zs_pool *pool, size_t size)\r\n{\r\nunsigned long obj;\r\nstruct link_free *link;\r\nint class_idx;\r\nstruct size_class *class;\r\nstruct page *first_page, *m_page;\r\nunsigned long m_objidx, m_offset;\r\nif (unlikely(!size || size > ZS_MAX_ALLOC_SIZE))\r\nreturn 0;\r\nclass_idx = get_size_class_index(size);\r\nclass = &pool->size_class[class_idx];\r\nBUG_ON(class_idx != class->index);\r\nspin_lock(&class->lock);\r\nfirst_page = find_get_zspage(class);\r\nif (!first_page) {\r\nspin_unlock(&class->lock);\r\nfirst_page = alloc_zspage(class, pool->flags);\r\nif (unlikely(!first_page))\r\nreturn 0;\r\nset_zspage_mapping(first_page, class->index, ZS_EMPTY);\r\nspin_lock(&class->lock);\r\nclass->pages_allocated += class->pages_per_zspage;\r\n}\r\nobj = (unsigned long)first_page->freelist;\r\nobj_handle_to_location(obj, &m_page, &m_objidx);\r\nm_offset = obj_idx_to_offset(m_page, m_objidx, class->size);\r\nlink = (struct link_free *)kmap_atomic(m_page) +\r\nm_offset / sizeof(*link);\r\nfirst_page->freelist = link->next;\r\nmemset(link, POISON_INUSE, sizeof(*link));\r\nkunmap_atomic(link);\r\nfirst_page->inuse++;\r\nfix_fullness_group(pool, first_page);\r\nspin_unlock(&class->lock);\r\nreturn obj;\r\n}\r\nvoid zs_free(struct zs_pool *pool, unsigned long obj)\r\n{\r\nstruct link_free *link;\r\nstruct page *first_page, *f_page;\r\nunsigned long f_objidx, f_offset;\r\nint class_idx;\r\nstruct size_class *class;\r\nenum fullness_group fullness;\r\nif (unlikely(!obj))\r\nreturn;\r\nobj_handle_to_location(obj, &f_page, &f_objidx);\r\nfirst_page = get_first_page(f_page);\r\nget_zspage_mapping(first_page, &class_idx, &fullness);\r\nclass = &pool->size_class[class_idx];\r\nf_offset = obj_idx_to_offset(f_page, f_objidx, class->size);\r\nspin_lock(&class->lock);\r\nlink = (struct link_free *)((unsigned char *)kmap_atomic(f_page)\r\n+ f_offset);\r\nlink->next = first_page->freelist;\r\nkunmap_atomic(link);\r\nfirst_page->freelist = (void *)obj;\r\nfirst_page->inuse--;\r\nfullness = fix_fullness_group(pool, first_page);\r\nif (fullness == ZS_EMPTY)\r\nclass->pages_allocated -= class->pages_per_zspage;\r\nspin_unlock(&class->lock);\r\nif (fullness == ZS_EMPTY)\r\nfree_zspage(first_page);\r\n}\r\nvoid *zs_map_object(struct zs_pool *pool, unsigned long handle,\r\nenum zs_mapmode mm)\r\n{\r\nstruct page *page;\r\nunsigned long obj_idx, off;\r\nunsigned int class_idx;\r\nenum fullness_group fg;\r\nstruct size_class *class;\r\nstruct mapping_area *area;\r\nstruct page *pages[2];\r\nBUG_ON(!handle);\r\nBUG_ON(in_interrupt());\r\nobj_handle_to_location(handle, &page, &obj_idx);\r\nget_zspage_mapping(get_first_page(page), &class_idx, &fg);\r\nclass = &pool->size_class[class_idx];\r\noff = obj_idx_to_offset(page, obj_idx, class->size);\r\narea = &get_cpu_var(zs_map_area);\r\narea->vm_mm = mm;\r\nif (off + class->size <= PAGE_SIZE) {\r\narea->vm_addr = kmap_atomic(page);\r\nreturn area->vm_addr + off;\r\n}\r\npages[0] = page;\r\npages[1] = get_next_page(page);\r\nBUG_ON(!pages[1]);\r\nreturn __zs_map_object(area, pages, off, class->size);\r\n}\r\nvoid zs_unmap_object(struct zs_pool *pool, unsigned long handle)\r\n{\r\nstruct page *page;\r\nunsigned long obj_idx, off;\r\nunsigned int class_idx;\r\nenum fullness_group fg;\r\nstruct size_class *class;\r\nstruct mapping_area *area;\r\nBUG_ON(!handle);\r\nobj_handle_to_location(handle, &page, &obj_idx);\r\nget_zspage_mapping(get_first_page(page), &class_idx, &fg);\r\nclass = &pool->size_class[class_idx];\r\noff = obj_idx_to_offset(page, obj_idx, class->size);\r\narea = &__get_cpu_var(zs_map_area);\r\nif (off + class->size <= PAGE_SIZE)\r\nkunmap_atomic(area->vm_addr);\r\nelse {\r\nstruct page *pages[2];\r\npages[0] = page;\r\npages[1] = get_next_page(page);\r\nBUG_ON(!pages[1]);\r\n__zs_unmap_object(area, pages, off, class->size);\r\n}\r\nput_cpu_var(zs_map_area);\r\n}\r\nu64 zs_get_total_size_bytes(struct zs_pool *pool)\r\n{\r\nint i;\r\nu64 npages = 0;\r\nfor (i = 0; i < ZS_SIZE_CLASSES; i++)\r\nnpages += pool->size_class[i].pages_allocated;\r\nreturn npages << PAGE_SHIFT;\r\n}
