void\r\ndo_sync_gen_syndrome(struct page **blocks, unsigned int offset, int disks,\r\nsize_t len, struct async_submit_ctl *submit)\r\n{\r\nvoid **srcs;\r\nint i;\r\nif (submit->scribble)\r\nsrcs = submit->scribble;\r\nelse\r\nsrcs = (void **) blocks;\r\nfor (i = 0; i < disks; i++) {\r\nif (blocks[i] == NULL) {\r\nBUG_ON(i > disks - 3);\r\nsrcs[i] = (void*)raid6_empty_zero_page;\r\n} else\r\nsrcs[i] = page_address(blocks[i]) + offset;\r\n}\r\nraid6_call.gen_syndrome(disks, len, srcs);\r\nasync_tx_sync_epilog(submit);\r\n}\r\nstruct dma_async_tx_descriptor *\r\nasync_gen_syndrome(struct page **blocks, unsigned int offset, int disks,\r\nsize_t len, struct async_submit_ctl *submit)\r\n{\r\nint src_cnt = disks - 2;\r\nstruct dma_chan *chan = async_tx_find_channel(submit, DMA_PQ,\r\n&P(blocks, disks), 2,\r\nblocks, src_cnt, len);\r\nstruct dma_device *device = chan ? chan->device : NULL;\r\ndma_addr_t *dma_src = NULL;\r\nBUG_ON(disks > 255 || !(P(blocks, disks) || Q(blocks, disks)));\r\nif (submit->scribble)\r\ndma_src = submit->scribble;\r\nelse if (sizeof(dma_addr_t) <= sizeof(struct page *))\r\ndma_src = (dma_addr_t *) blocks;\r\nif (dma_src && device &&\r\n(src_cnt <= dma_maxpq(device, 0) ||\r\ndma_maxpq(device, DMA_PREP_CONTINUE) > 0) &&\r\nis_dma_pq_aligned(device, offset, 0, len)) {\r\npr_debug("%s: (async) disks: %d len: %zu\n",\r\n__func__, disks, len);\r\nreturn do_async_gen_syndrome(chan, blocks, raid6_gfexp, offset,\r\ndisks, len, dma_src, submit);\r\n}\r\npr_debug("%s: (sync) disks: %d len: %zu\n", __func__, disks, len);\r\nasync_tx_quiesce(&submit->depend_tx);\r\nif (!P(blocks, disks)) {\r\nP(blocks, disks) = pq_scribble_page;\r\nBUG_ON(len + offset > PAGE_SIZE);\r\n}\r\nif (!Q(blocks, disks)) {\r\nQ(blocks, disks) = pq_scribble_page;\r\nBUG_ON(len + offset > PAGE_SIZE);\r\n}\r\ndo_sync_gen_syndrome(blocks, offset, disks, len, submit);\r\nreturn NULL;\r\n}\r\nstatic inline struct dma_chan *\r\npq_val_chan(struct async_submit_ctl *submit, struct page **blocks, int disks, size_t len)\r\n{\r\n#ifdef CONFIG_ASYNC_TX_DISABLE_PQ_VAL_DMA\r\nreturn NULL;\r\n#endif\r\nreturn async_tx_find_channel(submit, DMA_PQ_VAL, NULL, 0, blocks,\r\ndisks, len);\r\n}\r\nstruct dma_async_tx_descriptor *\r\nasync_syndrome_val(struct page **blocks, unsigned int offset, int disks,\r\nsize_t len, enum sum_check_flags *pqres, struct page *spare,\r\nstruct async_submit_ctl *submit)\r\n{\r\nstruct dma_chan *chan = pq_val_chan(submit, blocks, disks, len);\r\nstruct dma_device *device = chan ? chan->device : NULL;\r\nstruct dma_async_tx_descriptor *tx;\r\nunsigned char coefs[disks-2];\r\nenum dma_ctrl_flags dma_flags = submit->cb_fn ? DMA_PREP_INTERRUPT : 0;\r\ndma_addr_t *dma_src = NULL;\r\nint src_cnt = 0;\r\nBUG_ON(disks < 4);\r\nif (submit->scribble)\r\ndma_src = submit->scribble;\r\nelse if (sizeof(dma_addr_t) <= sizeof(struct page *))\r\ndma_src = (dma_addr_t *) blocks;\r\nif (dma_src && device && disks <= dma_maxpq(device, 0) &&\r\nis_dma_pq_aligned(device, offset, 0, len)) {\r\nstruct device *dev = device->dev;\r\ndma_addr_t *pq = &dma_src[disks-2];\r\nint i;\r\npr_debug("%s: (async) disks: %d len: %zu\n",\r\n__func__, disks, len);\r\nif (!P(blocks, disks))\r\ndma_flags |= DMA_PREP_PQ_DISABLE_P;\r\nelse\r\npq[0] = dma_map_page(dev, P(blocks, disks),\r\noffset, len,\r\nDMA_TO_DEVICE);\r\nif (!Q(blocks, disks))\r\ndma_flags |= DMA_PREP_PQ_DISABLE_Q;\r\nelse\r\npq[1] = dma_map_page(dev, Q(blocks, disks),\r\noffset, len,\r\nDMA_TO_DEVICE);\r\nif (submit->flags & ASYNC_TX_FENCE)\r\ndma_flags |= DMA_PREP_FENCE;\r\nfor (i = 0; i < disks-2; i++)\r\nif (likely(blocks[i])) {\r\ndma_src[src_cnt] = dma_map_page(dev, blocks[i],\r\noffset, len,\r\nDMA_TO_DEVICE);\r\ncoefs[src_cnt] = raid6_gfexp[i];\r\nsrc_cnt++;\r\n}\r\nfor (;;) {\r\ntx = device->device_prep_dma_pq_val(chan, pq, dma_src,\r\nsrc_cnt,\r\ncoefs,\r\nlen, pqres,\r\ndma_flags);\r\nif (likely(tx))\r\nbreak;\r\nasync_tx_quiesce(&submit->depend_tx);\r\ndma_async_issue_pending(chan);\r\n}\r\nasync_tx_submit(chan, tx, submit);\r\nreturn tx;\r\n} else {\r\nstruct page *p_src = P(blocks, disks);\r\nstruct page *q_src = Q(blocks, disks);\r\nenum async_tx_flags flags_orig = submit->flags;\r\ndma_async_tx_callback cb_fn_orig = submit->cb_fn;\r\nvoid *scribble = submit->scribble;\r\nvoid *cb_param_orig = submit->cb_param;\r\nvoid *p, *q, *s;\r\npr_debug("%s: (sync) disks: %d len: %zu\n",\r\n__func__, disks, len);\r\nBUG_ON(!spare || !scribble);\r\nasync_tx_quiesce(&submit->depend_tx);\r\ntx = NULL;\r\n*pqres = 0;\r\nif (p_src) {\r\ninit_async_submit(submit, ASYNC_TX_XOR_ZERO_DST, NULL,\r\nNULL, NULL, scribble);\r\ntx = async_xor(spare, blocks, offset, disks-2, len, submit);\r\nasync_tx_quiesce(&tx);\r\np = page_address(p_src) + offset;\r\ns = page_address(spare) + offset;\r\n*pqres |= !!memcmp(p, s, len) << SUM_CHECK_P;\r\n}\r\nif (q_src) {\r\nP(blocks, disks) = NULL;\r\nQ(blocks, disks) = spare;\r\ninit_async_submit(submit, 0, NULL, NULL, NULL, scribble);\r\ntx = async_gen_syndrome(blocks, offset, disks, len, submit);\r\nasync_tx_quiesce(&tx);\r\nq = page_address(q_src) + offset;\r\ns = page_address(spare) + offset;\r\n*pqres |= !!memcmp(q, s, len) << SUM_CHECK_Q;\r\n}\r\nP(blocks, disks) = p_src;\r\nQ(blocks, disks) = q_src;\r\nsubmit->cb_fn = cb_fn_orig;\r\nsubmit->cb_param = cb_param_orig;\r\nsubmit->flags = flags_orig;\r\nasync_tx_sync_epilog(submit);\r\nreturn NULL;\r\n}\r\n}\r\nstatic int __init async_pq_init(void)\r\n{\r\npq_scribble_page = alloc_page(GFP_KERNEL);\r\nif (pq_scribble_page)\r\nreturn 0;\r\npr_err("%s: failed to allocate required spare page\n", __func__);\r\nreturn -ENOMEM;\r\n}\r\nstatic void __exit async_pq_exit(void)\r\n{\r\nput_page(pq_scribble_page);\r\n}
