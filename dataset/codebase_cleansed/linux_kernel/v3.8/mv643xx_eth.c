static inline u32 rdl(struct mv643xx_eth_private *mp, int offset)\r\n{\r\nreturn readl(mp->shared->base + offset);\r\n}\r\nstatic inline u32 rdlp(struct mv643xx_eth_private *mp, int offset)\r\n{\r\nreturn readl(mp->base + offset);\r\n}\r\nstatic inline void wrl(struct mv643xx_eth_private *mp, int offset, u32 data)\r\n{\r\nwritel(data, mp->shared->base + offset);\r\n}\r\nstatic inline void wrlp(struct mv643xx_eth_private *mp, int offset, u32 data)\r\n{\r\nwritel(data, mp->base + offset);\r\n}\r\nstatic struct mv643xx_eth_private *rxq_to_mp(struct rx_queue *rxq)\r\n{\r\nreturn container_of(rxq, struct mv643xx_eth_private, rxq[rxq->index]);\r\n}\r\nstatic struct mv643xx_eth_private *txq_to_mp(struct tx_queue *txq)\r\n{\r\nreturn container_of(txq, struct mv643xx_eth_private, txq[txq->index]);\r\n}\r\nstatic void rxq_enable(struct rx_queue *rxq)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nwrlp(mp, RXQ_COMMAND, 1 << rxq->index);\r\n}\r\nstatic void rxq_disable(struct rx_queue *rxq)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nu8 mask = 1 << rxq->index;\r\nwrlp(mp, RXQ_COMMAND, mask << 8);\r\nwhile (rdlp(mp, RXQ_COMMAND) & mask)\r\nudelay(10);\r\n}\r\nstatic void txq_reset_hw_ptr(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nu32 addr;\r\naddr = (u32)txq->tx_desc_dma;\r\naddr += txq->tx_curr_desc * sizeof(struct tx_desc);\r\nwrlp(mp, TXQ_CURRENT_DESC_PTR(txq->index), addr);\r\n}\r\nstatic void txq_enable(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nwrlp(mp, TXQ_COMMAND, 1 << txq->index);\r\n}\r\nstatic void txq_disable(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nu8 mask = 1 << txq->index;\r\nwrlp(mp, TXQ_COMMAND, mask << 8);\r\nwhile (rdlp(mp, TXQ_COMMAND) & mask)\r\nudelay(10);\r\n}\r\nstatic void txq_maybe_wake(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\r\nif (netif_tx_queue_stopped(nq)) {\r\n__netif_tx_lock(nq, smp_processor_id());\r\nif (txq->tx_ring_size - txq->tx_desc_count >= MAX_SKB_FRAGS + 1)\r\nnetif_tx_wake_queue(nq);\r\n__netif_tx_unlock(nq);\r\n}\r\n}\r\nstatic int\r\nmv643xx_get_skb_header(struct sk_buff *skb, void **iphdr, void **tcph,\r\nu64 *hdr_flags, void *priv)\r\n{\r\nunsigned long cmd_sts = (unsigned long)priv;\r\nif ((cmd_sts & (RX_IP_HDR_OK | RX_PKT_IS_IPV4 |\r\nRX_PKT_IS_ETHERNETV2 | RX_PKT_LAYER4_TYPE_MASK |\r\nRX_PKT_IS_VLAN_TAGGED)) !=\r\n(RX_IP_HDR_OK | RX_PKT_IS_IPV4 |\r\nRX_PKT_IS_ETHERNETV2 | RX_PKT_LAYER4_TYPE_TCP_IPV4))\r\nreturn -1;\r\nskb_reset_network_header(skb);\r\nskb_set_transport_header(skb, ip_hdrlen(skb));\r\n*iphdr = ip_hdr(skb);\r\n*tcph = tcp_hdr(skb);\r\n*hdr_flags = LRO_IPV4 | LRO_TCP;\r\nreturn 0;\r\n}\r\nstatic int rxq_process(struct rx_queue *rxq, int budget)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nstruct net_device_stats *stats = &mp->dev->stats;\r\nint lro_flush_needed;\r\nint rx;\r\nlro_flush_needed = 0;\r\nrx = 0;\r\nwhile (rx < budget && rxq->rx_desc_count) {\r\nstruct rx_desc *rx_desc;\r\nunsigned int cmd_sts;\r\nstruct sk_buff *skb;\r\nu16 byte_cnt;\r\nrx_desc = &rxq->rx_desc_area[rxq->rx_curr_desc];\r\ncmd_sts = rx_desc->cmd_sts;\r\nif (cmd_sts & BUFFER_OWNED_BY_DMA)\r\nbreak;\r\nrmb();\r\nskb = rxq->rx_skb[rxq->rx_curr_desc];\r\nrxq->rx_skb[rxq->rx_curr_desc] = NULL;\r\nrxq->rx_curr_desc++;\r\nif (rxq->rx_curr_desc == rxq->rx_ring_size)\r\nrxq->rx_curr_desc = 0;\r\ndma_unmap_single(mp->dev->dev.parent, rx_desc->buf_ptr,\r\nrx_desc->buf_size, DMA_FROM_DEVICE);\r\nrxq->rx_desc_count--;\r\nrx++;\r\nmp->work_rx_refill |= 1 << rxq->index;\r\nbyte_cnt = rx_desc->byte_cnt;\r\nstats->rx_packets++;\r\nstats->rx_bytes += byte_cnt - 2;\r\nif ((cmd_sts & (RX_FIRST_DESC | RX_LAST_DESC | ERROR_SUMMARY))\r\n!= (RX_FIRST_DESC | RX_LAST_DESC))\r\ngoto err;\r\nskb_put(skb, byte_cnt - 2 - 4);\r\nif (cmd_sts & LAYER_4_CHECKSUM_OK)\r\nskb->ip_summed = CHECKSUM_UNNECESSARY;\r\nskb->protocol = eth_type_trans(skb, mp->dev);\r\nif (skb->dev->features & NETIF_F_LRO &&\r\nskb->ip_summed == CHECKSUM_UNNECESSARY) {\r\nlro_receive_skb(&rxq->lro_mgr, skb, (void *)cmd_sts);\r\nlro_flush_needed = 1;\r\n} else\r\nnetif_receive_skb(skb);\r\ncontinue;\r\nerr:\r\nstats->rx_dropped++;\r\nif ((cmd_sts & (RX_FIRST_DESC | RX_LAST_DESC)) !=\r\n(RX_FIRST_DESC | RX_LAST_DESC)) {\r\nif (net_ratelimit())\r\nnetdev_err(mp->dev,\r\n"received packet spanning multiple descriptors\n");\r\n}\r\nif (cmd_sts & ERROR_SUMMARY)\r\nstats->rx_errors++;\r\ndev_kfree_skb(skb);\r\n}\r\nif (lro_flush_needed)\r\nlro_flush_all(&rxq->lro_mgr);\r\nif (rx < budget)\r\nmp->work_rx &= ~(1 << rxq->index);\r\nreturn rx;\r\n}\r\nstatic int rxq_refill(struct rx_queue *rxq, int budget)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nint refilled;\r\nrefilled = 0;\r\nwhile (refilled < budget && rxq->rx_desc_count < rxq->rx_ring_size) {\r\nstruct sk_buff *skb;\r\nint rx;\r\nstruct rx_desc *rx_desc;\r\nint size;\r\nskb = netdev_alloc_skb(mp->dev, mp->skb_size);\r\nif (skb == NULL) {\r\nmp->oom = 1;\r\ngoto oom;\r\n}\r\nif (SKB_DMA_REALIGN)\r\nskb_reserve(skb, SKB_DMA_REALIGN);\r\nrefilled++;\r\nrxq->rx_desc_count++;\r\nrx = rxq->rx_used_desc++;\r\nif (rxq->rx_used_desc == rxq->rx_ring_size)\r\nrxq->rx_used_desc = 0;\r\nrx_desc = rxq->rx_desc_area + rx;\r\nsize = skb->end - skb->data;\r\nrx_desc->buf_ptr = dma_map_single(mp->dev->dev.parent,\r\nskb->data, size,\r\nDMA_FROM_DEVICE);\r\nrx_desc->buf_size = size;\r\nrxq->rx_skb[rx] = skb;\r\nwmb();\r\nrx_desc->cmd_sts = BUFFER_OWNED_BY_DMA | RX_ENABLE_INTERRUPT;\r\nwmb();\r\nskb_reserve(skb, 2);\r\n}\r\nif (refilled < budget)\r\nmp->work_rx_refill &= ~(1 << rxq->index);\r\noom:\r\nreturn refilled;\r\n}\r\nstatic inline unsigned int has_tiny_unaligned_frags(struct sk_buff *skb)\r\n{\r\nint frag;\r\nfor (frag = 0; frag < skb_shinfo(skb)->nr_frags; frag++) {\r\nconst skb_frag_t *fragp = &skb_shinfo(skb)->frags[frag];\r\nif (skb_frag_size(fragp) <= 8 && fragp->page_offset & 7)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void txq_submit_frag_skb(struct tx_queue *txq, struct sk_buff *skb)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint nr_frags = skb_shinfo(skb)->nr_frags;\r\nint frag;\r\nfor (frag = 0; frag < nr_frags; frag++) {\r\nskb_frag_t *this_frag;\r\nint tx_index;\r\nstruct tx_desc *desc;\r\nthis_frag = &skb_shinfo(skb)->frags[frag];\r\ntx_index = txq->tx_curr_desc++;\r\nif (txq->tx_curr_desc == txq->tx_ring_size)\r\ntxq->tx_curr_desc = 0;\r\ndesc = &txq->tx_desc_area[tx_index];\r\nif (frag == nr_frags - 1) {\r\ndesc->cmd_sts = BUFFER_OWNED_BY_DMA |\r\nZERO_PADDING | TX_LAST_DESC |\r\nTX_ENABLE_INTERRUPT;\r\n} else {\r\ndesc->cmd_sts = BUFFER_OWNED_BY_DMA;\r\n}\r\ndesc->l4i_chk = 0;\r\ndesc->byte_cnt = skb_frag_size(this_frag);\r\ndesc->buf_ptr = skb_frag_dma_map(mp->dev->dev.parent,\r\nthis_frag, 0,\r\nskb_frag_size(this_frag),\r\nDMA_TO_DEVICE);\r\n}\r\n}\r\nstatic inline __be16 sum16_as_be(__sum16 sum)\r\n{\r\nreturn (__force __be16)sum;\r\n}\r\nstatic int txq_submit_skb(struct tx_queue *txq, struct sk_buff *skb)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint nr_frags = skb_shinfo(skb)->nr_frags;\r\nint tx_index;\r\nstruct tx_desc *desc;\r\nu32 cmd_sts;\r\nu16 l4i_chk;\r\nint length;\r\ncmd_sts = TX_FIRST_DESC | GEN_CRC | BUFFER_OWNED_BY_DMA;\r\nl4i_chk = 0;\r\nif (skb->ip_summed == CHECKSUM_PARTIAL) {\r\nint hdr_len;\r\nint tag_bytes;\r\nBUG_ON(skb->protocol != htons(ETH_P_IP) &&\r\nskb->protocol != htons(ETH_P_8021Q));\r\nhdr_len = (void *)ip_hdr(skb) - (void *)skb->data;\r\ntag_bytes = hdr_len - ETH_HLEN;\r\nif (skb->len - hdr_len > mp->shared->tx_csum_limit ||\r\nunlikely(tag_bytes & ~12)) {\r\nif (skb_checksum_help(skb) == 0)\r\ngoto no_csum;\r\nkfree_skb(skb);\r\nreturn 1;\r\n}\r\nif (tag_bytes & 4)\r\ncmd_sts |= MAC_HDR_EXTRA_4_BYTES;\r\nif (tag_bytes & 8)\r\ncmd_sts |= MAC_HDR_EXTRA_8_BYTES;\r\ncmd_sts |= GEN_TCP_UDP_CHECKSUM |\r\nGEN_IP_V4_CHECKSUM |\r\nip_hdr(skb)->ihl << TX_IHL_SHIFT;\r\nswitch (ip_hdr(skb)->protocol) {\r\ncase IPPROTO_UDP:\r\ncmd_sts |= UDP_FRAME;\r\nl4i_chk = ntohs(sum16_as_be(udp_hdr(skb)->check));\r\nbreak;\r\ncase IPPROTO_TCP:\r\nl4i_chk = ntohs(sum16_as_be(tcp_hdr(skb)->check));\r\nbreak;\r\ndefault:\r\nBUG();\r\n}\r\n} else {\r\nno_csum:\r\ncmd_sts |= 5 << TX_IHL_SHIFT;\r\n}\r\ntx_index = txq->tx_curr_desc++;\r\nif (txq->tx_curr_desc == txq->tx_ring_size)\r\ntxq->tx_curr_desc = 0;\r\ndesc = &txq->tx_desc_area[tx_index];\r\nif (nr_frags) {\r\ntxq_submit_frag_skb(txq, skb);\r\nlength = skb_headlen(skb);\r\n} else {\r\ncmd_sts |= ZERO_PADDING | TX_LAST_DESC | TX_ENABLE_INTERRUPT;\r\nlength = skb->len;\r\n}\r\ndesc->l4i_chk = l4i_chk;\r\ndesc->byte_cnt = length;\r\ndesc->buf_ptr = dma_map_single(mp->dev->dev.parent, skb->data,\r\nlength, DMA_TO_DEVICE);\r\n__skb_queue_tail(&txq->tx_skb, skb);\r\nskb_tx_timestamp(skb);\r\nwmb();\r\ndesc->cmd_sts = cmd_sts;\r\nmp->work_tx_end &= ~(1 << txq->index);\r\nwmb();\r\ntxq_enable(txq);\r\ntxq->tx_desc_count += nr_frags + 1;\r\nreturn 0;\r\n}\r\nstatic netdev_tx_t mv643xx_eth_xmit(struct sk_buff *skb, struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint length, queue;\r\nstruct tx_queue *txq;\r\nstruct netdev_queue *nq;\r\nqueue = skb_get_queue_mapping(skb);\r\ntxq = mp->txq + queue;\r\nnq = netdev_get_tx_queue(dev, queue);\r\nif (has_tiny_unaligned_frags(skb) && __skb_linearize(skb)) {\r\ntxq->tx_dropped++;\r\nnetdev_printk(KERN_DEBUG, dev,\r\n"failed to linearize skb with tiny unaligned fragment\n");\r\nreturn NETDEV_TX_BUSY;\r\n}\r\nif (txq->tx_ring_size - txq->tx_desc_count < MAX_SKB_FRAGS + 1) {\r\nif (net_ratelimit())\r\nnetdev_err(dev, "tx queue full?!\n");\r\nkfree_skb(skb);\r\nreturn NETDEV_TX_OK;\r\n}\r\nlength = skb->len;\r\nif (!txq_submit_skb(txq, skb)) {\r\nint entries_left;\r\ntxq->tx_bytes += length;\r\ntxq->tx_packets++;\r\nentries_left = txq->tx_ring_size - txq->tx_desc_count;\r\nif (entries_left < MAX_SKB_FRAGS + 1)\r\nnetif_tx_stop_queue(nq);\r\n}\r\nreturn NETDEV_TX_OK;\r\n}\r\nstatic void txq_kick(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\r\nu32 hw_desc_ptr;\r\nu32 expected_ptr;\r\n__netif_tx_lock(nq, smp_processor_id());\r\nif (rdlp(mp, TXQ_COMMAND) & (1 << txq->index))\r\ngoto out;\r\nhw_desc_ptr = rdlp(mp, TXQ_CURRENT_DESC_PTR(txq->index));\r\nexpected_ptr = (u32)txq->tx_desc_dma +\r\ntxq->tx_curr_desc * sizeof(struct tx_desc);\r\nif (hw_desc_ptr != expected_ptr)\r\ntxq_enable(txq);\r\nout:\r\n__netif_tx_unlock(nq);\r\nmp->work_tx_end &= ~(1 << txq->index);\r\n}\r\nstatic int txq_reclaim(struct tx_queue *txq, int budget, int force)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nstruct netdev_queue *nq = netdev_get_tx_queue(mp->dev, txq->index);\r\nint reclaimed;\r\n__netif_tx_lock(nq, smp_processor_id());\r\nreclaimed = 0;\r\nwhile (reclaimed < budget && txq->tx_desc_count > 0) {\r\nint tx_index;\r\nstruct tx_desc *desc;\r\nu32 cmd_sts;\r\nstruct sk_buff *skb;\r\ntx_index = txq->tx_used_desc;\r\ndesc = &txq->tx_desc_area[tx_index];\r\ncmd_sts = desc->cmd_sts;\r\nif (cmd_sts & BUFFER_OWNED_BY_DMA) {\r\nif (!force)\r\nbreak;\r\ndesc->cmd_sts = cmd_sts & ~BUFFER_OWNED_BY_DMA;\r\n}\r\ntxq->tx_used_desc = tx_index + 1;\r\nif (txq->tx_used_desc == txq->tx_ring_size)\r\ntxq->tx_used_desc = 0;\r\nreclaimed++;\r\ntxq->tx_desc_count--;\r\nskb = NULL;\r\nif (cmd_sts & TX_LAST_DESC)\r\nskb = __skb_dequeue(&txq->tx_skb);\r\nif (cmd_sts & ERROR_SUMMARY) {\r\nnetdev_info(mp->dev, "tx error\n");\r\nmp->dev->stats.tx_errors++;\r\n}\r\nif (cmd_sts & TX_FIRST_DESC) {\r\ndma_unmap_single(mp->dev->dev.parent, desc->buf_ptr,\r\ndesc->byte_cnt, DMA_TO_DEVICE);\r\n} else {\r\ndma_unmap_page(mp->dev->dev.parent, desc->buf_ptr,\r\ndesc->byte_cnt, DMA_TO_DEVICE);\r\n}\r\ndev_kfree_skb(skb);\r\n}\r\n__netif_tx_unlock(nq);\r\nif (reclaimed < budget)\r\nmp->work_tx &= ~(1 << txq->index);\r\nreturn reclaimed;\r\n}\r\nstatic void tx_set_rate(struct mv643xx_eth_private *mp, int rate, int burst)\r\n{\r\nint token_rate;\r\nint mtu;\r\nint bucket_size;\r\ntoken_rate = ((rate / 1000) * 64) / (mp->t_clk / 1000);\r\nif (token_rate > 1023)\r\ntoken_rate = 1023;\r\nmtu = (mp->dev->mtu + 255) >> 8;\r\nif (mtu > 63)\r\nmtu = 63;\r\nbucket_size = (burst + 255) >> 8;\r\nif (bucket_size > 65535)\r\nbucket_size = 65535;\r\nswitch (mp->shared->tx_bw_control) {\r\ncase TX_BW_CONTROL_OLD_LAYOUT:\r\nwrlp(mp, TX_BW_RATE, token_rate);\r\nwrlp(mp, TX_BW_MTU, mtu);\r\nwrlp(mp, TX_BW_BURST, bucket_size);\r\nbreak;\r\ncase TX_BW_CONTROL_NEW_LAYOUT:\r\nwrlp(mp, TX_BW_RATE_MOVED, token_rate);\r\nwrlp(mp, TX_BW_MTU_MOVED, mtu);\r\nwrlp(mp, TX_BW_BURST_MOVED, bucket_size);\r\nbreak;\r\n}\r\n}\r\nstatic void txq_set_rate(struct tx_queue *txq, int rate, int burst)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint token_rate;\r\nint bucket_size;\r\ntoken_rate = ((rate / 1000) * 64) / (mp->t_clk / 1000);\r\nif (token_rate > 1023)\r\ntoken_rate = 1023;\r\nbucket_size = (burst + 255) >> 8;\r\nif (bucket_size > 65535)\r\nbucket_size = 65535;\r\nwrlp(mp, TXQ_BW_TOKENS(txq->index), token_rate << 14);\r\nwrlp(mp, TXQ_BW_CONF(txq->index), (bucket_size << 10) | token_rate);\r\n}\r\nstatic void txq_set_fixed_prio_mode(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\nint off;\r\nu32 val;\r\noff = 0;\r\nswitch (mp->shared->tx_bw_control) {\r\ncase TX_BW_CONTROL_OLD_LAYOUT:\r\noff = TXQ_FIX_PRIO_CONF;\r\nbreak;\r\ncase TX_BW_CONTROL_NEW_LAYOUT:\r\noff = TXQ_FIX_PRIO_CONF_MOVED;\r\nbreak;\r\n}\r\nif (off) {\r\nval = rdlp(mp, off);\r\nval |= 1 << txq->index;\r\nwrlp(mp, off, val);\r\n}\r\n}\r\nstatic irqreturn_t mv643xx_eth_err_irq(int irq, void *dev_id)\r\n{\r\nstruct mv643xx_eth_shared_private *msp = dev_id;\r\nif (readl(msp->base + ERR_INT_CAUSE) & ERR_INT_SMI_DONE) {\r\nwritel(~ERR_INT_SMI_DONE, msp->base + ERR_INT_CAUSE);\r\nwake_up(&msp->smi_busy_wait);\r\nreturn IRQ_HANDLED;\r\n}\r\nreturn IRQ_NONE;\r\n}\r\nstatic int smi_is_done(struct mv643xx_eth_shared_private *msp)\r\n{\r\nreturn !(readl(msp->base + SMI_REG) & SMI_BUSY);\r\n}\r\nstatic int smi_wait_ready(struct mv643xx_eth_shared_private *msp)\r\n{\r\nif (msp->err_interrupt == NO_IRQ) {\r\nint i;\r\nfor (i = 0; !smi_is_done(msp); i++) {\r\nif (i == 10)\r\nreturn -ETIMEDOUT;\r\nmsleep(10);\r\n}\r\nreturn 0;\r\n}\r\nif (!smi_is_done(msp)) {\r\nwait_event_timeout(msp->smi_busy_wait, smi_is_done(msp),\r\nmsecs_to_jiffies(100));\r\nif (!smi_is_done(msp))\r\nreturn -ETIMEDOUT;\r\n}\r\nreturn 0;\r\n}\r\nstatic int smi_bus_read(struct mii_bus *bus, int addr, int reg)\r\n{\r\nstruct mv643xx_eth_shared_private *msp = bus->priv;\r\nvoid __iomem *smi_reg = msp->base + SMI_REG;\r\nint ret;\r\nif (smi_wait_ready(msp)) {\r\npr_warn("SMI bus busy timeout\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nwritel(SMI_OPCODE_READ | (reg << 21) | (addr << 16), smi_reg);\r\nif (smi_wait_ready(msp)) {\r\npr_warn("SMI bus busy timeout\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nret = readl(smi_reg);\r\nif (!(ret & SMI_READ_VALID)) {\r\npr_warn("SMI bus read not valid\n");\r\nreturn -ENODEV;\r\n}\r\nreturn ret & 0xffff;\r\n}\r\nstatic int smi_bus_write(struct mii_bus *bus, int addr, int reg, u16 val)\r\n{\r\nstruct mv643xx_eth_shared_private *msp = bus->priv;\r\nvoid __iomem *smi_reg = msp->base + SMI_REG;\r\nif (smi_wait_ready(msp)) {\r\npr_warn("SMI bus busy timeout\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nwritel(SMI_OPCODE_WRITE | (reg << 21) |\r\n(addr << 16) | (val & 0xffff), smi_reg);\r\nif (smi_wait_ready(msp)) {\r\npr_warn("SMI bus busy timeout\n");\r\nreturn -ETIMEDOUT;\r\n}\r\nreturn 0;\r\n}\r\nstatic struct net_device_stats *mv643xx_eth_get_stats(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nstruct net_device_stats *stats = &dev->stats;\r\nunsigned long tx_packets = 0;\r\nunsigned long tx_bytes = 0;\r\nunsigned long tx_dropped = 0;\r\nint i;\r\nfor (i = 0; i < mp->txq_count; i++) {\r\nstruct tx_queue *txq = mp->txq + i;\r\ntx_packets += txq->tx_packets;\r\ntx_bytes += txq->tx_bytes;\r\ntx_dropped += txq->tx_dropped;\r\n}\r\nstats->tx_packets = tx_packets;\r\nstats->tx_bytes = tx_bytes;\r\nstats->tx_dropped = tx_dropped;\r\nreturn stats;\r\n}\r\nstatic void mv643xx_eth_grab_lro_stats(struct mv643xx_eth_private *mp)\r\n{\r\nu32 lro_aggregated = 0;\r\nu32 lro_flushed = 0;\r\nu32 lro_no_desc = 0;\r\nint i;\r\nfor (i = 0; i < mp->rxq_count; i++) {\r\nstruct rx_queue *rxq = mp->rxq + i;\r\nlro_aggregated += rxq->lro_mgr.stats.aggregated;\r\nlro_flushed += rxq->lro_mgr.stats.flushed;\r\nlro_no_desc += rxq->lro_mgr.stats.no_desc;\r\n}\r\nmp->lro_counters.lro_aggregated = lro_aggregated;\r\nmp->lro_counters.lro_flushed = lro_flushed;\r\nmp->lro_counters.lro_no_desc = lro_no_desc;\r\n}\r\nstatic inline u32 mib_read(struct mv643xx_eth_private *mp, int offset)\r\n{\r\nreturn rdl(mp, MIB_COUNTERS(mp->port_num) + offset);\r\n}\r\nstatic void mib_counters_clear(struct mv643xx_eth_private *mp)\r\n{\r\nint i;\r\nfor (i = 0; i < 0x80; i += 4)\r\nmib_read(mp, i);\r\nrdlp(mp, RX_DISCARD_FRAME_CNT);\r\nrdlp(mp, RX_OVERRUN_FRAME_CNT);\r\n}\r\nstatic void mib_counters_update(struct mv643xx_eth_private *mp)\r\n{\r\nstruct mib_counters *p = &mp->mib_counters;\r\nspin_lock_bh(&mp->mib_counters_lock);\r\np->good_octets_received += mib_read(mp, 0x00);\r\np->bad_octets_received += mib_read(mp, 0x08);\r\np->internal_mac_transmit_err += mib_read(mp, 0x0c);\r\np->good_frames_received += mib_read(mp, 0x10);\r\np->bad_frames_received += mib_read(mp, 0x14);\r\np->broadcast_frames_received += mib_read(mp, 0x18);\r\np->multicast_frames_received += mib_read(mp, 0x1c);\r\np->frames_64_octets += mib_read(mp, 0x20);\r\np->frames_65_to_127_octets += mib_read(mp, 0x24);\r\np->frames_128_to_255_octets += mib_read(mp, 0x28);\r\np->frames_256_to_511_octets += mib_read(mp, 0x2c);\r\np->frames_512_to_1023_octets += mib_read(mp, 0x30);\r\np->frames_1024_to_max_octets += mib_read(mp, 0x34);\r\np->good_octets_sent += mib_read(mp, 0x38);\r\np->good_frames_sent += mib_read(mp, 0x40);\r\np->excessive_collision += mib_read(mp, 0x44);\r\np->multicast_frames_sent += mib_read(mp, 0x48);\r\np->broadcast_frames_sent += mib_read(mp, 0x4c);\r\np->unrec_mac_control_received += mib_read(mp, 0x50);\r\np->fc_sent += mib_read(mp, 0x54);\r\np->good_fc_received += mib_read(mp, 0x58);\r\np->bad_fc_received += mib_read(mp, 0x5c);\r\np->undersize_received += mib_read(mp, 0x60);\r\np->fragments_received += mib_read(mp, 0x64);\r\np->oversize_received += mib_read(mp, 0x68);\r\np->jabber_received += mib_read(mp, 0x6c);\r\np->mac_receive_error += mib_read(mp, 0x70);\r\np->bad_crc_event += mib_read(mp, 0x74);\r\np->collision += mib_read(mp, 0x78);\r\np->late_collision += mib_read(mp, 0x7c);\r\np->rx_discard += rdlp(mp, RX_DISCARD_FRAME_CNT);\r\np->rx_overrun += rdlp(mp, RX_OVERRUN_FRAME_CNT);\r\nspin_unlock_bh(&mp->mib_counters_lock);\r\nmod_timer(&mp->mib_counters_timer, jiffies + 30 * HZ);\r\n}\r\nstatic void mib_counters_timer_wrapper(unsigned long _mp)\r\n{\r\nstruct mv643xx_eth_private *mp = (void *)_mp;\r\nmib_counters_update(mp);\r\n}\r\nstatic unsigned int get_rx_coal(struct mv643xx_eth_private *mp)\r\n{\r\nu32 val = rdlp(mp, SDMA_CONFIG);\r\nu64 temp;\r\nif (mp->shared->extended_rx_coal_limit)\r\ntemp = ((val & 0x02000000) >> 10) | ((val & 0x003fff80) >> 7);\r\nelse\r\ntemp = (val & 0x003fff00) >> 8;\r\ntemp *= 64000000;\r\ndo_div(temp, mp->t_clk);\r\nreturn (unsigned int)temp;\r\n}\r\nstatic void set_rx_coal(struct mv643xx_eth_private *mp, unsigned int usec)\r\n{\r\nu64 temp;\r\nu32 val;\r\ntemp = (u64)usec * mp->t_clk;\r\ntemp += 31999999;\r\ndo_div(temp, 64000000);\r\nval = rdlp(mp, SDMA_CONFIG);\r\nif (mp->shared->extended_rx_coal_limit) {\r\nif (temp > 0xffff)\r\ntemp = 0xffff;\r\nval &= ~0x023fff80;\r\nval |= (temp & 0x8000) << 10;\r\nval |= (temp & 0x7fff) << 7;\r\n} else {\r\nif (temp > 0x3fff)\r\ntemp = 0x3fff;\r\nval &= ~0x003fff00;\r\nval |= (temp & 0x3fff) << 8;\r\n}\r\nwrlp(mp, SDMA_CONFIG, val);\r\n}\r\nstatic unsigned int get_tx_coal(struct mv643xx_eth_private *mp)\r\n{\r\nu64 temp;\r\ntemp = (rdlp(mp, TX_FIFO_URGENT_THRESHOLD) & 0x3fff0) >> 4;\r\ntemp *= 64000000;\r\ndo_div(temp, mp->t_clk);\r\nreturn (unsigned int)temp;\r\n}\r\nstatic void set_tx_coal(struct mv643xx_eth_private *mp, unsigned int usec)\r\n{\r\nu64 temp;\r\ntemp = (u64)usec * mp->t_clk;\r\ntemp += 31999999;\r\ndo_div(temp, 64000000);\r\nif (temp > 0x3fff)\r\ntemp = 0x3fff;\r\nwrlp(mp, TX_FIFO_URGENT_THRESHOLD, temp << 4);\r\n}\r\nstatic int\r\nmv643xx_eth_get_settings_phy(struct mv643xx_eth_private *mp,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nint err;\r\nerr = phy_read_status(mp->phy);\r\nif (err == 0)\r\nerr = phy_ethtool_gset(mp->phy, cmd);\r\ncmd->supported &= ~SUPPORTED_1000baseT_Half;\r\ncmd->advertising &= ~ADVERTISED_1000baseT_Half;\r\nreturn err;\r\n}\r\nstatic int\r\nmv643xx_eth_get_settings_phyless(struct mv643xx_eth_private *mp,\r\nstruct ethtool_cmd *cmd)\r\n{\r\nu32 port_status;\r\nport_status = rdlp(mp, PORT_STATUS);\r\ncmd->supported = SUPPORTED_MII;\r\ncmd->advertising = ADVERTISED_MII;\r\nswitch (port_status & PORT_SPEED_MASK) {\r\ncase PORT_SPEED_10:\r\nethtool_cmd_speed_set(cmd, SPEED_10);\r\nbreak;\r\ncase PORT_SPEED_100:\r\nethtool_cmd_speed_set(cmd, SPEED_100);\r\nbreak;\r\ncase PORT_SPEED_1000:\r\nethtool_cmd_speed_set(cmd, SPEED_1000);\r\nbreak;\r\ndefault:\r\ncmd->speed = -1;\r\nbreak;\r\n}\r\ncmd->duplex = (port_status & FULL_DUPLEX) ? DUPLEX_FULL : DUPLEX_HALF;\r\ncmd->port = PORT_MII;\r\ncmd->phy_address = 0;\r\ncmd->transceiver = XCVR_INTERNAL;\r\ncmd->autoneg = AUTONEG_DISABLE;\r\ncmd->maxtxpkt = 1;\r\ncmd->maxrxpkt = 1;\r\nreturn 0;\r\n}\r\nstatic int\r\nmv643xx_eth_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (mp->phy != NULL)\r\nreturn mv643xx_eth_get_settings_phy(mp, cmd);\r\nelse\r\nreturn mv643xx_eth_get_settings_phyless(mp, cmd);\r\n}\r\nstatic int\r\nmv643xx_eth_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (mp->phy == NULL)\r\nreturn -EINVAL;\r\ncmd->advertising &= ~ADVERTISED_1000baseT_Half;\r\nreturn phy_ethtool_sset(mp->phy, cmd);\r\n}\r\nstatic void mv643xx_eth_get_drvinfo(struct net_device *dev,\r\nstruct ethtool_drvinfo *drvinfo)\r\n{\r\nstrlcpy(drvinfo->driver, mv643xx_eth_driver_name,\r\nsizeof(drvinfo->driver));\r\nstrlcpy(drvinfo->version, mv643xx_eth_driver_version,\r\nsizeof(drvinfo->version));\r\nstrlcpy(drvinfo->fw_version, "N/A", sizeof(drvinfo->fw_version));\r\nstrlcpy(drvinfo->bus_info, "platform", sizeof(drvinfo->bus_info));\r\ndrvinfo->n_stats = ARRAY_SIZE(mv643xx_eth_stats);\r\n}\r\nstatic int mv643xx_eth_nway_reset(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (mp->phy == NULL)\r\nreturn -EINVAL;\r\nreturn genphy_restart_aneg(mp->phy);\r\n}\r\nstatic int\r\nmv643xx_eth_get_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nec->rx_coalesce_usecs = get_rx_coal(mp);\r\nec->tx_coalesce_usecs = get_tx_coal(mp);\r\nreturn 0;\r\n}\r\nstatic int\r\nmv643xx_eth_set_coalesce(struct net_device *dev, struct ethtool_coalesce *ec)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nset_rx_coal(mp, ec->rx_coalesce_usecs);\r\nset_tx_coal(mp, ec->tx_coalesce_usecs);\r\nreturn 0;\r\n}\r\nstatic void\r\nmv643xx_eth_get_ringparam(struct net_device *dev, struct ethtool_ringparam *er)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\ner->rx_max_pending = 4096;\r\ner->tx_max_pending = 4096;\r\ner->rx_pending = mp->rx_ring_size;\r\ner->tx_pending = mp->tx_ring_size;\r\n}\r\nstatic int\r\nmv643xx_eth_set_ringparam(struct net_device *dev, struct ethtool_ringparam *er)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (er->rx_mini_pending || er->rx_jumbo_pending)\r\nreturn -EINVAL;\r\nmp->rx_ring_size = er->rx_pending < 4096 ? er->rx_pending : 4096;\r\nmp->tx_ring_size = er->tx_pending < 4096 ? er->tx_pending : 4096;\r\nif (netif_running(dev)) {\r\nmv643xx_eth_stop(dev);\r\nif (mv643xx_eth_open(dev)) {\r\nnetdev_err(dev,\r\n"fatal error on re-opening device after ring param change\n");\r\nreturn -ENOMEM;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int\r\nmv643xx_eth_set_features(struct net_device *dev, netdev_features_t features)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nbool rx_csum = features & NETIF_F_RXCSUM;\r\nwrlp(mp, PORT_CONFIG, rx_csum ? 0x02000000 : 0x00000000);\r\nreturn 0;\r\n}\r\nstatic void mv643xx_eth_get_strings(struct net_device *dev,\r\nuint32_t stringset, uint8_t *data)\r\n{\r\nint i;\r\nif (stringset == ETH_SS_STATS) {\r\nfor (i = 0; i < ARRAY_SIZE(mv643xx_eth_stats); i++) {\r\nmemcpy(data + i * ETH_GSTRING_LEN,\r\nmv643xx_eth_stats[i].stat_string,\r\nETH_GSTRING_LEN);\r\n}\r\n}\r\n}\r\nstatic void mv643xx_eth_get_ethtool_stats(struct net_device *dev,\r\nstruct ethtool_stats *stats,\r\nuint64_t *data)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint i;\r\nmv643xx_eth_get_stats(dev);\r\nmib_counters_update(mp);\r\nmv643xx_eth_grab_lro_stats(mp);\r\nfor (i = 0; i < ARRAY_SIZE(mv643xx_eth_stats); i++) {\r\nconst struct mv643xx_eth_stats *stat;\r\nvoid *p;\r\nstat = mv643xx_eth_stats + i;\r\nif (stat->netdev_off >= 0)\r\np = ((void *)mp->dev) + stat->netdev_off;\r\nelse\r\np = ((void *)mp) + stat->mp_off;\r\ndata[i] = (stat->sizeof_stat == 8) ?\r\n*(uint64_t *)p : *(uint32_t *)p;\r\n}\r\n}\r\nstatic int mv643xx_eth_get_sset_count(struct net_device *dev, int sset)\r\n{\r\nif (sset == ETH_SS_STATS)\r\nreturn ARRAY_SIZE(mv643xx_eth_stats);\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic void uc_addr_get(struct mv643xx_eth_private *mp, unsigned char *addr)\r\n{\r\nunsigned int mac_h = rdlp(mp, MAC_ADDR_HIGH);\r\nunsigned int mac_l = rdlp(mp, MAC_ADDR_LOW);\r\naddr[0] = (mac_h >> 24) & 0xff;\r\naddr[1] = (mac_h >> 16) & 0xff;\r\naddr[2] = (mac_h >> 8) & 0xff;\r\naddr[3] = mac_h & 0xff;\r\naddr[4] = (mac_l >> 8) & 0xff;\r\naddr[5] = mac_l & 0xff;\r\n}\r\nstatic void uc_addr_set(struct mv643xx_eth_private *mp, unsigned char *addr)\r\n{\r\nwrlp(mp, MAC_ADDR_HIGH,\r\n(addr[0] << 24) | (addr[1] << 16) | (addr[2] << 8) | addr[3]);\r\nwrlp(mp, MAC_ADDR_LOW, (addr[4] << 8) | addr[5]);\r\n}\r\nstatic u32 uc_addr_filter_mask(struct net_device *dev)\r\n{\r\nstruct netdev_hw_addr *ha;\r\nu32 nibbles;\r\nif (dev->flags & IFF_PROMISC)\r\nreturn 0;\r\nnibbles = 1 << (dev->dev_addr[5] & 0x0f);\r\nnetdev_for_each_uc_addr(ha, dev) {\r\nif (memcmp(dev->dev_addr, ha->addr, 5))\r\nreturn 0;\r\nif ((dev->dev_addr[5] ^ ha->addr[5]) & 0xf0)\r\nreturn 0;\r\nnibbles |= 1 << (ha->addr[5] & 0x0f);\r\n}\r\nreturn nibbles;\r\n}\r\nstatic void mv643xx_eth_program_unicast_filter(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nu32 port_config;\r\nu32 nibbles;\r\nint i;\r\nuc_addr_set(mp, dev->dev_addr);\r\nport_config = rdlp(mp, PORT_CONFIG) & ~UNICAST_PROMISCUOUS_MODE;\r\nnibbles = uc_addr_filter_mask(dev);\r\nif (!nibbles) {\r\nport_config |= UNICAST_PROMISCUOUS_MODE;\r\nnibbles = 0xffff;\r\n}\r\nfor (i = 0; i < 16; i += 4) {\r\nint off = UNICAST_TABLE(mp->port_num) + i;\r\nu32 v;\r\nv = 0;\r\nif (nibbles & 1)\r\nv |= 0x00000001;\r\nif (nibbles & 2)\r\nv |= 0x00000100;\r\nif (nibbles & 4)\r\nv |= 0x00010000;\r\nif (nibbles & 8)\r\nv |= 0x01000000;\r\nnibbles >>= 4;\r\nwrl(mp, off, v);\r\n}\r\nwrlp(mp, PORT_CONFIG, port_config);\r\n}\r\nstatic int addr_crc(unsigned char *addr)\r\n{\r\nint crc = 0;\r\nint i;\r\nfor (i = 0; i < 6; i++) {\r\nint j;\r\ncrc = (crc ^ addr[i]) << 8;\r\nfor (j = 7; j >= 0; j--) {\r\nif (crc & (0x100 << j))\r\ncrc ^= 0x107 << j;\r\n}\r\n}\r\nreturn crc;\r\n}\r\nstatic void mv643xx_eth_program_multicast_filter(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nu32 *mc_spec;\r\nu32 *mc_other;\r\nstruct netdev_hw_addr *ha;\r\nint i;\r\nif (dev->flags & (IFF_PROMISC | IFF_ALLMULTI)) {\r\nint port_num;\r\nu32 accept;\r\noom:\r\nport_num = mp->port_num;\r\naccept = 0x01010101;\r\nfor (i = 0; i < 0x100; i += 4) {\r\nwrl(mp, SPECIAL_MCAST_TABLE(port_num) + i, accept);\r\nwrl(mp, OTHER_MCAST_TABLE(port_num) + i, accept);\r\n}\r\nreturn;\r\n}\r\nmc_spec = kmalloc(0x200, GFP_ATOMIC);\r\nif (mc_spec == NULL)\r\ngoto oom;\r\nmc_other = mc_spec + (0x100 >> 2);\r\nmemset(mc_spec, 0, 0x100);\r\nmemset(mc_other, 0, 0x100);\r\nnetdev_for_each_mc_addr(ha, dev) {\r\nu8 *a = ha->addr;\r\nu32 *table;\r\nint entry;\r\nif (memcmp(a, "\x01\x00\x5e\x00\x00", 5) == 0) {\r\ntable = mc_spec;\r\nentry = a[5];\r\n} else {\r\ntable = mc_other;\r\nentry = addr_crc(a);\r\n}\r\ntable[entry >> 2] |= 1 << (8 * (entry & 3));\r\n}\r\nfor (i = 0; i < 0x100; i += 4) {\r\nwrl(mp, SPECIAL_MCAST_TABLE(mp->port_num) + i, mc_spec[i >> 2]);\r\nwrl(mp, OTHER_MCAST_TABLE(mp->port_num) + i, mc_other[i >> 2]);\r\n}\r\nkfree(mc_spec);\r\n}\r\nstatic void mv643xx_eth_set_rx_mode(struct net_device *dev)\r\n{\r\nmv643xx_eth_program_unicast_filter(dev);\r\nmv643xx_eth_program_multicast_filter(dev);\r\n}\r\nstatic int mv643xx_eth_set_mac_address(struct net_device *dev, void *addr)\r\n{\r\nstruct sockaddr *sa = addr;\r\nif (!is_valid_ether_addr(sa->sa_data))\r\nreturn -EADDRNOTAVAIL;\r\nmemcpy(dev->dev_addr, sa->sa_data, ETH_ALEN);\r\nnetif_addr_lock_bh(dev);\r\nmv643xx_eth_program_unicast_filter(dev);\r\nnetif_addr_unlock_bh(dev);\r\nreturn 0;\r\n}\r\nstatic int rxq_init(struct mv643xx_eth_private *mp, int index)\r\n{\r\nstruct rx_queue *rxq = mp->rxq + index;\r\nstruct rx_desc *rx_desc;\r\nint size;\r\nint i;\r\nrxq->index = index;\r\nrxq->rx_ring_size = mp->rx_ring_size;\r\nrxq->rx_desc_count = 0;\r\nrxq->rx_curr_desc = 0;\r\nrxq->rx_used_desc = 0;\r\nsize = rxq->rx_ring_size * sizeof(struct rx_desc);\r\nif (index == 0 && size <= mp->rx_desc_sram_size) {\r\nrxq->rx_desc_area = ioremap(mp->rx_desc_sram_addr,\r\nmp->rx_desc_sram_size);\r\nrxq->rx_desc_dma = mp->rx_desc_sram_addr;\r\n} else {\r\nrxq->rx_desc_area = dma_alloc_coherent(mp->dev->dev.parent,\r\nsize, &rxq->rx_desc_dma,\r\nGFP_KERNEL);\r\n}\r\nif (rxq->rx_desc_area == NULL) {\r\nnetdev_err(mp->dev,\r\n"can't allocate rx ring (%d bytes)\n", size);\r\ngoto out;\r\n}\r\nmemset(rxq->rx_desc_area, 0, size);\r\nrxq->rx_desc_area_size = size;\r\nrxq->rx_skb = kmalloc(rxq->rx_ring_size * sizeof(*rxq->rx_skb),\r\nGFP_KERNEL);\r\nif (rxq->rx_skb == NULL) {\r\nnetdev_err(mp->dev, "can't allocate rx skb ring\n");\r\ngoto out_free;\r\n}\r\nrx_desc = rxq->rx_desc_area;\r\nfor (i = 0; i < rxq->rx_ring_size; i++) {\r\nint nexti;\r\nnexti = i + 1;\r\nif (nexti == rxq->rx_ring_size)\r\nnexti = 0;\r\nrx_desc[i].next_desc_ptr = rxq->rx_desc_dma +\r\nnexti * sizeof(struct rx_desc);\r\n}\r\nrxq->lro_mgr.dev = mp->dev;\r\nmemset(&rxq->lro_mgr.stats, 0, sizeof(rxq->lro_mgr.stats));\r\nrxq->lro_mgr.features = LRO_F_NAPI;\r\nrxq->lro_mgr.ip_summed = CHECKSUM_UNNECESSARY;\r\nrxq->lro_mgr.ip_summed_aggr = CHECKSUM_UNNECESSARY;\r\nrxq->lro_mgr.max_desc = ARRAY_SIZE(rxq->lro_arr);\r\nrxq->lro_mgr.max_aggr = 32;\r\nrxq->lro_mgr.frag_align_pad = 0;\r\nrxq->lro_mgr.lro_arr = rxq->lro_arr;\r\nrxq->lro_mgr.get_skb_header = mv643xx_get_skb_header;\r\nmemset(&rxq->lro_arr, 0, sizeof(rxq->lro_arr));\r\nreturn 0;\r\nout_free:\r\nif (index == 0 && size <= mp->rx_desc_sram_size)\r\niounmap(rxq->rx_desc_area);\r\nelse\r\ndma_free_coherent(mp->dev->dev.parent, size,\r\nrxq->rx_desc_area,\r\nrxq->rx_desc_dma);\r\nout:\r\nreturn -ENOMEM;\r\n}\r\nstatic void rxq_deinit(struct rx_queue *rxq)\r\n{\r\nstruct mv643xx_eth_private *mp = rxq_to_mp(rxq);\r\nint i;\r\nrxq_disable(rxq);\r\nfor (i = 0; i < rxq->rx_ring_size; i++) {\r\nif (rxq->rx_skb[i]) {\r\ndev_kfree_skb(rxq->rx_skb[i]);\r\nrxq->rx_desc_count--;\r\n}\r\n}\r\nif (rxq->rx_desc_count) {\r\nnetdev_err(mp->dev, "error freeing rx ring -- %d skbs stuck\n",\r\nrxq->rx_desc_count);\r\n}\r\nif (rxq->index == 0 &&\r\nrxq->rx_desc_area_size <= mp->rx_desc_sram_size)\r\niounmap(rxq->rx_desc_area);\r\nelse\r\ndma_free_coherent(mp->dev->dev.parent, rxq->rx_desc_area_size,\r\nrxq->rx_desc_area, rxq->rx_desc_dma);\r\nkfree(rxq->rx_skb);\r\n}\r\nstatic int txq_init(struct mv643xx_eth_private *mp, int index)\r\n{\r\nstruct tx_queue *txq = mp->txq + index;\r\nstruct tx_desc *tx_desc;\r\nint size;\r\nint i;\r\ntxq->index = index;\r\ntxq->tx_ring_size = mp->tx_ring_size;\r\ntxq->tx_desc_count = 0;\r\ntxq->tx_curr_desc = 0;\r\ntxq->tx_used_desc = 0;\r\nsize = txq->tx_ring_size * sizeof(struct tx_desc);\r\nif (index == 0 && size <= mp->tx_desc_sram_size) {\r\ntxq->tx_desc_area = ioremap(mp->tx_desc_sram_addr,\r\nmp->tx_desc_sram_size);\r\ntxq->tx_desc_dma = mp->tx_desc_sram_addr;\r\n} else {\r\ntxq->tx_desc_area = dma_alloc_coherent(mp->dev->dev.parent,\r\nsize, &txq->tx_desc_dma,\r\nGFP_KERNEL);\r\n}\r\nif (txq->tx_desc_area == NULL) {\r\nnetdev_err(mp->dev,\r\n"can't allocate tx ring (%d bytes)\n", size);\r\nreturn -ENOMEM;\r\n}\r\nmemset(txq->tx_desc_area, 0, size);\r\ntxq->tx_desc_area_size = size;\r\ntx_desc = txq->tx_desc_area;\r\nfor (i = 0; i < txq->tx_ring_size; i++) {\r\nstruct tx_desc *txd = tx_desc + i;\r\nint nexti;\r\nnexti = i + 1;\r\nif (nexti == txq->tx_ring_size)\r\nnexti = 0;\r\ntxd->cmd_sts = 0;\r\ntxd->next_desc_ptr = txq->tx_desc_dma +\r\nnexti * sizeof(struct tx_desc);\r\n}\r\nskb_queue_head_init(&txq->tx_skb);\r\nreturn 0;\r\n}\r\nstatic void txq_deinit(struct tx_queue *txq)\r\n{\r\nstruct mv643xx_eth_private *mp = txq_to_mp(txq);\r\ntxq_disable(txq);\r\ntxq_reclaim(txq, txq->tx_ring_size, 1);\r\nBUG_ON(txq->tx_used_desc != txq->tx_curr_desc);\r\nif (txq->index == 0 &&\r\ntxq->tx_desc_area_size <= mp->tx_desc_sram_size)\r\niounmap(txq->tx_desc_area);\r\nelse\r\ndma_free_coherent(mp->dev->dev.parent, txq->tx_desc_area_size,\r\ntxq->tx_desc_area, txq->tx_desc_dma);\r\n}\r\nstatic int mv643xx_eth_collect_events(struct mv643xx_eth_private *mp)\r\n{\r\nu32 int_cause;\r\nu32 int_cause_ext;\r\nint_cause = rdlp(mp, INT_CAUSE) & mp->int_mask;\r\nif (int_cause == 0)\r\nreturn 0;\r\nint_cause_ext = 0;\r\nif (int_cause & INT_EXT) {\r\nint_cause &= ~INT_EXT;\r\nint_cause_ext = rdlp(mp, INT_CAUSE_EXT);\r\n}\r\nif (int_cause) {\r\nwrlp(mp, INT_CAUSE, ~int_cause);\r\nmp->work_tx_end |= ((int_cause & INT_TX_END) >> 19) &\r\n~(rdlp(mp, TXQ_COMMAND) & 0xff);\r\nmp->work_rx |= (int_cause & INT_RX) >> 2;\r\n}\r\nint_cause_ext &= INT_EXT_LINK_PHY | INT_EXT_TX;\r\nif (int_cause_ext) {\r\nwrlp(mp, INT_CAUSE_EXT, ~int_cause_ext);\r\nif (int_cause_ext & INT_EXT_LINK_PHY)\r\nmp->work_link = 1;\r\nmp->work_tx |= int_cause_ext & INT_EXT_TX;\r\n}\r\nreturn 1;\r\n}\r\nstatic irqreturn_t mv643xx_eth_irq(int irq, void *dev_id)\r\n{\r\nstruct net_device *dev = (struct net_device *)dev_id;\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (unlikely(!mv643xx_eth_collect_events(mp)))\r\nreturn IRQ_NONE;\r\nwrlp(mp, INT_MASK, 0);\r\nnapi_schedule(&mp->napi);\r\nreturn IRQ_HANDLED;\r\n}\r\nstatic void handle_link_event(struct mv643xx_eth_private *mp)\r\n{\r\nstruct net_device *dev = mp->dev;\r\nu32 port_status;\r\nint speed;\r\nint duplex;\r\nint fc;\r\nport_status = rdlp(mp, PORT_STATUS);\r\nif (!(port_status & LINK_UP)) {\r\nif (netif_carrier_ok(dev)) {\r\nint i;\r\nnetdev_info(dev, "link down\n");\r\nnetif_carrier_off(dev);\r\nfor (i = 0; i < mp->txq_count; i++) {\r\nstruct tx_queue *txq = mp->txq + i;\r\ntxq_reclaim(txq, txq->tx_ring_size, 1);\r\ntxq_reset_hw_ptr(txq);\r\n}\r\n}\r\nreturn;\r\n}\r\nswitch (port_status & PORT_SPEED_MASK) {\r\ncase PORT_SPEED_10:\r\nspeed = 10;\r\nbreak;\r\ncase PORT_SPEED_100:\r\nspeed = 100;\r\nbreak;\r\ncase PORT_SPEED_1000:\r\nspeed = 1000;\r\nbreak;\r\ndefault:\r\nspeed = -1;\r\nbreak;\r\n}\r\nduplex = (port_status & FULL_DUPLEX) ? 1 : 0;\r\nfc = (port_status & FLOW_CONTROL_ENABLED) ? 1 : 0;\r\nnetdev_info(dev, "link up, %d Mb/s, %s duplex, flow control %sabled\n",\r\nspeed, duplex ? "full" : "half", fc ? "en" : "dis");\r\nif (!netif_carrier_ok(dev))\r\nnetif_carrier_on(dev);\r\n}\r\nstatic int mv643xx_eth_poll(struct napi_struct *napi, int budget)\r\n{\r\nstruct mv643xx_eth_private *mp;\r\nint work_done;\r\nmp = container_of(napi, struct mv643xx_eth_private, napi);\r\nif (unlikely(mp->oom)) {\r\nmp->oom = 0;\r\ndel_timer(&mp->rx_oom);\r\n}\r\nwork_done = 0;\r\nwhile (work_done < budget) {\r\nu8 queue_mask;\r\nint queue;\r\nint work_tbd;\r\nif (mp->work_link) {\r\nmp->work_link = 0;\r\nhandle_link_event(mp);\r\nwork_done++;\r\ncontinue;\r\n}\r\nqueue_mask = mp->work_tx | mp->work_tx_end | mp->work_rx;\r\nif (likely(!mp->oom))\r\nqueue_mask |= mp->work_rx_refill;\r\nif (!queue_mask) {\r\nif (mv643xx_eth_collect_events(mp))\r\ncontinue;\r\nbreak;\r\n}\r\nqueue = fls(queue_mask) - 1;\r\nqueue_mask = 1 << queue;\r\nwork_tbd = budget - work_done;\r\nif (work_tbd > 16)\r\nwork_tbd = 16;\r\nif (mp->work_tx_end & queue_mask) {\r\ntxq_kick(mp->txq + queue);\r\n} else if (mp->work_tx & queue_mask) {\r\nwork_done += txq_reclaim(mp->txq + queue, work_tbd, 0);\r\ntxq_maybe_wake(mp->txq + queue);\r\n} else if (mp->work_rx & queue_mask) {\r\nwork_done += rxq_process(mp->rxq + queue, work_tbd);\r\n} else if (!mp->oom && (mp->work_rx_refill & queue_mask)) {\r\nwork_done += rxq_refill(mp->rxq + queue, work_tbd);\r\n} else {\r\nBUG();\r\n}\r\n}\r\nif (work_done < budget) {\r\nif (mp->oom)\r\nmod_timer(&mp->rx_oom, jiffies + (HZ / 10));\r\nnapi_complete(napi);\r\nwrlp(mp, INT_MASK, mp->int_mask);\r\n}\r\nreturn work_done;\r\n}\r\nstatic inline void oom_timer_wrapper(unsigned long data)\r\n{\r\nstruct mv643xx_eth_private *mp = (void *)data;\r\nnapi_schedule(&mp->napi);\r\n}\r\nstatic void phy_reset(struct mv643xx_eth_private *mp)\r\n{\r\nint data;\r\ndata = phy_read(mp->phy, MII_BMCR);\r\nif (data < 0)\r\nreturn;\r\ndata |= BMCR_RESET;\r\nif (phy_write(mp->phy, MII_BMCR, data) < 0)\r\nreturn;\r\ndo {\r\ndata = phy_read(mp->phy, MII_BMCR);\r\n} while (data >= 0 && data & BMCR_RESET);\r\n}\r\nstatic void port_start(struct mv643xx_eth_private *mp)\r\n{\r\nu32 pscr;\r\nint i;\r\nif (mp->phy != NULL) {\r\nstruct ethtool_cmd cmd;\r\nmv643xx_eth_get_settings(mp->dev, &cmd);\r\nphy_reset(mp);\r\nmv643xx_eth_set_settings(mp->dev, &cmd);\r\n}\r\npscr = rdlp(mp, PORT_SERIAL_CONTROL);\r\npscr |= SERIAL_PORT_ENABLE;\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\npscr |= DO_NOT_FORCE_LINK_FAIL;\r\nif (mp->phy == NULL)\r\npscr |= FORCE_LINK_PASS;\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\ntx_set_rate(mp, 1000000000, 16777216);\r\nfor (i = 0; i < mp->txq_count; i++) {\r\nstruct tx_queue *txq = mp->txq + i;\r\ntxq_reset_hw_ptr(txq);\r\ntxq_set_rate(txq, 1000000000, 16777216);\r\ntxq_set_fixed_prio_mode(txq);\r\n}\r\nmv643xx_eth_set_features(mp->dev, mp->dev->features);\r\nwrlp(mp, PORT_CONFIG_EXT, 0x00000000);\r\nmv643xx_eth_program_unicast_filter(mp->dev);\r\nfor (i = 0; i < mp->rxq_count; i++) {\r\nstruct rx_queue *rxq = mp->rxq + i;\r\nu32 addr;\r\naddr = (u32)rxq->rx_desc_dma;\r\naddr += rxq->rx_curr_desc * sizeof(struct rx_desc);\r\nwrlp(mp, RXQ_CURRENT_DESC_PTR(i), addr);\r\nrxq_enable(rxq);\r\n}\r\n}\r\nstatic void mv643xx_eth_recalc_skb_size(struct mv643xx_eth_private *mp)\r\n{\r\nint skb_size;\r\nskb_size = mp->dev->mtu + 36;\r\nmp->skb_size = (skb_size + 7) & ~7;\r\nmp->skb_size += SKB_DMA_REALIGN;\r\n}\r\nstatic int mv643xx_eth_open(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint err;\r\nint i;\r\nwrlp(mp, INT_CAUSE, 0);\r\nwrlp(mp, INT_CAUSE_EXT, 0);\r\nrdlp(mp, INT_CAUSE_EXT);\r\nerr = request_irq(dev->irq, mv643xx_eth_irq,\r\nIRQF_SHARED, dev->name, dev);\r\nif (err) {\r\nnetdev_err(dev, "can't assign irq\n");\r\nreturn -EAGAIN;\r\n}\r\nmv643xx_eth_recalc_skb_size(mp);\r\nnapi_enable(&mp->napi);\r\nmp->int_mask = INT_EXT;\r\nfor (i = 0; i < mp->rxq_count; i++) {\r\nerr = rxq_init(mp, i);\r\nif (err) {\r\nwhile (--i >= 0)\r\nrxq_deinit(mp->rxq + i);\r\ngoto out;\r\n}\r\nrxq_refill(mp->rxq + i, INT_MAX);\r\nmp->int_mask |= INT_RX_0 << i;\r\n}\r\nif (mp->oom) {\r\nmp->rx_oom.expires = jiffies + (HZ / 10);\r\nadd_timer(&mp->rx_oom);\r\n}\r\nfor (i = 0; i < mp->txq_count; i++) {\r\nerr = txq_init(mp, i);\r\nif (err) {\r\nwhile (--i >= 0)\r\ntxq_deinit(mp->txq + i);\r\ngoto out_free;\r\n}\r\nmp->int_mask |= INT_TX_END_0 << i;\r\n}\r\nport_start(mp);\r\nwrlp(mp, INT_MASK_EXT, INT_EXT_LINK_PHY | INT_EXT_TX);\r\nwrlp(mp, INT_MASK, mp->int_mask);\r\nreturn 0;\r\nout_free:\r\nfor (i = 0; i < mp->rxq_count; i++)\r\nrxq_deinit(mp->rxq + i);\r\nout:\r\nfree_irq(dev->irq, dev);\r\nreturn err;\r\n}\r\nstatic void port_reset(struct mv643xx_eth_private *mp)\r\n{\r\nunsigned int data;\r\nint i;\r\nfor (i = 0; i < mp->rxq_count; i++)\r\nrxq_disable(mp->rxq + i);\r\nfor (i = 0; i < mp->txq_count; i++)\r\ntxq_disable(mp->txq + i);\r\nwhile (1) {\r\nu32 ps = rdlp(mp, PORT_STATUS);\r\nif ((ps & (TX_IN_PROGRESS | TX_FIFO_EMPTY)) == TX_FIFO_EMPTY)\r\nbreak;\r\nudelay(10);\r\n}\r\ndata = rdlp(mp, PORT_SERIAL_CONTROL);\r\ndata &= ~(SERIAL_PORT_ENABLE |\r\nDO_NOT_FORCE_LINK_FAIL |\r\nFORCE_LINK_PASS);\r\nwrlp(mp, PORT_SERIAL_CONTROL, data);\r\n}\r\nstatic int mv643xx_eth_stop(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nint i;\r\nwrlp(mp, INT_MASK_EXT, 0x00000000);\r\nwrlp(mp, INT_MASK, 0x00000000);\r\nrdlp(mp, INT_MASK);\r\nnapi_disable(&mp->napi);\r\ndel_timer_sync(&mp->rx_oom);\r\nnetif_carrier_off(dev);\r\nfree_irq(dev->irq, dev);\r\nport_reset(mp);\r\nmv643xx_eth_get_stats(dev);\r\nmib_counters_update(mp);\r\ndel_timer_sync(&mp->mib_counters_timer);\r\nfor (i = 0; i < mp->rxq_count; i++)\r\nrxq_deinit(mp->rxq + i);\r\nfor (i = 0; i < mp->txq_count; i++)\r\ntxq_deinit(mp->txq + i);\r\nreturn 0;\r\n}\r\nstatic int mv643xx_eth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (mp->phy != NULL)\r\nreturn phy_mii_ioctl(mp->phy, ifr, cmd);\r\nreturn -EOPNOTSUPP;\r\n}\r\nstatic int mv643xx_eth_change_mtu(struct net_device *dev, int new_mtu)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nif (new_mtu < 64 || new_mtu > 9500)\r\nreturn -EINVAL;\r\ndev->mtu = new_mtu;\r\nmv643xx_eth_recalc_skb_size(mp);\r\ntx_set_rate(mp, 1000000000, 16777216);\r\nif (!netif_running(dev))\r\nreturn 0;\r\nmv643xx_eth_stop(dev);\r\nif (mv643xx_eth_open(dev)) {\r\nnetdev_err(dev,\r\n"fatal error on re-opening device after MTU change\n");\r\n}\r\nreturn 0;\r\n}\r\nstatic void tx_timeout_task(struct work_struct *ugly)\r\n{\r\nstruct mv643xx_eth_private *mp;\r\nmp = container_of(ugly, struct mv643xx_eth_private, tx_timeout_task);\r\nif (netif_running(mp->dev)) {\r\nnetif_tx_stop_all_queues(mp->dev);\r\nport_reset(mp);\r\nport_start(mp);\r\nnetif_tx_wake_all_queues(mp->dev);\r\n}\r\n}\r\nstatic void mv643xx_eth_tx_timeout(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nnetdev_info(dev, "tx timeout\n");\r\nschedule_work(&mp->tx_timeout_task);\r\n}\r\nstatic void mv643xx_eth_netpoll(struct net_device *dev)\r\n{\r\nstruct mv643xx_eth_private *mp = netdev_priv(dev);\r\nwrlp(mp, INT_MASK, 0x00000000);\r\nrdlp(mp, INT_MASK);\r\nmv643xx_eth_irq(dev->irq, dev);\r\nwrlp(mp, INT_MASK, mp->int_mask);\r\n}\r\nstatic void\r\nmv643xx_eth_conf_mbus_windows(struct mv643xx_eth_shared_private *msp,\r\nconst struct mbus_dram_target_info *dram)\r\n{\r\nvoid __iomem *base = msp->base;\r\nu32 win_enable;\r\nu32 win_protect;\r\nint i;\r\nfor (i = 0; i < 6; i++) {\r\nwritel(0, base + WINDOW_BASE(i));\r\nwritel(0, base + WINDOW_SIZE(i));\r\nif (i < 4)\r\nwritel(0, base + WINDOW_REMAP_HIGH(i));\r\n}\r\nwin_enable = 0x3f;\r\nwin_protect = 0;\r\nfor (i = 0; i < dram->num_cs; i++) {\r\nconst struct mbus_dram_window *cs = dram->cs + i;\r\nwritel((cs->base & 0xffff0000) |\r\n(cs->mbus_attr << 8) |\r\ndram->mbus_dram_target_id, base + WINDOW_BASE(i));\r\nwritel((cs->size - 1) & 0xffff0000, base + WINDOW_SIZE(i));\r\nwin_enable &= ~(1 << i);\r\nwin_protect |= 3 << (2 * i);\r\n}\r\nwritel(win_enable, base + WINDOW_BAR_ENABLE);\r\nmsp->win_protect = win_protect;\r\n}\r\nstatic void infer_hw_params(struct mv643xx_eth_shared_private *msp)\r\n{\r\nwritel(0x02000000, msp->base + 0x0400 + SDMA_CONFIG);\r\nif (readl(msp->base + 0x0400 + SDMA_CONFIG) & 0x02000000)\r\nmsp->extended_rx_coal_limit = 1;\r\nelse\r\nmsp->extended_rx_coal_limit = 0;\r\nwritel(1, msp->base + 0x0400 + TX_BW_MTU_MOVED);\r\nif (readl(msp->base + 0x0400 + TX_BW_MTU_MOVED) & 1) {\r\nmsp->tx_bw_control = TX_BW_CONTROL_NEW_LAYOUT;\r\n} else {\r\nwritel(7, msp->base + 0x0400 + TX_BW_RATE);\r\nif (readl(msp->base + 0x0400 + TX_BW_RATE) & 7)\r\nmsp->tx_bw_control = TX_BW_CONTROL_OLD_LAYOUT;\r\nelse\r\nmsp->tx_bw_control = TX_BW_CONTROL_ABSENT;\r\n}\r\n}\r\nstatic int mv643xx_eth_shared_probe(struct platform_device *pdev)\r\n{\r\nstatic int mv643xx_eth_version_printed;\r\nstruct mv643xx_eth_shared_platform_data *pd = pdev->dev.platform_data;\r\nstruct mv643xx_eth_shared_private *msp;\r\nconst struct mbus_dram_target_info *dram;\r\nstruct resource *res;\r\nint ret;\r\nif (!mv643xx_eth_version_printed++)\r\npr_notice("MV-643xx 10/100/1000 ethernet driver version %s\n",\r\nmv643xx_eth_driver_version);\r\nret = -EINVAL;\r\nres = platform_get_resource(pdev, IORESOURCE_MEM, 0);\r\nif (res == NULL)\r\ngoto out;\r\nret = -ENOMEM;\r\nmsp = kzalloc(sizeof(*msp), GFP_KERNEL);\r\nif (msp == NULL)\r\ngoto out;\r\nmsp->base = ioremap(res->start, resource_size(res));\r\nif (msp->base == NULL)\r\ngoto out_free;\r\nif (pd == NULL || pd->shared_smi == NULL) {\r\nmsp->smi_bus = mdiobus_alloc();\r\nif (msp->smi_bus == NULL)\r\ngoto out_unmap;\r\nmsp->smi_bus->priv = msp;\r\nmsp->smi_bus->name = "mv643xx_eth smi";\r\nmsp->smi_bus->read = smi_bus_read;\r\nmsp->smi_bus->write = smi_bus_write,\r\nsnprintf(msp->smi_bus->id, MII_BUS_ID_SIZE, "%s-%d",\r\npdev->name, pdev->id);\r\nmsp->smi_bus->parent = &pdev->dev;\r\nmsp->smi_bus->phy_mask = 0xffffffff;\r\nif (mdiobus_register(msp->smi_bus) < 0)\r\ngoto out_free_mii_bus;\r\nmsp->smi = msp;\r\n} else {\r\nmsp->smi = platform_get_drvdata(pd->shared_smi);\r\n}\r\nmsp->err_interrupt = NO_IRQ;\r\ninit_waitqueue_head(&msp->smi_busy_wait);\r\nres = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nif (res != NULL) {\r\nint err;\r\nerr = request_irq(res->start, mv643xx_eth_err_irq,\r\nIRQF_SHARED, "mv643xx_eth", msp);\r\nif (!err) {\r\nwritel(ERR_INT_SMI_DONE, msp->base + ERR_INT_MASK);\r\nmsp->err_interrupt = res->start;\r\n}\r\n}\r\ndram = mv_mbus_dram_info();\r\nif (dram)\r\nmv643xx_eth_conf_mbus_windows(msp, dram);\r\nmsp->tx_csum_limit = (pd != NULL && pd->tx_csum_limit) ?\r\npd->tx_csum_limit : 9 * 1024;\r\ninfer_hw_params(msp);\r\nplatform_set_drvdata(pdev, msp);\r\nreturn 0;\r\nout_free_mii_bus:\r\nmdiobus_free(msp->smi_bus);\r\nout_unmap:\r\niounmap(msp->base);\r\nout_free:\r\nkfree(msp);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int mv643xx_eth_shared_remove(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_shared_private *msp = platform_get_drvdata(pdev);\r\nstruct mv643xx_eth_shared_platform_data *pd = pdev->dev.platform_data;\r\nif (pd == NULL || pd->shared_smi == NULL) {\r\nmdiobus_unregister(msp->smi_bus);\r\nmdiobus_free(msp->smi_bus);\r\n}\r\nif (msp->err_interrupt != NO_IRQ)\r\nfree_irq(msp->err_interrupt, msp);\r\niounmap(msp->base);\r\nkfree(msp);\r\nreturn 0;\r\n}\r\nstatic void phy_addr_set(struct mv643xx_eth_private *mp, int phy_addr)\r\n{\r\nint addr_shift = 5 * mp->port_num;\r\nu32 data;\r\ndata = rdl(mp, PHY_ADDR);\r\ndata &= ~(0x1f << addr_shift);\r\ndata |= (phy_addr & 0x1f) << addr_shift;\r\nwrl(mp, PHY_ADDR, data);\r\n}\r\nstatic int phy_addr_get(struct mv643xx_eth_private *mp)\r\n{\r\nunsigned int data;\r\ndata = rdl(mp, PHY_ADDR);\r\nreturn (data >> (5 * mp->port_num)) & 0x1f;\r\n}\r\nstatic void set_params(struct mv643xx_eth_private *mp,\r\nstruct mv643xx_eth_platform_data *pd)\r\n{\r\nstruct net_device *dev = mp->dev;\r\nif (is_valid_ether_addr(pd->mac_addr))\r\nmemcpy(dev->dev_addr, pd->mac_addr, 6);\r\nelse\r\nuc_addr_get(mp, dev->dev_addr);\r\nmp->rx_ring_size = DEFAULT_RX_QUEUE_SIZE;\r\nif (pd->rx_queue_size)\r\nmp->rx_ring_size = pd->rx_queue_size;\r\nmp->rx_desc_sram_addr = pd->rx_sram_addr;\r\nmp->rx_desc_sram_size = pd->rx_sram_size;\r\nmp->rxq_count = pd->rx_queue_count ? : 1;\r\nmp->tx_ring_size = DEFAULT_TX_QUEUE_SIZE;\r\nif (pd->tx_queue_size)\r\nmp->tx_ring_size = pd->tx_queue_size;\r\nmp->tx_desc_sram_addr = pd->tx_sram_addr;\r\nmp->tx_desc_sram_size = pd->tx_sram_size;\r\nmp->txq_count = pd->tx_queue_count ? : 1;\r\n}\r\nstatic struct phy_device *phy_scan(struct mv643xx_eth_private *mp,\r\nint phy_addr)\r\n{\r\nstruct mii_bus *bus = mp->shared->smi->smi_bus;\r\nstruct phy_device *phydev;\r\nint start;\r\nint num;\r\nint i;\r\nif (phy_addr == MV643XX_ETH_PHY_ADDR_DEFAULT) {\r\nstart = phy_addr_get(mp) & 0x1f;\r\nnum = 32;\r\n} else {\r\nstart = phy_addr & 0x1f;\r\nnum = 1;\r\n}\r\nphydev = NULL;\r\nfor (i = 0; i < num; i++) {\r\nint addr = (start + i) & 0x1f;\r\nif (bus->phy_map[addr] == NULL)\r\nmdiobus_scan(bus, addr);\r\nif (phydev == NULL) {\r\nphydev = bus->phy_map[addr];\r\nif (phydev != NULL)\r\nphy_addr_set(mp, addr);\r\n}\r\n}\r\nreturn phydev;\r\n}\r\nstatic void phy_init(struct mv643xx_eth_private *mp, int speed, int duplex)\r\n{\r\nstruct phy_device *phy = mp->phy;\r\nphy_reset(mp);\r\nphy_attach(mp->dev, dev_name(&phy->dev), 0, PHY_INTERFACE_MODE_GMII);\r\nif (speed == 0) {\r\nphy->autoneg = AUTONEG_ENABLE;\r\nphy->speed = 0;\r\nphy->duplex = 0;\r\nphy->advertising = phy->supported | ADVERTISED_Autoneg;\r\n} else {\r\nphy->autoneg = AUTONEG_DISABLE;\r\nphy->advertising = 0;\r\nphy->speed = speed;\r\nphy->duplex = duplex;\r\n}\r\nphy_start_aneg(phy);\r\n}\r\nstatic void init_pscr(struct mv643xx_eth_private *mp, int speed, int duplex)\r\n{\r\nu32 pscr;\r\npscr = rdlp(mp, PORT_SERIAL_CONTROL);\r\nif (pscr & SERIAL_PORT_ENABLE) {\r\npscr &= ~SERIAL_PORT_ENABLE;\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\n}\r\npscr = MAX_RX_PACKET_9700BYTE | SERIAL_PORT_CONTROL_RESERVED;\r\nif (mp->phy == NULL) {\r\npscr |= DISABLE_AUTO_NEG_SPEED_GMII;\r\nif (speed == SPEED_1000)\r\npscr |= SET_GMII_SPEED_TO_1000;\r\nelse if (speed == SPEED_100)\r\npscr |= SET_MII_SPEED_TO_100;\r\npscr |= DISABLE_AUTO_NEG_FOR_FLOW_CTRL;\r\npscr |= DISABLE_AUTO_NEG_FOR_DUPLEX;\r\nif (duplex == DUPLEX_FULL)\r\npscr |= SET_FULL_DUPLEX_MODE;\r\n}\r\nwrlp(mp, PORT_SERIAL_CONTROL, pscr);\r\n}\r\nstatic int mv643xx_eth_probe(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_platform_data *pd;\r\nstruct mv643xx_eth_private *mp;\r\nstruct net_device *dev;\r\nstruct resource *res;\r\nint err;\r\npd = pdev->dev.platform_data;\r\nif (pd == NULL) {\r\ndev_err(&pdev->dev, "no mv643xx_eth_platform_data\n");\r\nreturn -ENODEV;\r\n}\r\nif (pd->shared == NULL) {\r\ndev_err(&pdev->dev, "no mv643xx_eth_platform_data->shared\n");\r\nreturn -ENODEV;\r\n}\r\ndev = alloc_etherdev_mq(sizeof(struct mv643xx_eth_private), 8);\r\nif (!dev)\r\nreturn -ENOMEM;\r\nmp = netdev_priv(dev);\r\nplatform_set_drvdata(pdev, mp);\r\nmp->shared = platform_get_drvdata(pd->shared);\r\nmp->base = mp->shared->base + 0x0400 + (pd->port_number << 10);\r\nmp->port_num = pd->port_number;\r\nmp->dev = dev;\r\nmp->t_clk = 133000000;\r\n#if defined(CONFIG_HAVE_CLK)\r\nmp->clk = clk_get(&pdev->dev, (pdev->id ? "1" : "0"));\r\nif (!IS_ERR(mp->clk)) {\r\nclk_prepare_enable(mp->clk);\r\nmp->t_clk = clk_get_rate(mp->clk);\r\n}\r\n#endif\r\nset_params(mp, pd);\r\nnetif_set_real_num_tx_queues(dev, mp->txq_count);\r\nnetif_set_real_num_rx_queues(dev, mp->rxq_count);\r\nif (pd->phy_addr != MV643XX_ETH_PHY_NONE)\r\nmp->phy = phy_scan(mp, pd->phy_addr);\r\nif (mp->phy != NULL)\r\nphy_init(mp, pd->speed, pd->duplex);\r\nSET_ETHTOOL_OPS(dev, &mv643xx_eth_ethtool_ops);\r\ninit_pscr(mp, pd->speed, pd->duplex);\r\nmib_counters_clear(mp);\r\ninit_timer(&mp->mib_counters_timer);\r\nmp->mib_counters_timer.data = (unsigned long)mp;\r\nmp->mib_counters_timer.function = mib_counters_timer_wrapper;\r\nmp->mib_counters_timer.expires = jiffies + 30 * HZ;\r\nadd_timer(&mp->mib_counters_timer);\r\nspin_lock_init(&mp->mib_counters_lock);\r\nINIT_WORK(&mp->tx_timeout_task, tx_timeout_task);\r\nnetif_napi_add(dev, &mp->napi, mv643xx_eth_poll, 128);\r\ninit_timer(&mp->rx_oom);\r\nmp->rx_oom.data = (unsigned long)mp;\r\nmp->rx_oom.function = oom_timer_wrapper;\r\nres = platform_get_resource(pdev, IORESOURCE_IRQ, 0);\r\nBUG_ON(!res);\r\ndev->irq = res->start;\r\ndev->netdev_ops = &mv643xx_eth_netdev_ops;\r\ndev->watchdog_timeo = 2 * HZ;\r\ndev->base_addr = 0;\r\ndev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM |\r\nNETIF_F_RXCSUM | NETIF_F_LRO;\r\ndev->features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_RXCSUM;\r\ndev->vlan_features = NETIF_F_SG | NETIF_F_IP_CSUM;\r\ndev->priv_flags |= IFF_UNICAST_FLT;\r\nSET_NETDEV_DEV(dev, &pdev->dev);\r\nif (mp->shared->win_protect)\r\nwrl(mp, WINDOW_PROTECT(mp->port_num), mp->shared->win_protect);\r\nnetif_carrier_off(dev);\r\nwrlp(mp, SDMA_CONFIG, PORT_SDMA_CONFIG_DEFAULT_VALUE);\r\nset_rx_coal(mp, 250);\r\nset_tx_coal(mp, 0);\r\nerr = register_netdev(dev);\r\nif (err)\r\ngoto out;\r\nnetdev_notice(dev, "port %d with MAC address %pM\n",\r\nmp->port_num, dev->dev_addr);\r\nif (mp->tx_desc_sram_size > 0)\r\nnetdev_notice(dev, "configured with sram\n");\r\nreturn 0;\r\nout:\r\n#if defined(CONFIG_HAVE_CLK)\r\nif (!IS_ERR(mp->clk)) {\r\nclk_disable_unprepare(mp->clk);\r\nclk_put(mp->clk);\r\n}\r\n#endif\r\nfree_netdev(dev);\r\nreturn err;\r\n}\r\nstatic int mv643xx_eth_remove(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_private *mp = platform_get_drvdata(pdev);\r\nunregister_netdev(mp->dev);\r\nif (mp->phy != NULL)\r\nphy_detach(mp->phy);\r\ncancel_work_sync(&mp->tx_timeout_task);\r\n#if defined(CONFIG_HAVE_CLK)\r\nif (!IS_ERR(mp->clk)) {\r\nclk_disable_unprepare(mp->clk);\r\nclk_put(mp->clk);\r\n}\r\n#endif\r\nfree_netdev(mp->dev);\r\nplatform_set_drvdata(pdev, NULL);\r\nreturn 0;\r\n}\r\nstatic void mv643xx_eth_shutdown(struct platform_device *pdev)\r\n{\r\nstruct mv643xx_eth_private *mp = platform_get_drvdata(pdev);\r\nwrlp(mp, INT_MASK, 0);\r\nrdlp(mp, INT_MASK);\r\nif (netif_running(mp->dev))\r\nport_reset(mp);\r\n}\r\nstatic int __init mv643xx_eth_init_module(void)\r\n{\r\nint rc;\r\nrc = platform_driver_register(&mv643xx_eth_shared_driver);\r\nif (!rc) {\r\nrc = platform_driver_register(&mv643xx_eth_driver);\r\nif (rc)\r\nplatform_driver_unregister(&mv643xx_eth_shared_driver);\r\n}\r\nreturn rc;\r\n}\r\nstatic void __exit mv643xx_eth_cleanup_module(void)\r\n{\r\nplatform_driver_unregister(&mv643xx_eth_driver);\r\nplatform_driver_unregister(&mv643xx_eth_shared_driver);\r\n}
