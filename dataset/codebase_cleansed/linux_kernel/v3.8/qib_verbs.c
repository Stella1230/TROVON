static inline struct qib_ucontext *to_iucontext(struct ib_ucontext\r\n*ibucontext)\r\n{\r\nreturn container_of(ibucontext, struct qib_ucontext, ibucontext);\r\n}\r\nvoid qib_copy_sge(struct qib_sge_state *ss, void *data, u32 length, int release)\r\n{\r\nstruct qib_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nmemcpy(sge->vaddr, data, len);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (release)\r\nqib_put_mr(sge->mr);\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= QIB_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\ndata += len;\r\nlength -= len;\r\n}\r\n}\r\nvoid qib_skip_sge(struct qib_sge_state *ss, u32 length, int release)\r\n{\r\nstruct qib_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (release)\r\nqib_put_mr(sge->mr);\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= QIB_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\nlength -= len;\r\n}\r\n}\r\nstatic u32 qib_count_sge(struct qib_sge_state *ss, u32 length)\r\n{\r\nstruct qib_sge *sg_list = ss->sg_list;\r\nstruct qib_sge sge = ss->sge;\r\nu8 num_sge = ss->num_sge;\r\nu32 ndesc = 1;\r\nwhile (length) {\r\nu32 len = sge.length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge.sge_length)\r\nlen = sge.sge_length;\r\nBUG_ON(len == 0);\r\nif (((long) sge.vaddr & (sizeof(u32) - 1)) ||\r\n(len != length && (len & (sizeof(u32) - 1)))) {\r\nndesc = 0;\r\nbreak;\r\n}\r\nndesc++;\r\nsge.vaddr += len;\r\nsge.length -= len;\r\nsge.sge_length -= len;\r\nif (sge.sge_length == 0) {\r\nif (--num_sge)\r\nsge = *sg_list++;\r\n} else if (sge.length == 0 && sge.mr->lkey) {\r\nif (++sge.n >= QIB_SEGSZ) {\r\nif (++sge.m >= sge.mr->mapsz)\r\nbreak;\r\nsge.n = 0;\r\n}\r\nsge.vaddr =\r\nsge.mr->map[sge.m]->segs[sge.n].vaddr;\r\nsge.length =\r\nsge.mr->map[sge.m]->segs[sge.n].length;\r\n}\r\nlength -= len;\r\n}\r\nreturn ndesc;\r\n}\r\nstatic void qib_copy_from_sge(void *data, struct qib_sge_state *ss, u32 length)\r\n{\r\nstruct qib_sge *sge = &ss->sge;\r\nwhile (length) {\r\nu32 len = sge->length;\r\nif (len > length)\r\nlen = length;\r\nif (len > sge->sge_length)\r\nlen = sge->sge_length;\r\nBUG_ON(len == 0);\r\nmemcpy(data, sge->vaddr, len);\r\nsge->vaddr += len;\r\nsge->length -= len;\r\nsge->sge_length -= len;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= QIB_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nbreak;\r\nsge->n = 0;\r\n}\r\nsge->vaddr =\r\nsge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length =\r\nsge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\ndata += len;\r\nlength -= len;\r\n}\r\n}\r\nstatic int qib_post_one_send(struct qib_qp *qp, struct ib_send_wr *wr,\r\nint *scheduled)\r\n{\r\nstruct qib_swqe *wqe;\r\nu32 next;\r\nint i;\r\nint j;\r\nint acc;\r\nint ret;\r\nunsigned long flags;\r\nstruct qib_lkey_table *rkt;\r\nstruct qib_pd *pd;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (unlikely(!(ib_qib_state_ops[qp->state] & QIB_POST_SEND_OK)))\r\ngoto bail_inval;\r\nif (wr->num_sge > qp->s_max_sge)\r\ngoto bail_inval;\r\nif (wr->opcode == IB_WR_FAST_REG_MR) {\r\nif (qib_fast_reg_mr(qp, wr))\r\ngoto bail_inval;\r\n} else if (qp->ibqp.qp_type == IB_QPT_UC) {\r\nif ((unsigned) wr->opcode >= IB_WR_RDMA_READ)\r\ngoto bail_inval;\r\n} else if (qp->ibqp.qp_type != IB_QPT_RC) {\r\nif (wr->opcode != IB_WR_SEND &&\r\nwr->opcode != IB_WR_SEND_WITH_IMM)\r\ngoto bail_inval;\r\nif (qp->ibqp.pd != wr->wr.ud.ah->pd)\r\ngoto bail_inval;\r\n} else if ((unsigned) wr->opcode > IB_WR_ATOMIC_FETCH_AND_ADD)\r\ngoto bail_inval;\r\nelse if (wr->opcode >= IB_WR_ATOMIC_CMP_AND_SWP &&\r\n(wr->num_sge == 0 ||\r\nwr->sg_list[0].length < sizeof(u64) ||\r\nwr->sg_list[0].addr & (sizeof(u64) - 1)))\r\ngoto bail_inval;\r\nelse if (wr->opcode >= IB_WR_RDMA_READ && !qp->s_max_rd_atomic)\r\ngoto bail_inval;\r\nnext = qp->s_head + 1;\r\nif (next >= qp->s_size)\r\nnext = 0;\r\nif (next == qp->s_last) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nrkt = &to_idev(qp->ibqp.device)->lk_table;\r\npd = to_ipd(qp->ibqp.pd);\r\nwqe = get_swqe_ptr(qp, qp->s_head);\r\nwqe->wr = *wr;\r\nwqe->length = 0;\r\nj = 0;\r\nif (wr->num_sge) {\r\nacc = wr->opcode >= IB_WR_RDMA_READ ?\r\nIB_ACCESS_LOCAL_WRITE : 0;\r\nfor (i = 0; i < wr->num_sge; i++) {\r\nu32 length = wr->sg_list[i].length;\r\nint ok;\r\nif (length == 0)\r\ncontinue;\r\nok = qib_lkey_ok(rkt, pd, &wqe->sg_list[j],\r\n&wr->sg_list[i], acc);\r\nif (!ok)\r\ngoto bail_inval_free;\r\nwqe->length += length;\r\nj++;\r\n}\r\nwqe->wr.num_sge = j;\r\n}\r\nif (qp->ibqp.qp_type == IB_QPT_UC ||\r\nqp->ibqp.qp_type == IB_QPT_RC) {\r\nif (wqe->length > 0x80000000U)\r\ngoto bail_inval_free;\r\n} else if (wqe->length > (dd_from_ibdev(qp->ibqp.device)->pport +\r\nqp->port_num - 1)->ibmtu)\r\ngoto bail_inval_free;\r\nelse\r\natomic_inc(&to_iah(wr->wr.ud.ah)->refcount);\r\nwqe->ssn = qp->s_ssn++;\r\nqp->s_head = next;\r\nret = 0;\r\ngoto bail;\r\nbail_inval_free:\r\nwhile (j) {\r\nstruct qib_sge *sge = &wqe->sg_list[--j];\r\nqib_put_mr(sge->mr);\r\n}\r\nbail_inval:\r\nret = -EINVAL;\r\nbail:\r\nif (!ret && !wr->next &&\r\n!qib_sdma_empty(\r\ndd_from_ibdev(qp->ibqp.device)->pport + qp->port_num - 1)) {\r\nqib_schedule_send(qp);\r\n*scheduled = 1;\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int qib_post_send(struct ib_qp *ibqp, struct ib_send_wr *wr,\r\nstruct ib_send_wr **bad_wr)\r\n{\r\nstruct qib_qp *qp = to_iqp(ibqp);\r\nint err = 0;\r\nint scheduled = 0;\r\nfor (; wr; wr = wr->next) {\r\nerr = qib_post_one_send(qp, wr, &scheduled);\r\nif (err) {\r\n*bad_wr = wr;\r\ngoto bail;\r\n}\r\n}\r\nif (!scheduled)\r\nqib_do_send(&qp->s_work);\r\nbail:\r\nreturn err;\r\n}\r\nstatic int qib_post_receive(struct ib_qp *ibqp, struct ib_recv_wr *wr,\r\nstruct ib_recv_wr **bad_wr)\r\n{\r\nstruct qib_qp *qp = to_iqp(ibqp);\r\nstruct qib_rwq *wq = qp->r_rq.wq;\r\nunsigned long flags;\r\nint ret;\r\nif (!(ib_qib_state_ops[qp->state] & QIB_POST_RECV_OK) || !wq) {\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nfor (; wr; wr = wr->next) {\r\nstruct qib_rwqe *wqe;\r\nu32 next;\r\nint i;\r\nif ((unsigned) wr->num_sge > qp->r_rq.max_sge) {\r\n*bad_wr = wr;\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nspin_lock_irqsave(&qp->r_rq.lock, flags);\r\nnext = wq->head + 1;\r\nif (next >= qp->r_rq.size)\r\nnext = 0;\r\nif (next == wq->tail) {\r\nspin_unlock_irqrestore(&qp->r_rq.lock, flags);\r\n*bad_wr = wr;\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nwqe = get_rwqe_ptr(&qp->r_rq, wq->head);\r\nwqe->wr_id = wr->wr_id;\r\nwqe->num_sge = wr->num_sge;\r\nfor (i = 0; i < wr->num_sge; i++)\r\nwqe->sg_list[i] = wr->sg_list[i];\r\nsmp_wmb();\r\nwq->head = next;\r\nspin_unlock_irqrestore(&qp->r_rq.lock, flags);\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void qib_qp_rcv(struct qib_ctxtdata *rcd, struct qib_ib_header *hdr,\r\nint has_grh, void *data, u32 tlen, struct qib_qp *qp)\r\n{\r\nstruct qib_ibport *ibp = &rcd->ppd->ibport_data;\r\nspin_lock(&qp->r_lock);\r\nif (!(ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK)) {\r\nibp->n_pkt_drops++;\r\ngoto unlock;\r\n}\r\nswitch (qp->ibqp.qp_type) {\r\ncase IB_QPT_SMI:\r\ncase IB_QPT_GSI:\r\nif (ib_qib_disable_sma)\r\nbreak;\r\ncase IB_QPT_UD:\r\nqib_ud_rcv(ibp, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ncase IB_QPT_RC:\r\nqib_rc_rcv(rcd, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ncase IB_QPT_UC:\r\nqib_uc_rcv(ibp, hdr, has_grh, data, tlen, qp);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nunlock:\r\nspin_unlock(&qp->r_lock);\r\n}\r\nvoid qib_ib_rcv(struct qib_ctxtdata *rcd, void *rhdr, void *data, u32 tlen)\r\n{\r\nstruct qib_pportdata *ppd = rcd->ppd;\r\nstruct qib_ibport *ibp = &ppd->ibport_data;\r\nstruct qib_ib_header *hdr = rhdr;\r\nstruct qib_other_headers *ohdr;\r\nstruct qib_qp *qp;\r\nu32 qp_num;\r\nint lnh;\r\nu8 opcode;\r\nu16 lid;\r\nif (unlikely(tlen < 24))\r\ngoto drop;\r\nlid = be16_to_cpu(hdr->lrh[1]);\r\nif (lid < QIB_MULTICAST_LID_BASE) {\r\nlid &= ~((1 << ppd->lmc) - 1);\r\nif (unlikely(lid != ppd->lid))\r\ngoto drop;\r\n}\r\nlnh = be16_to_cpu(hdr->lrh[0]) & 3;\r\nif (lnh == QIB_LRH_BTH)\r\nohdr = &hdr->u.oth;\r\nelse if (lnh == QIB_LRH_GRH) {\r\nu32 vtf;\r\nohdr = &hdr->u.l.oth;\r\nif (hdr->u.l.grh.next_hdr != IB_GRH_NEXT_HDR)\r\ngoto drop;\r\nvtf = be32_to_cpu(hdr->u.l.grh.version_tclass_flow);\r\nif ((vtf >> IB_GRH_VERSION_SHIFT) != IB_GRH_VERSION)\r\ngoto drop;\r\n} else\r\ngoto drop;\r\nopcode = be32_to_cpu(ohdr->bth[0]) >> 24;\r\nibp->opstats[opcode & 0x7f].n_bytes += tlen;\r\nibp->opstats[opcode & 0x7f].n_packets++;\r\nqp_num = be32_to_cpu(ohdr->bth[1]) & QIB_QPN_MASK;\r\nif (qp_num == QIB_MULTICAST_QPN) {\r\nstruct qib_mcast *mcast;\r\nstruct qib_mcast_qp *p;\r\nif (lnh != QIB_LRH_GRH)\r\ngoto drop;\r\nmcast = qib_mcast_find(ibp, &hdr->u.l.grh.dgid);\r\nif (mcast == NULL)\r\ngoto drop;\r\nibp->n_multicast_rcv++;\r\nlist_for_each_entry_rcu(p, &mcast->qp_list, list)\r\nqib_qp_rcv(rcd, hdr, 1, data, tlen, p->qp);\r\nif (atomic_dec_return(&mcast->refcount) <= 1)\r\nwake_up(&mcast->wait);\r\n} else {\r\nif (rcd->lookaside_qp) {\r\nif (rcd->lookaside_qpn != qp_num) {\r\nif (atomic_dec_and_test(\r\n&rcd->lookaside_qp->refcount))\r\nwake_up(\r\n&rcd->lookaside_qp->wait);\r\nrcd->lookaside_qp = NULL;\r\n}\r\n}\r\nif (!rcd->lookaside_qp) {\r\nqp = qib_lookup_qpn(ibp, qp_num);\r\nif (!qp)\r\ngoto drop;\r\nrcd->lookaside_qp = qp;\r\nrcd->lookaside_qpn = qp_num;\r\n} else\r\nqp = rcd->lookaside_qp;\r\nibp->n_unicast_rcv++;\r\nqib_qp_rcv(rcd, hdr, lnh == QIB_LRH_GRH, data, tlen, qp);\r\n}\r\nreturn;\r\ndrop:\r\nibp->n_pkt_drops++;\r\n}\r\nstatic void mem_timer(unsigned long data)\r\n{\r\nstruct qib_ibdev *dev = (struct qib_ibdev *) data;\r\nstruct list_head *list = &dev->memwait;\r\nstruct qib_qp *qp = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->pending_lock, flags);\r\nif (!list_empty(list)) {\r\nqp = list_entry(list->next, struct qib_qp, iowait);\r\nlist_del_init(&qp->iowait);\r\natomic_inc(&qp->refcount);\r\nif (!list_empty(list))\r\nmod_timer(&dev->mem_timer, jiffies + 1);\r\n}\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\nif (qp) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->s_flags & QIB_S_WAIT_KMEM) {\r\nqp->s_flags &= ~QIB_S_WAIT_KMEM;\r\nqib_schedule_send(qp);\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\n}\r\nstatic void update_sge(struct qib_sge_state *ss, u32 length)\r\n{\r\nstruct qib_sge *sge = &ss->sge;\r\nsge->vaddr += length;\r\nsge->length -= length;\r\nsge->sge_length -= length;\r\nif (sge->sge_length == 0) {\r\nif (--ss->num_sge)\r\n*sge = *ss->sg_list++;\r\n} else if (sge->length == 0 && sge->mr->lkey) {\r\nif (++sge->n >= QIB_SEGSZ) {\r\nif (++sge->m >= sge->mr->mapsz)\r\nreturn;\r\nsge->n = 0;\r\n}\r\nsge->vaddr = sge->mr->map[sge->m]->segs[sge->n].vaddr;\r\nsge->length = sge->mr->map[sge->m]->segs[sge->n].length;\r\n}\r\n}\r\nstatic inline u32 get_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data >> shift;\r\n}\r\nstatic inline u32 set_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data << shift;\r\n}\r\nstatic inline u32 clear_upper_bytes(u32 data, u32 n, u32 off)\r\n{\r\ndata <<= ((sizeof(u32) - n) * BITS_PER_BYTE);\r\ndata >>= ((sizeof(u32) - n - off) * BITS_PER_BYTE);\r\nreturn data;\r\n}\r\nstatic inline u32 get_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data << shift;\r\n}\r\nstatic inline u32 set_upper_bits(u32 data, u32 shift)\r\n{\r\nreturn data >> shift;\r\n}\r\nstatic inline u32 clear_upper_bytes(u32 data, u32 n, u32 off)\r\n{\r\ndata >>= ((sizeof(u32) - n) * BITS_PER_BYTE);\r\ndata <<= ((sizeof(u32) - n - off) * BITS_PER_BYTE);\r\nreturn data;\r\n}\r\nstatic void copy_io(u32 __iomem *piobuf, struct qib_sge_state *ss,\r\nu32 length, unsigned flush_wc)\r\n{\r\nu32 extra = 0;\r\nu32 data = 0;\r\nu32 last;\r\nwhile (1) {\r\nu32 len = ss->sge.length;\r\nu32 off;\r\nif (len > length)\r\nlen = length;\r\nif (len > ss->sge.sge_length)\r\nlen = ss->sge.sge_length;\r\nBUG_ON(len == 0);\r\noff = (unsigned long)ss->sge.vaddr & (sizeof(u32) - 1);\r\nif (off) {\r\nu32 *addr = (u32 *)((unsigned long)ss->sge.vaddr &\r\n~(sizeof(u32) - 1));\r\nu32 v = get_upper_bits(*addr, off * BITS_PER_BYTE);\r\nu32 y;\r\ny = sizeof(u32) - off;\r\nif (len > y)\r\nlen = y;\r\nif (len + extra >= sizeof(u32)) {\r\ndata |= set_upper_bits(v, extra *\r\nBITS_PER_BYTE);\r\nlen = sizeof(u32) - extra;\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n__raw_writel(data, piobuf);\r\npiobuf++;\r\nextra = 0;\r\ndata = 0;\r\n} else {\r\ndata |= clear_upper_bytes(v, len, extra);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\nextra += len;\r\n}\r\n} else if (extra) {\r\nu32 *addr = (u32 *) ss->sge.vaddr;\r\nint shift = extra * BITS_PER_BYTE;\r\nint ushift = 32 - shift;\r\nu32 l = len;\r\nwhile (l >= sizeof(u32)) {\r\nu32 v = *addr;\r\ndata |= set_upper_bits(v, shift);\r\n__raw_writel(data, piobuf);\r\ndata = get_upper_bits(v, ushift);\r\npiobuf++;\r\naddr++;\r\nl -= sizeof(u32);\r\n}\r\nif (l) {\r\nu32 v = *addr;\r\nif (l + extra >= sizeof(u32)) {\r\ndata |= set_upper_bits(v, shift);\r\nlen -= l + extra - sizeof(u32);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n__raw_writel(data, piobuf);\r\npiobuf++;\r\nextra = 0;\r\ndata = 0;\r\n} else {\r\ndata |= clear_upper_bytes(v, l, extra);\r\nif (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\nextra += l;\r\n}\r\n} else if (len == length) {\r\nlast = data;\r\nbreak;\r\n}\r\n} else if (len == length) {\r\nu32 w;\r\nw = (len + 3) >> 2;\r\nqib_pio_copy(piobuf, ss->sge.vaddr, w - 1);\r\npiobuf += w - 1;\r\nlast = ((u32 *) ss->sge.vaddr)[w - 1];\r\nbreak;\r\n} else {\r\nu32 w = len >> 2;\r\nqib_pio_copy(piobuf, ss->sge.vaddr, w);\r\npiobuf += w;\r\nextra = len & (sizeof(u32) - 1);\r\nif (extra) {\r\nu32 v = ((u32 *) ss->sge.vaddr)[w];\r\ndata = clear_upper_bytes(v, extra, 0);\r\n}\r\n}\r\nupdate_sge(ss, len);\r\nlength -= len;\r\n}\r\nupdate_sge(ss, length);\r\nif (flush_wc) {\r\nqib_flush_wc();\r\n__raw_writel(last, piobuf);\r\nqib_flush_wc();\r\n} else\r\n__raw_writel(last, piobuf);\r\n}\r\ninline struct qib_verbs_txreq *get_txreq(struct qib_ibdev *dev,\r\nstruct qib_qp *qp)\r\n{\r\nstruct qib_verbs_txreq *tx;\r\nunsigned long flags;\r\nspin_lock_irqsave(&dev->pending_lock, flags);\r\nif (likely(!list_empty(&dev->txreq_free))) {\r\nstruct list_head *l = dev->txreq_free.next;\r\nlist_del(l);\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\ntx = list_entry(l, struct qib_verbs_txreq, txreq.list);\r\n} else {\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\ntx = __get_txreq(dev, qp);\r\n}\r\nreturn tx;\r\n}\r\nvoid qib_put_txreq(struct qib_verbs_txreq *tx)\r\n{\r\nstruct qib_ibdev *dev;\r\nstruct qib_qp *qp;\r\nunsigned long flags;\r\nqp = tx->qp;\r\ndev = to_idev(qp->ibqp.device);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\nif (tx->mr) {\r\nqib_put_mr(tx->mr);\r\ntx->mr = NULL;\r\n}\r\nif (tx->txreq.flags & QIB_SDMA_TXREQ_F_FREEBUF) {\r\ntx->txreq.flags &= ~QIB_SDMA_TXREQ_F_FREEBUF;\r\ndma_unmap_single(&dd_from_dev(dev)->pcidev->dev,\r\ntx->txreq.addr, tx->hdr_dwords << 2,\r\nDMA_TO_DEVICE);\r\nkfree(tx->align_buf);\r\n}\r\nspin_lock_irqsave(&dev->pending_lock, flags);\r\nlist_add(&tx->txreq.list, &dev->txreq_free);\r\nif (!list_empty(&dev->txwait)) {\r\nqp = list_entry(dev->txwait.next, struct qib_qp, iowait);\r\nlist_del_init(&qp->iowait);\r\natomic_inc(&qp->refcount);\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->s_flags & QIB_S_WAIT_TX) {\r\nqp->s_flags &= ~QIB_S_WAIT_TX;\r\nqib_schedule_send(qp);\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n} else\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\n}\r\nvoid qib_verbs_sdma_desc_avail(struct qib_pportdata *ppd, unsigned avail)\r\n{\r\nstruct qib_qp *qp, *nqp;\r\nstruct qib_qp *qps[20];\r\nstruct qib_ibdev *dev;\r\nunsigned i, n;\r\nn = 0;\r\ndev = &ppd->dd->verbs_dev;\r\nspin_lock(&dev->pending_lock);\r\nlist_for_each_entry_safe(qp, nqp, &dev->dmawait, iowait) {\r\nif (qp->port_num != ppd->port)\r\ncontinue;\r\nif (n == ARRAY_SIZE(qps))\r\nbreak;\r\nif (qp->s_tx->txreq.sg_count > avail)\r\nbreak;\r\navail -= qp->s_tx->txreq.sg_count;\r\nlist_del_init(&qp->iowait);\r\natomic_inc(&qp->refcount);\r\nqps[n++] = qp;\r\n}\r\nspin_unlock(&dev->pending_lock);\r\nfor (i = 0; i < n; i++) {\r\nqp = qps[i];\r\nspin_lock(&qp->s_lock);\r\nif (qp->s_flags & QIB_S_WAIT_DMA_DESC) {\r\nqp->s_flags &= ~QIB_S_WAIT_DMA_DESC;\r\nqib_schedule_send(qp);\r\n}\r\nspin_unlock(&qp->s_lock);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\n}\r\nstatic void sdma_complete(struct qib_sdma_txreq *cookie, int status)\r\n{\r\nstruct qib_verbs_txreq *tx =\r\ncontainer_of(cookie, struct qib_verbs_txreq, txreq);\r\nstruct qib_qp *qp = tx->qp;\r\nspin_lock(&qp->s_lock);\r\nif (tx->wqe)\r\nqib_send_complete(qp, tx->wqe, IB_WC_SUCCESS);\r\nelse if (qp->ibqp.qp_type == IB_QPT_RC) {\r\nstruct qib_ib_header *hdr;\r\nif (tx->txreq.flags & QIB_SDMA_TXREQ_F_FREEBUF)\r\nhdr = &tx->align_buf->hdr;\r\nelse {\r\nstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\r\nhdr = &dev->pio_hdrs[tx->hdr_inx].hdr;\r\n}\r\nqib_rc_send_complete(qp, hdr);\r\n}\r\nif (atomic_dec_and_test(&qp->s_dma_busy)) {\r\nif (qp->state == IB_QPS_RESET)\r\nwake_up(&qp->wait_dma);\r\nelse if (qp->s_flags & QIB_S_WAIT_DMA) {\r\nqp->s_flags &= ~QIB_S_WAIT_DMA;\r\nqib_schedule_send(qp);\r\n}\r\n}\r\nspin_unlock(&qp->s_lock);\r\nqib_put_txreq(tx);\r\n}\r\nstatic int wait_kmem(struct qib_ibdev *dev, struct qib_qp *qp)\r\n{\r\nunsigned long flags;\r\nint ret = 0;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) {\r\nspin_lock(&dev->pending_lock);\r\nif (list_empty(&qp->iowait)) {\r\nif (list_empty(&dev->memwait))\r\nmod_timer(&dev->mem_timer, jiffies + 1);\r\nqp->s_flags |= QIB_S_WAIT_KMEM;\r\nlist_add_tail(&qp->iowait, &dev->memwait);\r\n}\r\nspin_unlock(&dev->pending_lock);\r\nqp->s_flags &= ~QIB_S_BUSY;\r\nret = -EBUSY;\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int qib_verbs_send_dma(struct qib_qp *qp, struct qib_ib_header *hdr,\r\nu32 hdrwords, struct qib_sge_state *ss, u32 len,\r\nu32 plen, u32 dwords)\r\n{\r\nstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct qib_devdata *dd = dd_from_dev(dev);\r\nstruct qib_ibport *ibp = to_iport(qp->ibqp.device, qp->port_num);\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nstruct qib_verbs_txreq *tx;\r\nstruct qib_pio_header *phdr;\r\nu32 control;\r\nu32 ndesc;\r\nint ret;\r\ntx = qp->s_tx;\r\nif (tx) {\r\nqp->s_tx = NULL;\r\nret = qib_sdma_verbs_send(ppd, tx->ss, tx->dwords, tx);\r\ngoto bail;\r\n}\r\ntx = get_txreq(dev, qp);\r\nif (IS_ERR(tx))\r\ngoto bail_tx;\r\ncontrol = dd->f_setpbc_control(ppd, plen, qp->s_srate,\r\nbe16_to_cpu(hdr->lrh[0]) >> 12);\r\ntx->qp = qp;\r\natomic_inc(&qp->refcount);\r\ntx->wqe = qp->s_wqe;\r\ntx->mr = qp->s_rdma_mr;\r\nif (qp->s_rdma_mr)\r\nqp->s_rdma_mr = NULL;\r\ntx->txreq.callback = sdma_complete;\r\nif (dd->flags & QIB_HAS_SDMA_TIMEOUT)\r\ntx->txreq.flags = QIB_SDMA_TXREQ_F_HEADTOHOST;\r\nelse\r\ntx->txreq.flags = QIB_SDMA_TXREQ_F_INTREQ;\r\nif (plen + 1 > dd->piosize2kmax_dwords)\r\ntx->txreq.flags |= QIB_SDMA_TXREQ_F_USELARGEBUF;\r\nif (len) {\r\nndesc = qib_count_sge(ss, len);\r\nif (ndesc >= ppd->sdma_descq_cnt)\r\nndesc = 0;\r\n} else\r\nndesc = 1;\r\nif (ndesc) {\r\nphdr = &dev->pio_hdrs[tx->hdr_inx];\r\nphdr->pbc[0] = cpu_to_le32(plen);\r\nphdr->pbc[1] = cpu_to_le32(control);\r\nmemcpy(&phdr->hdr, hdr, hdrwords << 2);\r\ntx->txreq.flags |= QIB_SDMA_TXREQ_F_FREEDESC;\r\ntx->txreq.sg_count = ndesc;\r\ntx->txreq.addr = dev->pio_hdrs_phys +\r\ntx->hdr_inx * sizeof(struct qib_pio_header);\r\ntx->hdr_dwords = hdrwords + 2;\r\nret = qib_sdma_verbs_send(ppd, ss, dwords, tx);\r\ngoto bail;\r\n}\r\ntx->hdr_dwords = plen + 1;\r\nphdr = kmalloc(tx->hdr_dwords << 2, GFP_ATOMIC);\r\nif (!phdr)\r\ngoto err_tx;\r\nphdr->pbc[0] = cpu_to_le32(plen);\r\nphdr->pbc[1] = cpu_to_le32(control);\r\nmemcpy(&phdr->hdr, hdr, hdrwords << 2);\r\nqib_copy_from_sge((u32 *) &phdr->hdr + hdrwords, ss, len);\r\ntx->txreq.addr = dma_map_single(&dd->pcidev->dev, phdr,\r\ntx->hdr_dwords << 2, DMA_TO_DEVICE);\r\nif (dma_mapping_error(&dd->pcidev->dev, tx->txreq.addr))\r\ngoto map_err;\r\ntx->align_buf = phdr;\r\ntx->txreq.flags |= QIB_SDMA_TXREQ_F_FREEBUF;\r\ntx->txreq.sg_count = 1;\r\nret = qib_sdma_verbs_send(ppd, NULL, 0, tx);\r\ngoto unaligned;\r\nmap_err:\r\nkfree(phdr);\r\nerr_tx:\r\nqib_put_txreq(tx);\r\nret = wait_kmem(dev, qp);\r\nunaligned:\r\nibp->n_unaligned++;\r\nbail:\r\nreturn ret;\r\nbail_tx:\r\nret = PTR_ERR(tx);\r\ngoto bail;\r\n}\r\nstatic int no_bufs_available(struct qib_qp *qp)\r\n{\r\nstruct qib_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct qib_devdata *dd;\r\nunsigned long flags;\r\nint ret = 0;\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (ib_qib_state_ops[qp->state] & QIB_PROCESS_RECV_OK) {\r\nspin_lock(&dev->pending_lock);\r\nif (list_empty(&qp->iowait)) {\r\ndev->n_piowait++;\r\nqp->s_flags |= QIB_S_WAIT_PIO;\r\nlist_add_tail(&qp->iowait, &dev->piowait);\r\ndd = dd_from_dev(dev);\r\ndd->f_wantpiobuf_intr(dd, 1);\r\n}\r\nspin_unlock(&dev->pending_lock);\r\nqp->s_flags &= ~QIB_S_BUSY;\r\nret = -EBUSY;\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int qib_verbs_send_pio(struct qib_qp *qp, struct qib_ib_header *ibhdr,\r\nu32 hdrwords, struct qib_sge_state *ss, u32 len,\r\nu32 plen, u32 dwords)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(qp->ibqp.device);\r\nstruct qib_pportdata *ppd = dd->pport + qp->port_num - 1;\r\nu32 *hdr = (u32 *) ibhdr;\r\nu32 __iomem *piobuf_orig;\r\nu32 __iomem *piobuf;\r\nu64 pbc;\r\nunsigned long flags;\r\nunsigned flush_wc;\r\nu32 control;\r\nu32 pbufn;\r\ncontrol = dd->f_setpbc_control(ppd, plen, qp->s_srate,\r\nbe16_to_cpu(ibhdr->lrh[0]) >> 12);\r\npbc = ((u64) control << 32) | plen;\r\npiobuf = dd->f_getsendbuf(ppd, pbc, &pbufn);\r\nif (unlikely(piobuf == NULL))\r\nreturn no_bufs_available(qp);\r\nwriteq(pbc, piobuf);\r\npiobuf_orig = piobuf;\r\npiobuf += 2;\r\nflush_wc = dd->flags & QIB_PIO_FLUSH_WC;\r\nif (len == 0) {\r\nif (flush_wc) {\r\nqib_flush_wc();\r\nqib_pio_copy(piobuf, hdr, hdrwords - 1);\r\nqib_flush_wc();\r\n__raw_writel(hdr[hdrwords - 1], piobuf + hdrwords - 1);\r\nqib_flush_wc();\r\n} else\r\nqib_pio_copy(piobuf, hdr, hdrwords);\r\ngoto done;\r\n}\r\nif (flush_wc)\r\nqib_flush_wc();\r\nqib_pio_copy(piobuf, hdr, hdrwords);\r\npiobuf += hdrwords;\r\nif (likely(ss->num_sge == 1 && len <= ss->sge.length &&\r\n!((unsigned long)ss->sge.vaddr & (sizeof(u32) - 1)))) {\r\nu32 *addr = (u32 *) ss->sge.vaddr;\r\nupdate_sge(ss, len);\r\nif (flush_wc) {\r\nqib_pio_copy(piobuf, addr, dwords - 1);\r\nqib_flush_wc();\r\n__raw_writel(addr[dwords - 1], piobuf + dwords - 1);\r\nqib_flush_wc();\r\n} else\r\nqib_pio_copy(piobuf, addr, dwords);\r\ngoto done;\r\n}\r\ncopy_io(piobuf, ss, len, flush_wc);\r\ndone:\r\nif (dd->flags & QIB_USE_SPCL_TRIG) {\r\nu32 spcl_off = (pbufn >= dd->piobcnt2k) ? 2047 : 1023;\r\nqib_flush_wc();\r\n__raw_writel(0xaebecede, piobuf_orig + spcl_off);\r\n}\r\nqib_sendbuf_done(dd, pbufn);\r\nif (qp->s_rdma_mr) {\r\nqib_put_mr(qp->s_rdma_mr);\r\nqp->s_rdma_mr = NULL;\r\n}\r\nif (qp->s_wqe) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nqib_send_complete(qp, qp->s_wqe, IB_WC_SUCCESS);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\n} else if (qp->ibqp.qp_type == IB_QPT_RC) {\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nqib_rc_send_complete(qp, ibhdr);\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\n}\r\nreturn 0;\r\n}\r\nint qib_verbs_send(struct qib_qp *qp, struct qib_ib_header *hdr,\r\nu32 hdrwords, struct qib_sge_state *ss, u32 len)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(qp->ibqp.device);\r\nu32 plen;\r\nint ret;\r\nu32 dwords = (len + 3) >> 2;\r\nplen = hdrwords + dwords + 1;\r\nif (qp->ibqp.qp_type == IB_QPT_SMI ||\r\n!(dd->flags & QIB_HAS_SEND_DMA))\r\nret = qib_verbs_send_pio(qp, hdr, hdrwords, ss, len,\r\nplen, dwords);\r\nelse\r\nret = qib_verbs_send_dma(qp, hdr, hdrwords, ss, len,\r\nplen, dwords);\r\nreturn ret;\r\n}\r\nint qib_snapshot_counters(struct qib_pportdata *ppd, u64 *swords,\r\nu64 *rwords, u64 *spkts, u64 *rpkts,\r\nu64 *xmit_wait)\r\n{\r\nint ret;\r\nstruct qib_devdata *dd = ppd->dd;\r\nif (!(dd->flags & QIB_PRESENT)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\n*swords = dd->f_portcntr(ppd, QIBPORTCNTR_WORDSEND);\r\n*rwords = dd->f_portcntr(ppd, QIBPORTCNTR_WORDRCV);\r\n*spkts = dd->f_portcntr(ppd, QIBPORTCNTR_PKTSEND);\r\n*rpkts = dd->f_portcntr(ppd, QIBPORTCNTR_PKTRCV);\r\n*xmit_wait = dd->f_portcntr(ppd, QIBPORTCNTR_SENDSTALL);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nint qib_get_counters(struct qib_pportdata *ppd,\r\nstruct qib_verbs_counters *cntrs)\r\n{\r\nint ret;\r\nif (!(ppd->dd->flags & QIB_PRESENT)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\ncntrs->symbol_error_counter =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBSYMBOLERR);\r\ncntrs->link_error_recovery_counter =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBLINKERRRECOV);\r\ncntrs->link_downed_counter =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_IBLINKDOWN);\r\ncntrs->port_rcv_errors =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXDROPPKT) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RCVOVFL) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERR_RLEN) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_INVALIDRLEN) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRLINK) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRICRC) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRVCRC) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_ERRLPCRC) +\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_BADFORMAT);\r\ncntrs->port_rcv_errors +=\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXLOCALPHYERR);\r\ncntrs->port_rcv_errors +=\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RXVLERR);\r\ncntrs->port_rcv_remphys_errors =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_RCVEBP);\r\ncntrs->port_xmit_discards =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_UNSUPVL);\r\ncntrs->port_xmit_data = ppd->dd->f_portcntr(ppd,\r\nQIBPORTCNTR_WORDSEND);\r\ncntrs->port_rcv_data = ppd->dd->f_portcntr(ppd,\r\nQIBPORTCNTR_WORDRCV);\r\ncntrs->port_xmit_packets = ppd->dd->f_portcntr(ppd,\r\nQIBPORTCNTR_PKTSEND);\r\ncntrs->port_rcv_packets = ppd->dd->f_portcntr(ppd,\r\nQIBPORTCNTR_PKTRCV);\r\ncntrs->local_link_integrity_errors =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_LLI);\r\ncntrs->excessive_buffer_overrun_errors =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_EXCESSBUFOVFL);\r\ncntrs->vl15_dropped =\r\nppd->dd->f_portcntr(ppd, QIBPORTCNTR_VL15PKTDROP);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nvoid qib_ib_piobufavail(struct qib_devdata *dd)\r\n{\r\nstruct qib_ibdev *dev = &dd->verbs_dev;\r\nstruct list_head *list;\r\nstruct qib_qp *qps[5];\r\nstruct qib_qp *qp;\r\nunsigned long flags;\r\nunsigned i, n;\r\nlist = &dev->piowait;\r\nn = 0;\r\nspin_lock_irqsave(&dev->pending_lock, flags);\r\nwhile (!list_empty(list)) {\r\nif (n == ARRAY_SIZE(qps))\r\ngoto full;\r\nqp = list_entry(list->next, struct qib_qp, iowait);\r\nlist_del_init(&qp->iowait);\r\natomic_inc(&qp->refcount);\r\nqps[n++] = qp;\r\n}\r\ndd->f_wantpiobuf_intr(dd, 0);\r\nfull:\r\nspin_unlock_irqrestore(&dev->pending_lock, flags);\r\nfor (i = 0; i < n; i++) {\r\nqp = qps[i];\r\nspin_lock_irqsave(&qp->s_lock, flags);\r\nif (qp->s_flags & QIB_S_WAIT_PIO) {\r\nqp->s_flags &= ~QIB_S_WAIT_PIO;\r\nqib_schedule_send(qp);\r\n}\r\nspin_unlock_irqrestore(&qp->s_lock, flags);\r\nif (atomic_dec_and_test(&qp->refcount))\r\nwake_up(&qp->wait);\r\n}\r\n}\r\nstatic int qib_query_device(struct ib_device *ibdev,\r\nstruct ib_device_attr *props)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(ibdev);\r\nstruct qib_ibdev *dev = to_idev(ibdev);\r\nmemset(props, 0, sizeof(*props));\r\nprops->device_cap_flags = IB_DEVICE_BAD_PKEY_CNTR |\r\nIB_DEVICE_BAD_QKEY_CNTR | IB_DEVICE_SHUTDOWN_PORT |\r\nIB_DEVICE_SYS_IMAGE_GUID | IB_DEVICE_RC_RNR_NAK_GEN |\r\nIB_DEVICE_PORT_ACTIVE_EVENT | IB_DEVICE_SRQ_RESIZE;\r\nprops->page_size_cap = PAGE_SIZE;\r\nprops->vendor_id =\r\nQIB_SRC_OUI_1 << 16 | QIB_SRC_OUI_2 << 8 | QIB_SRC_OUI_3;\r\nprops->vendor_part_id = dd->deviceid;\r\nprops->hw_ver = dd->minrev;\r\nprops->sys_image_guid = ib_qib_sys_image_guid;\r\nprops->max_mr_size = ~0ULL;\r\nprops->max_qp = ib_qib_max_qps;\r\nprops->max_qp_wr = ib_qib_max_qp_wrs;\r\nprops->max_sge = ib_qib_max_sges;\r\nprops->max_cq = ib_qib_max_cqs;\r\nprops->max_ah = ib_qib_max_ahs;\r\nprops->max_cqe = ib_qib_max_cqes;\r\nprops->max_mr = dev->lk_table.max;\r\nprops->max_fmr = dev->lk_table.max;\r\nprops->max_map_per_fmr = 32767;\r\nprops->max_pd = ib_qib_max_pds;\r\nprops->max_qp_rd_atom = QIB_MAX_RDMA_ATOMIC;\r\nprops->max_qp_init_rd_atom = 255;\r\nprops->max_srq = ib_qib_max_srqs;\r\nprops->max_srq_wr = ib_qib_max_srq_wrs;\r\nprops->max_srq_sge = ib_qib_max_srq_sges;\r\nprops->atomic_cap = IB_ATOMIC_GLOB;\r\nprops->max_pkeys = qib_get_npkeys(dd);\r\nprops->max_mcast_grp = ib_qib_max_mcast_grps;\r\nprops->max_mcast_qp_attach = ib_qib_max_mcast_qp_attached;\r\nprops->max_total_mcast_qp_attach = props->max_mcast_qp_attach *\r\nprops->max_mcast_grp;\r\nreturn 0;\r\n}\r\nstatic int qib_query_port(struct ib_device *ibdev, u8 port,\r\nstruct ib_port_attr *props)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(ibdev);\r\nstruct qib_ibport *ibp = to_iport(ibdev, port);\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nenum ib_mtu mtu;\r\nu16 lid = ppd->lid;\r\nmemset(props, 0, sizeof(*props));\r\nprops->lid = lid ? lid : be16_to_cpu(IB_LID_PERMISSIVE);\r\nprops->lmc = ppd->lmc;\r\nprops->sm_lid = ibp->sm_lid;\r\nprops->sm_sl = ibp->sm_sl;\r\nprops->state = dd->f_iblink_state(ppd->lastibcstat);\r\nprops->phys_state = dd->f_ibphys_portstate(ppd->lastibcstat);\r\nprops->port_cap_flags = ibp->port_cap_flags;\r\nprops->gid_tbl_len = QIB_GUIDS_PER_PORT;\r\nprops->max_msg_sz = 0x80000000;\r\nprops->pkey_tbl_len = qib_get_npkeys(dd);\r\nprops->bad_pkey_cntr = ibp->pkey_violations;\r\nprops->qkey_viol_cntr = ibp->qkey_violations;\r\nprops->active_width = ppd->link_width_active;\r\nprops->active_speed = ppd->link_speed_active;\r\nprops->max_vl_num = qib_num_vls(ppd->vls_supported);\r\nprops->init_type_reply = 0;\r\nprops->max_mtu = qib_ibmtu ? qib_ibmtu : IB_MTU_4096;\r\nswitch (ppd->ibmtu) {\r\ncase 4096:\r\nmtu = IB_MTU_4096;\r\nbreak;\r\ncase 2048:\r\nmtu = IB_MTU_2048;\r\nbreak;\r\ncase 1024:\r\nmtu = IB_MTU_1024;\r\nbreak;\r\ncase 512:\r\nmtu = IB_MTU_512;\r\nbreak;\r\ncase 256:\r\nmtu = IB_MTU_256;\r\nbreak;\r\ndefault:\r\nmtu = IB_MTU_2048;\r\n}\r\nprops->active_mtu = mtu;\r\nprops->subnet_timeout = ibp->subnet_timeout;\r\nreturn 0;\r\n}\r\nstatic int qib_modify_device(struct ib_device *device,\r\nint device_modify_mask,\r\nstruct ib_device_modify *device_modify)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(device);\r\nunsigned i;\r\nint ret;\r\nif (device_modify_mask & ~(IB_DEVICE_MODIFY_SYS_IMAGE_GUID |\r\nIB_DEVICE_MODIFY_NODE_DESC)) {\r\nret = -EOPNOTSUPP;\r\ngoto bail;\r\n}\r\nif (device_modify_mask & IB_DEVICE_MODIFY_NODE_DESC) {\r\nmemcpy(device->node_desc, device_modify->node_desc, 64);\r\nfor (i = 0; i < dd->num_pports; i++) {\r\nstruct qib_ibport *ibp = &dd->pport[i].ibport_data;\r\nqib_node_desc_chg(ibp);\r\n}\r\n}\r\nif (device_modify_mask & IB_DEVICE_MODIFY_SYS_IMAGE_GUID) {\r\nib_qib_sys_image_guid =\r\ncpu_to_be64(device_modify->sys_image_guid);\r\nfor (i = 0; i < dd->num_pports; i++) {\r\nstruct qib_ibport *ibp = &dd->pport[i].ibport_data;\r\nqib_sys_guid_chg(ibp);\r\n}\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int qib_modify_port(struct ib_device *ibdev, u8 port,\r\nint port_modify_mask, struct ib_port_modify *props)\r\n{\r\nstruct qib_ibport *ibp = to_iport(ibdev, port);\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nibp->port_cap_flags |= props->set_port_cap_mask;\r\nibp->port_cap_flags &= ~props->clr_port_cap_mask;\r\nif (props->set_port_cap_mask || props->clr_port_cap_mask)\r\nqib_cap_mask_chg(ibp);\r\nif (port_modify_mask & IB_PORT_SHUTDOWN)\r\nqib_set_linkstate(ppd, QIB_IB_LINKDOWN);\r\nif (port_modify_mask & IB_PORT_RESET_QKEY_CNTR)\r\nibp->qkey_violations = 0;\r\nreturn 0;\r\n}\r\nstatic int qib_query_gid(struct ib_device *ibdev, u8 port,\r\nint index, union ib_gid *gid)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(ibdev);\r\nint ret = 0;\r\nif (!port || port > dd->num_pports)\r\nret = -EINVAL;\r\nelse {\r\nstruct qib_ibport *ibp = to_iport(ibdev, port);\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\ngid->global.subnet_prefix = ibp->gid_prefix;\r\nif (index == 0)\r\ngid->global.interface_id = ppd->guid;\r\nelse if (index < QIB_GUIDS_PER_PORT)\r\ngid->global.interface_id = ibp->guids[index - 1];\r\nelse\r\nret = -EINVAL;\r\n}\r\nreturn ret;\r\n}\r\nstatic struct ib_pd *qib_alloc_pd(struct ib_device *ibdev,\r\nstruct ib_ucontext *context,\r\nstruct ib_udata *udata)\r\n{\r\nstruct qib_ibdev *dev = to_idev(ibdev);\r\nstruct qib_pd *pd;\r\nstruct ib_pd *ret;\r\npd = kmalloc(sizeof *pd, GFP_KERNEL);\r\nif (!pd) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nspin_lock(&dev->n_pds_lock);\r\nif (dev->n_pds_allocated == ib_qib_max_pds) {\r\nspin_unlock(&dev->n_pds_lock);\r\nkfree(pd);\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\ndev->n_pds_allocated++;\r\nspin_unlock(&dev->n_pds_lock);\r\npd->user = udata != NULL;\r\nret = &pd->ibpd;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int qib_dealloc_pd(struct ib_pd *ibpd)\r\n{\r\nstruct qib_pd *pd = to_ipd(ibpd);\r\nstruct qib_ibdev *dev = to_idev(ibpd->device);\r\nspin_lock(&dev->n_pds_lock);\r\ndev->n_pds_allocated--;\r\nspin_unlock(&dev->n_pds_lock);\r\nkfree(pd);\r\nreturn 0;\r\n}\r\nint qib_check_ah(struct ib_device *ibdev, struct ib_ah_attr *ah_attr)\r\n{\r\nif (ah_attr->dlid >= QIB_MULTICAST_LID_BASE &&\r\nah_attr->dlid != QIB_PERMISSIVE_LID &&\r\n!(ah_attr->ah_flags & IB_AH_GRH))\r\ngoto bail;\r\nif ((ah_attr->ah_flags & IB_AH_GRH) &&\r\nah_attr->grh.sgid_index >= QIB_GUIDS_PER_PORT)\r\ngoto bail;\r\nif (ah_attr->dlid == 0)\r\ngoto bail;\r\nif (ah_attr->port_num < 1 ||\r\nah_attr->port_num > ibdev->phys_port_cnt)\r\ngoto bail;\r\nif (ah_attr->static_rate != IB_RATE_PORT_CURRENT &&\r\nib_rate_to_mult(ah_attr->static_rate) < 0)\r\ngoto bail;\r\nif (ah_attr->sl > 15)\r\ngoto bail;\r\nreturn 0;\r\nbail:\r\nreturn -EINVAL;\r\n}\r\nstatic struct ib_ah *qib_create_ah(struct ib_pd *pd,\r\nstruct ib_ah_attr *ah_attr)\r\n{\r\nstruct qib_ah *ah;\r\nstruct ib_ah *ret;\r\nstruct qib_ibdev *dev = to_idev(pd->device);\r\nunsigned long flags;\r\nif (qib_check_ah(pd->device, ah_attr)) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nah = kmalloc(sizeof *ah, GFP_ATOMIC);\r\nif (!ah) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nspin_lock_irqsave(&dev->n_ahs_lock, flags);\r\nif (dev->n_ahs_allocated == ib_qib_max_ahs) {\r\nspin_unlock_irqrestore(&dev->n_ahs_lock, flags);\r\nkfree(ah);\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\ndev->n_ahs_allocated++;\r\nspin_unlock_irqrestore(&dev->n_ahs_lock, flags);\r\nah->attr = *ah_attr;\r\natomic_set(&ah->refcount, 0);\r\nret = &ah->ibah;\r\nbail:\r\nreturn ret;\r\n}\r\nstruct ib_ah *qib_create_qp0_ah(struct qib_ibport *ibp, u16 dlid)\r\n{\r\nstruct ib_ah_attr attr;\r\nstruct ib_ah *ah = ERR_PTR(-EINVAL);\r\nstruct qib_qp *qp0;\r\nmemset(&attr, 0, sizeof attr);\r\nattr.dlid = dlid;\r\nattr.port_num = ppd_from_ibp(ibp)->port;\r\nrcu_read_lock();\r\nqp0 = rcu_dereference(ibp->qp0);\r\nif (qp0)\r\nah = ib_create_ah(qp0->ibqp.pd, &attr);\r\nrcu_read_unlock();\r\nreturn ah;\r\n}\r\nstatic int qib_destroy_ah(struct ib_ah *ibah)\r\n{\r\nstruct qib_ibdev *dev = to_idev(ibah->device);\r\nstruct qib_ah *ah = to_iah(ibah);\r\nunsigned long flags;\r\nif (atomic_read(&ah->refcount) != 0)\r\nreturn -EBUSY;\r\nspin_lock_irqsave(&dev->n_ahs_lock, flags);\r\ndev->n_ahs_allocated--;\r\nspin_unlock_irqrestore(&dev->n_ahs_lock, flags);\r\nkfree(ah);\r\nreturn 0;\r\n}\r\nstatic int qib_modify_ah(struct ib_ah *ibah, struct ib_ah_attr *ah_attr)\r\n{\r\nstruct qib_ah *ah = to_iah(ibah);\r\nif (qib_check_ah(ibah->device, ah_attr))\r\nreturn -EINVAL;\r\nah->attr = *ah_attr;\r\nreturn 0;\r\n}\r\nstatic int qib_query_ah(struct ib_ah *ibah, struct ib_ah_attr *ah_attr)\r\n{\r\nstruct qib_ah *ah = to_iah(ibah);\r\n*ah_attr = ah->attr;\r\nreturn 0;\r\n}\r\nunsigned qib_get_npkeys(struct qib_devdata *dd)\r\n{\r\nreturn ARRAY_SIZE(dd->rcd[0]->pkeys);\r\n}\r\nunsigned qib_get_pkey(struct qib_ibport *ibp, unsigned index)\r\n{\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nstruct qib_devdata *dd = ppd->dd;\r\nunsigned ctxt = ppd->hw_pidx;\r\nunsigned ret;\r\nif (!dd->rcd || index >= ARRAY_SIZE(dd->rcd[ctxt]->pkeys))\r\nret = 0;\r\nelse\r\nret = dd->rcd[ctxt]->pkeys[index];\r\nreturn ret;\r\n}\r\nstatic int qib_query_pkey(struct ib_device *ibdev, u8 port, u16 index,\r\nu16 *pkey)\r\n{\r\nstruct qib_devdata *dd = dd_from_ibdev(ibdev);\r\nint ret;\r\nif (index >= qib_get_npkeys(dd)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\n*pkey = qib_get_pkey(to_iport(ibdev, port), index);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic struct ib_ucontext *qib_alloc_ucontext(struct ib_device *ibdev,\r\nstruct ib_udata *udata)\r\n{\r\nstruct qib_ucontext *context;\r\nstruct ib_ucontext *ret;\r\ncontext = kmalloc(sizeof *context, GFP_KERNEL);\r\nif (!context) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nret = &context->ibucontext;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int qib_dealloc_ucontext(struct ib_ucontext *context)\r\n{\r\nkfree(to_iucontext(context));\r\nreturn 0;\r\n}\r\nstatic void init_ibport(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_verbs_counters cntrs;\r\nstruct qib_ibport *ibp = &ppd->ibport_data;\r\nspin_lock_init(&ibp->lock);\r\nibp->gid_prefix = IB_DEFAULT_GID_PREFIX;\r\nibp->sm_lid = be16_to_cpu(IB_LID_PERMISSIVE);\r\nibp->port_cap_flags = IB_PORT_SYS_IMAGE_GUID_SUP |\r\nIB_PORT_CLIENT_REG_SUP | IB_PORT_SL_MAP_SUP |\r\nIB_PORT_TRAP_SUP | IB_PORT_AUTO_MIGR_SUP |\r\nIB_PORT_DR_NOTICE_SUP | IB_PORT_CAP_MASK_NOTICE_SUP |\r\nIB_PORT_OTHER_LOCAL_CHANGES_SUP;\r\nif (ppd->dd->flags & QIB_HAS_LINK_LATENCY)\r\nibp->port_cap_flags |= IB_PORT_LINK_LATENCY_SUP;\r\nibp->pma_counter_select[0] = IB_PMA_PORT_XMIT_DATA;\r\nibp->pma_counter_select[1] = IB_PMA_PORT_RCV_DATA;\r\nibp->pma_counter_select[2] = IB_PMA_PORT_XMIT_PKTS;\r\nibp->pma_counter_select[3] = IB_PMA_PORT_RCV_PKTS;\r\nibp->pma_counter_select[4] = IB_PMA_PORT_XMIT_WAIT;\r\nqib_get_counters(ppd, &cntrs);\r\nibp->z_symbol_error_counter = cntrs.symbol_error_counter;\r\nibp->z_link_error_recovery_counter =\r\ncntrs.link_error_recovery_counter;\r\nibp->z_link_downed_counter = cntrs.link_downed_counter;\r\nibp->z_port_rcv_errors = cntrs.port_rcv_errors;\r\nibp->z_port_rcv_remphys_errors = cntrs.port_rcv_remphys_errors;\r\nibp->z_port_xmit_discards = cntrs.port_xmit_discards;\r\nibp->z_port_xmit_data = cntrs.port_xmit_data;\r\nibp->z_port_rcv_data = cntrs.port_rcv_data;\r\nibp->z_port_xmit_packets = cntrs.port_xmit_packets;\r\nibp->z_port_rcv_packets = cntrs.port_rcv_packets;\r\nibp->z_local_link_integrity_errors =\r\ncntrs.local_link_integrity_errors;\r\nibp->z_excessive_buffer_overrun_errors =\r\ncntrs.excessive_buffer_overrun_errors;\r\nibp->z_vl15_dropped = cntrs.vl15_dropped;\r\nRCU_INIT_POINTER(ibp->qp0, NULL);\r\nRCU_INIT_POINTER(ibp->qp1, NULL);\r\n}\r\nint qib_register_ib_device(struct qib_devdata *dd)\r\n{\r\nstruct qib_ibdev *dev = &dd->verbs_dev;\r\nstruct ib_device *ibdev = &dev->ibdev;\r\nstruct qib_pportdata *ppd = dd->pport;\r\nunsigned i, lk_tab_size;\r\nint ret;\r\ndev->qp_table_size = ib_qib_qp_table_size;\r\nget_random_bytes(&dev->qp_rnd, sizeof(dev->qp_rnd));\r\ndev->qp_table = kmalloc(dev->qp_table_size * sizeof *dev->qp_table,\r\nGFP_KERNEL);\r\nif (!dev->qp_table) {\r\nret = -ENOMEM;\r\ngoto err_qpt;\r\n}\r\nfor (i = 0; i < dev->qp_table_size; i++)\r\nRCU_INIT_POINTER(dev->qp_table[i], NULL);\r\nfor (i = 0; i < dd->num_pports; i++)\r\ninit_ibport(ppd + i);\r\nspin_lock_init(&dev->qpt_lock);\r\nspin_lock_init(&dev->n_pds_lock);\r\nspin_lock_init(&dev->n_ahs_lock);\r\nspin_lock_init(&dev->n_cqs_lock);\r\nspin_lock_init(&dev->n_qps_lock);\r\nspin_lock_init(&dev->n_srqs_lock);\r\nspin_lock_init(&dev->n_mcast_grps_lock);\r\ninit_timer(&dev->mem_timer);\r\ndev->mem_timer.function = mem_timer;\r\ndev->mem_timer.data = (unsigned long) dev;\r\nqib_init_qpn_table(dd, &dev->qpn_table);\r\nspin_lock_init(&dev->lk_table.lock);\r\ndev->lk_table.max = 1 << ib_qib_lkey_table_size;\r\nlk_tab_size = dev->lk_table.max * sizeof(*dev->lk_table.table);\r\ndev->lk_table.table = (struct qib_mregion __rcu **)\r\n__get_free_pages(GFP_KERNEL, get_order(lk_tab_size));\r\nif (dev->lk_table.table == NULL) {\r\nret = -ENOMEM;\r\ngoto err_lk;\r\n}\r\nRCU_INIT_POINTER(dev->dma_mr, NULL);\r\nfor (i = 0; i < dev->lk_table.max; i++)\r\nRCU_INIT_POINTER(dev->lk_table.table[i], NULL);\r\nINIT_LIST_HEAD(&dev->pending_mmaps);\r\nspin_lock_init(&dev->pending_lock);\r\ndev->mmap_offset = PAGE_SIZE;\r\nspin_lock_init(&dev->mmap_offset_lock);\r\nINIT_LIST_HEAD(&dev->piowait);\r\nINIT_LIST_HEAD(&dev->dmawait);\r\nINIT_LIST_HEAD(&dev->txwait);\r\nINIT_LIST_HEAD(&dev->memwait);\r\nINIT_LIST_HEAD(&dev->txreq_free);\r\nif (ppd->sdma_descq_cnt) {\r\ndev->pio_hdrs = dma_alloc_coherent(&dd->pcidev->dev,\r\nppd->sdma_descq_cnt *\r\nsizeof(struct qib_pio_header),\r\n&dev->pio_hdrs_phys,\r\nGFP_KERNEL);\r\nif (!dev->pio_hdrs) {\r\nret = -ENOMEM;\r\ngoto err_hdrs;\r\n}\r\n}\r\nfor (i = 0; i < ppd->sdma_descq_cnt; i++) {\r\nstruct qib_verbs_txreq *tx;\r\ntx = kzalloc(sizeof *tx, GFP_KERNEL);\r\nif (!tx) {\r\nret = -ENOMEM;\r\ngoto err_tx;\r\n}\r\ntx->hdr_inx = i;\r\nlist_add(&tx->txreq.list, &dev->txreq_free);\r\n}\r\nif (!ib_qib_sys_image_guid)\r\nib_qib_sys_image_guid = ppd->guid;\r\nstrlcpy(ibdev->name, "qib%d", IB_DEVICE_NAME_MAX);\r\nibdev->owner = THIS_MODULE;\r\nibdev->node_guid = ppd->guid;\r\nibdev->uverbs_abi_ver = QIB_UVERBS_ABI_VERSION;\r\nibdev->uverbs_cmd_mask =\r\n(1ull << IB_USER_VERBS_CMD_GET_CONTEXT) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_DEVICE) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_PORT) |\r\n(1ull << IB_USER_VERBS_CMD_ALLOC_PD) |\r\n(1ull << IB_USER_VERBS_CMD_DEALLOC_PD) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_AH) |\r\n(1ull << IB_USER_VERBS_CMD_MODIFY_AH) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_AH) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_AH) |\r\n(1ull << IB_USER_VERBS_CMD_REG_MR) |\r\n(1ull << IB_USER_VERBS_CMD_DEREG_MR) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_COMP_CHANNEL) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_RESIZE_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_POLL_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_REQ_NOTIFY_CQ) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_QP) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_MODIFY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_QP) |\r\n(1ull << IB_USER_VERBS_CMD_POST_SEND) |\r\n(1ull << IB_USER_VERBS_CMD_POST_RECV) |\r\n(1ull << IB_USER_VERBS_CMD_ATTACH_MCAST) |\r\n(1ull << IB_USER_VERBS_CMD_DETACH_MCAST) |\r\n(1ull << IB_USER_VERBS_CMD_CREATE_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_MODIFY_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_QUERY_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_DESTROY_SRQ) |\r\n(1ull << IB_USER_VERBS_CMD_POST_SRQ_RECV);\r\nibdev->node_type = RDMA_NODE_IB_CA;\r\nibdev->phys_port_cnt = dd->num_pports;\r\nibdev->num_comp_vectors = 1;\r\nibdev->dma_device = &dd->pcidev->dev;\r\nibdev->query_device = qib_query_device;\r\nibdev->modify_device = qib_modify_device;\r\nibdev->query_port = qib_query_port;\r\nibdev->modify_port = qib_modify_port;\r\nibdev->query_pkey = qib_query_pkey;\r\nibdev->query_gid = qib_query_gid;\r\nibdev->alloc_ucontext = qib_alloc_ucontext;\r\nibdev->dealloc_ucontext = qib_dealloc_ucontext;\r\nibdev->alloc_pd = qib_alloc_pd;\r\nibdev->dealloc_pd = qib_dealloc_pd;\r\nibdev->create_ah = qib_create_ah;\r\nibdev->destroy_ah = qib_destroy_ah;\r\nibdev->modify_ah = qib_modify_ah;\r\nibdev->query_ah = qib_query_ah;\r\nibdev->create_srq = qib_create_srq;\r\nibdev->modify_srq = qib_modify_srq;\r\nibdev->query_srq = qib_query_srq;\r\nibdev->destroy_srq = qib_destroy_srq;\r\nibdev->create_qp = qib_create_qp;\r\nibdev->modify_qp = qib_modify_qp;\r\nibdev->query_qp = qib_query_qp;\r\nibdev->destroy_qp = qib_destroy_qp;\r\nibdev->post_send = qib_post_send;\r\nibdev->post_recv = qib_post_receive;\r\nibdev->post_srq_recv = qib_post_srq_receive;\r\nibdev->create_cq = qib_create_cq;\r\nibdev->destroy_cq = qib_destroy_cq;\r\nibdev->resize_cq = qib_resize_cq;\r\nibdev->poll_cq = qib_poll_cq;\r\nibdev->req_notify_cq = qib_req_notify_cq;\r\nibdev->get_dma_mr = qib_get_dma_mr;\r\nibdev->reg_phys_mr = qib_reg_phys_mr;\r\nibdev->reg_user_mr = qib_reg_user_mr;\r\nibdev->dereg_mr = qib_dereg_mr;\r\nibdev->alloc_fast_reg_mr = qib_alloc_fast_reg_mr;\r\nibdev->alloc_fast_reg_page_list = qib_alloc_fast_reg_page_list;\r\nibdev->free_fast_reg_page_list = qib_free_fast_reg_page_list;\r\nibdev->alloc_fmr = qib_alloc_fmr;\r\nibdev->map_phys_fmr = qib_map_phys_fmr;\r\nibdev->unmap_fmr = qib_unmap_fmr;\r\nibdev->dealloc_fmr = qib_dealloc_fmr;\r\nibdev->attach_mcast = qib_multicast_attach;\r\nibdev->detach_mcast = qib_multicast_detach;\r\nibdev->process_mad = qib_process_mad;\r\nibdev->mmap = qib_mmap;\r\nibdev->dma_ops = &qib_dma_mapping_ops;\r\nsnprintf(ibdev->node_desc, sizeof(ibdev->node_desc),\r\n"QLogic Infiniband HCA %s", init_utsname()->nodename);\r\nret = ib_register_device(ibdev, qib_create_port_files);\r\nif (ret)\r\ngoto err_reg;\r\nret = qib_create_agents(dev);\r\nif (ret)\r\ngoto err_agents;\r\nif (qib_verbs_register_sysfs(dd))\r\ngoto err_class;\r\ngoto bail;\r\nerr_class:\r\nqib_free_agents(dev);\r\nerr_agents:\r\nib_unregister_device(ibdev);\r\nerr_reg:\r\nerr_tx:\r\nwhile (!list_empty(&dev->txreq_free)) {\r\nstruct list_head *l = dev->txreq_free.next;\r\nstruct qib_verbs_txreq *tx;\r\nlist_del(l);\r\ntx = list_entry(l, struct qib_verbs_txreq, txreq.list);\r\nkfree(tx);\r\n}\r\nif (ppd->sdma_descq_cnt)\r\ndma_free_coherent(&dd->pcidev->dev,\r\nppd->sdma_descq_cnt *\r\nsizeof(struct qib_pio_header),\r\ndev->pio_hdrs, dev->pio_hdrs_phys);\r\nerr_hdrs:\r\nfree_pages((unsigned long) dev->lk_table.table, get_order(lk_tab_size));\r\nerr_lk:\r\nkfree(dev->qp_table);\r\nerr_qpt:\r\nqib_dev_err(dd, "cannot register verbs: %d!\n", -ret);\r\nbail:\r\nreturn ret;\r\n}\r\nvoid qib_unregister_ib_device(struct qib_devdata *dd)\r\n{\r\nstruct qib_ibdev *dev = &dd->verbs_dev;\r\nstruct ib_device *ibdev = &dev->ibdev;\r\nu32 qps_inuse;\r\nunsigned lk_tab_size;\r\nqib_verbs_unregister_sysfs(dd);\r\nqib_free_agents(dev);\r\nib_unregister_device(ibdev);\r\nif (!list_empty(&dev->piowait))\r\nqib_dev_err(dd, "piowait list not empty!\n");\r\nif (!list_empty(&dev->dmawait))\r\nqib_dev_err(dd, "dmawait list not empty!\n");\r\nif (!list_empty(&dev->txwait))\r\nqib_dev_err(dd, "txwait list not empty!\n");\r\nif (!list_empty(&dev->memwait))\r\nqib_dev_err(dd, "memwait list not empty!\n");\r\nif (dev->dma_mr)\r\nqib_dev_err(dd, "DMA MR not NULL!\n");\r\nqps_inuse = qib_free_all_qps(dd);\r\nif (qps_inuse)\r\nqib_dev_err(dd, "QP memory leak! %u still in use\n",\r\nqps_inuse);\r\ndel_timer_sync(&dev->mem_timer);\r\nqib_free_qpn_table(&dev->qpn_table);\r\nwhile (!list_empty(&dev->txreq_free)) {\r\nstruct list_head *l = dev->txreq_free.next;\r\nstruct qib_verbs_txreq *tx;\r\nlist_del(l);\r\ntx = list_entry(l, struct qib_verbs_txreq, txreq.list);\r\nkfree(tx);\r\n}\r\nif (dd->pport->sdma_descq_cnt)\r\ndma_free_coherent(&dd->pcidev->dev,\r\ndd->pport->sdma_descq_cnt *\r\nsizeof(struct qib_pio_header),\r\ndev->pio_hdrs, dev->pio_hdrs_phys);\r\nlk_tab_size = dev->lk_table.max * sizeof(*dev->lk_table.table);\r\nfree_pages((unsigned long) dev->lk_table.table,\r\nget_order(lk_tab_size));\r\nkfree(dev->qp_table);\r\n}\r\nvoid qib_schedule_send(struct qib_qp *qp)\r\n{\r\nif (qib_send_ok(qp)) {\r\nstruct qib_ibport *ibp =\r\nto_iport(qp->ibqp.device, qp->port_num);\r\nstruct qib_pportdata *ppd = ppd_from_ibp(ibp);\r\nqueue_work(ppd->qib_wq, &qp->s_work);\r\n}\r\n}
