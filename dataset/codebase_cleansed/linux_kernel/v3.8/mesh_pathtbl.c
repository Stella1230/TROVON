static inline struct mesh_table *resize_dereference_mesh_paths(void)\r\n{\r\nreturn rcu_dereference_protected(mesh_paths,\r\nlockdep_is_held(&pathtbl_resize_lock));\r\n}\r\nstatic inline struct mesh_table *resize_dereference_mpp_paths(void)\r\n{\r\nreturn rcu_dereference_protected(mpp_paths,\r\nlockdep_is_held(&pathtbl_resize_lock));\r\n}\r\nstatic struct mesh_table *mesh_table_alloc(int size_order)\r\n{\r\nint i;\r\nstruct mesh_table *newtbl;\r\nnewtbl = kmalloc(sizeof(struct mesh_table), GFP_ATOMIC);\r\nif (!newtbl)\r\nreturn NULL;\r\nnewtbl->hash_buckets = kzalloc(sizeof(struct hlist_head) *\r\n(1 << size_order), GFP_ATOMIC);\r\nif (!newtbl->hash_buckets) {\r\nkfree(newtbl);\r\nreturn NULL;\r\n}\r\nnewtbl->hashwlock = kmalloc(sizeof(spinlock_t) *\r\n(1 << size_order), GFP_ATOMIC);\r\nif (!newtbl->hashwlock) {\r\nkfree(newtbl->hash_buckets);\r\nkfree(newtbl);\r\nreturn NULL;\r\n}\r\nnewtbl->size_order = size_order;\r\nnewtbl->hash_mask = (1 << size_order) - 1;\r\natomic_set(&newtbl->entries, 0);\r\nget_random_bytes(&newtbl->hash_rnd,\r\nsizeof(newtbl->hash_rnd));\r\nfor (i = 0; i <= newtbl->hash_mask; i++)\r\nspin_lock_init(&newtbl->hashwlock[i]);\r\nspin_lock_init(&newtbl->gates_lock);\r\nreturn newtbl;\r\n}\r\nstatic void __mesh_table_free(struct mesh_table *tbl)\r\n{\r\nkfree(tbl->hash_buckets);\r\nkfree(tbl->hashwlock);\r\nkfree(tbl);\r\n}\r\nstatic void mesh_table_free(struct mesh_table *tbl, bool free_leafs)\r\n{\r\nstruct hlist_head *mesh_hash;\r\nstruct hlist_node *p, *q;\r\nstruct mpath_node *gate;\r\nint i;\r\nmesh_hash = tbl->hash_buckets;\r\nfor (i = 0; i <= tbl->hash_mask; i++) {\r\nspin_lock_bh(&tbl->hashwlock[i]);\r\nhlist_for_each_safe(p, q, &mesh_hash[i]) {\r\ntbl->free_node(p, free_leafs);\r\natomic_dec(&tbl->entries);\r\n}\r\nspin_unlock_bh(&tbl->hashwlock[i]);\r\n}\r\nif (free_leafs) {\r\nspin_lock_bh(&tbl->gates_lock);\r\nhlist_for_each_entry_safe(gate, p, q,\r\ntbl->known_gates, list) {\r\nhlist_del(&gate->list);\r\nkfree(gate);\r\n}\r\nkfree(tbl->known_gates);\r\nspin_unlock_bh(&tbl->gates_lock);\r\n}\r\n__mesh_table_free(tbl);\r\n}\r\nstatic int mesh_table_grow(struct mesh_table *oldtbl,\r\nstruct mesh_table *newtbl)\r\n{\r\nstruct hlist_head *oldhash;\r\nstruct hlist_node *p, *q;\r\nint i;\r\nif (atomic_read(&oldtbl->entries)\r\n< oldtbl->mean_chain_len * (oldtbl->hash_mask + 1))\r\nreturn -EAGAIN;\r\nnewtbl->free_node = oldtbl->free_node;\r\nnewtbl->mean_chain_len = oldtbl->mean_chain_len;\r\nnewtbl->copy_node = oldtbl->copy_node;\r\nnewtbl->known_gates = oldtbl->known_gates;\r\natomic_set(&newtbl->entries, atomic_read(&oldtbl->entries));\r\noldhash = oldtbl->hash_buckets;\r\nfor (i = 0; i <= oldtbl->hash_mask; i++)\r\nhlist_for_each(p, &oldhash[i])\r\nif (oldtbl->copy_node(p, newtbl) < 0)\r\ngoto errcopy;\r\nreturn 0;\r\nerrcopy:\r\nfor (i = 0; i <= newtbl->hash_mask; i++) {\r\nhlist_for_each_safe(p, q, &newtbl->hash_buckets[i])\r\noldtbl->free_node(p, 0);\r\n}\r\nreturn -ENOMEM;\r\n}\r\nstatic u32 mesh_table_hash(u8 *addr, struct ieee80211_sub_if_data *sdata,\r\nstruct mesh_table *tbl)\r\n{\r\nreturn jhash_2words(*(u32 *)(addr+2), sdata->dev->ifindex, tbl->hash_rnd)\r\n& tbl->hash_mask;\r\n}\r\nvoid mesh_path_assign_nexthop(struct mesh_path *mpath, struct sta_info *sta)\r\n{\r\nstruct sk_buff *skb;\r\nstruct ieee80211_hdr *hdr;\r\nunsigned long flags;\r\nrcu_assign_pointer(mpath->next_hop, sta);\r\nspin_lock_irqsave(&mpath->frame_queue.lock, flags);\r\nskb_queue_walk(&mpath->frame_queue, skb) {\r\nhdr = (struct ieee80211_hdr *) skb->data;\r\nmemcpy(hdr->addr1, sta->sta.addr, ETH_ALEN);\r\nmemcpy(hdr->addr2, mpath->sdata->vif.addr, ETH_ALEN);\r\n}\r\nspin_unlock_irqrestore(&mpath->frame_queue.lock, flags);\r\n}\r\nstatic void prepare_for_gate(struct sk_buff *skb, char *dst_addr,\r\nstruct mesh_path *gate_mpath)\r\n{\r\nstruct ieee80211_hdr *hdr;\r\nstruct ieee80211s_hdr *mshdr;\r\nint mesh_hdrlen, hdrlen;\r\nchar *next_hop;\r\nhdr = (struct ieee80211_hdr *) skb->data;\r\nhdrlen = ieee80211_hdrlen(hdr->frame_control);\r\nmshdr = (struct ieee80211s_hdr *) (skb->data + hdrlen);\r\nif (!(mshdr->flags & MESH_FLAGS_AE)) {\r\nmesh_hdrlen = 6;\r\nskb_push(skb, 2 * ETH_ALEN);\r\nmemmove(skb->data, hdr, hdrlen + mesh_hdrlen);\r\nhdr = (struct ieee80211_hdr *) skb->data;\r\nmshdr = (struct ieee80211s_hdr *) (skb->data + hdrlen);\r\nmshdr->flags = MESH_FLAGS_AE_A5_A6;\r\nmemcpy(mshdr->eaddr1, hdr->addr3, ETH_ALEN);\r\nmemcpy(mshdr->eaddr2, hdr->addr4, ETH_ALEN);\r\n}\r\nhdr = (struct ieee80211_hdr *) skb->data;\r\nrcu_read_lock();\r\nnext_hop = rcu_dereference(gate_mpath->next_hop)->sta.addr;\r\nmemcpy(hdr->addr1, next_hop, ETH_ALEN);\r\nrcu_read_unlock();\r\nmemcpy(hdr->addr2, gate_mpath->sdata->vif.addr, ETH_ALEN);\r\nmemcpy(hdr->addr3, dst_addr, ETH_ALEN);\r\n}\r\nstatic void mesh_path_move_to_queue(struct mesh_path *gate_mpath,\r\nstruct mesh_path *from_mpath,\r\nbool copy)\r\n{\r\nstruct sk_buff *skb, *fskb, *tmp;\r\nstruct sk_buff_head failq;\r\nunsigned long flags;\r\nBUG_ON(gate_mpath == from_mpath);\r\nBUG_ON(!gate_mpath->next_hop);\r\n__skb_queue_head_init(&failq);\r\nspin_lock_irqsave(&from_mpath->frame_queue.lock, flags);\r\nskb_queue_splice_init(&from_mpath->frame_queue, &failq);\r\nspin_unlock_irqrestore(&from_mpath->frame_queue.lock, flags);\r\nskb_queue_walk_safe(&failq, fskb, tmp) {\r\nif (skb_queue_len(&gate_mpath->frame_queue) >=\r\nMESH_FRAME_QUEUE_LEN) {\r\nmpath_dbg(gate_mpath->sdata, "mpath queue full!\n");\r\nbreak;\r\n}\r\nskb = skb_copy(fskb, GFP_ATOMIC);\r\nif (WARN_ON(!skb))\r\nbreak;\r\nprepare_for_gate(skb, gate_mpath->dst, gate_mpath);\r\nskb_queue_tail(&gate_mpath->frame_queue, skb);\r\nif (copy)\r\ncontinue;\r\n__skb_unlink(fskb, &failq);\r\nkfree_skb(fskb);\r\n}\r\nmpath_dbg(gate_mpath->sdata, "Mpath queue for gate %pM has %d frames\n",\r\ngate_mpath->dst, skb_queue_len(&gate_mpath->frame_queue));\r\nif (!copy)\r\nreturn;\r\nspin_lock_irqsave(&from_mpath->frame_queue.lock, flags);\r\nskb_queue_splice(&failq, &from_mpath->frame_queue);\r\nspin_unlock_irqrestore(&from_mpath->frame_queue.lock, flags);\r\n}\r\nstatic struct mesh_path *mpath_lookup(struct mesh_table *tbl, u8 *dst,\r\nstruct ieee80211_sub_if_data *sdata)\r\n{\r\nstruct mesh_path *mpath;\r\nstruct hlist_node *n;\r\nstruct hlist_head *bucket;\r\nstruct mpath_node *node;\r\nbucket = &tbl->hash_buckets[mesh_table_hash(dst, sdata, tbl)];\r\nhlist_for_each_entry_rcu(node, n, bucket, list) {\r\nmpath = node->mpath;\r\nif (mpath->sdata == sdata &&\r\nether_addr_equal(dst, mpath->dst)) {\r\nif (MPATH_EXPIRED(mpath)) {\r\nspin_lock_bh(&mpath->state_lock);\r\nmpath->flags &= ~MESH_PATH_ACTIVE;\r\nspin_unlock_bh(&mpath->state_lock);\r\n}\r\nreturn mpath;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nstruct mesh_path *mesh_path_lookup(u8 *dst, struct ieee80211_sub_if_data *sdata)\r\n{\r\nreturn mpath_lookup(rcu_dereference(mesh_paths), dst, sdata);\r\n}\r\nstruct mesh_path *mpp_path_lookup(u8 *dst, struct ieee80211_sub_if_data *sdata)\r\n{\r\nreturn mpath_lookup(rcu_dereference(mpp_paths), dst, sdata);\r\n}\r\nstruct mesh_path *mesh_path_lookup_by_idx(int idx, struct ieee80211_sub_if_data *sdata)\r\n{\r\nstruct mesh_table *tbl = rcu_dereference(mesh_paths);\r\nstruct mpath_node *node;\r\nstruct hlist_node *p;\r\nint i;\r\nint j = 0;\r\nfor_each_mesh_entry(tbl, p, node, i) {\r\nif (sdata && node->mpath->sdata != sdata)\r\ncontinue;\r\nif (j++ == idx) {\r\nif (MPATH_EXPIRED(node->mpath)) {\r\nspin_lock_bh(&node->mpath->state_lock);\r\nnode->mpath->flags &= ~MESH_PATH_ACTIVE;\r\nspin_unlock_bh(&node->mpath->state_lock);\r\n}\r\nreturn node->mpath;\r\n}\r\n}\r\nreturn NULL;\r\n}\r\nint mesh_path_add_gate(struct mesh_path *mpath)\r\n{\r\nstruct mesh_table *tbl;\r\nstruct mpath_node *gate, *new_gate;\r\nstruct hlist_node *n;\r\nint err;\r\nrcu_read_lock();\r\ntbl = rcu_dereference(mesh_paths);\r\nhlist_for_each_entry_rcu(gate, n, tbl->known_gates, list)\r\nif (gate->mpath == mpath) {\r\nerr = -EEXIST;\r\ngoto err_rcu;\r\n}\r\nnew_gate = kzalloc(sizeof(struct mpath_node), GFP_ATOMIC);\r\nif (!new_gate) {\r\nerr = -ENOMEM;\r\ngoto err_rcu;\r\n}\r\nmpath->is_gate = true;\r\nmpath->sdata->u.mesh.num_gates++;\r\nnew_gate->mpath = mpath;\r\nspin_lock_bh(&tbl->gates_lock);\r\nhlist_add_head_rcu(&new_gate->list, tbl->known_gates);\r\nspin_unlock_bh(&tbl->gates_lock);\r\nrcu_read_unlock();\r\nmpath_dbg(mpath->sdata,\r\n"Mesh path: Recorded new gate: %pM. %d known gates\n",\r\nmpath->dst, mpath->sdata->u.mesh.num_gates);\r\nreturn 0;\r\nerr_rcu:\r\nrcu_read_unlock();\r\nreturn err;\r\n}\r\nstatic int mesh_gate_del(struct mesh_table *tbl, struct mesh_path *mpath)\r\n{\r\nstruct mpath_node *gate;\r\nstruct hlist_node *p, *q;\r\nhlist_for_each_entry_safe(gate, p, q, tbl->known_gates, list)\r\nif (gate->mpath == mpath) {\r\nspin_lock_bh(&tbl->gates_lock);\r\nhlist_del_rcu(&gate->list);\r\nkfree_rcu(gate, rcu);\r\nspin_unlock_bh(&tbl->gates_lock);\r\nmpath->sdata->u.mesh.num_gates--;\r\nmpath->is_gate = false;\r\nmpath_dbg(mpath->sdata,\r\n"Mesh path: Deleted gate: %pM. %d known gates\n",\r\nmpath->dst, mpath->sdata->u.mesh.num_gates);\r\nbreak;\r\n}\r\nreturn 0;\r\n}\r\nint mesh_gate_num(struct ieee80211_sub_if_data *sdata)\r\n{\r\nreturn sdata->u.mesh.num_gates;\r\n}\r\nint mesh_path_add(u8 *dst, struct ieee80211_sub_if_data *sdata)\r\n{\r\nstruct ieee80211_if_mesh *ifmsh = &sdata->u.mesh;\r\nstruct ieee80211_local *local = sdata->local;\r\nstruct mesh_table *tbl;\r\nstruct mesh_path *mpath, *new_mpath;\r\nstruct mpath_node *node, *new_node;\r\nstruct hlist_head *bucket;\r\nstruct hlist_node *n;\r\nint grow = 0;\r\nint err = 0;\r\nu32 hash_idx;\r\nif (ether_addr_equal(dst, sdata->vif.addr))\r\nreturn -ENOTSUPP;\r\nif (is_multicast_ether_addr(dst))\r\nreturn -ENOTSUPP;\r\nif (atomic_add_unless(&sdata->u.mesh.mpaths, 1, MESH_MAX_MPATHS) == 0)\r\nreturn -ENOSPC;\r\nerr = -ENOMEM;\r\nnew_mpath = kzalloc(sizeof(struct mesh_path), GFP_ATOMIC);\r\nif (!new_mpath)\r\ngoto err_path_alloc;\r\nnew_node = kmalloc(sizeof(struct mpath_node), GFP_ATOMIC);\r\nif (!new_node)\r\ngoto err_node_alloc;\r\nread_lock_bh(&pathtbl_resize_lock);\r\nmemcpy(new_mpath->dst, dst, ETH_ALEN);\r\neth_broadcast_addr(new_mpath->rann_snd_addr);\r\nnew_mpath->is_root = false;\r\nnew_mpath->sdata = sdata;\r\nnew_mpath->flags = 0;\r\nskb_queue_head_init(&new_mpath->frame_queue);\r\nnew_node->mpath = new_mpath;\r\nnew_mpath->timer.data = (unsigned long) new_mpath;\r\nnew_mpath->timer.function = mesh_path_timer;\r\nnew_mpath->exp_time = jiffies;\r\nspin_lock_init(&new_mpath->state_lock);\r\ninit_timer(&new_mpath->timer);\r\ntbl = resize_dereference_mesh_paths();\r\nhash_idx = mesh_table_hash(dst, sdata, tbl);\r\nbucket = &tbl->hash_buckets[hash_idx];\r\nspin_lock(&tbl->hashwlock[hash_idx]);\r\nerr = -EEXIST;\r\nhlist_for_each_entry(node, n, bucket, list) {\r\nmpath = node->mpath;\r\nif (mpath->sdata == sdata &&\r\nether_addr_equal(dst, mpath->dst))\r\ngoto err_exists;\r\n}\r\nhlist_add_head_rcu(&new_node->list, bucket);\r\nif (atomic_inc_return(&tbl->entries) >=\r\ntbl->mean_chain_len * (tbl->hash_mask + 1))\r\ngrow = 1;\r\nmesh_paths_generation++;\r\nspin_unlock(&tbl->hashwlock[hash_idx]);\r\nread_unlock_bh(&pathtbl_resize_lock);\r\nif (grow) {\r\nset_bit(MESH_WORK_GROW_MPATH_TABLE, &ifmsh->wrkq_flags);\r\nieee80211_queue_work(&local->hw, &sdata->work);\r\n}\r\nreturn 0;\r\nerr_exists:\r\nspin_unlock(&tbl->hashwlock[hash_idx]);\r\nread_unlock_bh(&pathtbl_resize_lock);\r\nkfree(new_node);\r\nerr_node_alloc:\r\nkfree(new_mpath);\r\nerr_path_alloc:\r\natomic_dec(&sdata->u.mesh.mpaths);\r\nreturn err;\r\n}\r\nstatic void mesh_table_free_rcu(struct rcu_head *rcu)\r\n{\r\nstruct mesh_table *tbl = container_of(rcu, struct mesh_table, rcu_head);\r\nmesh_table_free(tbl, false);\r\n}\r\nvoid mesh_mpath_table_grow(void)\r\n{\r\nstruct mesh_table *oldtbl, *newtbl;\r\nwrite_lock_bh(&pathtbl_resize_lock);\r\noldtbl = resize_dereference_mesh_paths();\r\nnewtbl = mesh_table_alloc(oldtbl->size_order + 1);\r\nif (!newtbl)\r\ngoto out;\r\nif (mesh_table_grow(oldtbl, newtbl) < 0) {\r\n__mesh_table_free(newtbl);\r\ngoto out;\r\n}\r\nrcu_assign_pointer(mesh_paths, newtbl);\r\ncall_rcu(&oldtbl->rcu_head, mesh_table_free_rcu);\r\nout:\r\nwrite_unlock_bh(&pathtbl_resize_lock);\r\n}\r\nvoid mesh_mpp_table_grow(void)\r\n{\r\nstruct mesh_table *oldtbl, *newtbl;\r\nwrite_lock_bh(&pathtbl_resize_lock);\r\noldtbl = resize_dereference_mpp_paths();\r\nnewtbl = mesh_table_alloc(oldtbl->size_order + 1);\r\nif (!newtbl)\r\ngoto out;\r\nif (mesh_table_grow(oldtbl, newtbl) < 0) {\r\n__mesh_table_free(newtbl);\r\ngoto out;\r\n}\r\nrcu_assign_pointer(mpp_paths, newtbl);\r\ncall_rcu(&oldtbl->rcu_head, mesh_table_free_rcu);\r\nout:\r\nwrite_unlock_bh(&pathtbl_resize_lock);\r\n}\r\nint mpp_path_add(u8 *dst, u8 *mpp, struct ieee80211_sub_if_data *sdata)\r\n{\r\nstruct ieee80211_if_mesh *ifmsh = &sdata->u.mesh;\r\nstruct ieee80211_local *local = sdata->local;\r\nstruct mesh_table *tbl;\r\nstruct mesh_path *mpath, *new_mpath;\r\nstruct mpath_node *node, *new_node;\r\nstruct hlist_head *bucket;\r\nstruct hlist_node *n;\r\nint grow = 0;\r\nint err = 0;\r\nu32 hash_idx;\r\nif (ether_addr_equal(dst, sdata->vif.addr))\r\nreturn -ENOTSUPP;\r\nif (is_multicast_ether_addr(dst))\r\nreturn -ENOTSUPP;\r\nerr = -ENOMEM;\r\nnew_mpath = kzalloc(sizeof(struct mesh_path), GFP_ATOMIC);\r\nif (!new_mpath)\r\ngoto err_path_alloc;\r\nnew_node = kmalloc(sizeof(struct mpath_node), GFP_ATOMIC);\r\nif (!new_node)\r\ngoto err_node_alloc;\r\nread_lock_bh(&pathtbl_resize_lock);\r\nmemcpy(new_mpath->dst, dst, ETH_ALEN);\r\nmemcpy(new_mpath->mpp, mpp, ETH_ALEN);\r\nnew_mpath->sdata = sdata;\r\nnew_mpath->flags = 0;\r\nskb_queue_head_init(&new_mpath->frame_queue);\r\nnew_node->mpath = new_mpath;\r\ninit_timer(&new_mpath->timer);\r\nnew_mpath->exp_time = jiffies;\r\nspin_lock_init(&new_mpath->state_lock);\r\ntbl = resize_dereference_mpp_paths();\r\nhash_idx = mesh_table_hash(dst, sdata, tbl);\r\nbucket = &tbl->hash_buckets[hash_idx];\r\nspin_lock(&tbl->hashwlock[hash_idx]);\r\nerr = -EEXIST;\r\nhlist_for_each_entry(node, n, bucket, list) {\r\nmpath = node->mpath;\r\nif (mpath->sdata == sdata &&\r\nether_addr_equal(dst, mpath->dst))\r\ngoto err_exists;\r\n}\r\nhlist_add_head_rcu(&new_node->list, bucket);\r\nif (atomic_inc_return(&tbl->entries) >=\r\ntbl->mean_chain_len * (tbl->hash_mask + 1))\r\ngrow = 1;\r\nspin_unlock(&tbl->hashwlock[hash_idx]);\r\nread_unlock_bh(&pathtbl_resize_lock);\r\nif (grow) {\r\nset_bit(MESH_WORK_GROW_MPP_TABLE, &ifmsh->wrkq_flags);\r\nieee80211_queue_work(&local->hw, &sdata->work);\r\n}\r\nreturn 0;\r\nerr_exists:\r\nspin_unlock(&tbl->hashwlock[hash_idx]);\r\nread_unlock_bh(&pathtbl_resize_lock);\r\nkfree(new_node);\r\nerr_node_alloc:\r\nkfree(new_mpath);\r\nerr_path_alloc:\r\nreturn err;\r\n}\r\nvoid mesh_plink_broken(struct sta_info *sta)\r\n{\r\nstruct mesh_table *tbl;\r\nstatic const u8 bcast[ETH_ALEN] = {0xff, 0xff, 0xff, 0xff, 0xff, 0xff};\r\nstruct mesh_path *mpath;\r\nstruct mpath_node *node;\r\nstruct hlist_node *p;\r\nstruct ieee80211_sub_if_data *sdata = sta->sdata;\r\nint i;\r\n__le16 reason = cpu_to_le16(WLAN_REASON_MESH_PATH_DEST_UNREACHABLE);\r\nrcu_read_lock();\r\ntbl = rcu_dereference(mesh_paths);\r\nfor_each_mesh_entry(tbl, p, node, i) {\r\nmpath = node->mpath;\r\nif (rcu_dereference(mpath->next_hop) == sta &&\r\nmpath->flags & MESH_PATH_ACTIVE &&\r\n!(mpath->flags & MESH_PATH_FIXED)) {\r\nspin_lock_bh(&mpath->state_lock);\r\nmpath->flags &= ~MESH_PATH_ACTIVE;\r\n++mpath->sn;\r\nspin_unlock_bh(&mpath->state_lock);\r\nmesh_path_error_tx(sdata->u.mesh.mshcfg.element_ttl,\r\nmpath->dst, cpu_to_le32(mpath->sn),\r\nreason, bcast, sdata);\r\n}\r\n}\r\nrcu_read_unlock();\r\n}\r\nstatic void mesh_path_node_reclaim(struct rcu_head *rp)\r\n{\r\nstruct mpath_node *node = container_of(rp, struct mpath_node, rcu);\r\nstruct ieee80211_sub_if_data *sdata = node->mpath->sdata;\r\ndel_timer_sync(&node->mpath->timer);\r\natomic_dec(&sdata->u.mesh.mpaths);\r\nkfree(node->mpath);\r\nkfree(node);\r\n}\r\nstatic void __mesh_path_del(struct mesh_table *tbl, struct mpath_node *node)\r\n{\r\nstruct mesh_path *mpath;\r\nmpath = node->mpath;\r\nspin_lock(&mpath->state_lock);\r\nmpath->flags |= MESH_PATH_RESOLVING;\r\nif (mpath->is_gate)\r\nmesh_gate_del(tbl, mpath);\r\nhlist_del_rcu(&node->list);\r\ncall_rcu(&node->rcu, mesh_path_node_reclaim);\r\nspin_unlock(&mpath->state_lock);\r\natomic_dec(&tbl->entries);\r\n}\r\nvoid mesh_path_flush_by_nexthop(struct sta_info *sta)\r\n{\r\nstruct mesh_table *tbl;\r\nstruct mesh_path *mpath;\r\nstruct mpath_node *node;\r\nstruct hlist_node *p;\r\nint i;\r\nrcu_read_lock();\r\nread_lock_bh(&pathtbl_resize_lock);\r\ntbl = resize_dereference_mesh_paths();\r\nfor_each_mesh_entry(tbl, p, node, i) {\r\nmpath = node->mpath;\r\nif (rcu_dereference(mpath->next_hop) == sta) {\r\nspin_lock(&tbl->hashwlock[i]);\r\n__mesh_path_del(tbl, node);\r\nspin_unlock(&tbl->hashwlock[i]);\r\n}\r\n}\r\nread_unlock_bh(&pathtbl_resize_lock);\r\nrcu_read_unlock();\r\n}\r\nstatic void table_flush_by_iface(struct mesh_table *tbl,\r\nstruct ieee80211_sub_if_data *sdata)\r\n{\r\nstruct mesh_path *mpath;\r\nstruct mpath_node *node;\r\nstruct hlist_node *p;\r\nint i;\r\nWARN_ON(!rcu_read_lock_held());\r\nfor_each_mesh_entry(tbl, p, node, i) {\r\nmpath = node->mpath;\r\nif (mpath->sdata != sdata)\r\ncontinue;\r\nspin_lock_bh(&tbl->hashwlock[i]);\r\n__mesh_path_del(tbl, node);\r\nspin_unlock_bh(&tbl->hashwlock[i]);\r\n}\r\n}\r\nvoid mesh_path_flush_by_iface(struct ieee80211_sub_if_data *sdata)\r\n{\r\nstruct mesh_table *tbl;\r\nrcu_read_lock();\r\nread_lock_bh(&pathtbl_resize_lock);\r\ntbl = resize_dereference_mesh_paths();\r\ntable_flush_by_iface(tbl, sdata);\r\ntbl = resize_dereference_mpp_paths();\r\ntable_flush_by_iface(tbl, sdata);\r\nread_unlock_bh(&pathtbl_resize_lock);\r\nrcu_read_unlock();\r\n}\r\nint mesh_path_del(u8 *addr, struct ieee80211_sub_if_data *sdata)\r\n{\r\nstruct mesh_table *tbl;\r\nstruct mesh_path *mpath;\r\nstruct mpath_node *node;\r\nstruct hlist_head *bucket;\r\nstruct hlist_node *n;\r\nint hash_idx;\r\nint err = 0;\r\nread_lock_bh(&pathtbl_resize_lock);\r\ntbl = resize_dereference_mesh_paths();\r\nhash_idx = mesh_table_hash(addr, sdata, tbl);\r\nbucket = &tbl->hash_buckets[hash_idx];\r\nspin_lock(&tbl->hashwlock[hash_idx]);\r\nhlist_for_each_entry(node, n, bucket, list) {\r\nmpath = node->mpath;\r\nif (mpath->sdata == sdata &&\r\nether_addr_equal(addr, mpath->dst)) {\r\n__mesh_path_del(tbl, node);\r\ngoto enddel;\r\n}\r\n}\r\nerr = -ENXIO;\r\nenddel:\r\nmesh_paths_generation++;\r\nspin_unlock(&tbl->hashwlock[hash_idx]);\r\nread_unlock_bh(&pathtbl_resize_lock);\r\nreturn err;\r\n}\r\nvoid mesh_path_tx_pending(struct mesh_path *mpath)\r\n{\r\nif (mpath->flags & MESH_PATH_ACTIVE)\r\nieee80211_add_pending_skbs(mpath->sdata->local,\r\n&mpath->frame_queue);\r\n}\r\nint mesh_path_send_to_gates(struct mesh_path *mpath)\r\n{\r\nstruct ieee80211_sub_if_data *sdata = mpath->sdata;\r\nstruct hlist_node *n;\r\nstruct mesh_table *tbl;\r\nstruct mesh_path *from_mpath = mpath;\r\nstruct mpath_node *gate = NULL;\r\nbool copy = false;\r\nstruct hlist_head *known_gates;\r\nrcu_read_lock();\r\ntbl = rcu_dereference(mesh_paths);\r\nknown_gates = tbl->known_gates;\r\nrcu_read_unlock();\r\nif (!known_gates)\r\nreturn -EHOSTUNREACH;\r\nhlist_for_each_entry_rcu(gate, n, known_gates, list) {\r\nif (gate->mpath->sdata != sdata)\r\ncontinue;\r\nif (gate->mpath->flags & MESH_PATH_ACTIVE) {\r\nmpath_dbg(sdata, "Forwarding to %pM\n", gate->mpath->dst);\r\nmesh_path_move_to_queue(gate->mpath, from_mpath, copy);\r\nfrom_mpath = gate->mpath;\r\ncopy = true;\r\n} else {\r\nmpath_dbg(sdata,\r\n"Not forwarding %p (flags %#x)\n",\r\ngate->mpath, gate->mpath->flags);\r\n}\r\n}\r\nhlist_for_each_entry_rcu(gate, n, known_gates, list)\r\nif (gate->mpath->sdata == sdata) {\r\nmpath_dbg(sdata, "Sending to %pM\n", gate->mpath->dst);\r\nmesh_path_tx_pending(gate->mpath);\r\n}\r\nreturn (from_mpath == mpath) ? -EHOSTUNREACH : 0;\r\n}\r\nvoid mesh_path_discard_frame(struct sk_buff *skb,\r\nstruct ieee80211_sub_if_data *sdata)\r\n{\r\nkfree_skb(skb);\r\nsdata->u.mesh.mshstats.dropped_frames_no_route++;\r\n}\r\nvoid mesh_path_flush_pending(struct mesh_path *mpath)\r\n{\r\nstruct sk_buff *skb;\r\nwhile ((skb = skb_dequeue(&mpath->frame_queue)) != NULL)\r\nmesh_path_discard_frame(skb, mpath->sdata);\r\n}\r\nvoid mesh_path_fix_nexthop(struct mesh_path *mpath, struct sta_info *next_hop)\r\n{\r\nspin_lock_bh(&mpath->state_lock);\r\nmesh_path_assign_nexthop(mpath, next_hop);\r\nmpath->sn = 0xffff;\r\nmpath->metric = 0;\r\nmpath->hop_count = 0;\r\nmpath->exp_time = 0;\r\nmpath->flags |= MESH_PATH_FIXED;\r\nmesh_path_activate(mpath);\r\nspin_unlock_bh(&mpath->state_lock);\r\nmesh_path_tx_pending(mpath);\r\n}\r\nstatic void mesh_path_node_free(struct hlist_node *p, bool free_leafs)\r\n{\r\nstruct mesh_path *mpath;\r\nstruct mpath_node *node = hlist_entry(p, struct mpath_node, list);\r\nmpath = node->mpath;\r\nhlist_del_rcu(p);\r\nif (free_leafs) {\r\ndel_timer_sync(&mpath->timer);\r\nkfree(mpath);\r\n}\r\nkfree(node);\r\n}\r\nstatic int mesh_path_node_copy(struct hlist_node *p, struct mesh_table *newtbl)\r\n{\r\nstruct mesh_path *mpath;\r\nstruct mpath_node *node, *new_node;\r\nu32 hash_idx;\r\nnew_node = kmalloc(sizeof(struct mpath_node), GFP_ATOMIC);\r\nif (new_node == NULL)\r\nreturn -ENOMEM;\r\nnode = hlist_entry(p, struct mpath_node, list);\r\nmpath = node->mpath;\r\nnew_node->mpath = mpath;\r\nhash_idx = mesh_table_hash(mpath->dst, mpath->sdata, newtbl);\r\nhlist_add_head(&new_node->list,\r\n&newtbl->hash_buckets[hash_idx]);\r\nreturn 0;\r\n}\r\nint mesh_pathtbl_init(void)\r\n{\r\nstruct mesh_table *tbl_path, *tbl_mpp;\r\nint ret;\r\ntbl_path = mesh_table_alloc(INIT_PATHS_SIZE_ORDER);\r\nif (!tbl_path)\r\nreturn -ENOMEM;\r\ntbl_path->free_node = &mesh_path_node_free;\r\ntbl_path->copy_node = &mesh_path_node_copy;\r\ntbl_path->mean_chain_len = MEAN_CHAIN_LEN;\r\ntbl_path->known_gates = kzalloc(sizeof(struct hlist_head), GFP_ATOMIC);\r\nif (!tbl_path->known_gates) {\r\nret = -ENOMEM;\r\ngoto free_path;\r\n}\r\nINIT_HLIST_HEAD(tbl_path->known_gates);\r\ntbl_mpp = mesh_table_alloc(INIT_PATHS_SIZE_ORDER);\r\nif (!tbl_mpp) {\r\nret = -ENOMEM;\r\ngoto free_path;\r\n}\r\ntbl_mpp->free_node = &mesh_path_node_free;\r\ntbl_mpp->copy_node = &mesh_path_node_copy;\r\ntbl_mpp->mean_chain_len = MEAN_CHAIN_LEN;\r\ntbl_mpp->known_gates = kzalloc(sizeof(struct hlist_head), GFP_ATOMIC);\r\nif (!tbl_mpp->known_gates) {\r\nret = -ENOMEM;\r\ngoto free_mpp;\r\n}\r\nINIT_HLIST_HEAD(tbl_mpp->known_gates);\r\nRCU_INIT_POINTER(mesh_paths, tbl_path);\r\nRCU_INIT_POINTER(mpp_paths, tbl_mpp);\r\nreturn 0;\r\nfree_mpp:\r\nmesh_table_free(tbl_mpp, true);\r\nfree_path:\r\nmesh_table_free(tbl_path, true);\r\nreturn ret;\r\n}\r\nvoid mesh_path_expire(struct ieee80211_sub_if_data *sdata)\r\n{\r\nstruct mesh_table *tbl;\r\nstruct mesh_path *mpath;\r\nstruct mpath_node *node;\r\nstruct hlist_node *p;\r\nint i;\r\nrcu_read_lock();\r\ntbl = rcu_dereference(mesh_paths);\r\nfor_each_mesh_entry(tbl, p, node, i) {\r\nif (node->mpath->sdata != sdata)\r\ncontinue;\r\nmpath = node->mpath;\r\nif ((!(mpath->flags & MESH_PATH_RESOLVING)) &&\r\n(!(mpath->flags & MESH_PATH_FIXED)) &&\r\ntime_after(jiffies, mpath->exp_time + MESH_PATH_EXPIRE))\r\nmesh_path_del(mpath->dst, mpath->sdata);\r\n}\r\nrcu_read_unlock();\r\n}\r\nvoid mesh_pathtbl_unregister(void)\r\n{\r\nmesh_table_free(rcu_dereference_protected(mesh_paths, 1), true);\r\nmesh_table_free(rcu_dereference_protected(mpp_paths, 1), true);\r\n}
