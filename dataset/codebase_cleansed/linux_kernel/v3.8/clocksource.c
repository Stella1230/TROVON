void timecounter_init(struct timecounter *tc,\r\nconst struct cyclecounter *cc,\r\nu64 start_tstamp)\r\n{\r\ntc->cc = cc;\r\ntc->cycle_last = cc->read(cc);\r\ntc->nsec = start_tstamp;\r\n}\r\nstatic u64 timecounter_read_delta(struct timecounter *tc)\r\n{\r\ncycle_t cycle_now, cycle_delta;\r\nu64 ns_offset;\r\ncycle_now = tc->cc->read(tc->cc);\r\ncycle_delta = (cycle_now - tc->cycle_last) & tc->cc->mask;\r\nns_offset = cyclecounter_cyc2ns(tc->cc, cycle_delta);\r\ntc->cycle_last = cycle_now;\r\nreturn ns_offset;\r\n}\r\nu64 timecounter_read(struct timecounter *tc)\r\n{\r\nu64 nsec;\r\nnsec = timecounter_read_delta(tc);\r\nnsec += tc->nsec;\r\ntc->nsec = nsec;\r\nreturn nsec;\r\n}\r\nu64 timecounter_cyc2time(struct timecounter *tc,\r\ncycle_t cycle_tstamp)\r\n{\r\nu64 cycle_delta = (cycle_tstamp - tc->cycle_last) & tc->cc->mask;\r\nu64 nsec;\r\nif (cycle_delta > tc->cc->mask / 2) {\r\ncycle_delta = (tc->cycle_last - cycle_tstamp) & tc->cc->mask;\r\nnsec = tc->nsec - cyclecounter_cyc2ns(tc->cc, cycle_delta);\r\n} else {\r\nnsec = cyclecounter_cyc2ns(tc->cc, cycle_delta) + tc->nsec;\r\n}\r\nreturn nsec;\r\n}\r\nvoid\r\nclocks_calc_mult_shift(u32 *mult, u32 *shift, u32 from, u32 to, u32 maxsec)\r\n{\r\nu64 tmp;\r\nu32 sft, sftacc= 32;\r\ntmp = ((u64)maxsec * from) >> 32;\r\nwhile (tmp) {\r\ntmp >>=1;\r\nsftacc--;\r\n}\r\nfor (sft = 32; sft > 0; sft--) {\r\ntmp = (u64) to << sft;\r\ntmp += from / 2;\r\ndo_div(tmp, from);\r\nif ((tmp >> sftacc) == 0)\r\nbreak;\r\n}\r\n*mult = tmp;\r\n*shift = sft;\r\n}\r\nstatic void clocksource_watchdog_work(struct work_struct *work)\r\n{\r\nkthread_run(clocksource_watchdog_kthread, NULL, "kwatchdog");\r\n}\r\nstatic void __clocksource_unstable(struct clocksource *cs)\r\n{\r\ncs->flags &= ~(CLOCK_SOURCE_VALID_FOR_HRES | CLOCK_SOURCE_WATCHDOG);\r\ncs->flags |= CLOCK_SOURCE_UNSTABLE;\r\nif (finished_booting)\r\nschedule_work(&watchdog_work);\r\n}\r\nstatic void clocksource_unstable(struct clocksource *cs, int64_t delta)\r\n{\r\nprintk(KERN_WARNING "Clocksource %s unstable (delta = %Ld ns)\n",\r\ncs->name, delta);\r\n__clocksource_unstable(cs);\r\n}\r\nvoid clocksource_mark_unstable(struct clocksource *cs)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&watchdog_lock, flags);\r\nif (!(cs->flags & CLOCK_SOURCE_UNSTABLE)) {\r\nif (list_empty(&cs->wd_list))\r\nlist_add(&cs->wd_list, &watchdog_list);\r\n__clocksource_unstable(cs);\r\n}\r\nspin_unlock_irqrestore(&watchdog_lock, flags);\r\n}\r\nstatic void clocksource_watchdog(unsigned long data)\r\n{\r\nstruct clocksource *cs;\r\ncycle_t csnow, wdnow;\r\nint64_t wd_nsec, cs_nsec;\r\nint next_cpu, reset_pending;\r\nspin_lock(&watchdog_lock);\r\nif (!watchdog_running)\r\ngoto out;\r\nreset_pending = atomic_read(&watchdog_reset_pending);\r\nlist_for_each_entry(cs, &watchdog_list, wd_list) {\r\nif (cs->flags & CLOCK_SOURCE_UNSTABLE) {\r\nif (finished_booting)\r\nschedule_work(&watchdog_work);\r\ncontinue;\r\n}\r\nlocal_irq_disable();\r\ncsnow = cs->read(cs);\r\nwdnow = watchdog->read(watchdog);\r\nlocal_irq_enable();\r\nif (!(cs->flags & CLOCK_SOURCE_WATCHDOG) ||\r\natomic_read(&watchdog_reset_pending)) {\r\ncs->flags |= CLOCK_SOURCE_WATCHDOG;\r\ncs->wd_last = wdnow;\r\ncs->cs_last = csnow;\r\ncontinue;\r\n}\r\nwd_nsec = clocksource_cyc2ns((wdnow - cs->wd_last) & watchdog->mask,\r\nwatchdog->mult, watchdog->shift);\r\ncs_nsec = clocksource_cyc2ns((csnow - cs->cs_last) &\r\ncs->mask, cs->mult, cs->shift);\r\ncs->cs_last = csnow;\r\ncs->wd_last = wdnow;\r\nif (atomic_read(&watchdog_reset_pending))\r\ncontinue;\r\nif ((abs(cs_nsec - wd_nsec) > WATCHDOG_THRESHOLD)) {\r\nclocksource_unstable(cs, cs_nsec - wd_nsec);\r\ncontinue;\r\n}\r\nif (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) &&\r\n(cs->flags & CLOCK_SOURCE_IS_CONTINUOUS) &&\r\n(watchdog->flags & CLOCK_SOURCE_IS_CONTINUOUS)) {\r\ncs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;\r\ntick_clock_notify();\r\n}\r\n}\r\nif (reset_pending)\r\natomic_dec(&watchdog_reset_pending);\r\nnext_cpu = cpumask_next(raw_smp_processor_id(), cpu_online_mask);\r\nif (next_cpu >= nr_cpu_ids)\r\nnext_cpu = cpumask_first(cpu_online_mask);\r\nwatchdog_timer.expires += WATCHDOG_INTERVAL;\r\nadd_timer_on(&watchdog_timer, next_cpu);\r\nout:\r\nspin_unlock(&watchdog_lock);\r\n}\r\nstatic inline void clocksource_start_watchdog(void)\r\n{\r\nif (watchdog_running || !watchdog || list_empty(&watchdog_list))\r\nreturn;\r\ninit_timer(&watchdog_timer);\r\nwatchdog_timer.function = clocksource_watchdog;\r\nwatchdog_timer.expires = jiffies + WATCHDOG_INTERVAL;\r\nadd_timer_on(&watchdog_timer, cpumask_first(cpu_online_mask));\r\nwatchdog_running = 1;\r\n}\r\nstatic inline void clocksource_stop_watchdog(void)\r\n{\r\nif (!watchdog_running || (watchdog && !list_empty(&watchdog_list)))\r\nreturn;\r\ndel_timer(&watchdog_timer);\r\nwatchdog_running = 0;\r\n}\r\nstatic inline void clocksource_reset_watchdog(void)\r\n{\r\nstruct clocksource *cs;\r\nlist_for_each_entry(cs, &watchdog_list, wd_list)\r\ncs->flags &= ~CLOCK_SOURCE_WATCHDOG;\r\n}\r\nstatic void clocksource_resume_watchdog(void)\r\n{\r\natomic_inc(&watchdog_reset_pending);\r\n}\r\nstatic void clocksource_enqueue_watchdog(struct clocksource *cs)\r\n{\r\nunsigned long flags;\r\nspin_lock_irqsave(&watchdog_lock, flags);\r\nif (cs->flags & CLOCK_SOURCE_MUST_VERIFY) {\r\nlist_add(&cs->wd_list, &watchdog_list);\r\ncs->flags &= ~CLOCK_SOURCE_WATCHDOG;\r\n} else {\r\nif (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS)\r\ncs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;\r\nif (!watchdog || cs->rating > watchdog->rating) {\r\nwatchdog = cs;\r\nclocksource_reset_watchdog();\r\n}\r\n}\r\nclocksource_start_watchdog();\r\nspin_unlock_irqrestore(&watchdog_lock, flags);\r\n}\r\nstatic void clocksource_dequeue_watchdog(struct clocksource *cs)\r\n{\r\nstruct clocksource *tmp;\r\nunsigned long flags;\r\nspin_lock_irqsave(&watchdog_lock, flags);\r\nif (cs->flags & CLOCK_SOURCE_MUST_VERIFY) {\r\nlist_del_init(&cs->wd_list);\r\n} else if (cs == watchdog) {\r\nclocksource_reset_watchdog();\r\nwatchdog = NULL;\r\nlist_for_each_entry(tmp, &clocksource_list, list) {\r\nif (tmp == cs || tmp->flags & CLOCK_SOURCE_MUST_VERIFY)\r\ncontinue;\r\nif (!watchdog || tmp->rating > watchdog->rating)\r\nwatchdog = tmp;\r\n}\r\n}\r\ncs->flags &= ~CLOCK_SOURCE_WATCHDOG;\r\nclocksource_stop_watchdog();\r\nspin_unlock_irqrestore(&watchdog_lock, flags);\r\n}\r\nstatic int clocksource_watchdog_kthread(void *data)\r\n{\r\nstruct clocksource *cs, *tmp;\r\nunsigned long flags;\r\nLIST_HEAD(unstable);\r\nmutex_lock(&clocksource_mutex);\r\nspin_lock_irqsave(&watchdog_lock, flags);\r\nlist_for_each_entry_safe(cs, tmp, &watchdog_list, wd_list)\r\nif (cs->flags & CLOCK_SOURCE_UNSTABLE) {\r\nlist_del_init(&cs->wd_list);\r\nlist_add(&cs->wd_list, &unstable);\r\n}\r\nclocksource_stop_watchdog();\r\nspin_unlock_irqrestore(&watchdog_lock, flags);\r\nlist_for_each_entry_safe(cs, tmp, &unstable, wd_list) {\r\nlist_del_init(&cs->wd_list);\r\n__clocksource_change_rating(cs, 0);\r\n}\r\nmutex_unlock(&clocksource_mutex);\r\nreturn 0;\r\n}\r\nstatic void clocksource_enqueue_watchdog(struct clocksource *cs)\r\n{\r\nif (cs->flags & CLOCK_SOURCE_IS_CONTINUOUS)\r\ncs->flags |= CLOCK_SOURCE_VALID_FOR_HRES;\r\n}\r\nstatic inline void clocksource_dequeue_watchdog(struct clocksource *cs) { }\r\nstatic inline void clocksource_resume_watchdog(void) { }\r\nstatic inline int clocksource_watchdog_kthread(void *data) { return 0; }\r\nvoid clocksource_suspend(void)\r\n{\r\nstruct clocksource *cs;\r\nlist_for_each_entry_reverse(cs, &clocksource_list, list)\r\nif (cs->suspend)\r\ncs->suspend(cs);\r\n}\r\nvoid clocksource_resume(void)\r\n{\r\nstruct clocksource *cs;\r\nlist_for_each_entry(cs, &clocksource_list, list)\r\nif (cs->resume)\r\ncs->resume(cs);\r\nclocksource_resume_watchdog();\r\n}\r\nvoid clocksource_touch_watchdog(void)\r\n{\r\nclocksource_resume_watchdog();\r\n}\r\nstatic u32 clocksource_max_adjustment(struct clocksource *cs)\r\n{\r\nu64 ret;\r\nret = (u64)cs->mult * 11;\r\ndo_div(ret,100);\r\nreturn (u32)ret;\r\n}\r\nstatic u64 clocksource_max_deferment(struct clocksource *cs)\r\n{\r\nu64 max_nsecs, max_cycles;\r\nmax_cycles = 1ULL << (63 - (ilog2(cs->mult + cs->maxadj) + 1));\r\nmax_cycles = min_t(u64, max_cycles, (u64) cs->mask);\r\nmax_nsecs = clocksource_cyc2ns(max_cycles, cs->mult - cs->maxadj,\r\ncs->shift);\r\nreturn max_nsecs - (max_nsecs >> 3);\r\n}\r\nstatic void clocksource_select(void)\r\n{\r\nstruct clocksource *best, *cs;\r\nif (!finished_booting || list_empty(&clocksource_list))\r\nreturn;\r\nbest = list_first_entry(&clocksource_list, struct clocksource, list);\r\nlist_for_each_entry(cs, &clocksource_list, list) {\r\nif (strcmp(cs->name, override_name) != 0)\r\ncontinue;\r\nif (!(cs->flags & CLOCK_SOURCE_VALID_FOR_HRES) &&\r\ntick_oneshot_mode_active()) {\r\nprintk(KERN_WARNING "Override clocksource %s is not "\r\n"HRT compatible. Cannot switch while in "\r\n"HRT/NOHZ mode\n", cs->name);\r\noverride_name[0] = 0;\r\n} else\r\nbest = cs;\r\nbreak;\r\n}\r\nif (curr_clocksource != best) {\r\nprintk(KERN_INFO "Switching to clocksource %s\n", best->name);\r\ncurr_clocksource = best;\r\ntimekeeping_notify(curr_clocksource);\r\n}\r\n}\r\nstatic inline void clocksource_select(void) { }\r\nstatic int __init clocksource_done_booting(void)\r\n{\r\nmutex_lock(&clocksource_mutex);\r\ncurr_clocksource = clocksource_default_clock();\r\nmutex_unlock(&clocksource_mutex);\r\nfinished_booting = 1;\r\nclocksource_watchdog_kthread(NULL);\r\nmutex_lock(&clocksource_mutex);\r\nclocksource_select();\r\nmutex_unlock(&clocksource_mutex);\r\nreturn 0;\r\n}\r\nstatic void clocksource_enqueue(struct clocksource *cs)\r\n{\r\nstruct list_head *entry = &clocksource_list;\r\nstruct clocksource *tmp;\r\nlist_for_each_entry(tmp, &clocksource_list, list)\r\nif (tmp->rating >= cs->rating)\r\nentry = &tmp->list;\r\nlist_add(&cs->list, entry);\r\n}\r\nvoid __clocksource_updatefreq_scale(struct clocksource *cs, u32 scale, u32 freq)\r\n{\r\nu64 sec;\r\nsec = (cs->mask - (cs->mask >> 3));\r\ndo_div(sec, freq);\r\ndo_div(sec, scale);\r\nif (!sec)\r\nsec = 1;\r\nelse if (sec > 600 && cs->mask > UINT_MAX)\r\nsec = 600;\r\nclocks_calc_mult_shift(&cs->mult, &cs->shift, freq,\r\nNSEC_PER_SEC / scale, sec * scale);\r\ncs->maxadj = clocksource_max_adjustment(cs);\r\nwhile ((cs->mult + cs->maxadj < cs->mult)\r\n|| (cs->mult - cs->maxadj > cs->mult)) {\r\ncs->mult >>= 1;\r\ncs->shift--;\r\ncs->maxadj = clocksource_max_adjustment(cs);\r\n}\r\ncs->max_idle_ns = clocksource_max_deferment(cs);\r\n}\r\nint __clocksource_register_scale(struct clocksource *cs, u32 scale, u32 freq)\r\n{\r\n__clocksource_updatefreq_scale(cs, scale, freq);\r\nmutex_lock(&clocksource_mutex);\r\nclocksource_enqueue(cs);\r\nclocksource_enqueue_watchdog(cs);\r\nclocksource_select();\r\nmutex_unlock(&clocksource_mutex);\r\nreturn 0;\r\n}\r\nint clocksource_register(struct clocksource *cs)\r\n{\r\ncs->maxadj = clocksource_max_adjustment(cs);\r\nWARN_ONCE(cs->mult + cs->maxadj < cs->mult,\r\n"Clocksource %s might overflow on 11%% adjustment\n",\r\ncs->name);\r\ncs->max_idle_ns = clocksource_max_deferment(cs);\r\nmutex_lock(&clocksource_mutex);\r\nclocksource_enqueue(cs);\r\nclocksource_enqueue_watchdog(cs);\r\nclocksource_select();\r\nmutex_unlock(&clocksource_mutex);\r\nreturn 0;\r\n}\r\nstatic void __clocksource_change_rating(struct clocksource *cs, int rating)\r\n{\r\nlist_del(&cs->list);\r\ncs->rating = rating;\r\nclocksource_enqueue(cs);\r\nclocksource_select();\r\n}\r\nvoid clocksource_change_rating(struct clocksource *cs, int rating)\r\n{\r\nmutex_lock(&clocksource_mutex);\r\n__clocksource_change_rating(cs, rating);\r\nmutex_unlock(&clocksource_mutex);\r\n}\r\nvoid clocksource_unregister(struct clocksource *cs)\r\n{\r\nmutex_lock(&clocksource_mutex);\r\nclocksource_dequeue_watchdog(cs);\r\nlist_del(&cs->list);\r\nclocksource_select();\r\nmutex_unlock(&clocksource_mutex);\r\n}\r\nstatic ssize_t\r\nsysfs_show_current_clocksources(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nssize_t count = 0;\r\nmutex_lock(&clocksource_mutex);\r\ncount = snprintf(buf, PAGE_SIZE, "%s\n", curr_clocksource->name);\r\nmutex_unlock(&clocksource_mutex);\r\nreturn count;\r\n}\r\nstatic ssize_t sysfs_override_clocksource(struct device *dev,\r\nstruct device_attribute *attr,\r\nconst char *buf, size_t count)\r\n{\r\nsize_t ret = count;\r\nif (count >= sizeof(override_name))\r\nreturn -EINVAL;\r\nif (buf[count-1] == '\n')\r\ncount--;\r\nmutex_lock(&clocksource_mutex);\r\nif (count > 0)\r\nmemcpy(override_name, buf, count);\r\noverride_name[count] = 0;\r\nclocksource_select();\r\nmutex_unlock(&clocksource_mutex);\r\nreturn ret;\r\n}\r\nstatic ssize_t\r\nsysfs_show_available_clocksources(struct device *dev,\r\nstruct device_attribute *attr,\r\nchar *buf)\r\n{\r\nstruct clocksource *src;\r\nssize_t count = 0;\r\nmutex_lock(&clocksource_mutex);\r\nlist_for_each_entry(src, &clocksource_list, list) {\r\nif (!tick_oneshot_mode_active() ||\r\n(src->flags & CLOCK_SOURCE_VALID_FOR_HRES))\r\ncount += snprintf(buf + count,\r\nmax((ssize_t)PAGE_SIZE - count, (ssize_t)0),\r\n"%s ", src->name);\r\n}\r\nmutex_unlock(&clocksource_mutex);\r\ncount += snprintf(buf + count,\r\nmax((ssize_t)PAGE_SIZE - count, (ssize_t)0), "\n");\r\nreturn count;\r\n}\r\nstatic int __init init_clocksource_sysfs(void)\r\n{\r\nint error = subsys_system_register(&clocksource_subsys, NULL);\r\nif (!error)\r\nerror = device_register(&device_clocksource);\r\nif (!error)\r\nerror = device_create_file(\r\n&device_clocksource,\r\n&dev_attr_current_clocksource);\r\nif (!error)\r\nerror = device_create_file(\r\n&device_clocksource,\r\n&dev_attr_available_clocksource);\r\nreturn error;\r\n}\r\nstatic int __init boot_override_clocksource(char* str)\r\n{\r\nmutex_lock(&clocksource_mutex);\r\nif (str)\r\nstrlcpy(override_name, str, sizeof(override_name));\r\nmutex_unlock(&clocksource_mutex);\r\nreturn 1;\r\n}\r\nstatic int __init boot_override_clock(char* str)\r\n{\r\nif (!strcmp(str, "pmtmr")) {\r\nprintk("Warning: clock=pmtmr is deprecated. "\r\n"Use clocksource=acpi_pm.\n");\r\nreturn boot_override_clocksource("acpi_pm");\r\n}\r\nprintk("Warning! clock= boot option is deprecated. "\r\n"Use clocksource=xyz\n");\r\nreturn boot_override_clocksource(str);\r\n}
