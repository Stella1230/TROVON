static void get_map_page(struct ipath_qp_table *qpt, struct qpn_map *map)\r\n{\r\nunsigned long page = get_zeroed_page(GFP_KERNEL);\r\nunsigned long flags;\r\nspin_lock_irqsave(&qpt->lock, flags);\r\nif (map->page)\r\nfree_page(page);\r\nelse\r\nmap->page = (void *)page;\r\nspin_unlock_irqrestore(&qpt->lock, flags);\r\n}\r\nstatic int alloc_qpn(struct ipath_qp_table *qpt, enum ib_qp_type type)\r\n{\r\nu32 i, offset, max_scan, qpn;\r\nstruct qpn_map *map;\r\nu32 ret = -1;\r\nif (type == IB_QPT_SMI)\r\nret = 0;\r\nelse if (type == IB_QPT_GSI)\r\nret = 1;\r\nif (ret != -1) {\r\nmap = &qpt->map[0];\r\nif (unlikely(!map->page)) {\r\nget_map_page(qpt, map);\r\nif (unlikely(!map->page)) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\n}\r\nif (!test_and_set_bit(ret, map->page))\r\natomic_dec(&map->n_free);\r\nelse\r\nret = -EBUSY;\r\ngoto bail;\r\n}\r\nqpn = qpt->last + 1;\r\nif (qpn >= QPN_MAX)\r\nqpn = 2;\r\noffset = qpn & BITS_PER_PAGE_MASK;\r\nmap = &qpt->map[qpn / BITS_PER_PAGE];\r\nmax_scan = qpt->nmaps - !offset;\r\nfor (i = 0;;) {\r\nif (unlikely(!map->page)) {\r\nget_map_page(qpt, map);\r\nif (unlikely(!map->page))\r\nbreak;\r\n}\r\nif (likely(atomic_read(&map->n_free))) {\r\ndo {\r\nif (!test_and_set_bit(offset, map->page)) {\r\natomic_dec(&map->n_free);\r\nqpt->last = qpn;\r\nret = qpn;\r\ngoto bail;\r\n}\r\noffset = find_next_offset(map, offset);\r\nqpn = mk_qpn(qpt, map, offset);\r\n} while (offset < BITS_PER_PAGE && qpn < QPN_MAX);\r\n}\r\nif (++i > max_scan) {\r\nif (qpt->nmaps == QPNMAP_ENTRIES)\r\nbreak;\r\nmap = &qpt->map[qpt->nmaps++];\r\noffset = 0;\r\n} else if (map < &qpt->map[qpt->nmaps]) {\r\n++map;\r\noffset = 0;\r\n} else {\r\nmap = &qpt->map[0];\r\noffset = 2;\r\n}\r\nqpn = mk_qpn(qpt, map, offset);\r\n}\r\nret = -ENOMEM;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void free_qpn(struct ipath_qp_table *qpt, u32 qpn)\r\n{\r\nstruct qpn_map *map;\r\nmap = qpt->map + qpn / BITS_PER_PAGE;\r\nif (map->page)\r\nclear_bit(qpn & BITS_PER_PAGE_MASK, map->page);\r\natomic_inc(&map->n_free);\r\n}\r\nstatic int ipath_alloc_qpn(struct ipath_qp_table *qpt, struct ipath_qp *qp,\r\nenum ib_qp_type type)\r\n{\r\nunsigned long flags;\r\nint ret;\r\nret = alloc_qpn(qpt, type);\r\nif (ret < 0)\r\ngoto bail;\r\nqp->ibqp.qp_num = ret;\r\nspin_lock_irqsave(&qpt->lock, flags);\r\nret %= qpt->max;\r\nqp->next = qpt->table[ret];\r\nqpt->table[ret] = qp;\r\natomic_inc(&qp->refcount);\r\nspin_unlock_irqrestore(&qpt->lock, flags);\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void ipath_free_qp(struct ipath_qp_table *qpt, struct ipath_qp *qp)\r\n{\r\nstruct ipath_qp *q, **qpp;\r\nunsigned long flags;\r\nspin_lock_irqsave(&qpt->lock, flags);\r\nqpp = &qpt->table[qp->ibqp.qp_num % qpt->max];\r\nfor (; (q = *qpp) != NULL; qpp = &q->next) {\r\nif (q == qp) {\r\n*qpp = qp->next;\r\nqp->next = NULL;\r\natomic_dec(&qp->refcount);\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&qpt->lock, flags);\r\n}\r\nunsigned ipath_free_all_qps(struct ipath_qp_table *qpt)\r\n{\r\nunsigned long flags;\r\nstruct ipath_qp *qp;\r\nu32 n, qp_inuse = 0;\r\nspin_lock_irqsave(&qpt->lock, flags);\r\nfor (n = 0; n < qpt->max; n++) {\r\nqp = qpt->table[n];\r\nqpt->table[n] = NULL;\r\nfor (; qp; qp = qp->next)\r\nqp_inuse++;\r\n}\r\nspin_unlock_irqrestore(&qpt->lock, flags);\r\nfor (n = 0; n < ARRAY_SIZE(qpt->map); n++)\r\nif (qpt->map[n].page)\r\nfree_page((unsigned long) qpt->map[n].page);\r\nreturn qp_inuse;\r\n}\r\nstruct ipath_qp *ipath_lookup_qpn(struct ipath_qp_table *qpt, u32 qpn)\r\n{\r\nunsigned long flags;\r\nstruct ipath_qp *qp;\r\nspin_lock_irqsave(&qpt->lock, flags);\r\nfor (qp = qpt->table[qpn % qpt->max]; qp; qp = qp->next) {\r\nif (qp->ibqp.qp_num == qpn) {\r\natomic_inc(&qp->refcount);\r\nbreak;\r\n}\r\n}\r\nspin_unlock_irqrestore(&qpt->lock, flags);\r\nreturn qp;\r\n}\r\nstatic void ipath_reset_qp(struct ipath_qp *qp, enum ib_qp_type type)\r\n{\r\nqp->remote_qpn = 0;\r\nqp->qkey = 0;\r\nqp->qp_access_flags = 0;\r\natomic_set(&qp->s_dma_busy, 0);\r\nqp->s_flags &= IPATH_S_SIGNAL_REQ_WR;\r\nqp->s_hdrwords = 0;\r\nqp->s_wqe = NULL;\r\nqp->s_pkt_delay = 0;\r\nqp->s_draining = 0;\r\nqp->s_psn = 0;\r\nqp->r_psn = 0;\r\nqp->r_msn = 0;\r\nif (type == IB_QPT_RC) {\r\nqp->s_state = IB_OPCODE_RC_SEND_LAST;\r\nqp->r_state = IB_OPCODE_RC_SEND_LAST;\r\n} else {\r\nqp->s_state = IB_OPCODE_UC_SEND_LAST;\r\nqp->r_state = IB_OPCODE_UC_SEND_LAST;\r\n}\r\nqp->s_ack_state = IB_OPCODE_RC_ACKNOWLEDGE;\r\nqp->r_nak_state = 0;\r\nqp->r_aflags = 0;\r\nqp->r_flags = 0;\r\nqp->s_rnr_timeout = 0;\r\nqp->s_head = 0;\r\nqp->s_tail = 0;\r\nqp->s_cur = 0;\r\nqp->s_last = 0;\r\nqp->s_ssn = 1;\r\nqp->s_lsn = 0;\r\nmemset(qp->s_ack_queue, 0, sizeof(qp->s_ack_queue));\r\nqp->r_head_ack_queue = 0;\r\nqp->s_tail_ack_queue = 0;\r\nqp->s_num_rd_atomic = 0;\r\nif (qp->r_rq.wq) {\r\nqp->r_rq.wq->head = 0;\r\nqp->r_rq.wq->tail = 0;\r\n}\r\n}\r\nint ipath_error_qp(struct ipath_qp *qp, enum ib_wc_status err)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(qp->ibqp.device);\r\nstruct ib_wc wc;\r\nint ret = 0;\r\nif (qp->state == IB_QPS_ERR)\r\ngoto bail;\r\nqp->state = IB_QPS_ERR;\r\nspin_lock(&dev->pending_lock);\r\nif (!list_empty(&qp->timerwait))\r\nlist_del_init(&qp->timerwait);\r\nif (!list_empty(&qp->piowait))\r\nlist_del_init(&qp->piowait);\r\nspin_unlock(&dev->pending_lock);\r\nif (qp->s_last != qp->s_head)\r\nipath_schedule_send(qp);\r\nmemset(&wc, 0, sizeof(wc));\r\nwc.qp = &qp->ibqp;\r\nwc.opcode = IB_WC_RECV;\r\nif (test_and_clear_bit(IPATH_R_WRID_VALID, &qp->r_aflags)) {\r\nwc.wr_id = qp->r_wr_id;\r\nwc.status = err;\r\nipath_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);\r\n}\r\nwc.status = IB_WC_WR_FLUSH_ERR;\r\nif (qp->r_rq.wq) {\r\nstruct ipath_rwq *wq;\r\nu32 head;\r\nu32 tail;\r\nspin_lock(&qp->r_rq.lock);\r\nwq = qp->r_rq.wq;\r\nhead = wq->head;\r\nif (head >= qp->r_rq.size)\r\nhead = 0;\r\ntail = wq->tail;\r\nif (tail >= qp->r_rq.size)\r\ntail = 0;\r\nwhile (tail != head) {\r\nwc.wr_id = get_rwqe_ptr(&qp->r_rq, tail)->wr_id;\r\nif (++tail >= qp->r_rq.size)\r\ntail = 0;\r\nipath_cq_enter(to_icq(qp->ibqp.recv_cq), &wc, 1);\r\n}\r\nwq->tail = tail;\r\nspin_unlock(&qp->r_rq.lock);\r\n} else if (qp->ibqp.event_handler)\r\nret = 1;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_modify_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\r\nint attr_mask, struct ib_udata *udata)\r\n{\r\nstruct ipath_ibdev *dev = to_idev(ibqp->device);\r\nstruct ipath_qp *qp = to_iqp(ibqp);\r\nenum ib_qp_state cur_state, new_state;\r\nint lastwqe = 0;\r\nint ret;\r\nspin_lock_irq(&qp->s_lock);\r\ncur_state = attr_mask & IB_QP_CUR_STATE ?\r\nattr->cur_qp_state : qp->state;\r\nnew_state = attr_mask & IB_QP_STATE ? attr->qp_state : cur_state;\r\nif (!ib_modify_qp_is_ok(cur_state, new_state, ibqp->qp_type,\r\nattr_mask))\r\ngoto inval;\r\nif (attr_mask & IB_QP_AV) {\r\nif (attr->ah_attr.dlid == 0 ||\r\nattr->ah_attr.dlid >= IPATH_MULTICAST_LID_BASE)\r\ngoto inval;\r\nif ((attr->ah_attr.ah_flags & IB_AH_GRH) &&\r\n(attr->ah_attr.grh.sgid_index > 1))\r\ngoto inval;\r\n}\r\nif (attr_mask & IB_QP_PKEY_INDEX)\r\nif (attr->pkey_index >= ipath_get_npkeys(dev->dd))\r\ngoto inval;\r\nif (attr_mask & IB_QP_MIN_RNR_TIMER)\r\nif (attr->min_rnr_timer > 31)\r\ngoto inval;\r\nif (attr_mask & IB_QP_PORT)\r\nif (attr->port_num == 0 ||\r\nattr->port_num > ibqp->device->phys_port_cnt)\r\ngoto inval;\r\nif ((attr_mask & IB_QP_PATH_MTU) &&\r\n(ib_mtu_enum_to_int(attr->path_mtu) == -1 ||\r\n(attr->path_mtu > IB_MTU_2048 && !ipath_mtu4096)))\r\ngoto inval;\r\nif (attr_mask & IB_QP_PATH_MIG_STATE)\r\nif (attr->path_mig_state != IB_MIG_MIGRATED &&\r\nattr->path_mig_state != IB_MIG_REARM)\r\ngoto inval;\r\nif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\r\nif (attr->max_dest_rd_atomic > IPATH_MAX_RDMA_ATOMIC)\r\ngoto inval;\r\nswitch (new_state) {\r\ncase IB_QPS_RESET:\r\nif (qp->state != IB_QPS_RESET) {\r\nqp->state = IB_QPS_RESET;\r\nspin_lock(&dev->pending_lock);\r\nif (!list_empty(&qp->timerwait))\r\nlist_del_init(&qp->timerwait);\r\nif (!list_empty(&qp->piowait))\r\nlist_del_init(&qp->piowait);\r\nspin_unlock(&dev->pending_lock);\r\nqp->s_flags &= ~IPATH_S_ANY_WAIT;\r\nspin_unlock_irq(&qp->s_lock);\r\ntasklet_kill(&qp->s_task);\r\nwait_event(qp->wait_dma, !atomic_read(&qp->s_dma_busy));\r\nspin_lock_irq(&qp->s_lock);\r\n}\r\nipath_reset_qp(qp, ibqp->qp_type);\r\nbreak;\r\ncase IB_QPS_SQD:\r\nqp->s_draining = qp->s_last != qp->s_cur;\r\nqp->state = new_state;\r\nbreak;\r\ncase IB_QPS_SQE:\r\nif (qp->ibqp.qp_type == IB_QPT_RC)\r\ngoto inval;\r\nqp->state = new_state;\r\nbreak;\r\ncase IB_QPS_ERR:\r\nlastwqe = ipath_error_qp(qp, IB_WC_WR_FLUSH_ERR);\r\nbreak;\r\ndefault:\r\nqp->state = new_state;\r\nbreak;\r\n}\r\nif (attr_mask & IB_QP_PKEY_INDEX)\r\nqp->s_pkey_index = attr->pkey_index;\r\nif (attr_mask & IB_QP_DEST_QPN)\r\nqp->remote_qpn = attr->dest_qp_num;\r\nif (attr_mask & IB_QP_SQ_PSN) {\r\nqp->s_psn = qp->s_next_psn = attr->sq_psn;\r\nqp->s_last_psn = qp->s_next_psn - 1;\r\n}\r\nif (attr_mask & IB_QP_RQ_PSN)\r\nqp->r_psn = attr->rq_psn;\r\nif (attr_mask & IB_QP_ACCESS_FLAGS)\r\nqp->qp_access_flags = attr->qp_access_flags;\r\nif (attr_mask & IB_QP_AV) {\r\nqp->remote_ah_attr = attr->ah_attr;\r\nqp->s_dmult = ipath_ib_rate_to_mult(attr->ah_attr.static_rate);\r\n}\r\nif (attr_mask & IB_QP_PATH_MTU)\r\nqp->path_mtu = attr->path_mtu;\r\nif (attr_mask & IB_QP_RETRY_CNT)\r\nqp->s_retry = qp->s_retry_cnt = attr->retry_cnt;\r\nif (attr_mask & IB_QP_RNR_RETRY) {\r\nqp->s_rnr_retry = attr->rnr_retry;\r\nif (qp->s_rnr_retry > 7)\r\nqp->s_rnr_retry = 7;\r\nqp->s_rnr_retry_cnt = qp->s_rnr_retry;\r\n}\r\nif (attr_mask & IB_QP_MIN_RNR_TIMER)\r\nqp->r_min_rnr_timer = attr->min_rnr_timer;\r\nif (attr_mask & IB_QP_TIMEOUT)\r\nqp->timeout = attr->timeout;\r\nif (attr_mask & IB_QP_QKEY)\r\nqp->qkey = attr->qkey;\r\nif (attr_mask & IB_QP_MAX_DEST_RD_ATOMIC)\r\nqp->r_max_rd_atomic = attr->max_dest_rd_atomic;\r\nif (attr_mask & IB_QP_MAX_QP_RD_ATOMIC)\r\nqp->s_max_rd_atomic = attr->max_rd_atomic;\r\nspin_unlock_irq(&qp->s_lock);\r\nif (lastwqe) {\r\nstruct ib_event ev;\r\nev.device = qp->ibqp.device;\r\nev.element.qp = &qp->ibqp;\r\nev.event = IB_EVENT_QP_LAST_WQE_REACHED;\r\nqp->ibqp.event_handler(&ev, qp->ibqp.qp_context);\r\n}\r\nret = 0;\r\ngoto bail;\r\ninval:\r\nspin_unlock_irq(&qp->s_lock);\r\nret = -EINVAL;\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_query_qp(struct ib_qp *ibqp, struct ib_qp_attr *attr,\r\nint attr_mask, struct ib_qp_init_attr *init_attr)\r\n{\r\nstruct ipath_qp *qp = to_iqp(ibqp);\r\nattr->qp_state = qp->state;\r\nattr->cur_qp_state = attr->qp_state;\r\nattr->path_mtu = qp->path_mtu;\r\nattr->path_mig_state = 0;\r\nattr->qkey = qp->qkey;\r\nattr->rq_psn = qp->r_psn;\r\nattr->sq_psn = qp->s_next_psn;\r\nattr->dest_qp_num = qp->remote_qpn;\r\nattr->qp_access_flags = qp->qp_access_flags;\r\nattr->cap.max_send_wr = qp->s_size - 1;\r\nattr->cap.max_recv_wr = qp->ibqp.srq ? 0 : qp->r_rq.size - 1;\r\nattr->cap.max_send_sge = qp->s_max_sge;\r\nattr->cap.max_recv_sge = qp->r_rq.max_sge;\r\nattr->cap.max_inline_data = 0;\r\nattr->ah_attr = qp->remote_ah_attr;\r\nmemset(&attr->alt_ah_attr, 0, sizeof(attr->alt_ah_attr));\r\nattr->pkey_index = qp->s_pkey_index;\r\nattr->alt_pkey_index = 0;\r\nattr->en_sqd_async_notify = 0;\r\nattr->sq_draining = qp->s_draining;\r\nattr->max_rd_atomic = qp->s_max_rd_atomic;\r\nattr->max_dest_rd_atomic = qp->r_max_rd_atomic;\r\nattr->min_rnr_timer = qp->r_min_rnr_timer;\r\nattr->port_num = 1;\r\nattr->timeout = qp->timeout;\r\nattr->retry_cnt = qp->s_retry_cnt;\r\nattr->rnr_retry = qp->s_rnr_retry_cnt;\r\nattr->alt_port_num = 0;\r\nattr->alt_timeout = 0;\r\ninit_attr->event_handler = qp->ibqp.event_handler;\r\ninit_attr->qp_context = qp->ibqp.qp_context;\r\ninit_attr->send_cq = qp->ibqp.send_cq;\r\ninit_attr->recv_cq = qp->ibqp.recv_cq;\r\ninit_attr->srq = qp->ibqp.srq;\r\ninit_attr->cap = attr->cap;\r\nif (qp->s_flags & IPATH_S_SIGNAL_REQ_WR)\r\ninit_attr->sq_sig_type = IB_SIGNAL_REQ_WR;\r\nelse\r\ninit_attr->sq_sig_type = IB_SIGNAL_ALL_WR;\r\ninit_attr->qp_type = qp->ibqp.qp_type;\r\ninit_attr->port_num = 1;\r\nreturn 0;\r\n}\r\n__be32 ipath_compute_aeth(struct ipath_qp *qp)\r\n{\r\nu32 aeth = qp->r_msn & IPATH_MSN_MASK;\r\nif (qp->ibqp.srq) {\r\naeth |= IPATH_AETH_CREDIT_INVAL << IPATH_AETH_CREDIT_SHIFT;\r\n} else {\r\nu32 min, max, x;\r\nu32 credits;\r\nstruct ipath_rwq *wq = qp->r_rq.wq;\r\nu32 head;\r\nu32 tail;\r\nhead = wq->head;\r\nif (head >= qp->r_rq.size)\r\nhead = 0;\r\ntail = wq->tail;\r\nif (tail >= qp->r_rq.size)\r\ntail = 0;\r\ncredits = head - tail;\r\nif ((int)credits < 0)\r\ncredits += qp->r_rq.size;\r\nmin = 0;\r\nmax = 31;\r\nfor (;;) {\r\nx = (min + max) / 2;\r\nif (credit_table[x] == credits)\r\nbreak;\r\nif (credit_table[x] > credits)\r\nmax = x;\r\nelse if (min == x)\r\nbreak;\r\nelse\r\nmin = x;\r\n}\r\naeth |= x << IPATH_AETH_CREDIT_SHIFT;\r\n}\r\nreturn cpu_to_be32(aeth);\r\n}\r\nstruct ib_qp *ipath_create_qp(struct ib_pd *ibpd,\r\nstruct ib_qp_init_attr *init_attr,\r\nstruct ib_udata *udata)\r\n{\r\nstruct ipath_qp *qp;\r\nint err;\r\nstruct ipath_swqe *swq = NULL;\r\nstruct ipath_ibdev *dev;\r\nsize_t sz;\r\nsize_t sg_list_sz;\r\nstruct ib_qp *ret;\r\nif (init_attr->create_flags) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nif (init_attr->cap.max_send_sge > ib_ipath_max_sges ||\r\ninit_attr->cap.max_send_wr > ib_ipath_max_qp_wrs) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nif (!init_attr->srq) {\r\nif (init_attr->cap.max_recv_sge > ib_ipath_max_sges ||\r\ninit_attr->cap.max_recv_wr > ib_ipath_max_qp_wrs) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\nif (init_attr->cap.max_send_sge +\r\ninit_attr->cap.max_send_wr +\r\ninit_attr->cap.max_recv_sge +\r\ninit_attr->cap.max_recv_wr == 0) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto bail;\r\n}\r\n}\r\nswitch (init_attr->qp_type) {\r\ncase IB_QPT_UC:\r\ncase IB_QPT_RC:\r\ncase IB_QPT_UD:\r\ncase IB_QPT_SMI:\r\ncase IB_QPT_GSI:\r\nsz = sizeof(struct ipath_sge) *\r\ninit_attr->cap.max_send_sge +\r\nsizeof(struct ipath_swqe);\r\nswq = vmalloc((init_attr->cap.max_send_wr + 1) * sz);\r\nif (swq == NULL) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail;\r\n}\r\nsz = sizeof(*qp);\r\nsg_list_sz = 0;\r\nif (init_attr->srq) {\r\nstruct ipath_srq *srq = to_isrq(init_attr->srq);\r\nif (srq->rq.max_sge > 1)\r\nsg_list_sz = sizeof(*qp->r_sg_list) *\r\n(srq->rq.max_sge - 1);\r\n} else if (init_attr->cap.max_recv_sge > 1)\r\nsg_list_sz = sizeof(*qp->r_sg_list) *\r\n(init_attr->cap.max_recv_sge - 1);\r\nqp = kmalloc(sz + sg_list_sz, GFP_KERNEL);\r\nif (!qp) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_swq;\r\n}\r\nif (sg_list_sz && (init_attr->qp_type == IB_QPT_UD ||\r\ninit_attr->qp_type == IB_QPT_SMI ||\r\ninit_attr->qp_type == IB_QPT_GSI)) {\r\nqp->r_ud_sg_list = kmalloc(sg_list_sz, GFP_KERNEL);\r\nif (!qp->r_ud_sg_list) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_qp;\r\n}\r\n} else\r\nqp->r_ud_sg_list = NULL;\r\nif (init_attr->srq) {\r\nsz = 0;\r\nqp->r_rq.size = 0;\r\nqp->r_rq.max_sge = 0;\r\nqp->r_rq.wq = NULL;\r\ninit_attr->cap.max_recv_wr = 0;\r\ninit_attr->cap.max_recv_sge = 0;\r\n} else {\r\nqp->r_rq.size = init_attr->cap.max_recv_wr + 1;\r\nqp->r_rq.max_sge = init_attr->cap.max_recv_sge;\r\nsz = (sizeof(struct ib_sge) * qp->r_rq.max_sge) +\r\nsizeof(struct ipath_rwqe);\r\nqp->r_rq.wq = vmalloc_user(sizeof(struct ipath_rwq) +\r\nqp->r_rq.size * sz);\r\nif (!qp->r_rq.wq) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_sg_list;\r\n}\r\n}\r\nspin_lock_init(&qp->s_lock);\r\nspin_lock_init(&qp->r_rq.lock);\r\natomic_set(&qp->refcount, 0);\r\ninit_waitqueue_head(&qp->wait);\r\ninit_waitqueue_head(&qp->wait_dma);\r\ntasklet_init(&qp->s_task, ipath_do_send, (unsigned long)qp);\r\nINIT_LIST_HEAD(&qp->piowait);\r\nINIT_LIST_HEAD(&qp->timerwait);\r\nqp->state = IB_QPS_RESET;\r\nqp->s_wq = swq;\r\nqp->s_size = init_attr->cap.max_send_wr + 1;\r\nqp->s_max_sge = init_attr->cap.max_send_sge;\r\nif (init_attr->sq_sig_type == IB_SIGNAL_REQ_WR)\r\nqp->s_flags = IPATH_S_SIGNAL_REQ_WR;\r\nelse\r\nqp->s_flags = 0;\r\ndev = to_idev(ibpd->device);\r\nerr = ipath_alloc_qpn(&dev->qp_table, qp,\r\ninit_attr->qp_type);\r\nif (err) {\r\nret = ERR_PTR(err);\r\nvfree(qp->r_rq.wq);\r\ngoto bail_sg_list;\r\n}\r\nqp->ip = NULL;\r\nqp->s_tx = NULL;\r\nipath_reset_qp(qp, init_attr->qp_type);\r\nbreak;\r\ndefault:\r\nret = ERR_PTR(-ENOSYS);\r\ngoto bail;\r\n}\r\ninit_attr->cap.max_inline_data = 0;\r\nif (udata && udata->outlen >= sizeof(__u64)) {\r\nif (!qp->r_rq.wq) {\r\n__u64 offset = 0;\r\nerr = ib_copy_to_udata(udata, &offset,\r\nsizeof(offset));\r\nif (err) {\r\nret = ERR_PTR(err);\r\ngoto bail_ip;\r\n}\r\n} else {\r\nu32 s = sizeof(struct ipath_rwq) +\r\nqp->r_rq.size * sz;\r\nqp->ip =\r\nipath_create_mmap_info(dev, s,\r\nibpd->uobject->context,\r\nqp->r_rq.wq);\r\nif (!qp->ip) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_ip;\r\n}\r\nerr = ib_copy_to_udata(udata, &(qp->ip->offset),\r\nsizeof(qp->ip->offset));\r\nif (err) {\r\nret = ERR_PTR(err);\r\ngoto bail_ip;\r\n}\r\n}\r\n}\r\nspin_lock(&dev->n_qps_lock);\r\nif (dev->n_qps_allocated == ib_ipath_max_qps) {\r\nspin_unlock(&dev->n_qps_lock);\r\nret = ERR_PTR(-ENOMEM);\r\ngoto bail_ip;\r\n}\r\ndev->n_qps_allocated++;\r\nspin_unlock(&dev->n_qps_lock);\r\nif (qp->ip) {\r\nspin_lock_irq(&dev->pending_lock);\r\nlist_add(&qp->ip->pending_mmaps, &dev->pending_mmaps);\r\nspin_unlock_irq(&dev->pending_lock);\r\n}\r\nret = &qp->ibqp;\r\ngoto bail;\r\nbail_ip:\r\nif (qp->ip)\r\nkref_put(&qp->ip->ref, ipath_release_mmap_info);\r\nelse\r\nvfree(qp->r_rq.wq);\r\nipath_free_qp(&dev->qp_table, qp);\r\nfree_qpn(&dev->qp_table, qp->ibqp.qp_num);\r\nbail_sg_list:\r\nkfree(qp->r_ud_sg_list);\r\nbail_qp:\r\nkfree(qp);\r\nbail_swq:\r\nvfree(swq);\r\nbail:\r\nreturn ret;\r\n}\r\nint ipath_destroy_qp(struct ib_qp *ibqp)\r\n{\r\nstruct ipath_qp *qp = to_iqp(ibqp);\r\nstruct ipath_ibdev *dev = to_idev(ibqp->device);\r\nspin_lock_irq(&qp->s_lock);\r\nif (qp->state != IB_QPS_RESET) {\r\nqp->state = IB_QPS_RESET;\r\nspin_lock(&dev->pending_lock);\r\nif (!list_empty(&qp->timerwait))\r\nlist_del_init(&qp->timerwait);\r\nif (!list_empty(&qp->piowait))\r\nlist_del_init(&qp->piowait);\r\nspin_unlock(&dev->pending_lock);\r\nqp->s_flags &= ~IPATH_S_ANY_WAIT;\r\nspin_unlock_irq(&qp->s_lock);\r\ntasklet_kill(&qp->s_task);\r\nwait_event(qp->wait_dma, !atomic_read(&qp->s_dma_busy));\r\n} else\r\nspin_unlock_irq(&qp->s_lock);\r\nipath_free_qp(&dev->qp_table, qp);\r\nif (qp->s_tx) {\r\natomic_dec(&qp->refcount);\r\nif (qp->s_tx->txreq.flags & IPATH_SDMA_TXREQ_F_FREEBUF)\r\nkfree(qp->s_tx->txreq.map_addr);\r\nspin_lock_irq(&dev->pending_lock);\r\nlist_add(&qp->s_tx->txreq.list, &dev->txreq_free);\r\nspin_unlock_irq(&dev->pending_lock);\r\nqp->s_tx = NULL;\r\n}\r\nwait_event(qp->wait, !atomic_read(&qp->refcount));\r\nfree_qpn(&dev->qp_table, qp->ibqp.qp_num);\r\nspin_lock(&dev->n_qps_lock);\r\ndev->n_qps_allocated--;\r\nspin_unlock(&dev->n_qps_lock);\r\nif (qp->ip)\r\nkref_put(&qp->ip->ref, ipath_release_mmap_info);\r\nelse\r\nvfree(qp->r_rq.wq);\r\nkfree(qp->r_ud_sg_list);\r\nvfree(qp->s_wq);\r\nkfree(qp);\r\nreturn 0;\r\n}\r\nint ipath_init_qp_table(struct ipath_ibdev *idev, int size)\r\n{\r\nint i;\r\nint ret;\r\nidev->qp_table.last = 1;\r\nidev->qp_table.max = size;\r\nidev->qp_table.nmaps = 1;\r\nidev->qp_table.table = kzalloc(size * sizeof(*idev->qp_table.table),\r\nGFP_KERNEL);\r\nif (idev->qp_table.table == NULL) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(idev->qp_table.map); i++) {\r\natomic_set(&idev->qp_table.map[i].n_free, BITS_PER_PAGE);\r\nidev->qp_table.map[i].page = NULL;\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nvoid ipath_get_credit(struct ipath_qp *qp, u32 aeth)\r\n{\r\nu32 credit = (aeth >> IPATH_AETH_CREDIT_SHIFT) & IPATH_AETH_CREDIT_MASK;\r\nif (credit == IPATH_AETH_CREDIT_INVAL)\r\nqp->s_lsn = (u32) -1;\r\nelse if (qp->s_lsn != (u32) -1) {\r\ncredit = (aeth + credit_table[credit]) & IPATH_MSN_MASK;\r\nif (ipath_cmp24(credit, qp->s_lsn) > 0)\r\nqp->s_lsn = credit;\r\n}\r\nif ((qp->s_flags & IPATH_S_WAIT_SSN_CREDIT) &&\r\nqp->s_cur != qp->s_head &&\r\n(qp->s_lsn == (u32) -1 ||\r\nipath_cmp24(get_swqe_ptr(qp, qp->s_cur)->ssn,\r\nqp->s_lsn + 1) <= 0))\r\nipath_schedule_send(qp);\r\n}
