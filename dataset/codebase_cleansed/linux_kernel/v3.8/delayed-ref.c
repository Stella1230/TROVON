static int comp_tree_refs(struct btrfs_delayed_tree_ref *ref2,\r\nstruct btrfs_delayed_tree_ref *ref1)\r\n{\r\nif (ref1->root < ref2->root)\r\nreturn -1;\r\nif (ref1->root > ref2->root)\r\nreturn 1;\r\nif (ref1->parent < ref2->parent)\r\nreturn -1;\r\nif (ref1->parent > ref2->parent)\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int comp_data_refs(struct btrfs_delayed_data_ref *ref2,\r\nstruct btrfs_delayed_data_ref *ref1)\r\n{\r\nif (ref1->node.type == BTRFS_EXTENT_DATA_REF_KEY) {\r\nif (ref1->root < ref2->root)\r\nreturn -1;\r\nif (ref1->root > ref2->root)\r\nreturn 1;\r\nif (ref1->objectid < ref2->objectid)\r\nreturn -1;\r\nif (ref1->objectid > ref2->objectid)\r\nreturn 1;\r\nif (ref1->offset < ref2->offset)\r\nreturn -1;\r\nif (ref1->offset > ref2->offset)\r\nreturn 1;\r\n} else {\r\nif (ref1->parent < ref2->parent)\r\nreturn -1;\r\nif (ref1->parent > ref2->parent)\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic int comp_entry(struct btrfs_delayed_ref_node *ref2,\r\nstruct btrfs_delayed_ref_node *ref1,\r\nbool compare_seq)\r\n{\r\nif (ref1->bytenr < ref2->bytenr)\r\nreturn -1;\r\nif (ref1->bytenr > ref2->bytenr)\r\nreturn 1;\r\nif (ref1->is_head && ref2->is_head)\r\nreturn 0;\r\nif (ref2->is_head)\r\nreturn -1;\r\nif (ref1->is_head)\r\nreturn 1;\r\nif (ref1->type < ref2->type)\r\nreturn -1;\r\nif (ref1->type > ref2->type)\r\nreturn 1;\r\nif (compare_seq) {\r\nif (ref1->seq < ref2->seq)\r\nreturn -1;\r\nif (ref1->seq > ref2->seq)\r\nreturn 1;\r\n}\r\nif (ref1->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nref1->type == BTRFS_SHARED_BLOCK_REF_KEY) {\r\nreturn comp_tree_refs(btrfs_delayed_node_to_tree_ref(ref2),\r\nbtrfs_delayed_node_to_tree_ref(ref1));\r\n} else if (ref1->type == BTRFS_EXTENT_DATA_REF_KEY ||\r\nref1->type == BTRFS_SHARED_DATA_REF_KEY) {\r\nreturn comp_data_refs(btrfs_delayed_node_to_data_ref(ref2),\r\nbtrfs_delayed_node_to_data_ref(ref1));\r\n}\r\nBUG();\r\nreturn 0;\r\n}\r\nstatic struct btrfs_delayed_ref_node *tree_insert(struct rb_root *root,\r\nstruct rb_node *node)\r\n{\r\nstruct rb_node **p = &root->rb_node;\r\nstruct rb_node *parent_node = NULL;\r\nstruct btrfs_delayed_ref_node *entry;\r\nstruct btrfs_delayed_ref_node *ins;\r\nint cmp;\r\nins = rb_entry(node, struct btrfs_delayed_ref_node, rb_node);\r\nwhile (*p) {\r\nparent_node = *p;\r\nentry = rb_entry(parent_node, struct btrfs_delayed_ref_node,\r\nrb_node);\r\ncmp = comp_entry(entry, ins, 1);\r\nif (cmp < 0)\r\np = &(*p)->rb_left;\r\nelse if (cmp > 0)\r\np = &(*p)->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nrb_link_node(node, parent_node, p);\r\nrb_insert_color(node, root);\r\nreturn NULL;\r\n}\r\nstatic struct btrfs_delayed_ref_node *find_ref_head(struct rb_root *root,\r\nu64 bytenr,\r\nstruct btrfs_delayed_ref_node **last,\r\nint return_bigger)\r\n{\r\nstruct rb_node *n;\r\nstruct btrfs_delayed_ref_node *entry;\r\nint cmp = 0;\r\nagain:\r\nn = root->rb_node;\r\nentry = NULL;\r\nwhile (n) {\r\nentry = rb_entry(n, struct btrfs_delayed_ref_node, rb_node);\r\nWARN_ON(!entry->in_tree);\r\nif (last)\r\n*last = entry;\r\nif (bytenr < entry->bytenr)\r\ncmp = -1;\r\nelse if (bytenr > entry->bytenr)\r\ncmp = 1;\r\nelse if (!btrfs_delayed_ref_is_head(entry))\r\ncmp = 1;\r\nelse\r\ncmp = 0;\r\nif (cmp < 0)\r\nn = n->rb_left;\r\nelse if (cmp > 0)\r\nn = n->rb_right;\r\nelse\r\nreturn entry;\r\n}\r\nif (entry && return_bigger) {\r\nif (cmp > 0) {\r\nn = rb_next(&entry->rb_node);\r\nif (!n)\r\nn = rb_first(root);\r\nentry = rb_entry(n, struct btrfs_delayed_ref_node,\r\nrb_node);\r\nbytenr = entry->bytenr;\r\nreturn_bigger = 0;\r\ngoto again;\r\n}\r\nreturn entry;\r\n}\r\nreturn NULL;\r\n}\r\nint btrfs_delayed_ref_lock(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_head *head)\r\n{\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nassert_spin_locked(&delayed_refs->lock);\r\nif (mutex_trylock(&head->mutex))\r\nreturn 0;\r\natomic_inc(&head->node.refs);\r\nspin_unlock(&delayed_refs->lock);\r\nmutex_lock(&head->mutex);\r\nspin_lock(&delayed_refs->lock);\r\nif (!head->node.in_tree) {\r\nmutex_unlock(&head->mutex);\r\nbtrfs_put_delayed_ref(&head->node);\r\nreturn -EAGAIN;\r\n}\r\nbtrfs_put_delayed_ref(&head->node);\r\nreturn 0;\r\n}\r\nstatic void inline drop_delayed_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_node *ref)\r\n{\r\nrb_erase(&ref->rb_node, &delayed_refs->root);\r\nref->in_tree = 0;\r\nbtrfs_put_delayed_ref(ref);\r\ndelayed_refs->num_entries--;\r\nif (trans->delayed_ref_updates)\r\ntrans->delayed_ref_updates--;\r\n}\r\nstatic int merge_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_node *ref, u64 seq)\r\n{\r\nstruct rb_node *node;\r\nint merged = 0;\r\nint mod = 0;\r\nint done = 0;\r\nnode = rb_prev(&ref->rb_node);\r\nwhile (node) {\r\nstruct btrfs_delayed_ref_node *next;\r\nnext = rb_entry(node, struct btrfs_delayed_ref_node, rb_node);\r\nnode = rb_prev(node);\r\nif (next->bytenr != ref->bytenr)\r\nbreak;\r\nif (seq && next->seq >= seq)\r\nbreak;\r\nif (comp_entry(ref, next, 0))\r\ncontinue;\r\nif (ref->action == next->action) {\r\nmod = next->ref_mod;\r\n} else {\r\nif (ref->ref_mod < next->ref_mod) {\r\nstruct btrfs_delayed_ref_node *tmp;\r\ntmp = ref;\r\nref = next;\r\nnext = tmp;\r\ndone = 1;\r\n}\r\nmod = -next->ref_mod;\r\n}\r\nmerged++;\r\ndrop_delayed_ref(trans, delayed_refs, next);\r\nref->ref_mod += mod;\r\nif (ref->ref_mod == 0) {\r\ndrop_delayed_ref(trans, delayed_refs, ref);\r\nbreak;\r\n} else {\r\nWARN_ON(ref->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nref->type == BTRFS_SHARED_BLOCK_REF_KEY);\r\n}\r\nif (done)\r\nbreak;\r\nnode = rb_prev(&ref->rb_node);\r\n}\r\nreturn merged;\r\n}\r\nvoid btrfs_merge_delayed_refs(struct btrfs_trans_handle *trans,\r\nstruct btrfs_fs_info *fs_info,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_head *head)\r\n{\r\nstruct rb_node *node;\r\nu64 seq = 0;\r\nspin_lock(&fs_info->tree_mod_seq_lock);\r\nif (!list_empty(&fs_info->tree_mod_seq_list)) {\r\nstruct seq_list *elem;\r\nelem = list_first_entry(&fs_info->tree_mod_seq_list,\r\nstruct seq_list, list);\r\nseq = elem->seq;\r\n}\r\nspin_unlock(&fs_info->tree_mod_seq_lock);\r\nnode = rb_prev(&head->node.rb_node);\r\nwhile (node) {\r\nstruct btrfs_delayed_ref_node *ref;\r\nref = rb_entry(node, struct btrfs_delayed_ref_node,\r\nrb_node);\r\nif (ref->bytenr != head->node.bytenr)\r\nbreak;\r\nif (seq && ref->seq >= seq)\r\nbreak;\r\nif (merge_ref(trans, delayed_refs, ref, seq))\r\nnode = rb_prev(&head->node.rb_node);\r\nelse\r\nnode = rb_prev(node);\r\n}\r\n}\r\nint btrfs_check_delayed_seq(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nu64 seq)\r\n{\r\nstruct seq_list *elem;\r\nint ret = 0;\r\nspin_lock(&fs_info->tree_mod_seq_lock);\r\nif (!list_empty(&fs_info->tree_mod_seq_list)) {\r\nelem = list_first_entry(&fs_info->tree_mod_seq_list,\r\nstruct seq_list, list);\r\nif (seq >= elem->seq) {\r\npr_debug("holding back delayed_ref %llu, lowest is "\r\n"%llu (%p)\n", seq, elem->seq, delayed_refs);\r\nret = 1;\r\n}\r\n}\r\nspin_unlock(&fs_info->tree_mod_seq_lock);\r\nreturn ret;\r\n}\r\nint btrfs_find_ref_cluster(struct btrfs_trans_handle *trans,\r\nstruct list_head *cluster, u64 start)\r\n{\r\nint count = 0;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nstruct rb_node *node;\r\nstruct btrfs_delayed_ref_node *ref;\r\nstruct btrfs_delayed_ref_head *head;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nif (start == 0) {\r\nnode = rb_first(&delayed_refs->root);\r\n} else {\r\nref = NULL;\r\nfind_ref_head(&delayed_refs->root, start + 1, &ref, 1);\r\nif (ref) {\r\nnode = &ref->rb_node;\r\n} else\r\nnode = rb_first(&delayed_refs->root);\r\n}\r\nagain:\r\nwhile (node && count < 32) {\r\nref = rb_entry(node, struct btrfs_delayed_ref_node, rb_node);\r\nif (btrfs_delayed_ref_is_head(ref)) {\r\nhead = btrfs_delayed_node_to_head(ref);\r\nif (list_empty(&head->cluster)) {\r\nlist_add_tail(&head->cluster, cluster);\r\ndelayed_refs->run_delayed_start =\r\nhead->node.bytenr;\r\ncount++;\r\nWARN_ON(delayed_refs->num_heads_ready == 0);\r\ndelayed_refs->num_heads_ready--;\r\n} else if (count) {\r\nbreak;\r\n}\r\n}\r\nnode = rb_next(node);\r\n}\r\nif (count) {\r\nreturn 0;\r\n} else if (start) {\r\nstart = 0;\r\nnode = rb_first(&delayed_refs->root);\r\ngoto again;\r\n}\r\nreturn 1;\r\n}\r\nstatic noinline void\r\nupdate_existing_ref(struct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_root *delayed_refs,\r\nstruct btrfs_delayed_ref_node *existing,\r\nstruct btrfs_delayed_ref_node *update)\r\n{\r\nif (update->action != existing->action) {\r\nexisting->ref_mod--;\r\nif (existing->ref_mod == 0)\r\ndrop_delayed_ref(trans, delayed_refs, existing);\r\nelse\r\nWARN_ON(existing->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nexisting->type == BTRFS_SHARED_BLOCK_REF_KEY);\r\n} else {\r\nWARN_ON(existing->type == BTRFS_TREE_BLOCK_REF_KEY ||\r\nexisting->type == BTRFS_SHARED_BLOCK_REF_KEY);\r\nexisting->ref_mod += update->ref_mod;\r\n}\r\n}\r\nstatic noinline void\r\nupdate_existing_head_ref(struct btrfs_delayed_ref_node *existing,\r\nstruct btrfs_delayed_ref_node *update)\r\n{\r\nstruct btrfs_delayed_ref_head *existing_ref;\r\nstruct btrfs_delayed_ref_head *ref;\r\nexisting_ref = btrfs_delayed_node_to_head(existing);\r\nref = btrfs_delayed_node_to_head(update);\r\nBUG_ON(existing_ref->is_data != ref->is_data);\r\nif (ref->must_insert_reserved) {\r\nexisting_ref->must_insert_reserved = ref->must_insert_reserved;\r\nexisting->num_bytes = update->num_bytes;\r\n}\r\nif (ref->extent_op) {\r\nif (!existing_ref->extent_op) {\r\nexisting_ref->extent_op = ref->extent_op;\r\n} else {\r\nif (ref->extent_op->update_key) {\r\nmemcpy(&existing_ref->extent_op->key,\r\n&ref->extent_op->key,\r\nsizeof(ref->extent_op->key));\r\nexisting_ref->extent_op->update_key = 1;\r\n}\r\nif (ref->extent_op->update_flags) {\r\nexisting_ref->extent_op->flags_to_set |=\r\nref->extent_op->flags_to_set;\r\nexisting_ref->extent_op->update_flags = 1;\r\n}\r\nkfree(ref->extent_op);\r\n}\r\n}\r\nexisting->ref_mod += update->ref_mod;\r\n}\r\nstatic noinline void add_delayed_ref_head(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_node *ref,\r\nu64 bytenr, u64 num_bytes,\r\nint action, int is_data)\r\n{\r\nstruct btrfs_delayed_ref_node *existing;\r\nstruct btrfs_delayed_ref_head *head_ref = NULL;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nint count_mod = 1;\r\nint must_insert_reserved = 0;\r\nif (action == BTRFS_UPDATE_DELAYED_HEAD)\r\ncount_mod = 0;\r\nelse if (action == BTRFS_DROP_DELAYED_REF)\r\ncount_mod = -1;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\nmust_insert_reserved = 1;\r\nelse\r\nmust_insert_reserved = 0;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = count_mod;\r\nref->type = 0;\r\nref->action = 0;\r\nref->is_head = 1;\r\nref->in_tree = 1;\r\nref->seq = 0;\r\nhead_ref = btrfs_delayed_node_to_head(ref);\r\nhead_ref->must_insert_reserved = must_insert_reserved;\r\nhead_ref->is_data = is_data;\r\nINIT_LIST_HEAD(&head_ref->cluster);\r\nmutex_init(&head_ref->mutex);\r\ntrace_btrfs_delayed_ref_head(ref, head_ref, action);\r\nexisting = tree_insert(&delayed_refs->root, &ref->rb_node);\r\nif (existing) {\r\nupdate_existing_head_ref(existing, ref);\r\nkfree(head_ref);\r\n} else {\r\ndelayed_refs->num_heads++;\r\ndelayed_refs->num_heads_ready++;\r\ndelayed_refs->num_entries++;\r\ntrans->delayed_ref_updates++;\r\n}\r\n}\r\nstatic noinline void add_delayed_tree_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_node *ref,\r\nu64 bytenr, u64 num_bytes, u64 parent,\r\nu64 ref_root, int level, int action,\r\nint for_cow)\r\n{\r\nstruct btrfs_delayed_ref_node *existing;\r\nstruct btrfs_delayed_tree_ref *full_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nu64 seq = 0;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\naction = BTRFS_ADD_DELAYED_REF;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = 1;\r\nref->action = action;\r\nref->is_head = 0;\r\nref->in_tree = 1;\r\nif (need_ref_seq(for_cow, ref_root))\r\nseq = btrfs_get_tree_mod_seq(fs_info, &trans->delayed_ref_elem);\r\nref->seq = seq;\r\nfull_ref = btrfs_delayed_node_to_tree_ref(ref);\r\nfull_ref->parent = parent;\r\nfull_ref->root = ref_root;\r\nif (parent)\r\nref->type = BTRFS_SHARED_BLOCK_REF_KEY;\r\nelse\r\nref->type = BTRFS_TREE_BLOCK_REF_KEY;\r\nfull_ref->level = level;\r\ntrace_btrfs_delayed_tree_ref(ref, full_ref, action);\r\nexisting = tree_insert(&delayed_refs->root, &ref->rb_node);\r\nif (existing) {\r\nupdate_existing_ref(trans, delayed_refs, existing, ref);\r\nkfree(full_ref);\r\n} else {\r\ndelayed_refs->num_entries++;\r\ntrans->delayed_ref_updates++;\r\n}\r\n}\r\nstatic noinline void add_delayed_data_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nstruct btrfs_delayed_ref_node *ref,\r\nu64 bytenr, u64 num_bytes, u64 parent,\r\nu64 ref_root, u64 owner, u64 offset,\r\nint action, int for_cow)\r\n{\r\nstruct btrfs_delayed_ref_node *existing;\r\nstruct btrfs_delayed_data_ref *full_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nu64 seq = 0;\r\nif (action == BTRFS_ADD_DELAYED_EXTENT)\r\naction = BTRFS_ADD_DELAYED_REF;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\natomic_set(&ref->refs, 1);\r\nref->bytenr = bytenr;\r\nref->num_bytes = num_bytes;\r\nref->ref_mod = 1;\r\nref->action = action;\r\nref->is_head = 0;\r\nref->in_tree = 1;\r\nif (need_ref_seq(for_cow, ref_root))\r\nseq = btrfs_get_tree_mod_seq(fs_info, &trans->delayed_ref_elem);\r\nref->seq = seq;\r\nfull_ref = btrfs_delayed_node_to_data_ref(ref);\r\nfull_ref->parent = parent;\r\nfull_ref->root = ref_root;\r\nif (parent)\r\nref->type = BTRFS_SHARED_DATA_REF_KEY;\r\nelse\r\nref->type = BTRFS_EXTENT_DATA_REF_KEY;\r\nfull_ref->objectid = owner;\r\nfull_ref->offset = offset;\r\ntrace_btrfs_delayed_data_ref(ref, full_ref, action);\r\nexisting = tree_insert(&delayed_refs->root, &ref->rb_node);\r\nif (existing) {\r\nupdate_existing_ref(trans, delayed_refs, existing, ref);\r\nkfree(full_ref);\r\n} else {\r\ndelayed_refs->num_entries++;\r\ntrans->delayed_ref_updates++;\r\n}\r\n}\r\nint btrfs_add_delayed_tree_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes, u64 parent,\r\nu64 ref_root, int level, int action,\r\nstruct btrfs_delayed_extent_op *extent_op,\r\nint for_cow)\r\n{\r\nstruct btrfs_delayed_tree_ref *ref;\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nBUG_ON(extent_op && extent_op->is_data);\r\nref = kmalloc(sizeof(*ref), GFP_NOFS);\r\nif (!ref)\r\nreturn -ENOMEM;\r\nhead_ref = kmalloc(sizeof(*head_ref), GFP_NOFS);\r\nif (!head_ref) {\r\nkfree(ref);\r\nreturn -ENOMEM;\r\n}\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nadd_delayed_ref_head(fs_info, trans, &head_ref->node, bytenr,\r\nnum_bytes, action, 0);\r\nadd_delayed_tree_ref(fs_info, trans, &ref->node, bytenr,\r\nnum_bytes, parent, ref_root, level, action,\r\nfor_cow);\r\nspin_unlock(&delayed_refs->lock);\r\nif (need_ref_seq(for_cow, ref_root))\r\nbtrfs_qgroup_record_ref(trans, &ref->node, extent_op);\r\nreturn 0;\r\n}\r\nint btrfs_add_delayed_data_ref(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes,\r\nu64 parent, u64 ref_root,\r\nu64 owner, u64 offset, int action,\r\nstruct btrfs_delayed_extent_op *extent_op,\r\nint for_cow)\r\n{\r\nstruct btrfs_delayed_data_ref *ref;\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nBUG_ON(extent_op && !extent_op->is_data);\r\nref = kmalloc(sizeof(*ref), GFP_NOFS);\r\nif (!ref)\r\nreturn -ENOMEM;\r\nhead_ref = kmalloc(sizeof(*head_ref), GFP_NOFS);\r\nif (!head_ref) {\r\nkfree(ref);\r\nreturn -ENOMEM;\r\n}\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nadd_delayed_ref_head(fs_info, trans, &head_ref->node, bytenr,\r\nnum_bytes, action, 1);\r\nadd_delayed_data_ref(fs_info, trans, &ref->node, bytenr,\r\nnum_bytes, parent, ref_root, owner, offset,\r\naction, for_cow);\r\nspin_unlock(&delayed_refs->lock);\r\nif (need_ref_seq(for_cow, ref_root))\r\nbtrfs_qgroup_record_ref(trans, &ref->node, extent_op);\r\nreturn 0;\r\n}\r\nint btrfs_add_delayed_extent_op(struct btrfs_fs_info *fs_info,\r\nstruct btrfs_trans_handle *trans,\r\nu64 bytenr, u64 num_bytes,\r\nstruct btrfs_delayed_extent_op *extent_op)\r\n{\r\nstruct btrfs_delayed_ref_head *head_ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\nhead_ref = kmalloc(sizeof(*head_ref), GFP_NOFS);\r\nif (!head_ref)\r\nreturn -ENOMEM;\r\nhead_ref->extent_op = extent_op;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nspin_lock(&delayed_refs->lock);\r\nadd_delayed_ref_head(fs_info, trans, &head_ref->node, bytenr,\r\nnum_bytes, BTRFS_UPDATE_DELAYED_HEAD,\r\nextent_op->is_data);\r\nspin_unlock(&delayed_refs->lock);\r\nreturn 0;\r\n}\r\nstruct btrfs_delayed_ref_head *\r\nbtrfs_find_delayed_ref_head(struct btrfs_trans_handle *trans, u64 bytenr)\r\n{\r\nstruct btrfs_delayed_ref_node *ref;\r\nstruct btrfs_delayed_ref_root *delayed_refs;\r\ndelayed_refs = &trans->transaction->delayed_refs;\r\nref = find_ref_head(&delayed_refs->root, bytenr, NULL, 0);\r\nif (ref)\r\nreturn btrfs_delayed_node_to_head(ref);\r\nreturn NULL;\r\n}
