static void inet_frag_secret_rebuild(unsigned long dummy)\r\n{\r\nstruct inet_frags *f = (struct inet_frags *)dummy;\r\nunsigned long now = jiffies;\r\nint i;\r\nwrite_lock(&f->lock);\r\nget_random_bytes(&f->rnd, sizeof(u32));\r\nfor (i = 0; i < INETFRAGS_HASHSZ; i++) {\r\nstruct inet_frag_queue *q;\r\nstruct hlist_node *p, *n;\r\nhlist_for_each_entry_safe(q, p, n, &f->hash[i], list) {\r\nunsigned int hval = f->hashfn(q);\r\nif (hval != i) {\r\nhlist_del(&q->list);\r\nhlist_add_head(&q->list, &f->hash[hval]);\r\n}\r\n}\r\n}\r\nwrite_unlock(&f->lock);\r\nmod_timer(&f->secret_timer, now + f->secret_interval);\r\n}\r\nvoid inet_frags_init(struct inet_frags *f)\r\n{\r\nint i;\r\nfor (i = 0; i < INETFRAGS_HASHSZ; i++)\r\nINIT_HLIST_HEAD(&f->hash[i]);\r\nrwlock_init(&f->lock);\r\nf->rnd = (u32) ((num_physpages ^ (num_physpages>>7)) ^\r\n(jiffies ^ (jiffies >> 6)));\r\nsetup_timer(&f->secret_timer, inet_frag_secret_rebuild,\r\n(unsigned long)f);\r\nf->secret_timer.expires = jiffies + f->secret_interval;\r\nadd_timer(&f->secret_timer);\r\n}\r\nvoid inet_frags_init_net(struct netns_frags *nf)\r\n{\r\nnf->nqueues = 0;\r\natomic_set(&nf->mem, 0);\r\nINIT_LIST_HEAD(&nf->lru_list);\r\n}\r\nvoid inet_frags_fini(struct inet_frags *f)\r\n{\r\ndel_timer(&f->secret_timer);\r\n}\r\nvoid inet_frags_exit_net(struct netns_frags *nf, struct inet_frags *f)\r\n{\r\nnf->low_thresh = 0;\r\nlocal_bh_disable();\r\ninet_frag_evictor(nf, f, true);\r\nlocal_bh_enable();\r\n}\r\nstatic inline void fq_unlink(struct inet_frag_queue *fq, struct inet_frags *f)\r\n{\r\nwrite_lock(&f->lock);\r\nhlist_del(&fq->list);\r\nlist_del(&fq->lru_list);\r\nfq->net->nqueues--;\r\nwrite_unlock(&f->lock);\r\n}\r\nvoid inet_frag_kill(struct inet_frag_queue *fq, struct inet_frags *f)\r\n{\r\nif (del_timer(&fq->timer))\r\natomic_dec(&fq->refcnt);\r\nif (!(fq->last_in & INET_FRAG_COMPLETE)) {\r\nfq_unlink(fq, f);\r\natomic_dec(&fq->refcnt);\r\nfq->last_in |= INET_FRAG_COMPLETE;\r\n}\r\n}\r\nstatic inline void frag_kfree_skb(struct netns_frags *nf, struct inet_frags *f,\r\nstruct sk_buff *skb, int *work)\r\n{\r\nif (work)\r\n*work -= skb->truesize;\r\natomic_sub(skb->truesize, &nf->mem);\r\nif (f->skb_free)\r\nf->skb_free(skb);\r\nkfree_skb(skb);\r\n}\r\nvoid inet_frag_destroy(struct inet_frag_queue *q, struct inet_frags *f,\r\nint *work)\r\n{\r\nstruct sk_buff *fp;\r\nstruct netns_frags *nf;\r\nWARN_ON(!(q->last_in & INET_FRAG_COMPLETE));\r\nWARN_ON(del_timer(&q->timer) != 0);\r\nfp = q->fragments;\r\nnf = q->net;\r\nwhile (fp) {\r\nstruct sk_buff *xp = fp->next;\r\nfrag_kfree_skb(nf, f, fp, work);\r\nfp = xp;\r\n}\r\nif (work)\r\n*work -= f->qsize;\r\natomic_sub(f->qsize, &nf->mem);\r\nif (f->destructor)\r\nf->destructor(q);\r\nkfree(q);\r\n}\r\nint inet_frag_evictor(struct netns_frags *nf, struct inet_frags *f, bool force)\r\n{\r\nstruct inet_frag_queue *q;\r\nint work, evicted = 0;\r\nif (!force) {\r\nif (atomic_read(&nf->mem) <= nf->high_thresh)\r\nreturn 0;\r\n}\r\nwork = atomic_read(&nf->mem) - nf->low_thresh;\r\nwhile (work > 0) {\r\nread_lock(&f->lock);\r\nif (list_empty(&nf->lru_list)) {\r\nread_unlock(&f->lock);\r\nbreak;\r\n}\r\nq = list_first_entry(&nf->lru_list,\r\nstruct inet_frag_queue, lru_list);\r\natomic_inc(&q->refcnt);\r\nread_unlock(&f->lock);\r\nspin_lock(&q->lock);\r\nif (!(q->last_in & INET_FRAG_COMPLETE))\r\ninet_frag_kill(q, f);\r\nspin_unlock(&q->lock);\r\nif (atomic_dec_and_test(&q->refcnt))\r\ninet_frag_destroy(q, f, &work);\r\nevicted++;\r\n}\r\nreturn evicted;\r\n}\r\nstatic struct inet_frag_queue *inet_frag_intern(struct netns_frags *nf,\r\nstruct inet_frag_queue *qp_in, struct inet_frags *f,\r\nvoid *arg)\r\n{\r\nstruct inet_frag_queue *qp;\r\n#ifdef CONFIG_SMP\r\nstruct hlist_node *n;\r\n#endif\r\nunsigned int hash;\r\nwrite_lock(&f->lock);\r\nhash = f->hashfn(qp_in);\r\n#ifdef CONFIG_SMP\r\nhlist_for_each_entry(qp, n, &f->hash[hash], list) {\r\nif (qp->net == nf && f->match(qp, arg)) {\r\natomic_inc(&qp->refcnt);\r\nwrite_unlock(&f->lock);\r\nqp_in->last_in |= INET_FRAG_COMPLETE;\r\ninet_frag_put(qp_in, f);\r\nreturn qp;\r\n}\r\n}\r\n#endif\r\nqp = qp_in;\r\nif (!mod_timer(&qp->timer, jiffies + nf->timeout))\r\natomic_inc(&qp->refcnt);\r\natomic_inc(&qp->refcnt);\r\nhlist_add_head(&qp->list, &f->hash[hash]);\r\nlist_add_tail(&qp->lru_list, &nf->lru_list);\r\nnf->nqueues++;\r\nwrite_unlock(&f->lock);\r\nreturn qp;\r\n}\r\nstatic struct inet_frag_queue *inet_frag_alloc(struct netns_frags *nf,\r\nstruct inet_frags *f, void *arg)\r\n{\r\nstruct inet_frag_queue *q;\r\nq = kzalloc(f->qsize, GFP_ATOMIC);\r\nif (q == NULL)\r\nreturn NULL;\r\nq->net = nf;\r\nf->constructor(q, arg);\r\natomic_add(f->qsize, &nf->mem);\r\nsetup_timer(&q->timer, f->frag_expire, (unsigned long)q);\r\nspin_lock_init(&q->lock);\r\natomic_set(&q->refcnt, 1);\r\nreturn q;\r\n}\r\nstatic struct inet_frag_queue *inet_frag_create(struct netns_frags *nf,\r\nstruct inet_frags *f, void *arg)\r\n{\r\nstruct inet_frag_queue *q;\r\nq = inet_frag_alloc(nf, f, arg);\r\nif (q == NULL)\r\nreturn NULL;\r\nreturn inet_frag_intern(nf, q, f, arg);\r\n}\r\nstruct inet_frag_queue *inet_frag_find(struct netns_frags *nf,\r\nstruct inet_frags *f, void *key, unsigned int hash)\r\n__releases(&f->lock
