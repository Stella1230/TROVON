static inline bool PageWasActive(struct page *page)\r\n{\r\nreturn true;\r\n}\r\nstatic inline void SetPageWasActive(struct page *page)\r\n{\r\n}\r\nstatic inline void frontswap_tmem_exclusive_gets(bool b)\r\n{\r\n}\r\nstatic inline int zcache_comp_op(enum comp_op op,\r\nconst u8 *src, unsigned int slen,\r\nu8 *dst, unsigned int *dlen)\r\n{\r\nstruct crypto_comp *tfm;\r\nint ret = -1;\r\nBUG_ON(!zcache_comp_pcpu_tfms);\r\ntfm = *per_cpu_ptr(zcache_comp_pcpu_tfms, get_cpu());\r\nBUG_ON(!tfm);\r\nswitch (op) {\r\ncase ZCACHE_COMPOP_COMPRESS:\r\nret = crypto_comp_compress(tfm, src, slen, dst, dlen);\r\nbreak;\r\ncase ZCACHE_COMPOP_DECOMPRESS:\r\nret = crypto_comp_decompress(tfm, src, slen, dst, dlen);\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\n}\r\nput_cpu();\r\nreturn ret;\r\n}\r\nstatic int zcache_debugfs_init(void)\r\n{\r\nstruct dentry *root = debugfs_create_dir("zcache", NULL);\r\nif (root == NULL)\r\nreturn -ENXIO;\r\nzdfs("obj_count", S_IRUGO, root, &zcache_obj_count);\r\nzdfs("obj_count_max", S_IRUGO, root, &zcache_obj_count_max);\r\nzdfs("objnode_count", S_IRUGO, root, &zcache_objnode_count);\r\nzdfs("objnode_count_max", S_IRUGO, root, &zcache_objnode_count_max);\r\nzdfs("flush_total", S_IRUGO, root, &zcache_flush_total);\r\nzdfs("flush_found", S_IRUGO, root, &zcache_flush_found);\r\nzdfs("flobj_total", S_IRUGO, root, &zcache_flobj_total);\r\nzdfs("flobj_found", S_IRUGO, root, &zcache_flobj_found);\r\nzdfs("failed_eph_puts", S_IRUGO, root, &zcache_failed_eph_puts);\r\nzdfs("failed_pers_puts", S_IRUGO, root, &zcache_failed_pers_puts);\r\nzdfs("failed_get_free_pages", S_IRUGO, root,\r\n&zcache_failed_getfreepages);\r\nzdfs("failed_alloc", S_IRUGO, root, &zcache_failed_alloc);\r\nzdfs("put_to_flush", S_IRUGO, root, &zcache_put_to_flush);\r\nzdfs("compress_poor", S_IRUGO, root, &zcache_compress_poor);\r\nzdfs("mean_compress_poor", S_IRUGO, root, &zcache_mean_compress_poor);\r\nzdfs("eph_ate_tail", S_IRUGO, root, &zcache_eph_ate_tail);\r\nzdfs("eph_ate_tail_failed", S_IRUGO, root, &zcache_eph_ate_tail_failed);\r\nzdfs("pers_ate_eph", S_IRUGO, root, &zcache_pers_ate_eph);\r\nzdfs("pers_ate_eph_failed", S_IRUGO, root, &zcache_pers_ate_eph_failed);\r\nzdfs("evicted_eph_zpages", S_IRUGO, root, &zcache_evicted_eph_zpages);\r\nzdfs("evicted_eph_pageframes", S_IRUGO, root,\r\n&zcache_evicted_eph_pageframes);\r\nzdfs("eph_pageframes", S_IRUGO, root, &zcache_eph_pageframes);\r\nzdfs("eph_pageframes_max", S_IRUGO, root, &zcache_eph_pageframes_max);\r\nzdfs("pers_pageframes", S_IRUGO, root, &zcache_pers_pageframes);\r\nzdfs("pers_pageframes_max", S_IRUGO, root, &zcache_pers_pageframes_max);\r\nzdfs("eph_zpages", S_IRUGO, root, &zcache_eph_zpages);\r\nzdfs("eph_zpages_max", S_IRUGO, root, &zcache_eph_zpages_max);\r\nzdfs("pers_zpages", S_IRUGO, root, &zcache_pers_zpages);\r\nzdfs("pers_zpages_max", S_IRUGO, root, &zcache_pers_zpages_max);\r\nzdfs("last_active_file_pageframes", S_IRUGO, root,\r\n&zcache_last_active_file_pageframes);\r\nzdfs("last_inactive_file_pageframes", S_IRUGO, root,\r\n&zcache_last_inactive_file_pageframes);\r\nzdfs("last_active_anon_pageframes", S_IRUGO, root,\r\n&zcache_last_active_anon_pageframes);\r\nzdfs("last_inactive_anon_pageframes", S_IRUGO, root,\r\n&zcache_last_inactive_anon_pageframes);\r\nzdfs("eph_nonactive_puts_ignored", S_IRUGO, root,\r\n&zcache_eph_nonactive_puts_ignored);\r\nzdfs("pers_nonactive_puts_ignored", S_IRUGO, root,\r\n&zcache_pers_nonactive_puts_ignored);\r\nzdfs64("eph_zbytes", S_IRUGO, root, &zcache_eph_zbytes);\r\nzdfs64("eph_zbytes_max", S_IRUGO, root, &zcache_eph_zbytes_max);\r\nzdfs64("pers_zbytes", S_IRUGO, root, &zcache_pers_zbytes);\r\nzdfs64("pers_zbytes_max", S_IRUGO, root, &zcache_pers_zbytes_max);\r\nreturn 0;\r\n}\r\nvoid zcache_dump(void)\r\n{\r\npr_info("zcache: obj_count=%lu\n", zcache_obj_count);\r\npr_info("zcache: obj_count_max=%lu\n", zcache_obj_count_max);\r\npr_info("zcache: objnode_count=%lu\n", zcache_objnode_count);\r\npr_info("zcache: objnode_count_max=%lu\n", zcache_objnode_count_max);\r\npr_info("zcache: flush_total=%lu\n", zcache_flush_total);\r\npr_info("zcache: flush_found=%lu\n", zcache_flush_found);\r\npr_info("zcache: flobj_total=%lu\n", zcache_flobj_total);\r\npr_info("zcache: flobj_found=%lu\n", zcache_flobj_found);\r\npr_info("zcache: failed_eph_puts=%lu\n", zcache_failed_eph_puts);\r\npr_info("zcache: failed_pers_puts=%lu\n", zcache_failed_pers_puts);\r\npr_info("zcache: failed_get_free_pages=%lu\n",\r\nzcache_failed_getfreepages);\r\npr_info("zcache: failed_alloc=%lu\n", zcache_failed_alloc);\r\npr_info("zcache: put_to_flush=%lu\n", zcache_put_to_flush);\r\npr_info("zcache: compress_poor=%lu\n", zcache_compress_poor);\r\npr_info("zcache: mean_compress_poor=%lu\n",\r\nzcache_mean_compress_poor);\r\npr_info("zcache: eph_ate_tail=%lu\n", zcache_eph_ate_tail);\r\npr_info("zcache: eph_ate_tail_failed=%lu\n",\r\nzcache_eph_ate_tail_failed);\r\npr_info("zcache: pers_ate_eph=%lu\n", zcache_pers_ate_eph);\r\npr_info("zcache: pers_ate_eph_failed=%lu\n",\r\nzcache_pers_ate_eph_failed);\r\npr_info("zcache: evicted_eph_zpages=%lu\n", zcache_evicted_eph_zpages);\r\npr_info("zcache: evicted_eph_pageframes=%lu\n",\r\nzcache_evicted_eph_pageframes);\r\npr_info("zcache: eph_pageframes=%lu\n", zcache_eph_pageframes);\r\npr_info("zcache: eph_pageframes_max=%lu\n", zcache_eph_pageframes_max);\r\npr_info("zcache: pers_pageframes=%lu\n", zcache_pers_pageframes);\r\npr_info("zcache: pers_pageframes_max=%lu\n",\r\nzcache_pers_pageframes_max);\r\npr_info("zcache: eph_zpages=%lu\n", zcache_eph_zpages);\r\npr_info("zcache: eph_zpages_max=%lu\n", zcache_eph_zpages_max);\r\npr_info("zcache: pers_zpages=%lu\n", zcache_pers_zpages);\r\npr_info("zcache: pers_zpages_max=%lu\n", zcache_pers_zpages_max);\r\npr_info("zcache: eph_zbytes=%llu\n",\r\n(unsigned long long)zcache_eph_zbytes);\r\npr_info("zcache: eph_zbytes_max=%llu\n",\r\n(unsigned long long)zcache_eph_zbytes_max);\r\npr_info("zcache: pers_zbytes=%llu\n",\r\n(unsigned long long)zcache_pers_zbytes);\r\npr_info("zcache: pers_zbytes_max=%llu\n",\r\n(unsigned long long)zcache_pers_zbytes_max);\r\n}\r\nstatic inline bool is_local_client(struct zcache_client *cli)\r\n{\r\nreturn cli == &zcache_host;\r\n}\r\nstatic struct zcache_client *zcache_get_client_by_id(uint16_t cli_id)\r\n{\r\nstruct zcache_client *cli = &zcache_host;\r\nif (cli_id != LOCAL_CLIENT) {\r\nif (cli_id >= MAX_CLIENTS)\r\ngoto out;\r\ncli = &zcache_clients[cli_id];\r\n}\r\nout:\r\nreturn cli;\r\n}\r\nstruct tmem_pool *zcache_get_pool_by_id(uint16_t cli_id, uint16_t poolid)\r\n{\r\nstruct tmem_pool *pool = NULL;\r\nstruct zcache_client *cli = NULL;\r\ncli = zcache_get_client_by_id(cli_id);\r\nif (cli == NULL)\r\ngoto out;\r\nif (!is_local_client(cli))\r\natomic_inc(&cli->refcount);\r\nif (poolid < MAX_POOLS_PER_CLIENT) {\r\npool = cli->tmem_pools[poolid];\r\nif (pool != NULL)\r\natomic_inc(&pool->refcount);\r\n}\r\nout:\r\nreturn pool;\r\n}\r\nvoid zcache_put_pool(struct tmem_pool *pool)\r\n{\r\nstruct zcache_client *cli = NULL;\r\nif (pool == NULL)\r\nBUG();\r\ncli = pool->client;\r\natomic_dec(&pool->refcount);\r\nif (!is_local_client(cli))\r\natomic_dec(&cli->refcount);\r\n}\r\nint zcache_new_client(uint16_t cli_id)\r\n{\r\nstruct zcache_client *cli;\r\nint ret = -1;\r\ncli = zcache_get_client_by_id(cli_id);\r\nif (cli == NULL)\r\ngoto out;\r\nif (cli->allocated)\r\ngoto out;\r\ncli->allocated = 1;\r\nret = 0;\r\nout:\r\nreturn ret;\r\n}\r\nstatic struct tmem_objnode *zcache_objnode_alloc(struct tmem_pool *pool)\r\n{\r\nstruct tmem_objnode *objnode = NULL;\r\nstruct zcache_preload *kp;\r\nint i;\r\nkp = &__get_cpu_var(zcache_preloads);\r\nfor (i = 0; i < ARRAY_SIZE(kp->objnodes); i++) {\r\nobjnode = kp->objnodes[i];\r\nif (objnode != NULL) {\r\nkp->objnodes[i] = NULL;\r\nbreak;\r\n}\r\n}\r\nBUG_ON(objnode == NULL);\r\nzcache_objnode_count = atomic_inc_return(&zcache_objnode_atomic);\r\nif (zcache_objnode_count > zcache_objnode_count_max)\r\nzcache_objnode_count_max = zcache_objnode_count;\r\nreturn objnode;\r\n}\r\nstatic void zcache_objnode_free(struct tmem_objnode *objnode,\r\nstruct tmem_pool *pool)\r\n{\r\nzcache_objnode_count =\r\natomic_dec_return(&zcache_objnode_atomic);\r\nBUG_ON(zcache_objnode_count < 0);\r\nkmem_cache_free(zcache_objnode_cache, objnode);\r\n}\r\nstatic struct tmem_obj *zcache_obj_alloc(struct tmem_pool *pool)\r\n{\r\nstruct tmem_obj *obj = NULL;\r\nstruct zcache_preload *kp;\r\nkp = &__get_cpu_var(zcache_preloads);\r\nobj = kp->obj;\r\nBUG_ON(obj == NULL);\r\nkp->obj = NULL;\r\nzcache_obj_count = atomic_inc_return(&zcache_obj_atomic);\r\nif (zcache_obj_count > zcache_obj_count_max)\r\nzcache_obj_count_max = zcache_obj_count;\r\nreturn obj;\r\n}\r\nstatic void zcache_obj_free(struct tmem_obj *obj, struct tmem_pool *pool)\r\n{\r\nzcache_obj_count =\r\natomic_dec_return(&zcache_obj_atomic);\r\nBUG_ON(zcache_obj_count < 0);\r\nkmem_cache_free(zcache_obj_cache, obj);\r\n}\r\nstatic struct page *zcache_alloc_page(void)\r\n{\r\nstruct page *page = alloc_page(ZCACHE_GFP_MASK);\r\nif (page != NULL)\r\nzcache_pageframes_alloced =\r\natomic_inc_return(&zcache_pageframes_alloced_atomic);\r\nreturn page;\r\n}\r\nstatic void zcache_unacct_page(void)\r\n{\r\nzcache_pageframes_freed =\r\natomic_inc_return(&zcache_pageframes_freed_atomic);\r\n}\r\nstatic void zcache_free_page(struct page *page)\r\n{\r\nlong curr_pageframes;\r\nstatic long max_pageframes, min_pageframes;\r\nif (page == NULL)\r\nBUG();\r\n__free_page(page);\r\nzcache_pageframes_freed =\r\natomic_inc_return(&zcache_pageframes_freed_atomic);\r\ncurr_pageframes = zcache_pageframes_alloced -\r\natomic_read(&zcache_pageframes_freed_atomic) -\r\natomic_read(&zcache_eph_pageframes_atomic) -\r\natomic_read(&zcache_pers_pageframes_atomic);\r\nif (curr_pageframes > max_pageframes)\r\nmax_pageframes = curr_pageframes;\r\nif (curr_pageframes < min_pageframes)\r\nmin_pageframes = curr_pageframes;\r\n#ifdef ZCACHE_DEBUG\r\nif (curr_pageframes > 2L || curr_pageframes < -2L) {\r\n}\r\n#endif\r\n}\r\nstatic void *zcache_pampd_eph_create(char *data, size_t size, bool raw,\r\nstruct tmem_handle *th)\r\n{\r\nvoid *pampd = NULL, *cdata = data;\r\nunsigned clen = size;\r\nstruct page *page = (struct page *)(data), *newpage;\r\nif (!raw) {\r\nzcache_compress(page, &cdata, &clen);\r\nif (clen > zbud_max_buddy_size()) {\r\nzcache_compress_poor++;\r\ngoto out;\r\n}\r\n} else {\r\nBUG_ON(clen > zbud_max_buddy_size());\r\n}\r\npampd = (void *)zbud_match_prep(th, true, cdata, clen);\r\nif (pampd != NULL)\r\ngoto got_pampd;\r\nnewpage = zcache_alloc_page();\r\nif (newpage != NULL)\r\ngoto create_in_new_page;\r\nzcache_failed_getfreepages++;\r\nnewpage = zcache_evict_eph_pageframe();\r\nif (newpage == NULL) {\r\nzcache_eph_ate_tail_failed++;\r\ngoto out;\r\n}\r\nzcache_eph_ate_tail++;\r\ncreate_in_new_page:\r\npampd = (void *)zbud_create_prep(th, true, cdata, clen, newpage);\r\nBUG_ON(pampd == NULL);\r\nzcache_eph_pageframes =\r\natomic_inc_return(&zcache_eph_pageframes_atomic);\r\nif (zcache_eph_pageframes > zcache_eph_pageframes_max)\r\nzcache_eph_pageframes_max = zcache_eph_pageframes;\r\ngot_pampd:\r\nzcache_eph_zbytes =\r\natomic_long_add_return(clen, &zcache_eph_zbytes_atomic);\r\nif (zcache_eph_zbytes > zcache_eph_zbytes_max)\r\nzcache_eph_zbytes_max = zcache_eph_zbytes;\r\nzcache_eph_zpages = atomic_inc_return(&zcache_eph_zpages_atomic);\r\nif (zcache_eph_zpages > zcache_eph_zpages_max)\r\nzcache_eph_zpages_max = zcache_eph_zpages;\r\nif (ramster_enabled && raw)\r\nramster_count_foreign_pages(true, 1);\r\nout:\r\nreturn pampd;\r\n}\r\nstatic void *zcache_pampd_pers_create(char *data, size_t size, bool raw,\r\nstruct tmem_handle *th)\r\n{\r\nvoid *pampd = NULL, *cdata = data;\r\nunsigned clen = size;\r\nstruct page *page = (struct page *)(data), *newpage;\r\nunsigned long zbud_mean_zsize;\r\nunsigned long curr_pers_zpages, total_zsize;\r\nif (data == NULL) {\r\nBUG_ON(!ramster_enabled);\r\ngoto create_pampd;\r\n}\r\ncurr_pers_zpages = zcache_pers_zpages;\r\nif (!raw)\r\nzcache_compress(page, &cdata, &clen);\r\nif (clen > zbud_max_zsize) {\r\nzcache_compress_poor++;\r\ngoto out;\r\n}\r\nif ((clen > zbud_max_mean_zsize) && (curr_pers_zpages > 0)) {\r\ntotal_zsize = zcache_pers_zbytes;\r\nif ((long)total_zsize < 0)\r\ntotal_zsize = 0;\r\nzbud_mean_zsize = div_u64(total_zsize,\r\ncurr_pers_zpages);\r\nif (zbud_mean_zsize > zbud_max_mean_zsize) {\r\nzcache_mean_compress_poor++;\r\ngoto out;\r\n}\r\n}\r\ncreate_pampd:\r\npampd = (void *)zbud_match_prep(th, false, cdata, clen);\r\nif (pampd != NULL)\r\ngoto got_pampd;\r\nnewpage = zcache_alloc_page();\r\nif (newpage != NULL)\r\ngoto create_in_new_page;\r\nzcache_failed_getfreepages++;\r\nnewpage = zcache_evict_eph_pageframe();\r\nif (newpage == NULL) {\r\nzcache_pers_ate_eph_failed++;\r\ngoto out;\r\n}\r\nzcache_pers_ate_eph++;\r\ncreate_in_new_page:\r\npampd = (void *)zbud_create_prep(th, false, cdata, clen, newpage);\r\nBUG_ON(pampd == NULL);\r\nzcache_pers_pageframes =\r\natomic_inc_return(&zcache_pers_pageframes_atomic);\r\nif (zcache_pers_pageframes > zcache_pers_pageframes_max)\r\nzcache_pers_pageframes_max = zcache_pers_pageframes;\r\ngot_pampd:\r\nzcache_pers_zpages = atomic_inc_return(&zcache_pers_zpages_atomic);\r\nif (zcache_pers_zpages > zcache_pers_zpages_max)\r\nzcache_pers_zpages_max = zcache_pers_zpages;\r\nzcache_pers_zbytes =\r\natomic_long_add_return(clen, &zcache_pers_zbytes_atomic);\r\nif (zcache_pers_zbytes > zcache_pers_zbytes_max)\r\nzcache_pers_zbytes_max = zcache_pers_zbytes;\r\nif (ramster_enabled && raw)\r\nramster_count_foreign_pages(false, 1);\r\nout:\r\nreturn pampd;\r\n}\r\nvoid *zcache_pampd_create(char *data, unsigned int size, bool raw,\r\nint eph, struct tmem_handle *th)\r\n{\r\nvoid *pampd = NULL;\r\nstruct zcache_preload *kp;\r\nstruct tmem_objnode *objnode;\r\nstruct tmem_obj *obj;\r\nint i;\r\nBUG_ON(!irqs_disabled());\r\nBUG_ON(zcache_objnode_cache == NULL);\r\nBUG_ON(zcache_obj_cache == NULL);\r\nkp = &__get_cpu_var(zcache_preloads);\r\nfor (i = 0; i < ARRAY_SIZE(kp->objnodes); i++) {\r\nobjnode = kp->objnodes[i];\r\nif (objnode == NULL) {\r\nobjnode = kmem_cache_alloc(zcache_objnode_cache,\r\nZCACHE_GFP_MASK);\r\nif (unlikely(objnode == NULL)) {\r\nzcache_failed_alloc++;\r\ngoto out;\r\n}\r\nkp->objnodes[i] = objnode;\r\n}\r\n}\r\nif (kp->obj == NULL) {\r\nobj = kmem_cache_alloc(zcache_obj_cache, ZCACHE_GFP_MASK);\r\nkp->obj = obj;\r\n}\r\nif (unlikely(kp->obj == NULL)) {\r\nzcache_failed_alloc++;\r\ngoto out;\r\n}\r\nif (eph)\r\npampd = zcache_pampd_eph_create(data, size, raw, th);\r\nelse\r\npampd = zcache_pampd_pers_create(data, size, raw, th);\r\nout:\r\nreturn pampd;\r\n}\r\nvoid zcache_pampd_create_finish(void *pampd, bool eph)\r\n{\r\nzbud_create_finish((struct zbudref *)pampd, eph);\r\n}\r\nstatic void zcache_decompress(char *from_va, unsigned int size, char *to_va)\r\n{\r\nint ret;\r\nunsigned int outlen = PAGE_SIZE;\r\nret = zcache_comp_op(ZCACHE_COMPOP_DECOMPRESS, from_va, size,\r\nto_va, &outlen);\r\nBUG_ON(ret);\r\nBUG_ON(outlen != PAGE_SIZE);\r\n}\r\nvoid zcache_decompress_to_page(char *from_va, unsigned int size,\r\nstruct page *to_page)\r\n{\r\nchar *to_va = kmap_atomic(to_page);\r\nzcache_decompress(from_va, size, to_va);\r\nkunmap_atomic(to_va);\r\n}\r\nstatic int zcache_pampd_get_data(char *data, size_t *sizep, bool raw,\r\nvoid *pampd, struct tmem_pool *pool,\r\nstruct tmem_oid *oid, uint32_t index)\r\n{\r\nint ret;\r\nbool eph = !is_persistent(pool);\r\nBUG_ON(preemptible());\r\nBUG_ON(eph);\r\nBUG_ON(pampd_is_remote(pampd));\r\nif (raw)\r\nret = zbud_copy_from_zbud(data, (struct zbudref *)pampd,\r\nsizep, eph);\r\nelse {\r\nret = zbud_decompress((struct page *)(data),\r\n(struct zbudref *)pampd, false,\r\nzcache_decompress);\r\n*sizep = PAGE_SIZE;\r\n}\r\nreturn ret;\r\n}\r\nstatic int zcache_pampd_get_data_and_free(char *data, size_t *sizep, bool raw,\r\nvoid *pampd, struct tmem_pool *pool,\r\nstruct tmem_oid *oid, uint32_t index)\r\n{\r\nint ret;\r\nbool eph = !is_persistent(pool);\r\nstruct page *page = NULL;\r\nunsigned int zsize, zpages;\r\nBUG_ON(preemptible());\r\nBUG_ON(pampd_is_remote(pampd));\r\nif (raw)\r\nret = zbud_copy_from_zbud(data, (struct zbudref *)pampd,\r\nsizep, eph);\r\nelse {\r\nret = zbud_decompress((struct page *)(data),\r\n(struct zbudref *)pampd, eph,\r\nzcache_decompress);\r\n*sizep = PAGE_SIZE;\r\n}\r\npage = zbud_free_and_delist((struct zbudref *)pampd, eph,\r\n&zsize, &zpages);\r\nif (eph) {\r\nif (page)\r\nzcache_eph_pageframes =\r\natomic_dec_return(&zcache_eph_pageframes_atomic);\r\nzcache_eph_zpages =\r\natomic_sub_return(zpages, &zcache_eph_zpages_atomic);\r\nzcache_eph_zbytes =\r\natomic_long_sub_return(zsize, &zcache_eph_zbytes_atomic);\r\n} else {\r\nif (page)\r\nzcache_pers_pageframes =\r\natomic_dec_return(&zcache_pers_pageframes_atomic);\r\nzcache_pers_zpages =\r\natomic_sub_return(zpages, &zcache_pers_zpages_atomic);\r\nzcache_pers_zbytes =\r\natomic_long_sub_return(zsize, &zcache_pers_zbytes_atomic);\r\n}\r\nif (!is_local_client(pool->client))\r\nramster_count_foreign_pages(eph, -1);\r\nif (page)\r\nzcache_free_page(page);\r\nreturn ret;\r\n}\r\nstatic void zcache_pampd_free(void *pampd, struct tmem_pool *pool,\r\nstruct tmem_oid *oid, uint32_t index, bool acct)\r\n{\r\nstruct page *page = NULL;\r\nunsigned int zsize, zpages;\r\nBUG_ON(preemptible());\r\nif (pampd_is_remote(pampd)) {\r\nBUG_ON(!ramster_enabled);\r\npampd = ramster_pampd_free(pampd, pool, oid, index, acct);\r\nif (pampd == NULL)\r\nreturn;\r\n}\r\nif (is_ephemeral(pool)) {\r\npage = zbud_free_and_delist((struct zbudref *)pampd,\r\ntrue, &zsize, &zpages);\r\nif (page)\r\nzcache_eph_pageframes =\r\natomic_dec_return(&zcache_eph_pageframes_atomic);\r\nzcache_eph_zpages =\r\natomic_sub_return(zpages, &zcache_eph_zpages_atomic);\r\nzcache_eph_zbytes =\r\natomic_long_sub_return(zsize, &zcache_eph_zbytes_atomic);\r\n} else {\r\npage = zbud_free_and_delist((struct zbudref *)pampd,\r\nfalse, &zsize, &zpages);\r\nif (page)\r\nzcache_pers_pageframes =\r\natomic_dec_return(&zcache_pers_pageframes_atomic);\r\nzcache_pers_zpages =\r\natomic_sub_return(zpages, &zcache_pers_zpages_atomic);\r\nzcache_pers_zbytes =\r\natomic_long_sub_return(zsize, &zcache_pers_zbytes_atomic);\r\n}\r\nif (!is_local_client(pool->client))\r\nramster_count_foreign_pages(is_ephemeral(pool), -1);\r\nif (page)\r\nzcache_free_page(page);\r\n}\r\nstatic void zcache_compress(struct page *from, void **out_va, unsigned *out_len)\r\n{\r\nint ret;\r\nunsigned char *dmem = __get_cpu_var(zcache_dstmem);\r\nchar *from_va;\r\nBUG_ON(!irqs_disabled());\r\nBUG_ON(dmem == NULL);\r\n*out_len = PAGE_SIZE << ZCACHE_DSTMEM_ORDER;\r\nfrom_va = kmap_atomic(from);\r\nmb();\r\nret = zcache_comp_op(ZCACHE_COMPOP_COMPRESS, from_va, PAGE_SIZE, dmem,\r\nout_len);\r\nBUG_ON(ret);\r\n*out_va = dmem;\r\nkunmap_atomic(from_va);\r\n}\r\nstatic int zcache_comp_cpu_up(int cpu)\r\n{\r\nstruct crypto_comp *tfm;\r\ntfm = crypto_alloc_comp(zcache_comp_name, 0, 0);\r\nif (IS_ERR(tfm))\r\nreturn NOTIFY_BAD;\r\n*per_cpu_ptr(zcache_comp_pcpu_tfms, cpu) = tfm;\r\nreturn NOTIFY_OK;\r\n}\r\nstatic void zcache_comp_cpu_down(int cpu)\r\n{\r\nstruct crypto_comp *tfm;\r\ntfm = *per_cpu_ptr(zcache_comp_pcpu_tfms, cpu);\r\ncrypto_free_comp(tfm);\r\n*per_cpu_ptr(zcache_comp_pcpu_tfms, cpu) = NULL;\r\n}\r\nstatic int zcache_cpu_notifier(struct notifier_block *nb,\r\nunsigned long action, void *pcpu)\r\n{\r\nint ret, i, cpu = (long)pcpu;\r\nstruct zcache_preload *kp;\r\nswitch (action) {\r\ncase CPU_UP_PREPARE:\r\nret = zcache_comp_cpu_up(cpu);\r\nif (ret != NOTIFY_OK) {\r\npr_err("%s: can't allocate compressor xform\n",\r\nnamestr);\r\nreturn ret;\r\n}\r\nper_cpu(zcache_dstmem, cpu) = (void *)__get_free_pages(\r\nGFP_KERNEL | __GFP_REPEAT, ZCACHE_DSTMEM_ORDER);\r\nif (ramster_enabled)\r\nramster_cpu_up(cpu);\r\nbreak;\r\ncase CPU_DEAD:\r\ncase CPU_UP_CANCELED:\r\nzcache_comp_cpu_down(cpu);\r\nfree_pages((unsigned long)per_cpu(zcache_dstmem, cpu),\r\nZCACHE_DSTMEM_ORDER);\r\nper_cpu(zcache_dstmem, cpu) = NULL;\r\nkp = &per_cpu(zcache_preloads, cpu);\r\nfor (i = 0; i < ARRAY_SIZE(kp->objnodes); i++) {\r\nif (kp->objnodes[i])\r\nkmem_cache_free(zcache_objnode_cache,\r\nkp->objnodes[i]);\r\n}\r\nif (kp->obj) {\r\nkmem_cache_free(zcache_obj_cache, kp->obj);\r\nkp->obj = NULL;\r\n}\r\nif (ramster_enabled)\r\nramster_cpu_down(cpu);\r\nbreak;\r\ndefault:\r\nbreak;\r\n}\r\nreturn NOTIFY_OK;\r\n}\r\nstatic struct page *zcache_evict_eph_pageframe(void)\r\n{\r\nstruct page *page;\r\nunsigned int zsize = 0, zpages = 0;\r\npage = zbud_evict_pageframe_lru(&zsize, &zpages);\r\nif (page == NULL)\r\ngoto out;\r\nzcache_eph_zbytes = atomic_long_sub_return(zsize,\r\n&zcache_eph_zbytes_atomic);\r\nzcache_eph_zpages = atomic_sub_return(zpages,\r\n&zcache_eph_zpages_atomic);\r\nzcache_evicted_eph_zpages++;\r\nzcache_eph_pageframes =\r\natomic_dec_return(&zcache_eph_pageframes_atomic);\r\nzcache_evicted_eph_pageframes++;\r\nout:\r\nreturn page;\r\n}\r\nstatic int zcache_frontswap_unuse(void)\r\n{\r\nstruct tmem_handle th[2];\r\nint ret = -ENOMEM;\r\nint nzbuds, unuse_ret;\r\nunsigned type;\r\nstruct page *newpage1 = NULL, *newpage2 = NULL;\r\nstruct page *evictpage1 = NULL, *evictpage2 = NULL;\r\npgoff_t offset;\r\nnewpage1 = alloc_page(ZCACHE_GFP_MASK);\r\nnewpage2 = alloc_page(ZCACHE_GFP_MASK);\r\nif (newpage1 == NULL)\r\nevictpage1 = zcache_evict_eph_pageframe();\r\nif (newpage2 == NULL)\r\nevictpage2 = zcache_evict_eph_pageframe();\r\nif (evictpage1 == NULL || evictpage2 == NULL)\r\ngoto free_and_out;\r\nnzbuds = zbud_make_zombie_lru(&th[0], NULL, NULL, false);\r\nif (nzbuds == 0) {\r\nret = -ENOENT;\r\ngoto free_and_out;\r\n}\r\nunswiz(th[0].oid, th[0].index, &type, &offset);\r\nunuse_ret = frontswap_unuse(type, offset,\r\nnewpage1 != NULL ? newpage1 : evictpage1,\r\nZCACHE_GFP_MASK);\r\nif (unuse_ret != 0)\r\ngoto free_and_out;\r\nelse if (evictpage1 != NULL)\r\nzcache_unacct_page();\r\nnewpage1 = NULL;\r\nevictpage1 = NULL;\r\nif (nzbuds == 2) {\r\nunswiz(th[1].oid, th[1].index, &type, &offset);\r\nunuse_ret = frontswap_unuse(type, offset,\r\nnewpage2 != NULL ? newpage2 : evictpage2,\r\nZCACHE_GFP_MASK);\r\nif (unuse_ret != 0) {\r\ngoto free_and_out;\r\n} else if (evictpage2 != NULL) {\r\nzcache_unacct_page();\r\n}\r\n}\r\nret = 0;\r\ngoto out;\r\nfree_and_out:\r\nif (newpage1 != NULL)\r\n__free_page(newpage1);\r\nif (newpage2 != NULL)\r\n__free_page(newpage2);\r\nif (evictpage1 != NULL)\r\nzcache_free_page(evictpage1);\r\nif (evictpage2 != NULL)\r\nzcache_free_page(evictpage2);\r\nout:\r\nreturn ret;\r\n}\r\nstatic int shrink_zcache_memory(struct shrinker *shrink,\r\nstruct shrink_control *sc)\r\n{\r\nstatic bool in_progress;\r\nint ret = -1;\r\nint nr = sc->nr_to_scan;\r\nint nr_evict = 0;\r\nint nr_unuse = 0;\r\nstruct page *page;\r\n#ifdef FRONTSWAP_HAS_UNUSE\r\nint unuse_ret;\r\n#endif\r\nif (nr <= 0)\r\ngoto skip_evict;\r\nif (in_progress)\r\ngoto skip_evict;\r\nin_progress = true;\r\nzcache_last_active_file_pageframes =\r\nglobal_page_state(NR_LRU_BASE + LRU_ACTIVE_FILE);\r\nzcache_last_inactive_file_pageframes =\r\nglobal_page_state(NR_LRU_BASE + LRU_INACTIVE_FILE);\r\nnr_evict = zcache_eph_pageframes - zcache_last_active_file_pageframes +\r\nzcache_last_inactive_file_pageframes;\r\nwhile (nr_evict-- > 0) {\r\npage = zcache_evict_eph_pageframe();\r\nif (page == NULL)\r\nbreak;\r\nzcache_free_page(page);\r\n}\r\nzcache_last_active_anon_pageframes =\r\nglobal_page_state(NR_LRU_BASE + LRU_ACTIVE_ANON);\r\nzcache_last_inactive_anon_pageframes =\r\nglobal_page_state(NR_LRU_BASE + LRU_INACTIVE_ANON);\r\nnr_unuse = zcache_pers_pageframes - zcache_last_active_anon_pageframes +\r\nzcache_last_inactive_anon_pageframes;\r\n#ifdef FRONTSWAP_HAS_UNUSE\r\nif (nr_unuse > 32)\r\nnr_unuse = 32;\r\nwhile (nr_unuse-- > 0) {\r\nunuse_ret = zcache_frontswap_unuse();\r\nif (unuse_ret == -ENOMEM)\r\nbreak;\r\n}\r\n#endif\r\nin_progress = false;\r\nskip_evict:\r\nzcache_last_active_file_pageframes =\r\nglobal_page_state(NR_LRU_BASE + LRU_ACTIVE_FILE);\r\nzcache_last_inactive_file_pageframes =\r\nglobal_page_state(NR_LRU_BASE + LRU_INACTIVE_FILE);\r\nret = zcache_eph_pageframes - zcache_last_active_file_pageframes +\r\nzcache_last_inactive_file_pageframes;\r\nif (ret < 0)\r\nret = 0;\r\nreturn ret;\r\n}\r\nint zcache_put_page(int cli_id, int pool_id, struct tmem_oid *oidp,\r\nuint32_t index, void *page,\r\nunsigned int size, bool raw, int ephemeral)\r\n{\r\nstruct tmem_pool *pool;\r\nstruct tmem_handle th;\r\nint ret = -1;\r\nvoid *pampd = NULL;\r\nBUG_ON(!irqs_disabled());\r\npool = zcache_get_pool_by_id(cli_id, pool_id);\r\nif (unlikely(pool == NULL))\r\ngoto out;\r\nif (!zcache_freeze) {\r\nret = 0;\r\nth.client_id = cli_id;\r\nth.pool_id = pool_id;\r\nth.oid = *oidp;\r\nth.index = index;\r\npampd = zcache_pampd_create((char *)page, size, raw,\r\nephemeral, &th);\r\nif (pampd == NULL) {\r\nret = -ENOMEM;\r\nif (ephemeral)\r\nzcache_failed_eph_puts++;\r\nelse\r\nzcache_failed_pers_puts++;\r\n} else {\r\nif (ramster_enabled)\r\nramster_do_preload_flnode(pool);\r\nret = tmem_put(pool, oidp, index, 0, pampd);\r\nif (ret < 0)\r\nBUG();\r\n}\r\nzcache_put_pool(pool);\r\n} else {\r\nzcache_put_to_flush++;\r\nif (ramster_enabled)\r\nramster_do_preload_flnode(pool);\r\nif (atomic_read(&pool->obj_count) > 0)\r\n(void)tmem_flush_page(pool, oidp, index);\r\nzcache_put_pool(pool);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nint zcache_get_page(int cli_id, int pool_id, struct tmem_oid *oidp,\r\nuint32_t index, void *page,\r\nsize_t *sizep, bool raw, int get_and_free)\r\n{\r\nstruct tmem_pool *pool;\r\nint ret = -1;\r\nbool eph;\r\nif (!raw) {\r\nBUG_ON(irqs_disabled());\r\nBUG_ON(in_softirq());\r\n}\r\npool = zcache_get_pool_by_id(cli_id, pool_id);\r\neph = is_ephemeral(pool);\r\nif (likely(pool != NULL)) {\r\nif (atomic_read(&pool->obj_count) > 0)\r\nret = tmem_get(pool, oidp, index, (char *)(page),\r\nsizep, raw, get_and_free);\r\nzcache_put_pool(pool);\r\n}\r\nWARN_ONCE((!is_ephemeral(pool) && (ret != 0)),\r\n"zcache_get fails on persistent pool, "\r\n"bad things are very likely to happen soon\n");\r\n#ifdef RAMSTER_TESTING\r\nif (ret != 0 && ret != -1 && !(ret == -EINVAL && is_ephemeral(pool)))\r\npr_err("TESTING zcache_get tmem_get returns ret=%d\n", ret);\r\n#endif\r\nreturn ret;\r\n}\r\nint zcache_flush_page(int cli_id, int pool_id,\r\nstruct tmem_oid *oidp, uint32_t index)\r\n{\r\nstruct tmem_pool *pool;\r\nint ret = -1;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nzcache_flush_total++;\r\npool = zcache_get_pool_by_id(cli_id, pool_id);\r\nif (ramster_enabled)\r\nramster_do_preload_flnode(pool);\r\nif (likely(pool != NULL)) {\r\nif (atomic_read(&pool->obj_count) > 0)\r\nret = tmem_flush_page(pool, oidp, index);\r\nzcache_put_pool(pool);\r\n}\r\nif (ret >= 0)\r\nzcache_flush_found++;\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nint zcache_flush_object(int cli_id, int pool_id,\r\nstruct tmem_oid *oidp)\r\n{\r\nstruct tmem_pool *pool;\r\nint ret = -1;\r\nunsigned long flags;\r\nlocal_irq_save(flags);\r\nzcache_flobj_total++;\r\npool = zcache_get_pool_by_id(cli_id, pool_id);\r\nif (ramster_enabled)\r\nramster_do_preload_flnode(pool);\r\nif (likely(pool != NULL)) {\r\nif (atomic_read(&pool->obj_count) > 0)\r\nret = tmem_flush_object(pool, oidp);\r\nzcache_put_pool(pool);\r\n}\r\nif (ret >= 0)\r\nzcache_flobj_found++;\r\nlocal_irq_restore(flags);\r\nreturn ret;\r\n}\r\nstatic int zcache_client_destroy_pool(int cli_id, int pool_id)\r\n{\r\nstruct tmem_pool *pool = NULL;\r\nstruct zcache_client *cli = NULL;\r\nint ret = -1;\r\nif (pool_id < 0)\r\ngoto out;\r\nif (cli_id == LOCAL_CLIENT)\r\ncli = &zcache_host;\r\nelse if ((unsigned int)cli_id < MAX_CLIENTS)\r\ncli = &zcache_clients[cli_id];\r\nif (cli == NULL)\r\ngoto out;\r\natomic_inc(&cli->refcount);\r\npool = cli->tmem_pools[pool_id];\r\nif (pool == NULL)\r\ngoto out;\r\ncli->tmem_pools[pool_id] = NULL;\r\nwhile (atomic_read(&pool->refcount) != 0)\r\n;\r\natomic_dec(&cli->refcount);\r\nlocal_bh_disable();\r\nret = tmem_destroy_pool(pool);\r\nlocal_bh_enable();\r\nkfree(pool);\r\nif (cli_id == LOCAL_CLIENT)\r\npr_info("%s: destroyed local pool id=%d\n", namestr, pool_id);\r\nelse\r\npr_info("%s: destroyed pool id=%d, client=%d\n",\r\nnamestr, pool_id, cli_id);\r\nout:\r\nreturn ret;\r\n}\r\nint zcache_new_pool(uint16_t cli_id, uint32_t flags)\r\n{\r\nint poolid = -1;\r\nstruct tmem_pool *pool;\r\nstruct zcache_client *cli = NULL;\r\nif (cli_id == LOCAL_CLIENT)\r\ncli = &zcache_host;\r\nelse if ((unsigned int)cli_id < MAX_CLIENTS)\r\ncli = &zcache_clients[cli_id];\r\nif (cli == NULL)\r\ngoto out;\r\natomic_inc(&cli->refcount);\r\npool = kmalloc(sizeof(struct tmem_pool), GFP_ATOMIC);\r\nif (pool == NULL) {\r\npr_info("%s: pool creation failed: out of memory\n", namestr);\r\ngoto out;\r\n}\r\nfor (poolid = 0; poolid < MAX_POOLS_PER_CLIENT; poolid++)\r\nif (cli->tmem_pools[poolid] == NULL)\r\nbreak;\r\nif (poolid >= MAX_POOLS_PER_CLIENT) {\r\npr_info("%s: pool creation failed: max exceeded\n", namestr);\r\nkfree(pool);\r\npoolid = -1;\r\ngoto out;\r\n}\r\natomic_set(&pool->refcount, 0);\r\npool->client = cli;\r\npool->pool_id = poolid;\r\ntmem_new_pool(pool, flags);\r\ncli->tmem_pools[poolid] = pool;\r\nif (cli_id == LOCAL_CLIENT)\r\npr_info("%s: created %s local tmem pool, id=%d\n", namestr,\r\nflags & TMEM_POOL_PERSIST ? "persistent" : "ephemeral",\r\npoolid);\r\nelse\r\npr_info("%s: created %s tmem pool, id=%d, client=%d\n", namestr,\r\nflags & TMEM_POOL_PERSIST ? "persistent" : "ephemeral",\r\npoolid, cli_id);\r\nout:\r\nif (cli != NULL)\r\natomic_dec(&cli->refcount);\r\nreturn poolid;\r\n}\r\nstatic int zcache_local_new_pool(uint32_t flags)\r\n{\r\nreturn zcache_new_pool(LOCAL_CLIENT, flags);\r\n}\r\nint zcache_autocreate_pool(unsigned int cli_id, unsigned int pool_id, bool eph)\r\n{\r\nstruct tmem_pool *pool;\r\nstruct zcache_client *cli;\r\nuint32_t flags = eph ? 0 : TMEM_POOL_PERSIST;\r\nint ret = -1;\r\nBUG_ON(!ramster_enabled);\r\nif (cli_id == LOCAL_CLIENT)\r\ngoto out;\r\nif (pool_id >= MAX_POOLS_PER_CLIENT)\r\ngoto out;\r\nif (cli_id >= MAX_CLIENTS)\r\ngoto out;\r\ncli = &zcache_clients[cli_id];\r\nif ((eph && disable_cleancache) || (!eph && disable_frontswap)) {\r\npr_err("zcache_autocreate_pool: pool type disabled\n");\r\ngoto out;\r\n}\r\nif (!cli->allocated) {\r\nif (zcache_new_client(cli_id)) {\r\npr_err("zcache_autocreate_pool: can't create client\n");\r\ngoto out;\r\n}\r\ncli = &zcache_clients[cli_id];\r\n}\r\natomic_inc(&cli->refcount);\r\npool = cli->tmem_pools[pool_id];\r\nif (pool != NULL) {\r\nif (pool->persistent && eph) {\r\npr_err("zcache_autocreate_pool: type mismatch\n");\r\ngoto out;\r\n}\r\nret = 0;\r\ngoto out;\r\n}\r\npool = kmalloc(sizeof(struct tmem_pool), GFP_KERNEL);\r\nif (pool == NULL) {\r\npr_info("%s: pool creation failed: out of memory\n", namestr);\r\ngoto out;\r\n}\r\natomic_set(&pool->refcount, 0);\r\npool->client = cli;\r\npool->pool_id = pool_id;\r\ntmem_new_pool(pool, flags);\r\ncli->tmem_pools[pool_id] = pool;\r\npr_info("%s: AUTOcreated %s tmem poolid=%d, for remote client=%d\n",\r\nnamestr, flags & TMEM_POOL_PERSIST ? "persistent" : "ephemeral",\r\npool_id, cli_id);\r\nret = 0;\r\nout:\r\nif (cli != NULL)\r\natomic_dec(&cli->refcount);\r\nreturn ret;\r\n}\r\nstatic void zcache_cleancache_put_page(int pool_id,\r\nstruct cleancache_filekey key,\r\npgoff_t index, struct page *page)\r\n{\r\nu32 ind = (u32) index;\r\nstruct tmem_oid oid = *(struct tmem_oid *)&key;\r\nif (!disable_cleancache_ignore_nonactive && !PageWasActive(page)) {\r\nzcache_eph_nonactive_puts_ignored++;\r\nreturn;\r\n}\r\nif (likely(ind == index))\r\n(void)zcache_put_page(LOCAL_CLIENT, pool_id, &oid, index,\r\npage, PAGE_SIZE, false, 1);\r\n}\r\nstatic int zcache_cleancache_get_page(int pool_id,\r\nstruct cleancache_filekey key,\r\npgoff_t index, struct page *page)\r\n{\r\nu32 ind = (u32) index;\r\nstruct tmem_oid oid = *(struct tmem_oid *)&key;\r\nsize_t size;\r\nint ret = -1;\r\nif (likely(ind == index)) {\r\nret = zcache_get_page(LOCAL_CLIENT, pool_id, &oid, index,\r\npage, &size, false, 0);\r\nBUG_ON(ret >= 0 && size != PAGE_SIZE);\r\nif (ret == 0)\r\nSetPageWasActive(page);\r\n}\r\nreturn ret;\r\n}\r\nstatic void zcache_cleancache_flush_page(int pool_id,\r\nstruct cleancache_filekey key,\r\npgoff_t index)\r\n{\r\nu32 ind = (u32) index;\r\nstruct tmem_oid oid = *(struct tmem_oid *)&key;\r\nif (likely(ind == index))\r\n(void)zcache_flush_page(LOCAL_CLIENT, pool_id, &oid, ind);\r\n}\r\nstatic void zcache_cleancache_flush_inode(int pool_id,\r\nstruct cleancache_filekey key)\r\n{\r\nstruct tmem_oid oid = *(struct tmem_oid *)&key;\r\n(void)zcache_flush_object(LOCAL_CLIENT, pool_id, &oid);\r\n}\r\nstatic void zcache_cleancache_flush_fs(int pool_id)\r\n{\r\nif (pool_id >= 0)\r\n(void)zcache_client_destroy_pool(LOCAL_CLIENT, pool_id);\r\n}\r\nstatic int zcache_cleancache_init_fs(size_t pagesize)\r\n{\r\nBUG_ON(sizeof(struct cleancache_filekey) !=\r\nsizeof(struct tmem_oid));\r\nBUG_ON(pagesize != PAGE_SIZE);\r\nreturn zcache_local_new_pool(0);\r\n}\r\nstatic int zcache_cleancache_init_shared_fs(char *uuid, size_t pagesize)\r\n{\r\nBUG_ON(sizeof(struct cleancache_filekey) !=\r\nsizeof(struct tmem_oid));\r\nBUG_ON(pagesize != PAGE_SIZE);\r\nreturn zcache_local_new_pool(0);\r\n}\r\nstruct cleancache_ops zcache_cleancache_register_ops(void)\r\n{\r\nstruct cleancache_ops old_ops =\r\ncleancache_register_ops(&zcache_cleancache_ops);\r\nreturn old_ops;\r\n}\r\nstatic inline struct tmem_oid oswiz(unsigned type, u32 ind)\r\n{\r\nstruct tmem_oid oid = { .oid = { 0 } };\r\noid.oid[0] = _oswiz(type, ind);\r\nreturn oid;\r\n}\r\nstatic void unswiz(struct tmem_oid oid, u32 index,\r\nunsigned *type, pgoff_t *offset)\r\n{\r\n*type = (unsigned)(oid.oid[0] >> SWIZ_BITS);\r\n*offset = (pgoff_t)((index << SWIZ_BITS) |\r\n(oid.oid[0] & SWIZ_MASK));\r\n}\r\nstatic int zcache_frontswap_put_page(unsigned type, pgoff_t offset,\r\nstruct page *page)\r\n{\r\nu64 ind64 = (u64)offset;\r\nu32 ind = (u32)offset;\r\nstruct tmem_oid oid = oswiz(type, ind);\r\nint ret = -1;\r\nunsigned long flags;\r\nBUG_ON(!PageLocked(page));\r\nif (!disable_frontswap_ignore_nonactive && !PageWasActive(page)) {\r\nzcache_pers_nonactive_puts_ignored++;\r\nret = -ERANGE;\r\ngoto out;\r\n}\r\nif (likely(ind64 == ind)) {\r\nlocal_irq_save(flags);\r\nret = zcache_put_page(LOCAL_CLIENT, zcache_frontswap_poolid,\r\n&oid, iswiz(ind),\r\npage, PAGE_SIZE, false, 0);\r\nlocal_irq_restore(flags);\r\n}\r\nout:\r\nreturn ret;\r\n}\r\nstatic int zcache_frontswap_get_page(unsigned type, pgoff_t offset,\r\nstruct page *page)\r\n{\r\nu64 ind64 = (u64)offset;\r\nu32 ind = (u32)offset;\r\nstruct tmem_oid oid = oswiz(type, ind);\r\nsize_t size;\r\nint ret = -1, get_and_free;\r\nif (frontswap_has_exclusive_gets)\r\nget_and_free = 1;\r\nelse\r\nget_and_free = -1;\r\nBUG_ON(!PageLocked(page));\r\nif (likely(ind64 == ind)) {\r\nret = zcache_get_page(LOCAL_CLIENT, zcache_frontswap_poolid,\r\n&oid, iswiz(ind),\r\npage, &size, false, get_and_free);\r\nBUG_ON(ret >= 0 && size != PAGE_SIZE);\r\n}\r\nreturn ret;\r\n}\r\nstatic void zcache_frontswap_flush_page(unsigned type, pgoff_t offset)\r\n{\r\nu64 ind64 = (u64)offset;\r\nu32 ind = (u32)offset;\r\nstruct tmem_oid oid = oswiz(type, ind);\r\nif (likely(ind64 == ind))\r\n(void)zcache_flush_page(LOCAL_CLIENT, zcache_frontswap_poolid,\r\n&oid, iswiz(ind));\r\n}\r\nstatic void zcache_frontswap_flush_area(unsigned type)\r\n{\r\nstruct tmem_oid oid;\r\nint ind;\r\nfor (ind = SWIZ_MASK; ind >= 0; ind--) {\r\noid = oswiz(type, ind);\r\n(void)zcache_flush_object(LOCAL_CLIENT,\r\nzcache_frontswap_poolid, &oid);\r\n}\r\n}\r\nstatic void zcache_frontswap_init(unsigned ignored)\r\n{\r\nif (zcache_frontswap_poolid < 0)\r\nzcache_frontswap_poolid =\r\nzcache_local_new_pool(TMEM_POOL_PERSIST);\r\n}\r\nstruct frontswap_ops zcache_frontswap_register_ops(void)\r\n{\r\nstruct frontswap_ops old_ops =\r\nfrontswap_register_ops(&zcache_frontswap_ops);\r\nreturn old_ops;\r\n}\r\nstatic int __init enable_zcache(char *s)\r\n{\r\nzcache_enabled = 1;\r\nreturn 1;\r\n}\r\nstatic int __init enable_ramster(char *s)\r\n{\r\nzcache_enabled = 1;\r\n#ifdef CONFIG_RAMSTER\r\nramster_enabled = 1;\r\n#endif\r\nreturn 1;\r\n}\r\nstatic int __init no_cleancache(char *s)\r\n{\r\ndisable_cleancache = 1;\r\nreturn 1;\r\n}\r\nstatic int __init no_frontswap(char *s)\r\n{\r\ndisable_frontswap = 1;\r\nreturn 1;\r\n}\r\nstatic int __init no_frontswap_exclusive_gets(char *s)\r\n{\r\nfrontswap_has_exclusive_gets = false;\r\nreturn 1;\r\n}\r\nstatic int __init no_frontswap_ignore_nonactive(char *s)\r\n{\r\ndisable_frontswap_ignore_nonactive = 1;\r\nreturn 1;\r\n}\r\nstatic int __init no_cleancache_ignore_nonactive(char *s)\r\n{\r\ndisable_cleancache_ignore_nonactive = 1;\r\nreturn 1;\r\n}\r\nstatic int __init enable_zcache_compressor(char *s)\r\n{\r\nstrncpy(zcache_comp_name, s, ZCACHE_COMP_NAME_SZ);\r\nzcache_enabled = 1;\r\nreturn 1;\r\n}\r\nstatic int __init zcache_comp_init(void)\r\n{\r\nint ret = 0;\r\nif (*zcache_comp_name != '\0') {\r\nret = crypto_has_comp(zcache_comp_name, 0, 0);\r\nif (!ret)\r\npr_info("zcache: %s not supported\n",\r\nzcache_comp_name);\r\n}\r\nif (!ret)\r\nstrcpy(zcache_comp_name, "lzo");\r\nret = crypto_has_comp(zcache_comp_name, 0, 0);\r\nif (!ret) {\r\nret = 1;\r\ngoto out;\r\n}\r\npr_info("zcache: using %s compressor\n", zcache_comp_name);\r\nret = 0;\r\nzcache_comp_pcpu_tfms = alloc_percpu(struct crypto_comp *);\r\nif (!zcache_comp_pcpu_tfms)\r\nret = 1;\r\nout:\r\nreturn ret;\r\n}\r\nstatic int __init zcache_init(void)\r\n{\r\nint ret = 0;\r\nif (ramster_enabled) {\r\nnamestr = "ramster";\r\nramster_register_pamops(&zcache_pamops);\r\n}\r\n#ifdef CONFIG_DEBUG_FS\r\nzcache_debugfs_init();\r\n#endif\r\nif (zcache_enabled) {\r\nunsigned int cpu;\r\ntmem_register_hostops(&zcache_hostops);\r\ntmem_register_pamops(&zcache_pamops);\r\nret = register_cpu_notifier(&zcache_cpu_notifier_block);\r\nif (ret) {\r\npr_err("%s: can't register cpu notifier\n", namestr);\r\ngoto out;\r\n}\r\nret = zcache_comp_init();\r\nif (ret) {\r\npr_err("%s: compressor initialization failed\n",\r\nnamestr);\r\ngoto out;\r\n}\r\nfor_each_online_cpu(cpu) {\r\nvoid *pcpu = (void *)(long)cpu;\r\nzcache_cpu_notifier(&zcache_cpu_notifier_block,\r\nCPU_UP_PREPARE, pcpu);\r\n}\r\n}\r\nzcache_objnode_cache = kmem_cache_create("zcache_objnode",\r\nsizeof(struct tmem_objnode), 0, 0, NULL);\r\nzcache_obj_cache = kmem_cache_create("zcache_obj",\r\nsizeof(struct tmem_obj), 0, 0, NULL);\r\nret = zcache_new_client(LOCAL_CLIENT);\r\nif (ret) {\r\npr_err("%s: can't create client\n", namestr);\r\ngoto out;\r\n}\r\nzbud_init();\r\nif (zcache_enabled && !disable_cleancache) {\r\nstruct cleancache_ops old_ops;\r\nregister_shrinker(&zcache_shrinker);\r\nold_ops = zcache_cleancache_register_ops();\r\npr_info("%s: cleancache enabled using kernel transcendent "\r\n"memory and compression buddies\n", namestr);\r\n#ifdef ZCACHE_DEBUG\r\npr_info("%s: cleancache: ignorenonactive = %d\n",\r\nnamestr, !disable_cleancache_ignore_nonactive);\r\n#endif\r\nif (old_ops.init_fs != NULL)\r\npr_warn("%s: cleancache_ops overridden\n", namestr);\r\n}\r\nif (zcache_enabled && !disable_frontswap) {\r\nstruct frontswap_ops old_ops;\r\nold_ops = zcache_frontswap_register_ops();\r\nif (frontswap_has_exclusive_gets)\r\nfrontswap_tmem_exclusive_gets(true);\r\npr_info("%s: frontswap enabled using kernel transcendent "\r\n"memory and compression buddies\n", namestr);\r\n#ifdef ZCACHE_DEBUG\r\npr_info("%s: frontswap: excl gets = %d active only = %d\n",\r\nnamestr, frontswap_has_exclusive_gets,\r\n!disable_frontswap_ignore_nonactive);\r\n#endif\r\nif (old_ops.init != NULL)\r\npr_warn("%s: frontswap_ops overridden\n", namestr);\r\n}\r\nif (ramster_enabled)\r\nramster_init(!disable_cleancache, !disable_frontswap,\r\nfrontswap_has_exclusive_gets);\r\nout:\r\nreturn ret;\r\n}
