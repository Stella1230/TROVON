static inline int dm_bufio_cache_index(struct dm_bufio_client *c)\r\n{\r\nunsigned ret = c->blocks_per_page_bits - 1;\r\nBUG_ON(ret >= ARRAY_SIZE(dm_bufio_caches));\r\nreturn ret;\r\n}\r\nstatic void dm_bufio_lock(struct dm_bufio_client *c)\r\n{\r\nmutex_lock_nested(&c->lock, dm_bufio_in_request());\r\n}\r\nstatic int dm_bufio_trylock(struct dm_bufio_client *c)\r\n{\r\nreturn mutex_trylock(&c->lock);\r\n}\r\nstatic void dm_bufio_unlock(struct dm_bufio_client *c)\r\n{\r\nmutex_unlock(&c->lock);\r\n}\r\nstatic void adjust_total_allocated(enum data_mode data_mode, long diff)\r\n{\r\nstatic unsigned long * const class_ptr[DATA_MODE_LIMIT] = {\r\n&dm_bufio_allocated_kmem_cache,\r\n&dm_bufio_allocated_get_free_pages,\r\n&dm_bufio_allocated_vmalloc,\r\n};\r\nspin_lock(&param_spinlock);\r\n*class_ptr[data_mode] += diff;\r\ndm_bufio_current_allocated += diff;\r\nif (dm_bufio_current_allocated > dm_bufio_peak_allocated)\r\ndm_bufio_peak_allocated = dm_bufio_current_allocated;\r\nspin_unlock(&param_spinlock);\r\n}\r\nstatic void __cache_size_refresh(void)\r\n{\r\nBUG_ON(!mutex_is_locked(&dm_bufio_clients_lock));\r\nBUG_ON(dm_bufio_client_count < 0);\r\ndm_bufio_cache_size_latch = ACCESS_ONCE(dm_bufio_cache_size);\r\nif (!dm_bufio_cache_size_latch) {\r\n(void)cmpxchg(&dm_bufio_cache_size, 0,\r\ndm_bufio_default_cache_size);\r\ndm_bufio_cache_size_latch = dm_bufio_default_cache_size;\r\n}\r\ndm_bufio_cache_size_per_client = dm_bufio_cache_size_latch /\r\n(dm_bufio_client_count ? : 1);\r\n}\r\nstatic void *alloc_buffer_data(struct dm_bufio_client *c, gfp_t gfp_mask,\r\nenum data_mode *data_mode)\r\n{\r\nif (c->block_size <= DM_BUFIO_BLOCK_SIZE_SLAB_LIMIT) {\r\n*data_mode = DATA_MODE_SLAB;\r\nreturn kmem_cache_alloc(DM_BUFIO_CACHE(c), gfp_mask);\r\n}\r\nif (c->block_size <= DM_BUFIO_BLOCK_SIZE_GFP_LIMIT &&\r\ngfp_mask & __GFP_NORETRY) {\r\n*data_mode = DATA_MODE_GET_FREE_PAGES;\r\nreturn (void *)__get_free_pages(gfp_mask,\r\nc->pages_per_block_bits);\r\n}\r\n*data_mode = DATA_MODE_VMALLOC;\r\nreturn __vmalloc(c->block_size, gfp_mask, PAGE_KERNEL);\r\n}\r\nstatic void free_buffer_data(struct dm_bufio_client *c,\r\nvoid *data, enum data_mode data_mode)\r\n{\r\nswitch (data_mode) {\r\ncase DATA_MODE_SLAB:\r\nkmem_cache_free(DM_BUFIO_CACHE(c), data);\r\nbreak;\r\ncase DATA_MODE_GET_FREE_PAGES:\r\nfree_pages((unsigned long)data, c->pages_per_block_bits);\r\nbreak;\r\ncase DATA_MODE_VMALLOC:\r\nvfree(data);\r\nbreak;\r\ndefault:\r\nDMCRIT("dm_bufio_free_buffer_data: bad data mode: %d",\r\ndata_mode);\r\nBUG();\r\n}\r\n}\r\nstatic struct dm_buffer *alloc_buffer(struct dm_bufio_client *c, gfp_t gfp_mask)\r\n{\r\nstruct dm_buffer *b = kmalloc(sizeof(struct dm_buffer) + c->aux_size,\r\ngfp_mask);\r\nif (!b)\r\nreturn NULL;\r\nb->c = c;\r\nb->data = alloc_buffer_data(c, gfp_mask, &b->data_mode);\r\nif (!b->data) {\r\nkfree(b);\r\nreturn NULL;\r\n}\r\nadjust_total_allocated(b->data_mode, (long)c->block_size);\r\nreturn b;\r\n}\r\nstatic void free_buffer(struct dm_buffer *b)\r\n{\r\nstruct dm_bufio_client *c = b->c;\r\nadjust_total_allocated(b->data_mode, -(long)c->block_size);\r\nfree_buffer_data(c, b->data, b->data_mode);\r\nkfree(b);\r\n}\r\nstatic void __link_buffer(struct dm_buffer *b, sector_t block, int dirty)\r\n{\r\nstruct dm_bufio_client *c = b->c;\r\nc->n_buffers[dirty]++;\r\nb->block = block;\r\nb->list_mode = dirty;\r\nlist_add(&b->lru_list, &c->lru[dirty]);\r\nhlist_add_head(&b->hash_list, &c->cache_hash[DM_BUFIO_HASH(block)]);\r\nb->last_accessed = jiffies;\r\n}\r\nstatic void __unlink_buffer(struct dm_buffer *b)\r\n{\r\nstruct dm_bufio_client *c = b->c;\r\nBUG_ON(!c->n_buffers[b->list_mode]);\r\nc->n_buffers[b->list_mode]--;\r\nhlist_del(&b->hash_list);\r\nlist_del(&b->lru_list);\r\n}\r\nstatic void __relink_lru(struct dm_buffer *b, int dirty)\r\n{\r\nstruct dm_bufio_client *c = b->c;\r\nBUG_ON(!c->n_buffers[b->list_mode]);\r\nc->n_buffers[b->list_mode]--;\r\nc->n_buffers[dirty]++;\r\nb->list_mode = dirty;\r\nlist_move(&b->lru_list, &c->lru[dirty]);\r\n}\r\nstatic void dmio_complete(unsigned long error, void *context)\r\n{\r\nstruct dm_buffer *b = context;\r\nb->bio.bi_end_io(&b->bio, error ? -EIO : 0);\r\n}\r\nstatic void use_dmio(struct dm_buffer *b, int rw, sector_t block,\r\nbio_end_io_t *end_io)\r\n{\r\nint r;\r\nstruct dm_io_request io_req = {\r\n.bi_rw = rw,\r\n.notify.fn = dmio_complete,\r\n.notify.context = b,\r\n.client = b->c->dm_io,\r\n};\r\nstruct dm_io_region region = {\r\n.bdev = b->c->bdev,\r\n.sector = block << b->c->sectors_per_block_bits,\r\n.count = b->c->block_size >> SECTOR_SHIFT,\r\n};\r\nif (b->data_mode != DATA_MODE_VMALLOC) {\r\nio_req.mem.type = DM_IO_KMEM;\r\nio_req.mem.ptr.addr = b->data;\r\n} else {\r\nio_req.mem.type = DM_IO_VMA;\r\nio_req.mem.ptr.vma = b->data;\r\n}\r\nb->bio.bi_end_io = end_io;\r\nr = dm_io(&io_req, 1, &region, NULL);\r\nif (r)\r\nend_io(&b->bio, r);\r\n}\r\nstatic void use_inline_bio(struct dm_buffer *b, int rw, sector_t block,\r\nbio_end_io_t *end_io)\r\n{\r\nchar *ptr;\r\nint len;\r\nbio_init(&b->bio);\r\nb->bio.bi_io_vec = b->bio_vec;\r\nb->bio.bi_max_vecs = DM_BUFIO_INLINE_VECS;\r\nb->bio.bi_sector = block << b->c->sectors_per_block_bits;\r\nb->bio.bi_bdev = b->c->bdev;\r\nb->bio.bi_end_io = end_io;\r\nptr = b->data;\r\nlen = b->c->block_size;\r\nif (len >= PAGE_SIZE)\r\nBUG_ON((unsigned long)ptr & (PAGE_SIZE - 1));\r\nelse\r\nBUG_ON((unsigned long)ptr & (len - 1));\r\ndo {\r\nif (!bio_add_page(&b->bio, virt_to_page(ptr),\r\nlen < PAGE_SIZE ? len : PAGE_SIZE,\r\nvirt_to_phys(ptr) & (PAGE_SIZE - 1))) {\r\nBUG_ON(b->c->block_size <= PAGE_SIZE);\r\nuse_dmio(b, rw, block, end_io);\r\nreturn;\r\n}\r\nlen -= PAGE_SIZE;\r\nptr += PAGE_SIZE;\r\n} while (len > 0);\r\nsubmit_bio(rw, &b->bio);\r\n}\r\nstatic void submit_io(struct dm_buffer *b, int rw, sector_t block,\r\nbio_end_io_t *end_io)\r\n{\r\nif (rw == WRITE && b->c->write_callback)\r\nb->c->write_callback(b);\r\nif (b->c->block_size <= DM_BUFIO_INLINE_VECS * PAGE_SIZE &&\r\nb->data_mode != DATA_MODE_VMALLOC)\r\nuse_inline_bio(b, rw, block, end_io);\r\nelse\r\nuse_dmio(b, rw, block, end_io);\r\n}\r\nstatic void write_endio(struct bio *bio, int error)\r\n{\r\nstruct dm_buffer *b = container_of(bio, struct dm_buffer, bio);\r\nb->write_error = error;\r\nif (unlikely(error)) {\r\nstruct dm_bufio_client *c = b->c;\r\n(void)cmpxchg(&c->async_write_error, 0, error);\r\n}\r\nBUG_ON(!test_bit(B_WRITING, &b->state));\r\nsmp_mb__before_clear_bit();\r\nclear_bit(B_WRITING, &b->state);\r\nsmp_mb__after_clear_bit();\r\nwake_up_bit(&b->state, B_WRITING);\r\n}\r\nstatic int do_io_schedule(void *word)\r\n{\r\nio_schedule();\r\nreturn 0;\r\n}\r\nstatic void __write_dirty_buffer(struct dm_buffer *b)\r\n{\r\nif (!test_bit(B_DIRTY, &b->state))\r\nreturn;\r\nclear_bit(B_DIRTY, &b->state);\r\nwait_on_bit_lock(&b->state, B_WRITING,\r\ndo_io_schedule, TASK_UNINTERRUPTIBLE);\r\nsubmit_io(b, WRITE, b->block, write_endio);\r\n}\r\nstatic void __make_buffer_clean(struct dm_buffer *b)\r\n{\r\nBUG_ON(b->hold_count);\r\nif (!b->state)\r\nreturn;\r\nwait_on_bit(&b->state, B_READING, do_io_schedule, TASK_UNINTERRUPTIBLE);\r\n__write_dirty_buffer(b);\r\nwait_on_bit(&b->state, B_WRITING, do_io_schedule, TASK_UNINTERRUPTIBLE);\r\n}\r\nstatic struct dm_buffer *__get_unclaimed_buffer(struct dm_bufio_client *c)\r\n{\r\nstruct dm_buffer *b;\r\nlist_for_each_entry_reverse(b, &c->lru[LIST_CLEAN], lru_list) {\r\nBUG_ON(test_bit(B_WRITING, &b->state));\r\nBUG_ON(test_bit(B_DIRTY, &b->state));\r\nif (!b->hold_count) {\r\n__make_buffer_clean(b);\r\n__unlink_buffer(b);\r\nreturn b;\r\n}\r\ndm_bufio_cond_resched();\r\n}\r\nlist_for_each_entry_reverse(b, &c->lru[LIST_DIRTY], lru_list) {\r\nBUG_ON(test_bit(B_READING, &b->state));\r\nif (!b->hold_count) {\r\n__make_buffer_clean(b);\r\n__unlink_buffer(b);\r\nreturn b;\r\n}\r\ndm_bufio_cond_resched();\r\n}\r\nreturn NULL;\r\n}\r\nstatic void __wait_for_free_buffer(struct dm_bufio_client *c)\r\n{\r\nDECLARE_WAITQUEUE(wait, current);\r\nadd_wait_queue(&c->free_buffer_wait, &wait);\r\nset_task_state(current, TASK_UNINTERRUPTIBLE);\r\ndm_bufio_unlock(c);\r\nio_schedule();\r\nset_task_state(current, TASK_RUNNING);\r\nremove_wait_queue(&c->free_buffer_wait, &wait);\r\ndm_bufio_lock(c);\r\n}\r\nstatic struct dm_buffer *__alloc_buffer_wait_no_callback(struct dm_bufio_client *c, enum new_flag nf)\r\n{\r\nstruct dm_buffer *b;\r\nwhile (1) {\r\nif (dm_bufio_cache_size_latch != 1) {\r\nb = alloc_buffer(c, GFP_NOIO | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);\r\nif (b)\r\nreturn b;\r\n}\r\nif (nf == NF_PREFETCH)\r\nreturn NULL;\r\nif (!list_empty(&c->reserved_buffers)) {\r\nb = list_entry(c->reserved_buffers.next,\r\nstruct dm_buffer, lru_list);\r\nlist_del(&b->lru_list);\r\nc->need_reserved_buffers++;\r\nreturn b;\r\n}\r\nb = __get_unclaimed_buffer(c);\r\nif (b)\r\nreturn b;\r\n__wait_for_free_buffer(c);\r\n}\r\n}\r\nstatic struct dm_buffer *__alloc_buffer_wait(struct dm_bufio_client *c, enum new_flag nf)\r\n{\r\nstruct dm_buffer *b = __alloc_buffer_wait_no_callback(c, nf);\r\nif (!b)\r\nreturn NULL;\r\nif (c->alloc_callback)\r\nc->alloc_callback(b);\r\nreturn b;\r\n}\r\nstatic void __free_buffer_wake(struct dm_buffer *b)\r\n{\r\nstruct dm_bufio_client *c = b->c;\r\nif (!c->need_reserved_buffers)\r\nfree_buffer(b);\r\nelse {\r\nlist_add(&b->lru_list, &c->reserved_buffers);\r\nc->need_reserved_buffers--;\r\n}\r\nwake_up(&c->free_buffer_wait);\r\n}\r\nstatic void __write_dirty_buffers_async(struct dm_bufio_client *c, int no_wait)\r\n{\r\nstruct dm_buffer *b, *tmp;\r\nlist_for_each_entry_safe_reverse(b, tmp, &c->lru[LIST_DIRTY], lru_list) {\r\nBUG_ON(test_bit(B_READING, &b->state));\r\nif (!test_bit(B_DIRTY, &b->state) &&\r\n!test_bit(B_WRITING, &b->state)) {\r\n__relink_lru(b, LIST_CLEAN);\r\ncontinue;\r\n}\r\nif (no_wait && test_bit(B_WRITING, &b->state))\r\nreturn;\r\n__write_dirty_buffer(b);\r\ndm_bufio_cond_resched();\r\n}\r\n}\r\nstatic void __get_memory_limit(struct dm_bufio_client *c,\r\nunsigned long *threshold_buffers,\r\nunsigned long *limit_buffers)\r\n{\r\nunsigned long buffers;\r\nif (ACCESS_ONCE(dm_bufio_cache_size) != dm_bufio_cache_size_latch) {\r\nmutex_lock(&dm_bufio_clients_lock);\r\n__cache_size_refresh();\r\nmutex_unlock(&dm_bufio_clients_lock);\r\n}\r\nbuffers = dm_bufio_cache_size_per_client >>\r\n(c->sectors_per_block_bits + SECTOR_SHIFT);\r\nif (buffers < DM_BUFIO_MIN_BUFFERS)\r\nbuffers = DM_BUFIO_MIN_BUFFERS;\r\n*limit_buffers = buffers;\r\n*threshold_buffers = buffers * DM_BUFIO_WRITEBACK_PERCENT / 100;\r\n}\r\nstatic void __check_watermark(struct dm_bufio_client *c)\r\n{\r\nunsigned long threshold_buffers, limit_buffers;\r\n__get_memory_limit(c, &threshold_buffers, &limit_buffers);\r\nwhile (c->n_buffers[LIST_CLEAN] + c->n_buffers[LIST_DIRTY] >\r\nlimit_buffers) {\r\nstruct dm_buffer *b = __get_unclaimed_buffer(c);\r\nif (!b)\r\nreturn;\r\n__free_buffer_wake(b);\r\ndm_bufio_cond_resched();\r\n}\r\nif (c->n_buffers[LIST_DIRTY] > threshold_buffers)\r\n__write_dirty_buffers_async(c, 1);\r\n}\r\nstatic struct dm_buffer *__find(struct dm_bufio_client *c, sector_t block)\r\n{\r\nstruct dm_buffer *b;\r\nstruct hlist_node *hn;\r\nhlist_for_each_entry(b, hn, &c->cache_hash[DM_BUFIO_HASH(block)],\r\nhash_list) {\r\ndm_bufio_cond_resched();\r\nif (b->block == block)\r\nreturn b;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct dm_buffer *__bufio_new(struct dm_bufio_client *c, sector_t block,\r\nenum new_flag nf, int *need_submit)\r\n{\r\nstruct dm_buffer *b, *new_b = NULL;\r\n*need_submit = 0;\r\nb = __find(c, block);\r\nif (b)\r\ngoto found_buffer;\r\nif (nf == NF_GET)\r\nreturn NULL;\r\nnew_b = __alloc_buffer_wait(c, nf);\r\nif (!new_b)\r\nreturn NULL;\r\nb = __find(c, block);\r\nif (b) {\r\n__free_buffer_wake(new_b);\r\ngoto found_buffer;\r\n}\r\n__check_watermark(c);\r\nb = new_b;\r\nb->hold_count = 1;\r\nb->read_error = 0;\r\nb->write_error = 0;\r\n__link_buffer(b, block, LIST_CLEAN);\r\nif (nf == NF_FRESH) {\r\nb->state = 0;\r\nreturn b;\r\n}\r\nb->state = 1 << B_READING;\r\n*need_submit = 1;\r\nreturn b;\r\nfound_buffer:\r\nif (nf == NF_PREFETCH)\r\nreturn NULL;\r\nif (nf == NF_GET && unlikely(test_bit(B_READING, &b->state)))\r\nreturn NULL;\r\nb->hold_count++;\r\n__relink_lru(b, test_bit(B_DIRTY, &b->state) ||\r\ntest_bit(B_WRITING, &b->state));\r\nreturn b;\r\n}\r\nstatic void read_endio(struct bio *bio, int error)\r\n{\r\nstruct dm_buffer *b = container_of(bio, struct dm_buffer, bio);\r\nb->read_error = error;\r\nBUG_ON(!test_bit(B_READING, &b->state));\r\nsmp_mb__before_clear_bit();\r\nclear_bit(B_READING, &b->state);\r\nsmp_mb__after_clear_bit();\r\nwake_up_bit(&b->state, B_READING);\r\n}\r\nstatic void *new_read(struct dm_bufio_client *c, sector_t block,\r\nenum new_flag nf, struct dm_buffer **bp)\r\n{\r\nint need_submit;\r\nstruct dm_buffer *b;\r\ndm_bufio_lock(c);\r\nb = __bufio_new(c, block, nf, &need_submit);\r\ndm_bufio_unlock(c);\r\nif (!b)\r\nreturn b;\r\nif (need_submit)\r\nsubmit_io(b, READ, b->block, read_endio);\r\nwait_on_bit(&b->state, B_READING, do_io_schedule, TASK_UNINTERRUPTIBLE);\r\nif (b->read_error) {\r\nint error = b->read_error;\r\ndm_bufio_release(b);\r\nreturn ERR_PTR(error);\r\n}\r\n*bp = b;\r\nreturn b->data;\r\n}\r\nvoid *dm_bufio_get(struct dm_bufio_client *c, sector_t block,\r\nstruct dm_buffer **bp)\r\n{\r\nreturn new_read(c, block, NF_GET, bp);\r\n}\r\nvoid *dm_bufio_read(struct dm_bufio_client *c, sector_t block,\r\nstruct dm_buffer **bp)\r\n{\r\nBUG_ON(dm_bufio_in_request());\r\nreturn new_read(c, block, NF_READ, bp);\r\n}\r\nvoid *dm_bufio_new(struct dm_bufio_client *c, sector_t block,\r\nstruct dm_buffer **bp)\r\n{\r\nBUG_ON(dm_bufio_in_request());\r\nreturn new_read(c, block, NF_FRESH, bp);\r\n}\r\nvoid dm_bufio_prefetch(struct dm_bufio_client *c,\r\nsector_t block, unsigned n_blocks)\r\n{\r\nstruct blk_plug plug;\r\nblk_start_plug(&plug);\r\ndm_bufio_lock(c);\r\nfor (; n_blocks--; block++) {\r\nint need_submit;\r\nstruct dm_buffer *b;\r\nb = __bufio_new(c, block, NF_PREFETCH, &need_submit);\r\nif (unlikely(b != NULL)) {\r\ndm_bufio_unlock(c);\r\nif (need_submit)\r\nsubmit_io(b, READ, b->block, read_endio);\r\ndm_bufio_release(b);\r\ndm_bufio_cond_resched();\r\nif (!n_blocks)\r\ngoto flush_plug;\r\ndm_bufio_lock(c);\r\n}\r\n}\r\ndm_bufio_unlock(c);\r\nflush_plug:\r\nblk_finish_plug(&plug);\r\n}\r\nvoid dm_bufio_release(struct dm_buffer *b)\r\n{\r\nstruct dm_bufio_client *c = b->c;\r\ndm_bufio_lock(c);\r\nBUG_ON(!b->hold_count);\r\nb->hold_count--;\r\nif (!b->hold_count) {\r\nwake_up(&c->free_buffer_wait);\r\nif ((b->read_error || b->write_error) &&\r\n!test_bit(B_READING, &b->state) &&\r\n!test_bit(B_WRITING, &b->state) &&\r\n!test_bit(B_DIRTY, &b->state)) {\r\n__unlink_buffer(b);\r\n__free_buffer_wake(b);\r\n}\r\n}\r\ndm_bufio_unlock(c);\r\n}\r\nvoid dm_bufio_mark_buffer_dirty(struct dm_buffer *b)\r\n{\r\nstruct dm_bufio_client *c = b->c;\r\ndm_bufio_lock(c);\r\nBUG_ON(test_bit(B_READING, &b->state));\r\nif (!test_and_set_bit(B_DIRTY, &b->state))\r\n__relink_lru(b, LIST_DIRTY);\r\ndm_bufio_unlock(c);\r\n}\r\nvoid dm_bufio_write_dirty_buffers_async(struct dm_bufio_client *c)\r\n{\r\nBUG_ON(dm_bufio_in_request());\r\ndm_bufio_lock(c);\r\n__write_dirty_buffers_async(c, 0);\r\ndm_bufio_unlock(c);\r\n}\r\nint dm_bufio_write_dirty_buffers(struct dm_bufio_client *c)\r\n{\r\nint a, f;\r\nunsigned long buffers_processed = 0;\r\nstruct dm_buffer *b, *tmp;\r\ndm_bufio_lock(c);\r\n__write_dirty_buffers_async(c, 0);\r\nagain:\r\nlist_for_each_entry_safe_reverse(b, tmp, &c->lru[LIST_DIRTY], lru_list) {\r\nint dropped_lock = 0;\r\nif (buffers_processed < c->n_buffers[LIST_DIRTY])\r\nbuffers_processed++;\r\nBUG_ON(test_bit(B_READING, &b->state));\r\nif (test_bit(B_WRITING, &b->state)) {\r\nif (buffers_processed < c->n_buffers[LIST_DIRTY]) {\r\ndropped_lock = 1;\r\nb->hold_count++;\r\ndm_bufio_unlock(c);\r\nwait_on_bit(&b->state, B_WRITING,\r\ndo_io_schedule,\r\nTASK_UNINTERRUPTIBLE);\r\ndm_bufio_lock(c);\r\nb->hold_count--;\r\n} else\r\nwait_on_bit(&b->state, B_WRITING,\r\ndo_io_schedule,\r\nTASK_UNINTERRUPTIBLE);\r\n}\r\nif (!test_bit(B_DIRTY, &b->state) &&\r\n!test_bit(B_WRITING, &b->state))\r\n__relink_lru(b, LIST_CLEAN);\r\ndm_bufio_cond_resched();\r\nif (dropped_lock)\r\ngoto again;\r\n}\r\nwake_up(&c->free_buffer_wait);\r\ndm_bufio_unlock(c);\r\na = xchg(&c->async_write_error, 0);\r\nf = dm_bufio_issue_flush(c);\r\nif (a)\r\nreturn a;\r\nreturn f;\r\n}\r\nint dm_bufio_issue_flush(struct dm_bufio_client *c)\r\n{\r\nstruct dm_io_request io_req = {\r\n.bi_rw = REQ_FLUSH,\r\n.mem.type = DM_IO_KMEM,\r\n.mem.ptr.addr = NULL,\r\n.client = c->dm_io,\r\n};\r\nstruct dm_io_region io_reg = {\r\n.bdev = c->bdev,\r\n.sector = 0,\r\n.count = 0,\r\n};\r\nBUG_ON(dm_bufio_in_request());\r\nreturn dm_io(&io_req, 1, &io_reg, NULL);\r\n}\r\nvoid dm_bufio_release_move(struct dm_buffer *b, sector_t new_block)\r\n{\r\nstruct dm_bufio_client *c = b->c;\r\nstruct dm_buffer *new;\r\nBUG_ON(dm_bufio_in_request());\r\ndm_bufio_lock(c);\r\nretry:\r\nnew = __find(c, new_block);\r\nif (new) {\r\nif (new->hold_count) {\r\n__wait_for_free_buffer(c);\r\ngoto retry;\r\n}\r\n__make_buffer_clean(new);\r\n__unlink_buffer(new);\r\n__free_buffer_wake(new);\r\n}\r\nBUG_ON(!b->hold_count);\r\nBUG_ON(test_bit(B_READING, &b->state));\r\n__write_dirty_buffer(b);\r\nif (b->hold_count == 1) {\r\nwait_on_bit(&b->state, B_WRITING,\r\ndo_io_schedule, TASK_UNINTERRUPTIBLE);\r\nset_bit(B_DIRTY, &b->state);\r\n__unlink_buffer(b);\r\n__link_buffer(b, new_block, LIST_DIRTY);\r\n} else {\r\nsector_t old_block;\r\nwait_on_bit_lock(&b->state, B_WRITING,\r\ndo_io_schedule, TASK_UNINTERRUPTIBLE);\r\nold_block = b->block;\r\n__unlink_buffer(b);\r\n__link_buffer(b, new_block, b->list_mode);\r\nsubmit_io(b, WRITE, new_block, write_endio);\r\nwait_on_bit(&b->state, B_WRITING,\r\ndo_io_schedule, TASK_UNINTERRUPTIBLE);\r\n__unlink_buffer(b);\r\n__link_buffer(b, old_block, b->list_mode);\r\n}\r\ndm_bufio_unlock(c);\r\ndm_bufio_release(b);\r\n}\r\nunsigned dm_bufio_get_block_size(struct dm_bufio_client *c)\r\n{\r\nreturn c->block_size;\r\n}\r\nsector_t dm_bufio_get_device_size(struct dm_bufio_client *c)\r\n{\r\nreturn i_size_read(c->bdev->bd_inode) >>\r\n(SECTOR_SHIFT + c->sectors_per_block_bits);\r\n}\r\nsector_t dm_bufio_get_block_number(struct dm_buffer *b)\r\n{\r\nreturn b->block;\r\n}\r\nvoid *dm_bufio_get_block_data(struct dm_buffer *b)\r\n{\r\nreturn b->data;\r\n}\r\nvoid *dm_bufio_get_aux_data(struct dm_buffer *b)\r\n{\r\nreturn b + 1;\r\n}\r\nstruct dm_bufio_client *dm_bufio_get_client(struct dm_buffer *b)\r\n{\r\nreturn b->c;\r\n}\r\nstatic void drop_buffers(struct dm_bufio_client *c)\r\n{\r\nstruct dm_buffer *b;\r\nint i;\r\nBUG_ON(dm_bufio_in_request());\r\ndm_bufio_write_dirty_buffers_async(c);\r\ndm_bufio_lock(c);\r\nwhile ((b = __get_unclaimed_buffer(c)))\r\n__free_buffer_wake(b);\r\nfor (i = 0; i < LIST_SIZE; i++)\r\nlist_for_each_entry(b, &c->lru[i], lru_list)\r\nDMERR("leaked buffer %llx, hold count %u, list %d",\r\n(unsigned long long)b->block, b->hold_count, i);\r\nfor (i = 0; i < LIST_SIZE; i++)\r\nBUG_ON(!list_empty(&c->lru[i]));\r\ndm_bufio_unlock(c);\r\n}\r\nstatic int __cleanup_old_buffer(struct dm_buffer *b, gfp_t gfp,\r\nunsigned long max_jiffies)\r\n{\r\nif (jiffies - b->last_accessed < max_jiffies)\r\nreturn 1;\r\nif (!(gfp & __GFP_IO)) {\r\nif (test_bit(B_READING, &b->state) ||\r\ntest_bit(B_WRITING, &b->state) ||\r\ntest_bit(B_DIRTY, &b->state))\r\nreturn 1;\r\n}\r\nif (b->hold_count)\r\nreturn 1;\r\n__make_buffer_clean(b);\r\n__unlink_buffer(b);\r\n__free_buffer_wake(b);\r\nreturn 0;\r\n}\r\nstatic void __scan(struct dm_bufio_client *c, unsigned long nr_to_scan,\r\nstruct shrink_control *sc)\r\n{\r\nint l;\r\nstruct dm_buffer *b, *tmp;\r\nfor (l = 0; l < LIST_SIZE; l++) {\r\nlist_for_each_entry_safe_reverse(b, tmp, &c->lru[l], lru_list)\r\nif (!__cleanup_old_buffer(b, sc->gfp_mask, 0) &&\r\n!--nr_to_scan)\r\nreturn;\r\ndm_bufio_cond_resched();\r\n}\r\n}\r\nstatic int shrink(struct shrinker *shrinker, struct shrink_control *sc)\r\n{\r\nstruct dm_bufio_client *c =\r\ncontainer_of(shrinker, struct dm_bufio_client, shrinker);\r\nunsigned long r;\r\nunsigned long nr_to_scan = sc->nr_to_scan;\r\nif (sc->gfp_mask & __GFP_IO)\r\ndm_bufio_lock(c);\r\nelse if (!dm_bufio_trylock(c))\r\nreturn !nr_to_scan ? 0 : -1;\r\nif (nr_to_scan)\r\n__scan(c, nr_to_scan, sc);\r\nr = c->n_buffers[LIST_CLEAN] + c->n_buffers[LIST_DIRTY];\r\nif (r > INT_MAX)\r\nr = INT_MAX;\r\ndm_bufio_unlock(c);\r\nreturn r;\r\n}\r\nstruct dm_bufio_client *dm_bufio_client_create(struct block_device *bdev, unsigned block_size,\r\nunsigned reserved_buffers, unsigned aux_size,\r\nvoid (*alloc_callback)(struct dm_buffer *),\r\nvoid (*write_callback)(struct dm_buffer *))\r\n{\r\nint r;\r\nstruct dm_bufio_client *c;\r\nunsigned i;\r\nBUG_ON(block_size < 1 << SECTOR_SHIFT ||\r\n(block_size & (block_size - 1)));\r\nc = kmalloc(sizeof(*c), GFP_KERNEL);\r\nif (!c) {\r\nr = -ENOMEM;\r\ngoto bad_client;\r\n}\r\nc->cache_hash = vmalloc(sizeof(struct hlist_head) << DM_BUFIO_HASH_BITS);\r\nif (!c->cache_hash) {\r\nr = -ENOMEM;\r\ngoto bad_hash;\r\n}\r\nc->bdev = bdev;\r\nc->block_size = block_size;\r\nc->sectors_per_block_bits = ffs(block_size) - 1 - SECTOR_SHIFT;\r\nc->pages_per_block_bits = (ffs(block_size) - 1 >= PAGE_SHIFT) ?\r\nffs(block_size) - 1 - PAGE_SHIFT : 0;\r\nc->blocks_per_page_bits = (ffs(block_size) - 1 < PAGE_SHIFT ?\r\nPAGE_SHIFT - (ffs(block_size) - 1) : 0);\r\nc->aux_size = aux_size;\r\nc->alloc_callback = alloc_callback;\r\nc->write_callback = write_callback;\r\nfor (i = 0; i < LIST_SIZE; i++) {\r\nINIT_LIST_HEAD(&c->lru[i]);\r\nc->n_buffers[i] = 0;\r\n}\r\nfor (i = 0; i < 1 << DM_BUFIO_HASH_BITS; i++)\r\nINIT_HLIST_HEAD(&c->cache_hash[i]);\r\nmutex_init(&c->lock);\r\nINIT_LIST_HEAD(&c->reserved_buffers);\r\nc->need_reserved_buffers = reserved_buffers;\r\ninit_waitqueue_head(&c->free_buffer_wait);\r\nc->async_write_error = 0;\r\nc->dm_io = dm_io_client_create();\r\nif (IS_ERR(c->dm_io)) {\r\nr = PTR_ERR(c->dm_io);\r\ngoto bad_dm_io;\r\n}\r\nmutex_lock(&dm_bufio_clients_lock);\r\nif (c->blocks_per_page_bits) {\r\nif (!DM_BUFIO_CACHE_NAME(c)) {\r\nDM_BUFIO_CACHE_NAME(c) = kasprintf(GFP_KERNEL, "dm_bufio_cache-%u", c->block_size);\r\nif (!DM_BUFIO_CACHE_NAME(c)) {\r\nr = -ENOMEM;\r\nmutex_unlock(&dm_bufio_clients_lock);\r\ngoto bad_cache;\r\n}\r\n}\r\nif (!DM_BUFIO_CACHE(c)) {\r\nDM_BUFIO_CACHE(c) = kmem_cache_create(DM_BUFIO_CACHE_NAME(c),\r\nc->block_size,\r\nc->block_size, 0, NULL);\r\nif (!DM_BUFIO_CACHE(c)) {\r\nr = -ENOMEM;\r\nmutex_unlock(&dm_bufio_clients_lock);\r\ngoto bad_cache;\r\n}\r\n}\r\n}\r\nmutex_unlock(&dm_bufio_clients_lock);\r\nwhile (c->need_reserved_buffers) {\r\nstruct dm_buffer *b = alloc_buffer(c, GFP_KERNEL);\r\nif (!b) {\r\nr = -ENOMEM;\r\ngoto bad_buffer;\r\n}\r\n__free_buffer_wake(b);\r\n}\r\nmutex_lock(&dm_bufio_clients_lock);\r\ndm_bufio_client_count++;\r\nlist_add(&c->client_list, &dm_bufio_all_clients);\r\n__cache_size_refresh();\r\nmutex_unlock(&dm_bufio_clients_lock);\r\nc->shrinker.shrink = shrink;\r\nc->shrinker.seeks = 1;\r\nc->shrinker.batch = 0;\r\nregister_shrinker(&c->shrinker);\r\nreturn c;\r\nbad_buffer:\r\nbad_cache:\r\nwhile (!list_empty(&c->reserved_buffers)) {\r\nstruct dm_buffer *b = list_entry(c->reserved_buffers.next,\r\nstruct dm_buffer, lru_list);\r\nlist_del(&b->lru_list);\r\nfree_buffer(b);\r\n}\r\ndm_io_client_destroy(c->dm_io);\r\nbad_dm_io:\r\nvfree(c->cache_hash);\r\nbad_hash:\r\nkfree(c);\r\nbad_client:\r\nreturn ERR_PTR(r);\r\n}\r\nvoid dm_bufio_client_destroy(struct dm_bufio_client *c)\r\n{\r\nunsigned i;\r\ndrop_buffers(c);\r\nunregister_shrinker(&c->shrinker);\r\nmutex_lock(&dm_bufio_clients_lock);\r\nlist_del(&c->client_list);\r\ndm_bufio_client_count--;\r\n__cache_size_refresh();\r\nmutex_unlock(&dm_bufio_clients_lock);\r\nfor (i = 0; i < 1 << DM_BUFIO_HASH_BITS; i++)\r\nBUG_ON(!hlist_empty(&c->cache_hash[i]));\r\nBUG_ON(c->need_reserved_buffers);\r\nwhile (!list_empty(&c->reserved_buffers)) {\r\nstruct dm_buffer *b = list_entry(c->reserved_buffers.next,\r\nstruct dm_buffer, lru_list);\r\nlist_del(&b->lru_list);\r\nfree_buffer(b);\r\n}\r\nfor (i = 0; i < LIST_SIZE; i++)\r\nif (c->n_buffers[i])\r\nDMERR("leaked buffer count %d: %ld", i, c->n_buffers[i]);\r\nfor (i = 0; i < LIST_SIZE; i++)\r\nBUG_ON(c->n_buffers[i]);\r\ndm_io_client_destroy(c->dm_io);\r\nvfree(c->cache_hash);\r\nkfree(c);\r\n}\r\nstatic void cleanup_old_buffers(void)\r\n{\r\nunsigned long max_age = ACCESS_ONCE(dm_bufio_max_age);\r\nstruct dm_bufio_client *c;\r\nif (max_age > ULONG_MAX / HZ)\r\nmax_age = ULONG_MAX / HZ;\r\nmutex_lock(&dm_bufio_clients_lock);\r\nlist_for_each_entry(c, &dm_bufio_all_clients, client_list) {\r\nif (!dm_bufio_trylock(c))\r\ncontinue;\r\nwhile (!list_empty(&c->lru[LIST_CLEAN])) {\r\nstruct dm_buffer *b;\r\nb = list_entry(c->lru[LIST_CLEAN].prev,\r\nstruct dm_buffer, lru_list);\r\nif (__cleanup_old_buffer(b, 0, max_age * HZ))\r\nbreak;\r\ndm_bufio_cond_resched();\r\n}\r\ndm_bufio_unlock(c);\r\ndm_bufio_cond_resched();\r\n}\r\nmutex_unlock(&dm_bufio_clients_lock);\r\n}\r\nstatic void work_fn(struct work_struct *w)\r\n{\r\ncleanup_old_buffers();\r\nqueue_delayed_work(dm_bufio_wq, &dm_bufio_work,\r\nDM_BUFIO_WORK_TIMER_SECS * HZ);\r\n}\r\nstatic int __init dm_bufio_init(void)\r\n{\r\n__u64 mem;\r\nmemset(&dm_bufio_caches, 0, sizeof dm_bufio_caches);\r\nmemset(&dm_bufio_cache_names, 0, sizeof dm_bufio_cache_names);\r\nmem = (__u64)((totalram_pages - totalhigh_pages) *\r\nDM_BUFIO_MEMORY_PERCENT / 100) << PAGE_SHIFT;\r\nif (mem > ULONG_MAX)\r\nmem = ULONG_MAX;\r\n#ifdef CONFIG_MMU\r\nif (mem > (VMALLOC_END - VMALLOC_START) * DM_BUFIO_VMALLOC_PERCENT / 100)\r\nmem = (VMALLOC_END - VMALLOC_START) * DM_BUFIO_VMALLOC_PERCENT / 100;\r\n#endif\r\ndm_bufio_default_cache_size = mem;\r\nmutex_lock(&dm_bufio_clients_lock);\r\n__cache_size_refresh();\r\nmutex_unlock(&dm_bufio_clients_lock);\r\ndm_bufio_wq = create_singlethread_workqueue("dm_bufio_cache");\r\nif (!dm_bufio_wq)\r\nreturn -ENOMEM;\r\nINIT_DELAYED_WORK(&dm_bufio_work, work_fn);\r\nqueue_delayed_work(dm_bufio_wq, &dm_bufio_work,\r\nDM_BUFIO_WORK_TIMER_SECS * HZ);\r\nreturn 0;\r\n}\r\nstatic void __exit dm_bufio_exit(void)\r\n{\r\nint bug = 0;\r\nint i;\r\ncancel_delayed_work_sync(&dm_bufio_work);\r\ndestroy_workqueue(dm_bufio_wq);\r\nfor (i = 0; i < ARRAY_SIZE(dm_bufio_caches); i++) {\r\nstruct kmem_cache *kc = dm_bufio_caches[i];\r\nif (kc)\r\nkmem_cache_destroy(kc);\r\n}\r\nfor (i = 0; i < ARRAY_SIZE(dm_bufio_cache_names); i++)\r\nkfree(dm_bufio_cache_names[i]);\r\nif (dm_bufio_client_count) {\r\nDMCRIT("%s: dm_bufio_client_count leaked: %d",\r\n__func__, dm_bufio_client_count);\r\nbug = 1;\r\n}\r\nif (dm_bufio_current_allocated) {\r\nDMCRIT("%s: dm_bufio_current_allocated leaked: %lu",\r\n__func__, dm_bufio_current_allocated);\r\nbug = 1;\r\n}\r\nif (dm_bufio_allocated_get_free_pages) {\r\nDMCRIT("%s: dm_bufio_allocated_get_free_pages leaked: %lu",\r\n__func__, dm_bufio_allocated_get_free_pages);\r\nbug = 1;\r\n}\r\nif (dm_bufio_allocated_vmalloc) {\r\nDMCRIT("%s: dm_bufio_vmalloc leaked: %lu",\r\n__func__, dm_bufio_allocated_vmalloc);\r\nbug = 1;\r\n}\r\nif (bug)\r\nBUG();\r\n}
