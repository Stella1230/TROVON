static inline struct ib_mad_port_private *\r\n__ib_get_mad_port(struct ib_device *device, int port_num)\r\n{\r\nstruct ib_mad_port_private *entry;\r\nlist_for_each_entry(entry, &ib_mad_port_list, port_list) {\r\nif (entry->device == device && entry->port_num == port_num)\r\nreturn entry;\r\n}\r\nreturn NULL;\r\n}\r\nstatic inline struct ib_mad_port_private *\r\nib_get_mad_port(struct ib_device *device, int port_num)\r\n{\r\nstruct ib_mad_port_private *entry;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ib_mad_port_list_lock, flags);\r\nentry = __ib_get_mad_port(device, port_num);\r\nspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\r\nreturn entry;\r\n}\r\nstatic inline u8 convert_mgmt_class(u8 mgmt_class)\r\n{\r\nreturn mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE ?\r\n0 : mgmt_class;\r\n}\r\nstatic int get_spl_qp_index(enum ib_qp_type qp_type)\r\n{\r\nswitch (qp_type)\r\n{\r\ncase IB_QPT_SMI:\r\nreturn 0;\r\ncase IB_QPT_GSI:\r\nreturn 1;\r\ndefault:\r\nreturn -1;\r\n}\r\n}\r\nstatic int vendor_class_index(u8 mgmt_class)\r\n{\r\nreturn mgmt_class - IB_MGMT_CLASS_VENDOR_RANGE2_START;\r\n}\r\nstatic int is_vendor_class(u8 mgmt_class)\r\n{\r\nif ((mgmt_class < IB_MGMT_CLASS_VENDOR_RANGE2_START) ||\r\n(mgmt_class > IB_MGMT_CLASS_VENDOR_RANGE2_END))\r\nreturn 0;\r\nreturn 1;\r\n}\r\nstatic int is_vendor_oui(char *oui)\r\n{\r\nif (oui[0] || oui[1] || oui[2])\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int is_vendor_method_in_use(\r\nstruct ib_mad_mgmt_vendor_class *vendor_class,\r\nstruct ib_mad_reg_req *mad_reg_req)\r\n{\r\nstruct ib_mad_mgmt_method_table *method;\r\nint i;\r\nfor (i = 0; i < MAX_MGMT_OUI; i++) {\r\nif (!memcmp(vendor_class->oui[i], mad_reg_req->oui, 3)) {\r\nmethod = vendor_class->method_table[i];\r\nif (method) {\r\nif (method_in_use(&method, mad_reg_req))\r\nreturn 1;\r\nelse\r\nbreak;\r\n}\r\n}\r\n}\r\nreturn 0;\r\n}\r\nint ib_response_mad(struct ib_mad *mad)\r\n{\r\nreturn ((mad->mad_hdr.method & IB_MGMT_METHOD_RESP) ||\r\n(mad->mad_hdr.method == IB_MGMT_METHOD_TRAP_REPRESS) ||\r\n((mad->mad_hdr.mgmt_class == IB_MGMT_CLASS_BM) &&\r\n(mad->mad_hdr.attr_mod & IB_BM_ATTR_MOD_RESP)));\r\n}\r\nstruct ib_mad_agent *ib_register_mad_agent(struct ib_device *device,\r\nu8 port_num,\r\nenum ib_qp_type qp_type,\r\nstruct ib_mad_reg_req *mad_reg_req,\r\nu8 rmpp_version,\r\nib_mad_send_handler send_handler,\r\nib_mad_recv_handler recv_handler,\r\nvoid *context)\r\n{\r\nstruct ib_mad_port_private *port_priv;\r\nstruct ib_mad_agent *ret = ERR_PTR(-EINVAL);\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_reg_req *reg_req = NULL;\r\nstruct ib_mad_mgmt_class_table *class;\r\nstruct ib_mad_mgmt_vendor_class_table *vendor;\r\nstruct ib_mad_mgmt_vendor_class *vendor_class;\r\nstruct ib_mad_mgmt_method_table *method;\r\nint ret2, qpn;\r\nunsigned long flags;\r\nu8 mgmt_class, vclass;\r\nqpn = get_spl_qp_index(qp_type);\r\nif (qpn == -1)\r\ngoto error1;\r\nif (rmpp_version && rmpp_version != IB_MGMT_RMPP_VERSION)\r\ngoto error1;\r\nif (mad_reg_req) {\r\nif (mad_reg_req->mgmt_class_version >= MAX_MGMT_VERSION)\r\ngoto error1;\r\nif (!recv_handler)\r\ngoto error1;\r\nif (mad_reg_req->mgmt_class >= MAX_MGMT_CLASS) {\r\nif (mad_reg_req->mgmt_class !=\r\nIB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)\r\ngoto error1;\r\n} else if (mad_reg_req->mgmt_class == 0) {\r\ngoto error1;\r\n} else if (is_vendor_class(mad_reg_req->mgmt_class)) {\r\nif (!is_vendor_oui(mad_reg_req->oui))\r\ngoto error1;\r\n}\r\nif (!ib_is_mad_class_rmpp(mad_reg_req->mgmt_class)) {\r\nif (rmpp_version)\r\ngoto error1;\r\n}\r\nif (qp_type == IB_QPT_SMI) {\r\nif ((mad_reg_req->mgmt_class !=\r\nIB_MGMT_CLASS_SUBN_LID_ROUTED) &&\r\n(mad_reg_req->mgmt_class !=\r\nIB_MGMT_CLASS_SUBN_DIRECTED_ROUTE))\r\ngoto error1;\r\n} else {\r\nif ((mad_reg_req->mgmt_class ==\r\nIB_MGMT_CLASS_SUBN_LID_ROUTED) ||\r\n(mad_reg_req->mgmt_class ==\r\nIB_MGMT_CLASS_SUBN_DIRECTED_ROUTE))\r\ngoto error1;\r\n}\r\n} else {\r\nif (!send_handler)\r\ngoto error1;\r\n}\r\nport_priv = ib_get_mad_port(device, port_num);\r\nif (!port_priv) {\r\nret = ERR_PTR(-ENODEV);\r\ngoto error1;\r\n}\r\nif (!port_priv->qp_info[qpn].qp) {\r\nret = ERR_PTR(-EPROTONOSUPPORT);\r\ngoto error1;\r\n}\r\nmad_agent_priv = kzalloc(sizeof *mad_agent_priv, GFP_KERNEL);\r\nif (!mad_agent_priv) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto error1;\r\n}\r\nmad_agent_priv->agent.mr = ib_get_dma_mr(port_priv->qp_info[qpn].qp->pd,\r\nIB_ACCESS_LOCAL_WRITE);\r\nif (IS_ERR(mad_agent_priv->agent.mr)) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto error2;\r\n}\r\nif (mad_reg_req) {\r\nreg_req = kmemdup(mad_reg_req, sizeof *reg_req, GFP_KERNEL);\r\nif (!reg_req) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto error3;\r\n}\r\n}\r\nmad_agent_priv->qp_info = &port_priv->qp_info[qpn];\r\nmad_agent_priv->reg_req = reg_req;\r\nmad_agent_priv->agent.rmpp_version = rmpp_version;\r\nmad_agent_priv->agent.device = device;\r\nmad_agent_priv->agent.recv_handler = recv_handler;\r\nmad_agent_priv->agent.send_handler = send_handler;\r\nmad_agent_priv->agent.context = context;\r\nmad_agent_priv->agent.qp = port_priv->qp_info[qpn].qp;\r\nmad_agent_priv->agent.port_num = port_num;\r\nspin_lock_init(&mad_agent_priv->lock);\r\nINIT_LIST_HEAD(&mad_agent_priv->send_list);\r\nINIT_LIST_HEAD(&mad_agent_priv->wait_list);\r\nINIT_LIST_HEAD(&mad_agent_priv->done_list);\r\nINIT_LIST_HEAD(&mad_agent_priv->rmpp_list);\r\nINIT_DELAYED_WORK(&mad_agent_priv->timed_work, timeout_sends);\r\nINIT_LIST_HEAD(&mad_agent_priv->local_list);\r\nINIT_WORK(&mad_agent_priv->local_work, local_completions);\r\natomic_set(&mad_agent_priv->refcount, 1);\r\ninit_completion(&mad_agent_priv->comp);\r\nspin_lock_irqsave(&port_priv->reg_lock, flags);\r\nmad_agent_priv->agent.hi_tid = ++ib_mad_client_id;\r\nif (mad_reg_req) {\r\nmgmt_class = convert_mgmt_class(mad_reg_req->mgmt_class);\r\nif (!is_vendor_class(mgmt_class)) {\r\nclass = port_priv->version[mad_reg_req->\r\nmgmt_class_version].class;\r\nif (class) {\r\nmethod = class->method_table[mgmt_class];\r\nif (method) {\r\nif (method_in_use(&method,\r\nmad_reg_req))\r\ngoto error4;\r\n}\r\n}\r\nret2 = add_nonoui_reg_req(mad_reg_req, mad_agent_priv,\r\nmgmt_class);\r\n} else {\r\nvendor = port_priv->version[mad_reg_req->\r\nmgmt_class_version].vendor;\r\nif (vendor) {\r\nvclass = vendor_class_index(mgmt_class);\r\nvendor_class = vendor->vendor_class[vclass];\r\nif (vendor_class) {\r\nif (is_vendor_method_in_use(\r\nvendor_class,\r\nmad_reg_req))\r\ngoto error4;\r\n}\r\n}\r\nret2 = add_oui_reg_req(mad_reg_req, mad_agent_priv);\r\n}\r\nif (ret2) {\r\nret = ERR_PTR(ret2);\r\ngoto error4;\r\n}\r\n}\r\nlist_add_tail(&mad_agent_priv->agent_list, &port_priv->agent_list);\r\nspin_unlock_irqrestore(&port_priv->reg_lock, flags);\r\nreturn &mad_agent_priv->agent;\r\nerror4:\r\nspin_unlock_irqrestore(&port_priv->reg_lock, flags);\r\nkfree(reg_req);\r\nerror3:\r\nib_dereg_mr(mad_agent_priv->agent.mr);\r\nerror2:\r\nkfree(mad_agent_priv);\r\nerror1:\r\nreturn ret;\r\n}\r\nstatic inline int is_snooping_sends(int mad_snoop_flags)\r\n{\r\nreturn (mad_snoop_flags &\r\n(\r\nIB_MAD_SNOOP_SEND_COMPLETIONS ));\r\n}\r\nstatic inline int is_snooping_recvs(int mad_snoop_flags)\r\n{\r\nreturn (mad_snoop_flags &\r\n(IB_MAD_SNOOP_RECVS ));\r\n}\r\nstatic int register_snoop_agent(struct ib_mad_qp_info *qp_info,\r\nstruct ib_mad_snoop_private *mad_snoop_priv)\r\n{\r\nstruct ib_mad_snoop_private **new_snoop_table;\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&qp_info->snoop_lock, flags);\r\nfor (i = 0; i < qp_info->snoop_table_size; i++)\r\nif (!qp_info->snoop_table[i])\r\nbreak;\r\nif (i == qp_info->snoop_table_size) {\r\nnew_snoop_table = krealloc(qp_info->snoop_table,\r\nsizeof mad_snoop_priv *\r\n(qp_info->snoop_table_size + 1),\r\nGFP_ATOMIC);\r\nif (!new_snoop_table) {\r\ni = -ENOMEM;\r\ngoto out;\r\n}\r\nqp_info->snoop_table = new_snoop_table;\r\nqp_info->snoop_table_size++;\r\n}\r\nqp_info->snoop_table[i] = mad_snoop_priv;\r\natomic_inc(&qp_info->snoop_count);\r\nout:\r\nspin_unlock_irqrestore(&qp_info->snoop_lock, flags);\r\nreturn i;\r\n}\r\nstruct ib_mad_agent *ib_register_mad_snoop(struct ib_device *device,\r\nu8 port_num,\r\nenum ib_qp_type qp_type,\r\nint mad_snoop_flags,\r\nib_mad_snoop_handler snoop_handler,\r\nib_mad_recv_handler recv_handler,\r\nvoid *context)\r\n{\r\nstruct ib_mad_port_private *port_priv;\r\nstruct ib_mad_agent *ret;\r\nstruct ib_mad_snoop_private *mad_snoop_priv;\r\nint qpn;\r\nif ((is_snooping_sends(mad_snoop_flags) && !snoop_handler) ||\r\n(is_snooping_recvs(mad_snoop_flags) && !recv_handler)) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto error1;\r\n}\r\nqpn = get_spl_qp_index(qp_type);\r\nif (qpn == -1) {\r\nret = ERR_PTR(-EINVAL);\r\ngoto error1;\r\n}\r\nport_priv = ib_get_mad_port(device, port_num);\r\nif (!port_priv) {\r\nret = ERR_PTR(-ENODEV);\r\ngoto error1;\r\n}\r\nmad_snoop_priv = kzalloc(sizeof *mad_snoop_priv, GFP_KERNEL);\r\nif (!mad_snoop_priv) {\r\nret = ERR_PTR(-ENOMEM);\r\ngoto error1;\r\n}\r\nmad_snoop_priv->qp_info = &port_priv->qp_info[qpn];\r\nmad_snoop_priv->agent.device = device;\r\nmad_snoop_priv->agent.recv_handler = recv_handler;\r\nmad_snoop_priv->agent.snoop_handler = snoop_handler;\r\nmad_snoop_priv->agent.context = context;\r\nmad_snoop_priv->agent.qp = port_priv->qp_info[qpn].qp;\r\nmad_snoop_priv->agent.port_num = port_num;\r\nmad_snoop_priv->mad_snoop_flags = mad_snoop_flags;\r\ninit_completion(&mad_snoop_priv->comp);\r\nmad_snoop_priv->snoop_index = register_snoop_agent(\r\n&port_priv->qp_info[qpn],\r\nmad_snoop_priv);\r\nif (mad_snoop_priv->snoop_index < 0) {\r\nret = ERR_PTR(mad_snoop_priv->snoop_index);\r\ngoto error2;\r\n}\r\natomic_set(&mad_snoop_priv->refcount, 1);\r\nreturn &mad_snoop_priv->agent;\r\nerror2:\r\nkfree(mad_snoop_priv);\r\nerror1:\r\nreturn ret;\r\n}\r\nstatic inline void deref_mad_agent(struct ib_mad_agent_private *mad_agent_priv)\r\n{\r\nif (atomic_dec_and_test(&mad_agent_priv->refcount))\r\ncomplete(&mad_agent_priv->comp);\r\n}\r\nstatic inline void deref_snoop_agent(struct ib_mad_snoop_private *mad_snoop_priv)\r\n{\r\nif (atomic_dec_and_test(&mad_snoop_priv->refcount))\r\ncomplete(&mad_snoop_priv->comp);\r\n}\r\nstatic void unregister_mad_agent(struct ib_mad_agent_private *mad_agent_priv)\r\n{\r\nstruct ib_mad_port_private *port_priv;\r\nunsigned long flags;\r\ncancel_mads(mad_agent_priv);\r\nport_priv = mad_agent_priv->qp_info->port_priv;\r\ncancel_delayed_work(&mad_agent_priv->timed_work);\r\nspin_lock_irqsave(&port_priv->reg_lock, flags);\r\nremove_mad_reg_req(mad_agent_priv);\r\nlist_del(&mad_agent_priv->agent_list);\r\nspin_unlock_irqrestore(&port_priv->reg_lock, flags);\r\nflush_workqueue(port_priv->wq);\r\nib_cancel_rmpp_recvs(mad_agent_priv);\r\nderef_mad_agent(mad_agent_priv);\r\nwait_for_completion(&mad_agent_priv->comp);\r\nkfree(mad_agent_priv->reg_req);\r\nib_dereg_mr(mad_agent_priv->agent.mr);\r\nkfree(mad_agent_priv);\r\n}\r\nstatic void unregister_mad_snoop(struct ib_mad_snoop_private *mad_snoop_priv)\r\n{\r\nstruct ib_mad_qp_info *qp_info;\r\nunsigned long flags;\r\nqp_info = mad_snoop_priv->qp_info;\r\nspin_lock_irqsave(&qp_info->snoop_lock, flags);\r\nqp_info->snoop_table[mad_snoop_priv->snoop_index] = NULL;\r\natomic_dec(&qp_info->snoop_count);\r\nspin_unlock_irqrestore(&qp_info->snoop_lock, flags);\r\nderef_snoop_agent(mad_snoop_priv);\r\nwait_for_completion(&mad_snoop_priv->comp);\r\nkfree(mad_snoop_priv);\r\n}\r\nint ib_unregister_mad_agent(struct ib_mad_agent *mad_agent)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_snoop_private *mad_snoop_priv;\r\nif (mad_agent->hi_tid) {\r\nmad_agent_priv = container_of(mad_agent,\r\nstruct ib_mad_agent_private,\r\nagent);\r\nunregister_mad_agent(mad_agent_priv);\r\n} else {\r\nmad_snoop_priv = container_of(mad_agent,\r\nstruct ib_mad_snoop_private,\r\nagent);\r\nunregister_mad_snoop(mad_snoop_priv);\r\n}\r\nreturn 0;\r\n}\r\nstatic void dequeue_mad(struct ib_mad_list_head *mad_list)\r\n{\r\nstruct ib_mad_queue *mad_queue;\r\nunsigned long flags;\r\nBUG_ON(!mad_list->mad_queue);\r\nmad_queue = mad_list->mad_queue;\r\nspin_lock_irqsave(&mad_queue->lock, flags);\r\nlist_del(&mad_list->list);\r\nmad_queue->count--;\r\nspin_unlock_irqrestore(&mad_queue->lock, flags);\r\n}\r\nstatic void snoop_send(struct ib_mad_qp_info *qp_info,\r\nstruct ib_mad_send_buf *send_buf,\r\nstruct ib_mad_send_wc *mad_send_wc,\r\nint mad_snoop_flags)\r\n{\r\nstruct ib_mad_snoop_private *mad_snoop_priv;\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&qp_info->snoop_lock, flags);\r\nfor (i = 0; i < qp_info->snoop_table_size; i++) {\r\nmad_snoop_priv = qp_info->snoop_table[i];\r\nif (!mad_snoop_priv ||\r\n!(mad_snoop_priv->mad_snoop_flags & mad_snoop_flags))\r\ncontinue;\r\natomic_inc(&mad_snoop_priv->refcount);\r\nspin_unlock_irqrestore(&qp_info->snoop_lock, flags);\r\nmad_snoop_priv->agent.snoop_handler(&mad_snoop_priv->agent,\r\nsend_buf, mad_send_wc);\r\nderef_snoop_agent(mad_snoop_priv);\r\nspin_lock_irqsave(&qp_info->snoop_lock, flags);\r\n}\r\nspin_unlock_irqrestore(&qp_info->snoop_lock, flags);\r\n}\r\nstatic void snoop_recv(struct ib_mad_qp_info *qp_info,\r\nstruct ib_mad_recv_wc *mad_recv_wc,\r\nint mad_snoop_flags)\r\n{\r\nstruct ib_mad_snoop_private *mad_snoop_priv;\r\nunsigned long flags;\r\nint i;\r\nspin_lock_irqsave(&qp_info->snoop_lock, flags);\r\nfor (i = 0; i < qp_info->snoop_table_size; i++) {\r\nmad_snoop_priv = qp_info->snoop_table[i];\r\nif (!mad_snoop_priv ||\r\n!(mad_snoop_priv->mad_snoop_flags & mad_snoop_flags))\r\ncontinue;\r\natomic_inc(&mad_snoop_priv->refcount);\r\nspin_unlock_irqrestore(&qp_info->snoop_lock, flags);\r\nmad_snoop_priv->agent.recv_handler(&mad_snoop_priv->agent,\r\nmad_recv_wc);\r\nderef_snoop_agent(mad_snoop_priv);\r\nspin_lock_irqsave(&qp_info->snoop_lock, flags);\r\n}\r\nspin_unlock_irqrestore(&qp_info->snoop_lock, flags);\r\n}\r\nstatic void build_smp_wc(struct ib_qp *qp,\r\nu64 wr_id, u16 slid, u16 pkey_index, u8 port_num,\r\nstruct ib_wc *wc)\r\n{\r\nmemset(wc, 0, sizeof *wc);\r\nwc->wr_id = wr_id;\r\nwc->status = IB_WC_SUCCESS;\r\nwc->opcode = IB_WC_RECV;\r\nwc->pkey_index = pkey_index;\r\nwc->byte_len = sizeof(struct ib_mad) + sizeof(struct ib_grh);\r\nwc->src_qp = IB_QP0;\r\nwc->qp = qp;\r\nwc->slid = slid;\r\nwc->sl = 0;\r\nwc->dlid_path_bits = 0;\r\nwc->port_num = port_num;\r\n}\r\nstatic int handle_outgoing_dr_smp(struct ib_mad_agent_private *mad_agent_priv,\r\nstruct ib_mad_send_wr_private *mad_send_wr)\r\n{\r\nint ret = 0;\r\nstruct ib_smp *smp = mad_send_wr->send_buf.mad;\r\nunsigned long flags;\r\nstruct ib_mad_local_private *local;\r\nstruct ib_mad_private *mad_priv;\r\nstruct ib_mad_port_private *port_priv;\r\nstruct ib_mad_agent_private *recv_mad_agent = NULL;\r\nstruct ib_device *device = mad_agent_priv->agent.device;\r\nu8 port_num;\r\nstruct ib_wc mad_wc;\r\nstruct ib_send_wr *send_wr = &mad_send_wr->send_wr;\r\nif (device->node_type == RDMA_NODE_IB_SWITCH &&\r\nsmp->mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)\r\nport_num = send_wr->wr.ud.port_num;\r\nelse\r\nport_num = mad_agent_priv->agent.port_num;\r\nif ((ib_get_smp_direction(smp) ? smp->dr_dlid : smp->dr_slid) ==\r\nIB_LID_PERMISSIVE &&\r\nsmi_handle_dr_smp_send(smp, device->node_type, port_num) ==\r\nIB_SMI_DISCARD) {\r\nret = -EINVAL;\r\nprintk(KERN_ERR PFX "Invalid directed route\n");\r\ngoto out;\r\n}\r\nif (smi_check_local_smp(smp, device) == IB_SMI_DISCARD &&\r\nsmi_check_local_returning_smp(smp, device) == IB_SMI_DISCARD)\r\ngoto out;\r\nlocal = kmalloc(sizeof *local, GFP_ATOMIC);\r\nif (!local) {\r\nret = -ENOMEM;\r\nprintk(KERN_ERR PFX "No memory for ib_mad_local_private\n");\r\ngoto out;\r\n}\r\nlocal->mad_priv = NULL;\r\nlocal->recv_mad_agent = NULL;\r\nmad_priv = kmem_cache_alloc(ib_mad_cache, GFP_ATOMIC);\r\nif (!mad_priv) {\r\nret = -ENOMEM;\r\nprintk(KERN_ERR PFX "No memory for local response MAD\n");\r\nkfree(local);\r\ngoto out;\r\n}\r\nbuild_smp_wc(mad_agent_priv->agent.qp,\r\nsend_wr->wr_id, be16_to_cpu(smp->dr_slid),\r\nsend_wr->wr.ud.pkey_index,\r\nsend_wr->wr.ud.port_num, &mad_wc);\r\nret = device->process_mad(device, 0, port_num, &mad_wc, NULL,\r\n(struct ib_mad *)smp,\r\n(struct ib_mad *)&mad_priv->mad);\r\nswitch (ret)\r\n{\r\ncase IB_MAD_RESULT_SUCCESS | IB_MAD_RESULT_REPLY:\r\nif (ib_response_mad(&mad_priv->mad.mad) &&\r\nmad_agent_priv->agent.recv_handler) {\r\nlocal->mad_priv = mad_priv;\r\nlocal->recv_mad_agent = mad_agent_priv;\r\natomic_inc(&mad_agent_priv->refcount);\r\n} else\r\nkmem_cache_free(ib_mad_cache, mad_priv);\r\nbreak;\r\ncase IB_MAD_RESULT_SUCCESS | IB_MAD_RESULT_CONSUMED:\r\nkmem_cache_free(ib_mad_cache, mad_priv);\r\nbreak;\r\ncase IB_MAD_RESULT_SUCCESS:\r\nport_priv = ib_get_mad_port(mad_agent_priv->agent.device,\r\nmad_agent_priv->agent.port_num);\r\nif (port_priv) {\r\nmemcpy(&mad_priv->mad.mad, smp, sizeof(struct ib_mad));\r\nrecv_mad_agent = find_mad_agent(port_priv,\r\n&mad_priv->mad.mad);\r\n}\r\nif (!port_priv || !recv_mad_agent) {\r\nkmem_cache_free(ib_mad_cache, mad_priv);\r\nbreak;\r\n}\r\nlocal->mad_priv = mad_priv;\r\nlocal->recv_mad_agent = recv_mad_agent;\r\nbreak;\r\ndefault:\r\nkmem_cache_free(ib_mad_cache, mad_priv);\r\nkfree(local);\r\nret = -EINVAL;\r\ngoto out;\r\n}\r\nlocal->mad_send_wr = mad_send_wr;\r\natomic_inc(&mad_agent_priv->refcount);\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nlist_add_tail(&local->completion_list, &mad_agent_priv->local_list);\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nqueue_work(mad_agent_priv->qp_info->port_priv->wq,\r\n&mad_agent_priv->local_work);\r\nret = 1;\r\nout:\r\nreturn ret;\r\n}\r\nstatic int get_pad_size(int hdr_len, int data_len)\r\n{\r\nint seg_size, pad;\r\nseg_size = sizeof(struct ib_mad) - hdr_len;\r\nif (data_len && seg_size) {\r\npad = seg_size - data_len % seg_size;\r\nreturn pad == seg_size ? 0 : pad;\r\n} else\r\nreturn seg_size;\r\n}\r\nstatic void free_send_rmpp_list(struct ib_mad_send_wr_private *mad_send_wr)\r\n{\r\nstruct ib_rmpp_segment *s, *t;\r\nlist_for_each_entry_safe(s, t, &mad_send_wr->rmpp_list, list) {\r\nlist_del(&s->list);\r\nkfree(s);\r\n}\r\n}\r\nstatic int alloc_send_rmpp_list(struct ib_mad_send_wr_private *send_wr,\r\ngfp_t gfp_mask)\r\n{\r\nstruct ib_mad_send_buf *send_buf = &send_wr->send_buf;\r\nstruct ib_rmpp_mad *rmpp_mad = send_buf->mad;\r\nstruct ib_rmpp_segment *seg = NULL;\r\nint left, seg_size, pad;\r\nsend_buf->seg_size = sizeof (struct ib_mad) - send_buf->hdr_len;\r\nseg_size = send_buf->seg_size;\r\npad = send_wr->pad;\r\nfor (left = send_buf->data_len + pad; left > 0; left -= seg_size) {\r\nseg = kmalloc(sizeof (*seg) + seg_size, gfp_mask);\r\nif (!seg) {\r\nprintk(KERN_ERR "alloc_send_rmpp_segs: RMPP mem "\r\n"alloc failed for len %zd, gfp %#x\n",\r\nsizeof (*seg) + seg_size, gfp_mask);\r\nfree_send_rmpp_list(send_wr);\r\nreturn -ENOMEM;\r\n}\r\nseg->num = ++send_buf->seg_count;\r\nlist_add_tail(&seg->list, &send_wr->rmpp_list);\r\n}\r\nif (pad)\r\nmemset(seg->data + seg_size - pad, 0, pad);\r\nrmpp_mad->rmpp_hdr.rmpp_version = send_wr->mad_agent_priv->\r\nagent.rmpp_version;\r\nrmpp_mad->rmpp_hdr.rmpp_type = IB_MGMT_RMPP_TYPE_DATA;\r\nib_set_rmpp_flags(&rmpp_mad->rmpp_hdr, IB_MGMT_RMPP_FLAG_ACTIVE);\r\nsend_wr->cur_seg = container_of(send_wr->rmpp_list.next,\r\nstruct ib_rmpp_segment, list);\r\nsend_wr->last_ack_seg = send_wr->cur_seg;\r\nreturn 0;\r\n}\r\nstruct ib_mad_send_buf * ib_create_send_mad(struct ib_mad_agent *mad_agent,\r\nu32 remote_qpn, u16 pkey_index,\r\nint rmpp_active,\r\nint hdr_len, int data_len,\r\ngfp_t gfp_mask)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nint pad, message_size, ret, size;\r\nvoid *buf;\r\nmad_agent_priv = container_of(mad_agent, struct ib_mad_agent_private,\r\nagent);\r\npad = get_pad_size(hdr_len, data_len);\r\nmessage_size = hdr_len + data_len + pad;\r\nif ((!mad_agent->rmpp_version &&\r\n(rmpp_active || message_size > sizeof(struct ib_mad))) ||\r\n(!rmpp_active && message_size > sizeof(struct ib_mad)))\r\nreturn ERR_PTR(-EINVAL);\r\nsize = rmpp_active ? hdr_len : sizeof(struct ib_mad);\r\nbuf = kzalloc(sizeof *mad_send_wr + size, gfp_mask);\r\nif (!buf)\r\nreturn ERR_PTR(-ENOMEM);\r\nmad_send_wr = buf + size;\r\nINIT_LIST_HEAD(&mad_send_wr->rmpp_list);\r\nmad_send_wr->send_buf.mad = buf;\r\nmad_send_wr->send_buf.hdr_len = hdr_len;\r\nmad_send_wr->send_buf.data_len = data_len;\r\nmad_send_wr->pad = pad;\r\nmad_send_wr->mad_agent_priv = mad_agent_priv;\r\nmad_send_wr->sg_list[0].length = hdr_len;\r\nmad_send_wr->sg_list[0].lkey = mad_agent->mr->lkey;\r\nmad_send_wr->sg_list[1].length = sizeof(struct ib_mad) - hdr_len;\r\nmad_send_wr->sg_list[1].lkey = mad_agent->mr->lkey;\r\nmad_send_wr->send_wr.wr_id = (unsigned long) mad_send_wr;\r\nmad_send_wr->send_wr.sg_list = mad_send_wr->sg_list;\r\nmad_send_wr->send_wr.num_sge = 2;\r\nmad_send_wr->send_wr.opcode = IB_WR_SEND;\r\nmad_send_wr->send_wr.send_flags = IB_SEND_SIGNALED;\r\nmad_send_wr->send_wr.wr.ud.remote_qpn = remote_qpn;\r\nmad_send_wr->send_wr.wr.ud.remote_qkey = IB_QP_SET_QKEY;\r\nmad_send_wr->send_wr.wr.ud.pkey_index = pkey_index;\r\nif (rmpp_active) {\r\nret = alloc_send_rmpp_list(mad_send_wr, gfp_mask);\r\nif (ret) {\r\nkfree(buf);\r\nreturn ERR_PTR(ret);\r\n}\r\n}\r\nmad_send_wr->send_buf.mad_agent = mad_agent;\r\natomic_inc(&mad_agent_priv->refcount);\r\nreturn &mad_send_wr->send_buf;\r\n}\r\nint ib_get_mad_data_offset(u8 mgmt_class)\r\n{\r\nif (mgmt_class == IB_MGMT_CLASS_SUBN_ADM)\r\nreturn IB_MGMT_SA_HDR;\r\nelse if ((mgmt_class == IB_MGMT_CLASS_DEVICE_MGMT) ||\r\n(mgmt_class == IB_MGMT_CLASS_DEVICE_ADM) ||\r\n(mgmt_class == IB_MGMT_CLASS_BIS))\r\nreturn IB_MGMT_DEVICE_HDR;\r\nelse if ((mgmt_class >= IB_MGMT_CLASS_VENDOR_RANGE2_START) &&\r\n(mgmt_class <= IB_MGMT_CLASS_VENDOR_RANGE2_END))\r\nreturn IB_MGMT_VENDOR_HDR;\r\nelse\r\nreturn IB_MGMT_MAD_HDR;\r\n}\r\nint ib_is_mad_class_rmpp(u8 mgmt_class)\r\n{\r\nif ((mgmt_class == IB_MGMT_CLASS_SUBN_ADM) ||\r\n(mgmt_class == IB_MGMT_CLASS_DEVICE_MGMT) ||\r\n(mgmt_class == IB_MGMT_CLASS_DEVICE_ADM) ||\r\n(mgmt_class == IB_MGMT_CLASS_BIS) ||\r\n((mgmt_class >= IB_MGMT_CLASS_VENDOR_RANGE2_START) &&\r\n(mgmt_class <= IB_MGMT_CLASS_VENDOR_RANGE2_END)))\r\nreturn 1;\r\nreturn 0;\r\n}\r\nvoid *ib_get_rmpp_segment(struct ib_mad_send_buf *send_buf, int seg_num)\r\n{\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nstruct list_head *list;\r\nmad_send_wr = container_of(send_buf, struct ib_mad_send_wr_private,\r\nsend_buf);\r\nlist = &mad_send_wr->cur_seg->list;\r\nif (mad_send_wr->cur_seg->num < seg_num) {\r\nlist_for_each_entry(mad_send_wr->cur_seg, list, list)\r\nif (mad_send_wr->cur_seg->num == seg_num)\r\nbreak;\r\n} else if (mad_send_wr->cur_seg->num > seg_num) {\r\nlist_for_each_entry_reverse(mad_send_wr->cur_seg, list, list)\r\nif (mad_send_wr->cur_seg->num == seg_num)\r\nbreak;\r\n}\r\nreturn mad_send_wr->cur_seg->data;\r\n}\r\nstatic inline void *ib_get_payload(struct ib_mad_send_wr_private *mad_send_wr)\r\n{\r\nif (mad_send_wr->send_buf.seg_count)\r\nreturn ib_get_rmpp_segment(&mad_send_wr->send_buf,\r\nmad_send_wr->seg_num);\r\nelse\r\nreturn mad_send_wr->send_buf.mad +\r\nmad_send_wr->send_buf.hdr_len;\r\n}\r\nvoid ib_free_send_mad(struct ib_mad_send_buf *send_buf)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nmad_agent_priv = container_of(send_buf->mad_agent,\r\nstruct ib_mad_agent_private, agent);\r\nmad_send_wr = container_of(send_buf, struct ib_mad_send_wr_private,\r\nsend_buf);\r\nfree_send_rmpp_list(mad_send_wr);\r\nkfree(send_buf->mad);\r\nderef_mad_agent(mad_agent_priv);\r\n}\r\nint ib_send_mad(struct ib_mad_send_wr_private *mad_send_wr)\r\n{\r\nstruct ib_mad_qp_info *qp_info;\r\nstruct list_head *list;\r\nstruct ib_send_wr *bad_send_wr;\r\nstruct ib_mad_agent *mad_agent;\r\nstruct ib_sge *sge;\r\nunsigned long flags;\r\nint ret;\r\nqp_info = mad_send_wr->mad_agent_priv->qp_info;\r\nmad_send_wr->send_wr.wr_id = (unsigned long)&mad_send_wr->mad_list;\r\nmad_send_wr->mad_list.mad_queue = &qp_info->send_queue;\r\nmad_agent = mad_send_wr->send_buf.mad_agent;\r\nsge = mad_send_wr->sg_list;\r\nsge[0].addr = ib_dma_map_single(mad_agent->device,\r\nmad_send_wr->send_buf.mad,\r\nsge[0].length,\r\nDMA_TO_DEVICE);\r\nmad_send_wr->header_mapping = sge[0].addr;\r\nsge[1].addr = ib_dma_map_single(mad_agent->device,\r\nib_get_payload(mad_send_wr),\r\nsge[1].length,\r\nDMA_TO_DEVICE);\r\nmad_send_wr->payload_mapping = sge[1].addr;\r\nspin_lock_irqsave(&qp_info->send_queue.lock, flags);\r\nif (qp_info->send_queue.count < qp_info->send_queue.max_active) {\r\nret = ib_post_send(mad_agent->qp, &mad_send_wr->send_wr,\r\n&bad_send_wr);\r\nlist = &qp_info->send_queue.list;\r\n} else {\r\nret = 0;\r\nlist = &qp_info->overflow_list;\r\n}\r\nif (!ret) {\r\nqp_info->send_queue.count++;\r\nlist_add_tail(&mad_send_wr->mad_list.list, list);\r\n}\r\nspin_unlock_irqrestore(&qp_info->send_queue.lock, flags);\r\nif (ret) {\r\nib_dma_unmap_single(mad_agent->device,\r\nmad_send_wr->header_mapping,\r\nsge[0].length, DMA_TO_DEVICE);\r\nib_dma_unmap_single(mad_agent->device,\r\nmad_send_wr->payload_mapping,\r\nsge[1].length, DMA_TO_DEVICE);\r\n}\r\nreturn ret;\r\n}\r\nint ib_post_send_mad(struct ib_mad_send_buf *send_buf,\r\nstruct ib_mad_send_buf **bad_send_buf)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_send_buf *next_send_buf;\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nunsigned long flags;\r\nint ret = -EINVAL;\r\nfor (; send_buf; send_buf = next_send_buf) {\r\nmad_send_wr = container_of(send_buf,\r\nstruct ib_mad_send_wr_private,\r\nsend_buf);\r\nmad_agent_priv = mad_send_wr->mad_agent_priv;\r\nif (!send_buf->mad_agent->send_handler ||\r\n(send_buf->timeout_ms &&\r\n!send_buf->mad_agent->recv_handler)) {\r\nret = -EINVAL;\r\ngoto error;\r\n}\r\nif (!ib_is_mad_class_rmpp(((struct ib_mad_hdr *) send_buf->mad)->mgmt_class)) {\r\nif (mad_agent_priv->agent.rmpp_version) {\r\nret = -EINVAL;\r\ngoto error;\r\n}\r\n}\r\nnext_send_buf = send_buf->next;\r\nmad_send_wr->send_wr.wr.ud.ah = send_buf->ah;\r\nif (((struct ib_mad_hdr *) send_buf->mad)->mgmt_class ==\r\nIB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {\r\nret = handle_outgoing_dr_smp(mad_agent_priv,\r\nmad_send_wr);\r\nif (ret < 0)\r\ngoto error;\r\nelse if (ret == 1)\r\ncontinue;\r\n}\r\nmad_send_wr->tid = ((struct ib_mad_hdr *) send_buf->mad)->tid;\r\nmad_send_wr->timeout = msecs_to_jiffies(send_buf->timeout_ms);\r\nmad_send_wr->max_retries = send_buf->retries;\r\nmad_send_wr->retries_left = send_buf->retries;\r\nsend_buf->retries = 0;\r\nmad_send_wr->refcount = 1 + (mad_send_wr->timeout > 0);\r\nmad_send_wr->status = IB_WC_SUCCESS;\r\natomic_inc(&mad_agent_priv->refcount);\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nlist_add_tail(&mad_send_wr->agent_list,\r\n&mad_agent_priv->send_list);\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nif (mad_agent_priv->agent.rmpp_version) {\r\nret = ib_send_rmpp_mad(mad_send_wr);\r\nif (ret >= 0 && ret != IB_RMPP_RESULT_CONSUMED)\r\nret = ib_send_mad(mad_send_wr);\r\n} else\r\nret = ib_send_mad(mad_send_wr);\r\nif (ret < 0) {\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nlist_del(&mad_send_wr->agent_list);\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\natomic_dec(&mad_agent_priv->refcount);\r\ngoto error;\r\n}\r\n}\r\nreturn 0;\r\nerror:\r\nif (bad_send_buf)\r\n*bad_send_buf = send_buf;\r\nreturn ret;\r\n}\r\nvoid ib_free_recv_mad(struct ib_mad_recv_wc *mad_recv_wc)\r\n{\r\nstruct ib_mad_recv_buf *mad_recv_buf, *temp_recv_buf;\r\nstruct ib_mad_private_header *mad_priv_hdr;\r\nstruct ib_mad_private *priv;\r\nstruct list_head free_list;\r\nINIT_LIST_HEAD(&free_list);\r\nlist_splice_init(&mad_recv_wc->rmpp_list, &free_list);\r\nlist_for_each_entry_safe(mad_recv_buf, temp_recv_buf,\r\n&free_list, list) {\r\nmad_recv_wc = container_of(mad_recv_buf, struct ib_mad_recv_wc,\r\nrecv_buf);\r\nmad_priv_hdr = container_of(mad_recv_wc,\r\nstruct ib_mad_private_header,\r\nrecv_wc);\r\npriv = container_of(mad_priv_hdr, struct ib_mad_private,\r\nheader);\r\nkmem_cache_free(ib_mad_cache, priv);\r\n}\r\n}\r\nstruct ib_mad_agent *ib_redirect_mad_qp(struct ib_qp *qp,\r\nu8 rmpp_version,\r\nib_mad_send_handler send_handler,\r\nib_mad_recv_handler recv_handler,\r\nvoid *context)\r\n{\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nint ib_process_mad_wc(struct ib_mad_agent *mad_agent,\r\nstruct ib_wc *wc)\r\n{\r\nprintk(KERN_ERR PFX "ib_process_mad_wc() not implemented yet\n");\r\nreturn 0;\r\n}\r\nstatic int method_in_use(struct ib_mad_mgmt_method_table **method,\r\nstruct ib_mad_reg_req *mad_reg_req)\r\n{\r\nint i;\r\nfor_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS) {\r\nif ((*method)->agent[i]) {\r\nprintk(KERN_ERR PFX "Method %d already in use\n", i);\r\nreturn -EINVAL;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int allocate_method_table(struct ib_mad_mgmt_method_table **method)\r\n{\r\n*method = kzalloc(sizeof **method, GFP_ATOMIC);\r\nif (!*method) {\r\nprintk(KERN_ERR PFX "No memory for "\r\n"ib_mad_mgmt_method_table\n");\r\nreturn -ENOMEM;\r\n}\r\nreturn 0;\r\n}\r\nstatic int check_method_table(struct ib_mad_mgmt_method_table *method)\r\n{\r\nint i;\r\nfor (i = 0; i < IB_MGMT_MAX_METHODS; i++)\r\nif (method->agent[i])\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int check_class_table(struct ib_mad_mgmt_class_table *class)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_MGMT_CLASS; i++)\r\nif (class->method_table[i])\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int check_vendor_class(struct ib_mad_mgmt_vendor_class *vendor_class)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_MGMT_OUI; i++)\r\nif (vendor_class->method_table[i])\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic int find_vendor_oui(struct ib_mad_mgmt_vendor_class *vendor_class,\r\nchar *oui)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_MGMT_OUI; i++)\r\nif (!memcmp(vendor_class->oui[i], oui, 3))\r\nreturn i;\r\nreturn -1;\r\n}\r\nstatic int check_vendor_table(struct ib_mad_mgmt_vendor_class_table *vendor)\r\n{\r\nint i;\r\nfor (i = 0; i < MAX_MGMT_VENDOR_RANGE2; i++)\r\nif (vendor->vendor_class[i])\r\nreturn 1;\r\nreturn 0;\r\n}\r\nstatic void remove_methods_mad_agent(struct ib_mad_mgmt_method_table *method,\r\nstruct ib_mad_agent_private *agent)\r\n{\r\nint i;\r\nfor (i = 0; i < IB_MGMT_MAX_METHODS; i++) {\r\nif (method->agent[i] == agent) {\r\nmethod->agent[i] = NULL;\r\n}\r\n}\r\n}\r\nstatic int add_nonoui_reg_req(struct ib_mad_reg_req *mad_reg_req,\r\nstruct ib_mad_agent_private *agent_priv,\r\nu8 mgmt_class)\r\n{\r\nstruct ib_mad_port_private *port_priv;\r\nstruct ib_mad_mgmt_class_table **class;\r\nstruct ib_mad_mgmt_method_table **method;\r\nint i, ret;\r\nport_priv = agent_priv->qp_info->port_priv;\r\nclass = &port_priv->version[mad_reg_req->mgmt_class_version].class;\r\nif (!*class) {\r\n*class = kzalloc(sizeof **class, GFP_ATOMIC);\r\nif (!*class) {\r\nprintk(KERN_ERR PFX "No memory for "\r\n"ib_mad_mgmt_class_table\n");\r\nret = -ENOMEM;\r\ngoto error1;\r\n}\r\nmethod = &(*class)->method_table[mgmt_class];\r\nif ((ret = allocate_method_table(method)))\r\ngoto error2;\r\n} else {\r\nmethod = &(*class)->method_table[mgmt_class];\r\nif (!*method) {\r\nif ((ret = allocate_method_table(method)))\r\ngoto error1;\r\n}\r\n}\r\nif (method_in_use(method, mad_reg_req))\r\ngoto error3;\r\nfor_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS)\r\n(*method)->agent[i] = agent_priv;\r\nreturn 0;\r\nerror3:\r\nremove_methods_mad_agent(*method, agent_priv);\r\nif (!check_method_table(*method)) {\r\nkfree(*method);\r\n*method = NULL;\r\n}\r\nret = -EINVAL;\r\ngoto error1;\r\nerror2:\r\nkfree(*class);\r\n*class = NULL;\r\nerror1:\r\nreturn ret;\r\n}\r\nstatic int add_oui_reg_req(struct ib_mad_reg_req *mad_reg_req,\r\nstruct ib_mad_agent_private *agent_priv)\r\n{\r\nstruct ib_mad_port_private *port_priv;\r\nstruct ib_mad_mgmt_vendor_class_table **vendor_table;\r\nstruct ib_mad_mgmt_vendor_class_table *vendor = NULL;\r\nstruct ib_mad_mgmt_vendor_class *vendor_class = NULL;\r\nstruct ib_mad_mgmt_method_table **method;\r\nint i, ret = -ENOMEM;\r\nu8 vclass;\r\nvclass = vendor_class_index(mad_reg_req->mgmt_class);\r\nport_priv = agent_priv->qp_info->port_priv;\r\nvendor_table = &port_priv->version[\r\nmad_reg_req->mgmt_class_version].vendor;\r\nif (!*vendor_table) {\r\nvendor = kzalloc(sizeof *vendor, GFP_ATOMIC);\r\nif (!vendor) {\r\nprintk(KERN_ERR PFX "No memory for "\r\n"ib_mad_mgmt_vendor_class_table\n");\r\ngoto error1;\r\n}\r\n*vendor_table = vendor;\r\n}\r\nif (!(*vendor_table)->vendor_class[vclass]) {\r\nvendor_class = kzalloc(sizeof *vendor_class, GFP_ATOMIC);\r\nif (!vendor_class) {\r\nprintk(KERN_ERR PFX "No memory for "\r\n"ib_mad_mgmt_vendor_class\n");\r\ngoto error2;\r\n}\r\n(*vendor_table)->vendor_class[vclass] = vendor_class;\r\n}\r\nfor (i = 0; i < MAX_MGMT_OUI; i++) {\r\nif (!memcmp((*vendor_table)->vendor_class[vclass]->oui[i],\r\nmad_reg_req->oui, 3)) {\r\nmethod = &(*vendor_table)->vendor_class[\r\nvclass]->method_table[i];\r\nBUG_ON(!*method);\r\ngoto check_in_use;\r\n}\r\n}\r\nfor (i = 0; i < MAX_MGMT_OUI; i++) {\r\nif (!is_vendor_oui((*vendor_table)->vendor_class[\r\nvclass]->oui[i])) {\r\nmethod = &(*vendor_table)->vendor_class[\r\nvclass]->method_table[i];\r\nBUG_ON(*method);\r\nif ((ret = allocate_method_table(method)))\r\ngoto error3;\r\nmemcpy((*vendor_table)->vendor_class[vclass]->oui[i],\r\nmad_reg_req->oui, 3);\r\ngoto check_in_use;\r\n}\r\n}\r\nprintk(KERN_ERR PFX "All OUI slots in use\n");\r\ngoto error3;\r\ncheck_in_use:\r\nif (method_in_use(method, mad_reg_req))\r\ngoto error4;\r\nfor_each_set_bit(i, mad_reg_req->method_mask, IB_MGMT_MAX_METHODS)\r\n(*method)->agent[i] = agent_priv;\r\nreturn 0;\r\nerror4:\r\nremove_methods_mad_agent(*method, agent_priv);\r\nif (!check_method_table(*method)) {\r\nkfree(*method);\r\n*method = NULL;\r\n}\r\nret = -EINVAL;\r\nerror3:\r\nif (vendor_class) {\r\n(*vendor_table)->vendor_class[vclass] = NULL;\r\nkfree(vendor_class);\r\n}\r\nerror2:\r\nif (vendor) {\r\n*vendor_table = NULL;\r\nkfree(vendor);\r\n}\r\nerror1:\r\nreturn ret;\r\n}\r\nstatic void remove_mad_reg_req(struct ib_mad_agent_private *agent_priv)\r\n{\r\nstruct ib_mad_port_private *port_priv;\r\nstruct ib_mad_mgmt_class_table *class;\r\nstruct ib_mad_mgmt_method_table *method;\r\nstruct ib_mad_mgmt_vendor_class_table *vendor;\r\nstruct ib_mad_mgmt_vendor_class *vendor_class;\r\nint index;\r\nu8 mgmt_class;\r\nif (!agent_priv->reg_req) {\r\ngoto out;\r\n}\r\nport_priv = agent_priv->qp_info->port_priv;\r\nmgmt_class = convert_mgmt_class(agent_priv->reg_req->mgmt_class);\r\nclass = port_priv->version[\r\nagent_priv->reg_req->mgmt_class_version].class;\r\nif (!class)\r\ngoto vendor_check;\r\nmethod = class->method_table[mgmt_class];\r\nif (method) {\r\nremove_methods_mad_agent(method, agent_priv);\r\nif (!check_method_table(method)) {\r\nkfree(method);\r\nclass->method_table[mgmt_class] = NULL;\r\nif (!check_class_table(class)) {\r\nkfree(class);\r\nport_priv->version[\r\nagent_priv->reg_req->\r\nmgmt_class_version].class = NULL;\r\n}\r\n}\r\n}\r\nvendor_check:\r\nif (!is_vendor_class(mgmt_class))\r\ngoto out;\r\nmgmt_class = vendor_class_index(agent_priv->reg_req->mgmt_class);\r\nvendor = port_priv->version[\r\nagent_priv->reg_req->mgmt_class_version].vendor;\r\nif (!vendor)\r\ngoto out;\r\nvendor_class = vendor->vendor_class[mgmt_class];\r\nif (vendor_class) {\r\nindex = find_vendor_oui(vendor_class, agent_priv->reg_req->oui);\r\nif (index < 0)\r\ngoto out;\r\nmethod = vendor_class->method_table[index];\r\nif (method) {\r\nremove_methods_mad_agent(method, agent_priv);\r\nif (!check_method_table(method)) {\r\nkfree(method);\r\nvendor_class->method_table[index] = NULL;\r\nmemset(vendor_class->oui[index], 0, 3);\r\nif (!check_vendor_class(vendor_class)) {\r\nkfree(vendor_class);\r\nvendor->vendor_class[mgmt_class] = NULL;\r\nif (!check_vendor_table(vendor)) {\r\nkfree(vendor);\r\nport_priv->version[\r\nagent_priv->reg_req->\r\nmgmt_class_version].\r\nvendor = NULL;\r\n}\r\n}\r\n}\r\n}\r\n}\r\nout:\r\nreturn;\r\n}\r\nstatic struct ib_mad_agent_private *\r\nfind_mad_agent(struct ib_mad_port_private *port_priv,\r\nstruct ib_mad *mad)\r\n{\r\nstruct ib_mad_agent_private *mad_agent = NULL;\r\nunsigned long flags;\r\nspin_lock_irqsave(&port_priv->reg_lock, flags);\r\nif (ib_response_mad(mad)) {\r\nu32 hi_tid;\r\nstruct ib_mad_agent_private *entry;\r\nhi_tid = be64_to_cpu(mad->mad_hdr.tid) >> 32;\r\nlist_for_each_entry(entry, &port_priv->agent_list, agent_list) {\r\nif (entry->agent.hi_tid == hi_tid) {\r\nmad_agent = entry;\r\nbreak;\r\n}\r\n}\r\n} else {\r\nstruct ib_mad_mgmt_class_table *class;\r\nstruct ib_mad_mgmt_method_table *method;\r\nstruct ib_mad_mgmt_vendor_class_table *vendor;\r\nstruct ib_mad_mgmt_vendor_class *vendor_class;\r\nstruct ib_vendor_mad *vendor_mad;\r\nint index;\r\nif (mad->mad_hdr.class_version >= MAX_MGMT_VERSION)\r\ngoto out;\r\nif (!is_vendor_class(mad->mad_hdr.mgmt_class)) {\r\nclass = port_priv->version[\r\nmad->mad_hdr.class_version].class;\r\nif (!class)\r\ngoto out;\r\nif (convert_mgmt_class(mad->mad_hdr.mgmt_class) >=\r\nIB_MGMT_MAX_METHODS)\r\ngoto out;\r\nmethod = class->method_table[convert_mgmt_class(\r\nmad->mad_hdr.mgmt_class)];\r\nif (method)\r\nmad_agent = method->agent[mad->mad_hdr.method &\r\n~IB_MGMT_METHOD_RESP];\r\n} else {\r\nvendor = port_priv->version[\r\nmad->mad_hdr.class_version].vendor;\r\nif (!vendor)\r\ngoto out;\r\nvendor_class = vendor->vendor_class[vendor_class_index(\r\nmad->mad_hdr.mgmt_class)];\r\nif (!vendor_class)\r\ngoto out;\r\nvendor_mad = (struct ib_vendor_mad *)mad;\r\nindex = find_vendor_oui(vendor_class, vendor_mad->oui);\r\nif (index == -1)\r\ngoto out;\r\nmethod = vendor_class->method_table[index];\r\nif (method) {\r\nmad_agent = method->agent[mad->mad_hdr.method &\r\n~IB_MGMT_METHOD_RESP];\r\n}\r\n}\r\n}\r\nif (mad_agent) {\r\nif (mad_agent->agent.recv_handler)\r\natomic_inc(&mad_agent->refcount);\r\nelse {\r\nprintk(KERN_NOTICE PFX "No receive handler for client "\r\n"%p on port %d\n",\r\n&mad_agent->agent, port_priv->port_num);\r\nmad_agent = NULL;\r\n}\r\n}\r\nout:\r\nspin_unlock_irqrestore(&port_priv->reg_lock, flags);\r\nreturn mad_agent;\r\n}\r\nstatic int validate_mad(struct ib_mad *mad, u32 qp_num)\r\n{\r\nint valid = 0;\r\nif (mad->mad_hdr.base_version != IB_MGMT_BASE_VERSION) {\r\nprintk(KERN_ERR PFX "MAD received with unsupported base "\r\n"version %d\n", mad->mad_hdr.base_version);\r\ngoto out;\r\n}\r\nif ((mad->mad_hdr.mgmt_class == IB_MGMT_CLASS_SUBN_LID_ROUTED) ||\r\n(mad->mad_hdr.mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)) {\r\nif (qp_num == 0)\r\nvalid = 1;\r\n} else {\r\nif (qp_num != 0)\r\nvalid = 1;\r\n}\r\nout:\r\nreturn valid;\r\n}\r\nstatic int is_data_mad(struct ib_mad_agent_private *mad_agent_priv,\r\nstruct ib_mad_hdr *mad_hdr)\r\n{\r\nstruct ib_rmpp_mad *rmpp_mad;\r\nrmpp_mad = (struct ib_rmpp_mad *)mad_hdr;\r\nreturn !mad_agent_priv->agent.rmpp_version ||\r\n!(ib_get_rmpp_flags(&rmpp_mad->rmpp_hdr) &\r\nIB_MGMT_RMPP_FLAG_ACTIVE) ||\r\n(rmpp_mad->rmpp_hdr.rmpp_type == IB_MGMT_RMPP_TYPE_DATA);\r\n}\r\nstatic inline int rcv_has_same_class(struct ib_mad_send_wr_private *wr,\r\nstruct ib_mad_recv_wc *rwc)\r\n{\r\nreturn ((struct ib_mad *)(wr->send_buf.mad))->mad_hdr.mgmt_class ==\r\nrwc->recv_buf.mad->mad_hdr.mgmt_class;\r\n}\r\nstatic inline int rcv_has_same_gid(struct ib_mad_agent_private *mad_agent_priv,\r\nstruct ib_mad_send_wr_private *wr,\r\nstruct ib_mad_recv_wc *rwc )\r\n{\r\nstruct ib_ah_attr attr;\r\nu8 send_resp, rcv_resp;\r\nunion ib_gid sgid;\r\nstruct ib_device *device = mad_agent_priv->agent.device;\r\nu8 port_num = mad_agent_priv->agent.port_num;\r\nu8 lmc;\r\nsend_resp = ib_response_mad((struct ib_mad *)wr->send_buf.mad);\r\nrcv_resp = ib_response_mad(rwc->recv_buf.mad);\r\nif (send_resp == rcv_resp)\r\nreturn 0;\r\nif (ib_query_ah(wr->send_buf.ah, &attr))\r\nreturn 0;\r\nif (!!(attr.ah_flags & IB_AH_GRH) !=\r\n!!(rwc->wc->wc_flags & IB_WC_GRH))\r\nreturn 0;\r\nif (!send_resp && rcv_resp) {\r\nif (!(attr.ah_flags & IB_AH_GRH)) {\r\nif (ib_get_cached_lmc(device, port_num, &lmc))\r\nreturn 0;\r\nreturn (!lmc || !((attr.src_path_bits ^\r\nrwc->wc->dlid_path_bits) &\r\n((1 << lmc) - 1)));\r\n} else {\r\nif (ib_get_cached_gid(device, port_num,\r\nattr.grh.sgid_index, &sgid))\r\nreturn 0;\r\nreturn !memcmp(sgid.raw, rwc->recv_buf.grh->dgid.raw,\r\n16);\r\n}\r\n}\r\nif (!(attr.ah_flags & IB_AH_GRH))\r\nreturn attr.dlid == rwc->wc->slid;\r\nelse\r\nreturn !memcmp(attr.grh.dgid.raw, rwc->recv_buf.grh->sgid.raw,\r\n16);\r\n}\r\nstatic inline int is_direct(u8 class)\r\n{\r\nreturn (class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE);\r\n}\r\nstruct ib_mad_send_wr_private*\r\nib_find_send_mad(struct ib_mad_agent_private *mad_agent_priv,\r\nstruct ib_mad_recv_wc *wc)\r\n{\r\nstruct ib_mad_send_wr_private *wr;\r\nstruct ib_mad *mad;\r\nmad = (struct ib_mad *)wc->recv_buf.mad;\r\nlist_for_each_entry(wr, &mad_agent_priv->wait_list, agent_list) {\r\nif ((wr->tid == mad->mad_hdr.tid) &&\r\nrcv_has_same_class(wr, wc) &&\r\n(is_direct(wc->recv_buf.mad->mad_hdr.mgmt_class) ||\r\nrcv_has_same_gid(mad_agent_priv, wr, wc)))\r\nreturn (wr->status == IB_WC_SUCCESS) ? wr : NULL;\r\n}\r\nlist_for_each_entry(wr, &mad_agent_priv->send_list, agent_list) {\r\nif (is_data_mad(mad_agent_priv, wr->send_buf.mad) &&\r\nwr->tid == mad->mad_hdr.tid &&\r\nwr->timeout &&\r\nrcv_has_same_class(wr, wc) &&\r\n(is_direct(wc->recv_buf.mad->mad_hdr.mgmt_class) ||\r\nrcv_has_same_gid(mad_agent_priv, wr, wc)))\r\nreturn (wr->status == IB_WC_SUCCESS) ? wr : NULL;\r\n}\r\nreturn NULL;\r\n}\r\nvoid ib_mark_mad_done(struct ib_mad_send_wr_private *mad_send_wr)\r\n{\r\nmad_send_wr->timeout = 0;\r\nif (mad_send_wr->refcount == 1)\r\nlist_move_tail(&mad_send_wr->agent_list,\r\n&mad_send_wr->mad_agent_priv->done_list);\r\n}\r\nstatic void ib_mad_complete_recv(struct ib_mad_agent_private *mad_agent_priv,\r\nstruct ib_mad_recv_wc *mad_recv_wc)\r\n{\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nstruct ib_mad_send_wc mad_send_wc;\r\nunsigned long flags;\r\nINIT_LIST_HEAD(&mad_recv_wc->rmpp_list);\r\nlist_add(&mad_recv_wc->recv_buf.list, &mad_recv_wc->rmpp_list);\r\nif (mad_agent_priv->agent.rmpp_version) {\r\nmad_recv_wc = ib_process_rmpp_recv_wc(mad_agent_priv,\r\nmad_recv_wc);\r\nif (!mad_recv_wc) {\r\nderef_mad_agent(mad_agent_priv);\r\nreturn;\r\n}\r\n}\r\nif (ib_response_mad(mad_recv_wc->recv_buf.mad)) {\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nmad_send_wr = ib_find_send_mad(mad_agent_priv, mad_recv_wc);\r\nif (!mad_send_wr) {\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nib_free_recv_mad(mad_recv_wc);\r\nderef_mad_agent(mad_agent_priv);\r\nreturn;\r\n}\r\nib_mark_mad_done(mad_send_wr);\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nmad_recv_wc->wc->wr_id = (unsigned long) &mad_send_wr->send_buf;\r\nmad_agent_priv->agent.recv_handler(&mad_agent_priv->agent,\r\nmad_recv_wc);\r\natomic_dec(&mad_agent_priv->refcount);\r\nmad_send_wc.status = IB_WC_SUCCESS;\r\nmad_send_wc.vendor_err = 0;\r\nmad_send_wc.send_buf = &mad_send_wr->send_buf;\r\nib_mad_complete_send_wr(mad_send_wr, &mad_send_wc);\r\n} else {\r\nmad_agent_priv->agent.recv_handler(&mad_agent_priv->agent,\r\nmad_recv_wc);\r\nderef_mad_agent(mad_agent_priv);\r\n}\r\n}\r\nstatic bool generate_unmatched_resp(struct ib_mad_private *recv,\r\nstruct ib_mad_private *response)\r\n{\r\nif (recv->mad.mad.mad_hdr.method == IB_MGMT_METHOD_GET ||\r\nrecv->mad.mad.mad_hdr.method == IB_MGMT_METHOD_SET) {\r\nmemcpy(response, recv, sizeof *response);\r\nresponse->header.recv_wc.wc = &response->header.wc;\r\nresponse->header.recv_wc.recv_buf.mad = &response->mad.mad;\r\nresponse->header.recv_wc.recv_buf.grh = &response->grh;\r\nresponse->mad.mad.mad_hdr.method = IB_MGMT_METHOD_GET_RESP;\r\nresponse->mad.mad.mad_hdr.status =\r\ncpu_to_be16(IB_MGMT_MAD_STATUS_UNSUPPORTED_METHOD_ATTRIB);\r\nif (recv->mad.mad.mad_hdr.mgmt_class == IB_MGMT_CLASS_SUBN_DIRECTED_ROUTE)\r\nresponse->mad.mad.mad_hdr.status |= IB_SMP_DIRECTION;\r\nreturn true;\r\n} else {\r\nreturn false;\r\n}\r\n}\r\nstatic void ib_mad_recv_done_handler(struct ib_mad_port_private *port_priv,\r\nstruct ib_wc *wc)\r\n{\r\nstruct ib_mad_qp_info *qp_info;\r\nstruct ib_mad_private_header *mad_priv_hdr;\r\nstruct ib_mad_private *recv, *response = NULL;\r\nstruct ib_mad_list_head *mad_list;\r\nstruct ib_mad_agent_private *mad_agent;\r\nint port_num;\r\nint ret = IB_MAD_RESULT_SUCCESS;\r\nmad_list = (struct ib_mad_list_head *)(unsigned long)wc->wr_id;\r\nqp_info = mad_list->mad_queue->qp_info;\r\ndequeue_mad(mad_list);\r\nmad_priv_hdr = container_of(mad_list, struct ib_mad_private_header,\r\nmad_list);\r\nrecv = container_of(mad_priv_hdr, struct ib_mad_private, header);\r\nib_dma_unmap_single(port_priv->device,\r\nrecv->header.mapping,\r\nsizeof(struct ib_mad_private) -\r\nsizeof(struct ib_mad_private_header),\r\nDMA_FROM_DEVICE);\r\nrecv->header.wc = *wc;\r\nrecv->header.recv_wc.wc = &recv->header.wc;\r\nrecv->header.recv_wc.mad_len = sizeof(struct ib_mad);\r\nrecv->header.recv_wc.recv_buf.mad = &recv->mad.mad;\r\nrecv->header.recv_wc.recv_buf.grh = &recv->grh;\r\nif (atomic_read(&qp_info->snoop_count))\r\nsnoop_recv(qp_info, &recv->header.recv_wc, IB_MAD_SNOOP_RECVS);\r\nif (!validate_mad(&recv->mad.mad, qp_info->qp->qp_num))\r\ngoto out;\r\nresponse = kmem_cache_alloc(ib_mad_cache, GFP_KERNEL);\r\nif (!response) {\r\nprintk(KERN_ERR PFX "ib_mad_recv_done_handler no memory "\r\n"for response buffer\n");\r\ngoto out;\r\n}\r\nif (port_priv->device->node_type == RDMA_NODE_IB_SWITCH)\r\nport_num = wc->port_num;\r\nelse\r\nport_num = port_priv->port_num;\r\nif (recv->mad.mad.mad_hdr.mgmt_class ==\r\nIB_MGMT_CLASS_SUBN_DIRECTED_ROUTE) {\r\nenum smi_forward_action retsmi;\r\nif (smi_handle_dr_smp_recv(&recv->mad.smp,\r\nport_priv->device->node_type,\r\nport_num,\r\nport_priv->device->phys_port_cnt) ==\r\nIB_SMI_DISCARD)\r\ngoto out;\r\nretsmi = smi_check_forward_dr_smp(&recv->mad.smp);\r\nif (retsmi == IB_SMI_LOCAL)\r\ngoto local;\r\nif (retsmi == IB_SMI_SEND) {\r\nif (smi_handle_dr_smp_send(&recv->mad.smp,\r\nport_priv->device->node_type,\r\nport_num) == IB_SMI_DISCARD)\r\ngoto out;\r\nif (smi_check_local_smp(&recv->mad.smp, port_priv->device) == IB_SMI_DISCARD)\r\ngoto out;\r\n} else if (port_priv->device->node_type == RDMA_NODE_IB_SWITCH) {\r\nmemcpy(response, recv, sizeof(*response));\r\nresponse->header.recv_wc.wc = &response->header.wc;\r\nresponse->header.recv_wc.recv_buf.mad = &response->mad.mad;\r\nresponse->header.recv_wc.recv_buf.grh = &response->grh;\r\nagent_send_response(&response->mad.mad,\r\n&response->grh, wc,\r\nport_priv->device,\r\nsmi_get_fwd_port(&recv->mad.smp),\r\nqp_info->qp->qp_num);\r\ngoto out;\r\n}\r\n}\r\nlocal:\r\nif (port_priv->device->process_mad) {\r\nret = port_priv->device->process_mad(port_priv->device, 0,\r\nport_priv->port_num,\r\nwc, &recv->grh,\r\n&recv->mad.mad,\r\n&response->mad.mad);\r\nif (ret & IB_MAD_RESULT_SUCCESS) {\r\nif (ret & IB_MAD_RESULT_CONSUMED)\r\ngoto out;\r\nif (ret & IB_MAD_RESULT_REPLY) {\r\nagent_send_response(&response->mad.mad,\r\n&recv->grh, wc,\r\nport_priv->device,\r\nport_num,\r\nqp_info->qp->qp_num);\r\ngoto out;\r\n}\r\n}\r\n}\r\nmad_agent = find_mad_agent(port_priv, &recv->mad.mad);\r\nif (mad_agent) {\r\nib_mad_complete_recv(mad_agent, &recv->header.recv_wc);\r\nrecv = NULL;\r\n} else if ((ret & IB_MAD_RESULT_SUCCESS) &&\r\ngenerate_unmatched_resp(recv, response)) {\r\nagent_send_response(&response->mad.mad, &recv->grh, wc,\r\nport_priv->device, port_num, qp_info->qp->qp_num);\r\n}\r\nout:\r\nif (response) {\r\nib_mad_post_receive_mads(qp_info, response);\r\nif (recv)\r\nkmem_cache_free(ib_mad_cache, recv);\r\n} else\r\nib_mad_post_receive_mads(qp_info, recv);\r\n}\r\nstatic void adjust_timeout(struct ib_mad_agent_private *mad_agent_priv)\r\n{\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nunsigned long delay;\r\nif (list_empty(&mad_agent_priv->wait_list)) {\r\ncancel_delayed_work(&mad_agent_priv->timed_work);\r\n} else {\r\nmad_send_wr = list_entry(mad_agent_priv->wait_list.next,\r\nstruct ib_mad_send_wr_private,\r\nagent_list);\r\nif (time_after(mad_agent_priv->timeout,\r\nmad_send_wr->timeout)) {\r\nmad_agent_priv->timeout = mad_send_wr->timeout;\r\ndelay = mad_send_wr->timeout - jiffies;\r\nif ((long)delay <= 0)\r\ndelay = 1;\r\nmod_delayed_work(mad_agent_priv->qp_info->port_priv->wq,\r\n&mad_agent_priv->timed_work, delay);\r\n}\r\n}\r\n}\r\nstatic void wait_for_response(struct ib_mad_send_wr_private *mad_send_wr)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_send_wr_private *temp_mad_send_wr;\r\nstruct list_head *list_item;\r\nunsigned long delay;\r\nmad_agent_priv = mad_send_wr->mad_agent_priv;\r\nlist_del(&mad_send_wr->agent_list);\r\ndelay = mad_send_wr->timeout;\r\nmad_send_wr->timeout += jiffies;\r\nif (delay) {\r\nlist_for_each_prev(list_item, &mad_agent_priv->wait_list) {\r\ntemp_mad_send_wr = list_entry(list_item,\r\nstruct ib_mad_send_wr_private,\r\nagent_list);\r\nif (time_after(mad_send_wr->timeout,\r\ntemp_mad_send_wr->timeout))\r\nbreak;\r\n}\r\n}\r\nelse\r\nlist_item = &mad_agent_priv->wait_list;\r\nlist_add(&mad_send_wr->agent_list, list_item);\r\nif (mad_agent_priv->wait_list.next == &mad_send_wr->agent_list)\r\nmod_delayed_work(mad_agent_priv->qp_info->port_priv->wq,\r\n&mad_agent_priv->timed_work, delay);\r\n}\r\nvoid ib_reset_mad_timeout(struct ib_mad_send_wr_private *mad_send_wr,\r\nint timeout_ms)\r\n{\r\nmad_send_wr->timeout = msecs_to_jiffies(timeout_ms);\r\nwait_for_response(mad_send_wr);\r\n}\r\nvoid ib_mad_complete_send_wr(struct ib_mad_send_wr_private *mad_send_wr,\r\nstruct ib_mad_send_wc *mad_send_wc)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nunsigned long flags;\r\nint ret;\r\nmad_agent_priv = mad_send_wr->mad_agent_priv;\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nif (mad_agent_priv->agent.rmpp_version) {\r\nret = ib_process_rmpp_send_wc(mad_send_wr, mad_send_wc);\r\nif (ret == IB_RMPP_RESULT_CONSUMED)\r\ngoto done;\r\n} else\r\nret = IB_RMPP_RESULT_UNHANDLED;\r\nif (mad_send_wc->status != IB_WC_SUCCESS &&\r\nmad_send_wr->status == IB_WC_SUCCESS) {\r\nmad_send_wr->status = mad_send_wc->status;\r\nmad_send_wr->refcount -= (mad_send_wr->timeout > 0);\r\n}\r\nif (--mad_send_wr->refcount > 0) {\r\nif (mad_send_wr->refcount == 1 && mad_send_wr->timeout &&\r\nmad_send_wr->status == IB_WC_SUCCESS) {\r\nwait_for_response(mad_send_wr);\r\n}\r\ngoto done;\r\n}\r\nlist_del(&mad_send_wr->agent_list);\r\nadjust_timeout(mad_agent_priv);\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nif (mad_send_wr->status != IB_WC_SUCCESS )\r\nmad_send_wc->status = mad_send_wr->status;\r\nif (ret == IB_RMPP_RESULT_INTERNAL)\r\nib_rmpp_send_handler(mad_send_wc);\r\nelse\r\nmad_agent_priv->agent.send_handler(&mad_agent_priv->agent,\r\nmad_send_wc);\r\nderef_mad_agent(mad_agent_priv);\r\nreturn;\r\ndone:\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\n}\r\nstatic void ib_mad_send_done_handler(struct ib_mad_port_private *port_priv,\r\nstruct ib_wc *wc)\r\n{\r\nstruct ib_mad_send_wr_private *mad_send_wr, *queued_send_wr;\r\nstruct ib_mad_list_head *mad_list;\r\nstruct ib_mad_qp_info *qp_info;\r\nstruct ib_mad_queue *send_queue;\r\nstruct ib_send_wr *bad_send_wr;\r\nstruct ib_mad_send_wc mad_send_wc;\r\nunsigned long flags;\r\nint ret;\r\nmad_list = (struct ib_mad_list_head *)(unsigned long)wc->wr_id;\r\nmad_send_wr = container_of(mad_list, struct ib_mad_send_wr_private,\r\nmad_list);\r\nsend_queue = mad_list->mad_queue;\r\nqp_info = send_queue->qp_info;\r\nretry:\r\nib_dma_unmap_single(mad_send_wr->send_buf.mad_agent->device,\r\nmad_send_wr->header_mapping,\r\nmad_send_wr->sg_list[0].length, DMA_TO_DEVICE);\r\nib_dma_unmap_single(mad_send_wr->send_buf.mad_agent->device,\r\nmad_send_wr->payload_mapping,\r\nmad_send_wr->sg_list[1].length, DMA_TO_DEVICE);\r\nqueued_send_wr = NULL;\r\nspin_lock_irqsave(&send_queue->lock, flags);\r\nlist_del(&mad_list->list);\r\nif (send_queue->count-- > send_queue->max_active) {\r\nmad_list = container_of(qp_info->overflow_list.next,\r\nstruct ib_mad_list_head, list);\r\nqueued_send_wr = container_of(mad_list,\r\nstruct ib_mad_send_wr_private,\r\nmad_list);\r\nlist_move_tail(&mad_list->list, &send_queue->list);\r\n}\r\nspin_unlock_irqrestore(&send_queue->lock, flags);\r\nmad_send_wc.send_buf = &mad_send_wr->send_buf;\r\nmad_send_wc.status = wc->status;\r\nmad_send_wc.vendor_err = wc->vendor_err;\r\nif (atomic_read(&qp_info->snoop_count))\r\nsnoop_send(qp_info, &mad_send_wr->send_buf, &mad_send_wc,\r\nIB_MAD_SNOOP_SEND_COMPLETIONS);\r\nib_mad_complete_send_wr(mad_send_wr, &mad_send_wc);\r\nif (queued_send_wr) {\r\nret = ib_post_send(qp_info->qp, &queued_send_wr->send_wr,\r\n&bad_send_wr);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "ib_post_send failed: %d\n", ret);\r\nmad_send_wr = queued_send_wr;\r\nwc->status = IB_WC_LOC_QP_OP_ERR;\r\ngoto retry;\r\n}\r\n}\r\n}\r\nstatic void mark_sends_for_retry(struct ib_mad_qp_info *qp_info)\r\n{\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nstruct ib_mad_list_head *mad_list;\r\nunsigned long flags;\r\nspin_lock_irqsave(&qp_info->send_queue.lock, flags);\r\nlist_for_each_entry(mad_list, &qp_info->send_queue.list, list) {\r\nmad_send_wr = container_of(mad_list,\r\nstruct ib_mad_send_wr_private,\r\nmad_list);\r\nmad_send_wr->retry = 1;\r\n}\r\nspin_unlock_irqrestore(&qp_info->send_queue.lock, flags);\r\n}\r\nstatic void mad_error_handler(struct ib_mad_port_private *port_priv,\r\nstruct ib_wc *wc)\r\n{\r\nstruct ib_mad_list_head *mad_list;\r\nstruct ib_mad_qp_info *qp_info;\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nint ret;\r\nmad_list = (struct ib_mad_list_head *)(unsigned long)wc->wr_id;\r\nqp_info = mad_list->mad_queue->qp_info;\r\nif (mad_list->mad_queue == &qp_info->recv_queue)\r\nreturn;\r\nmad_send_wr = container_of(mad_list, struct ib_mad_send_wr_private,\r\nmad_list);\r\nif (wc->status == IB_WC_WR_FLUSH_ERR) {\r\nif (mad_send_wr->retry) {\r\nstruct ib_send_wr *bad_send_wr;\r\nmad_send_wr->retry = 0;\r\nret = ib_post_send(qp_info->qp, &mad_send_wr->send_wr,\r\n&bad_send_wr);\r\nif (ret)\r\nib_mad_send_done_handler(port_priv, wc);\r\n} else\r\nib_mad_send_done_handler(port_priv, wc);\r\n} else {\r\nstruct ib_qp_attr *attr;\r\nattr = kmalloc(sizeof *attr, GFP_KERNEL);\r\nif (attr) {\r\nattr->qp_state = IB_QPS_RTS;\r\nattr->cur_qp_state = IB_QPS_SQE;\r\nret = ib_modify_qp(qp_info->qp, attr,\r\nIB_QP_STATE | IB_QP_CUR_STATE);\r\nkfree(attr);\r\nif (ret)\r\nprintk(KERN_ERR PFX "mad_error_handler - "\r\n"ib_modify_qp to RTS : %d\n", ret);\r\nelse\r\nmark_sends_for_retry(qp_info);\r\n}\r\nib_mad_send_done_handler(port_priv, wc);\r\n}\r\n}\r\nstatic void ib_mad_completion_handler(struct work_struct *work)\r\n{\r\nstruct ib_mad_port_private *port_priv;\r\nstruct ib_wc wc;\r\nport_priv = container_of(work, struct ib_mad_port_private, work);\r\nib_req_notify_cq(port_priv->cq, IB_CQ_NEXT_COMP);\r\nwhile (ib_poll_cq(port_priv->cq, 1, &wc) == 1) {\r\nif (wc.status == IB_WC_SUCCESS) {\r\nswitch (wc.opcode) {\r\ncase IB_WC_SEND:\r\nib_mad_send_done_handler(port_priv, &wc);\r\nbreak;\r\ncase IB_WC_RECV:\r\nib_mad_recv_done_handler(port_priv, &wc);\r\nbreak;\r\ndefault:\r\nBUG_ON(1);\r\nbreak;\r\n}\r\n} else\r\nmad_error_handler(port_priv, &wc);\r\n}\r\n}\r\nstatic void cancel_mads(struct ib_mad_agent_private *mad_agent_priv)\r\n{\r\nunsigned long flags;\r\nstruct ib_mad_send_wr_private *mad_send_wr, *temp_mad_send_wr;\r\nstruct ib_mad_send_wc mad_send_wc;\r\nstruct list_head cancel_list;\r\nINIT_LIST_HEAD(&cancel_list);\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nlist_for_each_entry_safe(mad_send_wr, temp_mad_send_wr,\r\n&mad_agent_priv->send_list, agent_list) {\r\nif (mad_send_wr->status == IB_WC_SUCCESS) {\r\nmad_send_wr->status = IB_WC_WR_FLUSH_ERR;\r\nmad_send_wr->refcount -= (mad_send_wr->timeout > 0);\r\n}\r\n}\r\nlist_splice_init(&mad_agent_priv->wait_list, &cancel_list);\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nmad_send_wc.status = IB_WC_WR_FLUSH_ERR;\r\nmad_send_wc.vendor_err = 0;\r\nlist_for_each_entry_safe(mad_send_wr, temp_mad_send_wr,\r\n&cancel_list, agent_list) {\r\nmad_send_wc.send_buf = &mad_send_wr->send_buf;\r\nlist_del(&mad_send_wr->agent_list);\r\nmad_agent_priv->agent.send_handler(&mad_agent_priv->agent,\r\n&mad_send_wc);\r\natomic_dec(&mad_agent_priv->refcount);\r\n}\r\n}\r\nstatic struct ib_mad_send_wr_private*\r\nfind_send_wr(struct ib_mad_agent_private *mad_agent_priv,\r\nstruct ib_mad_send_buf *send_buf)\r\n{\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nlist_for_each_entry(mad_send_wr, &mad_agent_priv->wait_list,\r\nagent_list) {\r\nif (&mad_send_wr->send_buf == send_buf)\r\nreturn mad_send_wr;\r\n}\r\nlist_for_each_entry(mad_send_wr, &mad_agent_priv->send_list,\r\nagent_list) {\r\nif (is_data_mad(mad_agent_priv, mad_send_wr->send_buf.mad) &&\r\n&mad_send_wr->send_buf == send_buf)\r\nreturn mad_send_wr;\r\n}\r\nreturn NULL;\r\n}\r\nint ib_modify_mad(struct ib_mad_agent *mad_agent,\r\nstruct ib_mad_send_buf *send_buf, u32 timeout_ms)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nunsigned long flags;\r\nint active;\r\nmad_agent_priv = container_of(mad_agent, struct ib_mad_agent_private,\r\nagent);\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nmad_send_wr = find_send_wr(mad_agent_priv, send_buf);\r\nif (!mad_send_wr || mad_send_wr->status != IB_WC_SUCCESS) {\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nreturn -EINVAL;\r\n}\r\nactive = (!mad_send_wr->timeout || mad_send_wr->refcount > 1);\r\nif (!timeout_ms) {\r\nmad_send_wr->status = IB_WC_WR_FLUSH_ERR;\r\nmad_send_wr->refcount -= (mad_send_wr->timeout > 0);\r\n}\r\nmad_send_wr->send_buf.timeout_ms = timeout_ms;\r\nif (active)\r\nmad_send_wr->timeout = msecs_to_jiffies(timeout_ms);\r\nelse\r\nib_reset_mad_timeout(mad_send_wr, timeout_ms);\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nreturn 0;\r\n}\r\nvoid ib_cancel_mad(struct ib_mad_agent *mad_agent,\r\nstruct ib_mad_send_buf *send_buf)\r\n{\r\nib_modify_mad(mad_agent, send_buf, 0);\r\n}\r\nstatic void local_completions(struct work_struct *work)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_local_private *local;\r\nstruct ib_mad_agent_private *recv_mad_agent;\r\nunsigned long flags;\r\nint free_mad;\r\nstruct ib_wc wc;\r\nstruct ib_mad_send_wc mad_send_wc;\r\nmad_agent_priv =\r\ncontainer_of(work, struct ib_mad_agent_private, local_work);\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nwhile (!list_empty(&mad_agent_priv->local_list)) {\r\nlocal = list_entry(mad_agent_priv->local_list.next,\r\nstruct ib_mad_local_private,\r\ncompletion_list);\r\nlist_del(&local->completion_list);\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nfree_mad = 0;\r\nif (local->mad_priv) {\r\nrecv_mad_agent = local->recv_mad_agent;\r\nif (!recv_mad_agent) {\r\nprintk(KERN_ERR PFX "No receive MAD agent for local completion\n");\r\nfree_mad = 1;\r\ngoto local_send_completion;\r\n}\r\nbuild_smp_wc(recv_mad_agent->agent.qp,\r\n(unsigned long) local->mad_send_wr,\r\nbe16_to_cpu(IB_LID_PERMISSIVE),\r\n0, recv_mad_agent->agent.port_num, &wc);\r\nlocal->mad_priv->header.recv_wc.wc = &wc;\r\nlocal->mad_priv->header.recv_wc.mad_len =\r\nsizeof(struct ib_mad);\r\nINIT_LIST_HEAD(&local->mad_priv->header.recv_wc.rmpp_list);\r\nlist_add(&local->mad_priv->header.recv_wc.recv_buf.list,\r\n&local->mad_priv->header.recv_wc.rmpp_list);\r\nlocal->mad_priv->header.recv_wc.recv_buf.grh = NULL;\r\nlocal->mad_priv->header.recv_wc.recv_buf.mad =\r\n&local->mad_priv->mad.mad;\r\nif (atomic_read(&recv_mad_agent->qp_info->snoop_count))\r\nsnoop_recv(recv_mad_agent->qp_info,\r\n&local->mad_priv->header.recv_wc,\r\nIB_MAD_SNOOP_RECVS);\r\nrecv_mad_agent->agent.recv_handler(\r\n&recv_mad_agent->agent,\r\n&local->mad_priv->header.recv_wc);\r\nspin_lock_irqsave(&recv_mad_agent->lock, flags);\r\natomic_dec(&recv_mad_agent->refcount);\r\nspin_unlock_irqrestore(&recv_mad_agent->lock, flags);\r\n}\r\nlocal_send_completion:\r\nmad_send_wc.status = IB_WC_SUCCESS;\r\nmad_send_wc.vendor_err = 0;\r\nmad_send_wc.send_buf = &local->mad_send_wr->send_buf;\r\nif (atomic_read(&mad_agent_priv->qp_info->snoop_count))\r\nsnoop_send(mad_agent_priv->qp_info,\r\n&local->mad_send_wr->send_buf,\r\n&mad_send_wc, IB_MAD_SNOOP_SEND_COMPLETIONS);\r\nmad_agent_priv->agent.send_handler(&mad_agent_priv->agent,\r\n&mad_send_wc);\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\natomic_dec(&mad_agent_priv->refcount);\r\nif (free_mad)\r\nkmem_cache_free(ib_mad_cache, local->mad_priv);\r\nkfree(local);\r\n}\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\n}\r\nstatic int retry_send(struct ib_mad_send_wr_private *mad_send_wr)\r\n{\r\nint ret;\r\nif (!mad_send_wr->retries_left)\r\nreturn -ETIMEDOUT;\r\nmad_send_wr->retries_left--;\r\nmad_send_wr->send_buf.retries++;\r\nmad_send_wr->timeout = msecs_to_jiffies(mad_send_wr->send_buf.timeout_ms);\r\nif (mad_send_wr->mad_agent_priv->agent.rmpp_version) {\r\nret = ib_retry_rmpp(mad_send_wr);\r\nswitch (ret) {\r\ncase IB_RMPP_RESULT_UNHANDLED:\r\nret = ib_send_mad(mad_send_wr);\r\nbreak;\r\ncase IB_RMPP_RESULT_CONSUMED:\r\nret = 0;\r\nbreak;\r\ndefault:\r\nret = -ECOMM;\r\nbreak;\r\n}\r\n} else\r\nret = ib_send_mad(mad_send_wr);\r\nif (!ret) {\r\nmad_send_wr->refcount++;\r\nlist_add_tail(&mad_send_wr->agent_list,\r\n&mad_send_wr->mad_agent_priv->send_list);\r\n}\r\nreturn ret;\r\n}\r\nstatic void timeout_sends(struct work_struct *work)\r\n{\r\nstruct ib_mad_agent_private *mad_agent_priv;\r\nstruct ib_mad_send_wr_private *mad_send_wr;\r\nstruct ib_mad_send_wc mad_send_wc;\r\nunsigned long flags, delay;\r\nmad_agent_priv = container_of(work, struct ib_mad_agent_private,\r\ntimed_work.work);\r\nmad_send_wc.vendor_err = 0;\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\nwhile (!list_empty(&mad_agent_priv->wait_list)) {\r\nmad_send_wr = list_entry(mad_agent_priv->wait_list.next,\r\nstruct ib_mad_send_wr_private,\r\nagent_list);\r\nif (time_after(mad_send_wr->timeout, jiffies)) {\r\ndelay = mad_send_wr->timeout - jiffies;\r\nif ((long)delay <= 0)\r\ndelay = 1;\r\nqueue_delayed_work(mad_agent_priv->qp_info->\r\nport_priv->wq,\r\n&mad_agent_priv->timed_work, delay);\r\nbreak;\r\n}\r\nlist_del(&mad_send_wr->agent_list);\r\nif (mad_send_wr->status == IB_WC_SUCCESS &&\r\n!retry_send(mad_send_wr))\r\ncontinue;\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\nif (mad_send_wr->status == IB_WC_SUCCESS)\r\nmad_send_wc.status = IB_WC_RESP_TIMEOUT_ERR;\r\nelse\r\nmad_send_wc.status = mad_send_wr->status;\r\nmad_send_wc.send_buf = &mad_send_wr->send_buf;\r\nmad_agent_priv->agent.send_handler(&mad_agent_priv->agent,\r\n&mad_send_wc);\r\natomic_dec(&mad_agent_priv->refcount);\r\nspin_lock_irqsave(&mad_agent_priv->lock, flags);\r\n}\r\nspin_unlock_irqrestore(&mad_agent_priv->lock, flags);\r\n}\r\nstatic void ib_mad_thread_completion_handler(struct ib_cq *cq, void *arg)\r\n{\r\nstruct ib_mad_port_private *port_priv = cq->cq_context;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ib_mad_port_list_lock, flags);\r\nif (!list_empty(&port_priv->port_list))\r\nqueue_work(port_priv->wq, &port_priv->work);\r\nspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\r\n}\r\nstatic int ib_mad_post_receive_mads(struct ib_mad_qp_info *qp_info,\r\nstruct ib_mad_private *mad)\r\n{\r\nunsigned long flags;\r\nint post, ret;\r\nstruct ib_mad_private *mad_priv;\r\nstruct ib_sge sg_list;\r\nstruct ib_recv_wr recv_wr, *bad_recv_wr;\r\nstruct ib_mad_queue *recv_queue = &qp_info->recv_queue;\r\nsg_list.length = sizeof *mad_priv - sizeof mad_priv->header;\r\nsg_list.lkey = (*qp_info->port_priv->mr).lkey;\r\nrecv_wr.next = NULL;\r\nrecv_wr.sg_list = &sg_list;\r\nrecv_wr.num_sge = 1;\r\ndo {\r\nif (mad) {\r\nmad_priv = mad;\r\nmad = NULL;\r\n} else {\r\nmad_priv = kmem_cache_alloc(ib_mad_cache, GFP_KERNEL);\r\nif (!mad_priv) {\r\nprintk(KERN_ERR PFX "No memory for receive buffer\n");\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\n}\r\nsg_list.addr = ib_dma_map_single(qp_info->port_priv->device,\r\n&mad_priv->grh,\r\nsizeof *mad_priv -\r\nsizeof mad_priv->header,\r\nDMA_FROM_DEVICE);\r\nmad_priv->header.mapping = sg_list.addr;\r\nrecv_wr.wr_id = (unsigned long)&mad_priv->header.mad_list;\r\nmad_priv->header.mad_list.mad_queue = recv_queue;\r\nspin_lock_irqsave(&recv_queue->lock, flags);\r\npost = (++recv_queue->count < recv_queue->max_active);\r\nlist_add_tail(&mad_priv->header.mad_list.list, &recv_queue->list);\r\nspin_unlock_irqrestore(&recv_queue->lock, flags);\r\nret = ib_post_recv(qp_info->qp, &recv_wr, &bad_recv_wr);\r\nif (ret) {\r\nspin_lock_irqsave(&recv_queue->lock, flags);\r\nlist_del(&mad_priv->header.mad_list.list);\r\nrecv_queue->count--;\r\nspin_unlock_irqrestore(&recv_queue->lock, flags);\r\nib_dma_unmap_single(qp_info->port_priv->device,\r\nmad_priv->header.mapping,\r\nsizeof *mad_priv -\r\nsizeof mad_priv->header,\r\nDMA_FROM_DEVICE);\r\nkmem_cache_free(ib_mad_cache, mad_priv);\r\nprintk(KERN_ERR PFX "ib_post_recv failed: %d\n", ret);\r\nbreak;\r\n}\r\n} while (post);\r\nreturn ret;\r\n}\r\nstatic void cleanup_recv_queue(struct ib_mad_qp_info *qp_info)\r\n{\r\nstruct ib_mad_private_header *mad_priv_hdr;\r\nstruct ib_mad_private *recv;\r\nstruct ib_mad_list_head *mad_list;\r\nif (!qp_info->qp)\r\nreturn;\r\nwhile (!list_empty(&qp_info->recv_queue.list)) {\r\nmad_list = list_entry(qp_info->recv_queue.list.next,\r\nstruct ib_mad_list_head, list);\r\nmad_priv_hdr = container_of(mad_list,\r\nstruct ib_mad_private_header,\r\nmad_list);\r\nrecv = container_of(mad_priv_hdr, struct ib_mad_private,\r\nheader);\r\nlist_del(&mad_list->list);\r\nib_dma_unmap_single(qp_info->port_priv->device,\r\nrecv->header.mapping,\r\nsizeof(struct ib_mad_private) -\r\nsizeof(struct ib_mad_private_header),\r\nDMA_FROM_DEVICE);\r\nkmem_cache_free(ib_mad_cache, recv);\r\n}\r\nqp_info->recv_queue.count = 0;\r\n}\r\nstatic int ib_mad_port_start(struct ib_mad_port_private *port_priv)\r\n{\r\nint ret, i;\r\nstruct ib_qp_attr *attr;\r\nstruct ib_qp *qp;\r\nattr = kmalloc(sizeof *attr, GFP_KERNEL);\r\nif (!attr) {\r\nprintk(KERN_ERR PFX "Couldn't kmalloc ib_qp_attr\n");\r\nreturn -ENOMEM;\r\n}\r\nfor (i = 0; i < IB_MAD_QPS_CORE; i++) {\r\nqp = port_priv->qp_info[i].qp;\r\nif (!qp)\r\ncontinue;\r\nattr->qp_state = IB_QPS_INIT;\r\nattr->pkey_index = 0;\r\nattr->qkey = (qp->qp_num == 0) ? 0 : IB_QP1_QKEY;\r\nret = ib_modify_qp(qp, attr, IB_QP_STATE |\r\nIB_QP_PKEY_INDEX | IB_QP_QKEY);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "Couldn't change QP%d state to "\r\n"INIT: %d\n", i, ret);\r\ngoto out;\r\n}\r\nattr->qp_state = IB_QPS_RTR;\r\nret = ib_modify_qp(qp, attr, IB_QP_STATE);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "Couldn't change QP%d state to "\r\n"RTR: %d\n", i, ret);\r\ngoto out;\r\n}\r\nattr->qp_state = IB_QPS_RTS;\r\nattr->sq_psn = IB_MAD_SEND_Q_PSN;\r\nret = ib_modify_qp(qp, attr, IB_QP_STATE | IB_QP_SQ_PSN);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "Couldn't change QP%d state to "\r\n"RTS: %d\n", i, ret);\r\ngoto out;\r\n}\r\n}\r\nret = ib_req_notify_cq(port_priv->cq, IB_CQ_NEXT_COMP);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "Failed to request completion "\r\n"notification: %d\n", ret);\r\ngoto out;\r\n}\r\nfor (i = 0; i < IB_MAD_QPS_CORE; i++) {\r\nif (!port_priv->qp_info[i].qp)\r\ncontinue;\r\nret = ib_mad_post_receive_mads(&port_priv->qp_info[i], NULL);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "Couldn't post receive WRs\n");\r\ngoto out;\r\n}\r\n}\r\nout:\r\nkfree(attr);\r\nreturn ret;\r\n}\r\nstatic void qp_event_handler(struct ib_event *event, void *qp_context)\r\n{\r\nstruct ib_mad_qp_info *qp_info = qp_context;\r\nprintk(KERN_ERR PFX "Fatal error (%d) on MAD QP (%d)\n",\r\nevent->event, qp_info->qp->qp_num);\r\n}\r\nstatic void init_mad_queue(struct ib_mad_qp_info *qp_info,\r\nstruct ib_mad_queue *mad_queue)\r\n{\r\nmad_queue->qp_info = qp_info;\r\nmad_queue->count = 0;\r\nspin_lock_init(&mad_queue->lock);\r\nINIT_LIST_HEAD(&mad_queue->list);\r\n}\r\nstatic void init_mad_qp(struct ib_mad_port_private *port_priv,\r\nstruct ib_mad_qp_info *qp_info)\r\n{\r\nqp_info->port_priv = port_priv;\r\ninit_mad_queue(qp_info, &qp_info->send_queue);\r\ninit_mad_queue(qp_info, &qp_info->recv_queue);\r\nINIT_LIST_HEAD(&qp_info->overflow_list);\r\nspin_lock_init(&qp_info->snoop_lock);\r\nqp_info->snoop_table = NULL;\r\nqp_info->snoop_table_size = 0;\r\natomic_set(&qp_info->snoop_count, 0);\r\n}\r\nstatic int create_mad_qp(struct ib_mad_qp_info *qp_info,\r\nenum ib_qp_type qp_type)\r\n{\r\nstruct ib_qp_init_attr qp_init_attr;\r\nint ret;\r\nmemset(&qp_init_attr, 0, sizeof qp_init_attr);\r\nqp_init_attr.send_cq = qp_info->port_priv->cq;\r\nqp_init_attr.recv_cq = qp_info->port_priv->cq;\r\nqp_init_attr.sq_sig_type = IB_SIGNAL_ALL_WR;\r\nqp_init_attr.cap.max_send_wr = mad_sendq_size;\r\nqp_init_attr.cap.max_recv_wr = mad_recvq_size;\r\nqp_init_attr.cap.max_send_sge = IB_MAD_SEND_REQ_MAX_SG;\r\nqp_init_attr.cap.max_recv_sge = IB_MAD_RECV_REQ_MAX_SG;\r\nqp_init_attr.qp_type = qp_type;\r\nqp_init_attr.port_num = qp_info->port_priv->port_num;\r\nqp_init_attr.qp_context = qp_info;\r\nqp_init_attr.event_handler = qp_event_handler;\r\nqp_info->qp = ib_create_qp(qp_info->port_priv->pd, &qp_init_attr);\r\nif (IS_ERR(qp_info->qp)) {\r\nprintk(KERN_ERR PFX "Couldn't create ib_mad QP%d\n",\r\nget_spl_qp_index(qp_type));\r\nret = PTR_ERR(qp_info->qp);\r\ngoto error;\r\n}\r\nqp_info->send_queue.max_active = mad_sendq_size;\r\nqp_info->recv_queue.max_active = mad_recvq_size;\r\nreturn 0;\r\nerror:\r\nreturn ret;\r\n}\r\nstatic void destroy_mad_qp(struct ib_mad_qp_info *qp_info)\r\n{\r\nif (!qp_info->qp)\r\nreturn;\r\nib_destroy_qp(qp_info->qp);\r\nkfree(qp_info->snoop_table);\r\n}\r\nstatic int ib_mad_port_open(struct ib_device *device,\r\nint port_num)\r\n{\r\nint ret, cq_size;\r\nstruct ib_mad_port_private *port_priv;\r\nunsigned long flags;\r\nchar name[sizeof "ib_mad123"];\r\nint has_smi;\r\nport_priv = kzalloc(sizeof *port_priv, GFP_KERNEL);\r\nif (!port_priv) {\r\nprintk(KERN_ERR PFX "No memory for ib_mad_port_private\n");\r\nreturn -ENOMEM;\r\n}\r\nport_priv->device = device;\r\nport_priv->port_num = port_num;\r\nspin_lock_init(&port_priv->reg_lock);\r\nINIT_LIST_HEAD(&port_priv->agent_list);\r\ninit_mad_qp(port_priv, &port_priv->qp_info[0]);\r\ninit_mad_qp(port_priv, &port_priv->qp_info[1]);\r\ncq_size = mad_sendq_size + mad_recvq_size;\r\nhas_smi = rdma_port_get_link_layer(device, port_num) == IB_LINK_LAYER_INFINIBAND;\r\nif (has_smi)\r\ncq_size *= 2;\r\nport_priv->cq = ib_create_cq(port_priv->device,\r\nib_mad_thread_completion_handler,\r\nNULL, port_priv, cq_size, 0);\r\nif (IS_ERR(port_priv->cq)) {\r\nprintk(KERN_ERR PFX "Couldn't create ib_mad CQ\n");\r\nret = PTR_ERR(port_priv->cq);\r\ngoto error3;\r\n}\r\nport_priv->pd = ib_alloc_pd(device);\r\nif (IS_ERR(port_priv->pd)) {\r\nprintk(KERN_ERR PFX "Couldn't create ib_mad PD\n");\r\nret = PTR_ERR(port_priv->pd);\r\ngoto error4;\r\n}\r\nport_priv->mr = ib_get_dma_mr(port_priv->pd, IB_ACCESS_LOCAL_WRITE);\r\nif (IS_ERR(port_priv->mr)) {\r\nprintk(KERN_ERR PFX "Couldn't get ib_mad DMA MR\n");\r\nret = PTR_ERR(port_priv->mr);\r\ngoto error5;\r\n}\r\nif (has_smi) {\r\nret = create_mad_qp(&port_priv->qp_info[0], IB_QPT_SMI);\r\nif (ret)\r\ngoto error6;\r\n}\r\nret = create_mad_qp(&port_priv->qp_info[1], IB_QPT_GSI);\r\nif (ret)\r\ngoto error7;\r\nsnprintf(name, sizeof name, "ib_mad%d", port_num);\r\nport_priv->wq = create_singlethread_workqueue(name);\r\nif (!port_priv->wq) {\r\nret = -ENOMEM;\r\ngoto error8;\r\n}\r\nINIT_WORK(&port_priv->work, ib_mad_completion_handler);\r\nspin_lock_irqsave(&ib_mad_port_list_lock, flags);\r\nlist_add_tail(&port_priv->port_list, &ib_mad_port_list);\r\nspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\r\nret = ib_mad_port_start(port_priv);\r\nif (ret) {\r\nprintk(KERN_ERR PFX "Couldn't start port\n");\r\ngoto error9;\r\n}\r\nreturn 0;\r\nerror9:\r\nspin_lock_irqsave(&ib_mad_port_list_lock, flags);\r\nlist_del_init(&port_priv->port_list);\r\nspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\r\ndestroy_workqueue(port_priv->wq);\r\nerror8:\r\ndestroy_mad_qp(&port_priv->qp_info[1]);\r\nerror7:\r\ndestroy_mad_qp(&port_priv->qp_info[0]);\r\nerror6:\r\nib_dereg_mr(port_priv->mr);\r\nerror5:\r\nib_dealloc_pd(port_priv->pd);\r\nerror4:\r\nib_destroy_cq(port_priv->cq);\r\ncleanup_recv_queue(&port_priv->qp_info[1]);\r\ncleanup_recv_queue(&port_priv->qp_info[0]);\r\nerror3:\r\nkfree(port_priv);\r\nreturn ret;\r\n}\r\nstatic int ib_mad_port_close(struct ib_device *device, int port_num)\r\n{\r\nstruct ib_mad_port_private *port_priv;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ib_mad_port_list_lock, flags);\r\nport_priv = __ib_get_mad_port(device, port_num);\r\nif (port_priv == NULL) {\r\nspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\r\nprintk(KERN_ERR PFX "Port %d not found\n", port_num);\r\nreturn -ENODEV;\r\n}\r\nlist_del_init(&port_priv->port_list);\r\nspin_unlock_irqrestore(&ib_mad_port_list_lock, flags);\r\ndestroy_workqueue(port_priv->wq);\r\ndestroy_mad_qp(&port_priv->qp_info[1]);\r\ndestroy_mad_qp(&port_priv->qp_info[0]);\r\nib_dereg_mr(port_priv->mr);\r\nib_dealloc_pd(port_priv->pd);\r\nib_destroy_cq(port_priv->cq);\r\ncleanup_recv_queue(&port_priv->qp_info[1]);\r\ncleanup_recv_queue(&port_priv->qp_info[0]);\r\nkfree(port_priv);\r\nreturn 0;\r\n}\r\nstatic void ib_mad_init_device(struct ib_device *device)\r\n{\r\nint start, end, i;\r\nif (rdma_node_get_transport(device->node_type) != RDMA_TRANSPORT_IB)\r\nreturn;\r\nif (device->node_type == RDMA_NODE_IB_SWITCH) {\r\nstart = 0;\r\nend = 0;\r\n} else {\r\nstart = 1;\r\nend = device->phys_port_cnt;\r\n}\r\nfor (i = start; i <= end; i++) {\r\nif (ib_mad_port_open(device, i)) {\r\nprintk(KERN_ERR PFX "Couldn't open %s port %d\n",\r\ndevice->name, i);\r\ngoto error;\r\n}\r\nif (ib_agent_port_open(device, i)) {\r\nprintk(KERN_ERR PFX "Couldn't open %s port %d "\r\n"for agents\n",\r\ndevice->name, i);\r\ngoto error_agent;\r\n}\r\n}\r\nreturn;\r\nerror_agent:\r\nif (ib_mad_port_close(device, i))\r\nprintk(KERN_ERR PFX "Couldn't close %s port %d\n",\r\ndevice->name, i);\r\nerror:\r\ni--;\r\nwhile (i >= start) {\r\nif (ib_agent_port_close(device, i))\r\nprintk(KERN_ERR PFX "Couldn't close %s port %d "\r\n"for agents\n",\r\ndevice->name, i);\r\nif (ib_mad_port_close(device, i))\r\nprintk(KERN_ERR PFX "Couldn't close %s port %d\n",\r\ndevice->name, i);\r\ni--;\r\n}\r\n}\r\nstatic void ib_mad_remove_device(struct ib_device *device)\r\n{\r\nint i, num_ports, cur_port;\r\nif (rdma_node_get_transport(device->node_type) != RDMA_TRANSPORT_IB)\r\nreturn;\r\nif (device->node_type == RDMA_NODE_IB_SWITCH) {\r\nnum_ports = 1;\r\ncur_port = 0;\r\n} else {\r\nnum_ports = device->phys_port_cnt;\r\ncur_port = 1;\r\n}\r\nfor (i = 0; i < num_ports; i++, cur_port++) {\r\nif (ib_agent_port_close(device, cur_port))\r\nprintk(KERN_ERR PFX "Couldn't close %s port %d "\r\n"for agents\n",\r\ndevice->name, cur_port);\r\nif (ib_mad_port_close(device, cur_port))\r\nprintk(KERN_ERR PFX "Couldn't close %s port %d\n",\r\ndevice->name, cur_port);\r\n}\r\n}\r\nstatic int __init ib_mad_init_module(void)\r\n{\r\nint ret;\r\nmad_recvq_size = min(mad_recvq_size, IB_MAD_QP_MAX_SIZE);\r\nmad_recvq_size = max(mad_recvq_size, IB_MAD_QP_MIN_SIZE);\r\nmad_sendq_size = min(mad_sendq_size, IB_MAD_QP_MAX_SIZE);\r\nmad_sendq_size = max(mad_sendq_size, IB_MAD_QP_MIN_SIZE);\r\nib_mad_cache = kmem_cache_create("ib_mad",\r\nsizeof(struct ib_mad_private),\r\n0,\r\nSLAB_HWCACHE_ALIGN,\r\nNULL);\r\nif (!ib_mad_cache) {\r\nprintk(KERN_ERR PFX "Couldn't create ib_mad cache\n");\r\nret = -ENOMEM;\r\ngoto error1;\r\n}\r\nINIT_LIST_HEAD(&ib_mad_port_list);\r\nif (ib_register_client(&mad_client)) {\r\nprintk(KERN_ERR PFX "Couldn't register ib_mad client\n");\r\nret = -EINVAL;\r\ngoto error2;\r\n}\r\nreturn 0;\r\nerror2:\r\nkmem_cache_destroy(ib_mad_cache);\r\nerror1:\r\nreturn ret;\r\n}\r\nstatic void __exit ib_mad_cleanup_module(void)\r\n{\r\nib_unregister_client(&mad_client);\r\nkmem_cache_destroy(ib_mad_cache);\r\n}
