static const char *get_state_string(enum mcast_group_state state)\r\n{\r\nswitch (state) {\r\ncase MCAST_IDLE:\r\nreturn "MCAST_IDLE";\r\ncase MCAST_JOIN_SENT:\r\nreturn "MCAST_JOIN_SENT";\r\ncase MCAST_LEAVE_SENT:\r\nreturn "MCAST_LEAVE_SENT";\r\ncase MCAST_RESP_READY:\r\nreturn "MCAST_RESP_READY";\r\n}\r\nreturn "Invalid State";\r\n}\r\nstatic struct mcast_group *mcast_find(struct mlx4_ib_demux_ctx *ctx,\r\nunion ib_gid *mgid)\r\n{\r\nstruct rb_node *node = ctx->mcg_table.rb_node;\r\nstruct mcast_group *group;\r\nint ret;\r\nwhile (node) {\r\ngroup = rb_entry(node, struct mcast_group, node);\r\nret = memcmp(mgid->raw, group->rec.mgid.raw, sizeof *mgid);\r\nif (!ret)\r\nreturn group;\r\nif (ret < 0)\r\nnode = node->rb_left;\r\nelse\r\nnode = node->rb_right;\r\n}\r\nreturn NULL;\r\n}\r\nstatic struct mcast_group *mcast_insert(struct mlx4_ib_demux_ctx *ctx,\r\nstruct mcast_group *group)\r\n{\r\nstruct rb_node **link = &ctx->mcg_table.rb_node;\r\nstruct rb_node *parent = NULL;\r\nstruct mcast_group *cur_group;\r\nint ret;\r\nwhile (*link) {\r\nparent = *link;\r\ncur_group = rb_entry(parent, struct mcast_group, node);\r\nret = memcmp(group->rec.mgid.raw, cur_group->rec.mgid.raw,\r\nsizeof group->rec.mgid);\r\nif (ret < 0)\r\nlink = &(*link)->rb_left;\r\nelse if (ret > 0)\r\nlink = &(*link)->rb_right;\r\nelse\r\nreturn cur_group;\r\n}\r\nrb_link_node(&group->node, parent, link);\r\nrb_insert_color(&group->node, &ctx->mcg_table);\r\nreturn NULL;\r\n}\r\nstatic int send_mad_to_wire(struct mlx4_ib_demux_ctx *ctx, struct ib_mad *mad)\r\n{\r\nstruct mlx4_ib_dev *dev = ctx->dev;\r\nstruct ib_ah_attr ah_attr;\r\nspin_lock(&dev->sm_lock);\r\nif (!dev->sm_ah[ctx->port - 1]) {\r\nspin_unlock(&dev->sm_lock);\r\nreturn -EAGAIN;\r\n}\r\nmlx4_ib_query_ah(dev->sm_ah[ctx->port - 1], &ah_attr);\r\nspin_unlock(&dev->sm_lock);\r\nreturn mlx4_ib_send_to_wire(dev, mlx4_master_func_num(dev->dev), ctx->port,\r\nIB_QPT_GSI, 0, 1, IB_QP1_QKEY, &ah_attr, mad);\r\n}\r\nstatic int send_mad_to_slave(int slave, struct mlx4_ib_demux_ctx *ctx,\r\nstruct ib_mad *mad)\r\n{\r\nstruct mlx4_ib_dev *dev = ctx->dev;\r\nstruct ib_mad_agent *agent = dev->send_agent[ctx->port - 1][1];\r\nstruct ib_wc wc;\r\nstruct ib_ah_attr ah_attr;\r\nif (!agent)\r\nreturn -EAGAIN;\r\nib_query_ah(dev->sm_ah[ctx->port - 1], &ah_attr);\r\nif (ib_find_cached_pkey(&dev->ib_dev, ctx->port, IB_DEFAULT_PKEY_FULL, &wc.pkey_index))\r\nreturn -EINVAL;\r\nwc.sl = 0;\r\nwc.dlid_path_bits = 0;\r\nwc.port_num = ctx->port;\r\nwc.slid = ah_attr.dlid;\r\nwc.src_qp = 1;\r\nreturn mlx4_ib_send_to_slave(dev, slave, ctx->port, IB_QPT_GSI, &wc, NULL, mad);\r\n}\r\nstatic int send_join_to_wire(struct mcast_group *group, struct ib_sa_mad *sa_mad)\r\n{\r\nstruct ib_sa_mad mad;\r\nstruct ib_sa_mcmember_data *sa_mad_data = (struct ib_sa_mcmember_data *)&mad.data;\r\nint ret;\r\nmemcpy(&mad, sa_mad, sizeof mad);\r\nsa_mad_data->port_gid.global.interface_id = group->demux->guid_cache[0];\r\nmad.mad_hdr.tid = mlx4_ib_get_new_demux_tid(group->demux);\r\ngroup->last_req_tid = mad.mad_hdr.tid;\r\nret = send_mad_to_wire(group->demux, (struct ib_mad *)&mad);\r\nif (!ret) {\r\nqueue_delayed_work(group->demux->mcg_wq, &group->timeout_work,\r\nmsecs_to_jiffies(MAD_TIMEOUT_MS));\r\n}\r\nreturn ret;\r\n}\r\nstatic int send_leave_to_wire(struct mcast_group *group, u8 join_state)\r\n{\r\nstruct ib_sa_mad mad;\r\nstruct ib_sa_mcmember_data *sa_data = (struct ib_sa_mcmember_data *)&mad.data;\r\nint ret;\r\nmemset(&mad, 0, sizeof mad);\r\nmad.mad_hdr.base_version = 1;\r\nmad.mad_hdr.mgmt_class = IB_MGMT_CLASS_SUBN_ADM;\r\nmad.mad_hdr.class_version = 2;\r\nmad.mad_hdr.method = IB_SA_METHOD_DELETE;\r\nmad.mad_hdr.status = cpu_to_be16(0);\r\nmad.mad_hdr.class_specific = cpu_to_be16(0);\r\nmad.mad_hdr.tid = mlx4_ib_get_new_demux_tid(group->demux);\r\ngroup->last_req_tid = mad.mad_hdr.tid;\r\nmad.mad_hdr.attr_id = cpu_to_be16(IB_SA_ATTR_MC_MEMBER_REC);\r\nmad.mad_hdr.attr_mod = cpu_to_be32(0);\r\nmad.sa_hdr.sm_key = 0x0;\r\nmad.sa_hdr.attr_offset = cpu_to_be16(7);\r\nmad.sa_hdr.comp_mask = IB_SA_MCMEMBER_REC_MGID |\r\nIB_SA_MCMEMBER_REC_PORT_GID | IB_SA_MCMEMBER_REC_JOIN_STATE;\r\n*sa_data = group->rec;\r\nsa_data->scope_join_state = join_state;\r\nret = send_mad_to_wire(group->demux, (struct ib_mad *)&mad);\r\nif (ret)\r\ngroup->state = MCAST_IDLE;\r\nif (!ret) {\r\nqueue_delayed_work(group->demux->mcg_wq, &group->timeout_work,\r\nmsecs_to_jiffies(MAD_TIMEOUT_MS));\r\n}\r\nreturn ret;\r\n}\r\nstatic int send_reply_to_slave(int slave, struct mcast_group *group,\r\nstruct ib_sa_mad *req_sa_mad, u16 status)\r\n{\r\nstruct ib_sa_mad mad;\r\nstruct ib_sa_mcmember_data *sa_data = (struct ib_sa_mcmember_data *)&mad.data;\r\nstruct ib_sa_mcmember_data *req_sa_data = (struct ib_sa_mcmember_data *)&req_sa_mad->data;\r\nint ret;\r\nmemset(&mad, 0, sizeof mad);\r\nmad.mad_hdr.base_version = 1;\r\nmad.mad_hdr.mgmt_class = IB_MGMT_CLASS_SUBN_ADM;\r\nmad.mad_hdr.class_version = 2;\r\nmad.mad_hdr.method = IB_MGMT_METHOD_GET_RESP;\r\nmad.mad_hdr.status = cpu_to_be16(status);\r\nmad.mad_hdr.class_specific = cpu_to_be16(0);\r\nmad.mad_hdr.tid = req_sa_mad->mad_hdr.tid;\r\n*(u8 *)&mad.mad_hdr.tid = 0;\r\nmad.mad_hdr.attr_id = cpu_to_be16(IB_SA_ATTR_MC_MEMBER_REC);\r\nmad.mad_hdr.attr_mod = cpu_to_be32(0);\r\nmad.sa_hdr.sm_key = req_sa_mad->sa_hdr.sm_key;\r\nmad.sa_hdr.attr_offset = cpu_to_be16(7);\r\nmad.sa_hdr.comp_mask = 0;\r\n*sa_data = group->rec;\r\nsa_data->scope_join_state &= 0xf0;\r\nsa_data->scope_join_state |= (group->func[slave].join_state & 0x0f);\r\nmemcpy(&sa_data->port_gid, &req_sa_data->port_gid, sizeof req_sa_data->port_gid);\r\nret = send_mad_to_slave(slave, group->demux, (struct ib_mad *)&mad);\r\nreturn ret;\r\n}\r\nstatic int check_selector(ib_sa_comp_mask comp_mask,\r\nib_sa_comp_mask selector_mask,\r\nib_sa_comp_mask value_mask,\r\nu8 src_value, u8 dst_value)\r\n{\r\nint err;\r\nu8 selector = dst_value >> 6;\r\ndst_value &= 0x3f;\r\nsrc_value &= 0x3f;\r\nif (!(comp_mask & selector_mask) || !(comp_mask & value_mask))\r\nreturn 0;\r\nswitch (selector) {\r\ncase IB_SA_GT:\r\nerr = (src_value <= dst_value);\r\nbreak;\r\ncase IB_SA_LT:\r\nerr = (src_value >= dst_value);\r\nbreak;\r\ncase IB_SA_EQ:\r\nerr = (src_value != dst_value);\r\nbreak;\r\ndefault:\r\nerr = 0;\r\nbreak;\r\n}\r\nreturn err;\r\n}\r\nstatic u16 cmp_rec(struct ib_sa_mcmember_data *src,\r\nstruct ib_sa_mcmember_data *dst, ib_sa_comp_mask comp_mask)\r\n{\r\n#define MAD_STATUS_REQ_INVALID 0x0200\r\nif (comp_mask & IB_SA_MCMEMBER_REC_QKEY && src->qkey != dst->qkey)\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (comp_mask & IB_SA_MCMEMBER_REC_MLID && src->mlid != dst->mlid)\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (check_selector(comp_mask, IB_SA_MCMEMBER_REC_MTU_SELECTOR,\r\nIB_SA_MCMEMBER_REC_MTU,\r\nsrc->mtusel_mtu, dst->mtusel_mtu))\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (comp_mask & IB_SA_MCMEMBER_REC_TRAFFIC_CLASS &&\r\nsrc->tclass != dst->tclass)\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (comp_mask & IB_SA_MCMEMBER_REC_PKEY && src->pkey != dst->pkey)\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (check_selector(comp_mask, IB_SA_MCMEMBER_REC_RATE_SELECTOR,\r\nIB_SA_MCMEMBER_REC_RATE,\r\nsrc->ratesel_rate, dst->ratesel_rate))\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (check_selector(comp_mask,\r\nIB_SA_MCMEMBER_REC_PACKET_LIFE_TIME_SELECTOR,\r\nIB_SA_MCMEMBER_REC_PACKET_LIFE_TIME,\r\nsrc->lifetmsel_lifetm, dst->lifetmsel_lifetm))\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (comp_mask & IB_SA_MCMEMBER_REC_SL &&\r\n(be32_to_cpu(src->sl_flowlabel_hoplimit) & 0xf0000000) !=\r\n(be32_to_cpu(dst->sl_flowlabel_hoplimit) & 0xf0000000))\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (comp_mask & IB_SA_MCMEMBER_REC_FLOW_LABEL &&\r\n(be32_to_cpu(src->sl_flowlabel_hoplimit) & 0x0fffff00) !=\r\n(be32_to_cpu(dst->sl_flowlabel_hoplimit) & 0x0fffff00))\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (comp_mask & IB_SA_MCMEMBER_REC_HOP_LIMIT &&\r\n(be32_to_cpu(src->sl_flowlabel_hoplimit) & 0x000000ff) !=\r\n(be32_to_cpu(dst->sl_flowlabel_hoplimit) & 0x000000ff))\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (comp_mask & IB_SA_MCMEMBER_REC_SCOPE &&\r\n(src->scope_join_state & 0xf0) !=\r\n(dst->scope_join_state & 0xf0))\r\nreturn MAD_STATUS_REQ_INVALID;\r\nreturn 0;\r\n}\r\nstatic int release_group(struct mcast_group *group, int from_timeout_handler)\r\n{\r\nstruct mlx4_ib_demux_ctx *ctx = group->demux;\r\nint nzgroup;\r\nmutex_lock(&ctx->mcg_table_lock);\r\nmutex_lock(&group->lock);\r\nif (atomic_dec_and_test(&group->refcount)) {\r\nif (!from_timeout_handler) {\r\nif (group->state != MCAST_IDLE &&\r\n!cancel_delayed_work(&group->timeout_work)) {\r\natomic_inc(&group->refcount);\r\nmutex_unlock(&group->lock);\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nreturn 0;\r\n}\r\n}\r\nnzgroup = memcmp(&group->rec.mgid, &mgid0, sizeof mgid0);\r\nif (nzgroup)\r\ndel_sysfs_port_mcg_attr(ctx->dev, ctx->port, &group->dentry.attr);\r\nif (!list_empty(&group->pending_list))\r\nmcg_warn_group(group, "releasing a group with non empty pending list\n");\r\nif (nzgroup)\r\nrb_erase(&group->node, &ctx->mcg_table);\r\nlist_del_init(&group->mgid0_list);\r\nmutex_unlock(&group->lock);\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nkfree(group);\r\nreturn 1;\r\n} else {\r\nmutex_unlock(&group->lock);\r\nmutex_unlock(&ctx->mcg_table_lock);\r\n}\r\nreturn 0;\r\n}\r\nstatic void adjust_membership(struct mcast_group *group, u8 join_state, int inc)\r\n{\r\nint i;\r\nfor (i = 0; i < 3; i++, join_state >>= 1)\r\nif (join_state & 0x1)\r\ngroup->members[i] += inc;\r\n}\r\nstatic u8 get_leave_state(struct mcast_group *group)\r\n{\r\nu8 leave_state = 0;\r\nint i;\r\nfor (i = 0; i < 3; i++)\r\nif (!group->members[i])\r\nleave_state |= (1 << i);\r\nreturn leave_state & (group->rec.scope_join_state & 7);\r\n}\r\nstatic int join_group(struct mcast_group *group, int slave, u8 join_mask)\r\n{\r\nint ret = 0;\r\nu8 join_state;\r\njoin_state = join_mask & (~group->func[slave].join_state);\r\nadjust_membership(group, join_state, 1);\r\ngroup->func[slave].join_state |= join_state;\r\nif (group->func[slave].state != MCAST_MEMBER && join_state) {\r\ngroup->func[slave].state = MCAST_MEMBER;\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic int leave_group(struct mcast_group *group, int slave, u8 leave_state)\r\n{\r\nint ret = 0;\r\nadjust_membership(group, leave_state, -1);\r\ngroup->func[slave].join_state &= ~leave_state;\r\nif (!group->func[slave].join_state) {\r\ngroup->func[slave].state = MCAST_NOT_MEMBER;\r\nret = 1;\r\n}\r\nreturn ret;\r\n}\r\nstatic int check_leave(struct mcast_group *group, int slave, u8 leave_mask)\r\n{\r\nif (group->func[slave].state != MCAST_MEMBER)\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (~group->func[slave].join_state & leave_mask)\r\nreturn MAD_STATUS_REQ_INVALID;\r\nif (!leave_mask)\r\nreturn MAD_STATUS_REQ_INVALID;\r\nreturn 0;\r\n}\r\nstatic void mlx4_ib_mcg_timeout_handler(struct work_struct *work)\r\n{\r\nstruct delayed_work *delay = to_delayed_work(work);\r\nstruct mcast_group *group;\r\nstruct mcast_req *req = NULL;\r\ngroup = container_of(delay, typeof(*group), timeout_work);\r\nmutex_lock(&group->lock);\r\nif (group->state == MCAST_JOIN_SENT) {\r\nif (!list_empty(&group->pending_list)) {\r\nreq = list_first_entry(&group->pending_list, struct mcast_req, group_list);\r\nlist_del(&req->group_list);\r\nlist_del(&req->func_list);\r\n--group->func[req->func].num_pend_reqs;\r\nmutex_unlock(&group->lock);\r\nkfree(req);\r\nif (memcmp(&group->rec.mgid, &mgid0, sizeof mgid0)) {\r\nif (release_group(group, 1))\r\nreturn;\r\n} else {\r\nkfree(group);\r\nreturn;\r\n}\r\nmutex_lock(&group->lock);\r\n} else\r\nmcg_warn_group(group, "DRIVER BUG\n");\r\n} else if (group->state == MCAST_LEAVE_SENT) {\r\nif (group->rec.scope_join_state & 7)\r\ngroup->rec.scope_join_state &= 0xf8;\r\ngroup->state = MCAST_IDLE;\r\nmutex_unlock(&group->lock);\r\nif (release_group(group, 1))\r\nreturn;\r\nmutex_lock(&group->lock);\r\n} else\r\nmcg_warn_group(group, "invalid state %s\n", get_state_string(group->state));\r\ngroup->state = MCAST_IDLE;\r\natomic_inc(&group->refcount);\r\nif (!queue_work(group->demux->mcg_wq, &group->work))\r\nsafe_atomic_dec(&group->refcount);\r\nmutex_unlock(&group->lock);\r\n}\r\nstatic int handle_leave_req(struct mcast_group *group, u8 leave_mask,\r\nstruct mcast_req *req)\r\n{\r\nu16 status;\r\nif (req->clean)\r\nleave_mask = group->func[req->func].join_state;\r\nstatus = check_leave(group, req->func, leave_mask);\r\nif (!status)\r\nleave_group(group, req->func, leave_mask);\r\nif (!req->clean)\r\nsend_reply_to_slave(req->func, group, &req->sa_mad, status);\r\n--group->func[req->func].num_pend_reqs;\r\nlist_del(&req->group_list);\r\nlist_del(&req->func_list);\r\nkfree(req);\r\nreturn 1;\r\n}\r\nstatic int handle_join_req(struct mcast_group *group, u8 join_mask,\r\nstruct mcast_req *req)\r\n{\r\nu8 group_join_state = group->rec.scope_join_state & 7;\r\nint ref = 0;\r\nu16 status;\r\nstruct ib_sa_mcmember_data *sa_data = (struct ib_sa_mcmember_data *)req->sa_mad.data;\r\nif (join_mask == (group_join_state & join_mask)) {\r\nstatus = cmp_rec(&group->rec, sa_data, req->sa_mad.sa_hdr.comp_mask);\r\nif (!status)\r\njoin_group(group, req->func, join_mask);\r\n--group->func[req->func].num_pend_reqs;\r\nsend_reply_to_slave(req->func, group, &req->sa_mad, status);\r\nlist_del(&req->group_list);\r\nlist_del(&req->func_list);\r\nkfree(req);\r\n++ref;\r\n} else {\r\ngroup->prev_state = group->state;\r\nif (send_join_to_wire(group, &req->sa_mad)) {\r\n--group->func[req->func].num_pend_reqs;\r\nlist_del(&req->group_list);\r\nlist_del(&req->func_list);\r\nkfree(req);\r\nref = 1;\r\ngroup->state = group->prev_state;\r\n} else\r\ngroup->state = MCAST_JOIN_SENT;\r\n}\r\nreturn ref;\r\n}\r\nstatic void mlx4_ib_mcg_work_handler(struct work_struct *work)\r\n{\r\nstruct mcast_group *group;\r\nstruct mcast_req *req = NULL;\r\nstruct ib_sa_mcmember_data *sa_data;\r\nu8 req_join_state;\r\nint rc = 1;\r\nu16 status;\r\nu8 method;\r\ngroup = container_of(work, typeof(*group), work);\r\nmutex_lock(&group->lock);\r\nif (group->state == MCAST_RESP_READY) {\r\ncancel_delayed_work(&group->timeout_work);\r\nstatus = be16_to_cpu(group->response_sa_mad.mad_hdr.status);\r\nmethod = group->response_sa_mad.mad_hdr.method;\r\nif (group->last_req_tid != group->response_sa_mad.mad_hdr.tid) {\r\nmcg_warn_group(group, "Got MAD response to existing MGID but wrong TID, dropping. Resp TID=%llx, group TID=%llx\n",\r\nbe64_to_cpu(group->response_sa_mad.mad_hdr.tid),\r\nbe64_to_cpu(group->last_req_tid));\r\ngroup->state = group->prev_state;\r\ngoto process_requests;\r\n}\r\nif (status) {\r\nif (!list_empty(&group->pending_list))\r\nreq = list_first_entry(&group->pending_list,\r\nstruct mcast_req, group_list);\r\nif ((method == IB_MGMT_METHOD_GET_RESP)) {\r\nif (req) {\r\nsend_reply_to_slave(req->func, group, &req->sa_mad, status);\r\n--group->func[req->func].num_pend_reqs;\r\nlist_del(&req->group_list);\r\nlist_del(&req->func_list);\r\nkfree(req);\r\n++rc;\r\n} else\r\nmcg_warn_group(group, "no request for failed join\n");\r\n} else if (method == IB_SA_METHOD_DELETE_RESP && group->demux->flushing)\r\n++rc;\r\n} else {\r\nu8 resp_join_state;\r\nu8 cur_join_state;\r\nresp_join_state = ((struct ib_sa_mcmember_data *)\r\ngroup->response_sa_mad.data)->scope_join_state & 7;\r\ncur_join_state = group->rec.scope_join_state & 7;\r\nif (method == IB_MGMT_METHOD_GET_RESP) {\r\nif (!cur_join_state && resp_join_state)\r\n--rc;\r\n} else if (!resp_join_state)\r\n++rc;\r\nmemcpy(&group->rec, group->response_sa_mad.data, sizeof group->rec);\r\n}\r\ngroup->state = MCAST_IDLE;\r\n}\r\nprocess_requests:\r\nwhile (!list_empty(&group->pending_list) && group->state == MCAST_IDLE) {\r\nreq = list_first_entry(&group->pending_list, struct mcast_req,\r\ngroup_list);\r\nsa_data = (struct ib_sa_mcmember_data *)req->sa_mad.data;\r\nreq_join_state = sa_data->scope_join_state & 0x7;\r\nif (req->sa_mad.mad_hdr.method == IB_SA_METHOD_DELETE)\r\nrc += handle_leave_req(group, req_join_state, req);\r\nelse\r\nrc += handle_join_req(group, req_join_state, req);\r\n}\r\nif (group->state == MCAST_IDLE) {\r\nreq_join_state = get_leave_state(group);\r\nif (req_join_state) {\r\ngroup->rec.scope_join_state &= ~req_join_state;\r\ngroup->prev_state = group->state;\r\nif (send_leave_to_wire(group, req_join_state)) {\r\ngroup->state = group->prev_state;\r\n++rc;\r\n} else\r\ngroup->state = MCAST_LEAVE_SENT;\r\n}\r\n}\r\nif (!list_empty(&group->pending_list) && group->state == MCAST_IDLE)\r\ngoto process_requests;\r\nmutex_unlock(&group->lock);\r\nwhile (rc--)\r\nrelease_group(group, 0);\r\n}\r\nstatic struct mcast_group *search_relocate_mgid0_group(struct mlx4_ib_demux_ctx *ctx,\r\n__be64 tid,\r\nunion ib_gid *new_mgid)\r\n{\r\nstruct mcast_group *group = NULL, *cur_group;\r\nstruct mcast_req *req;\r\nstruct list_head *pos;\r\nstruct list_head *n;\r\nmutex_lock(&ctx->mcg_table_lock);\r\nlist_for_each_safe(pos, n, &ctx->mcg_mgid0_list) {\r\ngroup = list_entry(pos, struct mcast_group, mgid0_list);\r\nmutex_lock(&group->lock);\r\nif (group->last_req_tid == tid) {\r\nif (memcmp(new_mgid, &mgid0, sizeof mgid0)) {\r\ngroup->rec.mgid = *new_mgid;\r\nsprintf(group->name, "%016llx%016llx",\r\nbe64_to_cpu(group->rec.mgid.global.subnet_prefix),\r\nbe64_to_cpu(group->rec.mgid.global.interface_id));\r\nlist_del_init(&group->mgid0_list);\r\ncur_group = mcast_insert(ctx, group);\r\nif (cur_group) {\r\nreq = list_first_entry(&group->pending_list,\r\nstruct mcast_req, group_list);\r\n--group->func[req->func].num_pend_reqs;\r\nlist_del(&req->group_list);\r\nlist_del(&req->func_list);\r\nkfree(req);\r\nmutex_unlock(&group->lock);\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nrelease_group(group, 0);\r\nreturn NULL;\r\n}\r\natomic_inc(&group->refcount);\r\nadd_sysfs_port_mcg_attr(ctx->dev, ctx->port, &group->dentry.attr);\r\nmutex_unlock(&group->lock);\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nreturn group;\r\n} else {\r\nstruct mcast_req *tmp1, *tmp2;\r\nlist_del(&group->mgid0_list);\r\nif (!list_empty(&group->pending_list) && group->state != MCAST_IDLE)\r\ncancel_delayed_work_sync(&group->timeout_work);\r\nlist_for_each_entry_safe(tmp1, tmp2, &group->pending_list, group_list) {\r\nlist_del(&tmp1->group_list);\r\nkfree(tmp1);\r\n}\r\nmutex_unlock(&group->lock);\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nkfree(group);\r\nreturn NULL;\r\n}\r\n}\r\nmutex_unlock(&group->lock);\r\n}\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nreturn NULL;\r\n}\r\nstatic struct mcast_group *acquire_group(struct mlx4_ib_demux_ctx *ctx,\r\nunion ib_gid *mgid, int create,\r\ngfp_t gfp_mask)\r\n{\r\nstruct mcast_group *group, *cur_group;\r\nint is_mgid0;\r\nint i;\r\nis_mgid0 = !memcmp(&mgid0, mgid, sizeof mgid0);\r\nif (!is_mgid0) {\r\ngroup = mcast_find(ctx, mgid);\r\nif (group)\r\ngoto found;\r\n}\r\nif (!create)\r\nreturn ERR_PTR(-ENOENT);\r\ngroup = kzalloc(sizeof *group, gfp_mask);\r\nif (!group)\r\nreturn ERR_PTR(-ENOMEM);\r\ngroup->demux = ctx;\r\ngroup->rec.mgid = *mgid;\r\nINIT_LIST_HEAD(&group->pending_list);\r\nINIT_LIST_HEAD(&group->mgid0_list);\r\nfor (i = 0; i < MAX_VFS; ++i)\r\nINIT_LIST_HEAD(&group->func[i].pending);\r\nINIT_WORK(&group->work, mlx4_ib_mcg_work_handler);\r\nINIT_DELAYED_WORK(&group->timeout_work, mlx4_ib_mcg_timeout_handler);\r\nmutex_init(&group->lock);\r\nsprintf(group->name, "%016llx%016llx",\r\nbe64_to_cpu(group->rec.mgid.global.subnet_prefix),\r\nbe64_to_cpu(group->rec.mgid.global.interface_id));\r\nsysfs_attr_init(&group->dentry.attr);\r\ngroup->dentry.show = sysfs_show_group;\r\ngroup->dentry.store = NULL;\r\ngroup->dentry.attr.name = group->name;\r\ngroup->dentry.attr.mode = 0400;\r\ngroup->state = MCAST_IDLE;\r\nif (is_mgid0) {\r\nlist_add(&group->mgid0_list, &ctx->mcg_mgid0_list);\r\ngoto found;\r\n}\r\ncur_group = mcast_insert(ctx, group);\r\nif (cur_group) {\r\nmcg_warn("group just showed up %s - confused\n", cur_group->name);\r\nkfree(group);\r\nreturn ERR_PTR(-EINVAL);\r\n}\r\nadd_sysfs_port_mcg_attr(ctx->dev, ctx->port, &group->dentry.attr);\r\nfound:\r\natomic_inc(&group->refcount);\r\nreturn group;\r\n}\r\nstatic void queue_req(struct mcast_req *req)\r\n{\r\nstruct mcast_group *group = req->group;\r\natomic_inc(&group->refcount);\r\natomic_inc(&group->refcount);\r\nlist_add_tail(&req->group_list, &group->pending_list);\r\nlist_add_tail(&req->func_list, &group->func[req->func].pending);\r\nif (!queue_work(group->demux->mcg_wq, &group->work))\r\nsafe_atomic_dec(&group->refcount);\r\n}\r\nint mlx4_ib_mcg_demux_handler(struct ib_device *ibdev, int port, int slave,\r\nstruct ib_sa_mad *mad)\r\n{\r\nstruct mlx4_ib_dev *dev = to_mdev(ibdev);\r\nstruct ib_sa_mcmember_data *rec = (struct ib_sa_mcmember_data *)mad->data;\r\nstruct mlx4_ib_demux_ctx *ctx = &dev->sriov.demux[port - 1];\r\nstruct mcast_group *group;\r\nswitch (mad->mad_hdr.method) {\r\ncase IB_MGMT_METHOD_GET_RESP:\r\ncase IB_SA_METHOD_DELETE_RESP:\r\nmutex_lock(&ctx->mcg_table_lock);\r\ngroup = acquire_group(ctx, &rec->mgid, 0, GFP_KERNEL);\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nif (IS_ERR(group)) {\r\nif (mad->mad_hdr.method == IB_MGMT_METHOD_GET_RESP) {\r\n__be64 tid = mad->mad_hdr.tid;\r\n*(u8 *)(&tid) = (u8)slave;\r\ngroup = search_relocate_mgid0_group(ctx, tid, &rec->mgid);\r\n} else\r\ngroup = NULL;\r\n}\r\nif (!group)\r\nreturn 1;\r\nmutex_lock(&group->lock);\r\ngroup->response_sa_mad = *mad;\r\ngroup->prev_state = group->state;\r\ngroup->state = MCAST_RESP_READY;\r\natomic_inc(&group->refcount);\r\nif (!queue_work(ctx->mcg_wq, &group->work))\r\nsafe_atomic_dec(&group->refcount);\r\nmutex_unlock(&group->lock);\r\nrelease_group(group, 0);\r\nreturn 1;\r\ncase IB_MGMT_METHOD_SET:\r\ncase IB_SA_METHOD_GET_TABLE:\r\ncase IB_SA_METHOD_GET_TABLE_RESP:\r\ncase IB_SA_METHOD_DELETE:\r\nreturn 0;\r\ndefault:\r\nmcg_warn("In demux, port %d: unexpected MCMember method: 0x%x, dropping\n",\r\nport, mad->mad_hdr.method);\r\nreturn 1;\r\n}\r\n}\r\nint mlx4_ib_mcg_multiplex_handler(struct ib_device *ibdev, int port,\r\nint slave, struct ib_sa_mad *sa_mad)\r\n{\r\nstruct mlx4_ib_dev *dev = to_mdev(ibdev);\r\nstruct ib_sa_mcmember_data *rec = (struct ib_sa_mcmember_data *)sa_mad->data;\r\nstruct mlx4_ib_demux_ctx *ctx = &dev->sriov.demux[port - 1];\r\nstruct mcast_group *group;\r\nstruct mcast_req *req;\r\nint may_create = 0;\r\nif (ctx->flushing)\r\nreturn -EAGAIN;\r\nswitch (sa_mad->mad_hdr.method) {\r\ncase IB_MGMT_METHOD_SET:\r\nmay_create = 1;\r\ncase IB_SA_METHOD_DELETE:\r\nreq = kzalloc(sizeof *req, GFP_KERNEL);\r\nif (!req)\r\nreturn -ENOMEM;\r\nreq->func = slave;\r\nreq->sa_mad = *sa_mad;\r\nmutex_lock(&ctx->mcg_table_lock);\r\ngroup = acquire_group(ctx, &rec->mgid, may_create, GFP_KERNEL);\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nif (IS_ERR(group)) {\r\nkfree(req);\r\nreturn PTR_ERR(group);\r\n}\r\nmutex_lock(&group->lock);\r\nif (group->func[slave].num_pend_reqs > MAX_PEND_REQS_PER_FUNC) {\r\nmutex_unlock(&group->lock);\r\nmcg_warn_group(group, "Port %d, Func %d has too many pending requests (%d), dropping\n",\r\nport, slave, MAX_PEND_REQS_PER_FUNC);\r\nrelease_group(group, 0);\r\nkfree(req);\r\nreturn -ENOMEM;\r\n}\r\n++group->func[slave].num_pend_reqs;\r\nreq->group = group;\r\nqueue_req(req);\r\nmutex_unlock(&group->lock);\r\nrelease_group(group, 0);\r\nreturn 1;\r\ncase IB_SA_METHOD_GET_TABLE:\r\ncase IB_MGMT_METHOD_GET_RESP:\r\ncase IB_SA_METHOD_GET_TABLE_RESP:\r\ncase IB_SA_METHOD_DELETE_RESP:\r\nreturn 0;\r\ndefault:\r\nmcg_warn("In multiplex, port %d, func %d: unexpected MCMember method: 0x%x, dropping\n",\r\nport, slave, sa_mad->mad_hdr.method);\r\nreturn 1;\r\n}\r\n}\r\nstatic ssize_t sysfs_show_group(struct device *dev,\r\nstruct device_attribute *attr, char *buf)\r\n{\r\nstruct mcast_group *group =\r\ncontainer_of(attr, struct mcast_group, dentry);\r\nstruct mcast_req *req = NULL;\r\nchar pending_str[40];\r\nchar state_str[40];\r\nssize_t len = 0;\r\nint f;\r\nif (group->state == MCAST_IDLE)\r\nsprintf(state_str, "%s", get_state_string(group->state));\r\nelse\r\nsprintf(state_str, "%s(TID=0x%llx)",\r\nget_state_string(group->state),\r\nbe64_to_cpu(group->last_req_tid));\r\nif (list_empty(&group->pending_list)) {\r\nsprintf(pending_str, "No");\r\n} else {\r\nreq = list_first_entry(&group->pending_list, struct mcast_req, group_list);\r\nsprintf(pending_str, "Yes(TID=0x%llx)",\r\nbe64_to_cpu(req->sa_mad.mad_hdr.tid));\r\n}\r\nlen += sprintf(buf + len, "%1d [%02d,%02d,%02d] %4d %4s %5s ",\r\ngroup->rec.scope_join_state & 0xf,\r\ngroup->members[2], group->members[1], group->members[0],\r\natomic_read(&group->refcount),\r\npending_str,\r\nstate_str);\r\nfor (f = 0; f < MAX_VFS; ++f)\r\nif (group->func[f].state == MCAST_MEMBER)\r\nlen += sprintf(buf + len, "%d[%1x] ",\r\nf, group->func[f].join_state);\r\nlen += sprintf(buf + len, "\t\t(%4hx %4x %2x %2x %2x %2x %2x "\r\n"%4x %4x %2x %2x)\n",\r\nbe16_to_cpu(group->rec.pkey),\r\nbe32_to_cpu(group->rec.qkey),\r\n(group->rec.mtusel_mtu & 0xc0) >> 6,\r\ngroup->rec.mtusel_mtu & 0x3f,\r\ngroup->rec.tclass,\r\n(group->rec.ratesel_rate & 0xc0) >> 6,\r\ngroup->rec.ratesel_rate & 0x3f,\r\n(be32_to_cpu(group->rec.sl_flowlabel_hoplimit) & 0xf0000000) >> 28,\r\n(be32_to_cpu(group->rec.sl_flowlabel_hoplimit) & 0x0fffff00) >> 8,\r\nbe32_to_cpu(group->rec.sl_flowlabel_hoplimit) & 0x000000ff,\r\ngroup->rec.proxy_join);\r\nreturn len;\r\n}\r\nint mlx4_ib_mcg_port_init(struct mlx4_ib_demux_ctx *ctx)\r\n{\r\nchar name[20];\r\natomic_set(&ctx->tid, 0);\r\nsprintf(name, "mlx4_ib_mcg%d", ctx->port);\r\nctx->mcg_wq = create_singlethread_workqueue(name);\r\nif (!ctx->mcg_wq)\r\nreturn -ENOMEM;\r\nmutex_init(&ctx->mcg_table_lock);\r\nctx->mcg_table = RB_ROOT;\r\nINIT_LIST_HEAD(&ctx->mcg_mgid0_list);\r\nctx->flushing = 0;\r\nreturn 0;\r\n}\r\nstatic void force_clean_group(struct mcast_group *group)\r\n{\r\nstruct mcast_req *req, *tmp\r\n;\r\nlist_for_each_entry_safe(req, tmp, &group->pending_list, group_list) {\r\nlist_del(&req->group_list);\r\nkfree(req);\r\n}\r\ndel_sysfs_port_mcg_attr(group->demux->dev, group->demux->port, &group->dentry.attr);\r\nrb_erase(&group->node, &group->demux->mcg_table);\r\nkfree(group);\r\n}\r\nstatic void _mlx4_ib_mcg_port_cleanup(struct mlx4_ib_demux_ctx *ctx, int destroy_wq)\r\n{\r\nint i;\r\nstruct rb_node *p;\r\nstruct mcast_group *group;\r\nunsigned long end;\r\nint count;\r\nfor (i = 0; i < MAX_VFS; ++i)\r\nclean_vf_mcast(ctx, i);\r\nend = jiffies + msecs_to_jiffies(MAD_TIMEOUT_MS + 3000);\r\ndo {\r\ncount = 0;\r\nmutex_lock(&ctx->mcg_table_lock);\r\nfor (p = rb_first(&ctx->mcg_table); p; p = rb_next(p))\r\n++count;\r\nmutex_unlock(&ctx->mcg_table_lock);\r\nif (!count)\r\nbreak;\r\nmsleep(1);\r\n} while (time_after(end, jiffies));\r\nflush_workqueue(ctx->mcg_wq);\r\nif (destroy_wq)\r\ndestroy_workqueue(ctx->mcg_wq);\r\nmutex_lock(&ctx->mcg_table_lock);\r\nwhile ((p = rb_first(&ctx->mcg_table)) != NULL) {\r\ngroup = rb_entry(p, struct mcast_group, node);\r\nif (atomic_read(&group->refcount))\r\nmcg_warn_group(group, "group refcount %d!!! (pointer %p)\n", atomic_read(&group->refcount), group);\r\nforce_clean_group(group);\r\n}\r\nmutex_unlock(&ctx->mcg_table_lock);\r\n}\r\nstatic void mcg_clean_task(struct work_struct *work)\r\n{\r\nstruct clean_work *cw = container_of(work, struct clean_work, work);\r\n_mlx4_ib_mcg_port_cleanup(cw->ctx, cw->destroy_wq);\r\ncw->ctx->flushing = 0;\r\nkfree(cw);\r\n}\r\nvoid mlx4_ib_mcg_port_cleanup(struct mlx4_ib_demux_ctx *ctx, int destroy_wq)\r\n{\r\nstruct clean_work *work;\r\nif (ctx->flushing)\r\nreturn;\r\nctx->flushing = 1;\r\nif (destroy_wq) {\r\n_mlx4_ib_mcg_port_cleanup(ctx, destroy_wq);\r\nctx->flushing = 0;\r\nreturn;\r\n}\r\nwork = kmalloc(sizeof *work, GFP_KERNEL);\r\nif (!work) {\r\nctx->flushing = 0;\r\nmcg_warn("failed allocating work for cleanup\n");\r\nreturn;\r\n}\r\nwork->ctx = ctx;\r\nwork->destroy_wq = destroy_wq;\r\nINIT_WORK(&work->work, mcg_clean_task);\r\nqueue_work(clean_wq, &work->work);\r\n}\r\nstatic void build_leave_mad(struct mcast_req *req)\r\n{\r\nstruct ib_sa_mad *mad = &req->sa_mad;\r\nmad->mad_hdr.method = IB_SA_METHOD_DELETE;\r\n}\r\nstatic void clear_pending_reqs(struct mcast_group *group, int vf)\r\n{\r\nstruct mcast_req *req, *tmp, *group_first = NULL;\r\nint clear;\r\nint pend = 0;\r\nif (!list_empty(&group->pending_list))\r\ngroup_first = list_first_entry(&group->pending_list, struct mcast_req, group_list);\r\nlist_for_each_entry_safe(req, tmp, &group->func[vf].pending, func_list) {\r\nclear = 1;\r\nif (group_first == req &&\r\n(group->state == MCAST_JOIN_SENT ||\r\ngroup->state == MCAST_LEAVE_SENT)) {\r\nclear = cancel_delayed_work(&group->timeout_work);\r\npend = !clear;\r\ngroup->state = MCAST_IDLE;\r\n}\r\nif (clear) {\r\n--group->func[vf].num_pend_reqs;\r\nlist_del(&req->group_list);\r\nlist_del(&req->func_list);\r\nkfree(req);\r\natomic_dec(&group->refcount);\r\n}\r\n}\r\nif (!pend && (!list_empty(&group->func[vf].pending) || group->func[vf].num_pend_reqs)) {\r\nmcg_warn_group(group, "DRIVER BUG: list_empty %d, num_pend_reqs %d\n",\r\nlist_empty(&group->func[vf].pending), group->func[vf].num_pend_reqs);\r\n}\r\n}\r\nstatic int push_deleteing_req(struct mcast_group *group, int slave)\r\n{\r\nstruct mcast_req *req;\r\nstruct mcast_req *pend_req;\r\nif (!group->func[slave].join_state)\r\nreturn 0;\r\nreq = kzalloc(sizeof *req, GFP_KERNEL);\r\nif (!req) {\r\nmcg_warn_group(group, "failed allocation - may leave stall groups\n");\r\nreturn -ENOMEM;\r\n}\r\nif (!list_empty(&group->func[slave].pending)) {\r\npend_req = list_entry(group->func[slave].pending.prev, struct mcast_req, group_list);\r\nif (pend_req->clean) {\r\nkfree(req);\r\nreturn 0;\r\n}\r\n}\r\nreq->clean = 1;\r\nreq->func = slave;\r\nreq->group = group;\r\n++group->func[slave].num_pend_reqs;\r\nbuild_leave_mad(req);\r\nqueue_req(req);\r\nreturn 0;\r\n}\r\nvoid clean_vf_mcast(struct mlx4_ib_demux_ctx *ctx, int slave)\r\n{\r\nstruct mcast_group *group;\r\nstruct rb_node *p;\r\nmutex_lock(&ctx->mcg_table_lock);\r\nfor (p = rb_first(&ctx->mcg_table); p; p = rb_next(p)) {\r\ngroup = rb_entry(p, struct mcast_group, node);\r\nmutex_lock(&group->lock);\r\nif (atomic_read(&group->refcount)) {\r\nclear_pending_reqs(group, slave);\r\npush_deleteing_req(group, slave);\r\n}\r\nmutex_unlock(&group->lock);\r\n}\r\nmutex_unlock(&ctx->mcg_table_lock);\r\n}\r\nint mlx4_ib_mcg_init(void)\r\n{\r\nclean_wq = create_singlethread_workqueue("mlx4_ib_mcg");\r\nif (!clean_wq)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid mlx4_ib_mcg_destroy(void)\r\n{\r\ndestroy_workqueue(clean_wq);\r\n}
