void __delete_from_page_cache(struct page *page)\r\n{\r\nstruct address_space *mapping = page->mapping;\r\nif (PageUptodate(page) && PageMappedToDisk(page))\r\ncleancache_put_page(page);\r\nelse\r\ncleancache_invalidate_page(mapping, page);\r\nradix_tree_delete(&mapping->page_tree, page->index);\r\npage->mapping = NULL;\r\nmapping->nrpages--;\r\n__dec_zone_page_state(page, NR_FILE_PAGES);\r\nif (PageSwapBacked(page))\r\n__dec_zone_page_state(page, NR_SHMEM);\r\nBUG_ON(page_mapped(page));\r\nif (PageDirty(page) && mapping_cap_account_dirty(mapping)) {\r\ndec_zone_page_state(page, NR_FILE_DIRTY);\r\ndec_bdi_stat(mapping->backing_dev_info, BDI_RECLAIMABLE);\r\n}\r\n}\r\nvoid delete_from_page_cache(struct page *page)\r\n{\r\nstruct address_space *mapping = page->mapping;\r\nvoid (*freepage)(struct page *);\r\nBUG_ON(!PageLocked(page));\r\nfreepage = mapping->a_ops->freepage;\r\nspin_lock_irq(&mapping->tree_lock);\r\n__delete_from_page_cache(page);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nmem_cgroup_uncharge_cache_page(page);\r\nif (freepage)\r\nfreepage(page);\r\npage_cache_release(page);\r\n}\r\nstatic int sleep_on_page(void *word)\r\n{\r\nio_schedule();\r\nreturn 0;\r\n}\r\nstatic int sleep_on_page_killable(void *word)\r\n{\r\nsleep_on_page(word);\r\nreturn fatal_signal_pending(current) ? -EINTR : 0;\r\n}\r\nint __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,\r\nloff_t end, int sync_mode)\r\n{\r\nint ret;\r\nstruct writeback_control wbc = {\r\n.sync_mode = sync_mode,\r\n.nr_to_write = LONG_MAX,\r\n.range_start = start,\r\n.range_end = end,\r\n};\r\nif (!mapping_cap_writeback_dirty(mapping))\r\nreturn 0;\r\nret = do_writepages(mapping, &wbc);\r\nreturn ret;\r\n}\r\nstatic inline int __filemap_fdatawrite(struct address_space *mapping,\r\nint sync_mode)\r\n{\r\nreturn __filemap_fdatawrite_range(mapping, 0, LLONG_MAX, sync_mode);\r\n}\r\nint filemap_fdatawrite(struct address_space *mapping)\r\n{\r\nreturn __filemap_fdatawrite(mapping, WB_SYNC_ALL);\r\n}\r\nint filemap_fdatawrite_range(struct address_space *mapping, loff_t start,\r\nloff_t end)\r\n{\r\nreturn __filemap_fdatawrite_range(mapping, start, end, WB_SYNC_ALL);\r\n}\r\nint filemap_flush(struct address_space *mapping)\r\n{\r\nreturn __filemap_fdatawrite(mapping, WB_SYNC_NONE);\r\n}\r\nint filemap_fdatawait_range(struct address_space *mapping, loff_t start_byte,\r\nloff_t end_byte)\r\n{\r\npgoff_t index = start_byte >> PAGE_CACHE_SHIFT;\r\npgoff_t end = end_byte >> PAGE_CACHE_SHIFT;\r\nstruct pagevec pvec;\r\nint nr_pages;\r\nint ret = 0;\r\nif (end_byte < start_byte)\r\nreturn 0;\r\npagevec_init(&pvec, 0);\r\nwhile ((index <= end) &&\r\n(nr_pages = pagevec_lookup_tag(&pvec, mapping, &index,\r\nPAGECACHE_TAG_WRITEBACK,\r\nmin(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1)) != 0) {\r\nunsigned i;\r\nfor (i = 0; i < nr_pages; i++) {\r\nstruct page *page = pvec.pages[i];\r\nif (page->index > end)\r\ncontinue;\r\nwait_on_page_writeback(page);\r\nif (TestClearPageError(page))\r\nret = -EIO;\r\n}\r\npagevec_release(&pvec);\r\ncond_resched();\r\n}\r\nif (test_and_clear_bit(AS_ENOSPC, &mapping->flags))\r\nret = -ENOSPC;\r\nif (test_and_clear_bit(AS_EIO, &mapping->flags))\r\nret = -EIO;\r\nreturn ret;\r\n}\r\nint filemap_fdatawait(struct address_space *mapping)\r\n{\r\nloff_t i_size = i_size_read(mapping->host);\r\nif (i_size == 0)\r\nreturn 0;\r\nreturn filemap_fdatawait_range(mapping, 0, i_size - 1);\r\n}\r\nint filemap_write_and_wait(struct address_space *mapping)\r\n{\r\nint err = 0;\r\nif (mapping->nrpages) {\r\nerr = filemap_fdatawrite(mapping);\r\nif (err != -EIO) {\r\nint err2 = filemap_fdatawait(mapping);\r\nif (!err)\r\nerr = err2;\r\n}\r\n}\r\nreturn err;\r\n}\r\nint filemap_write_and_wait_range(struct address_space *mapping,\r\nloff_t lstart, loff_t lend)\r\n{\r\nint err = 0;\r\nif (mapping->nrpages) {\r\nerr = __filemap_fdatawrite_range(mapping, lstart, lend,\r\nWB_SYNC_ALL);\r\nif (err != -EIO) {\r\nint err2 = filemap_fdatawait_range(mapping,\r\nlstart, lend);\r\nif (!err)\r\nerr = err2;\r\n}\r\n}\r\nreturn err;\r\n}\r\nint replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask)\r\n{\r\nint error;\r\nVM_BUG_ON(!PageLocked(old));\r\nVM_BUG_ON(!PageLocked(new));\r\nVM_BUG_ON(new->mapping);\r\nerror = radix_tree_preload(gfp_mask & ~__GFP_HIGHMEM);\r\nif (!error) {\r\nstruct address_space *mapping = old->mapping;\r\nvoid (*freepage)(struct page *);\r\npgoff_t offset = old->index;\r\nfreepage = mapping->a_ops->freepage;\r\npage_cache_get(new);\r\nnew->mapping = mapping;\r\nnew->index = offset;\r\nspin_lock_irq(&mapping->tree_lock);\r\n__delete_from_page_cache(old);\r\nerror = radix_tree_insert(&mapping->page_tree, offset, new);\r\nBUG_ON(error);\r\nmapping->nrpages++;\r\n__inc_zone_page_state(new, NR_FILE_PAGES);\r\nif (PageSwapBacked(new))\r\n__inc_zone_page_state(new, NR_SHMEM);\r\nspin_unlock_irq(&mapping->tree_lock);\r\nmem_cgroup_replace_page_cache(old, new);\r\nradix_tree_preload_end();\r\nif (freepage)\r\nfreepage(old);\r\npage_cache_release(old);\r\n}\r\nreturn error;\r\n}\r\nint add_to_page_cache_locked(struct page *page, struct address_space *mapping,\r\npgoff_t offset, gfp_t gfp_mask)\r\n{\r\nint error;\r\nVM_BUG_ON(!PageLocked(page));\r\nVM_BUG_ON(PageSwapBacked(page));\r\nerror = mem_cgroup_cache_charge(page, current->mm,\r\ngfp_mask & GFP_RECLAIM_MASK);\r\nif (error)\r\ngoto out;\r\nerror = radix_tree_preload(gfp_mask & ~__GFP_HIGHMEM);\r\nif (error == 0) {\r\npage_cache_get(page);\r\npage->mapping = mapping;\r\npage->index = offset;\r\nspin_lock_irq(&mapping->tree_lock);\r\nerror = radix_tree_insert(&mapping->page_tree, offset, page);\r\nif (likely(!error)) {\r\nmapping->nrpages++;\r\n__inc_zone_page_state(page, NR_FILE_PAGES);\r\nspin_unlock_irq(&mapping->tree_lock);\r\n} else {\r\npage->mapping = NULL;\r\nspin_unlock_irq(&mapping->tree_lock);\r\nmem_cgroup_uncharge_cache_page(page);\r\npage_cache_release(page);\r\n}\r\nradix_tree_preload_end();\r\n} else\r\nmem_cgroup_uncharge_cache_page(page);\r\nout:\r\nreturn error;\r\n}\r\nint add_to_page_cache_lru(struct page *page, struct address_space *mapping,\r\npgoff_t offset, gfp_t gfp_mask)\r\n{\r\nint ret;\r\nret = add_to_page_cache(page, mapping, offset, gfp_mask);\r\nif (ret == 0)\r\nlru_cache_add_file(page);\r\nreturn ret;\r\n}\r\nstruct page *__page_cache_alloc(gfp_t gfp)\r\n{\r\nint n;\r\nstruct page *page;\r\nif (cpuset_do_page_mem_spread()) {\r\nunsigned int cpuset_mems_cookie;\r\ndo {\r\ncpuset_mems_cookie = get_mems_allowed();\r\nn = cpuset_mem_spread_node();\r\npage = alloc_pages_exact_node(n, gfp, 0);\r\n} while (!put_mems_allowed(cpuset_mems_cookie) && !page);\r\nreturn page;\r\n}\r\nreturn alloc_pages(gfp, 0);\r\n}\r\nstatic wait_queue_head_t *page_waitqueue(struct page *page)\r\n{\r\nconst struct zone *zone = page_zone(page);\r\nreturn &zone->wait_table[hash_ptr(page, zone->wait_table_bits)];\r\n}\r\nstatic inline void wake_up_page(struct page *page, int bit)\r\n{\r\n__wake_up_bit(page_waitqueue(page), &page->flags, bit);\r\n}\r\nvoid wait_on_page_bit(struct page *page, int bit_nr)\r\n{\r\nDEFINE_WAIT_BIT(wait, &page->flags, bit_nr);\r\nif (test_bit(bit_nr, &page->flags))\r\n__wait_on_bit(page_waitqueue(page), &wait, sleep_on_page,\r\nTASK_UNINTERRUPTIBLE);\r\n}\r\nint wait_on_page_bit_killable(struct page *page, int bit_nr)\r\n{\r\nDEFINE_WAIT_BIT(wait, &page->flags, bit_nr);\r\nif (!test_bit(bit_nr, &page->flags))\r\nreturn 0;\r\nreturn __wait_on_bit(page_waitqueue(page), &wait,\r\nsleep_on_page_killable, TASK_KILLABLE);\r\n}\r\nvoid add_page_wait_queue(struct page *page, wait_queue_t *waiter)\r\n{\r\nwait_queue_head_t *q = page_waitqueue(page);\r\nunsigned long flags;\r\nspin_lock_irqsave(&q->lock, flags);\r\n__add_wait_queue(q, waiter);\r\nspin_unlock_irqrestore(&q->lock, flags);\r\n}\r\nvoid unlock_page(struct page *page)\r\n{\r\nVM_BUG_ON(!PageLocked(page));\r\nclear_bit_unlock(PG_locked, &page->flags);\r\nsmp_mb__after_clear_bit();\r\nwake_up_page(page, PG_locked);\r\n}\r\nvoid end_page_writeback(struct page *page)\r\n{\r\nif (TestClearPageReclaim(page))\r\nrotate_reclaimable_page(page);\r\nif (!test_clear_page_writeback(page))\r\nBUG();\r\nsmp_mb__after_clear_bit();\r\nwake_up_page(page, PG_writeback);\r\n}\r\nvoid __lock_page(struct page *page)\r\n{\r\nDEFINE_WAIT_BIT(wait, &page->flags, PG_locked);\r\n__wait_on_bit_lock(page_waitqueue(page), &wait, sleep_on_page,\r\nTASK_UNINTERRUPTIBLE);\r\n}\r\nint __lock_page_killable(struct page *page)\r\n{\r\nDEFINE_WAIT_BIT(wait, &page->flags, PG_locked);\r\nreturn __wait_on_bit_lock(page_waitqueue(page), &wait,\r\nsleep_on_page_killable, TASK_KILLABLE);\r\n}\r\nint __lock_page_or_retry(struct page *page, struct mm_struct *mm,\r\nunsigned int flags)\r\n{\r\nif (flags & FAULT_FLAG_ALLOW_RETRY) {\r\nif (flags & FAULT_FLAG_RETRY_NOWAIT)\r\nreturn 0;\r\nup_read(&mm->mmap_sem);\r\nif (flags & FAULT_FLAG_KILLABLE)\r\nwait_on_page_locked_killable(page);\r\nelse\r\nwait_on_page_locked(page);\r\nreturn 0;\r\n} else {\r\nif (flags & FAULT_FLAG_KILLABLE) {\r\nint ret;\r\nret = __lock_page_killable(page);\r\nif (ret) {\r\nup_read(&mm->mmap_sem);\r\nreturn 0;\r\n}\r\n} else\r\n__lock_page(page);\r\nreturn 1;\r\n}\r\n}\r\nstruct page *find_get_page(struct address_space *mapping, pgoff_t offset)\r\n{\r\nvoid **pagep;\r\nstruct page *page;\r\nrcu_read_lock();\r\nrepeat:\r\npage = NULL;\r\npagep = radix_tree_lookup_slot(&mapping->page_tree, offset);\r\nif (pagep) {\r\npage = radix_tree_deref_slot(pagep);\r\nif (unlikely(!page))\r\ngoto out;\r\nif (radix_tree_exception(page)) {\r\nif (radix_tree_deref_retry(page))\r\ngoto repeat;\r\ngoto out;\r\n}\r\nif (!page_cache_get_speculative(page))\r\ngoto repeat;\r\nif (unlikely(page != *pagep)) {\r\npage_cache_release(page);\r\ngoto repeat;\r\n}\r\n}\r\nout:\r\nrcu_read_unlock();\r\nreturn page;\r\n}\r\nstruct page *find_lock_page(struct address_space *mapping, pgoff_t offset)\r\n{\r\nstruct page *page;\r\nrepeat:\r\npage = find_get_page(mapping, offset);\r\nif (page && !radix_tree_exception(page)) {\r\nlock_page(page);\r\nif (unlikely(page->mapping != mapping)) {\r\nunlock_page(page);\r\npage_cache_release(page);\r\ngoto repeat;\r\n}\r\nVM_BUG_ON(page->index != offset);\r\n}\r\nreturn page;\r\n}\r\nstruct page *find_or_create_page(struct address_space *mapping,\r\npgoff_t index, gfp_t gfp_mask)\r\n{\r\nstruct page *page;\r\nint err;\r\nrepeat:\r\npage = find_lock_page(mapping, index);\r\nif (!page) {\r\npage = __page_cache_alloc(gfp_mask);\r\nif (!page)\r\nreturn NULL;\r\nerr = add_to_page_cache_lru(page, mapping, index,\r\n(gfp_mask & GFP_RECLAIM_MASK));\r\nif (unlikely(err)) {\r\npage_cache_release(page);\r\npage = NULL;\r\nif (err == -EEXIST)\r\ngoto repeat;\r\n}\r\n}\r\nreturn page;\r\n}\r\nunsigned find_get_pages(struct address_space *mapping, pgoff_t start,\r\nunsigned int nr_pages, struct page **pages)\r\n{\r\nstruct radix_tree_iter iter;\r\nvoid **slot;\r\nunsigned ret = 0;\r\nif (unlikely(!nr_pages))\r\nreturn 0;\r\nrcu_read_lock();\r\nrestart:\r\nradix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {\r\nstruct page *page;\r\nrepeat:\r\npage = radix_tree_deref_slot(slot);\r\nif (unlikely(!page))\r\ncontinue;\r\nif (radix_tree_exception(page)) {\r\nif (radix_tree_deref_retry(page)) {\r\nWARN_ON(iter.index);\r\ngoto restart;\r\n}\r\ncontinue;\r\n}\r\nif (!page_cache_get_speculative(page))\r\ngoto repeat;\r\nif (unlikely(page != *slot)) {\r\npage_cache_release(page);\r\ngoto repeat;\r\n}\r\npages[ret] = page;\r\nif (++ret == nr_pages)\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nunsigned find_get_pages_contig(struct address_space *mapping, pgoff_t index,\r\nunsigned int nr_pages, struct page **pages)\r\n{\r\nstruct radix_tree_iter iter;\r\nvoid **slot;\r\nunsigned int ret = 0;\r\nif (unlikely(!nr_pages))\r\nreturn 0;\r\nrcu_read_lock();\r\nrestart:\r\nradix_tree_for_each_contig(slot, &mapping->page_tree, &iter, index) {\r\nstruct page *page;\r\nrepeat:\r\npage = radix_tree_deref_slot(slot);\r\nif (unlikely(!page))\r\nbreak;\r\nif (radix_tree_exception(page)) {\r\nif (radix_tree_deref_retry(page)) {\r\ngoto restart;\r\n}\r\nbreak;\r\n}\r\nif (!page_cache_get_speculative(page))\r\ngoto repeat;\r\nif (unlikely(page != *slot)) {\r\npage_cache_release(page);\r\ngoto repeat;\r\n}\r\nif (page->mapping == NULL || page->index != iter.index) {\r\npage_cache_release(page);\r\nbreak;\r\n}\r\npages[ret] = page;\r\nif (++ret == nr_pages)\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\nreturn ret;\r\n}\r\nunsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index,\r\nint tag, unsigned int nr_pages, struct page **pages)\r\n{\r\nstruct radix_tree_iter iter;\r\nvoid **slot;\r\nunsigned ret = 0;\r\nif (unlikely(!nr_pages))\r\nreturn 0;\r\nrcu_read_lock();\r\nrestart:\r\nradix_tree_for_each_tagged(slot, &mapping->page_tree,\r\n&iter, *index, tag) {\r\nstruct page *page;\r\nrepeat:\r\npage = radix_tree_deref_slot(slot);\r\nif (unlikely(!page))\r\ncontinue;\r\nif (radix_tree_exception(page)) {\r\nif (radix_tree_deref_retry(page)) {\r\ngoto restart;\r\n}\r\nBUG();\r\n}\r\nif (!page_cache_get_speculative(page))\r\ngoto repeat;\r\nif (unlikely(page != *slot)) {\r\npage_cache_release(page);\r\ngoto repeat;\r\n}\r\npages[ret] = page;\r\nif (++ret == nr_pages)\r\nbreak;\r\n}\r\nrcu_read_unlock();\r\nif (ret)\r\n*index = pages[ret - 1]->index + 1;\r\nreturn ret;\r\n}\r\nstruct page *\r\ngrab_cache_page_nowait(struct address_space *mapping, pgoff_t index)\r\n{\r\nstruct page *page = find_get_page(mapping, index);\r\nif (page) {\r\nif (trylock_page(page))\r\nreturn page;\r\npage_cache_release(page);\r\nreturn NULL;\r\n}\r\npage = __page_cache_alloc(mapping_gfp_mask(mapping) & ~__GFP_FS);\r\nif (page && add_to_page_cache_lru(page, mapping, index, GFP_NOFS)) {\r\npage_cache_release(page);\r\npage = NULL;\r\n}\r\nreturn page;\r\n}\r\nstatic void shrink_readahead_size_eio(struct file *filp,\r\nstruct file_ra_state *ra)\r\n{\r\nra->ra_pages /= 4;\r\n}\r\nstatic void do_generic_file_read(struct file *filp, loff_t *ppos,\r\nread_descriptor_t *desc, read_actor_t actor)\r\n{\r\nstruct address_space *mapping = filp->f_mapping;\r\nstruct inode *inode = mapping->host;\r\nstruct file_ra_state *ra = &filp->f_ra;\r\npgoff_t index;\r\npgoff_t last_index;\r\npgoff_t prev_index;\r\nunsigned long offset;\r\nunsigned int prev_offset;\r\nint error;\r\nindex = *ppos >> PAGE_CACHE_SHIFT;\r\nprev_index = ra->prev_pos >> PAGE_CACHE_SHIFT;\r\nprev_offset = ra->prev_pos & (PAGE_CACHE_SIZE-1);\r\nlast_index = (*ppos + desc->count + PAGE_CACHE_SIZE-1) >> PAGE_CACHE_SHIFT;\r\noffset = *ppos & ~PAGE_CACHE_MASK;\r\nfor (;;) {\r\nstruct page *page;\r\npgoff_t end_index;\r\nloff_t isize;\r\nunsigned long nr, ret;\r\ncond_resched();\r\nfind_page:\r\npage = find_get_page(mapping, index);\r\nif (!page) {\r\npage_cache_sync_readahead(mapping,\r\nra, filp,\r\nindex, last_index - index);\r\npage = find_get_page(mapping, index);\r\nif (unlikely(page == NULL))\r\ngoto no_cached_page;\r\n}\r\nif (PageReadahead(page)) {\r\npage_cache_async_readahead(mapping,\r\nra, filp, page,\r\nindex, last_index - index);\r\n}\r\nif (!PageUptodate(page)) {\r\nif (inode->i_blkbits == PAGE_CACHE_SHIFT ||\r\n!mapping->a_ops->is_partially_uptodate)\r\ngoto page_not_up_to_date;\r\nif (!trylock_page(page))\r\ngoto page_not_up_to_date;\r\nif (!page->mapping)\r\ngoto page_not_up_to_date_locked;\r\nif (!mapping->a_ops->is_partially_uptodate(page,\r\ndesc, offset))\r\ngoto page_not_up_to_date_locked;\r\nunlock_page(page);\r\n}\r\npage_ok:\r\nisize = i_size_read(inode);\r\nend_index = (isize - 1) >> PAGE_CACHE_SHIFT;\r\nif (unlikely(!isize || index > end_index)) {\r\npage_cache_release(page);\r\ngoto out;\r\n}\r\nnr = PAGE_CACHE_SIZE;\r\nif (index == end_index) {\r\nnr = ((isize - 1) & ~PAGE_CACHE_MASK) + 1;\r\nif (nr <= offset) {\r\npage_cache_release(page);\r\ngoto out;\r\n}\r\n}\r\nnr = nr - offset;\r\nif (mapping_writably_mapped(mapping))\r\nflush_dcache_page(page);\r\nif (prev_index != index || offset != prev_offset)\r\nmark_page_accessed(page);\r\nprev_index = index;\r\nret = actor(desc, page, offset, nr);\r\noffset += ret;\r\nindex += offset >> PAGE_CACHE_SHIFT;\r\noffset &= ~PAGE_CACHE_MASK;\r\nprev_offset = offset;\r\npage_cache_release(page);\r\nif (ret == nr && desc->count)\r\ncontinue;\r\ngoto out;\r\npage_not_up_to_date:\r\nerror = lock_page_killable(page);\r\nif (unlikely(error))\r\ngoto readpage_error;\r\npage_not_up_to_date_locked:\r\nif (!page->mapping) {\r\nunlock_page(page);\r\npage_cache_release(page);\r\ncontinue;\r\n}\r\nif (PageUptodate(page)) {\r\nunlock_page(page);\r\ngoto page_ok;\r\n}\r\nreadpage:\r\nClearPageError(page);\r\nerror = mapping->a_ops->readpage(filp, page);\r\nif (unlikely(error)) {\r\nif (error == AOP_TRUNCATED_PAGE) {\r\npage_cache_release(page);\r\ngoto find_page;\r\n}\r\ngoto readpage_error;\r\n}\r\nif (!PageUptodate(page)) {\r\nerror = lock_page_killable(page);\r\nif (unlikely(error))\r\ngoto readpage_error;\r\nif (!PageUptodate(page)) {\r\nif (page->mapping == NULL) {\r\nunlock_page(page);\r\npage_cache_release(page);\r\ngoto find_page;\r\n}\r\nunlock_page(page);\r\nshrink_readahead_size_eio(filp, ra);\r\nerror = -EIO;\r\ngoto readpage_error;\r\n}\r\nunlock_page(page);\r\n}\r\ngoto page_ok;\r\nreadpage_error:\r\ndesc->error = error;\r\npage_cache_release(page);\r\ngoto out;\r\nno_cached_page:\r\npage = page_cache_alloc_cold(mapping);\r\nif (!page) {\r\ndesc->error = -ENOMEM;\r\ngoto out;\r\n}\r\nerror = add_to_page_cache_lru(page, mapping,\r\nindex, GFP_KERNEL);\r\nif (error) {\r\npage_cache_release(page);\r\nif (error == -EEXIST)\r\ngoto find_page;\r\ndesc->error = error;\r\ngoto out;\r\n}\r\ngoto readpage;\r\n}\r\nout:\r\nra->prev_pos = prev_index;\r\nra->prev_pos <<= PAGE_CACHE_SHIFT;\r\nra->prev_pos |= prev_offset;\r\n*ppos = ((loff_t)index << PAGE_CACHE_SHIFT) + offset;\r\nfile_accessed(filp);\r\n}\r\nint file_read_actor(read_descriptor_t *desc, struct page *page,\r\nunsigned long offset, unsigned long size)\r\n{\r\nchar *kaddr;\r\nunsigned long left, count = desc->count;\r\nif (size > count)\r\nsize = count;\r\nif (!fault_in_pages_writeable(desc->arg.buf, size)) {\r\nkaddr = kmap_atomic(page);\r\nleft = __copy_to_user_inatomic(desc->arg.buf,\r\nkaddr + offset, size);\r\nkunmap_atomic(kaddr);\r\nif (left == 0)\r\ngoto success;\r\n}\r\nkaddr = kmap(page);\r\nleft = __copy_to_user(desc->arg.buf, kaddr + offset, size);\r\nkunmap(page);\r\nif (left) {\r\nsize -= left;\r\ndesc->error = -EFAULT;\r\n}\r\nsuccess:\r\ndesc->count = count - size;\r\ndesc->written += size;\r\ndesc->arg.buf += size;\r\nreturn size;\r\n}\r\nint generic_segment_checks(const struct iovec *iov,\r\nunsigned long *nr_segs, size_t *count, int access_flags)\r\n{\r\nunsigned long seg;\r\nsize_t cnt = 0;\r\nfor (seg = 0; seg < *nr_segs; seg++) {\r\nconst struct iovec *iv = &iov[seg];\r\ncnt += iv->iov_len;\r\nif (unlikely((ssize_t)(cnt|iv->iov_len) < 0))\r\nreturn -EINVAL;\r\nif (access_ok(access_flags, iv->iov_base, iv->iov_len))\r\ncontinue;\r\nif (seg == 0)\r\nreturn -EFAULT;\r\n*nr_segs = seg;\r\ncnt -= iv->iov_len;\r\nbreak;\r\n}\r\n*count = cnt;\r\nreturn 0;\r\n}\r\nssize_t\r\ngeneric_file_aio_read(struct kiocb *iocb, const struct iovec *iov,\r\nunsigned long nr_segs, loff_t pos)\r\n{\r\nstruct file *filp = iocb->ki_filp;\r\nssize_t retval;\r\nunsigned long seg = 0;\r\nsize_t count;\r\nloff_t *ppos = &iocb->ki_pos;\r\ncount = 0;\r\nretval = generic_segment_checks(iov, &nr_segs, &count, VERIFY_WRITE);\r\nif (retval)\r\nreturn retval;\r\nif (filp->f_flags & O_DIRECT) {\r\nloff_t size;\r\nstruct address_space *mapping;\r\nstruct inode *inode;\r\nmapping = filp->f_mapping;\r\ninode = mapping->host;\r\nif (!count)\r\ngoto out;\r\nsize = i_size_read(inode);\r\nif (pos < size) {\r\nretval = filemap_write_and_wait_range(mapping, pos,\r\npos + iov_length(iov, nr_segs) - 1);\r\nif (!retval) {\r\nretval = mapping->a_ops->direct_IO(READ, iocb,\r\niov, pos, nr_segs);\r\n}\r\nif (retval > 0) {\r\n*ppos = pos + retval;\r\ncount -= retval;\r\n}\r\nif (retval < 0 || !count || *ppos >= size) {\r\nfile_accessed(filp);\r\ngoto out;\r\n}\r\n}\r\n}\r\ncount = retval;\r\nfor (seg = 0; seg < nr_segs; seg++) {\r\nread_descriptor_t desc;\r\nloff_t offset = 0;\r\nif (count) {\r\nif (count > iov[seg].iov_len) {\r\ncount -= iov[seg].iov_len;\r\ncontinue;\r\n}\r\noffset = count;\r\ncount = 0;\r\n}\r\ndesc.written = 0;\r\ndesc.arg.buf = iov[seg].iov_base + offset;\r\ndesc.count = iov[seg].iov_len - offset;\r\nif (desc.count == 0)\r\ncontinue;\r\ndesc.error = 0;\r\ndo_generic_file_read(filp, ppos, &desc, file_read_actor);\r\nretval += desc.written;\r\nif (desc.error) {\r\nretval = retval ?: desc.error;\r\nbreak;\r\n}\r\nif (desc.count > 0)\r\nbreak;\r\n}\r\nout:\r\nreturn retval;\r\n}\r\nstatic int page_cache_read(struct file *file, pgoff_t offset)\r\n{\r\nstruct address_space *mapping = file->f_mapping;\r\nstruct page *page;\r\nint ret;\r\ndo {\r\npage = page_cache_alloc_cold(mapping);\r\nif (!page)\r\nreturn -ENOMEM;\r\nret = add_to_page_cache_lru(page, mapping, offset, GFP_KERNEL);\r\nif (ret == 0)\r\nret = mapping->a_ops->readpage(file, page);\r\nelse if (ret == -EEXIST)\r\nret = 0;\r\npage_cache_release(page);\r\n} while (ret == AOP_TRUNCATED_PAGE);\r\nreturn ret;\r\n}\r\nstatic void do_sync_mmap_readahead(struct vm_area_struct *vma,\r\nstruct file_ra_state *ra,\r\nstruct file *file,\r\npgoff_t offset)\r\n{\r\nunsigned long ra_pages;\r\nstruct address_space *mapping = file->f_mapping;\r\nif (VM_RandomReadHint(vma))\r\nreturn;\r\nif (!ra->ra_pages)\r\nreturn;\r\nif (VM_SequentialReadHint(vma)) {\r\npage_cache_sync_readahead(mapping, ra, file, offset,\r\nra->ra_pages);\r\nreturn;\r\n}\r\nif (ra->mmap_miss < MMAP_LOTSAMISS * 10)\r\nra->mmap_miss++;\r\nif (ra->mmap_miss > MMAP_LOTSAMISS)\r\nreturn;\r\nra_pages = max_sane_readahead(ra->ra_pages);\r\nra->start = max_t(long, 0, offset - ra_pages / 2);\r\nra->size = ra_pages;\r\nra->async_size = ra_pages / 4;\r\nra_submit(ra, mapping, file);\r\n}\r\nstatic void do_async_mmap_readahead(struct vm_area_struct *vma,\r\nstruct file_ra_state *ra,\r\nstruct file *file,\r\nstruct page *page,\r\npgoff_t offset)\r\n{\r\nstruct address_space *mapping = file->f_mapping;\r\nif (VM_RandomReadHint(vma))\r\nreturn;\r\nif (ra->mmap_miss > 0)\r\nra->mmap_miss--;\r\nif (PageReadahead(page))\r\npage_cache_async_readahead(mapping, ra, file,\r\npage, offset, ra->ra_pages);\r\n}\r\nint filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nint error;\r\nstruct file *file = vma->vm_file;\r\nstruct address_space *mapping = file->f_mapping;\r\nstruct file_ra_state *ra = &file->f_ra;\r\nstruct inode *inode = mapping->host;\r\npgoff_t offset = vmf->pgoff;\r\nstruct page *page;\r\npgoff_t size;\r\nint ret = 0;\r\nsize = (i_size_read(inode) + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\r\nif (offset >= size)\r\nreturn VM_FAULT_SIGBUS;\r\npage = find_get_page(mapping, offset);\r\nif (likely(page) && !(vmf->flags & FAULT_FLAG_TRIED)) {\r\ndo_async_mmap_readahead(vma, ra, file, page, offset);\r\n} else if (!page) {\r\ndo_sync_mmap_readahead(vma, ra, file, offset);\r\ncount_vm_event(PGMAJFAULT);\r\nmem_cgroup_count_vm_event(vma->vm_mm, PGMAJFAULT);\r\nret = VM_FAULT_MAJOR;\r\nretry_find:\r\npage = find_get_page(mapping, offset);\r\nif (!page)\r\ngoto no_cached_page;\r\n}\r\nif (!lock_page_or_retry(page, vma->vm_mm, vmf->flags)) {\r\npage_cache_release(page);\r\nreturn ret | VM_FAULT_RETRY;\r\n}\r\nif (unlikely(page->mapping != mapping)) {\r\nunlock_page(page);\r\nput_page(page);\r\ngoto retry_find;\r\n}\r\nVM_BUG_ON(page->index != offset);\r\nif (unlikely(!PageUptodate(page)))\r\ngoto page_not_uptodate;\r\nsize = (i_size_read(inode) + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT;\r\nif (unlikely(offset >= size)) {\r\nunlock_page(page);\r\npage_cache_release(page);\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\nvmf->page = page;\r\nreturn ret | VM_FAULT_LOCKED;\r\nno_cached_page:\r\nerror = page_cache_read(file, offset);\r\nif (error >= 0)\r\ngoto retry_find;\r\nif (error == -ENOMEM)\r\nreturn VM_FAULT_OOM;\r\nreturn VM_FAULT_SIGBUS;\r\npage_not_uptodate:\r\nClearPageError(page);\r\nerror = mapping->a_ops->readpage(file, page);\r\nif (!error) {\r\nwait_on_page_locked(page);\r\nif (!PageUptodate(page))\r\nerror = -EIO;\r\n}\r\npage_cache_release(page);\r\nif (!error || error == AOP_TRUNCATED_PAGE)\r\ngoto retry_find;\r\nshrink_readahead_size_eio(file, ra);\r\nreturn VM_FAULT_SIGBUS;\r\n}\r\nint filemap_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct page *page = vmf->page;\r\nstruct inode *inode = vma->vm_file->f_path.dentry->d_inode;\r\nint ret = VM_FAULT_LOCKED;\r\nsb_start_pagefault(inode->i_sb);\r\nfile_update_time(vma->vm_file);\r\nlock_page(page);\r\nif (page->mapping != inode->i_mapping) {\r\nunlock_page(page);\r\nret = VM_FAULT_NOPAGE;\r\ngoto out;\r\n}\r\nset_page_dirty(page);\r\nout:\r\nsb_end_pagefault(inode->i_sb);\r\nreturn ret;\r\n}\r\nint generic_file_mmap(struct file * file, struct vm_area_struct * vma)\r\n{\r\nstruct address_space *mapping = file->f_mapping;\r\nif (!mapping->a_ops->readpage)\r\nreturn -ENOEXEC;\r\nfile_accessed(file);\r\nvma->vm_ops = &generic_file_vm_ops;\r\nreturn 0;\r\n}\r\nint generic_file_readonly_mmap(struct file *file, struct vm_area_struct *vma)\r\n{\r\nif ((vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_MAYWRITE))\r\nreturn -EINVAL;\r\nreturn generic_file_mmap(file, vma);\r\n}\r\nint generic_file_mmap(struct file * file, struct vm_area_struct * vma)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nint generic_file_readonly_mmap(struct file * file, struct vm_area_struct * vma)\r\n{\r\nreturn -ENOSYS;\r\n}\r\nstatic struct page *__read_cache_page(struct address_space *mapping,\r\npgoff_t index,\r\nint (*filler)(void *, struct page *),\r\nvoid *data,\r\ngfp_t gfp)\r\n{\r\nstruct page *page;\r\nint err;\r\nrepeat:\r\npage = find_get_page(mapping, index);\r\nif (!page) {\r\npage = __page_cache_alloc(gfp | __GFP_COLD);\r\nif (!page)\r\nreturn ERR_PTR(-ENOMEM);\r\nerr = add_to_page_cache_lru(page, mapping, index, gfp);\r\nif (unlikely(err)) {\r\npage_cache_release(page);\r\nif (err == -EEXIST)\r\ngoto repeat;\r\nreturn ERR_PTR(err);\r\n}\r\nerr = filler(data, page);\r\nif (err < 0) {\r\npage_cache_release(page);\r\npage = ERR_PTR(err);\r\n}\r\n}\r\nreturn page;\r\n}\r\nstatic struct page *do_read_cache_page(struct address_space *mapping,\r\npgoff_t index,\r\nint (*filler)(void *, struct page *),\r\nvoid *data,\r\ngfp_t gfp)\r\n{\r\nstruct page *page;\r\nint err;\r\nretry:\r\npage = __read_cache_page(mapping, index, filler, data, gfp);\r\nif (IS_ERR(page))\r\nreturn page;\r\nif (PageUptodate(page))\r\ngoto out;\r\nlock_page(page);\r\nif (!page->mapping) {\r\nunlock_page(page);\r\npage_cache_release(page);\r\ngoto retry;\r\n}\r\nif (PageUptodate(page)) {\r\nunlock_page(page);\r\ngoto out;\r\n}\r\nerr = filler(data, page);\r\nif (err < 0) {\r\npage_cache_release(page);\r\nreturn ERR_PTR(err);\r\n}\r\nout:\r\nmark_page_accessed(page);\r\nreturn page;\r\n}\r\nstruct page *read_cache_page_async(struct address_space *mapping,\r\npgoff_t index,\r\nint (*filler)(void *, struct page *),\r\nvoid *data)\r\n{\r\nreturn do_read_cache_page(mapping, index, filler, data, mapping_gfp_mask(mapping));\r\n}\r\nstatic struct page *wait_on_page_read(struct page *page)\r\n{\r\nif (!IS_ERR(page)) {\r\nwait_on_page_locked(page);\r\nif (!PageUptodate(page)) {\r\npage_cache_release(page);\r\npage = ERR_PTR(-EIO);\r\n}\r\n}\r\nreturn page;\r\n}\r\nstruct page *read_cache_page_gfp(struct address_space *mapping,\r\npgoff_t index,\r\ngfp_t gfp)\r\n{\r\nfiller_t *filler = (filler_t *)mapping->a_ops->readpage;\r\nreturn wait_on_page_read(do_read_cache_page(mapping, index, filler, NULL, gfp));\r\n}\r\nstruct page *read_cache_page(struct address_space *mapping,\r\npgoff_t index,\r\nint (*filler)(void *, struct page *),\r\nvoid *data)\r\n{\r\nreturn wait_on_page_read(read_cache_page_async(mapping, index, filler, data));\r\n}\r\nstatic size_t __iovec_copy_from_user_inatomic(char *vaddr,\r\nconst struct iovec *iov, size_t base, size_t bytes)\r\n{\r\nsize_t copied = 0, left = 0;\r\nwhile (bytes) {\r\nchar __user *buf = iov->iov_base + base;\r\nint copy = min(bytes, iov->iov_len - base);\r\nbase = 0;\r\nleft = __copy_from_user_inatomic(vaddr, buf, copy);\r\ncopied += copy;\r\nbytes -= copy;\r\nvaddr += copy;\r\niov++;\r\nif (unlikely(left))\r\nbreak;\r\n}\r\nreturn copied - left;\r\n}\r\nsize_t iov_iter_copy_from_user_atomic(struct page *page,\r\nstruct iov_iter *i, unsigned long offset, size_t bytes)\r\n{\r\nchar *kaddr;\r\nsize_t copied;\r\nBUG_ON(!in_atomic());\r\nkaddr = kmap_atomic(page);\r\nif (likely(i->nr_segs == 1)) {\r\nint left;\r\nchar __user *buf = i->iov->iov_base + i->iov_offset;\r\nleft = __copy_from_user_inatomic(kaddr + offset, buf, bytes);\r\ncopied = bytes - left;\r\n} else {\r\ncopied = __iovec_copy_from_user_inatomic(kaddr + offset,\r\ni->iov, i->iov_offset, bytes);\r\n}\r\nkunmap_atomic(kaddr);\r\nreturn copied;\r\n}\r\nsize_t iov_iter_copy_from_user(struct page *page,\r\nstruct iov_iter *i, unsigned long offset, size_t bytes)\r\n{\r\nchar *kaddr;\r\nsize_t copied;\r\nkaddr = kmap(page);\r\nif (likely(i->nr_segs == 1)) {\r\nint left;\r\nchar __user *buf = i->iov->iov_base + i->iov_offset;\r\nleft = __copy_from_user(kaddr + offset, buf, bytes);\r\ncopied = bytes - left;\r\n} else {\r\ncopied = __iovec_copy_from_user_inatomic(kaddr + offset,\r\ni->iov, i->iov_offset, bytes);\r\n}\r\nkunmap(page);\r\nreturn copied;\r\n}\r\nvoid iov_iter_advance(struct iov_iter *i, size_t bytes)\r\n{\r\nBUG_ON(i->count < bytes);\r\nif (likely(i->nr_segs == 1)) {\r\ni->iov_offset += bytes;\r\ni->count -= bytes;\r\n} else {\r\nconst struct iovec *iov = i->iov;\r\nsize_t base = i->iov_offset;\r\nunsigned long nr_segs = i->nr_segs;\r\nwhile (bytes || unlikely(i->count && !iov->iov_len)) {\r\nint copy;\r\ncopy = min(bytes, iov->iov_len - base);\r\nBUG_ON(!i->count || i->count < copy);\r\ni->count -= copy;\r\nbytes -= copy;\r\nbase += copy;\r\nif (iov->iov_len == base) {\r\niov++;\r\nnr_segs--;\r\nbase = 0;\r\n}\r\n}\r\ni->iov = iov;\r\ni->iov_offset = base;\r\ni->nr_segs = nr_segs;\r\n}\r\n}\r\nint iov_iter_fault_in_readable(struct iov_iter *i, size_t bytes)\r\n{\r\nchar __user *buf = i->iov->iov_base + i->iov_offset;\r\nbytes = min(bytes, i->iov->iov_len - i->iov_offset);\r\nreturn fault_in_pages_readable(buf, bytes);\r\n}\r\nsize_t iov_iter_single_seg_count(struct iov_iter *i)\r\n{\r\nconst struct iovec *iov = i->iov;\r\nif (i->nr_segs == 1)\r\nreturn i->count;\r\nelse\r\nreturn min(i->count, iov->iov_len - i->iov_offset);\r\n}\r\ninline int generic_write_checks(struct file *file, loff_t *pos, size_t *count, int isblk)\r\n{\r\nstruct inode *inode = file->f_mapping->host;\r\nunsigned long limit = rlimit(RLIMIT_FSIZE);\r\nif (unlikely(*pos < 0))\r\nreturn -EINVAL;\r\nif (!isblk) {\r\nif (file->f_flags & O_APPEND)\r\n*pos = i_size_read(inode);\r\nif (limit != RLIM_INFINITY) {\r\nif (*pos >= limit) {\r\nsend_sig(SIGXFSZ, current, 0);\r\nreturn -EFBIG;\r\n}\r\nif (*count > limit - (typeof(limit))*pos) {\r\n*count = limit - (typeof(limit))*pos;\r\n}\r\n}\r\n}\r\nif (unlikely(*pos + *count > MAX_NON_LFS &&\r\n!(file->f_flags & O_LARGEFILE))) {\r\nif (*pos >= MAX_NON_LFS) {\r\nreturn -EFBIG;\r\n}\r\nif (*count > MAX_NON_LFS - (unsigned long)*pos) {\r\n*count = MAX_NON_LFS - (unsigned long)*pos;\r\n}\r\n}\r\nif (likely(!isblk)) {\r\nif (unlikely(*pos >= inode->i_sb->s_maxbytes)) {\r\nif (*count || *pos > inode->i_sb->s_maxbytes) {\r\nreturn -EFBIG;\r\n}\r\n}\r\nif (unlikely(*pos + *count > inode->i_sb->s_maxbytes))\r\n*count = inode->i_sb->s_maxbytes - *pos;\r\n} else {\r\n#ifdef CONFIG_BLOCK\r\nloff_t isize;\r\nif (bdev_read_only(I_BDEV(inode)))\r\nreturn -EPERM;\r\nisize = i_size_read(inode);\r\nif (*pos >= isize) {\r\nif (*count || *pos > isize)\r\nreturn -ENOSPC;\r\n}\r\nif (*pos + *count > isize)\r\n*count = isize - *pos;\r\n#else\r\nreturn -EPERM;\r\n#endif\r\n}\r\nreturn 0;\r\n}\r\nint pagecache_write_begin(struct file *file, struct address_space *mapping,\r\nloff_t pos, unsigned len, unsigned flags,\r\nstruct page **pagep, void **fsdata)\r\n{\r\nconst struct address_space_operations *aops = mapping->a_ops;\r\nreturn aops->write_begin(file, mapping, pos, len, flags,\r\npagep, fsdata);\r\n}\r\nint pagecache_write_end(struct file *file, struct address_space *mapping,\r\nloff_t pos, unsigned len, unsigned copied,\r\nstruct page *page, void *fsdata)\r\n{\r\nconst struct address_space_operations *aops = mapping->a_ops;\r\nmark_page_accessed(page);\r\nreturn aops->write_end(file, mapping, pos, len, copied, page, fsdata);\r\n}\r\nssize_t\r\ngeneric_file_direct_write(struct kiocb *iocb, const struct iovec *iov,\r\nunsigned long *nr_segs, loff_t pos, loff_t *ppos,\r\nsize_t count, size_t ocount)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct address_space *mapping = file->f_mapping;\r\nstruct inode *inode = mapping->host;\r\nssize_t written;\r\nsize_t write_len;\r\npgoff_t end;\r\nif (count != ocount)\r\n*nr_segs = iov_shorten((struct iovec *)iov, *nr_segs, count);\r\nwrite_len = iov_length(iov, *nr_segs);\r\nend = (pos + write_len - 1) >> PAGE_CACHE_SHIFT;\r\nwritten = filemap_write_and_wait_range(mapping, pos, pos + write_len - 1);\r\nif (written)\r\ngoto out;\r\nif (mapping->nrpages) {\r\nwritten = invalidate_inode_pages2_range(mapping,\r\npos >> PAGE_CACHE_SHIFT, end);\r\nif (written) {\r\nif (written == -EBUSY)\r\nreturn 0;\r\ngoto out;\r\n}\r\n}\r\nwritten = mapping->a_ops->direct_IO(WRITE, iocb, iov, pos, *nr_segs);\r\nif (mapping->nrpages) {\r\ninvalidate_inode_pages2_range(mapping,\r\npos >> PAGE_CACHE_SHIFT, end);\r\n}\r\nif (written > 0) {\r\npos += written;\r\nif (pos > i_size_read(inode) && !S_ISBLK(inode->i_mode)) {\r\ni_size_write(inode, pos);\r\nmark_inode_dirty(inode);\r\n}\r\n*ppos = pos;\r\n}\r\nout:\r\nreturn written;\r\n}\r\nstruct page *grab_cache_page_write_begin(struct address_space *mapping,\r\npgoff_t index, unsigned flags)\r\n{\r\nint status;\r\ngfp_t gfp_mask;\r\nstruct page *page;\r\ngfp_t gfp_notmask = 0;\r\ngfp_mask = mapping_gfp_mask(mapping);\r\nif (mapping_cap_account_dirty(mapping))\r\ngfp_mask |= __GFP_WRITE;\r\nif (flags & AOP_FLAG_NOFS)\r\ngfp_notmask = __GFP_FS;\r\nrepeat:\r\npage = find_lock_page(mapping, index);\r\nif (page)\r\ngoto found;\r\npage = __page_cache_alloc(gfp_mask & ~gfp_notmask);\r\nif (!page)\r\nreturn NULL;\r\nstatus = add_to_page_cache_lru(page, mapping, index,\r\nGFP_KERNEL & ~gfp_notmask);\r\nif (unlikely(status)) {\r\npage_cache_release(page);\r\nif (status == -EEXIST)\r\ngoto repeat;\r\nreturn NULL;\r\n}\r\nfound:\r\nwait_on_page_writeback(page);\r\nreturn page;\r\n}\r\nstatic ssize_t generic_perform_write(struct file *file,\r\nstruct iov_iter *i, loff_t pos)\r\n{\r\nstruct address_space *mapping = file->f_mapping;\r\nconst struct address_space_operations *a_ops = mapping->a_ops;\r\nlong status = 0;\r\nssize_t written = 0;\r\nunsigned int flags = 0;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nflags |= AOP_FLAG_UNINTERRUPTIBLE;\r\ndo {\r\nstruct page *page;\r\nunsigned long offset;\r\nunsigned long bytes;\r\nsize_t copied;\r\nvoid *fsdata;\r\noffset = (pos & (PAGE_CACHE_SIZE - 1));\r\nbytes = min_t(unsigned long, PAGE_CACHE_SIZE - offset,\r\niov_iter_count(i));\r\nagain:\r\nif (unlikely(iov_iter_fault_in_readable(i, bytes))) {\r\nstatus = -EFAULT;\r\nbreak;\r\n}\r\nstatus = a_ops->write_begin(file, mapping, pos, bytes, flags,\r\n&page, &fsdata);\r\nif (unlikely(status))\r\nbreak;\r\nif (mapping_writably_mapped(mapping))\r\nflush_dcache_page(page);\r\npagefault_disable();\r\ncopied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);\r\npagefault_enable();\r\nflush_dcache_page(page);\r\nmark_page_accessed(page);\r\nstatus = a_ops->write_end(file, mapping, pos, bytes, copied,\r\npage, fsdata);\r\nif (unlikely(status < 0))\r\nbreak;\r\ncopied = status;\r\ncond_resched();\r\niov_iter_advance(i, copied);\r\nif (unlikely(copied == 0)) {\r\nbytes = min_t(unsigned long, PAGE_CACHE_SIZE - offset,\r\niov_iter_single_seg_count(i));\r\ngoto again;\r\n}\r\npos += copied;\r\nwritten += copied;\r\nbalance_dirty_pages_ratelimited(mapping);\r\nif (fatal_signal_pending(current)) {\r\nstatus = -EINTR;\r\nbreak;\r\n}\r\n} while (iov_iter_count(i));\r\nreturn written ? written : status;\r\n}\r\nssize_t\r\ngeneric_file_buffered_write(struct kiocb *iocb, const struct iovec *iov,\r\nunsigned long nr_segs, loff_t pos, loff_t *ppos,\r\nsize_t count, ssize_t written)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nssize_t status;\r\nstruct iov_iter i;\r\niov_iter_init(&i, iov, nr_segs, count, written);\r\nstatus = generic_perform_write(file, &i, pos);\r\nif (likely(status >= 0)) {\r\nwritten += status;\r\n*ppos = pos + status;\r\n}\r\nreturn written ? written : status;\r\n}\r\nssize_t __generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,\r\nunsigned long nr_segs, loff_t *ppos)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct address_space * mapping = file->f_mapping;\r\nsize_t ocount;\r\nsize_t count;\r\nstruct inode *inode = mapping->host;\r\nloff_t pos;\r\nssize_t written;\r\nssize_t err;\r\nocount = 0;\r\nerr = generic_segment_checks(iov, &nr_segs, &ocount, VERIFY_READ);\r\nif (err)\r\nreturn err;\r\ncount = ocount;\r\npos = *ppos;\r\ncurrent->backing_dev_info = mapping->backing_dev_info;\r\nwritten = 0;\r\nerr = generic_write_checks(file, &pos, &count, S_ISBLK(inode->i_mode));\r\nif (err)\r\ngoto out;\r\nif (count == 0)\r\ngoto out;\r\nerr = file_remove_suid(file);\r\nif (err)\r\ngoto out;\r\nerr = file_update_time(file);\r\nif (err)\r\ngoto out;\r\nif (unlikely(file->f_flags & O_DIRECT)) {\r\nloff_t endbyte;\r\nssize_t written_buffered;\r\nwritten = generic_file_direct_write(iocb, iov, &nr_segs, pos,\r\nppos, count, ocount);\r\nif (written < 0 || written == count)\r\ngoto out;\r\npos += written;\r\ncount -= written;\r\nwritten_buffered = generic_file_buffered_write(iocb, iov,\r\nnr_segs, pos, ppos, count,\r\nwritten);\r\nif (written_buffered < 0) {\r\nerr = written_buffered;\r\ngoto out;\r\n}\r\nendbyte = pos + written_buffered - written - 1;\r\nerr = filemap_write_and_wait_range(file->f_mapping, pos, endbyte);\r\nif (err == 0) {\r\nwritten = written_buffered;\r\ninvalidate_mapping_pages(mapping,\r\npos >> PAGE_CACHE_SHIFT,\r\nendbyte >> PAGE_CACHE_SHIFT);\r\n} else {\r\n}\r\n} else {\r\nwritten = generic_file_buffered_write(iocb, iov, nr_segs,\r\npos, ppos, count, written);\r\n}\r\nout:\r\ncurrent->backing_dev_info = NULL;\r\nreturn written ? written : err;\r\n}\r\nssize_t generic_file_aio_write(struct kiocb *iocb, const struct iovec *iov,\r\nunsigned long nr_segs, loff_t pos)\r\n{\r\nstruct file *file = iocb->ki_filp;\r\nstruct inode *inode = file->f_mapping->host;\r\nssize_t ret;\r\nBUG_ON(iocb->ki_pos != pos);\r\nsb_start_write(inode->i_sb);\r\nmutex_lock(&inode->i_mutex);\r\nret = __generic_file_aio_write(iocb, iov, nr_segs, &iocb->ki_pos);\r\nmutex_unlock(&inode->i_mutex);\r\nif (ret > 0 || ret == -EIOCBQUEUED) {\r\nssize_t err;\r\nerr = generic_write_sync(file, pos, ret);\r\nif (err < 0 && ret > 0)\r\nret = err;\r\n}\r\nsb_end_write(inode->i_sb);\r\nreturn ret;\r\n}\r\nint try_to_release_page(struct page *page, gfp_t gfp_mask)\r\n{\r\nstruct address_space * const mapping = page->mapping;\r\nBUG_ON(!PageLocked(page));\r\nif (PageWriteback(page))\r\nreturn 0;\r\nif (mapping && mapping->a_ops->releasepage)\r\nreturn mapping->a_ops->releasepage(page, gfp_mask);\r\nreturn try_to_free_buffers(page);\r\n}
