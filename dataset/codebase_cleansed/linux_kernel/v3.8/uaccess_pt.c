static __always_inline unsigned long follow_table(struct mm_struct *mm,\r\nunsigned long addr, int write)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *ptep;\r\npgd = pgd_offset(mm, addr);\r\nif (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))\r\nreturn -0x3aUL;\r\npud = pud_offset(pgd, addr);\r\nif (pud_none(*pud) || unlikely(pud_bad(*pud)))\r\nreturn -0x3bUL;\r\npmd = pmd_offset(pud, addr);\r\nif (pmd_none(*pmd))\r\nreturn -0x10UL;\r\nif (pmd_large(*pmd)) {\r\nif (write && (pmd_val(*pmd) & _SEGMENT_ENTRY_RO))\r\nreturn -0x04UL;\r\nreturn (pmd_val(*pmd) & HPAGE_MASK) + (addr & ~HPAGE_MASK);\r\n}\r\nif (unlikely(pmd_bad(*pmd)))\r\nreturn -0x10UL;\r\nptep = pte_offset_map(pmd, addr);\r\nif (!pte_present(*ptep))\r\nreturn -0x11UL;\r\nif (write && !pte_write(*ptep))\r\nreturn -0x04UL;\r\nreturn (pte_val(*ptep) & PAGE_MASK) + (addr & ~PAGE_MASK);\r\n}\r\nstatic __always_inline size_t __user_copy_pt(unsigned long uaddr, void *kptr,\r\nsize_t n, int write_user)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long offset, done, size, kaddr;\r\nvoid *from, *to;\r\ndone = 0;\r\nretry:\r\nspin_lock(&mm->page_table_lock);\r\ndo {\r\nkaddr = follow_table(mm, uaddr, write_user);\r\nif (IS_ERR_VALUE(kaddr))\r\ngoto fault;\r\noffset = uaddr & ~PAGE_MASK;\r\nsize = min(n - done, PAGE_SIZE - offset);\r\nif (write_user) {\r\nto = (void *) kaddr;\r\nfrom = kptr + done;\r\n} else {\r\nfrom = (void *) kaddr;\r\nto = kptr + done;\r\n}\r\nmemcpy(to, from, size);\r\ndone += size;\r\nuaddr += size;\r\n} while (done < n);\r\nspin_unlock(&mm->page_table_lock);\r\nreturn n - done;\r\nfault:\r\nspin_unlock(&mm->page_table_lock);\r\nif (__handle_fault(uaddr, -kaddr, write_user))\r\nreturn n - done;\r\ngoto retry;\r\n}\r\nstatic __always_inline unsigned long __dat_user_addr(unsigned long uaddr,\r\nint write)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long kaddr;\r\nint rc;\r\nretry:\r\nkaddr = follow_table(mm, uaddr, write);\r\nif (IS_ERR_VALUE(kaddr))\r\ngoto fault;\r\nreturn kaddr;\r\nfault:\r\nspin_unlock(&mm->page_table_lock);\r\nrc = __handle_fault(uaddr, -kaddr, write);\r\nspin_lock(&mm->page_table_lock);\r\nif (!rc)\r\ngoto retry;\r\nreturn 0;\r\n}\r\nsize_t copy_from_user_pt(size_t n, const void __user *from, void *to)\r\n{\r\nsize_t rc;\r\nif (segment_eq(get_fs(), KERNEL_DS)) {\r\nmemcpy(to, (void __kernel __force *) from, n);\r\nreturn 0;\r\n}\r\nrc = __user_copy_pt((unsigned long) from, to, n, 0);\r\nif (unlikely(rc))\r\nmemset(to + n - rc, 0, rc);\r\nreturn rc;\r\n}\r\nsize_t copy_to_user_pt(size_t n, void __user *to, const void *from)\r\n{\r\nif (segment_eq(get_fs(), KERNEL_DS)) {\r\nmemcpy((void __kernel __force *) to, from, n);\r\nreturn 0;\r\n}\r\nreturn __user_copy_pt((unsigned long) to, (void *) from, n, 1);\r\n}\r\nstatic size_t clear_user_pt(size_t n, void __user *to)\r\n{\r\nlong done, size, ret;\r\nif (segment_eq(get_fs(), KERNEL_DS)) {\r\nmemset((void __kernel __force *) to, 0, n);\r\nreturn 0;\r\n}\r\ndone = 0;\r\ndo {\r\nif (n - done > PAGE_SIZE)\r\nsize = PAGE_SIZE;\r\nelse\r\nsize = n - done;\r\nret = __user_copy_pt((unsigned long) to + done,\r\n&empty_zero_page, size, 1);\r\ndone += size;\r\nif (ret)\r\nreturn ret + n - done;\r\n} while (done < n);\r\nreturn 0;\r\n}\r\nstatic size_t strnlen_user_pt(size_t count, const char __user *src)\r\n{\r\nunsigned long uaddr = (unsigned long) src;\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long offset, done, len, kaddr;\r\nsize_t len_str;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn strnlen((const char __kernel __force *) src, count) + 1;\r\ndone = 0;\r\nretry:\r\nspin_lock(&mm->page_table_lock);\r\ndo {\r\nkaddr = follow_table(mm, uaddr, 0);\r\nif (IS_ERR_VALUE(kaddr))\r\ngoto fault;\r\noffset = uaddr & ~PAGE_MASK;\r\nlen = min(count - done, PAGE_SIZE - offset);\r\nlen_str = strnlen((char *) kaddr, len);\r\ndone += len_str;\r\nuaddr += len_str;\r\n} while ((len_str == len) && (done < count));\r\nspin_unlock(&mm->page_table_lock);\r\nreturn done + 1;\r\nfault:\r\nspin_unlock(&mm->page_table_lock);\r\nif (__handle_fault(uaddr, -kaddr, 0))\r\nreturn 0;\r\ngoto retry;\r\n}\r\nstatic size_t strncpy_from_user_pt(size_t count, const char __user *src,\r\nchar *dst)\r\n{\r\nsize_t n = strnlen_user_pt(count, src);\r\nif (!n)\r\nreturn -EFAULT;\r\nif (n > count)\r\nn = count;\r\nif (segment_eq(get_fs(), KERNEL_DS)) {\r\nmemcpy(dst, (const char __kernel __force *) src, n);\r\nif (dst[n-1] == '\0')\r\nreturn n-1;\r\nelse\r\nreturn n;\r\n}\r\nif (__user_copy_pt((unsigned long) src, dst, n, 0))\r\nreturn -EFAULT;\r\nif (dst[n-1] == '\0')\r\nreturn n-1;\r\nelse\r\nreturn n;\r\n}\r\nstatic size_t copy_in_user_pt(size_t n, void __user *to,\r\nconst void __user *from)\r\n{\r\nstruct mm_struct *mm = current->mm;\r\nunsigned long offset_max, uaddr, done, size, error_code;\r\nunsigned long uaddr_from = (unsigned long) from;\r\nunsigned long uaddr_to = (unsigned long) to;\r\nunsigned long kaddr_to, kaddr_from;\r\nint write_user;\r\nif (segment_eq(get_fs(), KERNEL_DS)) {\r\nmemcpy((void __force *) to, (void __force *) from, n);\r\nreturn 0;\r\n}\r\ndone = 0;\r\nretry:\r\nspin_lock(&mm->page_table_lock);\r\ndo {\r\nwrite_user = 0;\r\nuaddr = uaddr_from;\r\nkaddr_from = follow_table(mm, uaddr_from, 0);\r\nerror_code = kaddr_from;\r\nif (IS_ERR_VALUE(error_code))\r\ngoto fault;\r\nwrite_user = 1;\r\nuaddr = uaddr_to;\r\nkaddr_to = follow_table(mm, uaddr_to, 1);\r\nerror_code = (unsigned long) kaddr_to;\r\nif (IS_ERR_VALUE(error_code))\r\ngoto fault;\r\noffset_max = max(uaddr_from & ~PAGE_MASK,\r\nuaddr_to & ~PAGE_MASK);\r\nsize = min(n - done, PAGE_SIZE - offset_max);\r\nmemcpy((void *) kaddr_to, (void *) kaddr_from, size);\r\ndone += size;\r\nuaddr_from += size;\r\nuaddr_to += size;\r\n} while (done < n);\r\nspin_unlock(&mm->page_table_lock);\r\nreturn n - done;\r\nfault:\r\nspin_unlock(&mm->page_table_lock);\r\nif (__handle_fault(uaddr, -error_code, write_user))\r\nreturn n - done;\r\ngoto retry;\r\n}\r\nstatic int __futex_atomic_op_pt(int op, u32 __user *uaddr, int oparg, int *old)\r\n{\r\nint oldval = 0, newval, ret;\r\nswitch (op) {\r\ncase FUTEX_OP_SET:\r\n__futex_atomic_op("lr %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ncase FUTEX_OP_ADD:\r\n__futex_atomic_op("lr %2,%1\nar %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ncase FUTEX_OP_OR:\r\n__futex_atomic_op("lr %2,%1\nor %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ncase FUTEX_OP_ANDN:\r\n__futex_atomic_op("lr %2,%1\nnr %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ncase FUTEX_OP_XOR:\r\n__futex_atomic_op("lr %2,%1\nxr %2,%5\n",\r\nret, oldval, newval, uaddr, oparg);\r\nbreak;\r\ndefault:\r\nret = -ENOSYS;\r\n}\r\nif (ret == 0)\r\n*old = oldval;\r\nreturn ret;\r\n}\r\nint futex_atomic_op_pt(int op, u32 __user *uaddr, int oparg, int *old)\r\n{\r\nint ret;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn __futex_atomic_op_pt(op, uaddr, oparg, old);\r\nspin_lock(&current->mm->page_table_lock);\r\nuaddr = (u32 __force __user *)\r\n__dat_user_addr((__force unsigned long) uaddr, 1);\r\nif (!uaddr) {\r\nspin_unlock(&current->mm->page_table_lock);\r\nreturn -EFAULT;\r\n}\r\nget_page(virt_to_page(uaddr));\r\nspin_unlock(&current->mm->page_table_lock);\r\nret = __futex_atomic_op_pt(op, uaddr, oparg, old);\r\nput_page(virt_to_page(uaddr));\r\nreturn ret;\r\n}\r\nstatic int __futex_atomic_cmpxchg_pt(u32 *uval, u32 __user *uaddr,\r\nu32 oldval, u32 newval)\r\n{\r\nint ret;\r\nasm volatile("0: cs %1,%4,0(%5)\n"\r\n"1: la %0,0\n"\r\n"2:\n"\r\nEX_TABLE(0b,2b) EX_TABLE(1b,2b)\r\n: "=d" (ret), "+d" (oldval), "=m" (*uaddr)\r\n: "0" (-EFAULT), "d" (newval), "a" (uaddr), "m" (*uaddr)\r\n: "cc", "memory" );\r\n*uval = oldval;\r\nreturn ret;\r\n}\r\nint futex_atomic_cmpxchg_pt(u32 *uval, u32 __user *uaddr,\r\nu32 oldval, u32 newval)\r\n{\r\nint ret;\r\nif (segment_eq(get_fs(), KERNEL_DS))\r\nreturn __futex_atomic_cmpxchg_pt(uval, uaddr, oldval, newval);\r\nspin_lock(&current->mm->page_table_lock);\r\nuaddr = (u32 __force __user *)\r\n__dat_user_addr((__force unsigned long) uaddr, 1);\r\nif (!uaddr) {\r\nspin_unlock(&current->mm->page_table_lock);\r\nreturn -EFAULT;\r\n}\r\nget_page(virt_to_page(uaddr));\r\nspin_unlock(&current->mm->page_table_lock);\r\nret = __futex_atomic_cmpxchg_pt(uval, uaddr, oldval, newval);\r\nput_page(virt_to_page(uaddr));\r\nreturn ret;\r\n}
