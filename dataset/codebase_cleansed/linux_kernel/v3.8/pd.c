int mlx4_pd_alloc(struct mlx4_dev *dev, u32 *pdn)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\n*pdn = mlx4_bitmap_alloc(&priv->pd_bitmap);\r\nif (*pdn == -1)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid mlx4_pd_free(struct mlx4_dev *dev, u32 pdn)\r\n{\r\nmlx4_bitmap_free(&mlx4_priv(dev)->pd_bitmap, pdn);\r\n}\r\nint __mlx4_xrcd_alloc(struct mlx4_dev *dev, u32 *xrcdn)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\n*xrcdn = mlx4_bitmap_alloc(&priv->xrcd_bitmap);\r\nif (*xrcdn == -1)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nint mlx4_xrcd_alloc(struct mlx4_dev *dev, u32 *xrcdn)\r\n{\r\nu64 out_param;\r\nint err;\r\nif (mlx4_is_mfunc(dev)) {\r\nerr = mlx4_cmd_imm(dev, 0, &out_param,\r\nRES_XRCD, RES_OP_RESERVE,\r\nMLX4_CMD_ALLOC_RES,\r\nMLX4_CMD_TIME_CLASS_A, MLX4_CMD_WRAPPED);\r\nif (err)\r\nreturn err;\r\n*xrcdn = get_param_l(&out_param);\r\nreturn 0;\r\n}\r\nreturn __mlx4_xrcd_alloc(dev, xrcdn);\r\n}\r\nvoid __mlx4_xrcd_free(struct mlx4_dev *dev, u32 xrcdn)\r\n{\r\nmlx4_bitmap_free(&mlx4_priv(dev)->xrcd_bitmap, xrcdn);\r\n}\r\nvoid mlx4_xrcd_free(struct mlx4_dev *dev, u32 xrcdn)\r\n{\r\nu64 in_param;\r\nint err;\r\nif (mlx4_is_mfunc(dev)) {\r\nset_param_l(&in_param, xrcdn);\r\nerr = mlx4_cmd(dev, in_param, RES_XRCD,\r\nRES_OP_RESERVE, MLX4_CMD_FREE_RES,\r\nMLX4_CMD_TIME_CLASS_A, MLX4_CMD_WRAPPED);\r\nif (err)\r\nmlx4_warn(dev, "Failed to release xrcdn %d\n", xrcdn);\r\n} else\r\n__mlx4_xrcd_free(dev, xrcdn);\r\n}\r\nint mlx4_init_pd_table(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nreturn mlx4_bitmap_init(&priv->pd_bitmap, dev->caps.num_pds,\r\n(1 << NOT_MASKED_PD_BITS) - 1,\r\ndev->caps.reserved_pds, 0);\r\n}\r\nvoid mlx4_cleanup_pd_table(struct mlx4_dev *dev)\r\n{\r\nmlx4_bitmap_cleanup(&mlx4_priv(dev)->pd_bitmap);\r\n}\r\nint mlx4_init_xrcd_table(struct mlx4_dev *dev)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nreturn mlx4_bitmap_init(&priv->xrcd_bitmap, (1 << 16),\r\n(1 << 16) - 1, dev->caps.reserved_xrcds + 1, 0);\r\n}\r\nvoid mlx4_cleanup_xrcd_table(struct mlx4_dev *dev)\r\n{\r\nmlx4_bitmap_cleanup(&mlx4_priv(dev)->xrcd_bitmap);\r\n}\r\nint mlx4_uar_alloc(struct mlx4_dev *dev, struct mlx4_uar *uar)\r\n{\r\nint offset;\r\nuar->index = mlx4_bitmap_alloc(&mlx4_priv(dev)->uar_table.bitmap);\r\nif (uar->index == -1)\r\nreturn -ENOMEM;\r\nif (mlx4_is_slave(dev))\r\noffset = uar->index % ((int) pci_resource_len(dev->pdev, 2) /\r\ndev->caps.uar_page_size);\r\nelse\r\noffset = uar->index;\r\nuar->pfn = (pci_resource_start(dev->pdev, 2) >> PAGE_SHIFT) + offset;\r\nuar->map = NULL;\r\nreturn 0;\r\n}\r\nvoid mlx4_uar_free(struct mlx4_dev *dev, struct mlx4_uar *uar)\r\n{\r\nmlx4_bitmap_free(&mlx4_priv(dev)->uar_table.bitmap, uar->index);\r\n}\r\nint mlx4_bf_alloc(struct mlx4_dev *dev, struct mlx4_bf *bf)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nstruct mlx4_uar *uar;\r\nint err = 0;\r\nint idx;\r\nif (!priv->bf_mapping)\r\nreturn -ENOMEM;\r\nmutex_lock(&priv->bf_mutex);\r\nif (!list_empty(&priv->bf_list))\r\nuar = list_entry(priv->bf_list.next, struct mlx4_uar, bf_list);\r\nelse {\r\nif (mlx4_bitmap_avail(&priv->uar_table.bitmap) < MLX4_NUM_RESERVED_UARS) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nuar = kmalloc(sizeof *uar, GFP_KERNEL);\r\nif (!uar) {\r\nerr = -ENOMEM;\r\ngoto out;\r\n}\r\nerr = mlx4_uar_alloc(dev, uar);\r\nif (err)\r\ngoto free_kmalloc;\r\nuar->map = ioremap(uar->pfn << PAGE_SHIFT, PAGE_SIZE);\r\nif (!uar->map) {\r\nerr = -ENOMEM;\r\ngoto free_uar;\r\n}\r\nuar->bf_map = io_mapping_map_wc(priv->bf_mapping, uar->index << PAGE_SHIFT);\r\nif (!uar->bf_map) {\r\nerr = -ENOMEM;\r\ngoto unamp_uar;\r\n}\r\nuar->free_bf_bmap = 0;\r\nlist_add(&uar->bf_list, &priv->bf_list);\r\n}\r\nbf->uar = uar;\r\nidx = ffz(uar->free_bf_bmap);\r\nuar->free_bf_bmap |= 1 << idx;\r\nbf->uar = uar;\r\nbf->offset = 0;\r\nbf->buf_size = dev->caps.bf_reg_size / 2;\r\nbf->reg = uar->bf_map + idx * dev->caps.bf_reg_size;\r\nif (uar->free_bf_bmap == (1 << dev->caps.bf_regs_per_page) - 1)\r\nlist_del_init(&uar->bf_list);\r\ngoto out;\r\nunamp_uar:\r\nbf->uar = NULL;\r\niounmap(uar->map);\r\nfree_uar:\r\nmlx4_uar_free(dev, uar);\r\nfree_kmalloc:\r\nkfree(uar);\r\nout:\r\nmutex_unlock(&priv->bf_mutex);\r\nreturn err;\r\n}\r\nvoid mlx4_bf_free(struct mlx4_dev *dev, struct mlx4_bf *bf)\r\n{\r\nstruct mlx4_priv *priv = mlx4_priv(dev);\r\nint idx;\r\nif (!bf->uar || !bf->uar->bf_map)\r\nreturn;\r\nmutex_lock(&priv->bf_mutex);\r\nidx = (bf->reg - bf->uar->bf_map) / dev->caps.bf_reg_size;\r\nbf->uar->free_bf_bmap &= ~(1 << idx);\r\nif (!bf->uar->free_bf_bmap) {\r\nif (!list_empty(&bf->uar->bf_list))\r\nlist_del(&bf->uar->bf_list);\r\nio_mapping_unmap(bf->uar->bf_map);\r\niounmap(bf->uar->map);\r\nmlx4_uar_free(dev, bf->uar);\r\nkfree(bf->uar);\r\n} else if (list_empty(&bf->uar->bf_list))\r\nlist_add(&bf->uar->bf_list, &priv->bf_list);\r\nmutex_unlock(&priv->bf_mutex);\r\n}\r\nint mlx4_init_uar_table(struct mlx4_dev *dev)\r\n{\r\nif (dev->caps.num_uars <= 128) {\r\nmlx4_err(dev, "Only %d UAR pages (need more than 128)\n",\r\ndev->caps.num_uars);\r\nmlx4_err(dev, "Increase firmware log2_uar_bar_megabytes?\n");\r\nreturn -ENODEV;\r\n}\r\nreturn mlx4_bitmap_init(&mlx4_priv(dev)->uar_table.bitmap,\r\ndev->caps.num_uars, dev->caps.num_uars - 1,\r\ndev->caps.reserved_uars, 0);\r\n}\r\nvoid mlx4_cleanup_uar_table(struct mlx4_dev *dev)\r\n{\r\nmlx4_bitmap_cleanup(&mlx4_priv(dev)->uar_table.bitmap);\r\n}
