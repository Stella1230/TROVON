struct ib_pd *ehca_alloc_pd(struct ib_device *device,\r\nstruct ib_ucontext *context, struct ib_udata *udata)\r\n{\r\nstruct ehca_pd *pd;\r\nint i;\r\npd = kmem_cache_zalloc(pd_cache, GFP_KERNEL);\r\nif (!pd) {\r\nehca_err(device, "device=%p context=%p out of memory",\r\ndevice, context);\r\nreturn ERR_PTR(-ENOMEM);\r\n}\r\nfor (i = 0; i < 2; i++) {\r\nINIT_LIST_HEAD(&pd->free[i]);\r\nINIT_LIST_HEAD(&pd->full[i]);\r\n}\r\nmutex_init(&pd->lock);\r\nif (!context) {\r\nstruct ehca_shca *shca = container_of(device, struct ehca_shca,\r\nib_device);\r\npd->fw_pd.value = shca->pd->fw_pd.value;\r\n} else\r\npd->fw_pd.value = (u64)pd;\r\nreturn &pd->ib_pd;\r\n}\r\nint ehca_dealloc_pd(struct ib_pd *pd)\r\n{\r\nstruct ehca_pd *my_pd = container_of(pd, struct ehca_pd, ib_pd);\r\nint i, leftovers = 0;\r\nstruct ipz_small_queue_page *page, *tmp;\r\nfor (i = 0; i < 2; i++) {\r\nlist_splice(&my_pd->full[i], &my_pd->free[i]);\r\nlist_for_each_entry_safe(page, tmp, &my_pd->free[i], list) {\r\nleftovers = 1;\r\nfree_page(page->page);\r\nkmem_cache_free(small_qp_cache, page);\r\n}\r\n}\r\nif (leftovers)\r\nehca_warn(pd->device,\r\n"Some small queue pages were not freed");\r\nkmem_cache_free(pd_cache, my_pd);\r\nreturn 0;\r\n}\r\nint ehca_init_pd_cache(void)\r\n{\r\npd_cache = kmem_cache_create("ehca_cache_pd",\r\nsizeof(struct ehca_pd), 0,\r\nSLAB_HWCACHE_ALIGN,\r\nNULL);\r\nif (!pd_cache)\r\nreturn -ENOMEM;\r\nreturn 0;\r\n}\r\nvoid ehca_cleanup_pd_cache(void)\r\n{\r\nif (pd_cache)\r\nkmem_cache_destroy(pd_cache);\r\n}
