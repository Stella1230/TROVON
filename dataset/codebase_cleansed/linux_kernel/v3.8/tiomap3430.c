static inline void flush_all(struct bridge_dev_context *dev_context)\r\n{\r\nif (dev_context->brd_state == BRD_DSP_HIBERNATION ||\r\ndev_context->brd_state == BRD_HIBERNATION)\r\nwake_dsp(dev_context, NULL);\r\nhw_mmu_tlb_flush_all(dev_context->dsp_mmu_base);\r\n}\r\nstatic void bad_page_dump(u32 pa, struct page *pg)\r\n{\r\npr_emerg("DSPBRIDGE: MAP function: COUNT 0 FOR PA 0x%x\n", pa);\r\npr_emerg("Bad page state in process '%s'\n"\r\n"page:%p flags:0x%0*lx mapping:%p mapcount:%d count:%d\n"\r\n"Backtrace:\n",\r\ncurrent->comm, pg, (int)(2 * sizeof(unsigned long)),\r\n(unsigned long)pg->flags, pg->mapping,\r\npage_mapcount(pg), page_count(pg));\r\ndump_stack();\r\n}\r\nvoid bridge_drv_entry(struct bridge_drv_interface **drv_intf,\r\nconst char *driver_file_name)\r\n{\r\nif (strcmp(driver_file_name, "UMA") == 0)\r\n*drv_intf = &drv_interface_fxns;\r\nelse\r\ndev_dbg(bridge, "%s Unknown Bridge file name", __func__);\r\n}\r\nstatic int bridge_brd_monitor(struct bridge_dev_context *dev_ctxt)\r\n{\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nu32 temp;\r\nstruct omap_dsp_platform_data *pdata =\r\nomap_dspbridge_dev->dev.platform_data;\r\ntemp = (*pdata->dsp_prm_read)(OMAP3430_IVA2_MOD, OMAP2_PM_PWSTST) &\r\nOMAP_POWERSTATEST_MASK;\r\nif (!(temp & 0x02)) {\r\n(*pdata->dsp_prm_rmw_bits)(OMAP_POWERSTATEST_MASK,\r\nPWRDM_POWER_ON, OMAP3430_IVA2_MOD, OMAP2_PM_PWSTCTRL);\r\n(*pdata->dsp_cm_write)(OMAP34XX_CLKSTCTRL_FORCE_WAKEUP,\r\nOMAP3430_IVA2_MOD, OMAP2_CM_CLKSTCTRL);\r\nwhile ((*pdata->dsp_prm_read)(OMAP3430_IVA2_MOD, OMAP2_PM_PWSTST) &\r\nOMAP_INTRANSITION_MASK)\r\n;\r\n(*pdata->dsp_cm_write)(OMAP34XX_CLKSTCTRL_DISABLE_AUTO,\r\nOMAP3430_IVA2_MOD, OMAP2_CM_CLKSTCTRL);\r\n}\r\n(*pdata->dsp_prm_rmw_bits)(OMAP3430_RST2_IVA2_MASK, 0,\r\nOMAP3430_IVA2_MOD, OMAP2_RM_RSTCTRL);\r\ndsp_clk_enable(DSP_CLK_IVA2);\r\ndev_context->brd_state = BRD_IDLE;\r\nreturn 0;\r\n}\r\nstatic int bridge_brd_read(struct bridge_dev_context *dev_ctxt,\r\nu8 *host_buff, u32 dsp_addr,\r\nu32 ul_num_bytes, u32 mem_type)\r\n{\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nu32 offset;\r\nu32 dsp_base_addr = dev_ctxt->dsp_base_addr;\r\nif (dsp_addr < dev_context->dsp_start_add) {\r\nstatus = -EPERM;\r\nreturn status;\r\n}\r\nif ((dsp_addr - dev_context->dsp_start_add) <\r\ndev_context->internal_size) {\r\noffset = dsp_addr - dev_context->dsp_start_add;\r\n} else {\r\nstatus = read_ext_dsp_data(dev_context, host_buff, dsp_addr,\r\nul_num_bytes, mem_type);\r\nreturn status;\r\n}\r\nmemcpy(host_buff, (void *)(dsp_base_addr + offset), ul_num_bytes);\r\nreturn status;\r\n}\r\nstatic int bridge_brd_set_state(struct bridge_dev_context *dev_ctxt,\r\nu32 brd_state)\r\n{\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\ndev_context->brd_state = brd_state;\r\nreturn status;\r\n}\r\nstatic int bridge_brd_start(struct bridge_dev_context *dev_ctxt,\r\nu32 dsp_addr)\r\n{\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nvoid __iomem *sync_addr;\r\nu32 ul_shm_base;\r\nu32 ul_shm_base_virt;\r\nu32 ul_tlb_base_virt;\r\nu32 shm_sync_pa;\r\nu32 ul_shm_offset_virt;\r\ns32 entry_ndx;\r\ns32 itmp_entry_ndx = 0;\r\nstruct cfg_hostres *resources = NULL;\r\nu32 temp;\r\nu32 ul_dsp_clk_rate;\r\nu32 ul_dsp_clk_addr;\r\nu32 ul_bios_gp_timer;\r\nu32 clk_cmd;\r\nstruct io_mgr *hio_mgr;\r\nu32 ul_load_monitor_timer;\r\nu32 wdt_en = 0;\r\nstruct omap_dsp_platform_data *pdata =\r\nomap_dspbridge_dev->dev.platform_data;\r\n(void)dev_get_symbol(dev_context->dev_obj, SHMBASENAME,\r\n&ul_shm_base_virt);\r\nul_shm_base_virt *= DSPWORDSIZE;\r\nul_tlb_base_virt = dev_context->atlb_entry[0].dsp_va;\r\nul_shm_offset_virt =\r\nul_shm_base_virt - (ul_tlb_base_virt * DSPWORDSIZE);\r\nul_shm_base = dev_context->atlb_entry[0].gpp_va + ul_shm_offset_virt;\r\nshm_sync_pa = dev_context->atlb_entry[0].gpp_pa + ul_shm_offset_virt +\r\nSHMSYNCOFFSET;\r\nsync_addr = ioremap(shm_sync_pa, SZ_32);\r\nif (!sync_addr)\r\nreturn -ENOMEM;\r\nif ((ul_shm_base_virt == 0) || (ul_shm_base == 0)) {\r\npr_err("%s: Illegal SM base\n", __func__);\r\nstatus = -EPERM;\r\n} else\r\n__raw_writel(0xffffffff, sync_addr);\r\nif (!status) {\r\nresources = dev_context->resources;\r\nif (!resources)\r\nstatus = -EPERM;\r\nif (!status) {\r\nvoid __iomem *ctrl = ioremap(0x48002000, SZ_4K);\r\nif (!ctrl) {\r\niounmap(sync_addr);\r\nreturn -ENOMEM;\r\n}\r\n(*pdata->dsp_prm_rmw_bits)(OMAP3430_RST1_IVA2_MASK,\r\nOMAP3430_RST1_IVA2_MASK, OMAP3430_IVA2_MOD,\r\nOMAP2_RM_RSTCTRL);\r\n__raw_writel(dsp_addr & OMAP3_IVA2_BOOTADDR_MASK,\r\nctrl + OMAP343X_CONTROL_IVA2_BOOTADDR);\r\n__raw_writel((dsp_debug) ? OMAP3_IVA2_BOOTMOD_IDLE : 0,\r\nctrl + OMAP343X_CONTROL_IVA2_BOOTMOD);\r\niounmap(ctrl);\r\n}\r\n}\r\nif (!status) {\r\n(*pdata->dsp_prm_rmw_bits)(OMAP3430_RST2_IVA2_MASK,\r\nOMAP3430_RST2_IVA2_MASK, OMAP3430_IVA2_MOD, OMAP2_RM_RSTCTRL);\r\nudelay(100);\r\n(*pdata->dsp_prm_rmw_bits)(OMAP3430_RST2_IVA2_MASK, 0,\r\nOMAP3430_IVA2_MOD, OMAP2_RM_RSTCTRL);\r\nudelay(100);\r\nhw_mmu_disable(resources->dmmu_base);\r\nhw_mmu_twl_disable(resources->dmmu_base);\r\nfor (entry_ndx = 0; entry_ndx < BRDIOCTL_NUMOFMMUTLB;\r\nentry_ndx++) {\r\nstruct bridge_ioctl_extproc *e = &dev_context->atlb_entry[entry_ndx];\r\nstruct hw_mmu_map_attrs_t map_attrs = {\r\n.endianism = e->endianism,\r\n.element_size = e->elem_size,\r\n.mixed_size = e->mixed_mode,\r\n};\r\nif (!e->gpp_pa || !e->dsp_va)\r\ncontinue;\r\ndev_dbg(bridge,\r\n"MMU %d, pa: 0x%x, va: 0x%x, size: 0x%x",\r\nitmp_entry_ndx,\r\ne->gpp_pa,\r\ne->dsp_va,\r\ne->size);\r\nhw_mmu_tlb_add(dev_context->dsp_mmu_base,\r\ne->gpp_pa,\r\ne->dsp_va,\r\ne->size,\r\nitmp_entry_ndx,\r\n&map_attrs, 1, 1);\r\nitmp_entry_ndx++;\r\n}\r\n}\r\nif (!status) {\r\nhw_mmu_num_locked_set(resources->dmmu_base, itmp_entry_ndx);\r\nhw_mmu_victim_num_set(resources->dmmu_base, itmp_entry_ndx);\r\nhw_mmu_ttb_set(resources->dmmu_base,\r\ndev_context->pt_attrs->l1_base_pa);\r\nhw_mmu_twl_enable(resources->dmmu_base);\r\ntemp = __raw_readl((resources->dmmu_base) + 0x10);\r\ntemp = (temp & 0xFFFFFFEF) | 0x11;\r\n__raw_writel(temp, (resources->dmmu_base) + 0x10);\r\nhw_mmu_enable(resources->dmmu_base);\r\n(void)dev_get_symbol(dev_context->dev_obj,\r\nBRIDGEINIT_BIOSGPTIMER, &ul_bios_gp_timer);\r\n(void)dev_get_symbol(dev_context->dev_obj,\r\nBRIDGEINIT_LOADMON_GPTIMER,\r\n&ul_load_monitor_timer);\r\n}\r\nif (!status) {\r\nif (ul_load_monitor_timer != 0xFFFF) {\r\nclk_cmd = (BPWR_ENABLE_CLOCK << MBX_PM_CLK_CMDSHIFT) |\r\nul_load_monitor_timer;\r\ndsp_peripheral_clk_ctrl(dev_context, &clk_cmd);\r\n} else {\r\ndev_dbg(bridge, "Not able to get the symbol for Load "\r\n"Monitor Timer\n");\r\n}\r\n}\r\nif (!status) {\r\nif (ul_bios_gp_timer != 0xFFFF) {\r\nclk_cmd = (BPWR_ENABLE_CLOCK << MBX_PM_CLK_CMDSHIFT) |\r\nul_bios_gp_timer;\r\ndsp_peripheral_clk_ctrl(dev_context, &clk_cmd);\r\n} else {\r\ndev_dbg(bridge,\r\n"Not able to get the symbol for BIOS Timer\n");\r\n}\r\n}\r\nif (!status) {\r\n(void)dev_get_symbol(dev_context->dev_obj,\r\n"_BRIDGEINIT_DSP_FREQ", &ul_dsp_clk_addr);\r\n(*pdata->dsp_cm_write)(1 << OMAP3430_AUTO_IVA2_DPLL_SHIFT,\r\nOMAP3430_IVA2_MOD, OMAP3430_CM_AUTOIDLE_PLL);\r\nif ((unsigned int *)ul_dsp_clk_addr != NULL) {\r\nul_dsp_clk_rate = dsp_clk_get_iva2_rate();\r\ndev_dbg(bridge, "%s: DSP clock rate (KHZ): 0x%x \n",\r\n__func__, ul_dsp_clk_rate);\r\n(void)bridge_brd_write(dev_context,\r\n(u8 *) &ul_dsp_clk_rate,\r\nul_dsp_clk_addr, sizeof(u32), 0);\r\n}\r\ndev_context->mbox = omap_mbox_get("dsp", &dsp_mbox_notifier);\r\nif (IS_ERR(dev_context->mbox)) {\r\ndev_context->mbox = NULL;\r\npr_err("%s: Failed to get dsp mailbox handle\n",\r\n__func__);\r\nstatus = -EPERM;\r\n}\r\n}\r\nif (!status) {\r\ntemp = readl(resources->per_pm_base + 0xA8);\r\ntemp = (temp & 0xFFFFFF30) | 0xC0;\r\nwritel(temp, resources->per_pm_base + 0xA8);\r\ntemp = readl(resources->per_pm_base + 0xA4);\r\ntemp = (temp & 0xFFFFFF3F);\r\nwritel(temp, resources->per_pm_base + 0xA4);\r\ntemp = readl(resources->per_base + 0x44);\r\ntemp = (temp & 0xFFFFFFFB) | 0x04;\r\nwritel(temp, resources->per_base + 0x44);\r\n(*pdata->dsp_cm_write)(OMAP34XX_CLKSTCTRL_ENABLE_AUTO,\r\nOMAP3430_IVA2_MOD, OMAP2_CM_CLKSTCTRL);\r\ndev_dbg(bridge, "%s Unreset\n", __func__);\r\nhw_mmu_event_enable(resources->dmmu_base,\r\nHW_MMU_ALL_INTERRUPTS);\r\n(*pdata->dsp_prm_rmw_bits)(OMAP3430_RST1_IVA2_MASK, 0,\r\nOMAP3430_IVA2_MOD, OMAP2_RM_RSTCTRL);\r\ndev_dbg(bridge, "Waiting for Sync @ 0x%x\n", *(u32 *)sync_addr);\r\ndev_dbg(bridge, "DSP c_int00 Address = 0x%x\n", dsp_addr);\r\nif (dsp_debug)\r\nwhile (__raw_readw(sync_addr))\r\n;\r\nif (!wait_for_start(dev_context, sync_addr))\r\nstatus = -ETIMEDOUT;\r\ndev_get_symbol(dev_context->dev_obj, "_WDT_enable", &wdt_en);\r\nif (wdt_en) {\r\ndsp_wdt_sm_set((void *)ul_shm_base);\r\ndsp_wdt_enable(true);\r\n}\r\nstatus = dev_get_io_mgr(dev_context->dev_obj, &hio_mgr);\r\nif (hio_mgr) {\r\nio_sh_msetting(hio_mgr, SHM_OPPINFO, NULL);\r\n__raw_writel(0XCAFECAFE, sync_addr);\r\ndev_context->brd_state = BRD_RUNNING;\r\n} else {\r\ndev_context->brd_state = BRD_UNKNOWN;\r\n}\r\n}\r\niounmap(sync_addr);\r\nreturn status;\r\n}\r\nstatic int bridge_brd_stop(struct bridge_dev_context *dev_ctxt)\r\n{\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nstruct pg_table_attrs *pt_attrs;\r\nu32 dsp_pwr_state;\r\nstruct omap_dsp_platform_data *pdata =\r\nomap_dspbridge_dev->dev.platform_data;\r\nif (dev_context->brd_state == BRD_STOPPED)\r\nreturn status;\r\ndsp_pwr_state = (*pdata->dsp_prm_read)(OMAP3430_IVA2_MOD, OMAP2_PM_PWSTST) &\r\nOMAP_POWERSTATEST_MASK;\r\nif (dsp_pwr_state != PWRDM_POWER_OFF) {\r\n(*pdata->dsp_prm_rmw_bits)(OMAP3430_RST2_IVA2_MASK, 0,\r\nOMAP3430_IVA2_MOD, OMAP2_RM_RSTCTRL);\r\nsm_interrupt_dsp(dev_context, MBX_PM_DSPIDLE);\r\nmdelay(10);\r\n(*pdata->dsp_prm_rmw_bits)(OMAP_POWERSTATEST_MASK,\r\nPWRDM_POWER_OFF, OMAP3430_IVA2_MOD, OMAP2_PM_PWSTCTRL);\r\n(*pdata->dsp_cm_write)(OMAP34XX_CLKSTCTRL_FORCE_SLEEP,\r\nOMAP3430_IVA2_MOD, OMAP2_CM_CLKSTCTRL);\r\n}\r\nudelay(10);\r\nif (dev_context->dsp_ext_base_addr)\r\ndev_context->dsp_ext_base_addr = 0;\r\ndev_context->brd_state = BRD_STOPPED;\r\ndsp_wdt_enable(false);\r\nif (dev_context->pt_attrs) {\r\npt_attrs = dev_context->pt_attrs;\r\nmemset((u8 *) pt_attrs->l1_base_va, 0x00, pt_attrs->l1_size);\r\nmemset((u8 *) pt_attrs->l2_base_va, 0x00, pt_attrs->l2_size);\r\nmemset((u8 *) pt_attrs->pg_info, 0x00,\r\n(pt_attrs->l2_num_pages * sizeof(struct page_info)));\r\n}\r\nif (dev_context->mbox) {\r\nomap_mbox_disable_irq(dev_context->mbox, IRQ_RX);\r\nomap_mbox_put(dev_context->mbox, &dsp_mbox_notifier);\r\ndev_context->mbox = NULL;\r\n}\r\n(*pdata->dsp_prm_write)(OMAP3430_RST1_IVA2_MASK | OMAP3430_RST2_IVA2_MASK |\r\nOMAP3430_RST3_IVA2_MASK, OMAP3430_IVA2_MOD, OMAP2_RM_RSTCTRL);\r\ndsp_clock_disable_all(dev_context->dsp_per_clks);\r\ndsp_clk_disable(DSP_CLK_IVA2);\r\nreturn status;\r\n}\r\nstatic int bridge_brd_status(struct bridge_dev_context *dev_ctxt,\r\nint *board_state)\r\n{\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\n*board_state = dev_context->brd_state;\r\nreturn 0;\r\n}\r\nstatic int bridge_brd_write(struct bridge_dev_context *dev_ctxt,\r\nu8 *host_buff, u32 dsp_addr,\r\nu32 ul_num_bytes, u32 mem_type)\r\n{\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nif (dsp_addr < dev_context->dsp_start_add) {\r\nstatus = -EPERM;\r\nreturn status;\r\n}\r\nif ((dsp_addr - dev_context->dsp_start_add) <\r\ndev_context->internal_size) {\r\nstatus = write_dsp_data(dev_ctxt, host_buff, dsp_addr,\r\nul_num_bytes, mem_type);\r\n} else {\r\nstatus = write_ext_dsp_data(dev_context, host_buff, dsp_addr,\r\nul_num_bytes, mem_type, false);\r\n}\r\nreturn status;\r\n}\r\nstatic int bridge_dev_create(struct bridge_dev_context\r\n**dev_cntxt,\r\nstruct dev_object *hdev_obj,\r\nstruct cfg_hostres *config_param)\r\n{\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = NULL;\r\ns32 entry_ndx;\r\nstruct cfg_hostres *resources = config_param;\r\nstruct pg_table_attrs *pt_attrs;\r\nu32 pg_tbl_pa;\r\nu32 pg_tbl_va;\r\nu32 align_size;\r\nstruct drv_data *drv_datap = dev_get_drvdata(bridge);\r\ndev_context = kzalloc(sizeof(struct bridge_dev_context), GFP_KERNEL);\r\nif (!dev_context) {\r\nstatus = -ENOMEM;\r\ngoto func_end;\r\n}\r\ndev_context->dsp_start_add = (u32) OMAP_GEM_BASE;\r\ndev_context->self_loop = (u32) NULL;\r\ndev_context->dsp_per_clks = 0;\r\ndev_context->internal_size = OMAP_DSP_SIZE;\r\nfor (entry_ndx = 0; entry_ndx < BRDIOCTL_NUMOFMMUTLB; entry_ndx++) {\r\ndev_context->atlb_entry[entry_ndx].gpp_pa =\r\ndev_context->atlb_entry[entry_ndx].dsp_va = 0;\r\n}\r\ndev_context->dsp_base_addr = (u32) MEM_LINEAR_ADDRESS((void *)\r\n(config_param->\r\nmem_base\r\n[3]),\r\nconfig_param->\r\nmem_length\r\n[3]);\r\nif (!dev_context->dsp_base_addr)\r\nstatus = -EPERM;\r\npt_attrs = kzalloc(sizeof(struct pg_table_attrs), GFP_KERNEL);\r\nif (pt_attrs != NULL) {\r\npt_attrs->l1_size = SZ_16K;\r\nalign_size = pt_attrs->l1_size;\r\npg_tbl_va = (u32) mem_alloc_phys_mem(pt_attrs->l1_size,\r\nalign_size, &pg_tbl_pa);\r\nif ((pg_tbl_pa) & (align_size - 1)) {\r\nmem_free_phys_mem((void *)pg_tbl_va, pg_tbl_pa,\r\npt_attrs->l1_size);\r\npg_tbl_va =\r\n(u32) mem_alloc_phys_mem((pt_attrs->l1_size) * 2,\r\nalign_size, &pg_tbl_pa);\r\npt_attrs->l1_tbl_alloc_pa = pg_tbl_pa;\r\npt_attrs->l1_tbl_alloc_va = pg_tbl_va;\r\npt_attrs->l1_tbl_alloc_sz = pt_attrs->l1_size * 2;\r\npt_attrs->l1_base_pa =\r\n((pg_tbl_pa) +\r\n(align_size - 1)) & (~(align_size - 1));\r\npt_attrs->l1_base_va =\r\npg_tbl_va + (pt_attrs->l1_base_pa - pg_tbl_pa);\r\n} else {\r\npt_attrs->l1_tbl_alloc_pa = pg_tbl_pa;\r\npt_attrs->l1_tbl_alloc_va = pg_tbl_va;\r\npt_attrs->l1_tbl_alloc_sz = pt_attrs->l1_size;\r\npt_attrs->l1_base_pa = pg_tbl_pa;\r\npt_attrs->l1_base_va = pg_tbl_va;\r\n}\r\nif (pt_attrs->l1_base_va)\r\nmemset((u8 *) pt_attrs->l1_base_va, 0x00,\r\npt_attrs->l1_size);\r\npt_attrs->l2_num_pages = ((DMMPOOLSIZE >> 20) + 6);\r\npt_attrs->l2_size = HW_MMU_COARSE_PAGE_SIZE *\r\npt_attrs->l2_num_pages;\r\nalign_size = 4;\r\npg_tbl_va = (u32) mem_alloc_phys_mem(pt_attrs->l2_size,\r\nalign_size, &pg_tbl_pa);\r\npt_attrs->l2_tbl_alloc_pa = pg_tbl_pa;\r\npt_attrs->l2_tbl_alloc_va = pg_tbl_va;\r\npt_attrs->l2_tbl_alloc_sz = pt_attrs->l2_size;\r\npt_attrs->l2_base_pa = pg_tbl_pa;\r\npt_attrs->l2_base_va = pg_tbl_va;\r\nif (pt_attrs->l2_base_va)\r\nmemset((u8 *) pt_attrs->l2_base_va, 0x00,\r\npt_attrs->l2_size);\r\npt_attrs->pg_info = kzalloc(pt_attrs->l2_num_pages *\r\nsizeof(struct page_info), GFP_KERNEL);\r\ndev_dbg(bridge,\r\n"L1 pa %x, va %x, size %x\n L2 pa %x, va "\r\n"%x, size %x\n", pt_attrs->l1_base_pa,\r\npt_attrs->l1_base_va, pt_attrs->l1_size,\r\npt_attrs->l2_base_pa, pt_attrs->l2_base_va,\r\npt_attrs->l2_size);\r\ndev_dbg(bridge, "pt_attrs %p L2 NumPages %x pg_info %p\n",\r\npt_attrs, pt_attrs->l2_num_pages, pt_attrs->pg_info);\r\n}\r\nif ((pt_attrs != NULL) && (pt_attrs->l1_base_va != 0) &&\r\n(pt_attrs->l2_base_va != 0) && (pt_attrs->pg_info != NULL))\r\ndev_context->pt_attrs = pt_attrs;\r\nelse\r\nstatus = -ENOMEM;\r\nif (!status) {\r\nspin_lock_init(&pt_attrs->pg_lock);\r\ndev_context->tc_word_swap_on = drv_datap->tc_wordswapon;\r\nudelay(5);\r\ndev_context->dsp_mmu_base = resources->dmmu_base;\r\n}\r\nif (!status) {\r\ndev_context->dev_obj = hdev_obj;\r\ndev_context->brd_state = BRD_UNKNOWN;\r\ndev_context->resources = resources;\r\ndsp_clk_enable(DSP_CLK_IVA2);\r\nbridge_brd_stop(dev_context);\r\n*dev_cntxt = dev_context;\r\n} else {\r\nif (pt_attrs != NULL) {\r\nkfree(pt_attrs->pg_info);\r\nif (pt_attrs->l2_tbl_alloc_va) {\r\nmem_free_phys_mem((void *)\r\npt_attrs->l2_tbl_alloc_va,\r\npt_attrs->l2_tbl_alloc_pa,\r\npt_attrs->l2_tbl_alloc_sz);\r\n}\r\nif (pt_attrs->l1_tbl_alloc_va) {\r\nmem_free_phys_mem((void *)\r\npt_attrs->l1_tbl_alloc_va,\r\npt_attrs->l1_tbl_alloc_pa,\r\npt_attrs->l1_tbl_alloc_sz);\r\n}\r\n}\r\nkfree(pt_attrs);\r\nkfree(dev_context);\r\n}\r\nfunc_end:\r\nreturn status;\r\n}\r\nstatic int bridge_dev_ctrl(struct bridge_dev_context *dev_context,\r\nu32 dw_cmd, void *pargs)\r\n{\r\nint status = 0;\r\nstruct bridge_ioctl_extproc *pa_ext_proc =\r\n(struct bridge_ioctl_extproc *)pargs;\r\ns32 ndx;\r\nswitch (dw_cmd) {\r\ncase BRDIOCTL_CHNLREAD:\r\nbreak;\r\ncase BRDIOCTL_CHNLWRITE:\r\nbreak;\r\ncase BRDIOCTL_SETMMUCONFIG:\r\nfor (ndx = 0; ndx < BRDIOCTL_NUMOFMMUTLB; ndx++, pa_ext_proc++)\r\ndev_context->atlb_entry[ndx] = *pa_ext_proc;\r\nbreak;\r\ncase BRDIOCTL_DEEPSLEEP:\r\ncase BRDIOCTL_EMERGENCYSLEEP:\r\nstatus = sleep_dsp(dev_context, PWR_DEEPSLEEP, pargs);\r\nbreak;\r\ncase BRDIOCTL_WAKEUP:\r\nstatus = wake_dsp(dev_context, pargs);\r\nbreak;\r\ncase BRDIOCTL_CLK_CTRL:\r\nstatus = 0;\r\nstatus = dsp_peripheral_clk_ctrl(dev_context, pargs);\r\nbreak;\r\ncase BRDIOCTL_PWR_HIBERNATE:\r\nstatus = handle_hibernation_from_dsp(dev_context);\r\nbreak;\r\ncase BRDIOCTL_PRESCALE_NOTIFY:\r\nstatus = pre_scale_dsp(dev_context, pargs);\r\nbreak;\r\ncase BRDIOCTL_POSTSCALE_NOTIFY:\r\nstatus = post_scale_dsp(dev_context, pargs);\r\nbreak;\r\ncase BRDIOCTL_CONSTRAINT_REQUEST:\r\nstatus = handle_constraints_set(dev_context, pargs);\r\nbreak;\r\ndefault:\r\nstatus = -EPERM;\r\nbreak;\r\n}\r\nreturn status;\r\n}\r\nstatic int bridge_dev_destroy(struct bridge_dev_context *dev_ctxt)\r\n{\r\nstruct pg_table_attrs *pt_attrs;\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = (struct bridge_dev_context *)\r\ndev_ctxt;\r\nstruct cfg_hostres *host_res;\r\nu32 shm_size;\r\nstruct drv_data *drv_datap = dev_get_drvdata(bridge);\r\nif (!dev_ctxt)\r\nreturn -EFAULT;\r\nbridge_brd_stop(dev_context);\r\nif (dev_context->pt_attrs) {\r\npt_attrs = dev_context->pt_attrs;\r\nkfree(pt_attrs->pg_info);\r\nif (pt_attrs->l2_tbl_alloc_va) {\r\nmem_free_phys_mem((void *)pt_attrs->l2_tbl_alloc_va,\r\npt_attrs->l2_tbl_alloc_pa,\r\npt_attrs->l2_tbl_alloc_sz);\r\n}\r\nif (pt_attrs->l1_tbl_alloc_va) {\r\nmem_free_phys_mem((void *)pt_attrs->l1_tbl_alloc_va,\r\npt_attrs->l1_tbl_alloc_pa,\r\npt_attrs->l1_tbl_alloc_sz);\r\n}\r\nkfree(pt_attrs);\r\n}\r\nif (dev_context->resources) {\r\nhost_res = dev_context->resources;\r\nshm_size = drv_datap->shm_size;\r\nif (shm_size >= 0x10000) {\r\nif ((host_res->mem_base[1]) &&\r\n(host_res->mem_phys[1])) {\r\nmem_free_phys_mem((void *)\r\nhost_res->mem_base\r\n[1],\r\nhost_res->mem_phys\r\n[1], shm_size);\r\n}\r\n} else {\r\ndev_dbg(bridge, "%s: Error getting shm size "\r\n"from registry: %x. Not calling "\r\n"mem_free_phys_mem\n", __func__,\r\nstatus);\r\n}\r\nhost_res->mem_base[1] = 0;\r\nhost_res->mem_phys[1] = 0;\r\nif (host_res->mem_base[0])\r\niounmap((void *)host_res->mem_base[0]);\r\nif (host_res->mem_base[2])\r\niounmap((void *)host_res->mem_base[2]);\r\nif (host_res->mem_base[3])\r\niounmap((void *)host_res->mem_base[3]);\r\nif (host_res->mem_base[4])\r\niounmap((void *)host_res->mem_base[4]);\r\nif (host_res->dmmu_base)\r\niounmap(host_res->dmmu_base);\r\nif (host_res->per_base)\r\niounmap(host_res->per_base);\r\nif (host_res->per_pm_base)\r\niounmap((void *)host_res->per_pm_base);\r\nif (host_res->core_pm_base)\r\niounmap((void *)host_res->core_pm_base);\r\nhost_res->mem_base[0] = (u32) NULL;\r\nhost_res->mem_base[2] = (u32) NULL;\r\nhost_res->mem_base[3] = (u32) NULL;\r\nhost_res->mem_base[4] = (u32) NULL;\r\nhost_res->dmmu_base = NULL;\r\nkfree(host_res);\r\n}\r\nkfree(drv_datap->base_img);\r\nkfree((void *)dev_ctxt);\r\nreturn status;\r\n}\r\nstatic int bridge_brd_mem_copy(struct bridge_dev_context *dev_ctxt,\r\nu32 dsp_dest_addr, u32 dsp_src_addr,\r\nu32 ul_num_bytes, u32 mem_type)\r\n{\r\nint status = 0;\r\nu32 src_addr = dsp_src_addr;\r\nu32 dest_addr = dsp_dest_addr;\r\nu32 copy_bytes = 0;\r\nu32 total_bytes = ul_num_bytes;\r\nu8 host_buf[BUFFERSIZE];\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nwhile (total_bytes > 0 && !status) {\r\ncopy_bytes =\r\ntotal_bytes > BUFFERSIZE ? BUFFERSIZE : total_bytes;\r\nstatus = read_ext_dsp_data(dev_ctxt, host_buf, src_addr,\r\ncopy_bytes, mem_type);\r\nif (!status) {\r\nif (dest_addr < (dev_context->dsp_start_add +\r\ndev_context->internal_size)) {\r\nstatus = write_dsp_data(dev_ctxt, host_buf,\r\ndest_addr, copy_bytes,\r\nmem_type);\r\n} else {\r\nstatus =\r\nwrite_ext_dsp_data(dev_ctxt, host_buf,\r\ndest_addr, copy_bytes,\r\nmem_type, false);\r\n}\r\n}\r\ntotal_bytes -= copy_bytes;\r\nsrc_addr += copy_bytes;\r\ndest_addr += copy_bytes;\r\n}\r\nreturn status;\r\n}\r\nstatic int bridge_brd_mem_write(struct bridge_dev_context *dev_ctxt,\r\nu8 *host_buff, u32 dsp_addr,\r\nu32 ul_num_bytes, u32 mem_type)\r\n{\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nu32 ul_remain_bytes = 0;\r\nu32 ul_bytes = 0;\r\nul_remain_bytes = ul_num_bytes;\r\nwhile (ul_remain_bytes > 0 && !status) {\r\nul_bytes =\r\nul_remain_bytes > BUFFERSIZE ? BUFFERSIZE : ul_remain_bytes;\r\nif (dsp_addr < (dev_context->dsp_start_add +\r\ndev_context->internal_size)) {\r\nstatus =\r\nwrite_dsp_data(dev_ctxt, host_buff, dsp_addr,\r\nul_bytes, mem_type);\r\n} else {\r\nstatus = write_ext_dsp_data(dev_ctxt, host_buff,\r\ndsp_addr, ul_bytes,\r\nmem_type, true);\r\n}\r\nul_remain_bytes -= ul_bytes;\r\ndsp_addr += ul_bytes;\r\nhost_buff = host_buff + ul_bytes;\r\n}\r\nreturn status;\r\n}\r\nstatic int bridge_brd_mem_map(struct bridge_dev_context *dev_ctxt,\r\nu32 ul_mpu_addr, u32 virt_addr,\r\nu32 ul_num_bytes, u32 ul_map_attr,\r\nstruct page **mapped_pages)\r\n{\r\nu32 attrs;\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nstruct hw_mmu_map_attrs_t hw_attrs;\r\nstruct vm_area_struct *vma;\r\nstruct mm_struct *mm = current->mm;\r\nu32 write = 0;\r\nu32 num_usr_pgs = 0;\r\nstruct page *mapped_page, *pg;\r\ns32 pg_num;\r\nu32 va = virt_addr;\r\nstruct task_struct *curr_task = current;\r\nu32 pg_i = 0;\r\nu32 mpu_addr, pa;\r\ndev_dbg(bridge,\r\n"%s hDevCtxt %p, pa %x, va %x, size %x, ul_map_attr %x\n",\r\n__func__, dev_ctxt, ul_mpu_addr, virt_addr, ul_num_bytes,\r\nul_map_attr);\r\nif (ul_num_bytes == 0)\r\nreturn -EINVAL;\r\nif (ul_map_attr & DSP_MAP_DIR_MASK) {\r\nattrs = ul_map_attr;\r\n} else {\r\nattrs = ul_map_attr | (DSP_MAPVIRTUALADDR | DSP_MAPELEMSIZE16);\r\n}\r\nif (attrs & DSP_MAPBIGENDIAN)\r\nhw_attrs.endianism = HW_BIG_ENDIAN;\r\nelse\r\nhw_attrs.endianism = HW_LITTLE_ENDIAN;\r\nhw_attrs.mixed_size = (enum hw_mmu_mixed_size_t)\r\n((attrs & DSP_MAPMIXEDELEMSIZE) >> 2);\r\nif (hw_attrs.mixed_size == 0) {\r\nif (attrs & DSP_MAPELEMSIZE8) {\r\nhw_attrs.element_size = HW_ELEM_SIZE8BIT;\r\n} else if (attrs & DSP_MAPELEMSIZE16) {\r\nhw_attrs.element_size = HW_ELEM_SIZE16BIT;\r\n} else if (attrs & DSP_MAPELEMSIZE32) {\r\nhw_attrs.element_size = HW_ELEM_SIZE32BIT;\r\n} else if (attrs & DSP_MAPELEMSIZE64) {\r\nhw_attrs.element_size = HW_ELEM_SIZE64BIT;\r\n} else {\r\nreturn -EINVAL;\r\n}\r\n}\r\nif (attrs & DSP_MAPDONOTLOCK)\r\nhw_attrs.donotlockmpupage = 1;\r\nelse\r\nhw_attrs.donotlockmpupage = 0;\r\nif (attrs & DSP_MAPVMALLOCADDR) {\r\nreturn mem_map_vmalloc(dev_ctxt, ul_mpu_addr, virt_addr,\r\nul_num_bytes, &hw_attrs);\r\n}\r\nif ((attrs & DSP_MAPPHYSICALADDR)) {\r\nstatus = pte_update(dev_context, ul_mpu_addr, virt_addr,\r\nul_num_bytes, &hw_attrs);\r\ngoto func_cont;\r\n}\r\ndown_read(&mm->mmap_sem);\r\nvma = find_vma(mm, ul_mpu_addr);\r\nif (vma)\r\ndev_dbg(bridge,\r\n"VMAfor UserBuf: ul_mpu_addr=%x, ul_num_bytes=%x, "\r\n"vm_start=%lx, vm_end=%lx, vm_flags=%lx\n", ul_mpu_addr,\r\nul_num_bytes, vma->vm_start, vma->vm_end,\r\nvma->vm_flags);\r\nwhile ((vma) && (ul_mpu_addr + ul_num_bytes > vma->vm_end)) {\r\nvma = find_vma(mm, vma->vm_end + 1);\r\ndev_dbg(bridge,\r\n"VMA for UserBuf ul_mpu_addr=%x ul_num_bytes=%x, "\r\n"vm_start=%lx, vm_end=%lx, vm_flags=%lx\n", ul_mpu_addr,\r\nul_num_bytes, vma->vm_start, vma->vm_end,\r\nvma->vm_flags);\r\n}\r\nif (!vma) {\r\npr_err("%s: Failed to get VMA region for 0x%x (%d)\n",\r\n__func__, ul_mpu_addr, ul_num_bytes);\r\nstatus = -EINVAL;\r\nup_read(&mm->mmap_sem);\r\ngoto func_cont;\r\n}\r\nif (vma->vm_flags & VM_IO) {\r\nnum_usr_pgs = ul_num_bytes / PG_SIZE4K;\r\nmpu_addr = ul_mpu_addr;\r\nfor (pg_i = 0; pg_i < num_usr_pgs; pg_i++) {\r\npa = user_va2_pa(mm, mpu_addr);\r\nif (!pa) {\r\nstatus = -EPERM;\r\npr_err("DSPBRIDGE: VM_IO mapping physical"\r\n"address is invalid\n");\r\nbreak;\r\n}\r\nif (pfn_valid(__phys_to_pfn(pa))) {\r\npg = PHYS_TO_PAGE(pa);\r\nget_page(pg);\r\nif (page_count(pg) < 1) {\r\npr_err("Bad page in VM_IO buffer\n");\r\nbad_page_dump(pa, pg);\r\n}\r\n}\r\nstatus = pte_set(dev_context->pt_attrs, pa,\r\nva, HW_PAGE_SIZE4KB, &hw_attrs);\r\nif (status)\r\nbreak;\r\nva += HW_PAGE_SIZE4KB;\r\nmpu_addr += HW_PAGE_SIZE4KB;\r\npa += HW_PAGE_SIZE4KB;\r\n}\r\n} else {\r\nnum_usr_pgs = ul_num_bytes / PG_SIZE4K;\r\nif (vma->vm_flags & (VM_WRITE | VM_MAYWRITE))\r\nwrite = 1;\r\nfor (pg_i = 0; pg_i < num_usr_pgs; pg_i++) {\r\npg_num = get_user_pages(curr_task, mm, ul_mpu_addr, 1,\r\nwrite, 1, &mapped_page, NULL);\r\nif (pg_num > 0) {\r\nif (page_count(mapped_page) < 1) {\r\npr_err("Bad page count after doing"\r\n"get_user_pages on"\r\n"user buffer\n");\r\nbad_page_dump(page_to_phys(mapped_page),\r\nmapped_page);\r\n}\r\nstatus = pte_set(dev_context->pt_attrs,\r\npage_to_phys(mapped_page), va,\r\nHW_PAGE_SIZE4KB, &hw_attrs);\r\nif (status)\r\nbreak;\r\nif (mapped_pages)\r\nmapped_pages[pg_i] = mapped_page;\r\nva += HW_PAGE_SIZE4KB;\r\nul_mpu_addr += HW_PAGE_SIZE4KB;\r\n} else {\r\npr_err("DSPBRIDGE: get_user_pages FAILED,"\r\n"MPU addr = 0x%x,"\r\n"vma->vm_flags = 0x%lx,"\r\n"get_user_pages Err"\r\n"Value = %d, Buffer"\r\n"size=0x%x\n", ul_mpu_addr,\r\nvma->vm_flags, pg_num, ul_num_bytes);\r\nstatus = -EPERM;\r\nbreak;\r\n}\r\n}\r\n}\r\nup_read(&mm->mmap_sem);\r\nfunc_cont:\r\nif (status) {\r\nif (pg_i) {\r\nbridge_brd_mem_un_map(dev_context, virt_addr,\r\n(pg_i * PG_SIZE4K));\r\n}\r\nstatus = -EPERM;\r\n}\r\nflush_all(dev_context);\r\ndev_dbg(bridge, "%s status %x\n", __func__, status);\r\nreturn status;\r\n}\r\nstatic int bridge_brd_mem_un_map(struct bridge_dev_context *dev_ctxt,\r\nu32 virt_addr, u32 ul_num_bytes)\r\n{\r\nu32 l1_base_va;\r\nu32 l2_base_va;\r\nu32 l2_base_pa;\r\nu32 l2_page_num;\r\nu32 pte_val;\r\nu32 pte_size;\r\nu32 pte_count;\r\nu32 pte_addr_l1;\r\nu32 pte_addr_l2 = 0;\r\nu32 rem_bytes;\r\nu32 rem_bytes_l2;\r\nu32 va_curr;\r\nstruct page *pg = NULL;\r\nint status = 0;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nstruct pg_table_attrs *pt = dev_context->pt_attrs;\r\nu32 temp;\r\nu32 paddr;\r\nu32 numof4k_pages = 0;\r\nva_curr = virt_addr;\r\nrem_bytes = ul_num_bytes;\r\nrem_bytes_l2 = 0;\r\nl1_base_va = pt->l1_base_va;\r\npte_addr_l1 = hw_mmu_pte_addr_l1(l1_base_va, va_curr);\r\ndev_dbg(bridge, "%s dev_ctxt %p, va %x, NumBytes %x l1_base_va %x, "\r\n"pte_addr_l1 %x\n", __func__, dev_ctxt, virt_addr,\r\nul_num_bytes, l1_base_va, pte_addr_l1);\r\nwhile (rem_bytes && !status) {\r\nu32 va_curr_orig = va_curr;\r\npte_addr_l1 = hw_mmu_pte_addr_l1(l1_base_va, va_curr);\r\npte_val = *(u32 *) pte_addr_l1;\r\npte_size = hw_mmu_pte_size_l1(pte_val);\r\nif (pte_size != HW_MMU_COARSE_PAGE_SIZE)\r\ngoto skip_coarse_page;\r\nl2_base_pa = hw_mmu_pte_coarse_l1(pte_val);\r\nl2_base_va = l2_base_pa - pt->l2_base_pa + pt->l2_base_va;\r\nl2_page_num =\r\n(l2_base_pa - pt->l2_base_pa) / HW_MMU_COARSE_PAGE_SIZE;\r\npte_addr_l2 = hw_mmu_pte_addr_l2(l2_base_va, va_curr);\r\npte_count = pte_addr_l2 & (HW_MMU_COARSE_PAGE_SIZE - 1);\r\npte_count = (HW_MMU_COARSE_PAGE_SIZE - pte_count) / sizeof(u32);\r\nif (rem_bytes < (pte_count * PG_SIZE4K))\r\npte_count = rem_bytes / PG_SIZE4K;\r\nrem_bytes_l2 = pte_count * PG_SIZE4K;\r\nwhile (rem_bytes_l2 && !status) {\r\npte_val = *(u32 *) pte_addr_l2;\r\npte_size = hw_mmu_pte_size_l2(pte_val);\r\nif (pte_size == 0 || rem_bytes_l2 < pte_size ||\r\nva_curr & (pte_size - 1)) {\r\nstatus = -EPERM;\r\nbreak;\r\n}\r\npaddr = (pte_val & ~(pte_size - 1));\r\nif (pte_size == HW_PAGE_SIZE64KB)\r\nnumof4k_pages = 16;\r\nelse\r\nnumof4k_pages = 1;\r\ntemp = 0;\r\nwhile (temp++ < numof4k_pages) {\r\nif (!pfn_valid(__phys_to_pfn(paddr))) {\r\npaddr += HW_PAGE_SIZE4KB;\r\ncontinue;\r\n}\r\npg = PHYS_TO_PAGE(paddr);\r\nif (page_count(pg) < 1) {\r\npr_info("DSPBRIDGE: UNMAP function: "\r\n"COUNT 0 FOR PA 0x%x, size = "\r\n"0x%x\n", paddr, ul_num_bytes);\r\nbad_page_dump(paddr, pg);\r\n} else {\r\nset_page_dirty(pg);\r\npage_cache_release(pg);\r\n}\r\npaddr += HW_PAGE_SIZE4KB;\r\n}\r\nif (hw_mmu_pte_clear(pte_addr_l2, va_curr, pte_size)) {\r\nstatus = -EPERM;\r\ngoto EXIT_LOOP;\r\n}\r\nstatus = 0;\r\nrem_bytes_l2 -= pte_size;\r\nva_curr += pte_size;\r\npte_addr_l2 += (pte_size >> 12) * sizeof(u32);\r\n}\r\nspin_lock(&pt->pg_lock);\r\nif (rem_bytes_l2 == 0) {\r\npt->pg_info[l2_page_num].num_entries -= pte_count;\r\nif (pt->pg_info[l2_page_num].num_entries == 0) {\r\nif (!hw_mmu_pte_clear(l1_base_va, va_curr_orig,\r\nHW_MMU_COARSE_PAGE_SIZE))\r\nstatus = 0;\r\nelse {\r\nstatus = -EPERM;\r\nspin_unlock(&pt->pg_lock);\r\ngoto EXIT_LOOP;\r\n}\r\n}\r\nrem_bytes -= pte_count * PG_SIZE4K;\r\n} else\r\nstatus = -EPERM;\r\nspin_unlock(&pt->pg_lock);\r\ncontinue;\r\nskip_coarse_page:\r\nif (pte_size == 0 || rem_bytes < pte_size ||\r\nva_curr & (pte_size - 1)) {\r\nstatus = -EPERM;\r\nbreak;\r\n}\r\nif (pte_size == HW_PAGE_SIZE1MB)\r\nnumof4k_pages = 256;\r\nelse\r\nnumof4k_pages = 4096;\r\ntemp = 0;\r\npaddr = (pte_val & ~(pte_size - 1));\r\nwhile (temp++ < numof4k_pages) {\r\nif (pfn_valid(__phys_to_pfn(paddr))) {\r\npg = PHYS_TO_PAGE(paddr);\r\nif (page_count(pg) < 1) {\r\npr_info("DSPBRIDGE: UNMAP function: "\r\n"COUNT 0 FOR PA 0x%x, size = "\r\n"0x%x\n", paddr, ul_num_bytes);\r\nbad_page_dump(paddr, pg);\r\n} else {\r\nset_page_dirty(pg);\r\npage_cache_release(pg);\r\n}\r\n}\r\npaddr += HW_PAGE_SIZE4KB;\r\n}\r\nif (!hw_mmu_pte_clear(l1_base_va, va_curr, pte_size)) {\r\nstatus = 0;\r\nrem_bytes -= pte_size;\r\nva_curr += pte_size;\r\n} else {\r\nstatus = -EPERM;\r\ngoto EXIT_LOOP;\r\n}\r\n}\r\nEXIT_LOOP:\r\nflush_all(dev_context);\r\ndev_dbg(bridge,\r\n"%s: va_curr %x, pte_addr_l1 %x pte_addr_l2 %x rem_bytes %x,"\r\n" rem_bytes_l2 %x status %x\n", __func__, va_curr, pte_addr_l1,\r\npte_addr_l2, rem_bytes, rem_bytes_l2, status);\r\nreturn status;\r\n}\r\nstatic u32 user_va2_pa(struct mm_struct *mm, u32 address)\r\n{\r\npgd_t *pgd;\r\npud_t *pud;\r\npmd_t *pmd;\r\npte_t *ptep, pte;\r\npgd = pgd_offset(mm, address);\r\nif (pgd_none(*pgd) || pgd_bad(*pgd))\r\nreturn 0;\r\npud = pud_offset(pgd, address);\r\nif (pud_none(*pud) || pud_bad(*pud))\r\nreturn 0;\r\npmd = pmd_offset(pud, address);\r\nif (pmd_none(*pmd) || pmd_bad(*pmd))\r\nreturn 0;\r\nptep = pte_offset_map(pmd, address);\r\nif (ptep) {\r\npte = *ptep;\r\nif (pte_present(pte))\r\nreturn pte & PAGE_MASK;\r\n}\r\nreturn 0;\r\n}\r\nstatic int pte_update(struct bridge_dev_context *dev_ctxt, u32 pa,\r\nu32 va, u32 size,\r\nstruct hw_mmu_map_attrs_t *map_attrs)\r\n{\r\nu32 i;\r\nu32 all_bits;\r\nu32 pa_curr = pa;\r\nu32 va_curr = va;\r\nu32 num_bytes = size;\r\nstruct bridge_dev_context *dev_context = dev_ctxt;\r\nint status = 0;\r\nu32 page_size[] = { HW_PAGE_SIZE16MB, HW_PAGE_SIZE1MB,\r\nHW_PAGE_SIZE64KB, HW_PAGE_SIZE4KB\r\n};\r\nwhile (num_bytes && !status) {\r\nall_bits = pa_curr | va_curr;\r\nfor (i = 0; i < 4; i++) {\r\nif ((num_bytes >= page_size[i]) && ((all_bits &\r\n(page_size[i] -\r\n1)) == 0)) {\r\nstatus =\r\npte_set(dev_context->pt_attrs, pa_curr,\r\nva_curr, page_size[i], map_attrs);\r\npa_curr += page_size[i];\r\nva_curr += page_size[i];\r\nnum_bytes -= page_size[i];\r\nbreak;\r\n}\r\n}\r\n}\r\nreturn status;\r\n}\r\nstatic int pte_set(struct pg_table_attrs *pt, u32 pa, u32 va,\r\nu32 size, struct hw_mmu_map_attrs_t *attrs)\r\n{\r\nu32 i;\r\nu32 pte_val;\r\nu32 pte_addr_l1;\r\nu32 pte_size;\r\nu32 pg_tbl_va;\r\nu32 l1_base_va;\r\nu32 l2_base_va = 0;\r\nu32 l2_base_pa = 0;\r\nu32 l2_page_num = 0;\r\nint status = 0;\r\nl1_base_va = pt->l1_base_va;\r\npg_tbl_va = l1_base_va;\r\nif ((size == HW_PAGE_SIZE64KB) || (size == HW_PAGE_SIZE4KB)) {\r\npte_addr_l1 = hw_mmu_pte_addr_l1(l1_base_va, va);\r\nif (pte_addr_l1 <= (pt->l1_base_va + pt->l1_size)) {\r\npte_val = *(u32 *) pte_addr_l1;\r\npte_size = hw_mmu_pte_size_l1(pte_val);\r\n} else {\r\nreturn -EPERM;\r\n}\r\nspin_lock(&pt->pg_lock);\r\nif (pte_size == HW_MMU_COARSE_PAGE_SIZE) {\r\nl2_base_pa = hw_mmu_pte_coarse_l1(pte_val);\r\nl2_base_va =\r\nl2_base_pa - pt->l2_base_pa + pt->l2_base_va;\r\nl2_page_num =\r\n(l2_base_pa -\r\npt->l2_base_pa) / HW_MMU_COARSE_PAGE_SIZE;\r\n} else if (pte_size == 0) {\r\nfor (i = 0; (i < pt->l2_num_pages) &&\r\n(pt->pg_info[i].num_entries != 0); i++)\r\n;\r\nif (i < pt->l2_num_pages) {\r\nl2_page_num = i;\r\nl2_base_pa = pt->l2_base_pa + (l2_page_num *\r\nHW_MMU_COARSE_PAGE_SIZE);\r\nl2_base_va = pt->l2_base_va + (l2_page_num *\r\nHW_MMU_COARSE_PAGE_SIZE);\r\nstatus =\r\nhw_mmu_pte_set(l1_base_va, l2_base_pa, va,\r\nHW_MMU_COARSE_PAGE_SIZE,\r\nattrs);\r\n} else {\r\nstatus = -ENOMEM;\r\n}\r\n} else {\r\nstatus = -EPERM;\r\n}\r\nif (!status) {\r\npg_tbl_va = l2_base_va;\r\nif (size == HW_PAGE_SIZE64KB)\r\npt->pg_info[l2_page_num].num_entries += 16;\r\nelse\r\npt->pg_info[l2_page_num].num_entries++;\r\ndev_dbg(bridge, "PTE: L2 BaseVa %x, BasePa %x, PageNum "\r\n"%x, num_entries %x\n", l2_base_va,\r\nl2_base_pa, l2_page_num,\r\npt->pg_info[l2_page_num].num_entries);\r\n}\r\nspin_unlock(&pt->pg_lock);\r\n}\r\nif (!status) {\r\ndev_dbg(bridge, "PTE: pg_tbl_va %x, pa %x, va %x, size %x\n",\r\npg_tbl_va, pa, va, size);\r\ndev_dbg(bridge, "PTE: endianism %x, element_size %x, "\r\n"mixed_size %x\n", attrs->endianism,\r\nattrs->element_size, attrs->mixed_size);\r\nstatus = hw_mmu_pte_set(pg_tbl_va, pa, va, size, attrs);\r\n}\r\nreturn status;\r\n}\r\nstatic int mem_map_vmalloc(struct bridge_dev_context *dev_context,\r\nu32 ul_mpu_addr, u32 virt_addr,\r\nu32 ul_num_bytes,\r\nstruct hw_mmu_map_attrs_t *hw_attrs)\r\n{\r\nint status = 0;\r\nstruct page *page[1];\r\nu32 i;\r\nu32 pa_curr;\r\nu32 pa_next;\r\nu32 va_curr;\r\nu32 size_curr;\r\nu32 num_pages;\r\nu32 pa;\r\nu32 num_of4k_pages;\r\nu32 temp = 0;\r\nnum_pages = ul_num_bytes / PAGE_SIZE;\r\ni = 0;\r\nva_curr = ul_mpu_addr;\r\npage[0] = vmalloc_to_page((void *)va_curr);\r\npa_next = page_to_phys(page[0]);\r\nwhile (!status && (i < num_pages)) {\r\npa_curr = pa_next;\r\nsize_curr = PAGE_SIZE;\r\nwhile (++i < num_pages) {\r\npage[0] =\r\nvmalloc_to_page((void *)(va_curr + size_curr));\r\npa_next = page_to_phys(page[0]);\r\nif (pa_next == (pa_curr + size_curr))\r\nsize_curr += PAGE_SIZE;\r\nelse\r\nbreak;\r\n}\r\nif (pa_next == 0) {\r\nstatus = -ENOMEM;\r\nbreak;\r\n}\r\npa = pa_curr;\r\nnum_of4k_pages = size_curr / HW_PAGE_SIZE4KB;\r\nwhile (temp++ < num_of4k_pages) {\r\nget_page(PHYS_TO_PAGE(pa));\r\npa += HW_PAGE_SIZE4KB;\r\n}\r\nstatus = pte_update(dev_context, pa_curr, virt_addr +\r\n(va_curr - ul_mpu_addr), size_curr,\r\nhw_attrs);\r\nva_curr += size_curr;\r\n}\r\nflush_all(dev_context);\r\ndev_dbg(bridge, "%s status %x\n", __func__, status);\r\nreturn status;\r\n}\r\nbool wait_for_start(struct bridge_dev_context *dev_context,\r\nvoid __iomem *sync_addr)\r\n{\r\nu16 timeout = TIHELEN_ACKTIMEOUT;\r\nwhile (__raw_readw(sync_addr) && --timeout)\r\nudelay(10);\r\nif (!timeout) {\r\npr_err("%s: Timed out waiting DSP to Start\n", __func__);\r\nreturn false;\r\n}\r\nreturn true;\r\n}
