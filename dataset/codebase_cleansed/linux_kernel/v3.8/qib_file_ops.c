static u64 cvt_kvaddr(void *p)\r\n{\r\nstruct page *page;\r\nu64 paddr = 0;\r\npage = vmalloc_to_page(p);\r\nif (page)\r\npaddr = page_to_pfn(page) << PAGE_SHIFT;\r\nreturn paddr;\r\n}\r\nstatic int qib_get_base_info(struct file *fp, void __user *ubase,\r\nsize_t ubase_size)\r\n{\r\nstruct qib_ctxtdata *rcd = ctxt_fp(fp);\r\nint ret = 0;\r\nstruct qib_base_info *kinfo = NULL;\r\nstruct qib_devdata *dd = rcd->dd;\r\nstruct qib_pportdata *ppd = rcd->ppd;\r\nunsigned subctxt_cnt;\r\nint shared, master;\r\nsize_t sz;\r\nsubctxt_cnt = rcd->subctxt_cnt;\r\nif (!subctxt_cnt) {\r\nshared = 0;\r\nmaster = 0;\r\nsubctxt_cnt = 1;\r\n} else {\r\nshared = 1;\r\nmaster = !subctxt_fp(fp);\r\n}\r\nsz = sizeof(*kinfo);\r\nif (!shared)\r\nsz -= 7 * sizeof(u64);\r\nif (ubase_size < sz) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nkinfo = kzalloc(sizeof(*kinfo), GFP_KERNEL);\r\nif (kinfo == NULL) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nret = dd->f_get_base_info(rcd, kinfo);\r\nif (ret < 0)\r\ngoto bail;\r\nkinfo->spi_rcvhdr_cnt = dd->rcvhdrcnt;\r\nkinfo->spi_rcvhdrent_size = dd->rcvhdrentsize;\r\nkinfo->spi_tidegrcnt = rcd->rcvegrcnt;\r\nkinfo->spi_rcv_egrbufsize = dd->rcvegrbufsize;\r\nkinfo->spi_rcv_egrbuftotlen =\r\nrcd->rcvegrbuf_chunks * rcd->rcvegrbuf_size;\r\nkinfo->spi_rcv_egrperchunk = rcd->rcvegrbufs_perchunk;\r\nkinfo->spi_rcv_egrchunksize = kinfo->spi_rcv_egrbuftotlen /\r\nrcd->rcvegrbuf_chunks;\r\nkinfo->spi_tidcnt = dd->rcvtidcnt / subctxt_cnt;\r\nif (master)\r\nkinfo->spi_tidcnt += dd->rcvtidcnt % subctxt_cnt;\r\nkinfo->spi_nctxts = dd->cfgctxts;\r\nkinfo->spi_unit = dd->unit;\r\nkinfo->spi_port = ppd->port;\r\nkinfo->spi_tid_maxsize = PAGE_SIZE;\r\nkinfo->spi_rcvhdr_base = (u64) rcd->rcvhdrq_phys;\r\nkinfo->spi_rcvhdr_tailaddr = (u64) rcd->rcvhdrqtailaddr_phys;\r\nkinfo->spi_rhf_offset = dd->rhf_offset;\r\nkinfo->spi_rcv_egrbufs = (u64) rcd->rcvegr_phys;\r\nkinfo->spi_pioavailaddr = (u64) dd->pioavailregs_phys;\r\nkinfo->spi_status = (u64) kinfo->spi_pioavailaddr +\r\n(char *) ppd->statusp -\r\n(char *) dd->pioavailregs_dma;\r\nkinfo->spi_uregbase = (u64) dd->uregbase + dd->ureg_align * rcd->ctxt;\r\nif (!shared) {\r\nkinfo->spi_piocnt = rcd->piocnt;\r\nkinfo->spi_piobufbase = (u64) rcd->piobufs;\r\nkinfo->spi_sendbuf_status = cvt_kvaddr(rcd->user_event_mask);\r\n} else if (master) {\r\nkinfo->spi_piocnt = (rcd->piocnt / subctxt_cnt) +\r\n(rcd->piocnt % subctxt_cnt);\r\nkinfo->spi_piobufbase = (u64) rcd->piobufs +\r\ndd->palign *\r\n(rcd->piocnt - kinfo->spi_piocnt);\r\n} else {\r\nunsigned slave = subctxt_fp(fp) - 1;\r\nkinfo->spi_piocnt = rcd->piocnt / subctxt_cnt;\r\nkinfo->spi_piobufbase = (u64) rcd->piobufs +\r\ndd->palign * kinfo->spi_piocnt * slave;\r\n}\r\nif (shared) {\r\nkinfo->spi_sendbuf_status =\r\ncvt_kvaddr(&rcd->user_event_mask[subctxt_fp(fp)]);\r\nkinfo->spi_subctxt_uregbase = cvt_kvaddr(rcd->subctxt_uregbase);\r\nkinfo->spi_subctxt_rcvegrbuf =\r\ncvt_kvaddr(rcd->subctxt_rcvegrbuf);\r\nkinfo->spi_subctxt_rcvhdr_base =\r\ncvt_kvaddr(rcd->subctxt_rcvhdr_base);\r\n}\r\nkinfo->spi_pioindex = (kinfo->spi_piobufbase - dd->pio2k_bufbase) /\r\ndd->palign;\r\nkinfo->spi_pioalign = dd->palign;\r\nkinfo->spi_qpair = QIB_KD_QP;\r\nkinfo->spi_piosize = dd->piosize2k - 2 * sizeof(u32);\r\nkinfo->spi_mtu = ppd->ibmaxlen;\r\nkinfo->spi_ctxt = rcd->ctxt;\r\nkinfo->spi_subctxt = subctxt_fp(fp);\r\nkinfo->spi_sw_version = QIB_KERN_SWVERSION;\r\nkinfo->spi_sw_version |= 1U << 31;\r\nkinfo->spi_hw_version = dd->revision;\r\nif (master)\r\nkinfo->spi_runtime_flags |= QIB_RUNTIME_MASTER;\r\nsz = (ubase_size < sizeof(*kinfo)) ? ubase_size : sizeof(*kinfo);\r\nif (copy_to_user(ubase, kinfo, sz))\r\nret = -EFAULT;\r\nbail:\r\nkfree(kinfo);\r\nreturn ret;\r\n}\r\nstatic int qib_tid_update(struct qib_ctxtdata *rcd, struct file *fp,\r\nconst struct qib_tid_info *ti)\r\n{\r\nint ret = 0, ntids;\r\nu32 tid, ctxttid, cnt, i, tidcnt, tidoff;\r\nu16 *tidlist;\r\nstruct qib_devdata *dd = rcd->dd;\r\nu64 physaddr;\r\nunsigned long vaddr;\r\nu64 __iomem *tidbase;\r\nunsigned long tidmap[8];\r\nstruct page **pagep = NULL;\r\nunsigned subctxt = subctxt_fp(fp);\r\nif (!dd->pageshadow) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\ncnt = ti->tidcnt;\r\nif (!cnt) {\r\nret = -EFAULT;\r\ngoto done;\r\n}\r\nctxttid = rcd->ctxt * dd->rcvtidcnt;\r\nif (!rcd->subctxt_cnt) {\r\ntidcnt = dd->rcvtidcnt;\r\ntid = rcd->tidcursor;\r\ntidoff = 0;\r\n} else if (!subctxt) {\r\ntidcnt = (dd->rcvtidcnt / rcd->subctxt_cnt) +\r\n(dd->rcvtidcnt % rcd->subctxt_cnt);\r\ntidoff = dd->rcvtidcnt - tidcnt;\r\nctxttid += tidoff;\r\ntid = tidcursor_fp(fp);\r\n} else {\r\ntidcnt = dd->rcvtidcnt / rcd->subctxt_cnt;\r\ntidoff = tidcnt * (subctxt - 1);\r\nctxttid += tidoff;\r\ntid = tidcursor_fp(fp);\r\n}\r\nif (cnt > tidcnt) {\r\nqib_devinfo(dd->pcidev,\r\n"Process tried to allocate %u TIDs, only trying max (%u)\n",\r\ncnt, tidcnt);\r\ncnt = tidcnt;\r\n}\r\npagep = (struct page **) rcd->tid_pg_list;\r\ntidlist = (u16 *) &pagep[dd->rcvtidcnt];\r\npagep += tidoff;\r\ntidlist += tidoff;\r\nmemset(tidmap, 0, sizeof(tidmap));\r\nntids = tidcnt;\r\ntidbase = (u64 __iomem *) (((char __iomem *) dd->kregbase) +\r\ndd->rcvtidbase +\r\nctxttid * sizeof(*tidbase));\r\nvaddr = ti->tidvaddr;\r\nif (!access_ok(VERIFY_WRITE, (void __user *) vaddr,\r\ncnt * PAGE_SIZE)) {\r\nret = -EFAULT;\r\ngoto done;\r\n}\r\nret = qib_get_user_pages(vaddr, cnt, pagep);\r\nif (ret) {\r\nqib_devinfo(dd->pcidev,\r\n"Failed to lock addr %p, %u pages: "\r\n"errno %d\n", (void *) vaddr, cnt, -ret);\r\ngoto done;\r\n}\r\nfor (i = 0; i < cnt; i++, vaddr += PAGE_SIZE) {\r\nfor (; ntids--; tid++) {\r\nif (tid == tidcnt)\r\ntid = 0;\r\nif (!dd->pageshadow[ctxttid + tid])\r\nbreak;\r\n}\r\nif (ntids < 0) {\r\ni--;\r\nret = -ENOMEM;\r\nbreak;\r\n}\r\ntidlist[i] = tid + tidoff;\r\ndd->pageshadow[ctxttid + tid] = pagep[i];\r\ndd->physshadow[ctxttid + tid] =\r\nqib_map_page(dd->pcidev, pagep[i], 0, PAGE_SIZE,\r\nPCI_DMA_FROMDEVICE);\r\n__set_bit(tid, tidmap);\r\nphysaddr = dd->physshadow[ctxttid + tid];\r\ndd->f_put_tid(dd, &tidbase[tid],\r\nRCVHQ_RCV_TYPE_EXPECTED, physaddr);\r\ntid++;\r\n}\r\nif (ret) {\r\nu32 limit;\r\ncleanup:\r\nlimit = sizeof(tidmap) * BITS_PER_BYTE;\r\nif (limit > tidcnt)\r\nlimit = tidcnt;\r\ntid = find_first_bit((const unsigned long *)tidmap, limit);\r\nfor (; tid < limit; tid++) {\r\nif (!test_bit(tid, tidmap))\r\ncontinue;\r\nif (dd->pageshadow[ctxttid + tid]) {\r\ndma_addr_t phys;\r\nphys = dd->physshadow[ctxttid + tid];\r\ndd->physshadow[ctxttid + tid] = dd->tidinvalid;\r\ndd->f_put_tid(dd, &tidbase[tid],\r\nRCVHQ_RCV_TYPE_EXPECTED,\r\ndd->tidinvalid);\r\npci_unmap_page(dd->pcidev, phys, PAGE_SIZE,\r\nPCI_DMA_FROMDEVICE);\r\ndd->pageshadow[ctxttid + tid] = NULL;\r\n}\r\n}\r\nqib_release_user_pages(pagep, cnt);\r\n} else {\r\nif (copy_to_user((void __user *)\r\n(unsigned long) ti->tidlist,\r\ntidlist, cnt * sizeof(*tidlist))) {\r\nret = -EFAULT;\r\ngoto cleanup;\r\n}\r\nif (copy_to_user((void __user *) (unsigned long) ti->tidmap,\r\ntidmap, sizeof tidmap)) {\r\nret = -EFAULT;\r\ngoto cleanup;\r\n}\r\nif (tid == tidcnt)\r\ntid = 0;\r\nif (!rcd->subctxt_cnt)\r\nrcd->tidcursor = tid;\r\nelse\r\ntidcursor_fp(fp) = tid;\r\n}\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int qib_tid_free(struct qib_ctxtdata *rcd, unsigned subctxt,\r\nconst struct qib_tid_info *ti)\r\n{\r\nint ret = 0;\r\nu32 tid, ctxttid, cnt, limit, tidcnt;\r\nstruct qib_devdata *dd = rcd->dd;\r\nu64 __iomem *tidbase;\r\nunsigned long tidmap[8];\r\nif (!dd->pageshadow) {\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\nif (copy_from_user(tidmap, (void __user *)(unsigned long)ti->tidmap,\r\nsizeof tidmap)) {\r\nret = -EFAULT;\r\ngoto done;\r\n}\r\nctxttid = rcd->ctxt * dd->rcvtidcnt;\r\nif (!rcd->subctxt_cnt)\r\ntidcnt = dd->rcvtidcnt;\r\nelse if (!subctxt) {\r\ntidcnt = (dd->rcvtidcnt / rcd->subctxt_cnt) +\r\n(dd->rcvtidcnt % rcd->subctxt_cnt);\r\nctxttid += dd->rcvtidcnt - tidcnt;\r\n} else {\r\ntidcnt = dd->rcvtidcnt / rcd->subctxt_cnt;\r\nctxttid += tidcnt * (subctxt - 1);\r\n}\r\ntidbase = (u64 __iomem *) ((char __iomem *)(dd->kregbase) +\r\ndd->rcvtidbase +\r\nctxttid * sizeof(*tidbase));\r\nlimit = sizeof(tidmap) * BITS_PER_BYTE;\r\nif (limit > tidcnt)\r\nlimit = tidcnt;\r\ntid = find_first_bit(tidmap, limit);\r\nfor (cnt = 0; tid < limit; tid++) {\r\nif (!test_bit(tid, tidmap))\r\ncontinue;\r\ncnt++;\r\nif (dd->pageshadow[ctxttid + tid]) {\r\nstruct page *p;\r\ndma_addr_t phys;\r\np = dd->pageshadow[ctxttid + tid];\r\ndd->pageshadow[ctxttid + tid] = NULL;\r\nphys = dd->physshadow[ctxttid + tid];\r\ndd->physshadow[ctxttid + tid] = dd->tidinvalid;\r\ndd->f_put_tid(dd, &tidbase[tid],\r\nRCVHQ_RCV_TYPE_EXPECTED, dd->tidinvalid);\r\npci_unmap_page(dd->pcidev, phys, PAGE_SIZE,\r\nPCI_DMA_FROMDEVICE);\r\nqib_release_user_pages(&p, 1);\r\n}\r\n}\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int qib_set_part_key(struct qib_ctxtdata *rcd, u16 key)\r\n{\r\nstruct qib_pportdata *ppd = rcd->ppd;\r\nint i, any = 0, pidx = -1;\r\nu16 lkey = key & 0x7FFF;\r\nint ret;\r\nif (lkey == (QIB_DEFAULT_P_KEY & 0x7FFF)) {\r\nret = 0;\r\ngoto bail;\r\n}\r\nif (!lkey) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nkey |= 0x8000;\r\nfor (i = 0; i < ARRAY_SIZE(rcd->pkeys); i++) {\r\nif (!rcd->pkeys[i] && pidx == -1)\r\npidx = i;\r\nif (rcd->pkeys[i] == key) {\r\nret = -EEXIST;\r\ngoto bail;\r\n}\r\n}\r\nif (pidx == -1) {\r\nret = -EBUSY;\r\ngoto bail;\r\n}\r\nfor (any = i = 0; i < ARRAY_SIZE(ppd->pkeys); i++) {\r\nif (!ppd->pkeys[i]) {\r\nany++;\r\ncontinue;\r\n}\r\nif (ppd->pkeys[i] == key) {\r\natomic_t *pkrefs = &ppd->pkeyrefs[i];\r\nif (atomic_inc_return(pkrefs) > 1) {\r\nrcd->pkeys[pidx] = key;\r\nret = 0;\r\ngoto bail;\r\n} else {\r\natomic_dec(pkrefs);\r\nany++;\r\n}\r\n}\r\nif ((ppd->pkeys[i] & 0x7FFF) == lkey) {\r\nret = -EEXIST;\r\ngoto bail;\r\n}\r\n}\r\nif (!any) {\r\nret = -EBUSY;\r\ngoto bail;\r\n}\r\nfor (any = i = 0; i < ARRAY_SIZE(ppd->pkeys); i++) {\r\nif (!ppd->pkeys[i] &&\r\natomic_inc_return(&ppd->pkeyrefs[i]) == 1) {\r\nrcd->pkeys[pidx] = key;\r\nppd->pkeys[i] = key;\r\n(void) ppd->dd->f_set_ib_cfg(ppd, QIB_IB_CFG_PKEYS, 0);\r\nret = 0;\r\ngoto bail;\r\n}\r\n}\r\nret = -EBUSY;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int qib_manage_rcvq(struct qib_ctxtdata *rcd, unsigned subctxt,\r\nint start_stop)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nunsigned int rcvctrl_op;\r\nif (subctxt)\r\ngoto bail;\r\nif (start_stop) {\r\nif (rcd->rcvhdrtail_kvaddr)\r\nqib_clear_rcvhdrtail(rcd);\r\nrcvctrl_op = QIB_RCVCTRL_CTXT_ENB;\r\n} else\r\nrcvctrl_op = QIB_RCVCTRL_CTXT_DIS;\r\ndd->f_rcvctrl(rcd->ppd, rcvctrl_op, rcd->ctxt);\r\nbail:\r\nreturn 0;\r\n}\r\nstatic void qib_clean_part_key(struct qib_ctxtdata *rcd,\r\nstruct qib_devdata *dd)\r\n{\r\nint i, j, pchanged = 0;\r\nu64 oldpkey;\r\nstruct qib_pportdata *ppd = rcd->ppd;\r\noldpkey = (u64) ppd->pkeys[0] |\r\n((u64) ppd->pkeys[1] << 16) |\r\n((u64) ppd->pkeys[2] << 32) |\r\n((u64) ppd->pkeys[3] << 48);\r\nfor (i = 0; i < ARRAY_SIZE(rcd->pkeys); i++) {\r\nif (!rcd->pkeys[i])\r\ncontinue;\r\nfor (j = 0; j < ARRAY_SIZE(ppd->pkeys); j++) {\r\nif ((ppd->pkeys[j] & 0x7fff) !=\r\n(rcd->pkeys[i] & 0x7fff))\r\ncontinue;\r\nif (atomic_dec_and_test(&ppd->pkeyrefs[j])) {\r\nppd->pkeys[j] = 0;\r\npchanged++;\r\n}\r\nbreak;\r\n}\r\nrcd->pkeys[i] = 0;\r\n}\r\nif (pchanged)\r\n(void) ppd->dd->f_set_ib_cfg(ppd, QIB_IB_CFG_PKEYS, 0);\r\n}\r\nstatic int qib_mmap_mem(struct vm_area_struct *vma, struct qib_ctxtdata *rcd,\r\nunsigned len, void *kvaddr, u32 write_ok, char *what)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nunsigned long pfn;\r\nint ret;\r\nif ((vma->vm_end - vma->vm_start) > len) {\r\nqib_devinfo(dd->pcidev,\r\n"FAIL on %s: len %lx > %x\n", what,\r\nvma->vm_end - vma->vm_start, len);\r\nret = -EFAULT;\r\ngoto bail;\r\n}\r\nif (!write_ok) {\r\nif (vma->vm_flags & VM_WRITE) {\r\nqib_devinfo(dd->pcidev,\r\n"%s must be mapped readonly\n", what);\r\nret = -EPERM;\r\ngoto bail;\r\n}\r\nvma->vm_flags &= ~VM_MAYWRITE;\r\n}\r\npfn = virt_to_phys(kvaddr) >> PAGE_SHIFT;\r\nret = remap_pfn_range(vma, vma->vm_start, pfn,\r\nlen, vma->vm_page_prot);\r\nif (ret)\r\nqib_devinfo(dd->pcidev,\r\n"%s ctxt%u mmap of %lx, %x bytes failed: %d\n",\r\nwhat, rcd->ctxt, pfn, len, ret);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int mmap_ureg(struct vm_area_struct *vma, struct qib_devdata *dd,\r\nu64 ureg)\r\n{\r\nunsigned long phys;\r\nunsigned long sz;\r\nint ret;\r\nsz = dd->flags & QIB_HAS_HDRSUPP ? 2 * PAGE_SIZE : PAGE_SIZE;\r\nif ((vma->vm_end - vma->vm_start) > sz) {\r\nqib_devinfo(dd->pcidev,\r\n"FAIL mmap userreg: reqlen %lx > PAGE\n",\r\nvma->vm_end - vma->vm_start);\r\nret = -EFAULT;\r\n} else {\r\nphys = dd->physaddr + ureg;\r\nvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\r\nvma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND;\r\nret = io_remap_pfn_range(vma, vma->vm_start,\r\nphys >> PAGE_SHIFT,\r\nvma->vm_end - vma->vm_start,\r\nvma->vm_page_prot);\r\n}\r\nreturn ret;\r\n}\r\nstatic int mmap_piobufs(struct vm_area_struct *vma,\r\nstruct qib_devdata *dd,\r\nstruct qib_ctxtdata *rcd,\r\nunsigned piobufs, unsigned piocnt)\r\n{\r\nunsigned long phys;\r\nint ret;\r\nif ((vma->vm_end - vma->vm_start) > (piocnt * dd->palign)) {\r\nqib_devinfo(dd->pcidev,\r\n"FAIL mmap piobufs: reqlen %lx > PAGE\n",\r\nvma->vm_end - vma->vm_start);\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nphys = dd->physaddr + piobufs;\r\n#if defined(__powerpc__)\r\npgprot_val(vma->vm_page_prot) |= _PAGE_NO_CACHE;\r\npgprot_val(vma->vm_page_prot) |= _PAGE_WRITETHRU;\r\npgprot_val(vma->vm_page_prot) &= ~_PAGE_GUARDED;\r\n#endif\r\nvma->vm_flags &= ~VM_MAYREAD;\r\nvma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND;\r\nif (qib_wc_pat)\r\nvma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);\r\nret = io_remap_pfn_range(vma, vma->vm_start, phys >> PAGE_SHIFT,\r\nvma->vm_end - vma->vm_start,\r\nvma->vm_page_prot);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int mmap_rcvegrbufs(struct vm_area_struct *vma,\r\nstruct qib_ctxtdata *rcd)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nunsigned long start, size;\r\nsize_t total_size, i;\r\nunsigned long pfn;\r\nint ret;\r\nsize = rcd->rcvegrbuf_size;\r\ntotal_size = rcd->rcvegrbuf_chunks * size;\r\nif ((vma->vm_end - vma->vm_start) > total_size) {\r\nqib_devinfo(dd->pcidev,\r\n"FAIL on egr bufs: reqlen %lx > actual %lx\n",\r\nvma->vm_end - vma->vm_start,\r\n(unsigned long) total_size);\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nif (vma->vm_flags & VM_WRITE) {\r\nqib_devinfo(dd->pcidev,\r\n"Can't map eager buffers as writable (flags=%lx)\n",\r\nvma->vm_flags);\r\nret = -EPERM;\r\ngoto bail;\r\n}\r\nvma->vm_flags &= ~VM_MAYWRITE;\r\nstart = vma->vm_start;\r\nfor (i = 0; i < rcd->rcvegrbuf_chunks; i++, start += size) {\r\npfn = virt_to_phys(rcd->rcvegrbuf[i]) >> PAGE_SHIFT;\r\nret = remap_pfn_range(vma, start, pfn, size,\r\nvma->vm_page_prot);\r\nif (ret < 0)\r\ngoto bail;\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int qib_file_vma_fault(struct vm_area_struct *vma, struct vm_fault *vmf)\r\n{\r\nstruct page *page;\r\npage = vmalloc_to_page((void *)(vmf->pgoff << PAGE_SHIFT));\r\nif (!page)\r\nreturn VM_FAULT_SIGBUS;\r\nget_page(page);\r\nvmf->page = page;\r\nreturn 0;\r\n}\r\nstatic int mmap_kvaddr(struct vm_area_struct *vma, u64 pgaddr,\r\nstruct qib_ctxtdata *rcd, unsigned subctxt)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nunsigned subctxt_cnt;\r\nunsigned long len;\r\nvoid *addr;\r\nsize_t size;\r\nint ret = 0;\r\nsubctxt_cnt = rcd->subctxt_cnt;\r\nsize = rcd->rcvegrbuf_chunks * rcd->rcvegrbuf_size;\r\nif (pgaddr == cvt_kvaddr(rcd->subctxt_uregbase)) {\r\naddr = rcd->subctxt_uregbase;\r\nsize = PAGE_SIZE * subctxt_cnt;\r\n} else if (pgaddr == cvt_kvaddr(rcd->subctxt_rcvhdr_base)) {\r\naddr = rcd->subctxt_rcvhdr_base;\r\nsize = rcd->rcvhdrq_size * subctxt_cnt;\r\n} else if (pgaddr == cvt_kvaddr(rcd->subctxt_rcvegrbuf)) {\r\naddr = rcd->subctxt_rcvegrbuf;\r\nsize *= subctxt_cnt;\r\n} else if (pgaddr == cvt_kvaddr(rcd->subctxt_uregbase +\r\nPAGE_SIZE * subctxt)) {\r\naddr = rcd->subctxt_uregbase + PAGE_SIZE * subctxt;\r\nsize = PAGE_SIZE;\r\n} else if (pgaddr == cvt_kvaddr(rcd->subctxt_rcvhdr_base +\r\nrcd->rcvhdrq_size * subctxt)) {\r\naddr = rcd->subctxt_rcvhdr_base +\r\nrcd->rcvhdrq_size * subctxt;\r\nsize = rcd->rcvhdrq_size;\r\n} else if (pgaddr == cvt_kvaddr(&rcd->user_event_mask[subctxt])) {\r\naddr = rcd->user_event_mask;\r\nsize = PAGE_SIZE;\r\n} else if (pgaddr == cvt_kvaddr(rcd->subctxt_rcvegrbuf +\r\nsize * subctxt)) {\r\naddr = rcd->subctxt_rcvegrbuf + size * subctxt;\r\nif (vma->vm_flags & VM_WRITE) {\r\nqib_devinfo(dd->pcidev,\r\n"Can't map eager buffers as "\r\n"writable (flags=%lx)\n", vma->vm_flags);\r\nret = -EPERM;\r\ngoto bail;\r\n}\r\nvma->vm_flags &= ~VM_MAYWRITE;\r\n} else\r\ngoto bail;\r\nlen = vma->vm_end - vma->vm_start;\r\nif (len > size) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nvma->vm_pgoff = (unsigned long) addr >> PAGE_SHIFT;\r\nvma->vm_ops = &qib_file_vm_ops;\r\nvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\r\nret = 1;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int qib_mmapf(struct file *fp, struct vm_area_struct *vma)\r\n{\r\nstruct qib_ctxtdata *rcd;\r\nstruct qib_devdata *dd;\r\nu64 pgaddr, ureg;\r\nunsigned piobufs, piocnt;\r\nint ret, match = 1;\r\nrcd = ctxt_fp(fp);\r\nif (!rcd || !(vma->vm_flags & VM_SHARED)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\ndd = rcd->dd;\r\npgaddr = vma->vm_pgoff << PAGE_SHIFT;\r\nif (!pgaddr) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nret = mmap_kvaddr(vma, pgaddr, rcd, subctxt_fp(fp));\r\nif (ret) {\r\nif (ret > 0)\r\nret = 0;\r\ngoto bail;\r\n}\r\nureg = dd->uregbase + dd->ureg_align * rcd->ctxt;\r\nif (!rcd->subctxt_cnt) {\r\npiocnt = rcd->piocnt;\r\npiobufs = rcd->piobufs;\r\n} else if (!subctxt_fp(fp)) {\r\npiocnt = (rcd->piocnt / rcd->subctxt_cnt) +\r\n(rcd->piocnt % rcd->subctxt_cnt);\r\npiobufs = rcd->piobufs +\r\ndd->palign * (rcd->piocnt - piocnt);\r\n} else {\r\nunsigned slave = subctxt_fp(fp) - 1;\r\npiocnt = rcd->piocnt / rcd->subctxt_cnt;\r\npiobufs = rcd->piobufs + dd->palign * piocnt * slave;\r\n}\r\nif (pgaddr == ureg)\r\nret = mmap_ureg(vma, dd, ureg);\r\nelse if (pgaddr == piobufs)\r\nret = mmap_piobufs(vma, dd, rcd, piobufs, piocnt);\r\nelse if (pgaddr == dd->pioavailregs_phys)\r\nret = qib_mmap_mem(vma, rcd, PAGE_SIZE,\r\n(void *) dd->pioavailregs_dma, 0,\r\n"pioavail registers");\r\nelse if (pgaddr == rcd->rcvegr_phys)\r\nret = mmap_rcvegrbufs(vma, rcd);\r\nelse if (pgaddr == (u64) rcd->rcvhdrq_phys)\r\nret = qib_mmap_mem(vma, rcd, rcd->rcvhdrq_size,\r\nrcd->rcvhdrq, 1, "rcvhdrq");\r\nelse if (pgaddr == (u64) rcd->rcvhdrqtailaddr_phys)\r\nret = qib_mmap_mem(vma, rcd, PAGE_SIZE,\r\nrcd->rcvhdrtail_kvaddr, 0,\r\n"rcvhdrq tail");\r\nelse\r\nmatch = 0;\r\nif (!match)\r\nret = -EINVAL;\r\nvma->vm_private_data = NULL;\r\nif (ret < 0)\r\nqib_devinfo(dd->pcidev,\r\n"mmap Failure %d: off %llx len %lx\n",\r\n-ret, (unsigned long long)pgaddr,\r\nvma->vm_end - vma->vm_start);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic unsigned int qib_poll_urgent(struct qib_ctxtdata *rcd,\r\nstruct file *fp,\r\nstruct poll_table_struct *pt)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nunsigned pollflag;\r\npoll_wait(fp, &rcd->wait, pt);\r\nspin_lock_irq(&dd->uctxt_lock);\r\nif (rcd->urgent != rcd->urgent_poll) {\r\npollflag = POLLIN | POLLRDNORM;\r\nrcd->urgent_poll = rcd->urgent;\r\n} else {\r\npollflag = 0;\r\nset_bit(QIB_CTXT_WAITING_URG, &rcd->flag);\r\n}\r\nspin_unlock_irq(&dd->uctxt_lock);\r\nreturn pollflag;\r\n}\r\nstatic unsigned int qib_poll_next(struct qib_ctxtdata *rcd,\r\nstruct file *fp,\r\nstruct poll_table_struct *pt)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nunsigned pollflag;\r\npoll_wait(fp, &rcd->wait, pt);\r\nspin_lock_irq(&dd->uctxt_lock);\r\nif (dd->f_hdrqempty(rcd)) {\r\nset_bit(QIB_CTXT_WAITING_RCV, &rcd->flag);\r\ndd->f_rcvctrl(rcd->ppd, QIB_RCVCTRL_INTRAVAIL_ENB, rcd->ctxt);\r\npollflag = 0;\r\n} else\r\npollflag = POLLIN | POLLRDNORM;\r\nspin_unlock_irq(&dd->uctxt_lock);\r\nreturn pollflag;\r\n}\r\nstatic unsigned int qib_poll(struct file *fp, struct poll_table_struct *pt)\r\n{\r\nstruct qib_ctxtdata *rcd;\r\nunsigned pollflag;\r\nrcd = ctxt_fp(fp);\r\nif (!rcd)\r\npollflag = POLLERR;\r\nelse if (rcd->poll_type == QIB_POLL_TYPE_URGENT)\r\npollflag = qib_poll_urgent(rcd, fp, pt);\r\nelse if (rcd->poll_type == QIB_POLL_TYPE_ANYRCV)\r\npollflag = qib_poll_next(rcd, fp, pt);\r\nelse\r\npollflag = POLLERR;\r\nreturn pollflag;\r\n}\r\nstatic int qib_compatible_subctxts(int user_swmajor, int user_swminor)\r\n{\r\nif (QIB_USER_SWMAJOR != user_swmajor) {\r\nreturn 0;\r\n}\r\nif (QIB_USER_SWMAJOR == 1) {\r\nswitch (QIB_USER_SWMINOR) {\r\ncase 0:\r\ncase 1:\r\ncase 2:\r\nreturn 0;\r\ncase 3:\r\nreturn user_swminor == 3;\r\ndefault:\r\nreturn user_swminor >= 4;\r\n}\r\n}\r\nreturn 0;\r\n}\r\nstatic int init_subctxts(struct qib_devdata *dd,\r\nstruct qib_ctxtdata *rcd,\r\nconst struct qib_user_info *uinfo)\r\n{\r\nint ret = 0;\r\nunsigned num_subctxts;\r\nsize_t size;\r\nif (uinfo->spu_subctxt_cnt <= 0)\r\ngoto bail;\r\nnum_subctxts = uinfo->spu_subctxt_cnt;\r\nif (!qib_compatible_subctxts(uinfo->spu_userversion >> 16,\r\nuinfo->spu_userversion & 0xffff)) {\r\nqib_devinfo(dd->pcidev,\r\n"Mismatched user version (%d.%d) and driver "\r\n"version (%d.%d) while context sharing. Ensure "\r\n"that driver and library are from the same "\r\n"release.\n",\r\n(int) (uinfo->spu_userversion >> 16),\r\n(int) (uinfo->spu_userversion & 0xffff),\r\nQIB_USER_SWMAJOR, QIB_USER_SWMINOR);\r\ngoto bail;\r\n}\r\nif (num_subctxts > QLOGIC_IB_MAX_SUBCTXT) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nrcd->subctxt_uregbase = vmalloc_user(PAGE_SIZE * num_subctxts);\r\nif (!rcd->subctxt_uregbase) {\r\nret = -ENOMEM;\r\ngoto bail;\r\n}\r\nsize = ALIGN(dd->rcvhdrcnt * dd->rcvhdrentsize *\r\nsizeof(u32), PAGE_SIZE) * num_subctxts;\r\nrcd->subctxt_rcvhdr_base = vmalloc_user(size);\r\nif (!rcd->subctxt_rcvhdr_base) {\r\nret = -ENOMEM;\r\ngoto bail_ureg;\r\n}\r\nrcd->subctxt_rcvegrbuf = vmalloc_user(rcd->rcvegrbuf_chunks *\r\nrcd->rcvegrbuf_size *\r\nnum_subctxts);\r\nif (!rcd->subctxt_rcvegrbuf) {\r\nret = -ENOMEM;\r\ngoto bail_rhdr;\r\n}\r\nrcd->subctxt_cnt = uinfo->spu_subctxt_cnt;\r\nrcd->subctxt_id = uinfo->spu_subctxt_id;\r\nrcd->active_slaves = 1;\r\nrcd->redirect_seq_cnt = 1;\r\nset_bit(QIB_CTXT_MASTER_UNINIT, &rcd->flag);\r\ngoto bail;\r\nbail_rhdr:\r\nvfree(rcd->subctxt_rcvhdr_base);\r\nbail_ureg:\r\nvfree(rcd->subctxt_uregbase);\r\nrcd->subctxt_uregbase = NULL;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int setup_ctxt(struct qib_pportdata *ppd, int ctxt,\r\nstruct file *fp, const struct qib_user_info *uinfo)\r\n{\r\nstruct qib_devdata *dd = ppd->dd;\r\nstruct qib_ctxtdata *rcd;\r\nvoid *ptmp = NULL;\r\nint ret;\r\nrcd = qib_create_ctxtdata(ppd, ctxt);\r\nif (rcd)\r\nptmp = kmalloc(dd->rcvtidcnt * sizeof(u16) +\r\ndd->rcvtidcnt * sizeof(struct page **),\r\nGFP_KERNEL);\r\nif (!rcd || !ptmp) {\r\nqib_dev_err(dd,\r\n"Unable to allocate ctxtdata memory, failing open\n");\r\nret = -ENOMEM;\r\ngoto bailerr;\r\n}\r\nrcd->userversion = uinfo->spu_userversion;\r\nret = init_subctxts(dd, rcd, uinfo);\r\nif (ret)\r\ngoto bailerr;\r\nrcd->tid_pg_list = ptmp;\r\nrcd->pid = current->pid;\r\ninit_waitqueue_head(&dd->rcd[ctxt]->wait);\r\nstrlcpy(rcd->comm, current->comm, sizeof(rcd->comm));\r\nctxt_fp(fp) = rcd;\r\nqib_stats.sps_ctxts++;\r\ndd->freectxts--;\r\nret = 0;\r\ngoto bail;\r\nbailerr:\r\ndd->rcd[ctxt] = NULL;\r\nkfree(rcd);\r\nkfree(ptmp);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic inline int usable(struct qib_pportdata *ppd)\r\n{\r\nstruct qib_devdata *dd = ppd->dd;\r\nreturn dd && (dd->flags & QIB_PRESENT) && dd->kregbase && ppd->lid &&\r\n(ppd->lflags & QIBL_LINKACTIVE);\r\n}\r\nstatic int choose_port_ctxt(struct file *fp, struct qib_devdata *dd, u32 port,\r\nconst struct qib_user_info *uinfo)\r\n{\r\nstruct qib_pportdata *ppd = NULL;\r\nint ret, ctxt;\r\nif (port) {\r\nif (!usable(dd->pport + port - 1)) {\r\nret = -ENETDOWN;\r\ngoto done;\r\n} else\r\nppd = dd->pport + port - 1;\r\n}\r\nfor (ctxt = dd->first_user_ctxt; ctxt < dd->cfgctxts && dd->rcd[ctxt];\r\nctxt++)\r\n;\r\nif (ctxt == dd->cfgctxts) {\r\nret = -EBUSY;\r\ngoto done;\r\n}\r\nif (!ppd) {\r\nu32 pidx = ctxt % dd->num_pports;\r\nif (usable(dd->pport + pidx))\r\nppd = dd->pport + pidx;\r\nelse {\r\nfor (pidx = 0; pidx < dd->num_pports && !ppd;\r\npidx++)\r\nif (usable(dd->pport + pidx))\r\nppd = dd->pport + pidx;\r\n}\r\n}\r\nret = ppd ? setup_ctxt(ppd, ctxt, fp, uinfo) : -ENETDOWN;\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int find_free_ctxt(int unit, struct file *fp,\r\nconst struct qib_user_info *uinfo)\r\n{\r\nstruct qib_devdata *dd = qib_lookup(unit);\r\nint ret;\r\nif (!dd || (uinfo->spu_port && uinfo->spu_port > dd->num_pports))\r\nret = -ENODEV;\r\nelse\r\nret = choose_port_ctxt(fp, dd, uinfo->spu_port, uinfo);\r\nreturn ret;\r\n}\r\nstatic int get_a_ctxt(struct file *fp, const struct qib_user_info *uinfo,\r\nunsigned alg)\r\n{\r\nstruct qib_devdata *udd = NULL;\r\nint ret = 0, devmax, npresent, nup, ndev, dusable = 0, i;\r\nu32 port = uinfo->spu_port, ctxt;\r\ndevmax = qib_count_units(&npresent, &nup);\r\nif (!npresent) {\r\nret = -ENXIO;\r\ngoto done;\r\n}\r\nif (nup == 0) {\r\nret = -ENETDOWN;\r\ngoto done;\r\n}\r\nif (alg == QIB_PORT_ALG_ACROSS) {\r\nunsigned inuse = ~0U;\r\nfor (ndev = 0; ndev < devmax; ndev++) {\r\nstruct qib_devdata *dd = qib_lookup(ndev);\r\nunsigned cused = 0, cfree = 0, pusable = 0;\r\nif (!dd)\r\ncontinue;\r\nif (port && port <= dd->num_pports &&\r\nusable(dd->pport + port - 1))\r\npusable = 1;\r\nelse\r\nfor (i = 0; i < dd->num_pports; i++)\r\nif (usable(dd->pport + i))\r\npusable++;\r\nif (!pusable)\r\ncontinue;\r\nfor (ctxt = dd->first_user_ctxt; ctxt < dd->cfgctxts;\r\nctxt++)\r\nif (dd->rcd[ctxt])\r\ncused++;\r\nelse\r\ncfree++;\r\nif (pusable && cfree && cused < inuse) {\r\nudd = dd;\r\ninuse = cused;\r\n}\r\n}\r\nif (udd) {\r\nret = choose_port_ctxt(fp, udd, port, uinfo);\r\ngoto done;\r\n}\r\n} else {\r\nfor (ndev = 0; ndev < devmax; ndev++) {\r\nstruct qib_devdata *dd = qib_lookup(ndev);\r\nif (dd) {\r\nret = choose_port_ctxt(fp, dd, port, uinfo);\r\nif (!ret)\r\ngoto done;\r\nif (ret == -EBUSY)\r\ndusable++;\r\n}\r\n}\r\n}\r\nret = dusable ? -EBUSY : -ENETDOWN;\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int find_shared_ctxt(struct file *fp,\r\nconst struct qib_user_info *uinfo)\r\n{\r\nint devmax, ndev, i;\r\nint ret = 0;\r\ndevmax = qib_count_units(NULL, NULL);\r\nfor (ndev = 0; ndev < devmax; ndev++) {\r\nstruct qib_devdata *dd = qib_lookup(ndev);\r\nif (!(dd && (dd->flags & QIB_PRESENT) && dd->kregbase))\r\ncontinue;\r\nfor (i = dd->first_user_ctxt; i < dd->cfgctxts; i++) {\r\nstruct qib_ctxtdata *rcd = dd->rcd[i];\r\nif (!rcd || !rcd->cnt)\r\ncontinue;\r\nif (rcd->subctxt_id != uinfo->spu_subctxt_id)\r\ncontinue;\r\nif (rcd->subctxt_cnt != uinfo->spu_subctxt_cnt ||\r\nrcd->userversion != uinfo->spu_userversion ||\r\nrcd->cnt >= rcd->subctxt_cnt) {\r\nret = -EINVAL;\r\ngoto done;\r\n}\r\nctxt_fp(fp) = rcd;\r\nsubctxt_fp(fp) = rcd->cnt++;\r\nrcd->subpid[subctxt_fp(fp)] = current->pid;\r\ntidcursor_fp(fp) = 0;\r\nrcd->active_slaves |= 1 << subctxt_fp(fp);\r\nret = 1;\r\ngoto done;\r\n}\r\n}\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int qib_open(struct inode *in, struct file *fp)\r\n{\r\nfp->private_data = kzalloc(sizeof(struct qib_filedata), GFP_KERNEL);\r\nif (fp->private_data)\r\n((struct qib_filedata *)fp->private_data)->rec_cpu_num = -1;\r\nreturn fp->private_data ? 0 : -ENOMEM;\r\n}\r\nstatic int qib_assign_ctxt(struct file *fp, const struct qib_user_info *uinfo)\r\n{\r\nint ret;\r\nint i_minor;\r\nunsigned swmajor, swminor, alg = QIB_PORT_ALG_ACROSS;\r\nif (ctxt_fp(fp)) {\r\nret = -EINVAL;\r\ngoto done;\r\n}\r\nswmajor = uinfo->spu_userversion >> 16;\r\nif (swmajor != QIB_USER_SWMAJOR) {\r\nret = -ENODEV;\r\ngoto done;\r\n}\r\nswminor = uinfo->spu_userversion & 0xffff;\r\nif (swminor >= 11 && uinfo->spu_port_alg < QIB_PORT_ALG_COUNT)\r\nalg = uinfo->spu_port_alg;\r\nmutex_lock(&qib_mutex);\r\nif (qib_compatible_subctxts(swmajor, swminor) &&\r\nuinfo->spu_subctxt_cnt) {\r\nret = find_shared_ctxt(fp, uinfo);\r\nif (ret) {\r\nif (ret > 0)\r\nret = 0;\r\ngoto done_chk_sdma;\r\n}\r\n}\r\ni_minor = iminor(fp->f_dentry->d_inode) - QIB_USER_MINOR_BASE;\r\nif (i_minor)\r\nret = find_free_ctxt(i_minor - 1, fp, uinfo);\r\nelse\r\nret = get_a_ctxt(fp, uinfo, alg);\r\ndone_chk_sdma:\r\nif (!ret) {\r\nstruct qib_filedata *fd = fp->private_data;\r\nconst struct qib_ctxtdata *rcd = fd->rcd;\r\nconst struct qib_devdata *dd = rcd->dd;\r\nunsigned int weight;\r\nif (dd->flags & QIB_HAS_SEND_DMA) {\r\nfd->pq = qib_user_sdma_queue_create(&dd->pcidev->dev,\r\ndd->unit,\r\nrcd->ctxt,\r\nfd->subctxt);\r\nif (!fd->pq)\r\nret = -ENOMEM;\r\n}\r\nweight = cpumask_weight(tsk_cpus_allowed(current));\r\nif (!ret && weight >= qib_cpulist_count) {\r\nint cpu;\r\ncpu = find_first_zero_bit(qib_cpulist,\r\nqib_cpulist_count);\r\nif (cpu != qib_cpulist_count) {\r\n__set_bit(cpu, qib_cpulist);\r\nfd->rec_cpu_num = cpu;\r\n}\r\n} else if (weight == 1 &&\r\ntest_bit(cpumask_first(tsk_cpus_allowed(current)),\r\nqib_cpulist))\r\nqib_devinfo(dd->pcidev,\r\n"%s PID %u affinity set to cpu %d; already allocated\n",\r\ncurrent->comm, current->pid,\r\ncpumask_first(tsk_cpus_allowed(current)));\r\n}\r\nmutex_unlock(&qib_mutex);\r\ndone:\r\nreturn ret;\r\n}\r\nstatic int qib_do_user_init(struct file *fp,\r\nconst struct qib_user_info *uinfo)\r\n{\r\nint ret;\r\nstruct qib_ctxtdata *rcd = ctxt_fp(fp);\r\nstruct qib_devdata *dd;\r\nunsigned uctxt;\r\nif (subctxt_fp(fp)) {\r\nret = wait_event_interruptible(rcd->wait,\r\n!test_bit(QIB_CTXT_MASTER_UNINIT, &rcd->flag));\r\ngoto bail;\r\n}\r\ndd = rcd->dd;\r\nuctxt = rcd->ctxt - dd->first_user_ctxt;\r\nif (uctxt < dd->ctxts_extrabuf) {\r\nrcd->piocnt = dd->pbufsctxt + 1;\r\nrcd->pio_base = rcd->piocnt * uctxt;\r\n} else {\r\nrcd->piocnt = dd->pbufsctxt;\r\nrcd->pio_base = rcd->piocnt * uctxt +\r\ndd->ctxts_extrabuf;\r\n}\r\nif ((rcd->pio_base + rcd->piocnt) > dd->piobcnt2k) {\r\nif (rcd->pio_base >= dd->piobcnt2k) {\r\nqib_dev_err(dd,\r\n"%u:ctxt%u: no 2KB buffers available\n",\r\ndd->unit, rcd->ctxt);\r\nret = -ENOBUFS;\r\ngoto bail;\r\n}\r\nrcd->piocnt = dd->piobcnt2k - rcd->pio_base;\r\nqib_dev_err(dd, "Ctxt%u: would use 4KB bufs, using %u\n",\r\nrcd->ctxt, rcd->piocnt);\r\n}\r\nrcd->piobufs = dd->pio2k_bufbase + rcd->pio_base * dd->palign;\r\nqib_chg_pioavailkernel(dd, rcd->pio_base, rcd->piocnt,\r\nTXCHK_CHG_TYPE_USER, rcd);\r\ndd->f_sendctrl(dd->pport, QIB_SENDCTRL_AVAIL_BLIP);\r\nret = qib_create_rcvhdrq(dd, rcd);\r\nif (!ret)\r\nret = qib_setup_eagerbufs(rcd);\r\nif (ret)\r\ngoto bail_pio;\r\nrcd->tidcursor = 0;\r\nrcd->urgent = 0;\r\nrcd->urgent_poll = 0;\r\nif (rcd->rcvhdrtail_kvaddr)\r\nqib_clear_rcvhdrtail(rcd);\r\ndd->f_rcvctrl(rcd->ppd, QIB_RCVCTRL_CTXT_ENB | QIB_RCVCTRL_TIDFLOW_ENB,\r\nrcd->ctxt);\r\nif (rcd->subctxt_cnt) {\r\nclear_bit(QIB_CTXT_MASTER_UNINIT, &rcd->flag);\r\nwake_up(&rcd->wait);\r\n}\r\nreturn 0;\r\nbail_pio:\r\nqib_chg_pioavailkernel(dd, rcd->pio_base, rcd->piocnt,\r\nTXCHK_CHG_TYPE_KERN, rcd);\r\nbail:\r\nreturn ret;\r\n}\r\nstatic void unlock_expected_tids(struct qib_ctxtdata *rcd)\r\n{\r\nstruct qib_devdata *dd = rcd->dd;\r\nint ctxt_tidbase = rcd->ctxt * dd->rcvtidcnt;\r\nint i, cnt = 0, maxtid = ctxt_tidbase + dd->rcvtidcnt;\r\nfor (i = ctxt_tidbase; i < maxtid; i++) {\r\nstruct page *p = dd->pageshadow[i];\r\ndma_addr_t phys;\r\nif (!p)\r\ncontinue;\r\nphys = dd->physshadow[i];\r\ndd->physshadow[i] = dd->tidinvalid;\r\ndd->pageshadow[i] = NULL;\r\npci_unmap_page(dd->pcidev, phys, PAGE_SIZE,\r\nPCI_DMA_FROMDEVICE);\r\nqib_release_user_pages(&p, 1);\r\ncnt++;\r\n}\r\n}\r\nstatic int qib_close(struct inode *in, struct file *fp)\r\n{\r\nint ret = 0;\r\nstruct qib_filedata *fd;\r\nstruct qib_ctxtdata *rcd;\r\nstruct qib_devdata *dd;\r\nunsigned long flags;\r\nunsigned ctxt;\r\npid_t pid;\r\nmutex_lock(&qib_mutex);\r\nfd = fp->private_data;\r\nfp->private_data = NULL;\r\nrcd = fd->rcd;\r\nif (!rcd) {\r\nmutex_unlock(&qib_mutex);\r\ngoto bail;\r\n}\r\ndd = rcd->dd;\r\nqib_flush_wc();\r\nif (fd->pq) {\r\nqib_user_sdma_queue_drain(rcd->ppd, fd->pq);\r\nqib_user_sdma_queue_destroy(fd->pq);\r\n}\r\nif (fd->rec_cpu_num != -1)\r\n__clear_bit(fd->rec_cpu_num, qib_cpulist);\r\nif (--rcd->cnt) {\r\nrcd->active_slaves &= ~(1 << fd->subctxt);\r\nrcd->subpid[fd->subctxt] = 0;\r\nmutex_unlock(&qib_mutex);\r\ngoto bail;\r\n}\r\nspin_lock_irqsave(&dd->uctxt_lock, flags);\r\nctxt = rcd->ctxt;\r\ndd->rcd[ctxt] = NULL;\r\npid = rcd->pid;\r\nrcd->pid = 0;\r\nspin_unlock_irqrestore(&dd->uctxt_lock, flags);\r\nif (rcd->rcvwait_to || rcd->piowait_to ||\r\nrcd->rcvnowait || rcd->pionowait) {\r\nrcd->rcvwait_to = 0;\r\nrcd->piowait_to = 0;\r\nrcd->rcvnowait = 0;\r\nrcd->pionowait = 0;\r\n}\r\nif (rcd->flag)\r\nrcd->flag = 0;\r\nif (dd->kregbase) {\r\ndd->f_rcvctrl(rcd->ppd, QIB_RCVCTRL_CTXT_DIS |\r\nQIB_RCVCTRL_INTRAVAIL_DIS, ctxt);\r\nqib_clean_part_key(rcd, dd);\r\nqib_disarm_piobufs(dd, rcd->pio_base, rcd->piocnt);\r\nqib_chg_pioavailkernel(dd, rcd->pio_base,\r\nrcd->piocnt, TXCHK_CHG_TYPE_KERN, NULL);\r\ndd->f_clear_tids(dd, rcd);\r\nif (dd->pageshadow)\r\nunlock_expected_tids(rcd);\r\nqib_stats.sps_ctxts--;\r\ndd->freectxts++;\r\n}\r\nmutex_unlock(&qib_mutex);\r\nqib_free_ctxtdata(dd, rcd);\r\nbail:\r\nkfree(fd);\r\nreturn ret;\r\n}\r\nstatic int qib_ctxt_info(struct file *fp, struct qib_ctxt_info __user *uinfo)\r\n{\r\nstruct qib_ctxt_info info;\r\nint ret;\r\nsize_t sz;\r\nstruct qib_ctxtdata *rcd = ctxt_fp(fp);\r\nstruct qib_filedata *fd;\r\nfd = fp->private_data;\r\ninfo.num_active = qib_count_active_units();\r\ninfo.unit = rcd->dd->unit;\r\ninfo.port = rcd->ppd->port;\r\ninfo.ctxt = rcd->ctxt;\r\ninfo.subctxt = subctxt_fp(fp);\r\ninfo.num_ctxts = rcd->dd->cfgctxts - rcd->dd->first_user_ctxt;\r\ninfo.num_subctxts = rcd->subctxt_cnt;\r\ninfo.rec_cpu = fd->rec_cpu_num;\r\nsz = sizeof(info);\r\nif (copy_to_user(uinfo, &info, sz)) {\r\nret = -EFAULT;\r\ngoto bail;\r\n}\r\nret = 0;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic int qib_sdma_get_inflight(struct qib_user_sdma_queue *pq,\r\nu32 __user *inflightp)\r\n{\r\nconst u32 val = qib_user_sdma_inflight_counter(pq);\r\nif (put_user(val, inflightp))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int qib_sdma_get_complete(struct qib_pportdata *ppd,\r\nstruct qib_user_sdma_queue *pq,\r\nu32 __user *completep)\r\n{\r\nu32 val;\r\nint err;\r\nif (!pq)\r\nreturn -EINVAL;\r\nerr = qib_user_sdma_make_progress(ppd, pq);\r\nif (err < 0)\r\nreturn err;\r\nval = qib_user_sdma_complete_counter(pq);\r\nif (put_user(val, completep))\r\nreturn -EFAULT;\r\nreturn 0;\r\n}\r\nstatic int disarm_req_delay(struct qib_ctxtdata *rcd)\r\n{\r\nint ret = 0;\r\nif (!usable(rcd->ppd)) {\r\nint i;\r\nif (rcd->user_event_mask) {\r\nset_bit(_QIB_EVENT_DISARM_BUFS_BIT,\r\n&rcd->user_event_mask[0]);\r\nfor (i = 1; i < rcd->subctxt_cnt; i++)\r\nset_bit(_QIB_EVENT_DISARM_BUFS_BIT,\r\n&rcd->user_event_mask[i]);\r\n}\r\nfor (i = 0; !usable(rcd->ppd) && i < 300; i++)\r\nmsleep(100);\r\nret = -ENETDOWN;\r\n}\r\nreturn ret;\r\n}\r\nint qib_set_uevent_bits(struct qib_pportdata *ppd, const int evtbit)\r\n{\r\nstruct qib_ctxtdata *rcd;\r\nunsigned ctxt;\r\nint ret = 0;\r\nunsigned long flags;\r\nspin_lock_irqsave(&ppd->dd->uctxt_lock, flags);\r\nfor (ctxt = ppd->dd->first_user_ctxt; ctxt < ppd->dd->cfgctxts;\r\nctxt++) {\r\nrcd = ppd->dd->rcd[ctxt];\r\nif (!rcd)\r\ncontinue;\r\nif (rcd->user_event_mask) {\r\nint i;\r\nset_bit(evtbit, &rcd->user_event_mask[0]);\r\nfor (i = 1; i < rcd->subctxt_cnt; i++)\r\nset_bit(evtbit, &rcd->user_event_mask[i]);\r\n}\r\nret = 1;\r\nbreak;\r\n}\r\nspin_unlock_irqrestore(&ppd->dd->uctxt_lock, flags);\r\nreturn ret;\r\n}\r\nstatic int qib_user_event_ack(struct qib_ctxtdata *rcd, int subctxt,\r\nunsigned long events)\r\n{\r\nint ret = 0, i;\r\nfor (i = 0; i <= _QIB_MAX_EVENT_BIT; i++) {\r\nif (!test_bit(i, &events))\r\ncontinue;\r\nif (i == _QIB_EVENT_DISARM_BUFS_BIT) {\r\n(void)qib_disarm_piobufs_ifneeded(rcd);\r\nret = disarm_req_delay(rcd);\r\n} else\r\nclear_bit(i, &rcd->user_event_mask[subctxt]);\r\n}\r\nreturn ret;\r\n}\r\nstatic ssize_t qib_write(struct file *fp, const char __user *data,\r\nsize_t count, loff_t *off)\r\n{\r\nconst struct qib_cmd __user *ucmd;\r\nstruct qib_ctxtdata *rcd;\r\nconst void __user *src;\r\nsize_t consumed, copy = 0;\r\nstruct qib_cmd cmd;\r\nssize_t ret = 0;\r\nvoid *dest;\r\nif (count < sizeof(cmd.type)) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nucmd = (const struct qib_cmd __user *) data;\r\nif (copy_from_user(&cmd.type, &ucmd->type, sizeof(cmd.type))) {\r\nret = -EFAULT;\r\ngoto bail;\r\n}\r\nconsumed = sizeof(cmd.type);\r\nswitch (cmd.type) {\r\ncase QIB_CMD_ASSIGN_CTXT:\r\ncase QIB_CMD_USER_INIT:\r\ncopy = sizeof(cmd.cmd.user_info);\r\ndest = &cmd.cmd.user_info;\r\nsrc = &ucmd->cmd.user_info;\r\nbreak;\r\ncase QIB_CMD_RECV_CTRL:\r\ncopy = sizeof(cmd.cmd.recv_ctrl);\r\ndest = &cmd.cmd.recv_ctrl;\r\nsrc = &ucmd->cmd.recv_ctrl;\r\nbreak;\r\ncase QIB_CMD_CTXT_INFO:\r\ncopy = sizeof(cmd.cmd.ctxt_info);\r\ndest = &cmd.cmd.ctxt_info;\r\nsrc = &ucmd->cmd.ctxt_info;\r\nbreak;\r\ncase QIB_CMD_TID_UPDATE:\r\ncase QIB_CMD_TID_FREE:\r\ncopy = sizeof(cmd.cmd.tid_info);\r\ndest = &cmd.cmd.tid_info;\r\nsrc = &ucmd->cmd.tid_info;\r\nbreak;\r\ncase QIB_CMD_SET_PART_KEY:\r\ncopy = sizeof(cmd.cmd.part_key);\r\ndest = &cmd.cmd.part_key;\r\nsrc = &ucmd->cmd.part_key;\r\nbreak;\r\ncase QIB_CMD_DISARM_BUFS:\r\ncase QIB_CMD_PIOAVAILUPD:\r\ncopy = 0;\r\nsrc = NULL;\r\ndest = NULL;\r\nbreak;\r\ncase QIB_CMD_POLL_TYPE:\r\ncopy = sizeof(cmd.cmd.poll_type);\r\ndest = &cmd.cmd.poll_type;\r\nsrc = &ucmd->cmd.poll_type;\r\nbreak;\r\ncase QIB_CMD_ARMLAUNCH_CTRL:\r\ncopy = sizeof(cmd.cmd.armlaunch_ctrl);\r\ndest = &cmd.cmd.armlaunch_ctrl;\r\nsrc = &ucmd->cmd.armlaunch_ctrl;\r\nbreak;\r\ncase QIB_CMD_SDMA_INFLIGHT:\r\ncopy = sizeof(cmd.cmd.sdma_inflight);\r\ndest = &cmd.cmd.sdma_inflight;\r\nsrc = &ucmd->cmd.sdma_inflight;\r\nbreak;\r\ncase QIB_CMD_SDMA_COMPLETE:\r\ncopy = sizeof(cmd.cmd.sdma_complete);\r\ndest = &cmd.cmd.sdma_complete;\r\nsrc = &ucmd->cmd.sdma_complete;\r\nbreak;\r\ncase QIB_CMD_ACK_EVENT:\r\ncopy = sizeof(cmd.cmd.event_mask);\r\ndest = &cmd.cmd.event_mask;\r\nsrc = &ucmd->cmd.event_mask;\r\nbreak;\r\ndefault:\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nif (copy) {\r\nif ((count - consumed) < copy) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nif (copy_from_user(dest, src, copy)) {\r\nret = -EFAULT;\r\ngoto bail;\r\n}\r\nconsumed += copy;\r\n}\r\nrcd = ctxt_fp(fp);\r\nif (!rcd && cmd.type != QIB_CMD_ASSIGN_CTXT) {\r\nret = -EINVAL;\r\ngoto bail;\r\n}\r\nswitch (cmd.type) {\r\ncase QIB_CMD_ASSIGN_CTXT:\r\nret = qib_assign_ctxt(fp, &cmd.cmd.user_info);\r\nif (ret)\r\ngoto bail;\r\nbreak;\r\ncase QIB_CMD_USER_INIT:\r\nret = qib_do_user_init(fp, &cmd.cmd.user_info);\r\nif (ret)\r\ngoto bail;\r\nret = qib_get_base_info(fp, (void __user *) (unsigned long)\r\ncmd.cmd.user_info.spu_base_info,\r\ncmd.cmd.user_info.spu_base_info_size);\r\nbreak;\r\ncase QIB_CMD_RECV_CTRL:\r\nret = qib_manage_rcvq(rcd, subctxt_fp(fp), cmd.cmd.recv_ctrl);\r\nbreak;\r\ncase QIB_CMD_CTXT_INFO:\r\nret = qib_ctxt_info(fp, (struct qib_ctxt_info __user *)\r\n(unsigned long) cmd.cmd.ctxt_info);\r\nbreak;\r\ncase QIB_CMD_TID_UPDATE:\r\nret = qib_tid_update(rcd, fp, &cmd.cmd.tid_info);\r\nbreak;\r\ncase QIB_CMD_TID_FREE:\r\nret = qib_tid_free(rcd, subctxt_fp(fp), &cmd.cmd.tid_info);\r\nbreak;\r\ncase QIB_CMD_SET_PART_KEY:\r\nret = qib_set_part_key(rcd, cmd.cmd.part_key);\r\nbreak;\r\ncase QIB_CMD_DISARM_BUFS:\r\n(void)qib_disarm_piobufs_ifneeded(rcd);\r\nret = disarm_req_delay(rcd);\r\nbreak;\r\ncase QIB_CMD_PIOAVAILUPD:\r\nqib_force_pio_avail_update(rcd->dd);\r\nbreak;\r\ncase QIB_CMD_POLL_TYPE:\r\nrcd->poll_type = cmd.cmd.poll_type;\r\nbreak;\r\ncase QIB_CMD_ARMLAUNCH_CTRL:\r\nrcd->dd->f_set_armlaunch(rcd->dd, cmd.cmd.armlaunch_ctrl);\r\nbreak;\r\ncase QIB_CMD_SDMA_INFLIGHT:\r\nret = qib_sdma_get_inflight(user_sdma_queue_fp(fp),\r\n(u32 __user *) (unsigned long)\r\ncmd.cmd.sdma_inflight);\r\nbreak;\r\ncase QIB_CMD_SDMA_COMPLETE:\r\nret = qib_sdma_get_complete(rcd->ppd,\r\nuser_sdma_queue_fp(fp),\r\n(u32 __user *) (unsigned long)\r\ncmd.cmd.sdma_complete);\r\nbreak;\r\ncase QIB_CMD_ACK_EVENT:\r\nret = qib_user_event_ack(rcd, subctxt_fp(fp),\r\ncmd.cmd.event_mask);\r\nbreak;\r\n}\r\nif (ret >= 0)\r\nret = consumed;\r\nbail:\r\nreturn ret;\r\n}\r\nstatic ssize_t qib_aio_write(struct kiocb *iocb, const struct iovec *iov,\r\nunsigned long dim, loff_t off)\r\n{\r\nstruct qib_filedata *fp = iocb->ki_filp->private_data;\r\nstruct qib_ctxtdata *rcd = ctxt_fp(iocb->ki_filp);\r\nstruct qib_user_sdma_queue *pq = fp->pq;\r\nif (!dim || !pq)\r\nreturn -EINVAL;\r\nreturn qib_user_sdma_writev(rcd, pq, iov, dim);\r\n}\r\nint qib_cdev_init(int minor, const char *name,\r\nconst struct file_operations *fops,\r\nstruct cdev **cdevp, struct device **devp)\r\n{\r\nconst dev_t dev = MKDEV(MAJOR(qib_dev), minor);\r\nstruct cdev *cdev;\r\nstruct device *device = NULL;\r\nint ret;\r\ncdev = cdev_alloc();\r\nif (!cdev) {\r\npr_err("Could not allocate cdev for minor %d, %s\n",\r\nminor, name);\r\nret = -ENOMEM;\r\ngoto done;\r\n}\r\ncdev->owner = THIS_MODULE;\r\ncdev->ops = fops;\r\nkobject_set_name(&cdev->kobj, name);\r\nret = cdev_add(cdev, dev, 1);\r\nif (ret < 0) {\r\npr_err("Could not add cdev for minor %d, %s (err %d)\n",\r\nminor, name, -ret);\r\ngoto err_cdev;\r\n}\r\ndevice = device_create(qib_class, NULL, dev, NULL, name);\r\nif (!IS_ERR(device))\r\ngoto done;\r\nret = PTR_ERR(device);\r\ndevice = NULL;\r\npr_err("Could not create device for minor %d, %s (err %d)\n",\r\nminor, name, -ret);\r\nerr_cdev:\r\ncdev_del(cdev);\r\ncdev = NULL;\r\ndone:\r\n*cdevp = cdev;\r\n*devp = device;\r\nreturn ret;\r\n}\r\nvoid qib_cdev_cleanup(struct cdev **cdevp, struct device **devp)\r\n{\r\nstruct device *device = *devp;\r\nif (device) {\r\ndevice_unregister(device);\r\n*devp = NULL;\r\n}\r\nif (*cdevp) {\r\ncdev_del(*cdevp);\r\n*cdevp = NULL;\r\n}\r\n}\r\nint __init qib_dev_init(void)\r\n{\r\nint ret;\r\nret = alloc_chrdev_region(&qib_dev, 0, QIB_NMINORS, QIB_DRV_NAME);\r\nif (ret < 0) {\r\npr_err("Could not allocate chrdev region (err %d)\n", -ret);\r\ngoto done;\r\n}\r\nqib_class = class_create(THIS_MODULE, "ipath");\r\nif (IS_ERR(qib_class)) {\r\nret = PTR_ERR(qib_class);\r\npr_err("Could not create device class (err %d)\n", -ret);\r\nunregister_chrdev_region(qib_dev, QIB_NMINORS);\r\n}\r\ndone:\r\nreturn ret;\r\n}\r\nvoid qib_dev_cleanup(void)\r\n{\r\nif (qib_class) {\r\nclass_destroy(qib_class);\r\nqib_class = NULL;\r\n}\r\nunregister_chrdev_region(qib_dev, QIB_NMINORS);\r\n}\r\nstatic void qib_user_remove(struct qib_devdata *dd)\r\n{\r\nif (atomic_dec_return(&user_count) == 0)\r\nqib_cdev_cleanup(&wildcard_cdev, &wildcard_device);\r\nqib_cdev_cleanup(&dd->user_cdev, &dd->user_device);\r\n}\r\nstatic int qib_user_add(struct qib_devdata *dd)\r\n{\r\nchar name[10];\r\nint ret;\r\nif (atomic_inc_return(&user_count) == 1) {\r\nret = qib_cdev_init(0, "ipath", &qib_file_ops,\r\n&wildcard_cdev, &wildcard_device);\r\nif (ret)\r\ngoto done;\r\n}\r\nsnprintf(name, sizeof(name), "ipath%d", dd->unit);\r\nret = qib_cdev_init(dd->unit + 1, name, &qib_file_ops,\r\n&dd->user_cdev, &dd->user_device);\r\nif (ret)\r\nqib_user_remove(dd);\r\ndone:\r\nreturn ret;\r\n}\r\nint qib_device_create(struct qib_devdata *dd)\r\n{\r\nint r, ret;\r\nr = qib_user_add(dd);\r\nret = qib_diag_add(dd);\r\nif (r && !ret)\r\nret = r;\r\nreturn ret;\r\n}\r\nvoid qib_device_remove(struct qib_devdata *dd)\r\n{\r\nqib_user_remove(dd);\r\nqib_diag_remove(dd);\r\n}
