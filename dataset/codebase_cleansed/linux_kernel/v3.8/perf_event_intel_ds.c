void init_debug_store_on_cpu(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nif (!ds)\r\nreturn;\r\nwrmsr_on_cpu(cpu, MSR_IA32_DS_AREA,\r\n(u32)((u64)(unsigned long)ds),\r\n(u32)((u64)(unsigned long)ds >> 32));\r\n}\r\nvoid fini_debug_store_on_cpu(int cpu)\r\n{\r\nif (!per_cpu(cpu_hw_events, cpu).ds)\r\nreturn;\r\nwrmsr_on_cpu(cpu, MSR_IA32_DS_AREA, 0, 0);\r\n}\r\nstatic int alloc_pebs_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nint node = cpu_to_node(cpu);\r\nint max, thresh = 1;\r\nvoid *buffer;\r\nif (!x86_pmu.pebs)\r\nreturn 0;\r\nbuffer = kmalloc_node(PEBS_BUFFER_SIZE, GFP_KERNEL | __GFP_ZERO, node);\r\nif (unlikely(!buffer))\r\nreturn -ENOMEM;\r\nmax = PEBS_BUFFER_SIZE / x86_pmu.pebs_record_size;\r\nds->pebs_buffer_base = (u64)(unsigned long)buffer;\r\nds->pebs_index = ds->pebs_buffer_base;\r\nds->pebs_absolute_maximum = ds->pebs_buffer_base +\r\nmax * x86_pmu.pebs_record_size;\r\nds->pebs_interrupt_threshold = ds->pebs_buffer_base +\r\nthresh * x86_pmu.pebs_record_size;\r\nreturn 0;\r\n}\r\nstatic void release_pebs_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nif (!ds || !x86_pmu.pebs)\r\nreturn;\r\nkfree((void *)(unsigned long)ds->pebs_buffer_base);\r\nds->pebs_buffer_base = 0;\r\n}\r\nstatic int alloc_bts_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nint node = cpu_to_node(cpu);\r\nint max, thresh;\r\nvoid *buffer;\r\nif (!x86_pmu.bts)\r\nreturn 0;\r\nbuffer = kmalloc_node(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_ZERO, node);\r\nif (unlikely(!buffer))\r\nreturn -ENOMEM;\r\nmax = BTS_BUFFER_SIZE / BTS_RECORD_SIZE;\r\nthresh = max / 16;\r\nds->bts_buffer_base = (u64)(unsigned long)buffer;\r\nds->bts_index = ds->bts_buffer_base;\r\nds->bts_absolute_maximum = ds->bts_buffer_base +\r\nmax * BTS_RECORD_SIZE;\r\nds->bts_interrupt_threshold = ds->bts_absolute_maximum -\r\nthresh * BTS_RECORD_SIZE;\r\nreturn 0;\r\n}\r\nstatic void release_bts_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nif (!ds || !x86_pmu.bts)\r\nreturn;\r\nkfree((void *)(unsigned long)ds->bts_buffer_base);\r\nds->bts_buffer_base = 0;\r\n}\r\nstatic int alloc_ds_buffer(int cpu)\r\n{\r\nint node = cpu_to_node(cpu);\r\nstruct debug_store *ds;\r\nds = kmalloc_node(sizeof(*ds), GFP_KERNEL | __GFP_ZERO, node);\r\nif (unlikely(!ds))\r\nreturn -ENOMEM;\r\nper_cpu(cpu_hw_events, cpu).ds = ds;\r\nreturn 0;\r\n}\r\nstatic void release_ds_buffer(int cpu)\r\n{\r\nstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\r\nif (!ds)\r\nreturn;\r\nper_cpu(cpu_hw_events, cpu).ds = NULL;\r\nkfree(ds);\r\n}\r\nvoid release_ds_buffers(void)\r\n{\r\nint cpu;\r\nif (!x86_pmu.bts && !x86_pmu.pebs)\r\nreturn;\r\nget_online_cpus();\r\nfor_each_online_cpu(cpu)\r\nfini_debug_store_on_cpu(cpu);\r\nfor_each_possible_cpu(cpu) {\r\nrelease_pebs_buffer(cpu);\r\nrelease_bts_buffer(cpu);\r\nrelease_ds_buffer(cpu);\r\n}\r\nput_online_cpus();\r\n}\r\nvoid reserve_ds_buffers(void)\r\n{\r\nint bts_err = 0, pebs_err = 0;\r\nint cpu;\r\nx86_pmu.bts_active = 0;\r\nx86_pmu.pebs_active = 0;\r\nif (!x86_pmu.bts && !x86_pmu.pebs)\r\nreturn;\r\nif (!x86_pmu.bts)\r\nbts_err = 1;\r\nif (!x86_pmu.pebs)\r\npebs_err = 1;\r\nget_online_cpus();\r\nfor_each_possible_cpu(cpu) {\r\nif (alloc_ds_buffer(cpu)) {\r\nbts_err = 1;\r\npebs_err = 1;\r\n}\r\nif (!bts_err && alloc_bts_buffer(cpu))\r\nbts_err = 1;\r\nif (!pebs_err && alloc_pebs_buffer(cpu))\r\npebs_err = 1;\r\nif (bts_err && pebs_err)\r\nbreak;\r\n}\r\nif (bts_err) {\r\nfor_each_possible_cpu(cpu)\r\nrelease_bts_buffer(cpu);\r\n}\r\nif (pebs_err) {\r\nfor_each_possible_cpu(cpu)\r\nrelease_pebs_buffer(cpu);\r\n}\r\nif (bts_err && pebs_err) {\r\nfor_each_possible_cpu(cpu)\r\nrelease_ds_buffer(cpu);\r\n} else {\r\nif (x86_pmu.bts && !bts_err)\r\nx86_pmu.bts_active = 1;\r\nif (x86_pmu.pebs && !pebs_err)\r\nx86_pmu.pebs_active = 1;\r\nfor_each_online_cpu(cpu)\r\ninit_debug_store_on_cpu(cpu);\r\n}\r\nput_online_cpus();\r\n}\r\nvoid intel_pmu_enable_bts(u64 config)\r\n{\r\nunsigned long debugctlmsr;\r\ndebugctlmsr = get_debugctlmsr();\r\ndebugctlmsr |= DEBUGCTLMSR_TR;\r\ndebugctlmsr |= DEBUGCTLMSR_BTS;\r\ndebugctlmsr |= DEBUGCTLMSR_BTINT;\r\nif (!(config & ARCH_PERFMON_EVENTSEL_OS))\r\ndebugctlmsr |= DEBUGCTLMSR_BTS_OFF_OS;\r\nif (!(config & ARCH_PERFMON_EVENTSEL_USR))\r\ndebugctlmsr |= DEBUGCTLMSR_BTS_OFF_USR;\r\nupdate_debugctlmsr(debugctlmsr);\r\n}\r\nvoid intel_pmu_disable_bts(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nunsigned long debugctlmsr;\r\nif (!cpuc->ds)\r\nreturn;\r\ndebugctlmsr = get_debugctlmsr();\r\ndebugctlmsr &=\r\n~(DEBUGCTLMSR_TR | DEBUGCTLMSR_BTS | DEBUGCTLMSR_BTINT |\r\nDEBUGCTLMSR_BTS_OFF_OS | DEBUGCTLMSR_BTS_OFF_USR);\r\nupdate_debugctlmsr(debugctlmsr);\r\n}\r\nint intel_pmu_drain_bts_buffer(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct debug_store *ds = cpuc->ds;\r\nstruct bts_record {\r\nu64 from;\r\nu64 to;\r\nu64 flags;\r\n};\r\nstruct perf_event *event = cpuc->events[INTEL_PMC_IDX_FIXED_BTS];\r\nstruct bts_record *at, *top;\r\nstruct perf_output_handle handle;\r\nstruct perf_event_header header;\r\nstruct perf_sample_data data;\r\nstruct pt_regs regs;\r\nif (!event)\r\nreturn 0;\r\nif (!x86_pmu.bts_active)\r\nreturn 0;\r\nat = (struct bts_record *)(unsigned long)ds->bts_buffer_base;\r\ntop = (struct bts_record *)(unsigned long)ds->bts_index;\r\nif (top <= at)\r\nreturn 0;\r\nds->bts_index = ds->bts_buffer_base;\r\nperf_sample_data_init(&data, 0, event->hw.last_period);\r\nregs.ip = 0;\r\nperf_prepare_sample(&header, &data, event, &regs);\r\nif (perf_output_begin(&handle, event, header.size * (top - at)))\r\nreturn 1;\r\nfor (; at < top; at++) {\r\ndata.ip = at->from;\r\ndata.addr = at->to;\r\nperf_output_sample(&handle, &header, &data, event);\r\n}\r\nperf_output_end(&handle);\r\nevent->hw.interrupts++;\r\nevent->pending_kill = POLL_IN;\r\nreturn 1;\r\n}\r\nstruct event_constraint *intel_pebs_constraints(struct perf_event *event)\r\n{\r\nstruct event_constraint *c;\r\nif (!event->attr.precise_ip)\r\nreturn NULL;\r\nif (x86_pmu.pebs_constraints) {\r\nfor_each_event_constraint(c, x86_pmu.pebs_constraints) {\r\nif ((event->hw.config & c->cmask) == c->code)\r\nreturn c;\r\n}\r\n}\r\nreturn &emptyconstraint;\r\n}\r\nvoid intel_pmu_pebs_enable(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\nhwc->config &= ~ARCH_PERFMON_EVENTSEL_INT;\r\ncpuc->pebs_enabled |= 1ULL << hwc->idx;\r\n}\r\nvoid intel_pmu_pebs_disable(struct perf_event *event)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct hw_perf_event *hwc = &event->hw;\r\ncpuc->pebs_enabled &= ~(1ULL << hwc->idx);\r\nif (cpuc->enabled)\r\nwrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\r\nhwc->config |= ARCH_PERFMON_EVENTSEL_INT;\r\n}\r\nvoid intel_pmu_pebs_enable_all(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nif (cpuc->pebs_enabled)\r\nwrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\r\n}\r\nvoid intel_pmu_pebs_disable_all(void)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nif (cpuc->pebs_enabled)\r\nwrmsrl(MSR_IA32_PEBS_ENABLE, 0);\r\n}\r\nstatic int intel_pmu_pebs_fixup_ip(struct pt_regs *regs)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nunsigned long from = cpuc->lbr_entries[0].from;\r\nunsigned long old_to, to = cpuc->lbr_entries[0].to;\r\nunsigned long ip = regs->ip;\r\nint is_64bit = 0;\r\nif (!x86_pmu.intel_cap.pebs_trap)\r\nreturn 1;\r\nif (!cpuc->lbr_stack.nr || !from || !to)\r\nreturn 0;\r\nif (kernel_ip(ip) != kernel_ip(to))\r\nreturn 0;\r\nif ((ip - to) > PAGE_SIZE)\r\nreturn 0;\r\nif (ip == to) {\r\nset_linear_ip(regs, from);\r\nreturn 1;\r\n}\r\ndo {\r\nstruct insn insn;\r\nu8 buf[MAX_INSN_SIZE];\r\nvoid *kaddr;\r\nold_to = to;\r\nif (!kernel_ip(ip)) {\r\nint bytes, size = MAX_INSN_SIZE;\r\nbytes = copy_from_user_nmi(buf, (void __user *)to, size);\r\nif (bytes != size)\r\nreturn 0;\r\nkaddr = buf;\r\n} else\r\nkaddr = (void *)to;\r\n#ifdef CONFIG_X86_64\r\nis_64bit = kernel_ip(to) || !test_thread_flag(TIF_IA32);\r\n#endif\r\ninsn_init(&insn, kaddr, is_64bit);\r\ninsn_get_length(&insn);\r\nto += insn.length;\r\n} while (to < ip);\r\nif (to == ip) {\r\nset_linear_ip(regs, old_to);\r\nreturn 1;\r\n}\r\nreturn 0;\r\n}\r\nstatic void __intel_pmu_pebs_event(struct perf_event *event,\r\nstruct pt_regs *iregs, void *__pebs)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct pebs_record_core *pebs = __pebs;\r\nstruct perf_sample_data data;\r\nstruct pt_regs regs;\r\nif (!intel_pmu_save_and_restart(event))\r\nreturn;\r\nperf_sample_data_init(&data, 0, event->hw.last_period);\r\nregs = *iregs;\r\nregs.flags = pebs->flags;\r\nset_linear_ip(&regs, pebs->ip);\r\nregs.bp = pebs->bp;\r\nregs.sp = pebs->sp;\r\nif (event->attr.precise_ip > 1 && intel_pmu_pebs_fixup_ip(&regs))\r\nregs.flags |= PERF_EFLAGS_EXACT;\r\nelse\r\nregs.flags &= ~PERF_EFLAGS_EXACT;\r\nif (has_branch_stack(event))\r\ndata.br_stack = &cpuc->lbr_stack;\r\nif (perf_event_overflow(event, &data, &regs))\r\nx86_pmu_stop(event, 0);\r\n}\r\nstatic void intel_pmu_drain_pebs_core(struct pt_regs *iregs)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct debug_store *ds = cpuc->ds;\r\nstruct perf_event *event = cpuc->events[0];\r\nstruct pebs_record_core *at, *top;\r\nint n;\r\nif (!x86_pmu.pebs_active)\r\nreturn;\r\nat = (struct pebs_record_core *)(unsigned long)ds->pebs_buffer_base;\r\ntop = (struct pebs_record_core *)(unsigned long)ds->pebs_index;\r\nds->pebs_index = ds->pebs_buffer_base;\r\nif (!test_bit(0, cpuc->active_mask))\r\nreturn;\r\nWARN_ON_ONCE(!event);\r\nif (!event->attr.precise_ip)\r\nreturn;\r\nn = top - at;\r\nif (n <= 0)\r\nreturn;\r\nWARN_ONCE(n > 1, "bad leftover pebs %d\n", n);\r\nat += n - 1;\r\n__intel_pmu_pebs_event(event, iregs, at);\r\n}\r\nstatic void intel_pmu_drain_pebs_nhm(struct pt_regs *iregs)\r\n{\r\nstruct cpu_hw_events *cpuc = &__get_cpu_var(cpu_hw_events);\r\nstruct debug_store *ds = cpuc->ds;\r\nstruct pebs_record_nhm *at, *top;\r\nstruct perf_event *event = NULL;\r\nu64 status = 0;\r\nint bit, n;\r\nif (!x86_pmu.pebs_active)\r\nreturn;\r\nat = (struct pebs_record_nhm *)(unsigned long)ds->pebs_buffer_base;\r\ntop = (struct pebs_record_nhm *)(unsigned long)ds->pebs_index;\r\nds->pebs_index = ds->pebs_buffer_base;\r\nn = top - at;\r\nif (n <= 0)\r\nreturn;\r\nWARN_ONCE(n > x86_pmu.max_pebs_events, "Unexpected number of pebs records %d\n", n);\r\nfor ( ; at < top; at++) {\r\nfor_each_set_bit(bit, (unsigned long *)&at->status, x86_pmu.max_pebs_events) {\r\nevent = cpuc->events[bit];\r\nif (!test_bit(bit, cpuc->active_mask))\r\ncontinue;\r\nWARN_ON_ONCE(!event);\r\nif (!event->attr.precise_ip)\r\ncontinue;\r\nif (__test_and_set_bit(bit, (unsigned long *)&status))\r\ncontinue;\r\nbreak;\r\n}\r\nif (!event || bit >= x86_pmu.max_pebs_events)\r\ncontinue;\r\n__intel_pmu_pebs_event(event, iregs, at);\r\n}\r\n}\r\nvoid intel_ds_init(void)\r\n{\r\nif (!boot_cpu_has(X86_FEATURE_DTES64))\r\nreturn;\r\nx86_pmu.bts = boot_cpu_has(X86_FEATURE_BTS);\r\nx86_pmu.pebs = boot_cpu_has(X86_FEATURE_PEBS);\r\nif (x86_pmu.pebs) {\r\nchar pebs_type = x86_pmu.intel_cap.pebs_trap ? '+' : '-';\r\nint format = x86_pmu.intel_cap.pebs_format;\r\nswitch (format) {\r\ncase 0:\r\nprintk(KERN_CONT "PEBS fmt0%c, ", pebs_type);\r\nx86_pmu.pebs_record_size = sizeof(struct pebs_record_core);\r\nx86_pmu.drain_pebs = intel_pmu_drain_pebs_core;\r\nbreak;\r\ncase 1:\r\nprintk(KERN_CONT "PEBS fmt1%c, ", pebs_type);\r\nx86_pmu.pebs_record_size = sizeof(struct pebs_record_nhm);\r\nx86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;\r\nbreak;\r\ndefault:\r\nprintk(KERN_CONT "no PEBS fmt%d%c, ", format, pebs_type);\r\nx86_pmu.pebs = 0;\r\n}\r\n}\r\n}
