static int sha1_ssse3_init(struct shash_desc *desc)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\n*sctx = (struct sha1_state){\r\n.state = { SHA1_H0, SHA1_H1, SHA1_H2, SHA1_H3, SHA1_H4 },\r\n};\r\nreturn 0;\r\n}\r\nstatic int __sha1_ssse3_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len, unsigned int partial)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\nunsigned int done = 0;\r\nsctx->count += len;\r\nif (partial) {\r\ndone = SHA1_BLOCK_SIZE - partial;\r\nmemcpy(sctx->buffer + partial, data, done);\r\nsha1_transform_asm(sctx->state, sctx->buffer, 1);\r\n}\r\nif (len - done >= SHA1_BLOCK_SIZE) {\r\nconst unsigned int rounds = (len - done) / SHA1_BLOCK_SIZE;\r\nsha1_transform_asm(sctx->state, data + done, rounds);\r\ndone += rounds * SHA1_BLOCK_SIZE;\r\n}\r\nmemcpy(sctx->buffer, data + done, len - done);\r\nreturn 0;\r\n}\r\nstatic int sha1_ssse3_update(struct shash_desc *desc, const u8 *data,\r\nunsigned int len)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\nunsigned int partial = sctx->count % SHA1_BLOCK_SIZE;\r\nint res;\r\nif (partial + len < SHA1_BLOCK_SIZE) {\r\nsctx->count += len;\r\nmemcpy(sctx->buffer + partial, data, len);\r\nreturn 0;\r\n}\r\nif (!irq_fpu_usable()) {\r\nres = crypto_sha1_update(desc, data, len);\r\n} else {\r\nkernel_fpu_begin();\r\nres = __sha1_ssse3_update(desc, data, len, partial);\r\nkernel_fpu_end();\r\n}\r\nreturn res;\r\n}\r\nstatic int sha1_ssse3_final(struct shash_desc *desc, u8 *out)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\nunsigned int i, index, padlen;\r\n__be32 *dst = (__be32 *)out;\r\n__be64 bits;\r\nstatic const u8 padding[SHA1_BLOCK_SIZE] = { 0x80, };\r\nbits = cpu_to_be64(sctx->count << 3);\r\nindex = sctx->count % SHA1_BLOCK_SIZE;\r\npadlen = (index < 56) ? (56 - index) : ((SHA1_BLOCK_SIZE+56) - index);\r\nif (!irq_fpu_usable()) {\r\ncrypto_sha1_update(desc, padding, padlen);\r\ncrypto_sha1_update(desc, (const u8 *)&bits, sizeof(bits));\r\n} else {\r\nkernel_fpu_begin();\r\nif (padlen <= 56) {\r\nsctx->count += padlen;\r\nmemcpy(sctx->buffer + index, padding, padlen);\r\n} else {\r\n__sha1_ssse3_update(desc, padding, padlen, index);\r\n}\r\n__sha1_ssse3_update(desc, (const u8 *)&bits, sizeof(bits), 56);\r\nkernel_fpu_end();\r\n}\r\nfor (i = 0; i < 5; i++)\r\ndst[i] = cpu_to_be32(sctx->state[i]);\r\nmemset(sctx, 0, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic int sha1_ssse3_export(struct shash_desc *desc, void *out)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\nmemcpy(out, sctx, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic int sha1_ssse3_import(struct shash_desc *desc, const void *in)\r\n{\r\nstruct sha1_state *sctx = shash_desc_ctx(desc);\r\nmemcpy(sctx, in, sizeof(*sctx));\r\nreturn 0;\r\n}\r\nstatic bool __init avx_usable(void)\r\n{\r\nu64 xcr0;\r\nif (!cpu_has_avx || !cpu_has_osxsave)\r\nreturn false;\r\nxcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);\r\nif ((xcr0 & (XSTATE_SSE | XSTATE_YMM)) != (XSTATE_SSE | XSTATE_YMM)) {\r\npr_info("AVX detected but unusable.\n");\r\nreturn false;\r\n}\r\nreturn true;\r\n}\r\nstatic int __init sha1_ssse3_mod_init(void)\r\n{\r\nif (cpu_has_ssse3)\r\nsha1_transform_asm = sha1_transform_ssse3;\r\n#ifdef CONFIG_AS_AVX\r\nif (avx_usable())\r\nsha1_transform_asm = sha1_transform_avx;\r\n#endif\r\nif (sha1_transform_asm) {\r\npr_info("Using %s optimized SHA-1 implementation\n",\r\nsha1_transform_asm == sha1_transform_ssse3 ? "SSSE3"\r\n: "AVX");\r\nreturn crypto_register_shash(&alg);\r\n}\r\npr_info("Neither AVX nor SSSE3 is available/usable.\n");\r\nreturn -ENODEV;\r\n}\r\nstatic void __exit sha1_ssse3_mod_fini(void)\r\n{\r\ncrypto_unregister_shash(&alg);\r\n}
